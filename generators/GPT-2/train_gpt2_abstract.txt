Cosmological Collider Signatures of Massive Vectors from Non-Gaussian Gravitational Waves<|sep|>The cosmological collider provides a model-independent probe of particle physics during inflation. We extend the study of cosmological collider physics to much smaller scales through gravitational wave (GW) probes. With a Chern-Simons interaction, a massive vector field can obtain a chemical potential and its particle production can cause significant non-Gaussian GW signals. We calculate the mass and spin dependences of the induced GW 3-point correlation function in the squeezed limit, and estimate its amplitude. Such signals may be detectable in the current and upcoming GW interferometer experiments.
Green-Blue Stripe Pattern for Range Sensing from a Single Image<|sep|>In this paper, we present a novel method for rapid high-resolution range sensing using green-blue stripe pattern. We use green and blue for designing high-frequency stripe projection pattern. For accurate and reliable range recovery, we identify the stripe patterns by our color-stripe segmentation and unwrapping algorithms. The experimental result for a naked human face shows the effectiveness of our method.
Imprints of Massive Primordial Fields on Large-Scale Structure<|sep|>Attention has focussed recently on models of inflation that involve a second or more fields with a mass near the inflationary Hubble parameter $H$, as may occur in supersymmetric theories if the supersymmetry-breaking scale is not far from $H$. Quasi-single-field (QsF) inflation is a relatively simple family of phenomenological models that serve as a proxy for theories with additional fields with masses $m\sim H$. Since QsF inflation involves fields in addition to the inflaton, the consistency conditions (ccs) between correlations that arise in single-clock inflation are not necessarily satisfied. As a result, correlation functions in the squeezed limit may be larger than in single-field inflation. Scalar non-Gaussianities mediated by the massive isocurvature field in QsF have been shown to be potentially observable. These are especially interesting since they would convey information about the mass of the isocurvature field. Here we consider non-Gaussian correlators involving tensor modes and their observational signatures. A physical correlation between a (long-wavelength) tensor mode and two scalar modes (tss), for instance, may give rise to local departures from statistical isotropy or, in other words, a non-trivial four-point function. The presence of the tensor mode may moreover be inferred geometrically from the shape dependence of the four-point function. We compute tss and stt (one soft curvature mode and two hard tensors) bispectra in QsF inflation, identifying the conditions necessary for these to "violate" the ccs. We find that while ccs are violated by stt correlations, they are preserved by the tss in the minimal QsF model. Our study of primordial correlators which include gravitons in seeking imprints of additional fields with masses $m\sim H$ during inflation can be seen as complementary to the recent "cosmological collider physics" proposal.
Substructure and galaxy formation in the Copernicus Complexio warm dark matter simulations<|sep|>We use the Copernicus Complexio (COCO) high resolution $N$-body simulations to investigate differences in the properties of small-scale structures in the standard cold dark matter (CDM) model and in a model with a cutoff in the initial power spectrum of density fluctuations consistent with both a thermally produced warm dark matter (WDM) particle or a sterile neutrino with mass 7 keV and leptogenesis parameter $L_6=8.7$. The latter corresponds to the "coldest" model with this sterile neutrino mass compatible with the identification of the recently detected 3.5 keV X-ray line as resulting from particle decay. CDM and WDM predict very different number densities of subhaloes with mass $\leq 10^9\,h^{-1}\,M_\odot$ although they predict similar, nearly universal, normalised subhalo radial density distributions. Haloes and subhaloes in both models have cuspy NFW profiles, but WDM subhaloes below the cutoff scale in the power spectrum (corresponding to maximum circular velocities $V_{\mathrm{max}}^{z=0} \leq50~\mathrm{kms}^{-1}$) are less concentrated than their CDM counterparts. We make predictions for observable properties using the GALFORM semi-analytic model of galaxy formation. Both models predict Milky Way satellite luminosity functions consistent with observations, although the WDM model predicts fewer very faint satellites. This model, however, predicts slightly more UV bright galaxies at redshift $z>7$ than CDM, but both are consistent with observations. Gravitational lensing offers the best prospect of distinguishing between the models.
A Representation Learning Framework for Property Graphs<|sep|>Representation learning on graphs, also called graph embedding, has demonstrated its significant impact on a series of machine learning applications such as classification, prediction and recommendation. However, existing work has largely ignored the rich information contained in the properties (or attributes) of both nodes and edges of graphs in modern applications, e.g., those represented by property graphs. To date, most existing graph embedding methods either focus on plain graphs with only the graph topology, or consider properties on nodes only. We propose PGE, a graph representation learning framework that incorporates both node and edge properties into the graph embedding procedure. PGE uses node clustering to assign biases to differentiate neighbors of a node and leverages multiple data-driven matrices to aggregate the property information of neighbors sampled based on a biased strategy. PGE adopts the popular inductive model for neighborhood aggregation. We provide detailed analyses on the efficacy of our method and validate the performance of PGE by showing how PGE achieves better embedding results than the state-of-the-art graph embedding methods on benchmark applications such as node classification and link prediction over real-world datasets.
DeepEM: Deep 3D ConvNets With EM For Weakly Supervised Pulmonary Nodule Detection<|sep|>Recently deep learning has been witnessing widespread adoption in various medical image applications. However, training complex deep neural nets requires large-scale datasets labeled with ground truth, which are often unavailable in many medical image domains. For instance, to train a deep neural net to detect pulmonary nodules in lung computed tomography (CT) images, current practice is to manually label nodule locations and sizes in many CT images to construct a sufficiently large training dataset, which is costly and difficult to scale. On the other hand, electronic medical records (EMR) contain plenty of partial information on the content of each medical image. In this work, we explore how to tap this vast, but currently unexplored data source to improve pulmonary nodule detection. We propose DeepEM, a novel deep 3D ConvNet framework augmented with expectation-maximization (EM), to mine weakly supervised labels in EMRs for pulmonary nodule detection. Experimental results show that DeepEM can lead to 1.5\% and 3.9\% average improvement in free-response receiver operating characteristic (FROC) scores on LUNA16 and Tianchi datasets, respectively, demonstrating the utility of incomplete information in EMRs for improving deep learning algorithms.\footnote{https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git}
A diffuse interface model for the analysis of propagating bulges in cylindrical balloons<|sep|>With the aim to characterize the formation and propagation of bulges in cylindrical rubber balloons, we carry out an expansion of the non-linear axisymmetric membrane model assuming slow axial variations. We obtain a diffuse interface model similar to that introduced by van der Waals in the context of liquid-vapor phase transitions. This provides a quantitative basis to the well-known analogy between propagating bulges and phase transitions. The diffuse interface model is amenable to numerical as well as analytical solutions, including linear and non-linear bifurcation analyses. Comparisons to the original membrane model reveal that the diffuse interface model captures the bulging phenomenon very accurately, even for well-localized phase boundaries.
A Nonlocal Approach to the Cosmological Constant Problem<|sep|>We construct a model in which the cosmological constant is canceled from the gravitational equations of motion. Our model relies on two key ingredients: a nonlocal constraint on the action, which forces the spacetime average of the Lagrangian density to vanish, and a dynamical way for this condition to be satisfied classically with arbitrary matter content. We implement the former condition with a spatially-constant Lagrange multiplier associated with the volume form and the latter by including a free four-form gauge field strength in the action. These two features are enough to remove the cosmological constant from the Einstein equation. The model is consistent with all cosmological and experimental bounds on modification of gravity and allows for both cosmic inflation and the present epoch of acceleration.
Spin dynamics in a strongly driven system: very slow Rabi oscillations<|sep|>We consider joint effects of tunneling and spin-orbit coupling on driven by electric field spin dynamics in a double quantum dot with a multi-level resonance scenario. We demonstrate that tunneling plays the crucial role in the formation of the Rabi-like spin-flip transitions. In contrast to the linear behavior for weak electric fields, the spin flip rate becomes much smaller than expected for the two-level model and shows oscillating dependence on the driving field amplitude in stronger fields. In addition, the full spin flip is very difficult to achieve in a multi-level resonant system. These two effects have a similarity with the Zeno effect of slowing down the dynamics of an observable by its measurement. As a result, spin manipulation by electric field becomes much less efficient than expected.
Hyper-efficient model-independent Bayesian method for the analysis of pulsar timing data<|sep|>A new model independent method is presented for the analysis of pulsar timing data and the estimation of the spectral properties of an isotropic gravitational wave background (GWB). We show that by rephrasing the likelihood we are able to eliminate the most costly aspects of computation normally associated with this type of data analysis. When applied to the International Pulsar Timing Array Mock Data Challenge data sets this results in speedups of approximately 2 to 3 orders of magnitude compared to established methods. We present three applications of the new likelihood. In the low signal to noise regime we sample directly from the power spectrum coefficients of the GWB signal realization. In the high signal to noise regime, where the data can support a large number of coefficients, we sample from the joint probability density of the power spectrum coefficients for the individual pulsars and the GWB signal realization. Critically in both these cases we need make no assumptions about the form of the power spectrum of the GWB, or the individual pulsars. Finally we present a method for characterizing the spatial correlation between pulsars on the sky, making no assumptions about the form of that correlation, and therefore providing the only truly general Bayesian method of confirming a GWB detection from pulsar timing data.
A quantum neural network computes its own relative phase<|sep|>Complete characterization of the state of a quantum system made up of subsystems requires determination of relative phase, because of interference effects between the subsystems. For a system of qubits used as a quantum computer this is especially vital, because the entanglement, which is the basis for the quantum advantage in computing, depends intricately on phase. We present here a first step towards that determination, in which we use a two-qubit quantum system as a quantum neural network, which is trained to compute and output its own relative phase.
Traffic Flows Analysis in High-Speed Computer Networks Using Time Series<|sep|>This article explores the required amount of time series points from a high-speed traffic network to accurately estimate the Hurst exponent. The methodology consists in designing an experiment using estimators that are applied to time series, followed by addressing the minimum amount of points required to obtain accurate estimates of the Hurst exponent in real-time. The methodology addresses the exhaustive analysis of the Hurst exponent considering bias behavior, standard deviation, mean square error, and convergence using fractional gaussian noise signals with stationary increases. Our results show that the Whittle estimator successfully estimates the Hurst exponent in series with few points. Based on the results obtained, a minimum length for the time series is empirically proposed. Finally, to validate the results, the methodology is applied to real traffic captures in a high-speed network based on the IEEE 802.3ab standard.
M-Power Regularized Least Squares Regression<|sep|>Regularization is used to find a solution that both fits the data and is sufficiently smooth, and thereby is very effective for designing and refining learning algorithms. But the influence of its exponent remains poorly understood. In particular, it is unclear how the exponent of the reproducing kernel Hilbert space~(RKHS) regularization term affects the accuracy and the efficiency of kernel-based learning algorithms. Here we consider regularized least squares regression (RLSR) with an RKHS regularization raised to the power of m, where m is a variable real exponent. We design an efficient algorithm for solving the associated minimization problem, we provide a theoretical analysis of its stability, and we compare its advantage with respect to computational complexity, speed of convergence and prediction accuracy to the classical kernel ridge regression algorithm where the regularization exponent m is fixed at 2. Our results show that the m-power RLSR problem can be solved efficiently, and support the suggestion that one can use a regularization term that grows significantly slower than the standard quadratic growth in the RKHS norm.
Efficient Continual Learning Ensembles in Neural Network Subspaces<|sep|>A growing body of research in continual learning focuses on the catastrophic forgetting problem. While many attempts have been made to alleviate this problem, the majority of the methods assume a single model in the continual learning setup. In this work, we question this assumption and show that employing ensemble models can be a simple yet effective method to improve continual performance. However, the training and inference cost of ensembles can increase linearly with the number of models. Motivated by this limitation, we leverage the recent advances in the deep learning optimization literature, such as mode connectivity and neural network subspaces, to derive a new method that is both computationally advantageous and can outperform the state-of-the-art continual learning algorithms.
Fault Localization in Web Applications via Model Finding<|sep|>We describe a generic technique for fault localization independent from the nature of the object or the specification language used to declare its expected properties. This technique is based on the concept of "repair", a minimal set of transformations which, when applied to the original object, restores its satisfiability with respect to the specification. We show how this technique can be applied with various specification languages, including propositional and finite first-order logic. In particular, we focus on its use in the detection of layout faults in web applications.
The Magnetic Field of Active Region 11158 During the 2011 February 12-17 Flares : Differences between Photospheric Extrapolation and Coronal Forward-Fitting Methods<|sep|>We developed a {\sl coronal non-linear force-free field (COR-NLFFF)} forward-fitting code that fits an approximate {\sl non-linear force-free field (NLFFF)} solution to the observed geometry of automatically traced coronal loops. In contrast to photospheric NLFFF codes, which calculate a magnetic field solution from the constraints of the transverse photospheric field, this new code uses coronal constraints instead, and this way provides important information on systematic errors of each magnetic field calculation method, as well as on the non-forcefreeness in the lower chromosphere. In this study we applied the COR-NLFFF code to active region NOAA 11158, during the time interval of 2011 Feb 12 to 17, which includes an X2.2 GOES-class flare plus 35 M and C-class flares. We calcuated the free magnetic energy with a 6-minute cadence over 5 days. We find good agreement between the two types of codes for the total nonpotential $E_N$ and potential energy $E_P$, but find up to a factor of 4 discrepancy in the free energy $E_{free}=E_N-E_P$, and up to a factor of 10 discrepancy in the decrease of the free energy $\Delta E_{free}$ during flares. The coronal NLFFF code exhibits a larger time variability, and yields a decrease of free energy during the flare that is sufficient to satisfy the flare energy budget, while the photospheric NLFFF code shows much less time variability and an order of magnitude less free energy decrease during flares. The discrepancy may partly be due to the pre-processing of photospheric vector data, but more likely due to the non-forcefreeness in the lower chromosphere. We conclude that the coronal field cannot be correctly calculated based on photospheric data alone, but requires additional information on coronal loop geometries.
A Case-Study of Sample-Based Bayesian Forecasting Algorithms<|sep|>For a Bayesian, real-time forecasting with the posterior predictive distribution can be challenging for a variety of time series models. First, estimating the parameters of a time series model can be difficult with sample-based approaches when the model's likelihood is intractable and/or when the data set being used is large. Second, once samples from a parameter posterior are obtained on a fixed window of data, it is not clear how they will be used to generate forecasts, nor is it clear how, and in what sense, they will be ``updated" as interest shifts to newer posteriors as new data arrive. This paper provides a comparison of the sample-based forecasting algorithms that are available for Bayesians interested in real-time forecasting with nonlinear/non-Gaussian state space models. An applied analysis of financial returns is provided using a well-established stochastic volatility model. The principal aim of this paper is to provide guidance on how to select one of these algorithms, and to describe a variety of benefits and pitfalls associated with each approach.
A Three-Dimensional Mathematical Model of Collagen Contraction<|sep|>In this paper, we introduce a three-dimensional mathematical model of collagen contraction with microbuckling based on the two-dimensional model previously developed by the authors. The model both qualitatively and quantitatively replicates experimental data including lattice contraction over a time course of 40 hours for lattices with various cell densities, cell density profiles within contracted lattices, radial cut angles in lattices, and cell force propagation within a lattice. The importance of the model lattice formation and the crucial nature of its connectivity are discussed including differences with models which do not include microbuckling. The model suggests that most cells within contracting lattices are engaged in directed motion.
Head and Tail Localization of C. elegans<|sep|>C. elegans is commonly used in neuroscience for behaviour analysis because of it's compact nervous system with well-described connectivity. Localizing the animal and distinguishing between its head and tail are important tasks to track the worm during behavioural assays and to perform quantitative analyses. We demonstrate a neural network based approach to localize both the head and the tail of the worm in an image. To make empirical results in the paper reproducible and promote open source machine learning based solutions for C. elegans behavioural analysis, we also make our code publicly available.
Clusters in Light Nuclei<|sep|>A great deal of research work has been undertaken in the alpha-clustering study since the pioneering discovery, half a century ago, of 12C+12C molecular resonances. Our knowledge of the field of the physics of nuclear molecules has increased considerably and nuclear clustering remains one of the most fruitful domains of nuclear physics, facing some of the greatest challenges and opportunities in the years ahead. In this work, the occurence of "exotic" shapes in light N=Z alpha-like nuclei is investigated. Various approaches of superdeformed and hyperdeformed bands associated with quasimolecular resonant structures are presented. Results on clustering aspects are also discussed for light neutron-rich Oxygen isotopes.
Quantifying Chaos in Models of the Solar Neighbourhood<|sep|>{} {To quantify the amount of chaos that exists in the local phase space.} {A sample of orbits from four different models of the Solar neighbourhood phase space are analysed by a new chaos identification (and quantification) technique. While three of the used models bear the signature of the perturbation due to both the Galactic bar and the spiral pattern, the last of the models is a bar only one. We explore the models by inter-comparing the corresponding values of chaos strength that is induced at the various energy levels .}{(1) We find that of all the viable models that have been demonstrated to successfully reproduce the local phase space structure, i.e. those that include the bar as well as the spiral, bear strong chaoticity, though the model that implies the highest degree of chaos is the one in which overlap of the major resonances of the bar and the spiral occurs. The bar only model is found to display regularity. (2) We advance chaos to be primarily responsible for the splitting of the Hyades-Pleiades mode (the larger mode) of the local velocity distribution}{}
Robustness Assessment of Hetero-Functional Graph Theory Based Model of Interdependent Urban Utility Networks<|sep|>The increasing urban population imposes a substantial and growing burden on the supporting infrastructure, such as electricity, water, heating, natural gas, road transportation, etc. This paper presents a Hetero-functional graph theory (HFGT) based modeling framework for these integrated infrastructures followed by an analysis of network robustness. The supporting infrastructures along with the infrastructure repair facilities are considered. In contrast to conventional graph representations, a weighted HFGT model is used to capture the system processes and mutual dependencies among resources. To assess robustness of the inter-dependent networks, impacts of complete/partial and random/targeted attacks are quantified. Specifically, various contingency scenarios are simulated and the vulnerability of the network is evaluated. Additionally, several robustness metrics are proposed to provide a comprehensive evaluation of system robustness. The proposed weighted HFGT modeling and robustness assessment approach is tested using a synthetic interdependent network, comprising of an electrical power system, a water network, a district heating network, a natural gas system and a road transportation network. Results demonstrate that system robustness can be enhanced via securing system information and mitigating attack strength.
Graphical Fermat's Principle and Triangle-Free Graph Estimation<|sep|>We consider the problem of estimating undirected triangle-free graphs of high dimensional distributions. Triangle-free graphs form a rich graph family which allows arbitrary loopy structures but 3-cliques. For inferential tractability, we propose a graphical Fermat's principle to regularize the distribution family. Such principle enforces the existence of a distribution-dependent pseudo-metric such that any two nodes have a smaller distance than that of two other nodes who have a geodesic path include these two nodes. Guided by this principle, we show that a greedy strategy is able to recover the true graph. The resulting algorithm only requires a pairwise distance matrix as input and is computationally even more efficient than calculating the minimum spanning tree. We consider graph estimation problems under different settings, including discrete and nonparametric distribution families. Thorough numerical results are provided to illustrate the usefulness of the proposed method.
The Evolution towards Electron-capture Supernovae: the Flame Propagation and the Pre-bounce Electron-neutrino Radiation<|sep|>A critical mass ONe core with a high ignition density is considered to end in gravitational collapse leading to neutron star formation. Being distinct from a Fe core collapse, the final evolution involves combustion flame propagation, in which complex phase transition from ONe elements into the nuclear-statistical-equilibrium (NSE) state takes place. We simulate the core evolution from the O+Ne ignition until the bounce shock penetrates the whole core, using a state-of-the-art 1D Lagrangian neutrino-radiation-hydrodynamic code, in which important nuclear burning, electron capture, and neutrino reactions are taken into account. Special care is also taken in making a stable initial condition by importing the stellar EOS, which is used for the progenitor evolution calculation, and by improving the remapping process. We find that the central ignition leads to intense $\nu_e$ radiation with $L_{\nu_e} \gtrsim 10^{51}$ erg s$^{-1}$ powered by fast electron captures onto NSE isotopes. This pre-bounce $\nu_e$ radiation heats the surroundings by the neutrino-electron scattering, which acts as a new driving mechanism of the flame propagation together with the adiabatic contraction. The resulting flame velocity of $\sim10^8$ cm s$^{-1}$ will be more than one-order-of-magnitude faster than that of laminar flame driven by heat conduction. We also find that the duration of the pre-bounce $\nu_e$ radiation phase depends on the degree of the core hydrostatic/dynamical stability. Therefore, the future detection of the pre-bounce neutrino is important not only to discriminate the ONe core collapse from the Fe core collapse but also to constrain the progenitor hydrodynamical stability.
Design and performance of an absolute $^3$He/Cs magnetometer<|sep|>We report on the design and performance of a highly sensitive combined $^3$He/Cs magnetometer for the absolute measurement of magnetic fields. The magnetometer relies on the magnetometric detection of the free spin precession of nuclear spin polarized $^3$He gas by optically pumped cesium magnetometers. We plan to deploy this type of combined magnetometer in an experiment searching for a permanent electric dipole moment of ultracold neutrons at the Paul Scherrer Institute (Switzerland). A prototype magnetometer was built at the University of Fribourg (Switzerland) and tested at Physikalisch-Technische Bundesanstalt (Berlin, Germany). We demonstrate that the combined magnetometer allows Cram\'er-Rao- limited field determinations with recording times in the range of $\sim 500\mathrm{s}$, measurements above $500\mathrm{s}$ being limited by the stability of the applied magnetic field. % With a $100\mathrm{s}$ recording time we were able to perform an absolute measurement of a magnetic field of $\approx1\mathrm{\mu T}$ with a standard uncertainty of $\Delta B\sim60\mathrm{fT}$, corresponding to $\Delta B/B<$6$\times$10$^{-8}$.
Counting Polynomial Roots in Isabelle/HOL: A Formal Proof of the Budan-Fourier Theorem<|sep|>Many problems in computer algebra and numerical analysis can be reduced to counting or approximating the real roots of a polynomial within an interval. Existing verified root-counting procedures in major proof assistants are mainly based on the classical Sturm theorem, which only counts distinct roots. In this paper, we have strengthened the root-counting ability in Isabelle/HOL by first formally proving the Budan-Fourier theorem. Subsequently, based on Descartes' rule of signs and Taylor shift, we have provided a verified procedure to efficiently over-approximate the number of real roots within an interval, counting multiplicity. For counting multiple roots exactly, we have extended our previous formalisation of Sturm's theorem. Finally, we combine verified components in the developments above to improve our previous certified complex-root-counting procedures based on Cauchy indices. We believe those verified routines will be crucial for certifying programs and building tactics.
Counting Path Configurations in Parallel Diffusion<|sep|>Parallel Diffusion is a variant of Chip-Firing introduced in 2018 by Duffy et al. In Parallel Diffusion, chips move from places of high concentration to places of low concentration through a discrete-time process. At each time step, every vertex sends a chip to each of its poorer neighbours, allowing for some vertices to perhaps fall into debt (represented by negative stack sizes). In their recent paper, Long and Narayanan proved a conjecture from the original paper by Duffy et al. that every Parallel Diffusion process eventually, after some pre-period, exhibits periodic behaviour. With this result, we are now able to count the number of these periods that exist up to a definition of isomorphism. We determine a recurrence relation for calculating this number for a path of any length. If $T_n$ is the number of configurations with period length 2 that can exist on $P_n$ up to isomorphism and $n$ is an integer greater than 4, we conclude that $T_n = 3T_{n-1} + 2T_{n-2} + T_{n-3} - T_{n-4}$.
Determining the effects of clumping and porosity on the chemistry in a non-uniform AGB outflow<|sep|>(abridged) In the inner regions of AGB outflows, several molecules have been detected with abundances much higher than those predicted from thermodynamic equilibrium (TE) chemical models. The presence of the majority of these species can be explained by shock-induced non-TE chemical models, where shocks caused by the pulsating star take the chemistry out of TE in the inner region. Moreover, a non-uniform density structure has been detected in several AGB outflows. A detailed parameter study on the quantitative effects of a non-homogeneous outflow has so far not been performed. We implement a porosity formalism for treating the increased leakage of light associated with radiation transport through a clumpy, porous medium. The effects from the altered UV radiation field penetration on the chemistry, accounting also for the increased reaction rates of two-body processes in the overdense clumps, are examined. We present a parameter study of the effect of clumping and porosity on the chemistry throughout the outflow. Both the higher density within the clumps and the increased UV radiation field penetration have an important impact on the chemistry, as they both alter the chemical pathways. The increased amount of UV radiation in the inner region leads to photodissociation of parent species, releasing the otherwise deficient elements. We find an increased abundance in the inner region of all species not expected to be present assuming TE chemistry, such as HCN in O-rich outflows, H$_2$O in C-rich outflows, and NH$_3$ in both. Outflows whose clumps have a large overdensity and that are very porous to the interstellar UV radiation field yield abundances comparable to those observed in O- and C-rich outflows for most of the unexpected species investigated. The inner wind abundances of H$_2$O in C-rich outflows and of NH$_3$ in O- and C-rich outflows are however underpredicted.
Recursive Inference for Variational Autoencoders<|sep|>Inference networks of traditional Variational Autoencoders (VAEs) are typically amortized, resulting in relatively inaccurate posterior approximation compared to instance-wise variational optimization. Recent semi-amortized approaches were proposed to address this drawback; however, their iterative gradient update procedures can be computationally demanding. To address these issues, in this paper we introduce an accurate amortized inference algorithm. We propose a novel recursive mixture estimation algorithm for VAEs that iteratively augments the current mixture with new components so as to maximally reduce the divergence between the variational and the true posteriors. Using the functional gradient approach, we devise an intuitive learning criteria for selecting a new mixture component: the new component has to improve the data likelihood (lower bound) and, at the same time, be as divergent from the current mixture distribution as possible, thus increasing representational diversity. Compared to recently proposed boosted variational inference (BVI), our method relies on amortized inference in contrast to BVI's non-amortized single optimization instance. A crucial benefit of our approach is that the inference at test time requires a single feed-forward pass through the mixture inference network, making it significantly faster than the semi-amortized approaches. We show that our approach yields higher test data likelihood than the state-of-the-art on several benchmark datasets.
An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines<|sep|>The extraction of anglicisms (lexical borrowings from English) is relevant both for lexicographic purposes and for NLP downstream tasks. We introduce a corpus of European Spanish newspaper headlines annotated with anglicisms and a baseline model for anglicism extraction. In this paper we present: (1) a corpus of 21,570 newspaper headlines written in European Spanish annotated with emergent anglicisms and (2) a conditional random field baseline model with handcrafted features for anglicism extraction. We present the newspaper headlines corpus, describe the annotation tagset and guidelines and introduce a CRF model that can serve as baseline for the task of detecting anglicisms. The presented work is a first step towards the creation of an anglicism extractor for Spanish newswire.
Stable Roommates Problem with Random Preferences<|sep|>The stable roommates problem with $n$ agents has worst case complexity $O(n^2)$ in time and space. Random instances can be solved faster and with less memory, however. We introduce an algorithm that has average time and space complexity $O(n^\frac{3}{2})$ for random instances. We use this algorithm to simulate large instances of the stable roommates problem and to measure the probabilty $p_n$ that a random instance of size $n$ admits a stable matching. Our data supports the conjecture that $p_n = \Theta(n^{-1/4})$.
Developing a Machine Learning Algorithm-Based Classification Models for the Detection of High-Energy Gamma Particles<|sep|>Cherenkov gamma telescope observes high energy gamma rays, taking advantage of the radiation emitted by charged particles produced inside the electromagnetic showers initiated by the gammas, and developing in the atmosphere. The detector records and allows for the reconstruction of the shower parameters. The reconstruction of the parameter values was achieved using a Monte Carlo simulation algorithm called CORSIKA. The present study developed multiple machine-learning-based classification models and evaluated their performance. Different data transformation and feature extraction techniques were applied to the dataset to assess the impact on two separate performance metrics. The results of the proposed application reveal that the different data transformations did not significantly impact (p = 0.3165) the performance of the models. A pairwise comparison indicates that the performance from each transformed data was not significantly different from the performance of the raw data. Additionally, the SVM algorithm produced the highest performance score on the standardized dataset. In conclusion, this study suggests that high-energy gamma particles can be predicted with sufficient accuracy using SVM on a standardized dataset than the other algorithms with the various data transformations.
Cluster properties from two-particle angular correlations in p+p collisions at $\sqrt{s}$ = 200 and 410 GeV<|sep|>We present results on two-particle angular correlations in proton-proton collisions at center of mass energies of 200 and 410 GeV. The PHOBOS experiment at the Relativistic Heavy Ion Collider has a uniquely large coverage for charged particles, giving the opportunity to explore the correlations at both short- and long-range scales. At both energies, a complex two-dimensional correlation structure in $\Delta \eta$ and $\Delta \phi$ is observed. In the context of an independent cluster model of short-range correlations, the cluster size and its decay width are extracted from the two-particle pseudorapidity correlation function and compared with previous measurements in proton-proton and proton-antiproton collisions, as well as PYTHIA and HIJING predictions.
YAPA: A generic tool for computing intruder knowledge<|sep|>Reasoning about the knowledge of an attacker is a necessary step in many formal analyses of security protocols. In the framework of the applied pi calculus, as in similar languages based on equational logics, knowledge is typically expressed by two relations: deducibility and static equivalence. Several decision procedures have been proposed for these relations under a variety of equational theories. However, each theory has its particular algorithm, and none has been implemented so far. We provide a generic procedure for deducibility and static equivalence that takes as input any convergent rewrite system. We show that our algorithm covers most of the existing decision procedures for convergent theories. We also provide an efficient implementation, and compare it briefly with the tools ProVerif and KiSs.
Semileptonic Transition of $\Lambda_{b}$ Baryon<|sep|>The semileptonic transition of $\Lambda_b$ baryon is studied using the Hypercentral constituent quark model. The six-dimensional hyperradial $Schr\ddot{o}dinger$ equation is solved in the variational approach to get masses and wavefunctions of heavy baryons. The matrix elements of weak decay are written in terms of the overlap integrals of the baryon wave function. The Isgur-Wise function is determined to calculate exclusive semileptonic decay $\Lambda_b$ $\rightarrow$ $\Lambda_c$ $\ell$ $\bar{\nu}$. The calculated decay rate and the branching ratio of $\Lambda_b$ baryon are consistent with other theoretical predictions and with the available experimental observations.
Effects of the variation of mass on fermion localization and resonances on thick branes<|sep|>A few years ago, Campos investigated the critical phenomena of thick branes in warped spacetimes [Phys. Rev. Lett. {\bf 88} (2002) 141602]. Inspired by his work, we consider a toy model of thick branes generated by a real scalar field with the potential $V(\phi)=a\phi^2-b\phi^4+c\phi^6$, and investigate the variation of the mass parameter $a$ on the branes as well as the localization and resonances of fermions. An interesting result is found: there is a critical value for the mass parameter $a$, and when the critical value of $a$ is reached the solution of the background scalar field is not unique and has the shape of a double kink. This happens in both cases with and without gravity. It is also shown that the numbers of the bound Kaluza-Klein modes of fermions on the gravity-free brane and the resonant states of fermions on the brane with gravity increase with the value of $a$.
Highly-Economized Multi-View Binary Compression for Scalable Image Clustering<|sep|>How to economically cluster large-scale multi-view images is a long-standing problem in computer vision. To tackle this challenge, we introduce a novel approach named Highly-economized Scalable Image Clustering (HSIC) that radically surpasses conventional image clustering methods via binary compression. We intuitively unify the binary representation learning and efficient binary cluster structure learning into a joint framework. In particular, common binary representations are learned by exploiting both sharable and individual information across multiple views to capture their underlying correlations. Meanwhile, cluster assignment with robust binary centroids is also performed via effective discrete optimization under L21-norm constraint. By this means, heavy continuous-valued Euclidean distance computations can be successfully reduced by efficient binary XOR operations during the clustering procedure. To our best knowledge, HSIC is the first binary clustering work specifically designed for scalable multi-view image clustering. Extensive experimental results on four large-scale image datasets show that HSIC consistently outperforms the state-of-the-art approaches, whilst significantly reducing computational time and memory footprint.
Modified Theory of Gravity and Clustering of Multi-Component System of Galaxies<|sep|>In this paper, we analyze the clustering of galaxies using a modified theory of gravity, in which the field content of general relativity has been be increased. This increasing in the field content of general relativity changes the large distance behavior of the theory, and in weak field approximation, it will also modify the large distance behavior of Newtonian potential. So, we will analyzing the clustering of multi-component system of galaxies interacting through this modified Newtonian potential. We will obtain the partition function for this multi-component system, and study the thermodynamics of this system. So, we will analyze the effects of the large distance modification to the Newtonian potential on Helmholtz free energy, internal energy, entropy, pressure and chemical potential of this system. We obtain also the modified distribution function and the modified clustering parameter for this system, and hence observe the effect of large distance modification of Newtonian potential on clustering of galaxies.
Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation<|sep|>Monocular 3D human pose estimation technologies have the potential to greatly increase the availability of human movement data. The best-performing models for single-image 2D-3D lifting use graph convolutional networks (GCNs) that typically require some manual input to define the relationships between different body joints. We propose a novel transformer-based approach that uses the more generalised self-attention mechanism to learn these relationships within a sequence of tokens representing joints. We find that the use of intermediate supervision, as well as residual connections between the stacked encoders benefits performance. We also suggest that using error prediction as part of a multi-task learning framework improves performance by allowing the network to compensate for its confidence level. We perform extensive ablation studies to show that each of our contributions increases performance. Furthermore, we show that our approach outperforms the recent state of the art for single-frame 3D human pose estimation by a large margin. Our code and trained models are made publicly available on Github.
Probably Approximately Metric-Fair Learning<|sep|>The seminal work of Dwork {\em et al.} [ITCS 2012] introduced a metric-based notion of individual fairness. Given a task-specific similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational intractability even for simple fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of {\em approximate metric-fairness}: for a random pair of individuals sampled from the population, with all but a small probability of error, if they are similar then they should be treated similarly. We formalize the goal of achieving approximate metric-fairness simultaneously with best-possible accuracy as Probably Approximately Correct and Fair (PACF) Learning. We show that approximate metric-fairness {\em does} generalize, and leverage these generalization guarantees to construct polynomial-time PACF learning algorithms for the classes of linear and logistic predictors.
Bayesian modelling of time-varying conditional heteroscedasticity<|sep|>Conditional heteroscedastic (CH) models are routinely used to analyze financial datasets. The classical models such as ARCH-GARCH with time-invariant coefficients are often inadequate to describe frequent changes over time due to market variability. However we can achieve significantly better insight by considering the time-varying analogues of these models. In this paper, we propose a Bayesian approach to the estimation of such models and develop computationally efficient MCMC algorithm based on Hamiltonian Monte Carlo (HMC) sampling. We also established posterior contraction rates with increasing sample size in terms of the average Hellinger metric. The performance of our method is compared with frequentist estimates and estimates from the time constant analogues. To conclude the paper we obtain time-varying parameter estimates for some popular Forex (currency conversion rate) and stock market datasets.
Stability of general-relativistic accretion disks<|sep|>Self-gravitating relativistic disks around black holes can form as transient structures in a number of astrophysical scenarios such as binary neutron star and black hole-neutron star coalescences, as well as the core-collapse of massive stars. We explore the stability of such disks against runaway and non-axisymmetric instabilities using three-dimensional hydrodynamics simulations in full general relativity using the THOR code. We model the disk matter using the ideal fluid approximation with a $\Gamma$-law equation of state with $\Gamma=4/3$. We explore three disk models around non-rotating black holes with disk-to-black hole mass ratios of 0.24, 0.17 and 0.11. Due to metric blending in our initial data, all of our initial models contain an initial axisymmetric perturbation which induces radial disk oscillations. Despite these oscillations, our models do not develop the runaway instability during the first several orbital periods. Instead, all of the models develop unstable non-axisymmetric modes on a dynamical timescale. We observe two distinct types of instabilities: the Papaloizou-Pringle and the so-called intermediate type instabilities. The development of the non-axisymmetric mode with azimuthal number m = 1 is accompanied by an outspiraling motion of the black hole, which significantly amplifies the growth rate of the m = 1 mode in some cases. Overall, our simulations show that the properties of the unstable non-axisymmetric modes in our disk models are qualitatively similar to those in Newtonian theory.
Comparing Acoustic-based Approaches for Alzheimer's Disease Detection<|sep|>Robust strategies for Alzheimer's disease (AD) detection are important, given the high prevalence of AD. In this paper, we study the performance and generalizability of three approaches for AD detection from speech on the recent ADReSSo challenge dataset: 1) using conventional acoustic features 2) using novel pre-trained acoustic embeddings 3) combining acoustic features and embeddings. We find that while feature-based approaches have a higher precision, classification approaches relying on pre-trained embeddings prove to have a higher, and more balanced cross-validated performance across multiple metrics of performance. Further, embedding-only approaches are more generalizable. Our best model outperforms the acoustic baseline in the challenge by 2.8%.
Novelty search employed into the development of cancer treatment simulations<|sep|>Conventional optimization methodologies may be hindered when the automated search is stuck into local optima because of a deceptive objective function landscape. Consequently, open ended search methodologies, such as novelty search, have been proposed to tackle this issue. Overlooking the objective, while putting pressure into discovering novel solutions may lead to better solutions in practical problems. Novelty search was employed here to optimize the simulated design of a targeted drug delivery system for tumor treatment under the PhysiCell simulator. A hybrid objective equation was used containing both the actual objective of an effective tumour treatment and the novelty measure of the possible solutions. Different weights of the two components of the hybrid equation were investigated to unveil the significance of each one.
Random acceleration process on finite intervals under stochastic restarting<|sep|>The escape of the randomly accelerated undamped particle from the finite interval under action of stochastic resetting is studied. The motion of such a particle is described by the full Langevin equation and the particle is characterized by the position and velocity. We compare three resetting protocols, which restarts velocity or position (partial resetting) and the whole motion (position and velocity -- full resetting). Using the mean first passage time we assess efficiency of restarting protocols in facilitating or suppressing the escape kinetics. There are fundamental differences between partial resetting scenarios which restart velocity or position, as in the limit of very frequent resets only the position resetting (regardless of initial velocity) can trap the particle in the finite domain of motion. The velocity resetting or the simultaneous position and velocity restarting provide a possibility of facilitating the undamped escape kinetics.
Polyhedral Voronoi Cells<|sep|>Voronoi cells of a discrete set in Euclidean space are known as generalized polyhedra. We identify polyhedral cells of a discrete set through a direction cone. For an arbitrary set we distinguish polyhedral from non-polyhedral cells using inversion at a sphere and a theorem of semi-infinite linear programming.
Simple Local Attentions Remain Competitive for Long-Context Tasks<|sep|>Many NLP tasks require processing long contexts beyond the length limit of pretrained models. In order to scale these models to longer text sequences, many efficient long-range attention variants have been proposed. Despite the abundance of research along this direction, it is still difficult to gauge the relative effectiveness of these models in practical use cases, e.g., if we apply these models following the pretrain-and-finetune paradigm. In this work, we aim to conduct a thorough analysis of these emerging models with large-scale and controlled experiments. For each attention variant, we pretrain large-size models using the same long-doc corpus and then finetune these models for real-world long-context tasks. Our findings reveal pitfalls of an existing widely-used long-range benchmark and show none of the tested efficient attentions can beat a simple local window attention under standard pretraining paradigms. Further analysis on local attention variants suggests that even the commonly used attention-window overlap is not necessary to achieve good downstream results -- using disjoint local attentions, we are able to build a simpler and more efficient long-doc QA model that matches the performance of Longformer~\citep{longformer} with half of its pretraining compute. The code to replicate our experiments can be found at https://github.com/pytorch/fairseq/tree/main/examples/xformers
A Bayesian model for recognizing handwritten mathematical expressions<|sep|>Recognizing handwritten mathematics is a challenging classification problem, requiring simultaneous identification of all the symbols comprising an input as well as the complex two-dimensional relationships between symbols and subexpressions. Because of the ambiguity present in handwritten input, it is often unrealistic to hope for consistently perfect recognition accuracy. We present a system which captures all recognizable interpretations of the input and organizes them in a parse forest from which individual parse trees may be extracted and reported. If the top-ranked interpretation is incorrect, the user may request alternates and select the recognition result they desire. The tree extraction step uses a novel probabilistic tree scoring strategy in which a Bayesian network is constructed based on the structure of the input, and each joint variable assignment corresponds to a different parse tree. Parse trees are then reported in order of decreasing probability. Two accuracy evaluations demonstrate that the resulting recognition system is more accurate than previous versions (which used non-probabilistic methods) and other academic math recognizers.
Parametrics Resonances of a Forced Modified Rayleigh-Duffing Oscillator<|sep|>We investigate in this paper the superharmonic and subharmonic resonances of forced modified Rayleigh-Duffing oscillator. We analyse this equation by the method of multiple scales and we obtain superharmonic, subharmonic resonances order-two and order-three and primary resonance. We obtain also regions where steady-state subharmonic responses exist. We also use the amplitude-frequency curve for demonstrate the effect of various parameters on the response of the system. Finally, we focus our attention on chaotic motion of this oscillator by simulation. We obtain that this oscillator is chaotic for certains values for natural and excitation frequency but chaotic motion is not the same in subharmonic and superharmonic cases.
Using GANs to Synthesise Minimum Training Data for Deepfake Generation<|sep|>There are many applications of Generative Adversarial Networks (GANs) in fields like computer vision, natural language processing, speech synthesis, and more. Undoubtedly the most notable results have been in the area of image synthesis and in particular in the generation of deepfake videos. While deepfakes have received much negative media coverage, they can be a useful technology in applications like entertainment, customer relations, or even assistive care. One problem with generating deepfakes is the requirement for a lot of image training data of the subject which is not an issue if the subject is a celebrity for whom many images already exist. If there are only a small number of training images then the quality of the deepfake will be poor. Some media reports have indicated that a good deepfake can be produced with as few as 500 images but in practice, quality deepfakes require many thousands of images, one of the reasons why deepfakes of celebrities and politicians have become so popular. In this study, we exploit the property of a GAN to produce images of an individual with variable facial expressions which we then use to generate a deepfake. We observe that with such variability in facial expressions of synthetic GAN-generated training images and a reduced quantity of them, we can produce a near-realistic deepfake videos.
A Nonlinear Theory of Prestressed Elastic Stick-and-Spring Structures<|sep|>The discrete modeling of a large class of mechanical structures can be based on a stick-and-spring concept. We here present a stick-and-spring theory with potential application to the statics and the dynamics of such nanostructures as graphene, carbon nanotubes, viral capsids, and others. A key feature of our theory is its geometrical nonlinearity: we combine exactly defined strain measures with a general linear stress response; another, rarely found feature is a careful account of prestress states. A linear version is firstly proposed, where attention is restricted to study small displacements from an unstressed reference placement. Next, a theory linearized about a prestressed (preloaded or not) placement is displayed, which is based on a careful analysis of the tangent stiffness operator and its two parts, the elastic and prestress stiffness operators. Finally, two examples are proposed and solved; when an analytical solution is of prohibitive complication, numerical solutions are given, by the use of a specifically implemented `stick-and-spring' code.
Synthetic Latent Fingerprint Generator<|sep|>Given a full fingerprint image (rolled or slap), we present CycleGAN models to generate multiple latent impressions of the same identity as the full print. Our models can control the degree of distortion, noise, blurriness and occlusion in the generated latent print images to obtain Good, Bad and Ugly latent image categories as introduced in the NIST SD27 latent database. The contributions of our work are twofold: (i) demonstrate the similarity of synthetically generated latent fingerprint images to crime scene latents in NIST SD27 and MSP databases as evaluated by the NIST NFIQ 2 quality measure and ROC curves obtained by a SOTA fingerprint matcher, and (ii) use of synthetic latents to augment small-size latent training databases in the public domain to improve the performance of DeepPrint, a SOTA fingerprint matcher designed for rolled to rolled fingerprint matching on three latent databases (NIST SD27, NIST SD302, and IIITD-SLF). As an example, with synthetic latent data augmentation, the Rank-1 retrieval performance of DeepPrint is improved from 15.50% to 29.07% on challenging NIST SD27 latent database. Our approach for generating synthetic latent fingerprints can be used to improve the recognition performance of any latent matcher and its individual components (e.g., enhancement, segmentation and feature extraction).
CP Symmetry and Lepton Mixing from a Scan of Finite Discrete Groups<|sep|>Including the generalized CP symmetry, we have performed a comprehensive scan of leptonic mixing patterns which can be obtained from finite discrete groups with order less than 2000. Both the semidirect approach and its variant are considered. The lepton mixing matrices which can admit a good agreement with experimental data can be organized into eight different categories up to possible row and column permutations. These viable mixing patterns can be completely obtained from the discrete flavor groups $\Delta(6n^2)$, $D^{(1)}_{9n,3n}$, $A_5$ and $\Sigma(168)$ combined with CP symmetry. We perform a detailed analytical and numerical analysis for each possible mixing pattern. The resulting predictions for lepton mixing parameter, neutrinoless double decay and flavored leptogenesis are studied.
Impact of the Optimum Routing and Least Overhead Routing Approaches on Minimum Hop Routes and Connected Dominating Sets in Mobile Ad Hoc Networks I<|sep|>Communication protocols for mobile ad hoc networks (MANETs) follow either an Optimum Routing Approach (ORA) or the Least Overhead Routing Approach (LORA): With ORA, protocols tend to determine and use the optimal communication structure at every time instant; whereas with LORA, a protocol tends to use a chosen communication structure as long as it exists. In this paper, we study the impact of the ORA and LORA strategies on minimum hop routes and minimum connected dominating sets (MCDS) in MANETs. Our primary hypothesis is that the LORA strategy could yield routes with a larger time-averaged hop count and MCDS node size when compared to the minimum hop count of routes and the node size of the MCDS determined using the ORA strategy. Our secondary hypothesis is that the impact of ORA vs. LORA also depends on how long the communication structure is being used. Our hypotheses are evaluated using extensive simulations under diverse conditions of network density, node mobility and mobility models such as the Random Waypoint model, City Section model and the Manhattan model. In the case of minimum hop routes, which exist for relatively a much longer time compared to the MCDS, the hop count of routes maintained according to LORA, even though not dramatically high, is appreciably larger (6-12%) than those maintained according to ORA; on the other hand, the number of nodes constituting a MCDS maintained according to LORA is only at most 6% larger than the node size of a MCDS maintained under the ORA strategy.
Multi-Agent Reinforcement Learning with Graph Convolutional Neural Networks for optimal Bidding Strategies of Generation Units in Electricity Markets<|sep|>Finding optimal bidding strategies for generation units in electricity markets would result in higher profit. However, it is a challenging problem due to the system uncertainty which is due to the unknown other generation units' strategies. Distributed optimization, where each entity or agent decides on its bid individually, has become state of the art. However, it cannot overcome the challenges of system uncertainties. Deep reinforcement learning is a promising approach to learn the optimal strategy in uncertain environments. Nevertheless, it is not able to integrate the information on the spatial system topology in the learning process. This paper proposes a distributed learning algorithm based on deep reinforcement learning (DRL) combined with a graph convolutional neural network (GCN). In fact, the proposed framework helps the agents to update their decisions by getting feedback from the environment so that it can overcome the challenges of the uncertainties. In this proposed algorithm, the state and connection between nodes are the inputs of the GCN, which can make agents aware of the structure of the system. This information on the system topology helps the agents to improve their bidding strategies and increase the profit. We evaluate the proposed algorithm on the IEEE 30-bus system under different scenarios. Also, to investigate the generalization ability of the proposed approach, we test the trained model on IEEE 39-bus system. The results show that the proposed algorithm has more generalization abilities compare to the DRL and can result in higher profit when changing the topology of the system.
Spectral Clustering with Unbalanced Data<|sep|>Spectral clustering (SC) and graph-based semi-supervised learning (SSL) algorithms are sensitive to how graphs are constructed from data. In particular if the data has proximal and unbalanced clusters these algorithms can lead to poor performance on well-known graphs such as $k$-NN, full-RBF, $\epsilon$-graphs. This is because the objectives such as Ratio-Cut (RCut) or normalized cut (NCut) attempt to tradeoff cut values with cluster sizes, which are not tailored to unbalanced data. We propose a novel graph partitioning framework, which parameterizes a family of graphs by adaptively modulating node degrees in a $k$-NN graph. We then propose a model selection scheme to choose sizable clusters which are separated by smallest cut values. Our framework is able to adapt to varying levels of unbalancedness of data and can be naturally used for small cluster detection. We theoretically justify our ideas through limit cut analysis. Unsupervised and semi-supervised experiments on synthetic and real data sets demonstrate the superiority of our method.
Disks and Outflows in CO Rovibrational Emission from Embedded, Low-Mass Young Stellar Objects<|sep|>Young circumstellar disks that are still embedded in dense molecular envelopes may differ from their older counterparts, but are historically difficult to study because emission from a disk can be confused with envelope or outflow emission. CO fundamental emission is a potentially powerful probe of the disk/wind structure within a few AU of young protostars. In this paper, we present high spectral (R=90,000) and spatial (0.3") resolution VLT/CRIRES M-band spectra of 18 low-mass young stellar objects (YSOs) with dense envelopes in nearby star-froming regions to explore the utility of CO fundamental 4.6 micron emission as a probe of very young disks. CO fundamental emission is detected from 14 of the YSOs in our sample. The emission line profiles show a range of strengths and shapes, but can generally be classified into a broad, warm component and a narrow, cool component. The broad CO emission is detected more frequently from YSOs with bolometric luminosities of <15 Lsun than those with >15 Lsun, and as with CO emission from CTTSs is attributed to the warm (~1000 K) inner AU of the disk. The CO emission from objects with high bolometric luminosity is produced in cooler (~320 K), narrow lines in 12CO and in rarer isotopologues. From some objects, the narrow lines are blueshifted by up to ~10 km/s, indicating a slow wind origin. For other sources the lines are located at the systemic velocity of the star and likely arise in the disk. For a few YSOs, spatially-extended CO and H2 S(9) emission is detected up to 2" from the central source and is attributed to interactions between the wind and surrounding molecular material. Warm CO absorption is detected in the wind of six objects with velocities up to 100 km/s, often in discrete velocity components. That the wind is partially molecular where it is launched favors ejection in a disk wind rather than a coronal or chromospheric wind.
On the calibration of the relation between geometric albedo and polarimetric properties for the asteroids<|sep|>We present a new extensive analysis of the old problem of finding a satisfactory calibration of the relation between the geometric albedo and some measurable polarization properties of the asteroids. To achieve our goals, we use all polarimetric data at our disposal. For the purposes of calibration, we use a limited sample of objects for which we can be confident to know the albedo with good accuracy, according to previous investigations of other authors. We find a new set of updated calibration coefficients for the classical slope - albedo relation, but we generalize our analysis and we consider also alternative possibilities, including the use of other polarimetric parameters, one being proposed here for the first time, and the possibility to exclude from best-fit analyzes the asteroids having low albedos. We also consider a possible parabolic fit of the whole set of data.
Opportunistic Self Organizing Migrating Algorithm for Real-Time Dynamic Traveling Salesman Problem<|sep|>Self Organizing Migrating Algorithm (SOMA) is a meta-heuristic algorithm based on the self-organizing behavior of individuals in a simulated social environment. SOMA performs iterative computations on a population of potential solutions in the given search space to obtain an optimal solution. In this paper, an Opportunistic Self Organizing Migrating Algorithm (OSOMA) has been proposed that introduces a novel strategy to generate perturbations effectively. This strategy allows the individual to span across more possible solutions and thus, is able to produce better solutions. A comprehensive analysis of OSOMA on multi-dimensional unconstrained benchmark test functions is performed. OSOMA is then applied to solve real-time Dynamic Traveling Salesman Problem (DTSP). The problem of real-time DTSP has been stipulated and simulated using real-time data from Google Maps with a varying cost-metric between any two cities. Although DTSP is a very common and intuitive model in the real world, its presence in literature is still very limited. OSOMA performs exceptionally well on the problems mentioned above. To substantiate this claim, the performance of OSOMA is compared with SOMA, Differential Evolution and Particle Swarm Optimization.
Leading fermionic three-loop corrections to electroweak precision observables<|sep|>In this proceeding, we highlight the computation of leading fermionic three-loop corrections to electroweak precision observables (EWPOs) accomplished recently. We summarize the numerical analysis and provide an outlook.
Quantum entanglement degrees amplifier<|sep|>The quantum entangled degrees of entangled states become smaller with the transmission distance increasing, how to keep the purity of quantum entangled states is the puzzle in quantum communication. In the paper, we have designed a new type entanglement degrees amplifier by one-dimensional photonic crystal, which is similar as the relay station of classical electromagnetic communication. We find when the entangled states of two-photon and three-photon pass through photonic crystal, their entanglement degrees can be magnified, which make the entanglement states can be long range propagation and the quantum communication can be really realized.
Rotating cylinders with anisotropic fluids in general relativity<|sep|>We consider anisotropic fluids with directional pressures $p_i = w_i \rho$ ($\rho$ is the density, $w_i = $const, $i = 1,2,3$) as sources of gravity in stationary cylindrically symmetric space-times. We describe a general way of obtaining exact solutions with such sources, where the main features are splitting of the Ricci tensor into static and rotational parts and using the harmonic radial coordinate. Depending on the values of $w_i$, it appears possible to obtain general or special solutions to the Einstein equations, thus recovering some known solutions and finding new ones. Three particular examples of exact solutions are briefly described: with a stiff isotropic perfect fluid ($p = \rho$), with a distribution of cosmic strings of azimuthal direction (i.e., forming circles around the $z$ axis), and with a stationary combination of two opposite radiation flows along the $z$ axis.
Interaction between Injection Points during Hydraulic Fracturing<|sep|>We present a model of the hydraulic fracturing of heterogeneous poroelastic media. The formalism is an effective continuum model that captures the coupled dynamics of the fluid pressure and the fractured rock matrix and models both the tensile and shear failure of the rock. As an application of the formalism, we study the geomechanical stress interaction between two injection points during hydraulic fracturing (hydrofracking) and how this interaction influences the fracturing process. For injection points that are separated by less than a critical correlation length, we find that the fracturing process around each point is strongly correlated with the position of the neighboring point. The magnitude of the correlation length depends on the degree of heterogeneity of the rock and is on the order of 30-45 m for rocks with low permeabilities. In the strongly correlated regime, we predict a novel effective fracture-force that attracts the fractures toward the neighboring injection point.
Towards adversarial robustness with 01 loss neural networks<|sep|>Motivated by the general robustness properties of the 01 loss we propose a single hidden layer 01 loss neural network trained with stochastic coordinate descent as a defense against adversarial attacks in machine learning. One measure of a model's robustness is the minimum distortion required to make the input adversarial. This can be approximated with the Boundary Attack (Brendel et. al. 2018) and HopSkipJump (Chen et. al. 2019) methods. We compare the minimum distortion of the 01 loss network to the binarized neural network and the standard sigmoid activation network with cross-entropy loss all trained with and without Gaussian noise on the CIFAR10 benchmark binary classification between classes 0 and 1. Both with and without noise training we find our 01 loss network to have the largest adversarial distortion of the three models by non-trivial margins. To further validate these results we subject all models to substitute model black box attacks under different distortion thresholds and find that the 01 loss network is the hardest to attack across all distortions. At a distortion of 0.125 both sigmoid activated cross-entropy loss and binarized networks have almost 0% accuracy on adversarial examples whereas the 01 loss network is at 40%. Even though both 01 loss and the binarized network use sign activations their training algorithms are different which in turn give different solutions for robustness. Finally we compare our network to simple convolutional models under substitute model black box attacks and find their accuracies to be comparable. Our work shows that the 01 loss network has the potential to defend against black box adversarial attacks better than convex loss and binarized networks.
The Packing While Traveling Problem<|sep|>This paper introduces the Packing While Traveling problem as a new non-linear knapsack problem. Given are a set of cities that have a set of items of distinct profits and weights and a vehicle that may collect the items when visiting all the cities in a fixed order. Each selected item contributes its profit, but produces a transportation cost relative to its weight. The problem asks to find a subset of the items such that the total gain is maximized. We investigate constrained and unconstrained versions of the problem and show that both are NP-hard. We propose a pre-processing scheme that decreases the size of instances making them easier for computation. We provide lower and upper bounds based on mixed-integer programming (MIP) adopting the ideas of piecewise linear approximation. Furthermore, we introduce two exact approaches: one is based on MIP employing linearization technique, and another is a branch-infer-and-bound (BIB) hybrid approach that compounds the upper bound procedure with a constraint programming model strengthened with customized constraints. Our experimental results show the effectiveness of our exact and approximate solutions in terms of solution quality and computational time.
Pulsating stars in NGC 6231 Frequency analysis and photometric mode identification near the main sequence<|sep|>We used Johnson UBV photometric CCD observations to identify pulsating and other variable stars in the young open cluster NGC 6231. The multi-color information was used to classify pulsating variables, perform frequency analysis, and - where possible - to compare observed to theoretical amplitude ratios for mode identification. The data reduction was performed with standard IRAF tools. Differential light curves have been obtained by identifying a set of suitable comparison stars and the frequency analysis was then conducted on the basis of Fourier methods. Our classification of pulsating stars was based on the time scales and amplitudes of the variability with respect to the different filters and stellar parameters as calculated from published Str\"omgren and Geneva photometry. We identified 32 variable stars in the field of the cluster out of which 21 are confirmed members and twelve are newly detected variable stars. Ten stars were classified as Slowly Pulsating B (SPB) stars in NGC 6231 out of which seven are new discoveries. We also analyzed six previously reported {\beta} Cephei variables in more detail. One of them may be a hybrid {\beta} Cephei/SPB pulsator. In addition, we investigated five more previously suspected pulsators of this group which we cannot convincingly confirm. The remaining eleven variable stars are either not members of NGC 6231 or the membership status is questionable. Among them are three previously known {\delta} Scuti stars, two newly detected pulsators of this class, one new and two already known eclipsing binaries, one new SPB variable, one possible Pre-Main-Sequence (PMS) pulsator and another new variable star for which we cannot present a classification. With more than 20 main sequence pulsators of spectral type B, NGC 6231 becomes the open cluster with the largest population of such pulsating stars known.
Gaia Data Release 1. Open cluster astrometry: performance, limitations, and future prospects<|sep|>Context. The first Gaia Data Release contains the Tycho-Gaia Astrometric Solution (TGAS). This is a subset of about 2 million stars for which, besides the position and photometry, the proper motion and parallax are calculated using Hipparcos and Tycho-2 positions in 1991.25 as prior information. Aims. We investigate the scientific potential and limitations of the TGAS component by means of the astrometric data for open clusters. Methods. Mean cluster parallax and proper motion values are derived taking into account the error correlations within the astrometric solutions for individual stars, an estimate of the internal velocity dispersion in the cluster, and, where relevant, the effects of the depth of the cluster along the line of sight. Internal consistency of the TGAS data is assessed. Results. Values given for standard uncertainties are still inaccurate and may lead to unrealistic unit-weight standard deviations of least squares solutions for cluster parameters. Reconstructed mean cluster parallax and proper motion values are generally in very good agreement with earlier Hipparcos-based determination, although the Gaia mean parallax for the Pleiades is a significant exception. We have no current explanation for that discrepancy. Most clusters are observed to extend to nearly 15 pc from the cluster centre, and it will be up to future Gaia releases to establish whether those potential cluster-member stars are still dynamically bound to the clusters. Conclusions. The Gaia DR1 provides the means to examine open clusters far beyond their more easily visible cores, and can provide membership assessments based on proper motions and parallaxes. A combined HR diagram shows the same features as observed before using the Hipparcos data, with clearly increased luminosities for older A and F dwarfs.
A dual-Lagrangian description adapted to quantum optics in dispersive and dissipative dielectric media<|sep|>We develop a dual description of quantum optics adapted to dielectric systems without magnetic property. Our formalism, which is shown to be equivalent to the standard one within some dipolar approximations discussed in the article, is applied to the description of polaritons in dielectric media. We show that the dual formalism leads to the Huttner-Barnett equations [B. Huttner, S. M. Barnett, Phys. Rev. A \textbf{46}, 4306 (1992)] for QED in dielectric systems. More generally, we discuss the role of electromagnetic duality in the quantization procedure for optical systems and derive the structure of the dynamical laws in the various representations.
Strategies for Efficient Executions of Irregular Message-Driven Parallel Applications on GPU Systems<|sep|>Message-driven executions with over-decomposition of tasks constitute an important model for parallel programming and have been demonstrated for irregular applications. Supporting efficient execution of such message-driven irregular applications on GPU systems presents a number of challenges related to irregular data accesses and computations. In this work, we have developed strategies including coalescing irregular data accesses and combining with data reuse, and adaptive methods for hybrid executions to minimize idling. We have integrated these runtime strategies into our {\em G-Charm} framework for efficient execution of message-driven parallel applications on hybrid GPU systems. We demonstrate our strategies for irregular applications with an N-Body simulations and a molecular dynamics application and show that our dynamic strategies result in 8-38\% reduction in execution times for these irregular applications over the corresponding static strategies that are amenable for regular applications.
Interoperability, Trust Based Information Sharing Protocol and Security: Digital Government Key Issues<|sep|>Improved interoperability between public and private organizations is of key significance to make digital government newest triumphant. Digital Government interoperability, information sharing protocol and security are measured the key issue for achieving a refined stage of digital government. Flawless interoperability is essential to share the information between diverse and merely dispersed organisations in several network environments by using computer based tools. Digital government must ensure security for its information systems, including computers and networks for providing better service to the citizens. Governments around the world are increasingly revolving to information sharing and integration for solving problems in programs and policy areas. Evils of global worry such as syndrome discovery and manage, terror campaign, immigration and border control, prohibited drug trafficking, and more demand information sharing, harmonization and cooperation amid government agencies within a country and across national borders. A number of daunting challenges survive to the progress of an efficient information sharing protocol. A secure and trusted information-sharing protocol is required to enable users to interact and share information easily and perfectly across many diverse networks and databases globally.
Toward Learning a Unified Many-to-Many Mapping for Diverse Image Translation<|sep|>Image-to-image translation, which translates input images to a different domain with a learned one-to-one mapping, has achieved impressive success in recent years. The success of translation mainly relies on the network architecture to reserve the structural information while modify the appearance slightly at the pixel level through adversarial training. Although these networks are able to learn the mapping, the translated images are predictable without exclusion. It is more desirable to diversify them using image-to-image translation by introducing uncertainties, i.e., the generated images hold potential for variations in colors and textures in addition to the general similarity to the input images, and this happens in both the target and source domains. To this end, we propose a novel generative adversarial network (GAN) based model, InjectionGAN, to learn a many-to-many mapping. In this model, the input image is combined with latent variables, which comprise of domain-specific attribute and unspecific random variations. The domain-specific attribute indicates the target domain of the translation, while the unspecific random variations introduce uncertainty into the model. A unified framework is proposed to regroup these two parts and obtain diverse generations in each domain. Extensive experiments demonstrate that the diverse generations have high quality for the challenging image-to-image translation tasks where no pairing information of the training dataset exits. Both quantitative and qualitative results prove the superior performance of InjectionGAN over the state-of-the-art approaches.
Deep Temporal Linear Encoding Networks<|sep|>The CNN-encoding of features from entire videos for the representation of human actions has rarely been addressed. Instead, CNN work has focused on approaches to fuse spatial and temporal networks, but these were typically limited to processing shorter sequences. We present a new video representation, called temporal linear encoding (TLE) and embedded inside of CNNs as a new layer, which captures the appearance and motion throughout entire videos. It encodes this aggregated information into a robust video feature representation, via end-to-end learning. Advantages of TLEs are: (a) they encode the entire video into a compact feature representation, learning the semantics and a discriminative feature space; (b) they are applicable to all kinds of networks like 2D and 3D CNNs for video classification; and (c) they model feature interactions in a more expressive way and without loss of information. We conduct experiments on two challenging human action datasets: HMDB51 and UCF101. The experiments show that TLE outperforms current state-of-the-art methods on both datasets.
Charged Particle and Photon Multiplicity, and Transverse Energy Production in High-Energy Heavy-Ion Collisions<|sep|>We review the charged particle and photon multiplicity, and transverse energy production in heavy-ion collisions starting from few GeV to TeV energies. The experimental results of pseudorapidity distribution of charged particles and photons at different collision energies and centralities are discussed. We also discuss the hypothesis of limiting fragmentation and expansion dynamics using the Landau hydrodynamics and the underlying physics. Meanwhile, we present the estimation of initial energy density multiplied with formation time as a function of different collision energies and centralities. In the end, the transverse energy per charged particle in connection with the chemical freeze-out criteria is discussed. We invoke various models and phenomenological arguments to interpret and characterize the fireball created in heavy-ion collisions. This review overall provides a scope to understand the heavy-ion collision data and a possible formation of a deconfined phase of partons via the global observables like charged particles, photons and the transverse energy measurement.
Stellar, Brown Dwarf, and Multiple Star Properties from Hydrodynamical Simulations of Star Cluster Formation<|sep|>We report the statistical properties of stars, brown dwarfs and multiple systems obtained from the largest hydrodynamical simulation of star cluster formation to date that resolves masses down to the opacity limit for fragmentation (a few Jupiter masses). The simulation is essentially identical to that of Bate, Bonnell & Bromm except that the initial molecular cloud is larger and more massive. It produces more than 1250 stars and brown dwarfs, providing unprecedented statistical information that can be compared with observational surveys. We find that hydrodynamical/sink particle simulations can reproduce many of the observed stellar properties very well. Binarity as a function of primary mass, the frequency of very-low-mass (VLM) binaries, general trends for the separation and mass ratio distributions of binaries, and the relative orbital orientations of triples systems are all in reasonable agreement with observations. We also examine the radial variations of binarity, velocity dispersion, and mass function in the resulting stellar cluster and the distributions of disc truncation radii due to dynamical interactions. For VLM binaries, we find that their frequency when using small accretion radii and gravitational softening is similar to that expected from observational surveys (approximately 20 percent). We also find that VLM binaries evolve from wide, unequal-mass systems towards close equal-mass systems as they form. The two main deficiencies of the calculations are that they over produce brown dwarfs relative to stars and that there are too few unequal mass binaries with K and G-dwarf primaries. [Abridged]
Conformality loss and quantum criticality in topological Higgs electrodynamics in 2+1 dimensions<|sep|>The electromagnetic response of topological insulators and superconductors is governed by a modified set of Maxwell equations that derive from a topological Chern-Simons (CS) term in the effective Lagrangian with coupling constant $\kappa$. Here we consider a topological superconductor or, equivalently, an Abelian Higgs model in $2+1$ dimensions with a global $O(2N)$ symmetry in the presence of a CS term, but without a Maxwell term. At large $\kappa$, the gauge field decouples from the complex scalar field, leading to a quantum critical behavior in the $O(2N)$ universality class. When the Higgs field is massive, the universality class is still governed by the $O(2N)$ fixed point. However, we show that the massless theory belongs to a completely different universality class, exhibiting an exotic critical behavior beyond the Landau-Ginzburg-Wilson paradigm. For finite $\kappa$ above a certain critical value $\kappa_c$, a quantum critical behavior with continuously varying critical exponents arises. However, as a function $\kappa$ a transition takes place for $|\kappa| < \kappa_c$ where conformality is lost. Strongly modified scaling relations ensue. For instance, in the case where $\kappa^2>\kappa_c^2$, leading to the existence of a conformal fixed point, critical exponents are a function of $\kappa$.
Sparse Generalized Yule-Walker Estimation for Large Spatio-temporal Autoregressions with an Application to NO2 Satellite Data<|sep|>We consider a high-dimensional model in which variables are observed over time and space. The model consists of a spatio-temporal regression containing a time lag and a spatial lag of the dependent variable. Unlike classical spatial autoregressive models, we do not rely on a predetermined spatial interaction matrix, but infer all spatial interactions from the data. Assuming sparsity, we estimate the spatial and temporal dependence fully data-driven by penalizing a set of Yule-Walker equations. This regularization can be left unstructured, but we also propose customized shrinkage procedures when observations originate from spatial grids (e.g. satellite images). Finite sample error bounds are derived and estimation consistency is established in an asymptotic framework wherein the sample size and the number of spatial units diverge jointly. Exogenous variables can be included as well. A simulation exercise shows strong finite sample performance compared to competing procedures. As an empirical application, we model satellite measured NO2 concentrations in London. Our approach delivers forecast improvements over a competitive benchmark and we discover evidence for strong spatial interactions.
Tidal star-planet interaction and its observed impact on stellar activity in planet-hosting wide binary systems<|sep|>Tidal interaction between an exoplanet and its host star is a possible pathway to transfer angular momentum between the planetary orbit and the stellar spin. In cases where the planetary orbital period is shorter than the stellar rotation period, this may lead to angular momentum being transferred into the star's rotation, possibly counteracting the intrinsic stellar spin-down induced by magnetic braking. Observationally, detecting altered rotational states of single, cool field stars is challenging, as precise ages for such stars are rarely available. Here we present an empirical investigation of the rotation and magnetic activity of a sample of planet-hosting stars that are accompanied by wide stellar companions. Without needing knowledge about the absolute ages of the stars, we test for relative differences in activity and rotation of the planet hosts and their co-eval companions, using X-ray observations to measure the stellar activity levels. Employing three different tidal interaction models, we find that host stars with planets that are expected to tidally interact display elevated activity levels compared to their companion stars. We also find that those activity levels agree with the observed rotational periods for the host stars along the usual rotation-activity relationships, implying that the effect is indeed caused by a tidal interaction and not a purely magnetic interaction which would be expected to affect the stellar activity, but not necessarily the rotation. We conclude that massive, close-in planets have an impact on the stellar rotational evolution, while the smaller, more distant planets do not have a significant influence.
Recursive calculation of matrix elements for the generalized seniority shell model<|sep|>A recursive calculational scheme is developed for matrix elements in the generalized seniority scheme for the nuclear shell model. Recurrence relations are derived which permit straightforward and efficient computation of matrix elements of one-body and two-body operators and basis state overlaps.
Probabilistic Models for Query Approximation with Large Sparse Binary Datasets<|sep|>Large sparse sets of binary transaction data with millions of records and thousands of attributes occur in various domains: customers purchasing products, users visiting web pages, and documents containing words are just three typical examples. Real-time query selectivity estimation (the problem of estimating the number of rows in the data satisfying a given predicate) is an important practical problem for such databases. We investigate the application of probabilistic models to this problem. In particular, we study a Markov random field (MRF) approach based on frequent sets and maximum entropy, and compare it to the independence model and the Chow-Liu tree model. We find that the MRF model provides substantially more accurate probability estimates than the other methods but is more expensive from a computational and memory viewpoint. To alleviate the computational requirements we show how one can apply bucket elimination and clique tree approaches to take advantage of structure in the models and in the queries. We provide experimental results on two large real-world transaction datasets.
Age and helium content of the open cluster NGC 6791 from multiple eclipsing binary members. I. Measurements, methods, and first results<|sep|>Earlier measurements of the masses and radii of the detached eclipsing binary V20 in the open cluster NGC 6791 were accurate enough to demonstrate that there are significant differences between current stellar models. Here we improve on those results and add measurements of two additional detached eclipsing binaries, the cluster members V18 and V80. The enlarged sample sets much tighter constraints on the properties of stellar models than has hitherto been possible, thereby improving both the accuracy and precision of the cluster age. We employed (i) high-resolution UVES spectroscopy of V18, V20 and V80 to determine their spectroscopic effective temperatures, [Fe/H] values, and spectroscopic orbital elements, and (ii) time-series photometry from the Nordic Optical Telescope to obtain the photometric elements. The masses and radii of the V18 and V20 components are found to high accuracy, with errors on the masses in the range 0.27-0.36% and errors on the radii in the range 0.61-0.92%. V80 is found to be magnetically active, and more observations are needed to determine its parameters accurately. The metallicity of NGC 6791 is measured from disentangled spectra of the binaries and a few single stars to be [Fe/H]= +0.29 \pm 0.03 (random) \pm 0.07 (systematic). The cluster reddening and apparent distance modulus are found to be E(B - V) = 0.160 \pm 0.025 and (m - M)V = 13.51 \pm 0.06 . A first model comparison shows that we can constrain the helium content of the NGC 6791 stars, and thus reach a more accurate age than previously possible. It may be possible to constrain additional parameters, in particular the C, N, and O abundances. This will be investigated in paper II.
Electronic and transport properties of kinked graphene<|sep|>Local curvature, or bending, of a graphene sheet is known to increase the chemical reactivity presenting an opportunity for templated chemical functionalization. Using first principles calculations based on density functional theory (DFT) we investigate the reduction of the reaction barrier for adsorption of atomic hydrogen at linear bends in graphene. We find a significant lowering (15%) for realistic radii of curvature (20 {\AA}), and that adsorption along the linear bend leads to a stable linear kink. We compute the electronic transport properties of individual and multiple kink-lines, and demonstrate how these act as efficient barriers for electron transport. In particular, two parallel kink-lines form a graphene pseudo-nanoribbon structure with a semi-metallic/semi-conducting electronic structure closely related to the corresponding isolated ribbons; the ribbon band gap translates into a transport gap for transport across the kink lines. We finally consider pseudo-ribbon based heterostructures, and propose that such structures present a novel approach for band gap engineering in nanostructured graphene.
A memory-based method to select the number of relevant components in Principal Component Analysis<|sep|>We propose a new data-driven method to select the optimal number of relevant components in Principal Component Analysis (PCA). This new method applies to correlation matrices whose time autocorrelation function decays more slowly than an exponential, giving rise to long memory effects. In comparison with other available methods present in the literature, our procedure does not rely on subjective evaluations and is computationally inexpensive. The underlying basic idea is to use a suitable factor model to analyse the residual memory after sequentially removing more and more components, and stopping the process when the maximum amount of memory has been accounted for by the retained components. We validate our methodology on both synthetic and real financial data, and find in all cases a clear and computationally superior answer entirely compatible with available heuristic criteria, such as cumulative variance and cross-validation.
Star formation efficiency along the radio jet in Centaurus A<|sep|>Centaurus A is the most nearby powerful AGN, widely studied at all wavelengths. Molecular gas has been found in the halo at a distance of ~20 kpc from the galaxy centre, associated with HI shells. The molecular gas lies inside some IR and UV bright star-forming filaments that have recently been observed in the direction of the radio jets. These archival data show that there is dust and very weak star formation on scales of hundreds of parsecs. On top of analysing combined archival data, we have performed searches of HCN(1-0) and HCO+(1-0) emission with ATCA at the interaction of the northern filaments and the HI shell of Cen A. Measuring the dense gas is another indicator of star formation efficiency inside the filaments. However, we only derived upper limits of 1.6x10^3 K.km/s.pc^2 at 3 sigma in the synthesised beam of 3.1". We also compared the CO masses with the SFR estimates in order to measure a star formation efficiency. Using a standard conversion factor leads to long depletion times (7 Gyr). We then corrected the mass estimates from metallicity effect by using gas-to-dust mass ratio as a proxy. From MUSE data, we estimated the metallicity spread (0.4-0.8 Zsun) in the filament, corresponding to gas-to-dust ratios of ~200-400. The CO/H2 conversion ratio is corrected for low metallicity by a factor between 1.4 and 3.2. Such a low-metallicity correction leads to even more massive clouds with higher depletion times (16 Gyr). We finally present ALMA observations that detect 3 unresolved CO(2-1) clumps of size <37x21 pc and masses around 10^4 Msun. The velocity width of the CO emission line is ~10 km/s, leading to a rather high virial parameter. This is a hint of a turbulent gas probably powered by kinetic energy injection from the AGN jet/wind and leading to molecular gas reservoir not forming star efficiently.
Comparison between entangled and nonentangled two-reservoir Kondo systems<|sep|>We clarify the difference between entangled and nonentangled two-reservoir mesoscopic Kondo systems and reveal the reason why theories using the Keldysh formalism, quantum Monte Carlo calculations, and the renormalization group approaches cannot explain the line shapes of tunneling conductance of mesoscopic Kondo systems measured by using a two-terminal setup but explain those of a three-terminal setup. We emphasize that the previous theories study a nonentangled system, while real two-reservoir mesoscopic Kondo systems are entangled systems in which two reservoirs are within the coherent region. We show that two coherent side peaks appearing in tunneling conductance signify the entanglement between two reservoirs. These side peaks are essential for explaining the experimental observations for tunneling conductance.
Slowly synchronizing automata with fixed alphabet size<|sep|>It was conjectured by \v{C}ern\'y in 1964 that a synchronizing DFA on $n$ states always has a shortest synchronizing word of length at most $(n-1)^2$, and he gave a sequence of DFAs for which this bound is reached. In this paper, we investigate the role of the alphabet size. For each possible alphabet size, we count DFAs on $n \le 6$ states which synchronize in $(n-1)^2 - e$ steps, for all $e < 2\lceil n/2 \rceil$. Furthermore, we give constructions of automata with any number of states, and $3$, $4$, or $5$ symbols, which synchronize slowly, namely in $n^2 - 3n + O(1)$ steps. In addition, our results prove \v{C}ern\'y's conjecture for $n \le 6$. Our computation has led to $27$ DFAs on $3$, $4$, $5$ or $6$ states, which synchronize in $(n-1)^2$ steps, but do not belong to \v{C}ern\'y's sequence. Of these $27$ DFA's, $19$ are new, and the remaining $8$ which were already known are exactly the \emph{minimal} ones: they will not synchronize any more after removing a symbol. So the $19$ new DFAs are extensions of automata which were already known, including the \v{C}ern\'y automaton on $3$ states. But for $n > 3$, we prove that the \v{C}ern\'y automaton on $n$ states does not admit non-trivial extensions with the same smallest synchronizing word length $(n-1)^2$.
MeV scale model of SIMP dark matter, neutrino mass and leptogenesis<|sep|>We consider a simple extension of the Standard Model with two singlet scalar fields and three heavy right-handed neutrinos. One of the scalar fields serves as an MeV scale dark matter and its stability is ensured by the introduction of an extra $Z_2$ symmetry. The second scalar (which is even under the $Z_2$ symmetry) generates the mass term of the scalar, contributes to the $3 \rightarrow 2$ annihilation process required for the correct relic density of the dark matter and it also contributes to the leptogenesis. The right-handed neutrinos are responsible for the generation of light neutrino masses through Type-I seesaw mechanism. The decay of the heavy right-handed neutrino can generate the lepton asymmetry which can then be converted to baryon asymmetry through sphaleron transitions.
LMN at SemEval-2022 Task 11: A Transformer-based System for English Named Entity Recognition<|sep|>Processing complex and ambiguous named entities is a challenging research problem, but it has not received sufficient attention from the natural language processing community. In this short paper, we present our participation in the English track of SemEval-2022 Task 11: Multilingual Complex Named Entity Recognition. Inspired by the recent advances in pretrained Transformer language models, we propose a simple yet effective Transformer-based baseline for the task. Despite its simplicity, our proposed approach shows competitive results in the leaderboard as we ranked 12 over 30 teams. Our system achieved a macro F1 score of 72.50% on the held-out test set. We have also explored a data augmentation approach using entity linking. While the approach does not improve the final performance, we also discuss it in this paper.
Update-tolerant and Revocable Password Backup (Extended Version)<|sep|>It is practically impossible for users to memorize a large portfolio of strong and individual passwords for their online accounts. A solution is to generate passwords randomly and store them. Yet, storing passwords instead of memorizing them bears the risk of loss, e.g., in situations where the device on which the passwords are stored is damaged, lost, or stolen. This makes the creation of backups of the passwords indispensable. However, placing such backups at secure locations to protect them as well from loss and unauthorized access and keeping them up-to-date at the same time is an unsolved problem in practice. We present PASCO, a backup solution for passwords that solves this challenge. PASCO backups need not to be updated, even when the user's password portfolio is changed. PASCO backups can be revoked without having physical access to them. This prevents password leakage, even when a user loses control over a backup. Additionally, we show how to extend PASCO to enable a fully controllable emergency access. It allows a user to give someone else access to his passwords in urgent situations. We also present a security evaluation and an implementation of PASCO.
Molecular Simulations of the Fluctuating Conformational Dynamics of Intrinsically Disordered Proteins<|sep|>Intrinsically disordered proteins (IDPs) do not possess well-defined three-dimensional structures in solution under physiological conditions. We develop all-atom, united-atom, and coarse-grained Langevin dynamics simulations for the IDP alpha-synuclein that include geometric, attractive hydrophobic, and screened electrostatic interactions and are calibrated to the inter-residue separations measured in recent smFRET experiments. We find that alpha-synuclein is disordered with conformational statistics that are intermediate between random walk and collapsed globule behavior. An advantage of calibrated molecular simulations over constraint methods is that physical forces act on all residues, not only on residue pairs that are monitored experimentally, and these simulations can be used to study oligomerization and aggregation of multiple alpha-synuclein proteins that may precede amyloid formation.
Quantum Linear Systems Theory<|sep|>This paper surveys some recent results on the theory of quantum linear systems and presents them within a unified framework. Quantum linear systems are a class of systems whose dynamics, which are described by the laws of quantum mechanics, take the specific form of a set of linear quantum stochastic differential equations (QSDEs). Such systems commonly arise in the area of quantum optics and related disciplines. Systems whose dynamics can be described or approximated by linear QSDEs include interconnections of optical cavities, beam-spitters, phase-shifters, optical parametric amplifiers, optical squeezers, and cavity quantum electrodynamic systems. With advances in quantum technology, the feedback control of such quantum systems is generating new challenges in the field of control theory. Potential applications of such quantum feedback control systems include quantum computing, quantum error correction, quantum communications, gravity wave detection, metrology, atom lasers, and superconducting quantum circuits. A recently emerging approach to the feedback control of quantum linear systems involves the use of a controller which itself is a quantum linear system. This approach to quantum feedback control, referred to as coherent quantum feedback control, has the advantage that it does not destroy quantum information, is fast, and has the potential for efficient implementation. This paper discusses recent results concerning the synthesis of H-infinity optimal controllers for linear quantum systems in the coherent control case. An important issue which arises both in the modelling of linear quantum systems and in the synthesis of linear coherent quantum controllers is the issue of physical realizability. This issue relates to the property of whether a given set of QSDEs corresponds to a physical quantum system satisfying the laws of quantum mechanics.
Log-normal distribution based EMOS models for probabilistic wind speed forecasting<|sep|>Ensembles of forecasts are obtained from multiple runs of numerical weather forecasting models with different initial conditions and typically employed to account for forecast uncertainties. However, biases and dispersion errors often occur in forecast ensembles, they are usually under-dispersive and uncalibrated and require statistical post-processing. We present an Ensemble Model Output Statistics (EMOS) method for calibration of wind speed forecasts based on the log-normal (LN) distribution, and we also show a regime-switching extension of the model which combines the previously studied truncated normal (TN) distribution with the LN. Both presented models are applied to wind speed forecasts of the eight-member University of Washington mesoscale ensemble, of the fifty-member ECMWF ensemble and of the eleven-member ALADIN-HUNEPS ensemble of the Hungarian Meteorological Service, and their predictive performances are compared to those of the TN and general extreme value (GEV) distribution based EMOS methods and to the TN-GEV mixture model. The results indicate improved calibration of probabilistic and accuracy of point forecasts in comparison to the raw ensemble and to climatological forecasts. Further, the TN-LN mixture model outperforms the traditional TN method and its predictive performance is able to keep up with the models utilizing the GEV distribution without assigning mass to negative values.
A Temporal Logic for Strategic Hyperproperties<|sep|>Hyperproperties are commonly used in computer security to define information-flow policies and other requirements that reason about the relationship between multiple computations. In this paper, we study a novel class of hyperproperties where the individual computation paths are chosen by the strategic choices of a coalition of agents in a multi-agent system. We introduce HyperATL*, an extension of computation tree logic with path variables and strategy quantifiers. HyperATL* can express strategic hyperproperties, such as that the scheduler in a concurrent system has a strategy to avoid information leakage. HyperATL* is particularly useful to specify asynchronous hyperproperties, i.e., hyperproperties where the speed of the execution on the different computation paths depends on the choices of the scheduler. Unlike other recent logics for the specification of asynchronous hyperproperties, our logic is the first to admit decidable model checking for the full logic. We present a model checking algorithm for HyperATL* based on alternating word automata and show that our algorithm is asymptotically optimal by providing a matching lower bound. We have implemented a prototype model checker for a fragment of HyperATL*, able to check various security properties on small programs.
The kinematics of local thick discs do not support an accretion origin<|sep|>Thick discs are nearly ubiquitous components of the discs of present-day galaxies. It has been proposed that a fraction of their stars has been accreted. Here, we aim to find whether accretion of satellites is the main thick disc formation mechanism. To do so, we observed a sample of eight nearby edge-on galaxies with the MUSE integral field unit at the VLT. Six of the galaxies have a distinct thick disc. We derived thick disc velocities and velocity dispersions for the galaxies in our sample. We devise a formalism to estimate the fractions of retrograde material in the thick discs by using kinematical maps and thin/thick dis decompositions. None of the galaxies in our sample shows strong evidence for retrograde material at large distances from the centre. Including those found in the literature, there are seventeen thick discs with studied kinematics, with only one showing unambiguous signatures of retrograde material. Literature numerical studies of dynamical friction allow us to estimate that at the current cosmic time about one in six mergers for which the stars of the accreted galaxy ended in a thick disc were retrograde. This is in tension with the observed fraction of 1/17 of galaxies with a partly retrograde thick disc. We conclude that satellite accretion is not favoured by observations to be the main thick disk formation mechanism.
Statistical analysis of sampling methods in quantum tomography<|sep|>In quantum physics, all measured observables are subject to statistical uncertainties, which arise from the quantum nature as well as the experimental technique. We consider the statistical uncertainty of the so-called sampling method, in which one estimates the expectation value of a given observable by empirical means of suitable pattern functions. We show that if the observable can be written as a function of a single directly measurable operator, the variance of the estimate from the sampling method equals to the quantum mechanical one. In this sense, we say that the estimate is on the quantum mechanical level of uncertainty. In contrast, if the observable depends on non-commuting operators, e.g. different quadratures, the quantum mechanical level of uncertainty is not achieved. The impact of the results on quantum tomography is discussed, and different approaches to quantum tomographic measurements are compared. It is shown explicitly for the estimation of quasiprobabilities of a quantum state, that balanced homodyne tomography does not operate on the quantum mechanical level of uncertainty, while the unbalanced homodyne detection does.
Fault location in High Voltage Multi-terminal dc Networks Using Ensemble Learning<|sep|>Precise location of faults for large distance power transmission networks is essential for faster repair and restoration process. High Voltage direct current (HVdc) networks using modular multi-level converter (MMC) technology has found its prominence for interconnected multi-terminal networks. This allows for large distance bulk power transmission at lower costs. However, they cope with the challenge of dc faults. Fast and efficient methods to isolate the network under dc faults have been widely studied and investigated. After successful isolation, it is essential to precisely locate the fault. The post-fault voltage and current signatures are a function of multiple factors and thus accurately locating faults on a multi-terminal network is challenging. In this paper, we discuss a novel data-driven ensemble learning based approach for accurate fault location. Here we utilize the eXtreme Gradient Boosting (XGB) method for accurate fault location. The sensitivity of the proposed algorithm to measurement noise, fault location, resistance and current limiting inductance are performed on a radial three-terminal MTdc network designed in Power System Computer Aided Design (PSCAD)/Electromagnetic Transients including dc (EMTdc).
Dynamic Pooling Improves Nanopore Base Calling Accuracy<|sep|>In nanopore sequencing, electrical signal is measured as DNA molecules pass through the sequencing pores. Translating these signals into DNA bases (base calling) is a highly non-trivial task, and its quality has a large impact on the sequencing accuracy. The most successful nanopore base callers to date use convolutional neural networks (CNN) to accomplish the task. Convolutional layers in CNNs are typically composed of filters with constant window size, performing best in analysis of signals with uniform speed. However, the speed of nanopore sequencing varies greatly both within reads and between sequencing runs. Here, we present dynamic pooling, a novel neural network component, which addresses this problem by adaptively adjusting the pooling ratio. To demonstrate the usefulness of dynamic pooling, we developed two base callers: Heron and Osprey. Heron improves the accuracy beyond the experimental high-accuracy base caller Bonito developed by Oxford Nanopore. Osprey is a fast base caller that can compete in accuracy with Guppy high-accuracy mode, but does not require GPU acceleration and achieves a near real-time speed on common desktop CPUs. Availability: https://github.com/fmfi-compbio/osprey, https://github.com/fmfi-compbio/heron Keywords: nanopore sequencing, base calling, convolutional neural networks, pooling
Blockchain-Enabled Internet-of-Things Platform for End-to-End Industrial Hemp Supply Chain<|sep|>After being legalized as an agricultural commodity by the 2018 U.S. Farm Bill, the Industrial Hemp production is moved from limited pilot programs to a regulated agriculture production system, and the market keeps increasing since then. However, Industrial Hemp Supply Chain (IHSC) faces several critical challenges, including high complexity and variability, data tampering, and lack of immutable information tracking system. In this paper, we develop a blockchain enabled internet-of-things (IoT) platform for IHSC to support process tracking, scalability, interoperability, and risk management. Basically, we create a two-layer blockchain with proof-of-authority based smart contract, which can leverage local authorities with state/federal regulators to ensure and accelerate quality control verification and regulatory compliance. Then, we develop a user-friendly mobile app so that each participant can use smart phone to real-time collect and upload their data to the cloud, and further share the process verification and tracking information through the blockchain network. Our study indicates the proposed platform can support interoperability, improve the efficiency of quality control verification, and ensure the safety of regulated IHSC.
Two-Hop Routing with Traffic-Differentiation for QoS Guarantee in Wireless Sensor Networks<|sep|>This paper proposes a Traffic-Differentiated Two-Hop Routing protocol for Quality of Service (QoS) in Wireless Sensor Networks (WSNs). It targets WSN applications having different types of data traffic with several priorities. The protocol achieves to increase Packet Reception Ratio (PRR) and reduce end-to-end delay while considering multi-queue priority policy, two-hop neighborhood information, link reliability and power efficiency. The protocol is modular and utilizes effective methods for estimating the link metrics. Numerical results show that the proposed protocol is a feasible solution to addresses QoS service differenti- ation for traffic with different priorities.
LDPC Coded Modulation with Probabilistic Shaping for Optical Fiber Systems<|sep|>An LDPC coded modulation scheme with probabilistic shaping, optimized interleavers and noniterative demapping is proposed. Full-field simulations show an increase in transmission distance by 8% compared to uniformly distributed input.
Seidel Minor, Permutation Graphs and Combinatorial Properties<|sep|>A permutation graph is an intersection graph of segments lying between two parallel lines. A Seidel complementation of a finite graph at one of it vertex $v$ consists to complement the edges between the neighborhood and the non-neighborhood of $v$. Two graphs are Seidel complement equivalent if one can be obtained from the other by a successive application of Seidel complementation. In this paper we introduce the new concept of Seidel complementation and Seidel minor, we then show that this operation preserves cographs and the structure of modular decomposition. The main contribution of this paper is to provide a new and succinct characterization of permutation graphs i.e. A graph is a permutation graph \Iff it does not contain the following graphs: $C_5$, $C_7$, $XF_{6}^{2}$, $XF_{5}^{2n+3}$, $C_{2n}, n\geqslant6$ and their complement as Seidel minor. In addition we provide a $O(n+m)$-time algorithm to output one of the forbidden Seidel minor if the graph is not a permutation graph.
Italy goes to Stanford: a collection of CoreNLP modules for Italian<|sep|>In this we paper present Tint, an easy-to-use set of fast, accurate and extendable Natural Language Processing modules for Italian. It is based on Stanford CoreNLP and is freely available as a standalone software or a library that can be integrated in an existing project.
Is there a wave excitation in the Thalamus?<|sep|>This paper proposes that the thalamus is the site of a wave excitation, whose function is to represent the locations of things around the animal. Neurons couple to the wave as transmitters and receivers. The wave acts as an analogue representation of local space. This has benefits over a purely neural representation of space. Several lines of evidence support this hypothesis; both theoretical, concerning efficient Bayesian inference in the brain, and empirical, concerning the neuro-anatomy of the thalamus. Across all species, the most basic function of the brain is to coordinate movements in space. To represent positions in space only by neural firing rates would be complex and inefficient. It is possible that that the brain represents 3D space in a direct and natural way, by a 3D wave
Measurement of the K+ -> pi0 mu+ nu_mu gamma Branching Ratio<|sep|>A measurement of the decay K+ -> pi0 mu+ nu_mu gamma has been performed with the E787 detector at Brookhaven National Laboratory. Forty events were observed in the signal region with the background expectation of (16.5 +- 2.7) events. The branching ratio was measured to be (1.58 +- 0.46(stat.) +- 0.08(syst.)) x 10^{-5}} in the kinematic region Eg >30 MeV and theta_{mu gamma} > 20degree, where Eg is the energy of the emitted photon and theta_{mu gamma} is the angle between the muon and the photon in the K+ rest frame. The results were consistent with theoretical predictions.
Attacks on a Privacy-Preserving Publish-Subscribe System and a Ride-Hailing Service<|sep|>A privacy-preserving Context-Aware Publish-Subscribe System (CA-PSS) enables an intermediary (broker) to match the content from a publisher and the subscription by a subscriber based on the current context while preserving confidentiality of the subscriptions and notifications. While a privacy-preserving Ride-Hailing Service (RHS) enables an intermediary (service provider) to match a ride request with a taxi driver in a privacy-friendly manner. In this work, we attack a privacy-preserving CA-PSS proposed by Nabeel et al. (2013), where we show that any entity in the system including the broker can learn the confidential subscriptions of the subscribers. We also attack a privacy-preserving RHS called lpRide proposed by Yu et al. (2019), where we show that any rider/driver can efficiently recover the secret keys of all other riders and drivers. Also, we show that any rider/driver will be able to learn the location of any rider. The attacks are based on our cryptanalysis of the modified Paillier cryptosystem proposed by Nabeel et al. that forms a building block for both the above protocols.
Impact of Scaled Image on Robustness of Deep Neural Networks<|sep|>Deep neural networks (DNNs) have been widely used in computer vision tasks like image classification, object detection and segmentation. Whereas recent studies have shown their vulnerability to manual digital perturbations or distortion in the input images. The accuracy of the networks is remarkably influenced by the data distribution of their training dataset. Scaling the raw images creates out-of-distribution data, which makes it a possible adversarial attack to fool the networks. In this work, we propose a Scaling-distortion dataset ImageNet-CS by Scaling a subset of the ImageNet Challenge dataset by different multiples. The aim of our work is to study the impact of scaled images on the performance of advanced DNNs. We perform experiments on several state-of-the-art deep neural network architectures on the proposed ImageNet-CS, and the results show a significant positive correlation between scaling size and accuracy decline. Moreover, based on ResNet50 architecture, we demonstrate some tests on the performance of recent proposed robust training techniques and strategies like Augmix, Revisiting and Normalizer Free on our proposed ImageNet-CS. Experiment results have shown that these robust training techniques can improve networks' robustness to scaling transformation.
Quantifying Language Variation Acoustically with Few Resources<|sep|>Deep acoustic models represent linguistic information based on massive amounts of data. Unfortunately, for regional languages and dialects such resources are mostly not available. However, deep acoustic models might have learned linguistic information that transfers to low-resource languages. In this study, we evaluate whether this is the case through the task of distinguishing low-resource (Dutch) regional varieties. By extracting embeddings from the hidden layers of various wav2vec 2.0 models (including new models which are pre-trained and/or fine-tuned on Dutch) and using dynamic time warping, we compute pairwise pronunciation differences averaged over 10 words for over 100 individual dialects from four (regional) languages. We then cluster the resulting difference matrix in four groups and compare these to a gold standard, and a partitioning on the basis of comparing phonetic transcriptions. Our results show that acoustic models outperform the (traditional) transcription-based approach without requiring phonetic transcriptions, with the best performance achieved by the multilingual XLSR-53 model fine-tuned on Dutch. On the basis of only six seconds of speech, the resulting clustering closely matches the gold standard.
HEIDL: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop<|sep|>While the role of humans is increasingly recognized in machine learning community, representation of and interaction with models in current human-in-the-loop machine learning (HITL-ML) approaches are too low-level and far-removed from human's conceptual models. We demonstrate HEIDL, a prototype HITL-ML system that exposes the machine-learned model through high-level, explainable linguistic expressions formed of predicates representing semantic structure of text. In HEIDL, human's role is elevated from simply evaluating model predictions to interpreting and even updating the model logic directly by enabling interaction with rule predicates themselves. Raising the currency of interaction to such semantic levels calls for new interaction paradigms between humans and machines that result in improved productivity for text analytics model development process. Moreover, by involving humans in the process, the human-machine co-created models generalize better to unseen data as domain experts are able to instill their expertise by extrapolating from what has been learned by automated algorithms from few labelled data.
Electron spin rotations induced by oscillating Rashba interaction in a quantum wire<|sep|>A novel method and nanodevice are introduced that allows to rotate the single electron spin confined in a gated electrostatic InSb nanowire quantum dot. Proposed method does not require application of any (oscillating or static) external magnetic fields. Our proposal instead employs spatial and time modulation of confining potential induced by electric gates, which, in turn leads to oscillating Rashba type spin-orbit coupling. Moving electron back and forth in such a variable Rashba field allows for realization of spin rotations around two different axes separately without using an external magnetic field. The results are supported by realistic three-dimensional time dependent Poisson-Schr\"{o}dinger calculations for systems and material parameters corresponding to experimentally accessible structures.
Spherical model with Dzyaloshinskii-Moriya interactions<|sep|>We analyze the thermodynamic behavior of a ferromagnetic mean-spherical model with three distinct spin components and the addition of Dzyaloshinkii-Moriya interactions. Exact calculations are performed for classical and quantum versions of this lattice model system. We show the onset of space modulated structures at low temperatures.
Accelerating DEM simulations on GPUs by reducing the impact of warp divergences<|sep|>A way to accelerate DEM calculations on the GPUs is developed. We examined how warp divergences take place in the contact detection and the force calculations taking account of the GPU architecture. Then we showed a strategy to reduce the impact of the warp divergences on the runtime of the DEM force calculations.
Passive Haptic Rehearsal for Accelerated Piano Skill Acquisition<|sep|>Passive haptic learning (PHL) uses vibrotactile stimulation to train piano songs using repetition, even when the recipient of stimulation is focused on other tasks. However, many of the benefits of playing piano cannot be acquired without actively playing the instrument. In this position paper, we posit that passive haptic rehearsal, where active piano practice is assisted by separate sessions of passive stimulation, is of greater everyday use than solely PHL. We propose a study to examine the effects of passive haptic rehearsal for self-paced piano learners and consider how to incorporate passive rehearsal into everyday practice.
Developing a Spatial-Temporal Contextual and Semantic Trajectory Clustering Framework<|sep|>This paper reports on ongoing research investigating more expressive approaches to spatial-temporal trajectory clustering. Spatial-temporal data is increasingly becoming universal as a result of widespread use of GPS and mobile devices, which makes mining and predictive analyses based on trajectories a critical activity in many domains. Trajectory analysis methods based on clustering techniques heavily often rely on a similarity definition to properly provide insights. However, although trajectories are currently described in terms of its two dimensions (space and time), their representation is limited in that it is not expressive enough to capture, in a combined way, the structure of space and time as well as the contextual and semantic trajectory properties. Moreover, the massive amounts of available trajectory data make trajectory mining and analyses very challenging. In this paper, we briefly discuss (i) an improved trajectory representation that takes into consideration space-time structures, context and semantic properties of trajectories; (ii) new forms of relations between the dimensions of a pair of trajectories; and (iii) big data approaches that can be used to develop a novel spatial-temporal clustering framework.
Depth-controlled Bessel beams<|sep|>We present a ring aperture with independently switchable segments for the three-dimensional control of quasi propagation invariant beams. We demonstrate that our liquid crystal design concept preserves coherence and generates the Bessel beam structure.
A mechanism of bar formation in disk galaxies: synchronization of apsidal precession<|sep|>We discuss the mechanism(s) of bar formation in isolated and tidally interacting disk galaxies using the results of idealized collisionless Nbody simulations of the galaxies. In order to better understand the mechanism, we investigate orbital eccentricities (e), epochs of apocenter passages (t_a), azimuthal angles at t_a (varphi_a), precession rates (Omega_pre), for individual stars, as well as bar strengths represented by relative m=2 Fourier amplitude (A_2) and bar pattern speeds (Omega_bar). The main results are as follows. A significant fraction of stars with initially different varphi_a and Omega_pre in an isolated disk galaxy can have similar values within several dynamical timescales. This synchronization of varphi_a and Omega_pre, which is referred to as apsidal precession synchronization (``APS'') in the present study, is caused by the enhanced strength of the tangential component of gravitational force. A weak seed bar (A_2<0.1) is first formed through APS in local regions of a disk, then the bar grows due to APS. In the bar growth phase (0.1<A_2<0.4), APS can proceed more efficiently due to stronger tangential force from the bar so that it can enhance the bar strength further. This positive feedback loop in APS is the key physical mechanism of bar growth in isolated stellar disks. Bar formation can be severely suppressed in disks with lower disk mass fractions and/or higher $Q$ parameters due to much less efficient APS. APS proceeds more rapidly and more efficiently due to strong tidal perturbation in the formation of tidal bars compared to spontaneous bar formation.
A smooth particle hydrodynamics code to model collisions between solid, self-gravitating objects<|sep|>Modern graphics processing units (GPUs) lead to a major increase in the performance of the computation of astrophysical simulations. Owing to the different nature of GPU architecture compared to traditional central processing units (CPUs) such as x86 architecture, existing numerical codes cannot be easily migrated to run on GPU. Here, we present a new implementation of the numerical method smooth particle hydrodynamics (SPH) using CUDA and the first astrophysical application of the new code: the collision between Ceres-sized objects. The new code allows for a tremendous increase in speed of astrophysical simulations with SPH and self-gravity at low costs for new hardware. We have implemented the SPH equations to model gas, liquids and elastic, and plastic solid bodies and added a fragmentation model for brittle materials. Self-gravity may be optionally included in the simulations and is treated by the use of a Barnes-Hut tree. We find an impressive performance gain using NVIDIA consumer devices compared to our existing OpenMP code. The new code is freely available to the community upon request.
Neutrino Portal Dark Matter: From Dwarf Galaxies to IceCube<|sep|>It has been suggested that the baseline scenario of collisionless cold dark matter over-predicts the numbers of satellite galaxies, as well as the dark matter (DM) densities in galactic centers. This apparent lack of structure at small scales can be accounted for if one postulates neutrino-DM and DM-DM interactions mediated by light O(MeV) force carriers. In this letter, we consider a simple, consistent model of neutrinophilic DM with these features where DM and a "secluded" SM-singlet neutrino species are charged under a new $U(1)$ gauge symmetry. An important ingredient of this model is that the secluded sector couples to the Standard Model fields only through neutrino mixing. We observe that the secluded and active neutrinos recouple, leading to a large relic secluded neutrino population. This relic population can prevent small-scale halos from collapsing, while at same time significantly modifying the optical depth of ultra-high-energy neutrinos recently observed at Icecube. We find that the bulk of the parameter space accommodating an (a)symmetric thermal relic has potentially observable consequences for the IceCube high energy signal, with some of the parameter ranges already ruled out by the existing data. Future data may confirm this mechanism if either spectral absorption features or correlations with nearby sources are observed.
On the East-West Longitudinally Asymmetric Distribution of Solar Proton Events<|sep|>A large data set of 78 solar proton events observed near the Earth's orbit during 1996-2011 is investigated. An East-West longitudinal (azimuthal) asymmetry is found to exist in the distribution of flare sources of solar proton events. With the same longitudinal separation between the flare sources and the magnetic field line footpoint of observer, the number of the solar proton events originating from solar sources located on the eastern side of the nominal magnetic footpoint of observer is larger than the number of the solar proton events from solar sources located on the western side. We emphasize the importance of this statistical investigation in two aspects. On the one hand, this statistical finding confirms our previous simulation results obtained by numerically solving five-dimensional Fokker-Planck equation of solar energetic particle (SEP) transport. On the other hand, the East-West longitudinally (azimuthally) asymmetric distribution of solar proton events accumulated over a long time period provides an observational evidence for the effects of perpendicular diffusion on the SEP propagation in the heliosphere. We further point out that, in the sense of perpendicular diffusion, our numerical simulations and statistical results of SEP events confirm each other. We discuss in detail the important effects of perpendicular diffusion on the formation of the East-West azimuthal (longitudinal) asymmetry of SEP distribution in two physical scenarios, i.e., 'multiple SEP events with one spacecraft' and 'one SEP event with multiple spacecraft'. A functional relation I_{max}(r)=kr^{-1.7} quantifying the radial dependence of SEP peak intensities is obtained and utilized in the analysis of physical mechanism. The relationship between our results and those of Dresing et al. is also discussed.
Constructing Entanglement Witness Via Real Skew-Symmetric Operators<|sep|>In this work, new types of EWs are introduced. They are constructed by using real skew-symmetric operators defined on a single party subsystem of a bipartite dxd system and a maximal entangled state in that system. A canonical form for these witnesses is proposed which is called canonical EW in corresponding to canonical real skew-symmetric operator. Also for each possible partition of the canonical real skew-symmetric operator corresponding EW is obtained. The method used for dxd case is extended to d1xd2 systems. It is shown that there exist Cd2xd1 distinct possibilities to construct EWs for a given d1xd2 Hilbert space. The optimality and nd-optimality problem is studied for each type of EWs. In each step, a large class of quantum PPT states is introduced. It is shown that among them there exist entangled PPT states which are detected by the constructed witnesses. Also the idea of canonical EWs is extended to obtain other EWs with greater PPT entanglement detection power.
On the reconstruction of block-sparse signals with an optimal number of measurements<|sep|>Let A be an M by N matrix (M < N) which is an instance of a real random Gaussian ensemble. In compressed sensing we are interested in finding the sparsest solution to the system of equations A x = y for a given y. In general, whenever the sparsity of x is smaller than half the dimension of y then with overwhelming probability over A the sparsest solution is unique and can be found by an exhaustive search over x with an exponential time complexity for any y. The recent work of Cand\'es, Donoho, and Tao shows that minimization of the L_1 norm of x subject to A x = y results in the sparsest solution provided the sparsity of x, say K, is smaller than a certain threshold for a given number of measurements. Specifically, if the dimension of y approaches the dimension of x, the sparsity of x should be K < 0.239 N. Here, we consider the case where x is d-block sparse, i.e., x consists of n = N / d blocks where each block is either a zero vector or a nonzero vector. Instead of L_1-norm relaxation, we consider the following relaxation min x \| X_1 \|_2 + \| X_2 \|_2 + ... + \| X_n \|_2, subject to A x = y where X_i = (x_{(i-1)d+1}, x_{(i-1)d+2}, ..., x_{i d}) for i = 1,2, ..., N. Our main result is that as n -> \infty, the minimization finds the sparsest solution to Ax = y, with overwhelming probability in A, for any x whose block sparsity is k/n < 1/2 - O(\epsilon), provided M/N > 1 - 1/d, and d = \Omega(\log(1/\epsilon)/\epsilon). The relaxation can be solved in polynomial time using semi-definite programming.
An ASP approach for reasoning on neural networks under a finitely many-valued semantics for weighted conditional knowledge bases<|sep|>Weighted knowledge bases for description logics with typicality have been recently considered under a "concept-wise" multipreference semantics (in both the two-valued and fuzzy case), as the basis of a logical semantics of MultiLayer Perceptrons (MLPs). In this paper we consider weighted conditional ALC knowledge bases with typicality in the finitely many-valued case, through three different semantic constructions. For the boolean fragment LC of ALC we exploit ASP and "asprin" for reasoning with the concept-wise multipreference entailment under a phi-coherent semantics, suitable to characterize the stationary states of MLPs. As a proof of concept, we experiment the proposed approach for checking properties of trained MLPs. The paper is under consideration for acceptance in TPLP.
CFHTLenS: Weak lensing calibrated scaling relations for low mass clusters of galaxies<|sep|>We present weak lensing and X-ray analysis of 12 low mass clusters from the CFHTLenS and XMM-CFHTLS surveys. We combine these systems with high-mass systems from CCCP and low-mass systems from COSMOS to obtain a sample of 70 systems, spanning over two orders of magnitude in mass. We measure core-excised Lx-Tx, M-Lx and M-Tx scaling relations and include corrections for observational biases. By providing fully bias corrected relations, we give the current limitations for Lx and Tx as cluster mass proxies. We demonstrate that Tx benefits from a significantly lower intrinsic scatter at fixed mass than Lx. By studying the residuals of the bias corrected relations, we show for the first time using weak lensing masses that galaxy groups seem more luminous and warmer for their mass than clusters. This implies a steepening of the M-Lx and M-Tx relations at low masses. We verify the inferred steepening using a different high mass sample from the literature and show that variance between samples is the dominant effect leading to discrepant scaling relations. We divide our sample into subsamples of merging and relaxed systems, and find that mergers may have enhanced scatter in lensing measurements, most likely due to stronger triaxiality and more substructure. For the Lx-Tx relation, which is unaffected by lensing measurements, we find the opposite trend in scatter. We also explore the effects of X-ray cross-calibration and find that Chandra calibration leads to flatter Lx-Tx and M-Tx relations than XMM-Newton.
Optical Variability, Rotation Period and Inclination Angle of the M9.5 dwarf BRI 0021-0214<|sep|>We report $I$-band photometric observations of the radio-detected M9.5 dwarf BRI 0021-0214, obtained with the Galway Ultra Fast Imager (GUFI) on the 1.8m Vatican Advanced Technology Telescope VATT at Mt. Graham International Observatory, Arizona. In total, 19 hours of observations over a 73 day baseline were obtained. BRI 0021-0214 was shown to exhibit modulated emission with a period of $ 3.052 \pm 0.004$ hours with a mean amplitude variability of 0.0044 mag. When combined with rotational velocity data obtained from previous work, our newly discovered rotation period gives an inclination angle of 51.7$^{+5.0}_{-4.5}$ degrees for the rotation axis of BRI 0021-0214 relative to our line of sight. Previous studies have reported that the most plausible cause for optical variability from this dwarf is a consequence of suspended co-rotating dust clouds in its atmosphere. However reports of enhanced H$_{\alpha}$ and intermittent coherent radio emission suggest the possibility of auroral activity in its magnetosphere. Further, more coordinated multiwavlength observations of this dwarf could fully resolve the nature of this elusive rapid-rotator object's observational properties.
An Assertion-Based Program Logic for Probabilistic Programs<|sep|>Research on deductive verification of probabilistic programs has considered expectation-based logics, where pre- and post-conditions are real-valued functions on states, and assertion-based logics, where pre- and post-conditions are boolean predicates on state distributions. Both approaches have developed over nearly four decades, but they have different standings today. Expectation-based systems have managed to formalize many sophisticated case studies, while assertion-based systems today have more limited expressivity and have targeted simpler examples. We present Ellora, a sound and relatively complete assertion-based program logic, and demonstrate its expressivity by verifying several classical examples of randomized algorithms using an implementation in the EasyCrypt proof assistant. Ellora features new proof rules for loops and adversarial code, and supports richer assertions than existing program logics. We also show that Ellora allows convenient reasoning about complex probabilistic concepts by developing a new program logic for probabilistic independence and distribution law, and then smoothly embedding it into Ellora. Our work demonstrates that the assertion-based approach is not fundamentally limited and suggests that some notions are potentially easier to reason about in assertion-based systems.
Atomistic investigation of low-field mobility in graphene nanoribbons<|sep|>We have investigated the main scattering mechanisms affecting mobility in graphene nanoribbons using detailed atomistic simulations. We have considered carrier scattering due to acoustic and optical phonons, edge roughness, single defects, and ionized impurities, and we have defined a methodology based on simulations of statistically meaningful ensembles of nanoribbon segments. Edge disorder heavily affects mobility at room temperature in narrower nanoribbons, whereas charged impurities and phonons are hardly the limiting factors. Results are favorably compared to the few experiments available in the literature.
Interference of axially-shifted Laguerre-Gaussian beams and their interaction with atoms<|sep|>Counter-propagating co-axial Laguerre-Gaussian (LG) beams are considered, not in the familiar scenario where the focal planes coincide at $z=0$, but when they are separated by a finite axial distance $d$. The simplest case is where both beams are doughnut beams which have the same linear polarisation. The total fields of this system are shown to display novel amplitude and phase distributions and are shown to give rise to a ring or a finite ring lattice composed of double rings and single central ring. When the beams have slightly different frequencies the ring lattice pattern becomes a finite set of rotating Ferris wheels and the whole pattern also moves axially between the focal planes. We show that the field of such an axially shifted pair of counter-propagating LG beams generate trapping potentials due to the dipole force which can trap two-level atoms in the components of the ring lattice. We also highlight a unique feature of this system which involves the creation of a new longitudinal optical atom trapping potential due to the scattering force which arises solely when $d \ne 0$. The results are illustrated using realistic parameters which also confirm the importance of the Gouy and curvature effects in determining the ring separation both radially and axially and gives rise to the possibility of atom tunnelling between components of the double rings.
Low-Pass Filters, Fourier Series and Partial Differential Equations<|sep|>When Fourier series are used for applications in physics, involving partial differential equations, sometimes the process of resolution results in divergent series for some quantities. In this paper we argue that the use of linear low-pass filters is a valid way to regularize such divergent series. In particular, we show that these divergences are always the result of oversimplification in the proposition of the problems, and do not have any fundamental physical significance. We define the first-order linear low-pass filter in precise mathematical terms, establish some of its properties, and then use it to construct higher-order filters. We also show that the first-order linear low-pass filter, understood as a linear integral operator in the space of real functions, commutes with the second-derivative operator. This can greatly simplify the use of these filters in physics applications, and we give a few simple examples to illustrate this fact.
Effect of site dilution in the two-dimensional attractive Hubbard model<|sep|>We study the percolative superconducting transition as the density of randomly placed attractive centers grows in a host metal. Employing the Hubbard-Stratanovich transformation for the interaction and allowing for spatial, thermal fluctuations of the pairing field, we obtain real-space features of the transition from weak to strong coupling. Spectral and transport properties are studied in detail. BCS-BEC crossover is discussed in the context of site dilution of attractive centers.
Learning Soft Constraints From Constrained Expert Demonstrations<|sep|>Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. Despite the simplicity of the formulation, our method is able to obtain good results. We demonstrate our approach on synthetic environments and real world highway driving data.
Global Attention Mechanism: Retain Information to Enhance Channel-Spatial Interactions<|sep|>A variety of attention mechanisms have been studied to improve the performance of various computer vision tasks. However, the prior methods overlooked the significance of retaining the information on both channel and spatial aspects to enhance the cross-dimension interactions. Therefore, we propose a global attention mechanism that boosts the performance of deep neural networks by reducing information reduction and magnifying the global interactive representations. We introduce 3D-permutation with multilayer-perceptron for channel attention alongside a convolutional spatial attention submodule. The evaluation of the proposed mechanism for the image classification task on CIFAR-100 and ImageNet-1K indicates that our method stably outperforms several recent attention mechanisms with both ResNet and lightweight MobileNet.
Optical study of the hyper-luminous X-ray source 2XMM J011942.7+032421<|sep|>We present the identification and characterization of the optical counterpart to 2XMM J011942.7+032421, one of the most luminous and distant ultra-luminous X-ray sources (ULXs). The counterpart is located near a star forming region in a spiral arm of the galaxy NGC 470 with u, g, and r magnitudes of 21.53, 21.69, and 21.71 mags, respectively. The luminosity of the counterpart is much larger than that of a single O-type star, indicating that it may be a stellar cluster. Our optical spectroscopic observations confirm the association of the X-ray source and the optical counterpart with its host galaxy NGC 470, which validates the high, > 10^41 erg/s, X-ray luminosity of the source. Its optical spectrum is embedded with numerous emission lines, including H recombination lines, metallic forbidden lines and more notably the high-ionization HeII (lambda 4686 A) line. This line shows a large velocity dispersion of $\simeq$ 410 \kms, consistent with the existence of a compact (< 5 AU) highly-ionized accretion disc rotating around the central X-ray source. The 1.4 x 10^37 erg/s luminosity of the HeII line emission makes the source one of the most luminous ULXs in the emission of that line. This, together with the high X-ray luminosity and the large velocity dispersion of the HeII emission, suggests that the source is an ideal candidate for more extensive follow-up observations for understanding the nature of hyper-luminous X-ray sources, a more luminous subgroup of ULXs and more likely candidates for intermediate-mass black holes.
Card games as pointer structures: case studies in mobile CSP modelling<|sep|>The author has long enjoyed using the CSP refinement checker FDR to solve puzzles, as witnessed by examples in \cite{tpc,ucs}. Recent experiments have shown that a number of games of patience (card games for one) are now well within bounds. We discuss the modelling approaches used to tackle these and avoid symmetric states. For two such games we reveal much higher percentages of winnable games than was previously believed. The techniques developed for some of these card games -which employ various dynamic patterns of cards - suggest techniques for modelling pointer structures in CSP and FDR analogous to those used with the pi-calculus. Most of these use CSP's ability to express mobile systems.
Constraining the gauge and scalar sectors of the doublet left-right symmetric model<|sep|>We consider a left-right symmetric extension of the Standard Model where the spontaneous breakdown of the left-right symmetry is triggered by doublets. The electroweak $\rho$ parameter is protected from large corrections in this Doublet Left-Right Model (DLRM), contrary to the triplet case. This allows in principle for more diverse patterns of symmetry breaking. We consider several constraints on the gauge and scalar sectors of DLRM: the unitarity of scattering processes involving gauge bosons with longitudinal polarisations, the radiative corrections to the muon $\Delta r$ parameter and the electroweak precision observables measured at the $Z$ pole and at low energies. Combining these constraints within the frequentist CKMfitter approach, we see that the fit pushes the scale of left-right symmetry breaking up to a few TeV, while favouring an electroweak symmetry breaking triggered not only by the $ SU(2)_L \times SU(2)_R $ bi-doublet, which is the case most commonly considered in the literature, but also by the $ SU(2)_L $ doublet.
Oxygen issue in Core Collapse Supernovae<|sep|>We study the spectroscopic properties of a selected sample of 26 events within Core Collapse Supernovae (CCSNe) family. Special attention is paid to the nebular oxygen forbidden line [O I] 6300,6364\AA\ doublet. We analyze the line flux ratio "$F_{6300}/F_{6364}$", and infer information about the optical depth evolution, densities, volume-filling factors in the oxygen emitting zones. The line luminosity is measured for the sample events and its evolution is discussed on the basis of the bolometric light curve properties in type II and in type Ib-c SNe. The luminosities are then translated into oxygen abundances using two different methods. The resulting oxygen amounts are combined with the recovered $^{56}$Ni masses and compared with theoretical models by means of the "$[O/Fe] .vs. M_{ms}$" diagram. Two distinguishable and continuous populations, corresponding to Ib-c and type II SNe, are found. The higher mass nature of the ejecta in type II objects is also imprinted on the [Ca II] 7291,7324\AA\ over [O I] 6300,6364\AA\ luminosity ratios. Our results may be used as input parameters for theoretical models studying the chemical enrichment of galaxies.
The role of neutron star mergers in the chemical evolution of the Galactic halo<|sep|>Aims. We explore the problem of the site of production of Eu. We use also the information present in the observed spread in the Eu abundances in the early Galaxy, not only its average trend. Moreover, we extend to other heavy elements (Ba, Sr, Rb, Zr) our investigations to provide additional constraints to our results. Methods. We adopt a stochastic chemical evolution model taking into account inhomogeneous mixing. The adopted yields of Eu from neutron star mergers (NSM) and from core-collapse supernovae (SNII) are those that are able to explain the average [Eu/Fe]-[Fe/H] trend observed for solar neighborhood stars, in the framework of a well-tested homogeneous model for the chemical evolution of the MilkyWay. Rb, Sr, Zr, and Ba are produced by both the s- and r-process. The s-process contribution by spinstars is the same as in our previous papers. Results. NSM that merge in less than 10 Myr or NSM combined with a source of r-process generated by massive stars can explain the spread of [Eu/Fe] in the Galactic halo. The combination of r-process production by NSM and s-process production by spinstars is able to reproduce the available observational data for Sr, Zr and Ba. We also show the first predictions for Rb in the Galactic halo. Conclusions. We confirm previous results that either NSM with very short time scale or both NSM and at least a fraction of SNII should have contributed to the synthesis of Eu in the Galaxy. The r-process production by NSM - complemented by an s-process production by spinstars - provide results compatible with our previous findings based on other r-process sites. We critically discuss the weak and strong points of both NSM and SNII scenarios for producing Eu and eventually suggest that the best solution is probably a mixed one in which both sources produce Eu. In fact, this scenario better reproduces the scatter observed in all the studied elements. [abridged]
On-the-Fly Attention Modulation for Neural Generation<|sep|>Despite considerable advancements with deep neural language models (LMs), neural text generation still suffers from degeneration: the generated text is repetitive, generic, self-contradictory, and often lacks commonsense. Our analyses on sentence-level attention patterns in LMs reveal that neural degeneration may be associated with insufficient learning of task-specific characteristics by the attention mechanism. This finding motivates on-the-fly attention modulation -- a simple but effective method that enables the injection of priors into attention computation during inference. Automatic and human evaluation results on three text generation benchmarks demonstrate that attention modulation helps LMs generate text with enhanced fluency, creativity, and commonsense reasoning, in addition to significantly reduce sentence-level repetition.
Resource-Size matters: Improving Neural Named Entity Recognition with Optimized Large Corpora<|sep|>This study improves the performance of neural named entity recognition by a margin of up to 11% in F-score on the example of a low-resource language like German, thereby outperforming existing baselines and establishing a new state-of-the-art on each single open-source dataset. Rather than designing deeper and wider hybrid neural architectures, we gather all available resources and perform a detailed optimization and grammar-dependent morphological processing consisting of lemmatization and part-of-speech tagging prior to exposing the raw data to any training process. We test our approach in a threefold monolingual experimental setup of a) single, b) joint, and c) optimized training and shed light on the dependency of downstream-tasks on the size of corpora used to compute word embeddings.
Heuristic construction of exact experimental designs under multiple resource constraints<|sep|>The aim of this paper is twofold. First, we introduce "resource constraints" as a general concept that covers many practical restrictions on experimental design. Second, for computing efficient exact designs of experiments under any combination of resource constraints, we propose a tabu search heuristic that uses some ideas of the Detmax procedure. To illustrate the scope and performance of our heuristic, we computed D-efficient designs for 1) a block model with limits on the numbers of blocks and on the availability of experimental material; 2) a quadratic regression model with simultaneous marginal and cost constraints; 3) a non-linear regression model with simultaneous direct and cost constraints. As we show, the proposed heuristic generates comparable or better results than algorithms specialized for computing optimal designs under less general constraints.
Homogenized effective temperatures from stellar libraries<|sep|>External errors of effective temperatures of stars for selected libraries are estimated from data intercomparisons. It is found that the obtained errors are mainly in a good correspondence with the published data. The results may be used to homogenize the effective temperatures by averaging the data (with the weights inversely proportional to the squared errors) from independent sources.
Dual Graph Polynomials and a 4-face Formula<|sep|>We study the dual graph polynomials and the case when a Feynman graph has no triangles but has a 4-face. This leads to the proof of the duality-admissibility of all graphs up to 18 loops. As a consequence, the $c_2$ invariant is the same for all 4 Feynman period representations (position, momentum, parametric and dual parametric) for any physically relevant graph.
$\textit{Herschel}$/SPIRE Observations of Water Production Rates and Ortho-to-Para Ratios in Comets<|sep|>This paper presents $\textit{Herschel}$/SPIRE spectroscopic observations of several fundamental rotational ortho- and para-water transitions seen in three Jupiter-family comets and one Oort-cloud comet. Radiative transfer models that include excitation by collisions with neutrals and electrons, and by solar infrared radiation were used to produce synthetic emission line profiles originating in the cometary coma. Ortho-to-para ratios (OPRs) were determined and used to derived water production rates for all comets. Comparisons are made with the water production rates derived using an OPR of 3. The OPR of three of the comets in this study are much lower than the statistical equilibrium value of 3, however they agree with observations of comets 1P/Halley and C/2001 A2 (LINEAR), and the protoplanetary disc TW Hydrae. These results provide evidence suggesting that OPR variation is caused by post-sublimation gas-phase nuclear-spin conversion processes. The water production rates of all comets agree with previous work and, in general, decrease with increasing nucleocentric offset. This could be due to a temperature profile, additional water source, or OPR variation in the comae, or model inaccuracies.
From Exceptional Field Theory to Heterotic Double Field Theory via K3<|sep|>In this paper we show how to obtain the heterotic double field theory from exceptional field theory by breaking half of the supersymmetry. We focus on the $\mathrm{SL}(5)$ exceptional field theory and show that when the extended space contains a generalised $\mathrm{SU}(2)$-structure manifold one can define a reduction to obtain the heterotic $\mathrm{SO}(3,n)$ double field theory. In this picture, the reduction on the $\mathrm{SU}(2)$-structure breaks half of the supersymmetry of the exceptional field theory and the gauge group of the heterotic double field theory is given by the embedding tensor of the reduction used. Finally, we study the example of a consistent truncation of M-theory on K3 and recover the duality with the heterotic string on $T^3$. This suggests that the extended space can be made sense of even in the case of non-toroidal compactifications.
Ghost factors in Gauss-sum factorization with transmon qubits<|sep|>A challenge in the Gauss sums factorization scheme is the presence of ghost factors - non-factors that behave similarly to actual factors of an integer - which might lead to the misidentification of non-factors as factors or vice versa, especially in the presence of noise. We investigate Type II ghost factors, which are the class of ghost factors that cannot be suppressed with techniques previously laid out in the literature. The presence of Type II ghost factors and the coherence time of the qubit set an upper limit for the total experiment time, and hence the largest factorizable number with this scheme. Discernability is a figure of merit introduced to characterize this behavior. We introduce preprocessing as a strategy to increase the discernability of a system, and demonstrate the technique with a transmon qubit. This can bring the total experiment time of the system closer to its decoherence limit, and increase the largest factorizable number.
Flavour Physics in the Soft Wall Model<|sep|>We extend the description of flavour that exists in the Randall-Sundrum (RS) model to the soft wall (SW) model in which the IR brane is removed and the Higgs is free to propagate in the bulk. It is demonstrated that, like the RS model, one can generate the hierarchy of fermion masses by localising the fermions at different locations throughout the space. However, there are two significant differences. Firstly the possible fermion masses scale down, from the electroweak scale, less steeply than in the RS model and secondly there now exists a minimum fermion mass for fermions sitting towards the UV brane. With a quadratic Higgs VEV, this minimum mass is about fifteen orders of magnitude lower than the electroweak scale. We derive the gauge propagator and despite the KK masses scaling as $m_n^2\sim n$, it is demonstrated that the coefficients of four fermion operators are not divergent at tree level. FCNC's amongst kaons and leptons are considered and compared to calculations in the RS model, with a brane localised Higgs and equivalent levels of tuning. It is found that since the gauge fermion couplings are slightly more universal and the SM fermions typically sit slightly further towards the UV brane, the contributions to observables such as $\epsilon_K$ and $\Delta m_K$, from the exchange of KK gauge fields, are significantly reduced.
Generating functionals and Gaussian approximations for interruptible delay reactions<|sep|>We develop a generating functional description of the dynamics of non-Markovian individual-based systems, in which delay reactions can be terminated before completion. This generalises previous work in which a path-integral approach was applied to dynamics in which delay reactions complete with certainty. We construct a more widely applicable theory, and from it we derive Gaussian approximations of the dynamics, valid in the limit of large, but finite population sizes. As an application of our theory we study predator-prey models with delay dynamics due to gestation or lag periods to reach the reproductive age. In particular we focus on the effects of delay on noise-induced cycles.
The Mid-Infrared Spectrum of the Short Orbital Period Polar EF Eridani from the Spitzer Space Telescope<|sep|>We present the first mid-infrared (5.5-14.5 micron) spectrum of a highly magnetic cataclysmic variable, EF Eridani, obtained with the Infrared Spectrograph on the Spitzer Space Telescope. The spectrum displays a relatively flat, featureless continuum. A spectral energy distribution model consisting of a 9500 K white dwarf, L5 secondary star, cyclotron emission corresponding to a B~13 MG white dwarf magnetic field, and an optically thin circumbinary dust disk is in reasonable agreement with the extant 2MASS, IRAC, and IRS observations of EF Eri. Cyclotron emission is ruled out as a dominant contributor to the infrared flux density at wavelengths >3 microns. The spectral energy distribution longward of ~5 microns is dominated by dust emission. Even longer wavelength observations would test the model's prediction of a continuing gradual decline in the circumbinary disk-dominated region of the spectral energy distribution.
Jump-starting coordination in a stag hunt: Motivation, mechanisms, and their analysis<|sep|>The stag hunt (or assurance game) is a simple game that has been used as a prototype of a variety of social coordination problems (ranging from the social contract to the adoption of technical standards). Players have the option to either use a superior cooperative strategy whose payoff depends on the other players' choices or use an inferior strategy whose payoff is independent of what other players do; the cooperative strategy may incur a loss if sufficiently many other players do not cooperate. Stag hunts have two (strict) pure Nash equilibria, namely, universal cooperation and universal defection (as well as a mixed equilibrium of low predictive value). Selection of the inferior (pure) equilibrium is called a coordination failure. In this paper, we present and analyze using game-theoretic techniques mechanisms aiming to avert coordination failures and incite instead selection of the superior equilibrium. Our analysis is based on the solution concepts of Nash equilibrium, dominance solvability, as well as a formalization of the notion of "incremental deployability," which is shown to be keenly relevant to the sink equilibrium.
Tropical optimization problems in time-constrained project scheduling<|sep|>We consider a project that consists of activities to be performed in parallel under various temporal constraints, which include start-start, start-finish and finish-start precedence relationships, release times, deadlines, and due dates. Scheduling problems are formulated to find optimal schedules for the project with respect to different objective functions to be minimized, such as the project makespan, the maximum deviation from the due dates, the maximum flow-time, and the maximum deviation of finish times. We represent these problems as optimization problems in terms of tropical mathematics, and then solve them by applying direct solution methods of tropical optimization. As a result, new direct solutions of the scheduling problems are obtained in a compact vector form, which is ready for further analysis and practical implementation. The solutions are illustrated by simple numerical examples.
On the Data Fight Between Cities and Mobility Providers<|sep|>E-Scooters are changing transportation habits. In an attempt to oversee scooter usage, the Los Angeles Department of Transportation has put forth a specification that requests detailed data on scooter usage from scooter companies. In this work, we first argue that L.A.'s data request for using a new specification is not warranted as proposed use cases can be met by already existing specifications. Second, we show that even the existing specification, that requires companies to publish real-time data of parked scooters, puts the privacy of individuals using the scooters at risk. We then propose an algorithm that enables formal privacy and utility guarantees when publishing parked scooters data, allowing city authorities to meet their use cases while preserving riders' privacy.
Micro-Browsing Models for Search Snippets<|sep|>Click-through rate (CTR) is a key signal of relevance for search engine results, both organic and sponsored. CTR of a result has two core components: (a) the probability of examination of a result by a user, and (b) the perceived relevance of the result given that it has been examined by the user. There has been considerable work on user browsing models, to model and analyze both the examination and the relevance components of CTR. In this paper, we propose a novel formulation: a micro-browsing model for how users read result snippets. The snippet text of a result often plays a critical role in the perceived relevance of the result. We study how particular words within a line of snippet can influence user behavior. We validate this new micro-browsing user model by considering the problem of predicting which snippet will yield higher CTR, and show that classification accuracy is dramatically higher with our micro-browsing user model. The key insight in this paper is that varying relatively few words within a snippet, and even their location within a snippet, can have a significant influence on the clickthrough of a snippet.
Universality classes of non-Hermitian random matrices<|sep|>Non-Hermitian random matrices have been utilized in such diverse fields as dissipative and stochastic processes, mesoscopic physics, nuclear physics, and neural networks. However, the only known universal level-spacing statistics is that of the Ginibre ensemble characterized by complex-conjugation symmetry. Here we report our discovery of two other distinct universality classes characterized by transposition symmetry. We find that transposition symmetry alters repulsive interactions between two neighboring eigenvalues and deforms their spacing distribution. Such alteration is not possible with other symmetries including Ginibre's complex-conjugation symmetry which can affect only nonlocal correlations. Our results complete the non-Hermitian counterpart of Wigner-Dyson's threefold universal statistics of Hermitian random matrices and serve as a basis for characterizing nonintegrability and chaos in open quantum systems with symmetry.
Locally Recoverable Codes with Availability $t\geq 2$ from Fiber Products of Curves<|sep|>We generalize the construction of locally recoverable codes on algebraic curves given by Barg, Tamo and Vl\u{a}du\c{t} to those with arbitrarily many recovery sets by exploiting the structure of fiber products of curves. Employing maximal curves, we create several new families of locally recoverable codes with multiple recovery sets, including codes with two recovery sets from the generalized Giulietti and Korchm\'{a}ros (GK) curves and the Suzuki curves, and new locally recoverable codes with many recovery sets based on the Hermitian curve, using a fiber product construction of van der Geer and van der Vlugt. In addition, we consider the relationship between local error recovery and global error correction as well as the availability required to locally recover any pattern of a fixed number of erasures.
The Quantum Hall Effects: Philosophical Approach<|sep|>The Quantum Hall Effects offer a rich variety of theoretical and experimental advances. They provide interesting insights on such topics as complementarity, gauge invariance, strong interactions, emergence of new theoretical concepts. This paper focuses on some related philosophical questions. Hacking's views on Scientific Realism, Chalmers' on Non Figurative Realism are discussed. It is argued that the difficulties with those versions of realism may be resolved within a dialectical materialist approach. The latter is shown to provide a rational approach to the phenomena, the theory and the ontology of the Quantum Hall Effects.
Error estimation and adaptivity for stochastic collocation finite elements Part II: multilevel approximation<|sep|>A multilevel adaptive refinement strategy for solving linear elliptic partial differential equations with random data is recalled in this work. The strategy extends the a posteriori error estimation framework introduced by Guignard and Nobile in 2018 (SIAM J. Numer. Anal, 56, 3121--3143) to cover problems with a nonaffine parametric coefficient dependence. A suboptimal, but nonetheless reliable and convenient implementation of the strategy involves approximation of the decoupled PDE problems with a common finite element approximation space. Computational results obtained using such a single-level strategy are presented in part I of this work (Bespalov, Silvester and Xu, arXiv:2109.07320). Results obtained using a potentially more efficient multilevel approximation strategy, where meshes are individually tailored, are discussed herein. The codes used to generate the numerical results are available online.
An End-to-End Transformer Model for Crowd Localization<|sep|>Crowd localization, predicting head positions, is a more practical and high-level task than simply counting. Existing methods employ pseudo-bounding boxes or pre-designed localization maps, relying on complex post-processing to obtain the head positions. In this paper, we propose an elegant, end-to-end Crowd Localization Transformer named CLTR that solves the task in the regression-based paradigm. The proposed method views the crowd localization as a direct set prediction problem, taking extracted features and trainable embeddings as input of the transformer-decoder. To reduce the ambiguous points and generate more reasonable matching results, we introduce a KMO-based Hungarian matcher, which adopts the nearby context as the auxiliary matching cost. Extensive experiments conducted on five datasets in various data settings show the effectiveness of our method. In particular, the proposed method achieves the best localization performance on the NWPU-Crowd, UCF-QNRF, and ShanghaiTech Part A datasets.
Distortion-Adaptive Grape Bunch Counting for Omnidirectional Images<|sep|>This paper proposes the first object counting method for omnidirectional images. Because conventional object counting methods cannot handle the distortion of omnidirectional images, we propose to process them using stereographic projection, which enables conventional methods to obtain a good approximation of the density function. However, the images obtained by stereographic projection are still distorted. Hence, to manage this distortion, we propose two methods. One is a new data augmentation method designed for the stereographic projection of omnidirectional images. The other is a distortion-adaptive Gaussian kernel that generates a density map ground truth while taking into account the distortion of stereographic projection. Using the counting of grape bunches as a case study, we constructed an original grape-bunch image dataset consisting of omnidirectional images and conducted experiments to evaluate the proposed method. The results show that the proposed method performs better than a direct application of the conventional method, improving mean absolute error by 14.7% and mean squared error by 10.5%.
A resonant family of dynamically cold small bodies in the near-Earth asteroid belt<|sep|>Near-Earth objects (NEOs) moving in resonant, Earth-like orbits are potentially important. On the positive side, they are the ideal targets for robotic and human low-cost sample return missions and a much cheaper alternative to using the Moon as an astronomical observatory. On the negative side and even if small in size (2-50 m), they have an enhanced probability of colliding with the Earth causing local but still significant property damage and loss of life. Here, we show that the recently discovered asteroid 2013 BS45 is an Earth co-orbital, the sixth horseshoe librator to our planet. In contrast with other Earth's co-orbitals, its orbit is strikingly similar to that of the Earth yet at an absolute magnitude of 25.8, an artificial origin seems implausible. The study of the dynamics of 2013 BS45 coupled with the analysis of NEO data show that it is one of the largest and most stable members of a previously undiscussed dynamically cold group of small NEOs experiencing repeated trappings in the 1:1 commensurability with the Earth. This new resonant family is well constrained in orbital parameter space and it includes at least 10 other transient members: 2003 YN107, 2006 JY26, 2009 SH2 and 2012 FC71 among them. 2012 FC71 represents the best of both worlds as it is locked in a Kozai resonance and is unlikely to impact the Earth. Objects in this group could be responsible for the production of Earth's transient irregular natural satellites.
To go deep or wide in learning?<|sep|>To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multi-layered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.
Recursive Method for the Solution of Systems of Linear Equations<|sep|>New solution method for the systems of linear equations in commutative integral domains is proposed. Its complexity is the same that the complexity of the matrix multiplication.
Real Time Event Detection in Astronomical Data Streams: Lessons from the VLBA<|sep|>A new generation of observational science instruments is dramatically increasing collected data volumes in a range of fields. These instruments include the Square Kilometre Array (SKA), Large Synoptic Survey Telescope (LSST), terrestrial sensor networks, and NASA satellites participating in "decadal survey" missions. Their unprecedented coverage and sensitivity will likely reveal wholly new categories of unexpected and transient events. Commensal methods passively analyze these data streams, recognizing anomalous events of scientific interest and reacting in real time. We report on a case example: V-FASTR, an ongoing commensal experiment at the Very Long Baseline Array (VLBA) that uses online adaptive pattern recognition to search for anomalous fast radio transients. V-FASTR triages a millisecond-resolution stream of data and promotes candidate anomalies for further offline analysis. It tunes detection parameters in real time, injecting synthetic events to continually retrain itself for optimum performance. This self-tuning approach retains sensitivity to weak signals while adapting to changing instrument configurations and noise conditions. The system has operated since July 2011, making it the longest-running real time commensal radio transient experiment to date.
Efficient grid-based method in nonequilibrium Green's function calculations. Application to model atoms and molecules<|sep|>We propose and apply the finite-element discrete variable representation to express the nonequilibrium Green's function for strongly inhomogeneous quantum systems. This method is highly favorable against a general basis approach with regard to numerical complexity, memory resources, and computation time. Its flexibility also allows for an accurate representation of spatially extended hamiltonians, and thus opens the way towards a direct solution of the two-time Schwinger/Keldysh/Kadanoff-Baym equations on spatial grids, including e.g. the description of highly excited states in atoms. As first benchmarks, we compute and characterize, in Hartree-Fock and second Born approximation, the ground states of the He atom, the H$_2$ molecule and the LiH molecule in one spatial dimension. Thereby, the ground-state/binding energies, densities and bond-lengths are compared with the direct solution of the time-dependent Schr\"odinger equation.
Doubly Aligned Incomplete Multi-view Clustering<|sep|>Nowadays, multi-view clustering has attracted more and more attention. To date, almost all the previous studies assume that views are complete. However, in reality, it is often the case that each view may contain some missing instances. Such incompleteness makes it impossible to directly use traditional multi-view clustering methods. In this paper, we propose a Doubly Aligned Incomplete Multi-view Clustering algorithm (DAIMC) based on weighted semi-nonnegative matrix factorization (semi-NMF). Specifically, on the one hand, DAIMC utilizes the given instance alignment information to learn a common latent feature matrix for all the views. On the other hand, DAIMC establishes a consensus basis matrix with the help of $L_{2,1}$-Norm regularized regression for reducing the influence of missing instances. Consequently, compared with existing methods, besides inheriting the strength of semi-NMF with ability to handle negative entries, DAIMC has two unique advantages: 1) solving the incomplete view problem by introducing a respective weight matrix for each view, making it able to easily adapt to the case with more than two views; 2) reducing the influence of view incompleteness on clustering by enforcing the basis matrices of individual views being aligned with the help of regression. Experiments on four real-world datasets demonstrate its advantages.
The discovery of a radio galaxy of at least 5 Mpc<|sep|>We discover what is in projection the largest known structure of galactic origin: a giant radio galaxy with a projected proper length of $4.99 \pm 0.04\ \mathrm{Mpc}$. The source, named Alcyoneus, was first identified in low-resolution LOFAR Two-metre Sky Survey images from which angularly compact sources had been removed. Being an extreme example in its class, Alcyoneus could shed light on the main mechanisms that drive radio galaxy growth. We find that - beyond geometry - Alcyoneus and its host galaxy appear suspiciously ordinary: the total low-frequency luminosity density, stellar mass and supermassive black hole mass are all lower than, though similar to, those of the medial giant radio galaxy (percentiles $45 \pm 3\%$, $25 \pm 9 \%$ and $23 \pm 11 \%$, respectively). The source resides in a filament of the Cosmic Web, with which it might have significant thermodynamic interaction. At $5 \cdot 10^{-16}\ \mathrm{Pa}$, the pressures in the lobes are the lowest hitherto found, and Alcyoneus therefore represents one of the most promising radio galaxies yet to probe the warm-hot intergalactic medium.
Rapid Rotation in the Kepler Field: Not a Single Star Phenomenon<|sep|>Tens of thousands of rotation periods have been measured in the Kepler fields, including a substantial fraction of rapid rotators. We use Gaia parallaxes to distinguish photometric binaries (PBs) from single stars on the unevolved lower main sequence, and compare their distribution of rotation properties to those of single stars both with and without APOGEE spectroscopic characterization. We find that 59% of stars with 1.5 day < P < 7 day lie 0.3 mag above the main sequence, compared with 28% of the full rotation sample. The fraction of stars in the same period range is 1.7 $\pm$ 0.1% of the total sample analyzed for rotation periods. Both the photometric binary fraction and the fraction of rapid rotators are consistent with a population of non-eclipsing short period binaries inferred from Kepler eclipsing binary data after correcting for inclination. This suggests that the rapid rotators are dominated by tidally-synchronized binaries rather than single-stars obeying traditional angular momentum evolution. We caution against interpreting rapid rotation in the Kepler field as a signature of youth. Following up this new sample of 217 candidate tidally-synchronized binaries will help further understand tidal processes in stars.
On the use of structure functions to study blazar variability: caveats and problems<|sep|>The extensive use of the structure function (SF) in the field of blazar variability suggests that characteristics time-scales are embedded in the light curves of these objects. We argue that for blazar variability studies, the SF results are sometimes erroneously interpreted leading to misconceptions about the actual source properties. Based on extensive simulations we caution that spurious breaks will appear in the SFs of almost all light curves, even though these light curves may contain no intrinsic characteristic time-scales. i.e. having a featureless underlying power-spectral-density (PSD). We show that the time-scales of the spurious SF-breaks depend mainly on the length of the artificial data set and also on the character of the variability i.e. the shape of the PSD. The SF is often invoked in the framework of shot-noise models to determine the temporal properties of individual shots. We caution that although the SF may be fitted to infer the shot parameters, the resultant shot-noise model is usually inconsistent with the observed PSD. As any model should fit the data in both the time and the frequency domain the shot-noise model, in these particular cases, can not be valid. Moreover, we show that the lack of statistical independence between adjacent SF points, in the standard SF formulation, means that it is not possible to perform robust statistical model fitting following the commonly used least-squares fitting methodology. The latter yields uncertainties in the fitting parameters (i.e. slopes, breaks) that are far too small with respect to their true statistical scatter. Finally, it is also commonly thought that SFs are immune to the sampling problems, such as data gaps, which affects the estimators of the PSDs. However we show that SFs are also troubled by gaps which can induce artefacts.
Opportunistic Routing in Quantum Networks<|sep|>Unlike classical routing algorithms, quantum routing algorithms make use of entangled states - a type of resources that have a limited lifetime and need to be regenerated after consumption. In a nutshell, quantum routing algorithms have to use these resources efficiently, while optimizing some objectives such as the total waiting time. Current routing algorithms tend to keep a routing request waiting until all of the resources on its path are available. In this paper, we introduce a new way of managing entanglement resources in an opportunistic fashion: a request can move forward along its path as soon as possible (even if some resources on its path are not ready). We show that this opportunistic approach is fundamentally better than conventional approaches. In particular, our results indicate that this new approach achieves a 30-50% improvement in the average total waiting time and average link waiting time compared with several state-of-the-art routing algorithms. As a by-product of this work, we develop a new simulator for quantum routing, which can be used to evaluate various design choices under different scenarios.
Integrability of auto-B\"acklund transformations,and solutions of a torqued ABS equation<|sep|>An auto-B\"acklund transformation for the quad equation $\mathrm{Q1}_1$ is considered as a discrete equation, called $\mathrm{H2}^a$, which is a so called torqued version of $\mathrm{H2}$. The equations $\mathrm{H2}^a$ and $\mathrm{Q1}_1$ compose a consistent cube, from which a auto-B\"acklund transformation and a Lax pair for $\mathrm{H2}^a$ are obtained. More generally it is shown that auto-B\"acklund transformations admit auto-B\"acklund transformations. Using the auto-B\"acklund transformation for $\mathrm{H2}^a$ we derive a seed solution and a one-soliton solution. From this solution it is seen that $\mathrm{H2}^a$ is a semi-autonomous lattice equation, as the spacing parameter $q$ depends on $m$ but it disappears from the plain wave factor.
Classifier Fusion Method to Recognize Handwritten Kannada Numerals<|sep|>Optical Character Recognition (OCR) is one of the important fields in image processing and pattern recognition domain. Handwritten character recognition has always been a challenging task. Only a little work can be traced towards the recognition of handwritten characters for the south Indian languages. Kannada is one such south Indian language which is also one of the official language of India. Accurate recognition of Kannada characters is a challenging task because of the high degree of similarity between the characters. Hence, good quality features are to be extracted and better classifiers are needed to improve the accuracy of the OCR for Kannada characters. This paper explores the effectiveness of feature extraction method like run length count (RLC) and directional chain code (DCC) for the recognition of handwritten Kannada numerals. In this paper, a classifier fusion method is implemented to improve the recognition rate. For the classifier fusion, we have considered K-nearest neighbour (KNN) and Linear classifier (LC). The novelty of this method is to achieve better accuracy with few features using classifier fusion approach. Proposed method achieves an average recognition rate of 96%.
Efficient multi-level hp-finite elements in arbitrary dimensions<|sep|>We present an efficient algorithmic framework for constructing multi-level hp-bases that uses a data-oriented approach that easily extends to any number of dimensions and provides a natural framework for performance-optimized implementations. We only operate on the bounding faces of finite elements without considering their lower-dimensional topological features and demonstrate the potential of the presented methods using a newly written open-source library. First, we analyze a Fichera corner and show that the framework does not increase runtime and memory consumption when compared against the classical p-version of the finite element method. Then, we compute a transient example with dynamic refinement and derefinement, where we also obtain the expected convergence rates and excellent performance in computing time and memory usage.
On Projected Stochastic Gradient Descent Algorithm with Weighted Averaging for Least Squares Regression<|sep|>The problem of least squares regression of a $d$-dimensional unknown parameter is considered. A stochastic gradient descent based algorithm with weighted iterate-averaging that uses a single pass over the data is studied and its convergence rate is analyzed. We first consider a bounded constraint set of the unknown parameter. Under some standard regularity assumptions, we provide an explicit $O(1/k)$ upper bound on the convergence rate, depending on the variance (due to the additive noise in the measurements) and the size of the constraint set. We show that the variance term dominates the error and decreases with rate $1/k$, while the term which is related to the size of the constraint set decreases with rate $\log k/k^2$. We then compare the asymptotic ratio $\rho$ between the convergence rate of the proposed scheme and the empirical risk minimizer (ERM) as the number of iterations approaches infinity. We show that $\rho\leq 4$ under some mild conditions for all $d\geq 1$. We further improve the upper bound by showing that $\rho\leq 4/3$ for the case of $d=1$ and unbounded parameter set. Simulation results demonstrate strong performance of the algorithm as compared to existing methods, and coincide with $\rho\leq 4/3$ even for large $d$ in practice.
Eigenvalue bounds for compressible stratified magneto-shear flows varying in two transverse directions<|sep|>Three eigenvalue bounds are derived for the instability of ideal compressible stratified magnetohydrodynamic shear flows in which the base velocity, density, and magnetic field vary in two directions. The first bound can be obtained by combining the Howard semi-circle theorem with the energy principle of the Lagrangian displacement. Remarkably, no special conditions are needed to use this bound, and for some cases, we can establish the stability of the flow. The second and third bounds come out from a generalisation of the Miles-Howard theory and have some similarity to the semi-ellipse theorem by Kochar & Jain (J. Fluid Mech., vol. 91, 1979, 489) and the bound found by Cally (Astrophys. Fluid Dyn., vol. 31,1983, 43), respectively. An important byproduct of this investigation is that the Miles-Howard stability condition holds only when there is no applied magnetic field and, in addition, the directions of the shear and the stratification are aligned everywhere.
Learning Visual Shape Control of Novel 3D Deformable Objects from Partial-View Point Clouds<|sep|>If robots could reliably manipulate the shape of 3D deformable objects, they could find applications in fields ranging from home care to warehouse fulfillment to surgical assistance. Analytic models of elastic, 3D deformable objects require numerous parameters to describe the potentially infinite degrees of freedom present in determining the object's shape. Previous attempts at performing 3D shape control rely on hand-crafted features to represent the object shape and require training of object-specific control models. We overcome these issues through the use of our novel DeformerNet neural network architecture, which operates on a partial-view point cloud of the object being manipulated and a point cloud of the goal shape to learn a low-dimensional representation of the object shape. This shape embedding enables the robot to learn to define a visual servo controller that provides Cartesian pose changes to the robot end-effector causing the object to deform towards its target shape. Crucially, we demonstrate both in simulation and on a physical robot that DeformerNet reliably generalizes to object shapes and material stiffness not seen during training and outperforms comparison methods for both the generic shape control and the surgical task of retraction.
Cosmological dynamics of f(R) models in dynamical system analysis<|sep|>In this work we try to understand the late time acceleration of the universe by assuming some modification in the geometry of the space and using dynamical system analysis. This technique allows to understand the behavior of the universe without analytically solving the field equations. We study the acceleration phase of the universe and stability properties of the critical points which could be compared with observational results. We consider an asymptotic behavior of two particular models $f(R) = R - \mu R_{c} \frac{(R/R_c)^{2n}}{(R/R_c)^{2n} + 1}$ and $f(R) = R - \mu R_{c} \left[ 1 - (1 + R^2/R_{c}^2)^{-n} \right]$ with $n,\mu, R_c >0$ for the study. As a first case we fix the value of $\mu$ and analyzed for all $n$. Later as second case, we fix the value of $n$ and calculation are done for all $\mu$. At the end all the calculations for the generalized case have been shown and results have been discussed in detail.
Network model of human language<|sep|>The phenomenon of human language is widely studied from various points of view. It is interesting not only for social scientists, antropologists or philosophers, but also for those, interesting in the network dynamics. In several recent papers word web, or language as a graph has been investigated. In this paper I revise recent studies of syntactical word web. I present a model of growing network in which such processes as node addition, edge rewiring and new link creation are taken into account. I argue, that this model is a satisfactory minimal model explaining measured data.
Gas- and dust evolution in protoplanetary disks<|sep|>Context. Current models of the size- and radial evolution of dust in protoplanetary disks generally oversimplify either the radial evolution of the disk (by focussing at one single radius or by using steady state disk models) or they assume particle growth to proceed monodispersely or without fragmentation. Further studies of protoplanetary disks - such as observations, disk chemistry and structure calculations or planet population synthesis models - depend on the distribution of dust as a function of grain size and radial position in the disk. Aims. We attempt to improve upon current models to be able to investigate how the initial conditions, the build-up phase, and the evolution of the protoplanetary disk influence growth and transport of dust. Methods. We introduce a new version of the model of Brauer et al. (2008) in which we now include the time-dependent viscous evolution of the gas disk, and in which more advanced input physics and numerical integration methods are implemented. Results. We show that grain properties, the gas pressure gradient, and the amount of turbulence are much more influencing the evolution of dust than the initial conditions or the build-up phase of the protoplanetary disk. We quantify which conditions or environments are favorable for growth beyond the meter size barrier. High gas surface densities or zonal flows may help to overcome the problem of radial drift, however already a small amount of turbulence poses a much stronger obstacle for grain growth.
Electromagnetic two-point functions and the Casimir effect in Friedmann-Robertson-Walker cosmologies<|sep|>We evaluate the two-point functions of the electromagnetic field in (D+1) -dimensional spatially flat Friedmann-Robertson-Walker universes with a power-law scale factor, assuming that the field is prepared in the Bunch-Davies vacuum state. The range of powers are specified in which the two-point functions are infrared convergent and the Bunch-Davies vacuum for the electromagnetic field is a physically realizable state. The two-point functions are applied for the investigation of the vacuum expectation values of the field squared and the energy-momentum tensor, induced by a single and two parallel conducting plates. Unlike to the case of conducting plates in the Minkowski bulk, in the problem under consideration the stresses along the directions parallel to the plates are not equal to the energy density. We show that, in addition to the diagonal components, the vacuum energy-momentum tensor has a nonzero off-diagonal component which describes energy flux along the direction normal to the plates. For a single plate this flux is directed from the plate. The Casimir forces are investigated in the geometry of two plates. At separations between the plates smaller than the curvature radius of the background spacetime, to the leading order, we recover the corresponding result in the Minkowski spacetime and in this case the forces are attractive. At larger separations, the influence of the curvature on the Casimir forces is essential with different asymptotic behavior for decelerated and accelerated expansions. In particular, for the latter case there is a range of powers of the expansion law in which the forces become repulsive at large separations between the plates.
Gender-From-Iris or Gender-From-Mascara?<|sep|>Predicting a person's gender based on the iris texture has been explored by several researchers. This paper considers several dimensions of experimental work on this problem, including person-disjoint train and test, and the effect of cosmetics on eyelash occlusion and imperfect segmentation. We also consider the use of multi-layer perceptron and convolutional neural networks as classifiers, comparing the use of data-driven and hand-crafted features. Our results suggest that the gender-from-iris problem is more difficult than has so far been appreciated. Estimating accuracy using a mean of N person-disjoint train and test partitions, and considering the effect of makeup - a combination of experimental conditions not present in any previous work - we find a much weaker ability to predict gender-from-iris texture than has been suggested in previous work.
Open Access, Intellectual Property, and How Biotechnology Becomes a New Software Science<|sep|>Innovation is slowing greatly in the pharmaceutical sector. It is considered here how part of the problem is due to overly limiting intellectual property relations in the sector. On the other hand, computing and software in particular are characterized by great richness of intellectual property frameworks. Could the intellectual property ecosystem of computing come to the aid of the biosciences and life sciences? We look at how the answer might well be yes, by looking at (i) the extent to which a drug mirrors a software program, and (ii) what is to be gleaned from trends in research publishing in the life and biosciences.
A Video Database of Human Faces under Near Infra-Red Illumination for Human Computer Interaction Aplications<|sep|>Human Computer Interaction (HCI) is an evolving area of research for coherent communication between computers and human beings. Some of the important applications of HCI as reported in literature are face detection, face pose estimation, face tracking and eye gaze estimation. Development of algorithms for these applications is an active field of research. However, availability of standard database to validate such algorithms is insufficient. This paper discusses the creation of such a database created under Near Infra-Red (NIR) illumination. NIR illumination has gained its popularity for night mode applications since prolonged exposure to Infra-Red (IR) lighting may lead to many health issues. The database contains NIR videos of 60 subjects in different head orientations and with different facial expressions, facial occlusions and illumination variation. This new database can be a very valuable resource for development and evaluation of algorithms on face detection, eye detection, head tracking, eye gaze tracking etc. in NIR lighting.
Towards Fully Interpretable Deep Neural Networks: Are We There Yet?<|sep|>Despite the remarkable performance, Deep Neural Networks (DNNs) behave as black-boxes hindering user trust in Artificial Intelligence (AI) systems. Research on opening black-box DNN can be broadly categorized into post-hoc methods and inherently interpretable DNNs. While many surveys have been conducted on post-hoc interpretation methods, little effort is devoted to inherently interpretable DNNs. This paper provides a review of existing methods to develop DNNs with intrinsic interpretability, with a focus on Convolutional Neural Networks (CNNs). The aim is to understand the current progress towards fully interpretable DNNs that can cater to different interpretation requirements. Finally, we identify gaps in current work and suggest potential research directions.
One-loop corrections to the Nielsen-Olesen vortex: collective oscillations<|sep|>We connect the translation modes of the instanton in the two-dimensional Abelian Higgs model with local translations of the vortex of the related model in (3+1) dimensions, the Nielsen-Olesen vortex. In this context these modes describe collective oscillations of the string. We construct the wave function of this mode and we derive, via a virial theorem, an effective action for these oscillations, which is consistent with the action constructed by Nielsen and Olesen using general arguments. We discuss some aspects of renormalization, based on a recent computation of one loop corrections to string tension of the vortex.
Pinning down the ram-pressure-induced halt of star formation in the Virgo cluster spiral galaxy NGC 4388. A joint inversion of spectroscopic and photometric data<|sep|>In a galaxy cluster, the evolution of spiral galaxies depends on their cluster environment. Ram pressure due to the rapid motion of a spiral galaxy within the hot intracluster medium removes the galaxy's interstellar medium from the outer disk. Once the gas has left the disk, star formation stops. The passive evolution of the stellar populations should be detectable in optical spectroscopy and multi-wavelength photometry. The goal of our study is to recover the stripping age of the Virgo spiral galaxy NGC 4388, i.e. the time elapsed since the halt of star formation in the outer galactic disk using a combined analysis of optical spectra and photometry. We performed VLT FORS2 long-slit spectroscopy of the inner star-forming and outer gas-free disk of NGC 4388. We developed a non-parametric inversion tool that allows us to reconstruct the star formation history of a galaxy from spectroscopy and photometry. The tool was tested on a series of mock data using Monte Carlo simulations. The results from the non-parametric inversion were refined by applying a parametric inversion method. The star formation history of the unperturbed galactic disk is flat. The non-parametric method yields a rapid decline of star formation < 200 Myr ago in the outer disk. The parametric method is not able to distinguish between an instantaneous and a long-lasting star formation truncation. The time since the star formation has dropped by a factor of two from its pre-stripping value is 190 +- 30 Myr. We are able to give a precise stripping age that is consistent with revised dynamical models.
Deflating the Aharonov-Bohm Effect<|sep|>I argue that the metaphysical import of the Aharonov-Bohm effect has been overstated: correctly understood, it does not require either rejection of gauge invariance or any novel form of nonlocality. The conclusion that it does require one or the other follows from a failure to keep track, in the analysis, of the complex scalar field to which the magnetic vector potential is coupled. Once this is recognised, the way is clear to a local account of the ontology of electrodynamics (or at least, to an account no more nonlocal than quantum theory in general requires); I sketch a possible such account.
Communication on structure of biological networks<|sep|>Networks are widely used to represent interaction pattern among the components in complex systems. Structures of real networks from differ- ent domains may vary quite significantly. Since there is an interplay be- tween network architecture and dynamics, structure plays an important role in communication and information spreading on a network. Here we investigate the underlying undirected topology of different biological networks which support faster spreading of information and are better in communication. We analyze the good expansion property by using the spectral gap and communicability between nodes. Different epidemic models are also used to study the transmission of information in terms of disease spreading through individuals (nodes) in those networks. More- over, we explore the structural conformation and properties which may be responsible for better communication. Among all biological networks studied here, the undirected structure of neuronal networks not only pos- sesses the small-world property but the same is expressed remarkably to a higher degree than any randomly generated network which possesses the same degree sequence. A relatively high percentage of nodes, in neuronal networks, form a higher core in their structure. Our study shows that the underlying undirected topology in neuronal networks is significantly qualitatively different than the same from other biological networks and that they may have evolved in such a way that they inherit a (undirected) structure which is excellent and robust in communication.
Secure Quantum Communication with Orthogonal States<|sep|>In majority of protocols of secure quantum communication (such as, BB84, B92, etc.), the unconditional security of the protocols are obtained by using conjugate coding (two or more mutually unbiased bases). Initially all the conjugate-coding-based protocols of secure quantum communication were restricted to quantum key distribution (QKD), but later on they were extended to other cryptographic tasks (such as, secure direct quantum communication and quantum key agreement). In contrast to the conjugate-coding-based protocols, a few completely orthogonal-state-based protocols of unconditionally secure QKD (such as, Goldenberg-Vaidman (GV) and N09) were also proposed. However, till the recent past orthogonal-state-based protocols were only a theoretical concept and were limited to QKD. Only recently, orthogonal-state-based protocols of QKD are experimentally realized and extended to cryptographic tasks beyond QKD. This paper aims to briefly review the orthogonal-state-based protocols of secure quantum communication that are recently introduced by our group and other researchers.
First-principles analysis of cross-resonance gate operation<|sep|>We present a comprehensive theoretical study of the cross-resonance gate operation covering estimates for gate parameters and gate error as well as analyzing spectator qubits and multi-qubit frequency collisions. We start by revisiting the derivation of effective Hamiltonian models following Magesan et al. (arXiv:1804.04073). Transmon qubits are commonly modeled as a weakly anharmonic Kerr oscillator. Kerr theory only accounts for qubit frequency renormalization, while adopting number states as the eigenstates of the bare qubit Hamiltonian. Starting from the Josephson nonlinearity and by accounting for the eigenstates renormalization, due to counter-rotating terms, we derive a new starting model for the cross-resonance gate with modified qubit-qubit interaction and drive matrix elements. Employing time-dependent Schrieffer-Wolff perturbation theory, we derive an effective Hamiltonian for the cross-resonance gate with estimates for the gate parameters calculated up to the fourth order in drive amplitude. The new model with renormalized eigenstates lead to 10-15 percent relative correction of the effective gate parameters compared to Kerr theory. We find that gate operation is strongly dependent on the ratio of qubit-qubit detuning and anharmonicity. In particular, we characterize five distinct regions of operation, and propose candidate parameter choices for achieving high gate speed and low coherent gate error when the cross-resonance tone is equipped with an echo pulse sequence. Furthermore, we generalize our method to include a third spectator qubit and characterize possible detrimental multi-qubit frequency collisions.
A New Proof of Pappus's Theorem<|sep|>Any stretching of Ringel's non-Pappus pseudoline arrangement when projected into the Euclidean plane, implicitly contains a particular arrangement of nine triangles. This arrangement has a complex constraint involving the sines of its angles. These constraints cannot be satisfied by any projection of the initial arrangement. This is sufficient to prove Pappus's theorem. The derivation of the constraint is via systems of inequalities arising from the polar coordinates of the lines. These systems are linear in r for any given theta, and their solubility can be analysed in terms of the signs of determinants. The evaluation of the determinants is via a normal form for sums of products of sines, giving a powerful system of trigonometric identities. The particular result is generalized to arrangements derived from three edge connected totally cyclic directed graphs, conjectured to be sufficient for a complete analysis of angle constraining arrangements of lines, and thus a full response to Ringel's slope conjecture. These methods are generally applicable to the realizability problem for rank 3 oriented matroids.
PyXtal: a Python Library for Crystal Structure Generation and Symmetry Analysis<|sep|>We present PyXtal, a new package based on the Python programming language, used to generate structures with specific symmetry and chemical compositions for both atomic and molecular systems. This soft ware provides support for various systems described by point, rod, layer, and space group symmetries. With only the inputs of chemical composition and symmetry group information, PyXtal can automatically find a suitable combination of Wyckoff positions with a step-wise merging scheme. Further, when the molecular geometry is given, PyXtal can generate different dimensional organic crystals with molecules occupying both general and special Wyckoff positions. Optionally, PyXtal also accepts user-defined parameters (e.g., cell parameters, minimum distances and Wyckoff positions). In general, PyXtal serves three purposes: (1) to generate custom structures, (2) to modulate the structure by symmetry relations, (3) to interface the existing structure prediction codes that require the generation of random symmetric structures. In addition, we provide several utilities that facilitate the analysis of structures, including symmetry analysis, geometry optimization, and simulations of powder X-ray diffraction (XRD). Full documentation of PyXtal is available at \url{https://pyxtal.readthedocs.io}.
Evaluating kernels on Xeon Phi to accelerate Gysela application<|sep|>This work describes the challenges presented by porting parts ofthe Gysela code to the Intel Xeon Phi coprocessor, as well as techniques used for optimization, vectorization and tuning that can be applied to other applications. We evaluate the performance of somegeneric micro-benchmark on Phi versus Intel Sandy Bridge. Several interpolation kernels useful for the Gysela application are analyzed and the performance are shown. Some memory-bound and compute-bound kernels are accelerated by a factor 2 on the Phi device compared to Sandy architecture. Nevertheless, it is hard, if not impossible, to reach a large fraction of the peek performance on the Phi device,especially for real-life applications as Gysela. A collateral benefit of this optimization and tuning work is that the execution time of Gysela (using 4D advections) has decreased on a standard architecture such as Intel Sandy Bridge.
Named Entity Detection and Injection for Direct Speech Translation<|sep|>In a sentence, certain words are critical for its semantic. Among them, named entities (NEs) are notoriously challenging for neural models. Despite their importance, their accurate handling has been neglected in speech-to-text (S2T) translation research, and recent work has shown that S2T models perform poorly for locations and notably person names, whose spelling is challenging unless known in advance. In this work, we explore how to leverage dictionaries of NEs known to likely appear in a given context to improve S2T model outputs. Our experiments show that we can reliably detect NEs likely present in an utterance starting from S2T encoder outputs. Indeed, we demonstrate that the current detection quality is sufficient to improve NE accuracy in the translation with a 31% reduction in person name errors.
Electron tunneling into a quantum wire in the Fabry-Perot regime<|sep|>We study a gated quantum wire contacted to source and drain electrodes in the Fabry-Perot regime. The wire is also coupled to a third terminal (tip), and we allow for an asymmetry of the tip tunneling amplitudes of right and left moving electrons. We analyze configurations where the tip acts as an electron injector or as a voltage-probe, and show that the transport properties of this three-terminal set-up exhibit very rich physical behavior. For a non-interacting wire we find that a tip in the voltage-probe configuration affects the source-drain transport in different ways, namely by suppressing the conductance, by modulating the Fabry-Perot oscillations, and by reducing their visibility. The combined effect of electron electron interaction and finite length of the wire, accounted for by the inhomogeneous Luttinger liquid model, leads to significantly modified predictions as compared to models based on infinite wires. We show that when the tip injects electrons asymmetrically the charge fractionalization induced by interaction cannot be inferred from the asymmetry of the currents flowing in source and drain. Nevertheless interaction effects are visible as oscillations in the non-linear tip-source and tip-drain conductances. Important differences with respect to a two-terminal set-up emerge, suggesting new strategies for the experimental investigation of Luttinger liquid behavior.
Stopping GAN Violence: Generative Unadversarial Networks<|sep|>While the costs of human violence have attracted a great deal of attention from the research community, the effects of the network-on-network (NoN) violence popularised by Generative Adversarial Networks have yet to be addressed. In this work, we quantify the financial, social, spiritual, cultural, grammatical and dermatological impact of this aggression and address the issue by proposing a more peaceful approach which we term Generative Unadversarial Networks (GUNs). Under this framework, we simultaneously train two models: a generator G that does its best to capture whichever data distribution it feels it can manage, and a motivator M that helps G to achieve its dream. Fighting is strictly verboten and both models evolve by learning to respect their differences. The framework is both theoretically and electrically grounded in game theory, and can be viewed as a winner-shares-all two-player game in which both players work as a team to achieve the best score. Experiments show that by working in harmony, the proposed model is able to claim both the moral and log-likelihood high ground. Our work builds on a rich history of carefully argued position-papers, published as anonymous YouTube comments, which prove that the optimal solution to NoN violence is more GUNs.
Heating of the solar chromosphere through current dissipation<|sep|>The solar chromosphere is heated to temperatures higher than predicted by radiative equilibrium. This excess heating is greater in active regions where the magnetic field is stronger. We aim to investigate the magnetic topology associated with an area of enhanced millimeter (mm) brightness temperatures in a solar active region mapped by the Atacama Large Millimeter/submillimeter Array (ALMA) using spectropolarimetric co-observations with the 1-m Swedish Solar Telescope (SST). We used Milne-Eddington inversions, nonlocal thermodynamic equilibrium (non-LTE) inversions, and a magnetohydrostatic extrapolation to obtain constraints on the three-dimensional stratification of temperature, magnetic field, and radiative energy losses. We compared the observations to a snapshot of a magnetohydrodynamics simulation and investigate the formation of the thermal continuum at 3 mm using contribution functions. We find enhanced heating rates in the upper chromosphere of up to $\sim 5\rm\,kW\,m^{-2}$, where small-scale emerging loops interact with the overlying magnetic canopy leading to current sheets as shown by the magnetic field extrapolation. Our estimates are about a factor of two higher than canonical values, but they are limited by the ALMA spatial resolution ($\sim 1.2^{\prime\prime}$). Band 3 brightness temperatures reach about $\sim10^{4}\,$K in the region, and the transverse magnetic field strength inferred from the non-LTE inversions is on the order of $\sim 500\,$G in the chromosphere. We are able to quantitatively reproduce many of the observed features, including the integrated radiative losses in our numerical simulation. We conclude that the heating is caused by dissipation in current sheets. However, the simulation shows a complex stratification in the flux emergence region where distinct layers may contribute significantly to the emission in the mm continuum.
Artificial intelligence for detection and quantification of rust and leaf miner in coffee crop<|sep|>Pest and disease control plays a key role in agriculture since the damage caused by these agents are responsible for a huge economic loss every year. Based on this assumption, we create an algorithm capable of detecting rust (Hemileia vastatrix) and leaf miner (Leucoptera coffeella) in coffee leaves (Coffea arabica) and quantify disease severity using a mobile application as a high-level interface for the model inferences. We used different convolutional neural network architectures to create the object detector, besides the OpenCV library, k-means, and three treatments: the RGB and value to quantification, and the AFSoft software, in addition to the analysis of variance, where we compare the three methods. The results show an average precision of 81,5% in the detection and that there was no significant statistical difference between treatments to quantify the severity of coffee leaves, proposing a computationally less costly method. The application, together with the trained model, can detect the pest and disease over different image conditions and infection stages and also estimate the disease infection stage.
First Application of Pulse-Shape Analysis to Silicon Micro-Strip Detectors<|sep|>The method of pulse-shape analysis (PSA) for particle identification (PID) was applied to a double-sided silicon strip detector (DSSD) with a strip pitch of 300 \{mu}m. We present the results of test measurements with particles from the reactions of a 70 MeV 12C beam impinging on a mylar target. Good separation between protons and alpha particles down to 3 MeV has been obtained when excluding the interstrip events of the DSSD from the analysis.
The kinetic temperature of a molecular cloud at redshift 0.9: Ammonia in the gravitational lens PKS1830-211<|sep|>Using the Green Bank Telescope (GBT), we have detected the (J,K) = (1,1) to (10,10) ammonia inversion lines, up to 1030 K above the ground state, in a face-on spiral galaxy viewed against the radio continuum of the lensed background source PKS 1830-211. The ammonia absorption lines, seen at redshift 0.886, appear to be optically thin with absolute peak flux densities up to 2.5 percent of the total continuum of the background source. Measured intensities are consistent with a kinetic temperature of 80 K for 80-90 percent of the ammonia column. The remaining gas is warmer, reaching at least 600 K. Column density and fractional abundance are of order (5-10) x 10^14 cm^-2 and (1.5-3.0) x 10^-8. Similarities with a hot ammonia absorption component observed toward the Sgr B2 region close to the Galactic center may suggest that the Sgr B2 component also originates from warm diffuse low-density molecular gas. The warm ammonia column observed toward PKS 1830-211 is unique in the sense that it originates in a spiral arm.
GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer<|sep|>Group activity recognition is a crucial yet challenging problem, whose core lies in fully exploring spatial-temporal interactions among individuals and generating reasonable group representations. However, previous methods either model spatial and temporal information separately, or directly aggregate individual features to form group features. To address these issues, we propose a novel group activity recognition network termed GroupFormer. It captures spatial-temporal contextual information jointly to augment the individual and group representations effectively with a clustered spatial-temporal transformer. Specifically, our GroupFormer has three appealing advantages: (1) A tailor-modified Transformer, Clustered Spatial-Temporal Transformer, is proposed to enhance the individual representation and group representation. (2) It models the spatial and temporal dependencies integrally and utilizes decoders to build the bridge between the spatial and temporal information. (3) A clustered attention mechanism is utilized to dynamically divide individuals into multiple clusters for better learning activity-aware semantic representations. Moreover, experimental results show that the proposed framework outperforms state-of-the-art methods on the Volleyball dataset and Collective Activity dataset. Code is available at https://github.com/xueyee/GroupFormer.
A Categorial Semantic Representation of Quantum Event Structures<|sep|>The overwhelming majority of the attempts in exploring the problems related to quantum logical structures and their interpretation have been based on an underlying set-theoretic syntactic language. We propose a transition in the involved syntactic language to tackle these problems from the set-theoretic to the category-theoretic mode, together with a study of the consequent semantic transition in the logical interpretation of quantum event structures. In the present work, this is realized by representing categorically the global structure of a quantum algebra of events (or propositions) in terms of sheaves of local Boolean frames forming Boolean localization functors. The category of sheaves is a topos providing the possibility of applying the powerful logical classification methodology of topos theory with reference to the quantum world. In particular, we show that the topos-theoretic representation scheme of quantum event algebras by means of Boolean localization functors incorporates an object of truth values, which constitutes the appropriate tool for the definition of quantum truth-value assignments to propositions describing the behavior of quantum systems. Effectively, this scheme induces a revised realist account of truth in the quantum domain of discourse. We also include an appendix, where we compare our topos-theoretic representation scheme of quantum event algebras with other categorial and topos-theoretic approaches.
Cosmological model selection from standard siren detections by third-generation gravitational wave obervatories<|sep|>The multi-messenger observation of GW170817 enabled the first historic measurement of the Hubble constant via a standard siren, so-called in analogy to standard candles that enabled the measurement of the luminosity distance versus redshift relationship at small redshift. In the next decades, third-generation observatories are expected to detect hundreds to thousand of gravitational wave events from compact binary coalescences with potentially a joint electromagnetic counterpart. In the present work, we show how future standard siren detections can be used within the framework of Bayesian model selection to discriminate between cosmological models differing by the parameterization of the late-time acceleration. In particular, we found quantitative conditions for the standard LCDM model to be favored with respect to other models with varying dark energy content, by reducing the uncertainty in the gravitational determination of the luminosity distance with respect to current expectations.
Achieving Heisenberg Scaling on Measurement of A Three-Qubit System via Quantum Error Correction<|sep|>In many-body quantum systems, the quantum Fisher information an observer can obtain is susceptible to decoherence. Consequently, quantum enhanced metrology, such as Heisenberg scaling, cannot usually be achieved. We show, via two distinct methods, that by applying periodic quantum error corrections, we can achieve the Heisenberg scaling for an extended period of time on a three-qubit Tavis-Cumming Model, where three two-level atoms interact with a single cavity mode, under certain approximations. The generalization to arbitrary number of atoms case is also discussed.
KdV equation beyond standard assumptions on initial data<|sep|>We show that the Cauchy problem for the KdV equation can be solved by the inverse scattering transform (IST) for any initial data bounded from below, decaying sufficiently rapidly at plus infinity, but unrestricted otherwise. Thus our approach doesn't require any boundary condition at minus infinity.
A method for unbounded verification of privacy-type properties<|sep|>In this paper, we consider the problem of verifying anonymity and unlinkability in the symbolic model, where protocols are represented as processes in a variant of the applied pi calculus, notably used in the ProVerif tool. Existing tools and techniques do not allow to verify directly these properties, expressed as behavioral equivalences. We propose a different approach: we design two conditions on protocols which are sufficient to ensure anonymity and unlinkability, and which can then be effectively checked automatically using ProVerif. Our two conditions correspond to two broad classes of attacks on unlinkability, i.e. data and control-flow leaks. This theoretical result is general enough that it applies to a wide class of protocols based on a variety of cryptographic primitives. In particular, using our tool, UKano, we provide the first formal security proofs of protocols such as BAC and PACE (e-passport), Hash-Lock (RFID authentication), etc. Our work has also lead to the discovery of new attacks, including one on the LAK protocol (RFID authentication) which was previously claimed to be unlinkable (in a weak sense).
The PyCBC search for gravitational waves from compact binary coalescence<|sep|>We describe the PyCBC search for gravitational waves from compact-object binary coalescences in advanced gravitational-wave detector data. The search was used in the first Advanced LIGO observing run and unambiguously identified two black hole binary mergers, GW150914 and GW151226. At its core, the PyCBC search performs a matched-filter search for binary merger signals using a bank of gravitational-wave template waveforms. We provide a complete description of the search pipeline including the steps used to mitigate the effects of noise transients in the data, identify candidate events and measure their statistical significance. The analysis is able to measure false-alarm rates as low as one per million years, required for confident detection of signals. Using data from initial LIGO's sixth science run, we show that the new analysis reduces the background noise in the search, giving a 30% increase in sensitive volume for binary neutron star systems over previous searches.
Variational Equalities of Directed Information and Applications<|sep|>In this paper we introduce two variational equalities of directed information, which are analogous to those of mutual information employed in the Blahut-Arimoto Algorithm (BAA). Subsequently, we introduce nonanticipative Rate Distortion Function (RDF) ${R}^{na}_{0,n}(D)$ defined via directed information introduced in [1], and we establish its equivalence to Gorbunov-Pinsker's nonanticipatory $\epsilon$-entropy $R^{\varepsilon}_{0,n}(D)$. By invoking certain results we first establish existence of the infimizing reproduction distribution for ${R}^{na}_{0,n}(D)$, and then we give its implicit form for the stationary case. Finally, we utilize one of the variational equalities and the closed form expression of the optimal reproduction distribution to provide an algorithm for the computation of ${R}^{na}_{0,n}(D)$.
Covariant asymmetric wave packet for a field-theoretical description of neutrino oscillations<|sep|>We consider a class of models for the relativistic covariant wave packets which can be used as asymptotically free in and out states in the quantum field theoretical formalisms for description of the neutrino flavor oscillation phenomenon. We demonstrate that the new "asymmetric" wave packet (AWP) is an appropriate alternative for the more convenient "symmetric" wave packets, like the so-called relativistic Gaussian packet (RGP) widely used in the QFT-based approaches to neutrino oscillations. We show that RGP is not a particular case of AWP, although many properties of these models are almost identical in the quasistable regime. We discuss some features of AWP distinguishing it from RGP.
GROOT: Corrective Reward Optimization for Generative Sequential Labeling<|sep|>Sequential labeling is a fundamental NLP task, forming the backbone of many applications. Supervised learning of Seq2Seq models (like T5) has shown great success on these problems. However there remains a significant disconnect between the training objectives of these models vs the metrics and desiderata we care about in practical applications. For example, a practical sequence tagging application may want to optimize for a certain precision-recall trade-off (of the top-k predictions) which is quite different from the standard objective of maximizing the likelihood of the gold labeled sequence. Thus to bridge this gap, we propose GROOT -- a simple yet effective framework for Generative Reward Optimization Of Text sequences. GROOT works by training a generative sequential labeling model to match the decoder output distribution with that of the (black-box) reward function. Using an iterative training regime, we first generate prediction candidates, then correct errors in them, and finally contrast those candidates (based on their reward values). As demonstrated via extensive experiments on four public benchmarks, GROOT significantly improves all reward metrics. Furthermore, GROOT also leads to improvements of the overall decoder distribution as evidenced by the quality gains of the top-$k$ candidates.
The VLT-FLAMES survey of massive stars: Evolution of surface N abundances and effective temperature scales in the Galaxy and Magellanic Clouds<|sep|>We present an analysis of high resolution VLT-FLAMES spectra of 61 B-type stars with relatively narrow-lined spectra located in 4 fields centered on the Milky Way clusters; NGC3293 & NGC4755 and the Large and Small Magellanic cloud clusters; NGC2004 and NGC330. For each object a quantitative analysis was carried out using the non-LTE model atmosphere code TLUSTY; resulting in the determination of their atmospheric parameters and photospheric abundances of the dominant metal species (C, N, O, Mg, Si, Fe). The results are discussed in relation to our earlier work on 3 younger clusters in these galaxies; NGC6611, N11 and NGC346 paying particular attention to the nitrogen abundances which are an important probe of the role of rotation in the evolution of stars. This work along with that of the younger clusters provides a consistent dataset of abundances and atmospheric parameters for over 100 B-type stars in the three galaxies. We provide effective temperature scales for B-type dwarfs in all three galaxies and for giants and supergiants in the SMC and LMC. In each galaxy a dependence on luminosity is found between the three classes with the unevolved dwarf objects having significantly higher effective temperatures. A metallicity dependence is present between the SMC and Galactic dwarf objects, and whilst the LMC stars are only slightly cooler than the SMC stars, they are significantly hotter than their Galactic counterparts.
Fast redshift clustering with the Baire (ultra) metric<|sep|>The Baire metric induces an ultrametric on a dataset and is of linear computational complexity, contrasted with the standard quadratic time agglomerative hierarchical clustering algorithm. We apply the Baire distance to spectrometric and photometric redshifts from the Sloan Digital Sky Survey using, in this work, about half a million astronomical objects. We want to know how well the (more cos\ tly to determine) spectrometric redshifts can predict the (more easily obtained) photometric redshifts, i.e. we seek to regress the spectrometric on the photometric redshifts, and we develop a clusterwise nearest neighbor regression procedure for this.
Improv Chat: Second Response Generation for Chatbot<|sep|>Existing research on response generation for chatbot focuses on \textbf{First Response Generation} which aims to teach the chatbot to say the first response (e.g. a sentence) appropriate to the conversation context (e.g. the user's query). In this paper, we introduce a new task \textbf{Second Response Generation}, termed as Improv chat, which aims to teach the chatbot to say the second response after saying the first response with respect the conversation context, so as to lighten the burden on the user to keep the conversation going. Specifically, we propose a general learning based framework and develop a retrieval based system which can generate the second responses with the users' query and the chatbot's first response as input. We present the approach to building the conversation corpus for Improv chat from public forums and social networks, as well as the neural networks based models for response matching and ranking. We include the preliminary experiments and results in this paper. This work could be further advanced with better deep matching models for retrieval base systems or generative models for generation based systems as well as extensive evaluations in real-life applications.
Crystal-electric-field excitations in a quantum-spin-liquid candidate NaErS$_2$<|sep|>The delafossite family of compounds with a triangular lattice of rare earth ions has been recently proposed as a candidate host for quantum spin liquid (QSL) states. To realize QSLs, the crystal-electric-field (CEF) ground state of the rare earth ions should be composed of a doublet that allows sizable quantum tunneling, but till now the knowledge on CEF states in the delafossite compounds is still limited. Here we employ inelastic neutron scattering (INS) to study the CEF transitions in a powder sample of the delafossite NaErS$_2$, where the large total angular momentum $J = 15/2$ of the Er$^{3+} $ ions and the resulting plethora of CEF transitions enable an accurate fit of the CEF parameters. Our study reveals nearly isotropic spins with large $J_z = \pm 1/2$ components for the Er$^{3+}$ CEF ground states, which might facilitate the development of a QSL state. The scaling of the obtained CEF Hamiltonian to different rare earth ions suggests that sizable $J_z = \pm 1/2$ components are generally present in the CEF ground states, supporting the ternary sulfide delafossites as potential QSL hosts.
Considering Multiple Uncertainties in Stochastic Security-Constrained Unit Commitment Using Point Estimation Method<|sep|>Security-Constrained Unit Commitment (SCUC) is one of the most significant problems in secure and optimal operation of modern electricity markets. New sources of uncertainties such as wind speed volatility and price-sensitive loads impose additional challenges to this large-scale problem. This paper proposes a new Stochastic SCUC using point estimation method to model the power system uncertainties more efficiently. Conventional scenario-based Stochastic SCUC approaches consider the Mont Carlo method; which presents additional computational burdens to this large-scale problem. In this paper we use point estimation instead of scenario generating to detract computational burdens of the problem. The proposed approach is implemented on a six-bus system and on a modified IEEE 118-bus system with 94 uncertain variables. The efficacy of proposed algorithm is confirmed, especially in the last case with notable reduction in computational burden without considerable loss of precision.
Generalized Nonaveraging Integer Sequences<|sep|>Let the sequence S_m of nonnegative integers be generated by the following conditions: Set the first term a_0 = 0, and for all k \geq 0, let a_k+1 be the least integer greater than a_k such that no element of {a_0,...,a_k+1} is the average of m - 1 distinct other elements. Szekeres gave a closed-form description of S_3 in 1936, and Layman provided a similar description for S_4 in 1999. We first find closed forms for some similar greedy sequences that avoid averages in terms not all the same. Then, we extend the closed-form description of S_m from the known cases when m = 3 and m = 4 to any integer m \geq 3. With the help of a computer, we also generalize this to sequences that avoid solutions to specific weighted averages in distinct terms. Finally, from the closed forms of these sequences, we find bounds for their growth rates.
Thermodynamic uncertainty relation for Langevin dynamics by scaling time<|sep|>The thermodynamic uncertainty relation (TUR) quantifies a relationship between current fluctuations and dissipation in out-of-equilibrium overdamped Langevin dynamics, making it a natural counterpart of the fluctuation-dissipation theorem in equilibrium statistical mechanics. For underdamped Langevin dynamics, the situation is known to be more complicated, with dynamical activity also playing a role in limiting the magnitude of current fluctuations. Progress on those underdamped TUR-like bounds has largely come from applications of the information-theoretic Cram\'er-Rao inequality. Here, we present an alternative perspective by employing large deviation theory. The approach offers a general, unified treatment of TUR-like bounds for both overdamped and underdamped Langevin dynamics built upon current fluctuations achieved by scaling time. The bounds we derive following this approach are similar to known results but with differences we discuss and rationalize.
An Annihilating Filter-Based DOA Estimation for Uniform Linear Array<|sep|>In this paper, we propose a new method to design an annihilating filter (AF) for direction-of-arrival (DOA) estimation of multiple snapshots within an uniform linear array. To evaluate the proposed method, we firstly design a DOA estimation using multiple signal classification (MUSIC) algorithm, referred to as the MUSIC baseline. We then compare the proposed method with the MUSIC baseline in two environmental noise conditions: Only white noise, or both white noise and diffusion. The experimental results highlight two main contributions; the first is to modify conventional MUSIC algorithm for adapting different noise conditions, and the second is to propose an AF-based method that shows competitive accuracy of arrival angles detected and low complexity compared with the MUSIC baseline.
Model-based Lookahead Reinforcement Learning<|sep|>Model-based Reinforcement Learning (MBRL) allows data-efficient learning which is required in real world applications such as robotics. However, despite the impressive data-efficiency, MBRL does not achieve the final performance of state-of-the-art Model-free Reinforcement Learning (MFRL) methods. We leverage the strengths of both realms and propose an approach that obtains high performance with a small amount of data. In particular, we combine MFRL and Model Predictive Control (MPC). While MFRL's strength in exploration allows us to train a better forward dynamics model for MPC, MPC improves the performance of the MFRL policy by sampling-based planning. The experimental results in standard continuous control benchmarks show that our approach can achieve MFRL`s level of performance while being as data-efficient as MBRL.
Relational Approach to Knowledge Engineering for POMDP-based Assistance Systems as a Translation of a Psychological Model<|sep|>Assistive systems for persons with cognitive disabilities (e.g. dementia) are difficult to build due to the wide range of different approaches people can take to accomplishing the same task, and the significant uncertainties that arise from both the unpredictability of client's behaviours and from noise in sensor readings. Partially observable Markov decision process (POMDP) models have been used successfully as the reasoning engine behind such assistive systems for small multi-step tasks such as hand washing. POMDP models are a powerful, yet flexible framework for modelling assistance that can deal with uncertainty and utility. Unfortunately, POMDPs usually require a very labour intensive, manual procedure for their definition and construction. Our previous work has described a knowledge driven method for automatically generating POMDP activity recognition and context sensitive prompting systems for complex tasks. We call the resulting POMDP a SNAP (SyNdetic Assistance Process). The spreadsheet-like result of the analysis does not correspond to the POMDP model directly and the translation to a formal POMDP representation is required. To date, this translation had to be performed manually by a trained POMDP expert. In this paper, we formalise and automate this translation process using a probabilistic relational model (PRM) encoded in a relational database. We demonstrate the method by eliciting three assistance tasks from non-experts. We validate the resulting POMDP models using case-based simulations to show that they are reasonable for the domains. We also show a complete case study of a designer specifying one database, including an evaluation in a real-life experiment with a human actor.
Third order superintegrable systems separating in polar coordinates<|sep|>A complete classification is presented of quantum and classical superintegrable systems in $E_2$ that allow the separation of variables in polar coordinates and admit an additional integral of motion of order three in the momentum. New quantum superintegrable systems are discovered for which the potential is expressed in terms of the sixth Painlev\'e transcendent or in terms of the Weierstrass elliptic function.
Computing the electric field from Extensive Air Showers using a realistic description of the atmosphere<|sep|>The composition of ultra-high energy cosmic rays is still poorly known and constitutes a very important topic in the field of high-energy astrophysics. Detection of ultra-high energy cosmic rays is carried out via the extensive air showers they create after interacting with the atmosphere constituents. The secondary electrons and positrons within the showers emit a detectable electric field in the kHz-GHz range. It is possible to use this radio signal for the estimation of the atmospheric depth of maximal development of the showers \xmax, with a good accuracy and a duty cycle close to $100\%$. This value of \xmax\ is strongly correlated to the nature of the primary cosmic ray that initiated the shower. We show in this paper the importance of using a realistic atmospheric model in order to correct for systematic errors that can prevent a correct and unbiased estimation of~\xmax.
Universal scaling of the logarithmic negativity in massive quantum field theory<|sep|>We consider the logarithmic negativity, a measure of bipartite entanglement, in a general unitary 1+1-dimensional massive quantum field theory, not necessarily integrable. We compute the negativity between a finite region of length $r$ and an adjacent semi-infinite region, and that between two semi-infinite regions separated by a distance $r$. We show that the former saturates to a finite value, and that the latter tends to zero, as $r\rightarrow\infty$. We show that in both cases, the leading corrections are exponential decays in $r$ (described by modified Bessel functions) that are solely controlled by the mass spectrum of the model, independently of its scattering matrix. This implies that, like the entanglement entropy, the logarithmic negativity displays a very high level of universality, allowing one to extract information about the mass spectrum. Further, a study of sub-leading terms shows that, unlike the entanglement entropy, a large-$r$ analysis of the negativity allows for the detection of bound states.
Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks<|sep|>The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP). Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall training procedure; in particular, we find that data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for researchers aiming to explore the utility of contrastive loss in NLP.
Multi-agent Hierarchical Reinforcement Learning with Dynamic Termination<|sep|>In a multi-agent system, an agent's optimal policy will typically depend on the policies chosen by others. Therefore, a key issue in multi-agent systems research is that of predicting the behaviours of others, and responding promptly to changes in such behaviours. One obvious possibility is for each agent to broadcast their current intention, for example, the currently executed option in a hierarchical reinforcement learning framework. However, this approach results in inflexibility of agents if options have an extended duration and are dynamic. While adjusting the executed option at each step improves flexibility from a single-agent perspective, frequent changes in options can induce inconsistency between an agent's actual behaviour and its broadcast intention. In order to balance flexibility and predictability, we propose a dynamic termination Bellman equation that allows the agents to flexibly terminate their options. We evaluate our model empirically on a set of multi-agent pursuit and taxi tasks, and show that our agents learn to adapt flexibly across scenarios that require different termination behaviours.
A Consistent Spectral Model of WR 136 and its Associated Bubble NGC 6888<|sep|>We analyse whether a stellar atmosphere model computed with the code CMFGEN provides an optimal description of the stellar observations of WR 136 and simultaneously reproduces the nebular observations of NGC 6888, such as the ionization degree, which is modelled with the pyCloudy code. All the observational material available (far and near UV and optical spectra) were used to constrain such models. We found that even when the stellar luminosity and the mass-loss rate were well constrained, the stellar temperature T_* at tau = 20, can be in a range between 70 000 and 110 000 K. When using the nebula as an additional restriction we found that the stellar models with T_* \sim 70 000 K represent the best solution for both, the star and the nebula. Results from the photoionization model show that if we consider a chemically homogeneous nebula, the observed N^+/O^+ ratios found in different nebular zones can be reproduced, therefore it is not necessary to assume a chemical inhomogeneous nebula. Our work shows the importance of calculating coherent models including stellar and nebular constraints. This allowed us to determine, in a consistent way, all the physical parameters of both the star and its associated nebula. The chemical abundances derived are 12 + log(N/H) = 9.95, 12 + log(C/H) = 7.84 and 12 + log(O/H) = 8.76 for the star and 12 + log(N/H) = 8.40, 12 + log(C/H) = 8.86 and 12 + log(O/H) = 8.20. Thus the star and the nebula are largely N- and C- enriched and O-depleted.
Collisionless Hydrodynamics of Doped Graphene in a Magnetic Field<|sep|>The electrodynamics of a two-dimensional gas of massless fermions in graphene is studied by a collisionless hydrodynamic approach. A low-energy dispersion relation for the collective modes (plasmons) is derived both in the absence and in the presence of a perpendicular magnetic field. The results for graphene are compared to those for a standard two-dimensional gas of massive electrons. We further compare the results within the classical hydrodynamic approach to the full quantum mechanical calculation in the random phase approximation. The low-energy dispersion relation is shown to be a good approximation at small wave vectors. The limitations of this approach at higher order is also discussed.
Boosting to BMS<|sep|>Bondi-Metzner-Sachs (BMS) symmetries, or equivalently Conformal Carroll symmetries, are intrinsically associated to null manifolds and in two dimensions can be obtained as an In{\"o}n{\"u}-Wigner contraction of the two-dimensional ($2d$) relativistic conformal algebra. Instead of performing contractions, we demonstrate in this paper how this transmutation of symmetries can be achieved by infinite boosts or degenerate linear transformations on coordinates. Taking explicit cues from the worldsheet theory of null strings, we show boosting the system is equivalent to adding a current-current deformation term to the Hamiltonian. As the strength of this deformation term reaches a critical value, the classical symmetry algebra "flows" from two copies of Virasoro to the BMS algebra. We further explore the situation where the CFT coordinates are asymmetrically transformed, and degenerate limits lead to chiral theories.
Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver<|sep|>Although model-based reinforcement learning (RL) approaches are considered more sample efficient, existing algorithms are usually relying on sophisticated planning algorithm to couple tightly with the model-learning procedure. Hence the learned models may lack the ability of being re-used with more specialized planners. In this paper we address this issue and provide approaches to learn an RL model efficiently without the guidance of a reward signal. In particular, we take a plug-in solver approach, where we focus on learning a model in the exploration phase and demand that \emph{any planning algorithm} on the learned model can give a near-optimal policy. Specicially, we focus on the linear mixture MDP setting, where the probability transition matrix is a (unknown) convex combination of a set of existing models. We show that, by establishing a novel exploration algorithm, the plug-in approach learns a model by taking $\tilde{O}(d^2H^3/\epsilon^2)$ interactions with the environment and \emph{any} $\epsilon$-optimal planner on the model gives an $O(\epsilon)$-optimal policy on the original model. This sample complexity matches lower bounds for non-plug-in approaches and is \emph{statistically optimal}. We achieve this result by leveraging a careful maximum total-variance bound using Bernstein inequality and properties specified to linear mixture MDP.
QCD Constituent Counting Rules for Neutral Vector Mesons<|sep|>QCD constituent counting rules define the scaling behavior of exclusive hadronic scattering and electromagnetic scattering amplitudes at high momentum transfer in terms of the total number of fundamental constituents in the initial and final states participating in the hard subprocess. The scaling laws reflect the twist of the leading Fock state for each hadron and hence the leading operator that creates the composite state from the vacuum. Thus, the constituent counting scaling laws can be used to identify the twist of exotic hadronic candidates such as tetraquarks and pentaquarks. Effective field theories must consistently implement the scaling rules in order to be consistent with the fundamental theory. Here we examine how one can apply constituent counting rules for the exclusive production of one or two neutral vector mesons $V^0$ in $e^+ e^-$ annihilation, processes in which the $V^0$ can couple via intermediate photons. In case of a (narrow) real $V^0$, the photon virtuality is fixed to a precise value $s_1 = m_{V^0}^2$, in effect treating the $V^0$ as a single fundamental particle. Each real $V^0$ thus contributes to the constituent counting rules with $N_{V_0} = 1$. In effect, the leading operator underlying the $V^0$ has twist 1. Thus, in the specific physical case of single or double on-shell $V^0$ production via intermediate photons, the predicted scaling from counting rules coincides with Vector Meson Dominance (VMD), an effective theory that treats $V^0$ as an elementary field. However, the VMD prediction fails in the general case where the $V^0$ is not coupled through an elementary photon field, and then the leading-twist interpolating operator has twist $N_{V_0} = 2$. Analogous effects appear in $pp$ scattering processes.
Lattice determination of the pion mass difference $M_{\pi^{+}} - M_{\pi^{0}}$ at order $\mathcal{O}(\alpha_{em})$ and $\mathcal{O}( (m_{d}-m_{u})^{2})$ including disconnected diagrams<|sep|>We present our preliminary results concerning the charged/neutral pion mass difference $M_{\pi^{+}} - M_{\pi^{0}}$ at order $\mathcal{O}(\alpha_{em})$ in the QED interactions, and for $M_{\pi^{+}} - M_{\pi^{0}}$ at order $\mathcal{O}\left( (m_{d}-m_{u})^{2}\right)$ in the strong isospin-breaking term. The latter contribution provides a determination of the $\rm{SU}(2)$ chiral perturbation theory low-energy constant $\ell_{7}$, whose present estimate is affected by a rather large uncertainty. The disconnected contributions appearing in the diagrammatic expansion of $M_{\pi^{+}} - M_{\pi^{0}}$, being very noisy, are notoriously difficult to evaluate and have been neglected in our previous calculations. By making use of twisted mass Lattice QCD simulations and adopting the RM123 method, we will show that taking profit from our recently proposed rotated twisted-mass (RTM) scheme, tailored to improve the signal on these kinds of observables, it is possible to evaluate the disconnected diagrams with good precision. For the QED induced pion mass difference, we obtain, after performing the extrapolation towards the continuum and thermodynamic limit and at the physical point, the preliminary value $M_{\pi^{+}}-M_{\pi^{0}} = 4.622~(95)~{\rm MeV}$, that is in good agreement with the experimental result. For the determination of the low-energy constant $\ell_{7}$, our result $\ell_{7} = 2.5~(1.4)\times 10^{-3}$, which is limited so far to a single lattice spacing, is in agreement and improves phenomenological estimates.
Transmission Spectra of Three-Dimensional Hot Jupiter Model Atmospheres<|sep|>We compute models of the transmission spectra of planets HD 209458b, HD 189733b, and generic hot Jupiters. We examine the effects of temperature, surface gravity, and metallicity for the generic planets as a guide to understanding transmission spectra in general. We find that carbon dioxide absorption at 4.4 and 15 microns is prominent at high metallicity, and is a clear metallicity indicator. For HD 209458b and HD 189733b, we compute spectra for both one-dimensional and three-dimensional model atmospheres and examine the differences between them. The differences are usually small, but can be large if atmospheric temperatures are near important chemical abundance boundaries. The calculations for the 3D atmospheres, and their comparison with data, serve as constraints on these dynamical models that complement the secondary eclipse and light curve data sets. For HD 209458b, even if TiO and VO gases are abundant on the day side, their abundances can be considerably reduced on the cooler planetary limb. However, given the predicted limb temperatures and TiO abundances, the model's optical opacity is too high. For HD 189733b we find a good match with some infrared data sets and constrain the altitude of a postulated haze layer. For this planet, substantial differences can exist between the transmission spectra of the leading and trailing hemispheres, which is an excellent probe of carbon chemistry. In thermochemical equilibrium, the cooler leading hemisphere is methane-dominated, and the hotter trailing hemisphere is CO-dominated, but these differences may be eliminated by non-equilibrium chemistry due to vertical mixing. It may be possible to constrain the carbon chemistry of this planet, and its spatial variation, with JWST.
Online programming system for robotic fillet welding in Industry 4.0<|sep|>Fillet welding is one of the most widespread types of welding in the industry, which is still carried out manually or automated by contact. This paper aims to describe an online programming system for noncontact fillet welding robots with U and L shaped structures, which responds to the needs of the Fourth Industrial Revolution. In this paper, the authors propose an online robot programming methodology that eliminates unnecessary steps traditionally performed in robotic welding, so that the operator only performs three steps to complete the welding task. First, choose the piece to weld. Then, enter the welding parameters. Finally, it sends the automatically generated program to the robot. The system finally managed to perform the fillet welding task with the proposed method in a more efficient preparation time than the compared methods. For this, a reduced number of components was used compared to other systems, such as, a structured light 3D camera, two computers and a concentrator, in addition to the six axis industrial robotic arm. The operating complexity of the system has been reduced as much as possible. To the best of the authors knowledge, there is no scientific or commercial evidence of an online robot programming system capable of performing a fillet welding process, simplifying the process so that it is completely transparent for the operator and framed in the Industry 4.0 paradigm. Its commercial potential lies mainly in its simple and low cost implementation in a flexible system capable of adapting to any industrial fillet welding job and to any support that can accommodate it.
A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis<|sep|>Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any hand-engineered features or external resources.
DC3: A learning method for optimization with hard constraints<|sep|>Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap "approximate solvers." Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility.
Personalised product design using virtual interactive techniques<|sep|>Use of Virtual Interactive Techniques for personalized product design is described in this paper. Usually products are designed and built by considering general usage patterns and Prototyping is used to mimic the static or working behaviour of an actual product before manufacturing the product. The user does not have any control on the design of the product. Personalized design postpones design to a later stage. It allows for personalized selection of individual components by the user. This is implemented by displaying the individual components over a physical model constructed using Cardboard or Thermocol in the actual size and shape of the original product. The components of the equipment or product such as screen, buttons etc. are then projected using a projector connected to the computer into the physical model. Users can interact with the prototype like the original working equipment and they can select, shape, position the individual components displayed on the interaction panel using simple hand gestures. Computer Vision techniques as well as sound processing techniques are used to detect and recognize the user gestures captured using a web camera and microphone.
UCC: Uncertainty guided Cross-head Co-training for Semi-Supervised Semantic Segmentation<|sep|>Deep neural networks (DNNs) have witnessed great successes in semantic segmentation, which requires a large number of labeled data for training. We present a novel learning framework called Uncertainty guided Cross-head Co-training (UCC) for semi-supervised semantic segmentation. Our framework introduces weak and strong augmentations within a shared encoder to achieve co-training, which naturally combines the benefits of consistency and self-training. Every segmentation head interacts with its peers and, the weak augmentation result is used for supervising the strong. The consistency training samples' diversity can be boosted by Dynamic Cross-Set Copy-Paste (DCSCP), which also alleviates the distribution mismatch and class imbalance problems. Moreover, our proposed Uncertainty Guided Re-weight Module (UGRM) enhances the self-training pseudo labels by suppressing the effect of the low-quality pseudo labels from its peer via modeling uncertainty. Extensive experiments on Cityscapes and PASCAL VOC 2012 demonstrate the effectiveness of our UCC. Our approach significantly outperforms other state-of-the-art semi-supervised semantic segmentation methods. It achieves 77.17$\%$, 76.49$\%$ mIoU on Cityscapes and PASCAL VOC 2012 datasets respectively under 1/16 protocols, which are +10.1$\%$, +7.91$\%$ better than the supervised baseline.
Confronting the sound speed of dark energy with future cluster surveys<|sep|>Future cluster surveys will observe galaxy clusters numbering in the hundred thousands. We consider this work how these surveys can be used to constrain dark energy parameters: in particular, the equation of state parameter w and the non-adiabatic sound speed c_s^2. We demonstrate that, in combination with Cosmic Microwave Background (CMB) observations from Planck, cluster surveys such as that in the ESA Euclid project will be able to determine a time-independent w with subpercent precision. Likewise, if the dark energy sound horizon falls within the length scales probed by the cluster survey, then c_s^2 can be pinned down to within an order of magnitude. In the course of this work, we also investigate the process of dark energy virialisation in the presence of an arbitrary sound speed. We find that dark energy clustering and virialisation can lead to dark energy contributing to the total cluster mass at approximately the 0.1% level at maximum.
A Bi-Directional Refinement Algorithm for the Calculus of (Co)Inductive Constructions<|sep|>The paper describes the refinement algorithm for the Calculus of (Co)Inductive Constructions (CIC) implemented in the interactive theorem prover Matita. The refinement algorithm is in charge of giving a meaning to the terms, types and proof terms directly written by the user or generated by using tactics, decision procedures or general automation. The terms are written in an "external syntax" meant to be user friendly that allows omission of information, untyped binders and a certain liberal use of user defined sub-typing. The refiner modifies the terms to obtain related well typed terms in the internal syntax understood by the kernel of the ITP. In particular, it acts as a type inference algorithm when all the binders are untyped. The proposed algorithm is bi-directional: given a term in external syntax and a type expected for the term, it propagates as much typing information as possible towards the leaves of the term. Traditional mono-directional algorithms, instead, proceed in a bottom-up way by inferring the type of a sub-term and comparing (unifying) it with the type expected by its context only at the end. We propose some novel bi-directional rules for CIC that are particularly effective. Among the benefits of bi-directionality we have better error message reporting and better inference of dependent types. Moreover, thanks to bi-directionality, the coercion system for sub-typing is more effective and type inference generates simpler unification problems that are more likely to be solved by the inherently incomplete higher order unification algorithms implemented. Finally we introduce in the external syntax the notion of vector of placeholders that enables to omit at once an arbitrary number of arguments. Vectors of placeholders allow a trivial implementation of implicit arguments and greatly simplify the implementation of primitive and simple tactics.
Magnetic Fields and Massive Star Formation<|sep|>Massive stars ($M > 8$ \msun) typically form in parsec-scale molecular clumps that collapse and fragment, leading to the birth of a cluster of stellar objects. We investigate the role of magnetic fields in this process through dust polarization at 870 $\mu$m obtained with the Submillimeter Array (SMA). The SMA observations reveal polarization at scales of $\lsim$ 0.1 pc. The polarization pattern in these objects ranges from ordered hour-glass configurations to more chaotic distributions. By comparing the SMA data with the single dish data at parsec scales, we found that magnetic fields at dense core scales are either aligned within $40^\circ$ of or perpendicular to the parsec-scale magnetic fields. This finding indicates that magnetic fields play an important role during the collapse and fragmentation of massive molecular clumps and the formation of dense cores. We further compare magnetic fields in dense cores with the major axis of molecular outflows. Despite a limited number of outflows, we found that the outflow axis appears to be randomly oriented with respect to the magnetic field in the core. This result suggests that at the scale of accretion disks ($\lsim 10^3$ AU), angular momentum and dynamic interactions possibly due to close binary or multiple systems dominate over magnetic fields. With this unprecedentedly large sample massive clumps, we argue on a statistical basis that magnetic fields play an important role during the formation of dense cores at spatial scale of 0.01 - 0.1 pc in the context of massive star and cluster star formation.
Analysis and Control of Power-Temperature Dynamics in Heterogeneous Multiprocessors<|sep|>Virtually all electronic systems try to optimize a fundamental trade-off between higher performance and lower power consumption. The latter becomes critical in mobile computing systems, such as smartphones, which rely on passive cooling. Otherwise, the heat concentrated in a small area drives both the junction and skin temperatures up. High junction temperatures degrade the reliability, while skin temperature deteriorates the user experience. Therefore, there is a strong need for a formal analysis of power consumption-temperature dynamics and predictive thermal management algorithms. This paper presents a theoretical power-temperature analysis of multiprocessor systems, which are modeled as multi-input multi-output dynamic systems. We analyze the conditions under which the system converges to a stable steady-state temperature. Then, we use these models to design a control algorithm that manages the temperature of the system without affecting the performance of the application. Experiments on the Odroid-XU3 board show that the control algorithm is able to regulate the temperature with a minimal loss in performance when compared to the default thermal governors.
Light Higgsino Decays as a Probe of the NMSSM<|sep|>In the Next-to Minimal Supersymmetric Standard Model (NMSSM), a sizable coupling $\lambda$ between the singlet and Higgs fields can naturally accommodate the observed Higgs boson mass of 125 GeV. This large coupling also results in a large separation between the Higgsino and singlino mass scales in the neutralino sector to evade stringent constraints from direct detection experiments. Most of the natural parameter space in this setup therefore contains light Higgsinos that can decay into the singlino and the 125 GeV Higgs. If the Higgsinos are to be light enough to be produced at the LHC, the mass gap $m_{\tilde\chi^0_2} -(m_{\tilde\chi^0_1} +m_h)$ is generally fairly low, resulting in small missing energy. We study the collider phenomenology of this process and demonstrate that the 4$b+j+{E\!\!\!\!/_{\rm T}}$ signal arising from $pp\rightarrow\tilde\chi_i^0\tilde\chi_j^0jj$ process can be a viable channel to search for this low mass gap region at the 14 TeV LHC. In addition, we find that a potential signal from the LHC search, in tandem with direct detection experiments, can distinguish the NMSSM from the analogous process in the MSSM, where the bino plays the role of the singlino, for positive values of the $\mu$ parameter even when the additional singlet-dominated (pseudo)scalars are absent or indistinguishable from the Higgs in the decay.
Eccentricity generation in hierarchical triple systems with coplanar and initially circular orbits<|sep|>We develop a technique for estimating the inner eccentricity in hierarchical triple systems with well separated components. We investigate systems with initially circular and coplanar orbits and comparable masses. The technique is based on an expansion of the rate of change of the Runge-Lenz vector for calculating short period terms by using first order perturbation theory. The combination of the short period terms with terms arising from octupole level secular theory, results in the derivation of a rather simple formula for the eccentricity of the inner binary. The theoretical results are tested against numerical integrations of the full equations of motion. Comparison is also made with other results on the subject.
Power to the Relational Inductive Bias: Graph Neural Networks in Electrical Power Grids<|sep|>The application of graph neural networks (GNNs) to the domain of electrical power grids has high potential impact on smart grid monitoring. Even though there is a natural correspondence of power flow to message-passing in GNNs, their performance on power grids is not well-understood. We argue that there is a gap between GNN research driven by benchmarks which contain graphs that differ from power grids in several important aspects. Additionally, inductive learning of GNNs across multiple power grid topologies has not been explored with real-world data. We address this gap by means of (i) defining power grid graph datasets in inductive settings, (ii) an exploratory analysis of graph properties, and (iii) an empirical study of the concrete learning task of state estimation on real-world power grids. Our results show that GNNs are more robust to noise with up to 400% lower error compared to baselines. Furthermore, due to the unique properties of electrical grids, we do not observe the well known over-smoothing phenomenon of GNNs and find the best performing models to be exceptionally deep with up to 13 layers. This is in stark contrast to existing benchmark datasets where the consensus is that 2 to 3 layer GNNs perform best. Our results demonstrate that a key challenge in this domain is to effectively handle long-range dependence.
Tuning Magnetic Avalanches in Mn12-ac<|sep|>Using micron-sized Hall sensor arrays to obtain time-resolved measurements of the local magnetization, we report a systematic study in the molecular magnet Mn$_{12}$-acetate of magnetic avalanches controllably triggered in different fixed external magnetic fields and for different values of the initial magnetization. The speeds of propagation of the spin-reversal fronts are in good overall agreement with the theory of magnetic deflagration of Garanin and Chudnovsky \cite{Garanin}.
Cost objective PLM and CE<|sep|>Concurrent engineering taking into account product life-cycle factors seems to be one of the industrial challenges of the next years. Cost estimation and management are two main strategic tasks that imply the possibility of managing costs at the earliest stages of product development. This is why it is indispensable to let people from economics and from industrial engineering collaborates in order to find the best solution for enterprise progress for economical factors mastering. The objective of this paper is to present who we try to adapt costing methods in a PLM and CE point of view to the new industrial context and configuration in order to give pertinent decision aid for product and process choices. A very important factor is related to cost management problems when developing new products. A case study is introduced that presents how product development actors have referenced elements to product life-cycle costs and impacts, how they have an idea bout economical indicators when taking decisions during the progression of the project of product development.
Penguin contributions to B to J/Psi P Decays<|sep|>The high precision to which the standard model has been confirmed implies that new physics effects have to be small in the observed processes. Together with the outstanding precision expected from present and future collider experiments this renders the evaluation of subleading SM contributions necessary. For the "golden mode", B_d to J/Psi K, these so-called "penguin pollution" terms can be controlled by using flavour symmetry relations. A recent analysis is presented which yields a stronger bound on the maximal penguin influence than previous ones and shows how the corresponding uncertainty can be reduced with coming data.
Uncertainty quantification in electromagnetic observables of nuclei<|sep|>We present strategies to quantify theoretical uncertainties in modern ab-initio calculations of electromagnetic observables in light and medium-mass nuclei. We discuss how uncertainties build up from various sources, such as the approximations introduced by the few- or many-body solver and the truncation of the chiral effective field theory expansion. We review the recent progress encompassing a broad range of electromagnetic observables in stable and unstable nuclei.
Mean-field and direct numerical simulations of magnetic flux concentrations from vertical field<|sep|>Strongly stratified hydromagnetic turbulence has previously been found to produce magnetic flux concentrations if the domain is large enough compared with the size of turbulent eddies. Mean-field simulations (MFS) using parameterizations of the Reynolds and Maxwell stresses show a negative effective magnetic pressure instability and have been able to reproduce many aspects of direct numerical simulations (DNS) regarding the growth rate of this large-scale instability, shape of the resulting magnetic structures, and their height as a function of magnetic field strength. Unlike the case of an imposed horizontal field, for a vertical one, magnetic flux concentrations of equipartition strength with the turbulence can be reached. This results in magnetic spots that are reminiscent of sunspots. Here we want to find out under what conditions magnetic flux concentrations with vertical field occur and what their internal structure is. We use a combination of MFS, DNS, and implicit large-eddy simulations to characterize the resulting magnetic flux concentrations in forced isothermal turbulence with an imposed vertical magnetic field. We confirm earlier results that in the kinematic stage of the large-scale instability the horizontal wavelength of structures is about 10 times the density scale height. At later times, even larger structures are being produced in a fashion similar to inverse spectral transfer in helically driven turbulence. Using turbulence simulations, we find that magnetic flux concentrations occur for different values of the Mach number between 0.1 and 0.7. DNS and MFS show magnetic flux tubes with mean-field energies comparable to the turbulent kinetic energy. The resulting vertical magnetic flux tubes are being confined by downflows along the tubes and corresponding inflow from the sides, which keep the field concentrated.
Singularities of serial robots: Identification and distance computation using geometric algebra<|sep|>The singularities of serial robotic manipulators are those configurations in which the robot loses the ability to move in at least one direction. Hence, their identification is fundamental to enhance the performance of current control and motion planning strategies. While classical approaches entail the computation of the determinant of either a 6x n or nxn matrix for an n degrees of freedom serial robot, this work addresses a novel singularity identification method based on modelling the twists defined by the joint axes of the robot as vectors of the six-dimensional and three-dimensional geometric algebras. In particular, it consists of identifying which configurations cause the exterior product of these twists to vanish. In addition, since rotors represent rotations in geometric algebra, once these singularities have been identified, a distance function is defined in the configuration space C such that its restriction to the set of singular configurations S allows us to compute the distance of any configuration to a given singularity. This distance function is used to enhance how the singularities are handled in three different scenarios, namely motion planning, motion control and bilateral teleoperation.
To believe or not to believe: Validating explanation fidelity for dynamic malware analysis<|sep|>Converting malware into images followed by vision-based deep learning algorithms has shown superior threat detection efficacy compared with classical machine learning algorithms. When malware are visualized as images, visual-based interpretation schemes can also be applied to extract insights of why individual samples are classified as malicious. In this work, via two case studies of dynamic malware classification, we extend the local interpretable model-agnostic explanation algorithm to explain image-based dynamic malware classification and examine its interpretation fidelity. For both case studies, we first train deep learning models via transfer learning on malware images, demonstrate high classification effectiveness, apply an explanation method on the images, and correlate the results back to the samples to validate whether the algorithmic insights are consistent with security domain expertise. In our first case study, the interpretation framework identifies indirect calls that uniquely characterize the underlying exploit behavior of a malware family. In our second case study, the interpretation framework extracts insightful information such as cryptography-related APIs when applied on images created from API existence, but generate ambiguous interpretation on images created from API sequences and frequencies. Our findings indicate that current image-based interpretation techniques are promising for explaining vision-based malware classification. We continue to develop image-based interpretation schemes specifically for security applications.
Protein Folding in the 2D Hydrophobic-Hydrophilic (HP) Square Lattice Model is Chaotic<|sep|>Among the unsolved problems in computational biology, protein folding is one of the most interesting challenges. To study this folding, tools like neural networks and genetic algorithms have received a lot of attention, mainly due to the NP-completeness of the folding process. The background idea that has given rise to the use of these algorithms is obviously that the folding process is predictable. However, this important assumption is disputable as chaotic properties of such a process have been recently highlighted. In this paper, which is an extension of a former work accepted to the 2011 International Joint Conference on Neural Networks (IJCNN11), the topological behavior of a well-known dynamical system used for protein folding prediction is evaluated. It is mathematically established that the folding dynamics in the 2D hydrophobic-hydrophilic (HP) square lattice model, simply called "the 2D model" in this document, is indeed a chaotic dynamical system as defined by Devaney. Furthermore, the chaotic behavior of this model is qualitatively and quantitatively deepened, by studying other mathematical properties of disorder, namely: the indecomposability, instability, strong transitivity, and constants of expansivity and sensitivity. Some consequences for both biological paradigms and structure prediction using this model are then discussed. In particular, it is shown that some neural networks seems to be unable to predict the evolution of this model with accuracy, due to its complex behavior.
Simulations support protocol independency of the granular temperature<|sep|>A possible approach to the statistical description of granular assemblies starts from Edwards' assumption that all blocked states occupying the same volume are equally probable (S.F. Edwards, R. Oakeshott, Physica A 157, 1080 (1989)). We performed computer simulations using two-dimensional polygonal particles excited periodically according to two different protocols: excitation by pulses of "negative gravity" and excitation by "rotating gravity". The first protocol exhibits a non-monotonous dependency of the mean volume fraction on the pulse strength. The overlapping histogram method is used in order to test whether or not the volume distribution is described by a Boltzmann-like distribution, and to calculate the inverse compactivity as well as the logarithm of the partition sum. We find that the mean volume is a unique function of the measured granular temperature, independently of the protocol and of the branch in $\phi(g)$ and all determined quantities are in agreement with Edwards' theory.
Barotropic theory for the velocity profile of Jupiter turbulent jets: an example for an exact turbulent closure<|sep|>We model the dynamics of Jupiter's jets by averaging the dynamics of eddies, in a barotropic beta-plane model, and explicitly predicting the balance between Reynolds' stresses and dissipation, thus predicting the average velocity profile explicitly.In order to obtain this result, we adopt a non-equilibrium statistical mechanics approach. We consider a relevant limit for Jupiter troposphere, of a time scale separation between inertial dynamics on one hand, and stochastic forcing and dissipation on the other hand. We assume that the forcing acts on scales much smaller than the jet scale, and we obtain a very simple explicit relation between the Reynolds stress, the energy injection rate, and the average velocity shear, valid far from the jet edges (extrema of zonal velocity). A specific asymptotic expansion close to jet edges unravel an asymmetry between eastward and westward, velocity extrema. We recover Jupiter's jet specificities: a cusp on eastward jets and a smooth parabola on westward jets.
From Fields to Trees<|sep|>We present new MCMC algorithms for computing the posterior distributions and expectations of the unknown variables in undirected graphical models with regular structure. For demonstration purposes, we focus on Markov Random Fields (MRFs). By partitioning the MRFs into non-overlapping trees, it is possible to compute the posterior distribution of a particular tree exactly by conditioning on the remaining tree. These exact solutions allow us to construct efficient blocked and Rao-Blackwellised MCMC algorithms. We show empirically that tree sampling is considerably more efficient than other partitioned sampling schemes and the naive Gibbs sampler, even in cases where loopy belief propagation fails to converge. We prove that tree sampling exhibits lower variance than the naive Gibbs sampler and other naive partitioning schemes using the theoretical measure of maximal correlation. We also construct new information theory tools for comparing different MCMC schemes and show that, under these, tree sampling is more efficient.
A Robust Bayesian Dynamic Linear Model for Latin-American Economic Time Series: "The Mexico and Puerto Rico Cases"<|sep|>The traditional time series methodology requires at least a preliminary transformation of the data to get stationarity. On the other hand, Robust Bayesian Dynamic Models (RBDMs) do not assume a regular pattern or stability of the underlying system but can include points of statement breaks. In this paper we use RBDMs in order to account possible outliers and structural breaks in Latin-American economic time series. We work with important economic time series from Puerto Rico and Mexico. We show by using a random walk model how RBDMs can be applied for detecting historic changes in the economic inflation of Mexico. Also, we model the Consumer Price Index (CPI), the Economic Activity Index (EAI) and the total number of employments (TNE) economic time series in Puerto Rico using local linear trend and seasonal RBDMs with observational and states variances. The results illustrate how the model accounts the structural breaks for the historic recession periods in Puerto Rico.
Structured Bayesian Gaussian process latent variable model: applications to data-driven dimensionality reduction and high-dimensional inversion<|sep|>We introduce a methodology for nonlinear inverse problems using a variational Bayesian approach where the unknown quantity is a spatial field. A structured Bayesian Gaussian process latent variable model is used both to construct a low-dimensional generative model of the sample-based stochastic prior as well as a surrogate for the forward evaluation. Its Bayesian formulation captures epistemic uncertainty introduced by the limited number of input and output examples, automatically selects an appropriate dimensionality for the learned latent representation of the data, and rigorously propagates the uncertainty of the data-driven dimensionality reduction of the stochastic space through the forward model surrogate. The structured Gaussian process model explicitly leverages spatial information for an informative generative prior to improve sample efficiency while achieving computational tractability through Kronecker product decompositions of the relevant kernel matrices. Importantly, the Bayesian inversion is carried out by solving a variational optimization problem, replacing traditional computationally-expensive Monte Carlo sampling. The methodology is demonstrated on an elliptic PDE and is shown to return well-calibrated posteriors and is tractable with latent spaces with over 100 dimensions.
Cross-scale Attention Model for Acoustic Event Classification<|sep|>A major advantage of a deep convolutional neural network (CNN) is that the focused receptive field size is increased by stacking multiple convolutional layers. Accordingly, the model can explore the long-range dependency of features from the top layers. However, a potential limitation of the network is that the discriminative features from the bottom layers (which can model the short-range dependency) are smoothed out in the final representation. This limitation is especially evident in the acoustic event classification (AEC) task, where both short- and long-duration events are involved in an audio clip and needed to be classified. In this paper, we propose a cross-scale attention (CSA) model, which explicitly integrates features from different scales to form the final representation. Moreover, we propose the adoption of the attention mechanism to specify the weights of local and global features based on the spatial and temporal characteristics of acoustic events. Using mathematic formulations, we further reveal that the proposed CSA model can be regarded as a weighted residual CNN (ResCNN) model when the ResCNN is used as a backbone model. We tested the proposed model on two AEC datasets: one is an urban AEC task, and the other is an AEC task in smart car environments. Experimental results show that the proposed CSA model can effectively improve the performance of current state-of-the-art deep learning algorithms.
Drag force in a D-instanton background<|sep|>We study the drag force and diffusion coefficient with respect to a moving heavy quark in a D-instanton background, which corresponds to the Yang-Mills theory in the deconfining, high-temperature phase. It is shown that the presence of the D-instanton density tends to increase the drag force and decrease the diffusion coefficient, reverse to the effects of the velocity and the temperature. Moreover, the inclusion of the D-instanton density makes the medium less viscous.
Near-infrared counterparts of three transient very faint neutron star X-ray binaries<|sep|>We present near-infrared (NIR) imaging observations of three transient neutron star X-ray binaries, SAX J1753.5-2349, SAX J1806.5-2215 and AX J1754.2-2754. All three sources are members of the class of `very faint' X-ray transients which exhibit X-ray luminosities $L_X\lesssim10^{36}$ erg s$^{-1}$. The nature of this class of sources is still poorly understood. We detect NIR counterparts for all three systems and perform multi-band photometry for both SAX J1753.5-2349 and SAX J1806.5-2215, including narrow-band Br$_{\gamma}$ photometry for SAX J1806.5-2215. We find that SAX J1753.5-2349 is significantly redder than the field population, indicating that there may be absorption intrinsic to the system, or perhaps a jet is contributing to the infrared emission. SAX J1806.5-2215 appears to exhibit absorption in Br$_{\gamma}$, providing evidence for hydrogen in the system. Our observations of AX J1754.2--2754 represent the first detection of a NIR counterpart for this system. We find that none of the measured magnitudes are consistent with the expected quiescent magnitudes of these systems. Assuming that the infrared radiation is dominated by either the disc or the companion star, the observed magnitudes argue against an ultracompact nature for all three systems.
Improved Abdominal Multi-Organ Segmentation via 3D Boundary-Constrained Deep Neural Networks<|sep|>Quantitative assessment of the abdominal region from clinically acquired CT scans requires the simultaneous segmentation of abdominal organs. Thanks to the availability of high-performance computational resources, deep learning-based methods have resulted in state-of-the-art performance for the segmentation of 3D abdominal CT scans. However, the complex characterization of organs with fuzzy boundaries prevents the deep learning methods from accurately segmenting these anatomical organs. Specifically, the voxels on the boundary of organs are more vulnerable to misprediction due to the highly-varying intensity of inter-organ boundaries. This paper investigates the possibility of improving the abdominal image segmentation performance of the existing 3D encoder-decoder networks by leveraging organ-boundary prediction as a complementary task. To address the problem of abdominal multi-organ segmentation, we train the 3D encoder-decoder network to simultaneously segment the abdominal organs and their corresponding boundaries in CT scans via multi-task learning. The network is trained end-to-end using a loss function that combines two task-specific losses, i.e., complete organ segmentation loss and boundary prediction loss. We explore two different network topologies based on the extent of weights shared between the two tasks within a unified multi-task framework. To evaluate the utilization of complementary boundary prediction task in improving the abdominal multi-organ segmentation, we use three state-of-the-art encoder-decoder networks: 3D UNet, 3D UNet++, and 3D Attention-UNet. The effectiveness of utilizing the organs' boundary information for abdominal multi-organ segmentation is evaluated on two publically available abdominal CT datasets. A maximum relative improvement of 3.5% and 3.6% is observed in Mean Dice Score for Pancreas-CT and BTCV datasets, respectively.
Efficient Approximation of Diagonal Unitaries over the Clifford+T Basis<|sep|>We present an algorithm for the approximate decomposition of diagonal operators, focusing specifically on decompositions over the Clifford+$T$ basis, that minimize the number of phase-rotation gates in the synthesized approximation circuit. The equivalent $T$-count of the synthesized circuit is bounded by $k \, C_0 \log_2(1/\varepsilon) + E(n,k)$, where $k$ is the number of distinct phases in the diagonal $n$-qubit unitary, $\varepsilon$ is the desired precision, $C_0$ is a quality factor of the implementation method ($1<C_0<4$), and $E(n,k)$ is the total entanglement cost (in $T$ gates). We determine an optimal decision boundary in $(k,n,\varepsilon)$-space where our decomposition algorithm achieves lower entanglement cost than previous state-of-the-art techniques. Our method outperforms state-of-the-art techniques for a practical range of $\varepsilon$ values and diagonal operators and can reduce the number of $T$ gates exponentially in $n$ when $k << 2^n$.
Broken scale-invariance in time-dependent trapping potentials<|sep|>The response of a cold atom gas with contact interactions to a smoothly varying external harmonic confinement in the non-adiabatic regime is studied. The time variation of the angular frequency is varied such that the system is, for vanishing or infinitely strong contact interactions, scale invariant. The time evolution of the system with broken scale invariance (i.e., the time evolution of the system with finite interaction strength), is contrasted with that for a scale invariant system, which exhibits Efimovian-like expansion dynamics that is characterized by log-periodic oscillations with unique period and amplitude. It is found that the breaking of the scale invariance by the finiteness of the interactions leads to a time dependence of the oscillation period and amplitude. It is argued, based on analytical considerations for atomic gases of arbitrary size and numerical results for two one-dimensional particles, that the oscillation pe riod approaches that of the scale-invariant system at large times. The role of the time-dependent contact in the expansion dynamics is analyzed.
Nonlinear optical response of a local surface plasmon coupled to a 2D material<|sep|>We present a theoretical study of the optical response of a nonlinear oscillator formed by coupling a metal nanoparticle local surface plasmon resonance to excitonic degrees of freedom in a monolayer transition-metal dichalcogenide. We show that the combined system should exhibit strong anharmonicity in its low-lying states, predicting for example a seven order-of-magnitude increase in nonlinearity relative to a silicon photonic crystal cavity. Then, we demonstrate that such system exhibits strong quantum features such as antibunching and non-Gaussianity. Arrays of such nanoscale nonlinear oscillators could be used to realize novel optical metamaterials; alternatively, an individual nanoparticle-monolayer construct could be coupled to an optical resonator to mediate efficient input-output coupling to propagating fields.
Analysis and Design of Ultra Thin Electromagnetic Absorbers Comprising Resistively Loaded High Impedance Surfaces<|sep|>High-Impedance Surfaces (HIS) comprising lossy Frequency Selective Surfaces (FSS) are employed to design thin electromagnetic absorbers. The structure, despite its typical resonant behavior, is able to perform a very wideband absorption in a reduced thickness. Losses in the frequency selective surface are introduced by printing the periodic pattern through resistive inks and hence avoiding the typical soldering of a large number of lumped resistors. The effect of the surface resistance of the FSS and dielectric substrate characteristics on the input impedance of the absorber is discussed by means of a circuital model. It is shown that the optimum value of surface resistance is affected both by substrate parameters (thickness and permittivity) and by FSS element shape. The equivalent circuit model is then used to introduce the working principles of the narrowband and the wideband absorbing structure and to derive the best-suited element for wideband absorption. Finally, the experimental validation of the presented structures is presented.
Demoir\'eing of Camera-Captured Screen Images Using Deep Convolutional Neural Network<|sep|>Taking photos of optoelectronic displays is a direct and spontaneous way of transferring data and keeping records, which is widely practiced. However, due to the analog signal interference between the pixel grids of the display screen and camera sensor array, objectionable moir\'e (alias) patterns appear in captured screen images. As the moir\'e patterns are structured and highly variant, they are difficult to be completely removed without affecting the underneath latent image. In this paper, we propose an approach of deep convolutional neural network for demoir\'eing screen photos. The proposed DCNN consists of a coarse-scale network and a fine-scale network. In the coarse-scale network, the input image is first downsampled and then processed by stacked residual blocks to remove the moir\'e artifacts. After that, the fine-scale network upsamples the demoir\'ed low-resolution image back to the original resolution. Extensive experimental results have demonstrated that the proposed technique can efficiently remove the moir\'e patterns for camera acquired screen images; the new technique outperforms the existing ones.
Security of practical phase-coding quantum key distribution<|sep|>Security proof of practical quantum key distribution (QKD) has attracted a lot of attentions in recent years. Most of real-life QKD implementations are based on phase-coding BB84 protocol, which usually uses Unbalanced Mach-Zehnder Interferometer (UMZI) as the information coder and decoder. However, the long arm and short arm of UMZI will introduce different loss in practical experimental realizations, the state emitted by Alice's side is nolonger standard BB84 states. In this paper, we will give a security analysis in this situation. Counterintuitively, active compensation for this different loss will only lower the secret key bit rate.
Self-similar collapse in a circular magnetic field and electron beam jets by hybrid transverse plasmon<|sep|>Based on the set of nonlinear coupling equations describing the interaction of the high-frequency field, the self-generated magnetic field and the ion-acoustic field, the dispersion relation for the circular magnetic field is obtained. The numerical results indicate that the strength of the magnetic field have influence on the growth rate of modulation instability. The self-generated magnetic field has the tendency to self-similar collapse which makes the electron escapes along the axial region and form collimated jets. The velocity of jets is calculated and the results are consistent with experimental observations. The research may be applied to understand the dynamic process of electron beam jets in laboratory and space plasma.
Self-supervised Video-centralised Transformer for Video Face Clustering<|sep|>This paper presents a novel method for face clustering in videos using a video-centralised transformer. Previous works often employed contrastive learning to learn frame-level representation and used average pooling to aggregate the features along the temporal dimension. This approach may not fully capture the complicated video dynamics. In addition, despite the recent progress in video-based contrastive learning, few have attempted to learn a self-supervised clustering-friendly face representation that benefits the video face clustering task. To overcome these limitations, our method employs a transformer to directly learn video-level representations that can better reflect the temporally-varying property of faces in videos, while we also propose a video-centralised self-supervised framework to train the transformer model. We also investigate face clustering in egocentric videos, a fast-emerging field that has not been studied yet in works related to face clustering. To this end, we present and release the first large-scale egocentric video face clustering dataset named EasyCom-Clustering. We evaluate our proposed method on both the widely used Big Bang Theory (BBT) dataset and the new EasyCom-Clustering dataset. Results show the performance of our video-centralised transformer has surpassed all previous state-of-the-art methods on both benchmarks, exhibiting a self-attentive understanding of face videos.
On the Hierarchy of Distributed Majority Protocols<|sep|>We study the Consensus problem among $n$ agents, defined as follows. Initially, each agent holds one of two possible opinions. The goal is to reach a consensus configuration in which every agent shares the same opinion. To this end, agents randomly sample other agents and update their opinion according to a simple update function depending on the sampled opinions. We consider two communication models: the gossip model and a variant of the population model. In the gossip model, agents are activated in parallel, synchronous rounds. In the population model, one agent is activated after the other in a sequence of discrete time steps. For both models we analyze the following natural family of majority processes called $j$-Majority: when activated, every agent samples $j$ other agents uniformly at random (with replacement) and adopts the majority opinion among the sample (breaking ties uniformly at random). As our main result we show a hierarchy among majority protocols: $(j+1)$-Majority (for $j > 1$) converges stochastically faster than $j$-Majority for any initial opinion configuration. In our analysis we use Strassen's Theorem to prove the existence of a coupling. This gives an affirmative answer for the case of two opinions to an open question asked by Berenbrink et al. [2017].
Flat-band ferromagnetism in the multilayer Lieb optical lattice<|sep|>We theoretically study magnetic properties of two-component cold fermions in half-filled multilayer Lieb optical lattices, i.e., two, three, and several layers, using the dynamical mean-field theory. We clarify that the magnetic properties of this system become quite different depending on whether the number of layers is odd or even. In odd-number-th layers in an odd-number-layer system, finite magnetization emerges even with an infinitesimal interaction. This is a striking feature of the flatband ferromagnetic state in multilayer systems as a consequence of the Lieb theorem. In contrast, in even-number layers, magnetization develops from zero on a finite interaction. These different magnetic behaviours are triggered by the flat bands in the local density of states and become identical in the limit of the infinite-layer (i.e., three-dimensional) system. We also address how interlayer hopping affects the magnetization process. Further, we point out that layer magnetization, which is a population imbalance between up and down atoms on a layer, can be employed to detect the emergence of the flat-band ferromagnetic state without addressing sublattice magnetization.
Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books<|sep|>Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.
Pre-flight integration and characterization of the SPIDER balloon-borne telescope<|sep|>We present the results of integration and characterization of the SPIDER instrument after the 2013 pre-flight campaign. SPIDER is a balloon-borne polarimeter designed to probe the primordial gravitational wave signal in the degree-scale $B$-mode polarization of the cosmic microwave background. With six independent telescopes housing over 2000 detectors in the 94 GHz and 150 GHz frequency bands, SPIDER will map 7.5% of the sky with a depth of 11 to 14 $\mu$K$\cdot$arcmin at each frequency, which is a factor of $\sim$5 improvement over Planck. We discuss the integration of the pointing, cryogenic, electronics, and power sub-systems, as well as pre-flight characterization of the detectors and optical systems. SPIDER is well prepared for a December 2014 flight from Antarctica, and is expected to be limited by astrophysical foreground emission, and not instrumental sensitivity, over the survey region.
Design of Highly Efficient Hybrid Si-Au Taper for Dielectric Strip Waveguide to Plasmonic Slot Waveguide Mode Converter<|sep|>In this paper, we design a dielectric-to-plasmonic slot waveguide mode converter based on the hybrid silicon-gold taper. The effects of mode matching, the effective index matching, and the metallic absorption loss on the conversion efficiency are studied. Consequently, a metallic taper-funnel coupler with an overall length of 1.7um is designed to achieve a very high conversion efficiency of 93.3% at 1550 nm. The configuration limitations for not allowing this mode converter to achieve a 100% conversion efficiency are also investigated. Such a high-efficiency converter can provide practical routes to realize ultracompact integrated circuits.
The pigeonhole principle is not violated by quantum mechanics<|sep|>Bell's theorem has long been considered to establish local realism as the fundamental principle that contradicts quantum mechanics. It is therefore surprising that the quantum pigeonhole effect points to the pigeonhole principle as yet another source of contradiction [Aharonov et al., Proc. Natl. Acad. Sci. USA, 113, 532-535 (2016)]. Here we construct two new forms of Bell's inequality with the pigeonhole principle, then reconstruct Aharonov et al.'s weak measurement on a bipartite system. We show that in both cases it is counterfactual reasoning by the assumption of realism rather than the pigeonhole principle being violated by quantum mechanics. We further show that the quantum pigeonhole effect is in fact a new version of Bell's theorem without inequality. With the pigeonhole principle as the same conduit, a comparison between two versions of Bell's theorem becomes straightforward as it only relies on the orthonormality of the Bell states.
An LSTM-based Plagiarism Detection via Attention Mechanism and a Population-based Approach for Pre-Training Parameters with imbalanced Classes<|sep|>Plagiarism is one of the leading problems in academic and industrial environments, which its goal is to find the similar items in a typical document or source code. This paper proposes an architecture based on a Long Short-Term Memory (LSTM) and attention mechanism called LSTM-AM-ABC boosted by a population-based approach for parameter initialization. Gradient-based optimization algorithms such as back-propagation (BP) are widely used in the literature for learning process in LSTM, attention mechanism, and feed-forward neural network, while they suffer from some problems such as getting stuck in local optima. To tackle this problem, population-based metaheuristic (PBMH) algorithms can be used. To this end, this paper employs a PBMH algorithm, artificial bee colony (ABC), to moderate the problem. Our proposed algorithm can find the initial values for model learning in all LSTM, attention mechanism, and feed-forward neural network, simultaneously. In other words, ABC algorithm finds a promising point for starting BP algorithm. For evaluation, we compare our proposed algorithm with both conventional and population-based methods. The results clearly show that the proposed method can provide competitive performance.
Improving PARSEC models for very low mass stars<|sep|>Many stellar models present difficulties in reproducing basic observational relations of very low mass stars (VLMS), including the mass--radius relation and the optical colour--magnitudes of cool dwarfs. Here, we improve PARSEC models on these points. We implement the T--tau relations from PHOENIX BT-Settl model atmospheres as the outer boundary conditions in the PARSEC code, finding that this change alone reduces the discrepancy in the mass--radius relation from 8 to 5 per cent. We compare the models with multi--band photometry of clusters Praesepe and M67, showing that the use of T--tau relations clearly improves the description of the optical colours and magnitudes. But anyway, using both Kurucz and PHOENIX model spectra, model colours are still systematically fainter and bluer than the observations. We then apply a shift to the above T--tau relations, increasing from 0 at T_eff = 4730 K to ~14% at T_eff = 3160 K, to reproduce the observed mass--radius radius relation of dwarf stars. Taking this experiment as a calibration of the T--tau relations, we can reproduce the optical and near infrared CMDs of low mass stars in the old metal--poor globular clusters NGC6397 and 47Tuc, and in the intermediate--age and young solar--metallicity open clusters M67 and Praesepe. Thus, we extend PARSEC models using this calibration, providing VLMS models more suitable for the lower main sequence stars over a wide range of metallicities and wavelengths. Both sets of models are available on PARSEC webpage.
Optimal three-weight cubic codes<|sep|>In this paper, we construct an infinite family of three-weight binary codes from linear codes over the ring $R=\mathbb{F}_2+v\mathbb{F}_2+v^2\mathbb{F}_2$, where $v^3=1.$ These codes are defined as trace codes. They have the algebraic structure of abelian codes. Their Lee weight distributions are computed by employing character sums. The three-weight binary linear codes which we construct are shown to be optimal when $m$ is odd and $m>1$. They are cubic, that is to say quasi-cyclic of co-index three. An application to secret sharing schemes is given.
Non-perturbative renormalization of tensor bilinears in Schr\"odinger Functional schemes<|sep|>We present preliminary result for the study of the renormalization group evolution of tensor bilinears in Schr\"odinger Functional (SF) schemes for $N_f=0$ and $N_f=2$ QCD with non-perturbatively $\mathcal{O}(a)$-improved Wilson fermions. First $N_f=2+1$ results (proceeding in parallel with the ongoing computation of the running quark masses [1] are also discussed. A one-loop perturbative calculation of the discretisation effects for the relevant step scaling functions has been carried out for both Wilson and $\mathcal{O}(a)$-improved actions and for a large number of lattice resolutions. We also calculate the two-loop anomalous dimension in SF schemes for tensor currents through a scheme matching procedure with RI and $\overline{\rm MS}$. Thanks to the SF iterative procedure the non-perturbative running over two orders of magnitude in energy scales, as well as the corresponding Renormalization Group Invariant operators, have been determined.
The algebra of interpolatory cubature formulae for generic nodes<|sep|>We consider the classical problem of computing the expected value of a real function $f$ of the $d$-variate random variable $X$ using cubature formul\ae. We use in synergy tools from Commutative Algebra for cubature rul\ae, from elementary orthogonal polynomial theory and from Probability.
A way forward in the study of the symmetry energy: experiment, theory, and observation<|sep|>The symmetry energy describes how the energy of nuclear matter rises as one goes away from equal numbers of neutrons and protons. This is very important to describe neutron rich matter in astrophysics. This article reviews our knowledge of the symmetry energy from theoretical calculations, nuclear structure measurements, heavy ion collisions, and astronomical observations. We then present a roadmap to make progress in areas of relevance to the symmetry energy that promotes collaboration between the astrophysics and the nuclear physics communities.
Generalized Error Exponents For Small Sample Universal Hypothesis Testing<|sep|>The small sample universal hypothesis testing problem is investigated in this paper, in which the number of samples $n$ is smaller than the number of possible outcomes $m$. The goal of this work is to find an appropriate criterion to analyze statistical tests in this setting. A suitable model for analysis is the high-dimensional model in which both $n$ and $m$ increase to infinity, and $n=o(m)$. A new performance criterion based on large deviations analysis is proposed and it generalizes the classical error exponent applicable for large sample problems (in which $m=O(n)$). This generalized error exponent criterion provides insights that are not available from asymptotic consistency or central limit theorem analysis. The following results are established for the uniform null distribution: (i) The best achievable probability of error $P_e$ decays as $P_e=\exp\{-(n^2/m) J (1+o(1))\}$ for some $J>0$. (ii) A class of tests based on separable statistics, including the coincidence-based test, attains the optimal generalized error exponents. (iii) Pearson's chi-square test has a zero generalized error exponent and thus its probability of error is asymptotically larger than the optimal test.
BCS theory of hadronic matter at high densities<|sep|>The equilibrium between the so-called 2SC and CFL phases of strange quark matter at high densities is investigated in the framework of a simple schematic model of the NJL type. Equal densities are assumed for quarks $u,d$ and $s$. The 2SC phase is here described by a color-flavor symmetric state, in which the quark numbers are independent of the color-flavor combination. In the CFL phase the quark numbers depend on the color-flavor combination, that is, the number of quarks associated with the color-flavor combinations $ur,dg,sb$ is different from the number of quarks associated with the color flavor combinations $ug,ub,dr,db,sr,sg$. We find that the 2SC phase is stable for a chemical potential $\mu$ below $\mu_c=0.505$ GeV, while the CFL phase is stable above, the equilibrium pressure being $P_c=0.003$ GeV$^4$. We have used a 3-momentum regularizing cutoff $\Lambda=0.8$ GeV, which is somewhat larger than is usual in NJL type models. This should be adequate if the relevant chemical potential does not exceed 0.6 GeV.
Latent variable modeling with random features<|sep|>Gaussian process-based latent variable models are flexible and theoretically grounded tools for nonlinear dimension reduction, but generalizing to non-Gaussian data likelihoods within this nonlinear framework is statistically challenging. Here, we use random features to develop a family of nonlinear dimension reduction models that are easily extensible to non-Gaussian data likelihoods; we call these random feature latent variable models (RFLVMs). By approximating a nonlinear relationship between the latent space and the observations with a function that is linear with respect to random features, we induce closed-form gradients of the posterior distribution with respect to the latent variable. This allows the RFLVM framework to support computationally tractable nonlinear latent variable models for a variety of data likelihoods in the exponential family without specialized derivations. Our generalized RFLVMs produce results comparable with other state-of-the-art dimension reduction methods on diverse types of data, including neural spike train recordings, images, and text data.
Edge state inner products and real-space entanglement spectrum of trial quantum Hall states<|sep|>We consider the trial wavefunctions for the Fractional Quantum Hall Effect (FQHE) that are given by conformal blocks, and construct their associated edge excited states in full generality. The inner products between these edge states are computed in the thermodynamic limit, assuming generalized screening (i.e. short-range correlations only) inside the quantum Hall droplet, and using the language of boundary conformal field theory (boundary CFT). These inner products take universal values in this limit: they are equal to the corresponding inner products in the bulk 2d chiral CFT which underlies the trial wavefunction. This is a bulk/edge correspondence; it shows the equality between equal-time correlators along the edge and the correlators of the bulk CFT up to a Wick rotation. This approach is then used to analyze the entanglement spectrum (ES) of the ground state obtained with a bipartition A\cupB in real-space. Starting from our universal result for inner products in the thermodynamic limit, we tackle corrections to scaling using standard field-theoretic and renormalization group arguments. We prove that generalized screening implies that the entanglement Hamiltonian H_E = - log {\rho}_A is isospectral to an operator that is local along the cut between A and B. We also show that a similar analysis can be carried out for particle partition. We discuss the close analogy between the formalism of trial wavefunctions given by conformal blocks and Tensor Product States, for which results analogous to ours have appeared recently. Finally, the edge theory and entanglement spectrum of px + ipy paired superfluids are treated in a similar fashion in the appendix.
Fast Hybrid PSO and Tabu Search Approach for Optimization of a Fuzzy Controller<|sep|>In this paper, a fuzzy controller type Takagi_Sugeno zero order is optimized by the method of hybrid Particle Swarm Optimization (PSO) and Tabu Search (TS). The algorithm automatically adjusts the membership functions of fuzzy controller inputs and the conclusions of fuzzy rules. At each iteration of PSO, we calculate the best solution and we seek the best neighbor by Tabu search, this operation minimizes the number of iterations and computation time while maintaining accuracy and minimum response time. We apply this algorithm to optimize a fuzzy controller for a simple inverted pendulum with three rules.
Using arguments for making decisions: A possibilistic logic approach<|sep|>Humans currently use arguments for explaining choices which are already made, or for evaluating potential choices. Each potential choice has usually pros and cons of various strengths. In spite of the usefulness of arguments in a decision making process, there have been few formal proposals handling this idea if we except works by Fox and Parsons and by Bonet and Geffner. In this paper we propose a possibilistic logic framework where arguments are built from an uncertain knowledge base and a set of prioritized goals. The proposed approach can compute two kinds of decisions by distinguishing between pessimistic and optimistic attitudes. When the available, maybe uncertain, knowledge is consistent, as well as the set of prioritized goals (which have to be fulfilled as far as possible), the method for evaluating decisions on the basis of arguments agrees with the possibility theory-based approach to decision-making under uncertainty. Taking advantage of its relation with formal approaches to defeasible argumentation, the proposed framework can be generalized in case of partially inconsistent knowledge, or goal bases.
The structure of rotational bands in alpha-cluster nuclei<|sep|>In this contribution, I discuss an algebraic treatment of alpha-cluster nuclei based on the introduction of a spectrum generating algebra for the relative motion of the alpha-clusters. Particular attention is paid to the discrete symmetry of the geometric arrangement of the alpha-particles, and the consequences for the structure of the rotational bands in the 12C and 16O nuclei.
Descent Properties of an Anderson Accelerated Gradient Method With Restarting<|sep|>Anderson Acceleration (AA) is a popular acceleration technique to enhance the convergence of fixed-point iterations. The analysis of AA approaches typically focuses on the convergence behavior of a corresponding fixed-point residual, while the behavior of the underlying objective function values along the accelerated iterates is currently not well understood. In this paper, we investigate local properties of AA with restarting applied to a basic gradient scheme in terms of function values. Specifically, we show that AA with restarting is a local descent method and that it can decrease the objective function faster than the gradient method. These new results theoretically support the good numerical performance of AA when heuristic descent conditions are used for globalization and they provide a novel perspective on the convergence analysis of AA that is more amenable to nonconvex optimization problems. Numerical experiments are conducted to illustrate our theoretical findings.
Rotationally resolved spectroscopy of (20000) Varuna in the near-infrared<|sep|>Models of the escape and retention of volatiles by minor icy objects exclude any presence of volatile ices on the surface of TNOs smaller than ~1000km in diameter at the typical temperature in this region of the solar system, whereas the same models show that water ice is stable on the surface of objects over a wide range of diameters. Collisions and cometary activity have been used to explain the process of surface refreshing of TNOs and Centaurs. These processes can produce surface heterogeneity that can be studied by collecting information at different rotational phases. The aims of this work are to study the surface composition of (20000)Varuna, a TNO with a diameter ~650km and to search for indications of rotational variability. We observed Varuna during two consecutive nights in January 2011 with NICS@TNG obtaining a set of spectra covering the whole rotation period of Varuna. After studying the spectra corresponding to different rotational phases, we did not find any indication of surface variability. In all the spectra, we detect an absorption at 2{\mu}m, suggesting the presence of water ice on the surface. We do not detect any other volatiles on the surface, although the S/N is not high enough to discard their presence. Based on scattering models, we present two possible compositions compatible with our set of data and discuss their implications in the frame of the collisional history of the Kuiper Belt. We find that the most probable composition for the surface of Varuna is a mixture of amorphous silicates, complex organics, and water ice. This composition is compatible with all the materials being primordial. However, our data can also be fitted by models containing up to a 10% of methane ice. For an object with the characteristics of Varuna, this volatile could not be primordial, so an event, such as an energetic impact, would be needed to explain its presence on the surface.
Fourier Series-Based Approximation of Time-Varying Parameters Using the Ensemble Kalman Filter<|sep|>In this work, we propose a Fourier series-based approximation method using ensemble Kalman filtering to estimate time-varying parameters in deterministic dynamical systems. We demonstrate the capability of this approach in estimating both sinusoidal and polynomial forcing parameters in a mass-spring system. Results emphasize the importance of the choice of frequencies in the approximation model terms on the corresponding time-varying parameter estimates.
Medium scale anisotropy in the TeV cosmic ray flux observed by ARGO-YBJ<|sep|>Measuring the anisotropy of the arrival direction distribution of cosmic rays provides important information on the propagation mechanisms and the identification of their sources. In fact, the flux of cosmic rays is thought to be dependent on the arrival direction only due to the presence of nearby cosmic ray sources or particular magnetic-field structures. Recently, the observation of unexpected excesses at TeV energy down to angular scale as narrow as $\sim10\deg$ raised the possibility that the problem of the origin of galactic cosmic rays may be addressed by studying the anisotropy. The ARGO-YBJ experiment is a full-coverage EAS array, sensitive to cosmic rays with energy threshold of a few hundred GeV. Searching for small-size deviations from the isotropy, the ARGO-YBJ collaboration explored the declination region $\delta\sim-20^{\circ}\div 80^{\circ}$, making use of about 3.7$\cdot10^{11}$ events collected from November 2007 to May 2012. In this paper the detection of different significant (up to 13 standard deviations) medium-scale anisotropy regions in the arrival directions of CRs is reported. The observation was performed with unprecedented detail. The relative excess intensity with respect to the isotropic flux extends up to 10$^{-3}$. The maximum excess occurs for proton energies of 10-20 TeV, suggesting the presence of unknown features of the magnetic fields the charged cosmic rays propagate through, or some contribution of nearby sources never considered so far. The observation of new weaker few-degree excesses throughout the sky region $195^{\circ}\leq$ \ra $\leq 290^{\circ}$ is reported for the first time.
Chemical potential of an antiferromagnetic magnon gas<|sep|>Understanding the statistics of quasi-particle excitations in magnetic systems is essential for exploring new magnetic phases and collective quantum phenomena. While the chemical potential of a ferromagnetic gas has been extensively investigated both theoretically and experimentally, its antiferromagnetic counterpart remains uncharted. Here, we derive the statistics of a two-component U(1)-symmetric Bose gas and apply our results to an axially-symmetric antiferromagnetic insulator. We find that the two magnon eigenmodes of the system are described by an equal and opposite chemical potential, in analogy with a particle-antiparticle pair. Furthermore, we derive the thermomagnonic torques describing the interaction between the coherent and incoherent antiferromagnetic spin dynamics. Our results show that the magnitude and sign of the chemical potential can be tuned via an AC magnetic field driving resonantly one of the magnon modes. Finally, we propose NV-center relaxometry as a method to experimentally test our predictions.
Biomedical Interpretable Entity Representations<|sep|>Pre-trained language models induce dense entity representations that offer strong performance on entity-centric NLP tasks, but such representations are not immediately interpretable. This can be a barrier to model uptake in important domains such as biomedicine. There has been recent work on general interpretable representation learning (Onoe and Durrett, 2020), but these domain-agnostic representations do not readily transfer to the important domain of biomedicine. In this paper, we create a new entity type system and training set from a large corpus of biomedical texts by mapping entities to concepts in a medical ontology, and from these to Wikipedia pages whose categories are our types. From this mapping we derive Biomedical Interpretable Entity Representations(BIERs), in which dimensions correspond to fine-grained entity types, and values are predicted probabilities that a given entity is of the corresponding type. We propose a novel method that exploits BIER's final sparse and intermediate dense representations to facilitate model and entity type debugging. We show that BIERs achieve strong performance in biomedical tasks including named entity disambiguation and entity label classification, and we provide error analysis to highlight the utility of their interpretability, particularly in low-supervision settings. Finally, we provide our induced 68K biomedical type system, the corresponding 37 million triples of derived data used to train BIER models and our best performing model.
State refinements and coarse graining in a full theory embedding of loop quantum cosmology<|sep|>Bridging between descriptions involving few large and many small quantum numbers is the main open problem in loop quantum gravity. In other words, one would like to be able to represent the same physical system in terms of a few "coarse"' quantum numbers, while the effective dynamics at the coarse level should agree with the one induced by a description involving many small quantum numbers. Efforts to understand this relationship face the problem of the enormous computational complexity involved in evolving a generic state containing many quanta. In a cosmological context however, certain symmetry assumptions on the quantum states allow to simplify the problem. In this paper, we will show how quantum states describing a spatially flat homogeneous and isotropic universe can be refined and coarse grained. Invariance of the dynamics of the coarse observables is shown to require a certain scaling property (familiar from loop quantum cosmology) of the quantum states if no running of parameters is taken into account. The involved states are solutions to the Hamiltonian constraint when terms coming from spatial derivatives are neglected, i.e. one works in the approximation of non-interacting FRW patches. The technical means to arrive at this result are a version of loop quantum gravity based on variables inspired by loop quantum cosmology, as well as an exact solution to the quantum dynamics of loop quantum cosmology which extends to the full theory in the chosen approximation.
Imaging of SNR IC443 and W44 with the Sardinia Radio Telescope at 1.5 GHz and 7 GHz<|sep|>Observations of supernova remnants (SNRs) are a powerful tool for investigating the later stages of stellar evolution, the properties of the ambient interstellar medium, and the physics of particle acceleration and shocks. For a fraction of SNRs, multi-wavelength coverage from radio to ultra high-energies has been provided, constraining their contributions to the production of Galactic cosmic rays. Although radio emission is the most common identifier of SNRs and a prime probe for refining models, high-resolution images at frequencies above 5 GHz are surprisingly lacking, even for bright and well-known SNRs such as IC443 and W44. In the frameworks of the Astronomical Validation and Early Science Program with the 64-m single-dish Sardinia Radio Telescope, we provided, for the first time, single-dish deep imaging at 7 GHz of the IC443 and W44 complexes coupled with spatially-resolved spectra in the 1.5-7 GHz frequency range. Our images were obtained through on-the-fly mapping techniques, providing antenna beam oversampling and resulting in accurate continuum flux density measurements. The integrated flux densities associated with IC443 are S_1.5GHz = 134 +/- 4 Jy and S_7GHz = 67 +/- 3 Jy. For W44, we measured total flux densities of S_1.5GHz = 214 +/- 6 Jy and S_7GHz = 94 +/- 4 Jy. Spectral index maps provide evidence of a wide physical parameter scatter among different SNR regions: a flat spectrum is observed from the brightest SNR regions at the shock, while steeper spectral indices (up to 0.7) are observed in fainter cooling regions, disentangling in this way different populations and spectra of radio/gamma-ray-emitting electrons in these SNRs.
Deep Reinforcement Learning for Control of Probabilistic Boolean Networks<|sep|>Probabilistic Boolean Networks (PBNs) were introduced as a computational model for the study of complex dynamical systems, such as Gene Regulatory Networks (GRNs). Controllability in this context is the process of making strategic interventions to the state of a network in order to drive it towards some other state that exhibits favourable biological properties. In this paper we study the ability of a Double Deep Q-Network with Prioritized Experience Replay in learning control strategies within a finite number of time steps that drive a PBN towards a target state, typically an attractor. The control method is model-free and does not require knowledge of the network's underlying dynamics, making it suitable for applications where inference of such dynamics is intractable. We present extensive experiment results on two synthetic PBNs and the PBN model constructed directly from gene-expression data of a study on metastatic-melanoma.
Differentially private stochastic expectation propagation (DP-SEP)<|sep|>We are interested in privatizing an approximate posterior inference algorithm called Expectation Propagation (EP). EP approximates the posterior by iteratively refining approximations to the local likelihoods, and is known to provide better posterior uncertainties than those by variational inference (VI). However, EP needs a large memory to maintain all local approximates associated with each datapoint in the training data. To overcome this challenge, stochastic expectation propagation (SEP) considers a single unique local factor that captures the average effect of each likelihood term to the posterior and refines it in a way analogous to EP. In terms of privacy, SEP is more tractable than EP because at each refining step of a factor, the remaining factors are fixed and do not depend on other datapoints as in EP, which makes the sensitivity analysis straightforward. We provide a theoretical analysis of the privacy-accuracy trade-off in the posterior estimates under our method, called differentially private stochastic expectation propagation (DP-SEP). Furthermore, we demonstrate the performance of our DP-SEP algorithm evaluated on both synthetic and real-world datasets in terms of the quality of posterior estimates at different levels of guaranteed privacy.
Endochronic theory, non-linear kinematic hardening rule and generalized plasticity: a new interpretation based on generalized normality assumption<|sep|>A simple way to define the flow rules of plasticity models is the assumption of generalized normality associated with a suitable pseudo-potential function. This approach, however, is not usually employed to formulate endochronic theory and non-linear kinematic (NLK) hardening rules as well as generalized plasticity models. In this paper, generalized normality is used to give a new formulation of these classes of models. As a result, a suited pseudo-potential is introduced for endochronic models and a non-standard description of NLK hardening and generalized plasticity models is also provided. This new formulation allows for an effective investigation of the relationships between these three classes of plasticity models.
Braneworlds with field derivative coupling to the Einstein tensor<|sep|>In this paper, we investigate the Randall-Sundrum type braneworld models in the scalar-tensor theory with field derivative coupling to the Einstein tensor. We first formulate the generalized junction conditions of the metric and the scalar field on the timelike codimension-one hypersurface (=brane). With the use of these junction conditions, we then derive the Minkowski and de Sitter brane solutions embedded into the $Z_2$-symmetric five-dimensional anti-de Sitter (AdS) bulk spacetime. The configuration of the scalar field depends on the slice of the AdS spacetime. These branes are supported by the tension and not coupled to the scalar field. The Minkowski brane solution can be obtained when the brane tension is tuned to the bulk contributions. Once this tuning relation is broken, the de Sitter brane solutions are obtained. The de Sitter brane solutions have two branches. One has the smooth limit to the case where the scalar field becomes trivial, while the other branch does not. The latter branch has the upper bound on the brane tension, where the expansion rate vanishes. Finally, we investigate the low energy effective gravitational theory realized on the brane, which is given by the four-dimensional Einstein-scalar theory with the corrections from the bulk. In the case that there is no generation of the dark radiation from the bulk scalar field, we recover the the four-dimensional Einstein-scalar theory.
Evaluation of Similarity-based Explanations<|sep|>Explaining the predictions made by complex machine learning models helps users to understand and accept the predicted outputs with confidence. One promising way is to use similarity-based explanation that provides similar instances as evidence to support model predictions. Several relevance metrics are used for this purpose. In this study, we investigated relevance metrics that can provide reasonable explanations to users. Specifically, we adopted three tests to evaluate whether the relevance metrics satisfy the minimal requirements for similarity-based explanation. Our experiments revealed that the cosine similarity of the gradients of the loss performs best, which would be a recommended choice in practice. In addition, we showed that some metrics perform poorly in our tests and analyzed the reasons of their failure. We expect our insights to help practitioners in selecting appropriate relevance metrics and also aid further researches for designing better relevance metrics for explanations.
The Hvar survey for roAp stars: II. Final results (Research Note)<|sep|>The 60 known rapidly oscillating Ap (roAp) stars are excellent laboratories to test pulsation models in the presence of stellar magnetic fields. Our survey is dedicated to search for new group members in the Northern Hemisphere. We attempt to increase the number of known chemically peculiar stars that are known to be pulsationally unstable. About 40 h of new CCD photometric data of 21 roAp candidates, observed at the 1m Austrian-Croatian Telescope (Hvar Observatory) are presented. We carefully analysed these to search for pulsations in the frequency range of up to 10mHz. No new roAp star was detected among the observed targets. The distribution of the upper limits for roAp-like variations is similar to that of previoius similar efforts using photomultipliers and comparable telescope sizes. In addition to photometric observations, we need to consolidate spectroscopic information to select suitable targets.
Document Retrieval for Large Scale Content Analysis using Contextualized Dictionaries<|sep|>This paper presents a procedure to retrieve subsets of relevant documents from large text collections for Content Analysis, e.g. in social sciences. Document retrieval for this purpose needs to take account of the fact that analysts often cannot describe their research objective with a small set of key terms, especially when dealing with theoretical or rather abstract research interests. Instead, it is much easier to define a set of paradigmatic documents which reflect topics of interest as well as targeted manner of speech. Thus, in contrast to classic information retrieval tasks we employ manually compiled collections of reference documents to compose large queries of several hundred key terms, called dictionaries. We extract dictionaries via Topic Models and also use co-occurrence data from reference collections. Evaluations show that the procedure improves retrieval results for this purpose compared to alternative methods of key term extraction as well as neglecting co-occurrence data.
The role of multi-parton interactions in doubly-heavy hadron production<|sep|>Beauty and charm quarks are ideal probes of pertubative Quantum Chromodymanics in proton-proton collisions, owing to their large masses. In this paper the role of multi-parton interactions in the production of doubly-heavy hadrons is studied using simulation samples generated with Pythia, a Monte Carlo event generator. Comparisons are made to the stand-alone generators BcVegPy and GenXicc. New methods of speeding up Pythia simulations for events containing heavy quarks are described, enabling the production of large samples with multiple heavy-quark pairs. We show that significantly higher production rates of doubly-heavy hadrons are predicted in models that allow heavy quarks originating from different parton-parton interactions (within the same hadron-hadron collision) to combine to form such hadrons. Quantitative predictions are sensitive to the modelling of colour reconnections. We suggest a set of experimental measurements capable of differentiating these additional contributions.
Structured inversion of the Bernstein-Vandermonde Matrix<|sep|>Bernstein polynomials, long a staple of approximation theory and computational geometry, have also increasingly become of interest in finite element methods. Many fundamental problems in interpolation and approximation give rise to interesting linear algebra questions. When attempting to find a polynomial approximation of boundary or initial data, one encounters the Bernstein-Vandermonde matrix, which is found to be highly ill-conditioned. Previously, we used the relationship between monomial Bezout matrices and the inverse of Hankel matrices to obtain a decomposition of the inverse of the Bernstein mass matrix in terms of Hankel, Toeplitz, and diagonal matrices. In this paper, we use properties of the Bernstein-Bezout matrix to factor the inverse of the Bernstein-Vandermonde matrix into a difference of products of Hankel, Toeplitz, and diagonal matrices. We also use a nonstandard matrix norm to study the conditioning of the Bernstein-Vandermonde matrix, showing that the conditioning in this case is better than in the standard 2-norm. Additionally, we use properties of multivariate Bernstein polynomials to derive a block $LU$ decomposition of the Bernstein-Vandermonde matrix corresponding to equispaced nodes on the $d$-simplex.
Multipackings in Graphs<|sep|>A vertex subset M of a graph G is a multipacking if for each vertex v, and each positive integer s less than or equal to the diameter of G, v is within distance s of at most s vertices of M. The multipacking number of a graph is the maximum cardinality of a multipacking of G. A generalization of 2-packings, multipackings offer interesting insight into the minimum cost broadcast domination problem. This paper surveys recent results in the study of multipackings, including the equality of the multipacking number and broadcast number in trees, an extension of Farber's Algorithm for finding dominating sets and 2-packings of strongly chordal graphs, and some early results on fractional multipackings. The paper closes with a series of in-depth examples of the various algorithms presented.
You Can't Count on Luck: Why Decision Transformers Fail in Stochastic Environments<|sep|>Recently, methods such as Decision Transformer that reduce reinforcement learning to a prediction task and solve it via supervised learning (RvS) have become popular due to their simplicity, robustness to hyperparameters, and strong overall performance on offline RL tasks. However, simply conditioning a probabilistic model on a desired return and taking the predicted action can fail dramatically in stochastic environments since trajectories that result in a return may have only achieved that return due to luck. In this work, we describe the limitations of RvS approaches in stochastic environments and propose a solution. Rather than simply conditioning on the return of a single trajectory as is standard practice, our proposed method, ESPER, learns to cluster trajectories and conditions on average cluster returns, which are independent from environment stochasticity. Doing so allows ESPER to achieve strong alignment between target return and expected performance in real environments. We demonstrate this in several challenging stochastic offline-RL tasks including the challenging puzzle game 2048, and Connect Four playing against a stochastic opponent. In all tested domains, ESPER achieves significantly better alignment between the target return and achieved return than simply conditioning on returns. ESPER also achieves higher maximum performance than even the value-based baselines.
Statistical Topology of Three-Dimensional Poisson-Voronoi Cells and Cell Boundary Networks<|sep|>Voronoi tessellations of Poisson point processes are widely used for modeling many types of physical and biological systems. In this paper, we analyze simulated Poisson-Voronoi structures containing a total of 250,000,000 cells to provide topological and geometrical statistics of this important class of networks. We also report correlations between some of these topological and geometrical measures. Using these results, we are able to corroborate several conjectures regarding the properties of three-dimensional Poisson-Voronoi networks and refute others. In many cases, we provide accurate fits to these data to aid further analysis. We also demonstrate that topological measures represent powerful tools for describing cellular networks and for distinguishing among different types of networks.
Learning Stabilizable Dynamical Systems via Control Contraction Metrics<|sep|>We propose a novel framework for learning stabilizable nonlinear dynamical systems for continuous control tasks in robotics. The key idea is to develop a new control-theoretic regularizer for dynamics fitting rooted in the notion of stabilizability, which guarantees that the learned system can be accompanied by a robust controller capable of stabilizing any open-loop trajectory that the system may generate. By leveraging tools from contraction theory, statistical learning, and convex optimization, we provide a general and tractable semi-supervised algorithm to learn stabilizable dynamics, which can be applied to complex underactuated systems. We validated the proposed algorithm on a simulated planar quadrotor system and observed notably improved trajectory generation and tracking performance with the control-theoretic regularized model over models learned using traditional regression techniques, especially when using a small number of demonstration examples. The results presented illustrate the need to infuse standard model-based reinforcement learning algorithms with concepts drawn from nonlinear control theory for improved reliability.
Probabilistic solvers for partial differential equations<|sep|>This work is concerned with the quantification of the epistemic uncertainties induced the discretization of partial differential equations. Following the paradigm of probabilistic numerics, we quantify this uncertainty probabilistically. Namely, we develop a probabilistic solver suitable for linear partial differential equations (PDE) with mixed (Dirichlet and Neumann) boundary conditions defined on arbitrary geometries. The idea is to assign a probability measure on the space of solutions of the PDE and then condition this measure by enforcing that the PDE and the boundary conditions are satisfied at a finite set of spatial locations. The resulting posterior probability measure quantifies our state of knowledge about the solution of the problem given this finite discretization.
Model Checking for Modal Dependence Logic: An Approach Through Post's Lattice<|sep|>In this paper we investigate an extended version of modal dependence logic by allowing arbitrary Boolean connectives. Modal dependence logic was recently introduced by Jouko V\"a\"an\"anen by extending modal logic by a the dependence atom dep(.). In this paper we study the computational complexity of the model checking problem. For a complete classification of arbitrary Boolean functions we are using a Lattice approach introduced by Emil Post. This classification is done for all fragments of the logical language allowing modalities $\Diamond$ and $\Box$, the dependence atom, and logical symbols for arbitrary Boolean functions.
A systematic phenomenological study of the $\cos 2 \phi$ asymmetry in unpolarized semi--inclusive DIS<|sep|>We study the $\cos 2 \phi$ azimuthal asymmetry in unpolarized semi-inclusive DIS, taking into account both the perturbative contribution (gluon emission and splitting) and the non perturbative effects arising from intrinsic transverse motion and transverse spin of quarks. In particular we explore the possibility to extract from $<\cos 2 \phi>$ some information about the Boer--Mulders function $h_1^{\perp}$, which represents a transverse--polarization asymmetry of quarks inside an unpolarized hadron. Predictions are presented for the HERMES, COMPASS and JLab kinematics, where $<\cos 2 \phi>$ is dominated by the kinematical higher--twist contribution, and turns to be of order of few percent. We show that a larger asymmetry in $\pi^-$ production, compared to $\pi^+$ production, would represent a signature of the Boer--Mulders effect.
Giant optical oscillator strengths in perturbed hexagonal germanium<|sep|>We present ab initio calculations of electronic and optical properties of perturbed hexagonal germanium and demonstrate that it is a superior material for active optoelectronic devices in the infrared spectral region. It is known that perfect lonsdaleite Ge is a pseudodirect semiconductor, i.e., with direct fundamental band gap but almost vanishing oscillator strength for the lowest-energy optical transitions. Perturbing the system by replacing a Ge atom in the unit cell with a Si atom boosts of the oscillator strength at the minimum direct gap by orders of magnitude, with a concurrent blue shift of the interband distances. This effect is mainly due to the increased s character of the lowest conduction band because of the perturbation-induced wave function mixing. A purely structural modification of the lonsdaleite unit cell of hexagonal Ge yields as well increased optical oscillator strengths, but their magnitude significantly depends on the actual details of the atomic geometry. In particular, moderate tensile uniaxial strain can induce an inversion of the order of the two lowest conduction bands, immediately leading to an extremely efficient enhancement of optical transitions. In general, chemical and/or structural perturbations of the lonsdaleite lattice are shown to be the key to make hexagonal germanium suitable for light emitting devices.
Articulatory Features for ASR of Pathological Speech<|sep|>In this work, we investigate the joint use of articulatory and acoustic features for automatic speech recognition (ASR) of pathological speech. Despite long-lasting efforts to build speaker- and text-independent ASR systems for people with dysarthria, the performance of state-of-the-art systems is still considerably lower on this type of speech than on normal speech. The most prominent reason for the inferior performance is the high variability in pathological speech that is characterized by the spectrotemporal deviations caused by articulatory impairments due to various etiologies. To cope with this high variation, we propose to use speech representations which utilize articulatory information together with the acoustic properties. A designated acoustic model, namely a fused-feature-map convolutional neural network (fCNN), which performs frequency convolution on acoustic features and time convolution on articulatory features is trained and tested on a Dutch and a Flemish pathological speech corpus. The ASR performance of fCNN-based ASR system using joint features is compared to other neural network architectures such conventional CNNs and time-frequency convolutional networks (TFCNNs) in several training scenarios.
The Authors Matter: Understanding and Mitigating Implicit Bias in Deep Text Classification<|sep|>It is evident that deep text classification models trained on human data could be biased. In particular, they produce biased outcomes for texts that explicitly include identity terms of certain demographic groups. We refer to this type of bias as explicit bias, which has been extensively studied. However, deep text classification models can also produce biased outcomes for texts written by authors of certain demographic groups. We refer to such bias as implicit bias of which we still have a rather limited understanding. In this paper, we first demonstrate that implicit bias exists in different text classification tasks for different demographic groups. Then, we build a learning-based interpretation method to deepen our knowledge of implicit bias. Specifically, we verify that classifiers learn to make predictions based on language features that are related to the demographic attributes of the authors. Next, we propose a framework Debiased-TC to train deep text classifiers to make predictions on the right features and consequently mitigate implicit bias. We conduct extensive experiments on three real-world datasets. The results show that the text classification models trained under our proposed framework outperform traditional models significantly in terms of fairness, and also slightly in terms of classification performance.
Accelerating SARS-CoV-2 low frequency variant calling on ultra deep sequencing datasets<|sep|>With recent advances in sequencing technology it has become affordable and practical to sequence genomes to very high depth-of-coverage, allowing researchers to discover low-frequency variants in the genome. However, due to the errors in sequencing it is an active area of research to develop algorithms that can separate noise from the true variants. LoFreq is a state of the art algorithm for low-frequency variant detection but has a relatively long runtime compared to other tools. In addition to this, the interface for running in parallel could be simplified, allowing for multithreading as well as distributing jobs to a cluster. In this work we describe some specific contributions to LoFreq that remedy these issues.
Extrapolation Technique Pitfalls in Asymmetry Measurements at Colliders<|sep|>Asymmetry measurements are common in collider experiments and can sensitively probe particle properties. Typically, data can only be measured in a finite region covered by the detector, so an extrapolation from the visible asymmetry to the inclusive asymmetry is necessary. Often a constant multiplicative factor is more than adequate for the extrapolation and this factor can be readily determined using simulation methods. However, there is a potential, avoidable pitfall involved in the determination of this factor when the asymmetry in the simulated data sample is small. We find that to obtain a reliable estimate of the extrapolation factor, the number of simulated events required rises as the inverse square of the simulated asymmetry; this can mean that an unexpectedly large sample size is required when determining its value.
The Computational Theory of Intelligence: Information Entropy<|sep|>This paper presents an information theoretic approach to the concept of intelligence in the computational sense. We introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level. Using this new scheme, we develop a simple data driven clustering example and discuss its applications.
The Generic Superintegrable System on the 3-Sphere and the $9j$ Symbols of $\mathfrak{su}(1,1)$<|sep|>The $9j$ symbols of $\mathfrak{su}(1,1)$ are studied within the framework of the generic superintegrable system on the 3-sphere. The canonical bases corresponding to the binary coupling schemes of four $\mathfrak{su}(1,1)$ representations are constructed explicitly in terms of Jacobi polynomials and are seen to correspond to the separation of variables in different cylindrical coordinate systems. A triple integral expression for the $9j$ coefficients exhibiting their symmetries is derived. A double integral formula is obtained by extending the model to the complex three-sphere and taking the complex radius to zero. The explicit expression for the vacuum coefficients is given. Raising and lowering operators are constructed and are used to recover the relations between contiguous coefficients. It is seen that the $9j$ symbols can be expressed as the product of the vacuum coefficients and a rational function. The recurrence relations and the difference equations satisfied by the $9j$ coefficients are derived.
Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model<|sep|>The estimation of speaker characteristics such as age and height is a challenging task, having numerous applications in voice forensic analysis. In this work, we propose a bi-encoder transformer mixture model for speaker age and height estimation. Considering the wide differences in male and female voice characteristics such as differences in formant and fundamental frequencies, we propose the use of two separate transformer encoders for the extraction of specific voice features in the male and female gender, using wav2vec 2.0 as a common-level feature extractor. This architecture reduces the interference effects during backpropagation and improves the generalizability of the model. We perform our experiments on the TIMIT dataset and significantly outperform the current state-of-the-art results on age estimation. Specifically, we achieve root mean squared error (RMSE) of 5.54 years and 6.49 years for male and female age estimation, respectively. Further experiment to evaluate the relative importance of different phonetic types for our task demonstrate that vowel sounds are the most distinguishing for age estimation.
Keck Spectroscopy of the Coma Cluster Ultra-Diffuse Galaxy Y358: Dynamical Mass in a Wider Context<|sep|>We examine ultra-diffuse galaxies (UDGs) and their relation to non-UDGs in mass-radius-luminosity space. We begin by publishing Keck/KCWI spectroscopy for the Coma cluster UDG Y358, for which we measure both a recessional velocity and velocity dispersion. Our recessional velocity confirms association with the Coma cluster and Y358's status as a UDG. From our velocity dispersion (19 $\pm$ 3 km s$^{-1}$) we calculate a dynamical mass within the half-light radius which provides evidence for a core in Y358's dark matter halo. We compare this dynamical mass, along with those for globular cluster (GC)-rich/-poor UDGs in the literature, to mass profiles for isolated, gas-rich UDGs and UDGs in the NIHAO/FIRE simulations. We find GC-poor UDGs have dynamical masses similar to isolated, gas-rich UDGs, suggesting an evolutionary pathway may exist between the two. Conversely, GC-rich UDGs have dynamical masses too massive to be easily explained as the evolution of the isolated, gas-rich UDGs. The simulated UDGs match the dynamical masses of the GC-rich UDGs. However, once compared in stellar mass -- halo mass space, the FIRE/NIHAO simulated UDGs do not match the halo masses of either the isolated, gas-rich UDGs or the GC-rich UDGs at the same stellar mass. Finally, we supplement our data for Y358 with other UDGs that have measured velocity dispersions in the literature. We compare this sample to a wide range of non-UDGs in mass-radius-luminosity space, finding UDGs have a similar locus to non-UDGs of similar luminosity with the primary difference being their larger half-light radii.
Energy conservation issues in the numerical solution of the semilinear wave equation<|sep|>In this paper we discuss energy conservation issues related to the numerical solution of the nonlinear wave equation. As is well known, this problem can be cast as a Hamiltonian system that may be autonomous or not, depending on the specific boundary conditions at hand. We relate the conservation properties of the original problem to those of its semi-discrete version obtained by the method of lines. Subsequently, we show that the very same properties can be transferred to the solutions of the fully discretized problem, obtained by using energy-conserving methods in the HBVMs (Hamiltonian Boundary Value Methods) class. Similar arguments hold true for different types of Hamiltonian Partial Differential Equations, e.g., the nonlinear Schr\"odinger equation.
The origin of the 'blue tilt' of globular cluster populations in the E-MOSAICS simulations<|sep|>The metal-poor sub-population of globular cluster (GC) systems exhibits a correlation between the GC average colour and luminosity, especially in those systems associated with massive elliptical galaxies. More luminous (more massive) GCs are typically redder and hence more metal-rich. This 'blue tilt' is often interpreted as a mass-metallicity relation stemming from GC self-enrichment, whereby more massive GCs retain a greater fraction of the enriched gas ejected by their evolving stars, fostering the formation of more metal-rich secondary generations. We examine the E-MOSAICS simulations of the formation and evolution of galaxies and their GC populations, and find that their GCs exhibit a colour-luminosity relation similar to that observed in local galaxies, without the need to invoke mass-dependent self-enrichment. We find that the blue tilt is most appropriately interpreted as a dearth of massive, metal-poor GCs: the formation of massive GCs requires high interstellar gas surface densities, conditions that are most commonly fostered by the most massive, and hence most metal rich, galaxies, at the peak epoch of GC formation. The blue tilt is therefore a consequence of the intimate coupling between the small-scale physics of GC formation and the evolving properties of interstellar gas hosted by hierarchically-assembling galaxies.
Sequential Estimation Methods from Inclusion Principle<|sep|>In this paper, we propose new sequential estimation methods based on inclusion principle. The main idea is to reformulate the estimation problems as constructing sequential random intervals and use confidence sequences to control the associated coverage probabilities. In contrast to existing asymptotic sequential methods, our estimation procedures rigorously guarantee the pre-specified levels of confidence.
Massive charged-current coefficient functions in deep-inelastic scattering at NNLO and impact on strange-quark distributions<|sep|>We present details on calculation of next-to-next-to-leading order QCD corrections to massive charged-current coefficient functions in deep-inelastic scattering. Especially we focus on the application to charm-quark production in neutrino scattering on fixed target that can be measured via the dimuon final state. We construct a fast interface to the calculation so for any parton distributions the cross sections can be evaluated within milliseconds by using the pre-generated interpolation grids. We discuss agreements of various theoretical predicitons with the NuTeV and CCFR dimuon data and the impact of the results on determination of the strange-quark distributions.
A Multi-Scale Approach to Describe Electrical Impulses Propagating along Actin Filaments in both Intracellular and In-vitro Conditions<|sep|>An accurate and efficient characterization of the polyelectrolyte properties for cytoskeleton filaments are key to the molecular understanding of electrical signal propagation, bundle and network formation, as well as other relevant physicochemical processes associated with biological functions in eukaryotic cells and their potential nanotechnological applications. In this article, we introduce an innovative multi-scale approach able to account for the atomistic details of a proteins molecular structure, its biological environment, and their impact on electrical impulses propagating along wild type Actin filaments. The approach provides a novel, simple, accurate, approximate analytic expression for the characterization of electrical impulses in the shape of soliton waveforms. It has been successfully used to determine the effects of electrolyte conditions and voltage stimulus on the electrical impulse shape, attenuation and kern propagation velocity in these systems. Our results predict the propagation of electrical signal impulses in the form of solitons for the range of voltage stimulus and electrolyte solutions typically present in intracellular and in-vitro conditions. This multi-scale theory may also be applicable to other highly charged rod-like polyelectrolytes with relevance in biomedicine and biophysics. It is also able to account for molecular structure conformation (mutation) and biological environment (protonations/deprotonations) changes often present in pathological conditions.
Implicit U-Net for volumetric medical image segmentation<|sep|>U-Net has been the go-to architecture for medical image segmentation tasks, however computational challenges arise when extending the U-Net architecture to 3D images. We propose the Implicit U-Net architecture that adapts the efficient Implicit Representation paradigm to supervised image segmentation tasks. By combining a convolutional feature extractor with an implicit localization network, our implicit U-Net has 40% less parameters than the equivalent U-Net. Moreover, we propose training and inference procedures to capitalize sparse predictions. When comparing to an equivalent fully convolutional U-Net, Implicit U-Net reduces by approximately 30% inference and training time as well as training memory footprint while achieving comparable results in our experiments with two different abdominal CT scan datasets.
Randomized Aperture Imaging<|sep|>Speckled images of a binary broad band light source (600-670 nm), generated by randomized reflections or transmissions, were used to reconstruct a binary image by use of multi-frame blind deconvolution algorithms. Craft store glitter was used as reflective elements. Another experiment used perforated foil. Also reported here are numerical models that afforded controlled tip-tilt and piston aberrations. These results suggest the potential importance of a poorly figured, randomly varying segmented imaging system.
Event Selection Rules to Compute Explanations<|sep|>Explanations have been introduced in the previous century. Their interest in reducing the search space is no longer questioned. Yet, their efficient implementation into CSP solver is still a challenge. In this paper, we introduce ESeR, an Event Selection Rules algorithm that filters events generated during propagation. This dynamic selection enables an efficient computation of explanations for intelligent backtracking al- gorithms. We show the effectiveness of our approach on the instances of the last three MiniZinc challenges
On pricing kernels, information and risk<|sep|>We discuss the finding that cross-sectional characteristic based models have yielded portfolios with higher excess monthly returns but lower risk than their arbitrage pricing theory counterparts in an analysis of equity returns of stocks listed on the JSE. Under the assumption of general no-arbitrage conditions, we argue that evidence in favour of characteristic based pricing implies that information is more likely assimilated by means of nonlinear pricing kernels for the markets considered.
A variational approach to repulsively interacting three-fermion systems in a one-dimensional harmonic trap<|sep|>We study a three-body system with zero-range interactions in a one-dimensional harmonic trap. The system consists of two spin-polarized fermions and a third particle which is distinct from two others (2+1 system). First we assume that the particles have equal masses. For this case the system in the strongly and weakly interacting limits can be accurately described using wave function factorized in hyperspherical coordinates. Inspired by this result we propose an interpolation ansatz for the wave function for arbitrary repulsive zero-range interactions. By comparison to numerical calculations, we show that this interpolation scheme yields an extremely good approximation to the numerically exact solution both in terms of the energies and also in the spin-resolved densities. As an outlook, we discuss the case of mass imbalanced systems in the strongly interacting limit. Here we find spectra that demonstrate that the triply degenerate spectrum at infinite coupling strength of the equal mass case is in some sense a singular case as this degeneracy will be broken down to a doubly degenerate or non-degenerate ground state by any small mass imbalance.
Structure of Core-Periphery Communities<|sep|>It has been experimentally shown that communities in social networks tend to have a core-periphery topology. However, there is still a limited understanding of the precise structure of core-periphery communities in social networks including the connectivity structure and interaction rates between agents. In this paper, we use a game-theoretic approach to derive a more precise characterization of the structure of core-periphery communities.
Nonclassical phase-space trajectories for the damped harmonic quantum oscillator<|sep|>The phase-space path-integral approach to the damped harmonic oscillator is analyzed beyond the Markovian approximation. It is found that pairs of nonclassical trajectories contribute to the path-integral representation of the Wigner propagating function. Due to the linearity of the problem, the sum coordinate of a pair still satisfies the classical equation of motion. Furthermore, it is shown that the broadening of the Wigner propagating function of the damped oscillator arises due to the time-nonlocal interaction mediated by the heat bath.
Analysis of the $Z_c(4200)$ as axial-vector molecule-like state<|sep|>In this article, we assume the $Z_c(4200)$ as the color octet-octet type axial-vector molecule-like state, and construct the color octet-octet type axial-vector current to study its mass and width with the QCD sum rules. The numerical values $M_{Z_c(4200)}=4.19 \pm 0.08\,\rm{GeV}$ and $\Gamma_{Z_c(4200)}\approx 334\,\rm{MeV}$ are consistent with the experimental data $M_{Z_c(4200)} = 4196^{+31}_{-29}{}^{+17}_{-13} \,\rm{MeV}$ and $\Gamma_{Z_c(4200)} = 370^{+70}_{-70}{}^{+70}_{-132}\,\rm{MeV}$, and support assigning the $Z_c(4200)$ to be the color octet-octet type molecule-like state with $J^{PC}=1^{+-}$. Furthermore, we discuss the possible assignments of the $Z_c(3900)$, $Z_c(4200)$ and $Z(4430)$ as the diquark-antidiquark type tetraquark states with $J^{PC}=1^{+-}$.
Artap: Robust Design Optimization Framework for Engineering Applications<|sep|>The main goal of the Artap project is to provide an extensive infrastructure for robust design optimization, where usually many different numerical solvers have to be used together and the impact of the manufacturing uncertainties have to be minimized. Artap is an open-source software platform, developed jointly with the coupled numerical field solver, Agros Suite. Artap ensures interfaces for a broad collection of optimization algorithms (genetic and evolutionary algorithms, various interfaces to libraries such as Nlopt, Bayesopt, etc .), tools for machine learning (neural networks, Gaussian processes, etc. ), finite element solvers (Agros Suite, Comsol, Multiphysics, deal.II). The implemented tools offers an easy and straightforward solution not only for robust design optimization but parameter identification, model order reduction, and shape optimization, as well. Moreover, Artap provides automatic parallelization of the optimization process. The paper presents the structure of the framework and technologies powering the project. The main features of Artap are demonstrated on an induction brazing process design tasks.
RAFT I: Discovery of new planetary candidates and updated orbits from archival FEROS spectra<|sep|>A recent reanalysis of archival data has lead several authors to arrive at strikingly different conclusions for a number of planet-hosting candidate stars. In particular, some radial velocities measured using FEROS spectra have been shown to be inaccurate, throwing some doubt on the validity of a number of planet detections. Motivated by these results, we have begun the Reanalysis of Archival FEROS specTra (RAFT) program and here we discuss the first results from this work. We have reanalyzed FEROS data for the stars HD 11977, HD 47536, HD 70573, HD 110014 and HD 122430, all of which are claimed to have at least one planetary companion. We have reduced the raw data and computed the radial velocity variations of these stars, achieving a long-term precision of $\sim$ 10 m/s on the known stable star tau Ceti, and in good agreement with the residuals to our fits. We confirm the existence of planets around HD 11977, HD 47536 and HD 110014, but with different orbital parameters than those previously published. In addition, we found no evidence of the second planet candidate around HD 47536, nor any companions orbiting HD 122430 and HD 70573. Finally, we report the discovery of a second planet around HD 110014, with a minimum mass of 3.1 Mjup and a orbital period of 130 days. Analysis of activity indicators allow us to confirm the reality of our results and also to measure the impact of magnetic activity on our radial velocity measurements. These results confirm that very metal-poor stars down to [Fe/H]$\sim$ -0.7 dex, can indeed form giant planets given the right conditions.
A Nonlinear Moment Model for Radiative Transfer Equation in Slab Geometry<|sep|>This paper is concerned with the approximation of the radiative transfer equation for a grey medium in the slab geometry by the moment method. We develop a novel moment model inspired by the classical $P_N$ model and $M_N$ model. The new model takes the ansatz of the $M_1$ model as the weight function and follows the primary idea of the $P_N$ model to approximate the specific intensity by expanding it around the weight function in terms of orthogonal polynomials. The weight function uses the information of the first two moments, which brings the new model the capability to approximate an anisotropic distribution. Mathematical properties of the moment model are investigated, and particularly the hyperbolicity and the characteristic structure of the Riemann problem of the model with three moments are studied in detail. Some numerical simulations demonstrate its numerical efficiency and show its superior in comparison to the $P_N$ model.
Followers Are Not Enough: A Question-Oriented Approach to Community Detection in Online Social Networks<|sep|>Community detection in online social networks is typically based on the analysis of the explicit connections between users, such as "friends" on Facebook and "followers" on Twitter. But online users often have hundreds or even thousands of such connections, and many of these connections do not correspond to real friendships or more generally to accounts that users interact with. We claim that community detection in online social networks should be question-oriented and rely on additional information beyond the simple structure of the network. The concept of 'community' is very general, and different questions such as "whom do we interact with?" and "with whom do we share similar interests?" can lead to the discovery of different social groups. In this paper we focus on three types of communities beyond structural communities: activity-based, topic-based, and interaction-based. We analyze a Twitter dataset using three different weightings of the structural network meant to highlight these three community types, and then infer the communities associated with these weightings. We show that the communities obtained in the three weighted cases are highly different from each other, and from the communities obtained by considering only the unweighted structural network. Our results confirm that asking a precise question is an unavoidable first step in community detection in online social networks, and that different questions can lead to different insights about the network under study.
The Plebanski sectors of the EPRL vertex<|sep|>Modern spin-foam models of four dimensional gravity are based on a discrete version of the $Spin(4)$ Plebanski formulation. Beyond what is already in the literature, we clarify the meaning of different Plebanski sectors in this classical discrete model. We show that the linearized simplicity constraints used in the EPRL and FK models are not sufficient to impose a restriction to a single Plebanski sector, but rather, three Plebanski sectors are mixed. We propose this as the reason for certain extra `undesired' terms in the asymptotics of the EPRL vertex analyzed by Barrett et al. This explanation for the extra terms is new and different from that sometimes offered in the spin-foam literature thus far.
Multi-Phase Dusty Gas in the Center of NGC 4278<|sep|>We present the Spitzer spectroscopic mapping observations toward the central kpc region of the nearby elliptical LINER galaxy, NGC 4278, by using the Infrared Spectrograph (IRS). These observations reveal rich mid-IR emission features of extended ionized gas, warm molecular hydrogen and dust. Different phases of gas and dust are closely related and belong to a same elongated feature. We further study properties of multi-phase dusty gas to uncover the underlying mechanism of ionization and excitation. The band ratio and intensity of PAH features in the central region might reflect modified size distribution resulted from selective destruction. H$_{2}$ S(0)-S(7) pure rotational lines of molecular hydrogen show excessive intensity and moderately high excitation temperature comparing with photon dissociation region (PDR). A strong and extended [SiII] emission line is detected, which could be a sign of reduced depletion of silicon in interstellar dust. We also discover an extended high ionization region associated with enhanced H$_{2}$ S(1) emission. We conclude that a shock-heating component is required to account for observed emission characteristics, which could be triggered by cloud-cloud interactions during accretion of cold gas from the large HI disk.
Quasi-Two Dimensional Spin and Phonon Excitations in $La_{1.965}Ba_{0.035}CuO_{4}$<|sep|>We present time-of-flight inelastic neutron scattering measurements of $La_{1.965}Ba_{0.035}CuO_{4}$ (LBCO), a lightly doped member of the high temperature superconducting La-based cuprate family. By using time-of-flight neutron instrumentation coupled with single crystal sample rotation we obtain a four-dimensional data set (three {\bf Q} and one energy) that is both comprehensive and spans a large region of reciprocal space. Our measurements identify rich structure in the energy dependence of the highly dispersive spin excitations, which are centered at equivalent ($\frac{1}{2},\frac{1}{2}, L$) wave-vectors. These structures correlate strongly with several crossings of the spin excitations with the lightly dispersive phonons found in this system. These effects are significant and account for on the order of 25$\%$ of the total inelastic scattering for energies between $\sim$5 and 40 meV at low $|{\bf Q}|$. Interestingly, this scattering also presents little or no $L$-dependence. As the phonons and dispersive spin excitations centered at equivalent ($\frac{1}{2},\frac{1}{2}, L$) wave-vectors are common to all members of La-based 214 copper oxides, we conclude such strong quasi-two dimensional scattering enhancements are likely to occur in all such 214 families of materials, including those concentrations corresponding to superconducting ground states. Such a phenomenon appears to be a fundamental characteristic of these materials and is potentially related to superconducting pairing.
Near-Dirichlet quantum dynamics for a $p^3$-corrected particle on an interval<|sep|>We study a nonrelativistic quantum mechanical particle on an interval of finite length with a Hamiltonian that has a $p^3$ correction term, modelling potential low energy quantum gravity effects. We describe explicitly the $U(3)$ family of the self-adjoint extensions of the Hamiltonian and discuss several subfamilies of interest. As the main result, we find a family of self-adjoint Hamiltonians, indexed by four continuous parameters and one binary parameter, whose spectrum and eigenfunctions are perturbatively close to those of the uncorrected particle with Dirichlet boundary conditions, even though the Dirichlet condition as such is not in the $U(3)$ family. Our boundary conditions do not single out distinguished discrete values for the length of the interval in terms of the underlying quantum gravity scale.
On Finding Gray Pixels<|sep|>We propose a novel grayness index for finding gray pixels and demonstrate its effectiveness and efficiency in illumination estimation. The grayness index, GI in short, is derived using the Dichromatic Reflection Model and is learning-free. GI allows to estimate one or multiple illumination sources in color-biased images. On standard single-illumination and multiple-illumination estimation benchmarks, GI outperforms state-of-the-art statistical methods and many recent deep methods. GI is simple and fast, written in a few dozen lines of code, processing a 1080p image in ~0.4 seconds with a non-optimized Matlab code.
Ratings and rankings: Voodoo or Science?<|sep|>Composite indicators aggregate a set of variables using weights which are understood to reflect the variables' importance in the index. In this paper we propose to measure the importance of a given variable within existing composite indicators via Karl Pearson's `correlation ratio'; we call this measure `main effect'. Because socio-economic variables are heteroskedastic and correlated, (relative) nominal weights are hardly ever found to match (relative) main effects; we propose to summarize their discrepancy with a divergence measure. We further discuss to what extent the mapping from nominal weights to main effects can be inverted. This analysis is applied to five composite indicators, including the Human Development Index and two popular league tables of university performance. It is found that in many cases the declared importance of single indicators and their main effect are very different, and that the data correlation structure often prevents developers from obtaining the stated importance, even when modifying the nominal weights in the set of nonnegative numbers with unit sum.
Breakdown of fiber bundles with stochastic load-redistribution<|sep|>We study fracture processes within a stochastic fiber-bundle model where it is assumed that after the failure of a fiber, each intact fiber obtains a random fraction of the failing load. Within a Markov approximation, the breakdown properties of this model can be reduced to the solution of an integral equation. As examples we consider two different versions of this model that both can interpolate between global and local load redistribution. For the strength thresholds of the individual fibers, we consider a Weibull distribution and a uniform distribution, both truncated below a given initial stress. The breakdown behavior of our models is compared with corresponding results of other fiber-bundle models.
Evolution of primordial magnetic fields during large-scale structure formation<|sep|>Primordial magnetic fields could explain the large-scale magnetic fields present in the Universe. Inflation and phase transitions in the early Universe could give rise to such fields with unique characteristics. We investigate the magneto-hydrodynamic evolution of these magnetogenesis scenarios with cosmological simulations. We evolve inflation-generated magnetic fields either as (i) uniform (homogeneous) or as (ii) scale-invariant stochastic fields, and phase transition-generated ones either as (iii) helical or as (iv) non-helical fields from the radiation-dominated epoch. We find that the final distribution of magnetic fields in the simulated cosmic web shows a dependence on the initial strength and the topology of the seed field. Thus, the observed field configuration retains information on the initial conditions at the moment of the field generation. If detected, primordial magnetic field observations would open a new window for indirect probes of the early universe. The differences between the competing models are revealed on the scale of galaxy clusters, bridges, as well as filaments and voids. The distinctive spectral evolution of different seed fields produces imprints on the correlation length today. We discuss how the differences between rotation measures from highly ionized regions can potentially be probed with forthcoming surveys.
Measurement errors and scaling relations in astrophysics: a review<|sep|>This review article considers some of the most common methods used in astronomy for regressing one quantity against another in order to estimate the model parameters or to predict an observationally expensive quantity using trends between object values. These methods have to tackle some of the awkward features prevalent in astronomical data, namely heteroscedastic (point-dependent) errors, intrinsic scatter, non-ignorable data collection and selection effects, data structure and non-uniform population (often called Malmquist bias), non-Gaussian data, outliers and mixtures of regressions. We outline how least square fits, weighted least squares methods, Maximum Likelihood, survival analysis, and Bayesian methods have been applied in the astrophysics literature when one or more of these features is present. In particular we concentrate on errors-in-variables regression and we advocate Bayesian techniques.
Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs<|sep|>This paper studies few-shot relation extraction, which aims at predicting the relation for a pair of entities in a sentence by training with a few labeled examples in each relation. To more effectively generalize to new relations, in this paper we study the relationships between different relations and propose to leverage a global relation graph. We propose a novel Bayesian meta-learning approach to effectively learn the posterior distribution of the prototype vectors of relations, where the initial prior of the prototype vectors is parameterized with a graph neural network on the global relation graph. Moreover, to effectively optimize the posterior distribution of the prototype vectors, we propose to use the stochastic gradient Langevin dynamics, which is related to the MAML algorithm but is able to handle the uncertainty of the prototype vectors. The whole framework can be effectively and efficiently optimized in an end-to-end fashion. Experiments on two benchmark datasets prove the effectiveness of our proposed approach against competitive baselines in both the few-shot and zero-shot settings.
Soft-photon radiation in high-energy proton-proton collisions within the tensor-Pomeron approach: Bremsstrahlung<|sep|>We discuss diffractive processes in proton-proton collisions at small momentum transfers without and with photon radiation. We consider the soft exclusive reactions $pp \to pp$, $p\bar{p} \to p\bar{p}$, and $pp \to pp \gamma$ within the tensor-pomeron and vector-odderon approach. We compare our results with the data for $pp$ and $p\bar{p}$ total cross sections, for the ratio of real to imaginary part of the forward scattering amplitude, and for the elastic $pp$ cross sections, especially those from TOTEM. To describe the low-energy data more accurately the secondary reggeons must be included. We write down the amplitudes for the photon bremsstrahlung in high-energy proton-proton collisions using the tensor-pomeron model. These results are relevant for the c.m. energies presently available at the Relativistic Heavy Ion Collider and at the LHC. We present predictions for the proposed measurements of soft photons with the planned future upgrade of the ALICE experiment at the LHC. We investigate the limits of applicability of the soft-photon approximation (SPA) based on Low's theorem. The corresponding SPA results are compared to those obtained from our complete model. The regions of phase space are given quantitatively where SPA and our complete tensor-pomeron results are close to each other. As an example, let $k_{\perp}$, $\rm y$, and $\omega$, be the absolute value of the transverse momentum, the rapidity, and the energy of the photon, respectively, in the overall c.m. system. For the region $1 \; {\rm MeV} < k_{\perp} < 100 \; {\rm MeV}$ and $3.5 < |\rm y| < 5.0$, we find that the SPA Ansatz with only the pole terms $\propto \omega^{-1}$ agrees at the percent level with our complete model result up to $\omega \cong 2$ GeV.
Dissipation in dynamos at low and high magnetic Prandtl numbers<|sep|>Using simulations of helically driven turbulence, it is shown that the ratio of kinetic to magnetic energy dissipation scales with the magnetic Prandtl number in power law fashion with an exponent of approximately 0.6. Over six orders of magnitude in the magnetic Prandtl number the magnetic field is found to be sustained by large-scale dynamo action of alpha-squared type. This work extends a similar finding for small magnetic Prandtl numbers to the regime of large magnetic Prandtl numbers. At large magnetic Prandtl numbers, most of the energy is dissipated viscously, lowering thus the amount of magnetic energy dissipation, which means that simulations can be performed at magnetic Reynolds numbers that are large compared to the usual limits imposed by a given resolution. This is analogous to an earlier finding that at small magnetic Prandtl numbers, most of the energy is dissipated resistively, lowering the amount of kinetic energy dissipation, so simulations can then be performed at much larger fluid Reynolds numbers than otherwise. The decrease in magnetic energy dissipation at large magnetic Prandtl numbers is discussed in the context of underluminous accretion found in some quasars.
Iterative symmetry search and reduction of a wave water model in $2+1$ dimensions<|sep|>We present the iterative classical point symmetry analysis of a shallow water wave equation in $2+1$ dimensions and that of its corresponding nonisospectral, two component Lax pair. A few reductions arise and are identified with celebrate equations in the Physics and Mathematics literature of nonlinear waves. We pay particular attention to the isospectral or nonisospectral nature of the reduced spectral problems.
Back to the Future: Efficient, Time-Consistent Solutions in Reach-Avoid Games<|sep|>We study the class of reach-avoid dynamic games in which multiple agents interact noncooperatively, and each wishes to satisfy a distinct target criterion while avoiding a failure criterion. Reach-avoid games are commonly used to express safety-critical optimal control problems found in mobile robot motion planning. Here, we focus on finding time-consistent solutions, in which future motion plans remain optimal even when a robot diverges from the plan early on due to, e.g., intrinsic dynamic uncertainty or extrinsic environment disturbances. Our main contribution is a computationally-efficient algorithm for multi-agent reach-avoid games which renders time-consistent solutions for all players. We demonstrate our approach in two- and three-player simulated driving scenarios, in which our method provides safe control strategies for all agents.
Holographic sliding stripes<|sep|>Holographic models provide unique laboratories to investigate non-linear physics of transport in inhomogeneous systems. We provide a detailed account of both DC and AC conductivities in a defect CFT with spontaneous stripe order. The spatial symmetry is broken at large chemical potential and the resulting ground state is a combination of a spin and charge density wave. An infinitesimal applied electric field across the stripes will cause the stripes to slide over the underlying density of smeared impurities, a phenomenon which can be associated with the Goldstone mode for the spontaneously broken translation symmetry. We show that the presence of a spatially modulated background magnetization current thwarts the expression of some DC conductivities in terms of horizon data.
Implementation of PDE models of cardiac dynamics on GPUs using OpenCL<|sep|>Graphical processing units (GPUs) promise to revolutionize scientific computing in the near future. Already, they allow almost real-time integration of simplified numerical models of cardiac tissue dynamics. However, the integration methods that have been developed so far are typically of low order and use single precision arithmetics. In this work, we describe numerical implementation of double precision integrators required by, e.g., matrix-free Newton-Krylov solvers and compare several higher order, fully explicit numerical methods using finite-difference discretization of a range of models of two-dimensional cardiac tissue.
Reconstruction of the electric field of the Helmholtz equation in 3D<|sep|>In this paper, we rigorously investigate the truncation method for the Cauchy problem of Helmholtz equations which is widely used to model propagation phenomena in physical applications. The method is a well-known approach to the regularization of several types of ill-posed problems, including the model postulated by Regi\' nska and Regi\' nski \cite{RR06}. Under certain specific assumptions, we examine the ill-posedness of the non-homogeneous problem by exploring the representation of solutions based on Fourier mode. Then the so-called regularized solution is established with respect to a frequency bounded by an appropriate regularization parameter. Furthermore, we provide a short analysis of the nonlinear forcing term. The main results show the stability as well as the strong convergence confirmed by the error estimates in $L^2$-norm of such regularized solutions. Besides, the regularization parameters are formulated properly. Finally, some illustrative examples are provided to corroborate our qualitative analysis.
Distant Supervision for E-commerce Query Segmentation via Attention Network<|sep|>The booming online e-commerce platforms demand highly accurate approaches to segment queries that carry the product requirements of consumers. Recent works have shown that the supervised methods, especially those based on deep learning, are attractive for achieving better performance on the problem of query segmentation. However, the lack of labeled data is still a big challenge for training a deep segmentation network, and the problem of Out-of-Vocabulary (OOV) also adversely impacts the performance of query segmentation. Different from query segmentation task in an open domain, e-commerce scenario can provide external documents that are closely related to these queries. Thus, to deal with the two challenges, we employ the idea of distant supervision and design a novel method to find contexts in external documents and extract features from these contexts. In this work, we propose a BiLSTM-CRF based model with an attention module to encode external features, such that external contexts information, which can be utilized naturally and effectively to help query segmentation. Experiments on two datasets show the effectiveness of our approach compared with several kinds of baselines.
Dodging the Data Bottleneck: Automatic Subtitling with Automatically Segmented ST Corpora<|sep|>Speech translation for subtitling (SubST) is the task of automatically translating speech data into well-formed subtitles by inserting subtitle breaks compliant to specific displaying guidelines. Similar to speech translation (ST), model training requires parallel data comprising audio inputs paired with their textual translations. In SubST, however, the text has to be also annotated with subtitle breaks. So far, this requirement has represented a bottleneck for system development, as confirmed by the dearth of publicly available SubST corpora. To fill this gap, we propose a method to convert existing ST corpora into SubST resources without human intervention. We build a segmenter model that automatically segments texts into proper subtitles by exploiting audio and text in a multimodal fashion, achieving high segmentation quality in zero-shot conditions. Comparative experiments with SubST systems respectively trained on manual and automatic segmentations result in similar performance, showing the effectiveness of our approach.
Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood<|sep|>We consider probabilistic multinomial probit classification using Gaussian process (GP) priors. The challenges with the multiclass GP classification are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classification rely on numerical quadratures or independence assumptions between the latent values from different classes to facilitate the computations. In this paper, we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all between-class posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method with respect to MCMC sampling, but the differences between the compared methods were small if only the classification accuracy is concerned.
Absorption of a Particle by a Rotating Black Hole: The Potential Barrier<|sep|>For a test particle approaching a rapidly rotating black hole we find a range of values of the particle's energy and angular momentum, on the order of 1\% or more of the corresponding values of the hole, such that three conditions are satisfied. 1) The particle can reach the horizon. 2) After absorption the new hole still has a horizon. 3) The area of the new hole is less than the area of the original one, in apparent violation of a theorem of Hawking. We offer support for the claim that the test particle approximation is the cause of the violation.
Effects of dust abundance on the far-infrared colours of blue compact dwarf galaxies<|sep|>We investigate the FIR properties of a sample of BCDs observed by AKARI. By utilizing the data at wavelengths of $\lambda =65 \mu$m, 90 $\mu$m, and 140 $\mu$m, we find that the FIR colours of the BCDs are located at the natural high-temperature extension of those of the Milky Way and the Magellanic Clouds. This implies that the optical properties of dust in BCDs are similar to those in the Milky Way. Indeed, we explain the FIR colours by assuming the same grain optical properties, which may be appropriate for amorphous dust grains, and the same size distribution as those adopted for the Milky Way dust. Since both interstellar radiation field and dust optical depth affect the dust temperature, it is difficult to distinguish which of these two physical properties is responsible for the change of FIR colours. Then, in order to examine if the dust optical depth plays an important role in determining the dust temperature, we investigate the correlation between FIR colour (dust temperature) and dust-to-gas ratio. We find that the dust temperature tends to be high as the dust-to-gas ratio decreases but that this trend cannot be explained by the effect of dust optical depth. Rather, it indicates a correlation between dust-to-gas ratio and interstellar radiation field. Although the metallicity may also play a role in this correlation, we suggest that the dust optical depth could regulate the star formation activities, which govern the interstellar radiation field. We also mention the importance of submillimetre data in tracing the emission from highly shielded low-temperature dust.
V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control<|sep|>Some of the most successful applications of deep reinforcement learning to challenging domains in discrete and continuous control have used policy gradient methods in the on-policy setting. However, policy gradients can suffer from large variance that may limit performance, and in practice require carefully tuned entropy regularization to prevent policy collapse. As an alternative to policy gradient algorithms, we introduce V-MPO, an on-policy adaptation of Maximum a Posteriori Policy Optimization (MPO) that performs policy iteration based on a learned state-value function. We show that V-MPO surpasses previously reported scores for both the Atari-57 and DMLab-30 benchmark suites in the multi-task setting, and does so reliably without importance weighting, entropy regularization, or population-based tuning of hyperparameters. On individual DMLab and Atari levels, the proposed algorithm can achieve scores that are substantially higher than has previously been reported. V-MPO is also applicable to problems with high-dimensional, continuous action spaces, which we demonstrate in the context of learning to control simulated humanoids with 22 degrees of freedom from full state observations and 56 degrees of freedom from pixel observations, as well as example OpenAI Gym tasks where V-MPO achieves substantially higher asymptotic scores than previously reported.
Design and characterization of TES bolometers and SQUID readout electronics for a balloon-borne application<|sep|>We present measurements of the electrical and thermal properties of new arrays of bolometeric detectors that were fabricated as part of a program to develop bolometers optimized for the low photon background of the EBEX balloon-borne experiment. An array consists of 140 spider-web transition edge sensor bolometers microfabricated on a 4" diameter silicon wafer. The designed average thermal conductance of bolometers on a proto-type array is 32 pW/K, and measurements are in good agreement with this value. The measurements are taken with newly developed, digital frequency domain multiplexer SQUID readout electronics.
Deep learning investigation for chess player attention prediction using eye-tracking and game data<|sep|>This article reports on an investigation of the use of convolutional neural networks to predict the visual attention of chess players. The visual attention model described in this article has been created to generate saliency maps that capture hierarchical and spatial features of chessboard, in order to predict the probability fixation for individual pixels Using a skip-layer architecture of an autoencoder, with a unified decoder, we are able to use multiscale features to predict saliency of part of the board at different scales, showing multiple relations between pieces. We have used scan path and fixation data from players engaged in solving chess problems, to compute 6600 saliency maps associated to the corresponding chess piece configurations. This corpus is completed with synthetically generated data from actual games gathered from an online chess platform. Experiments realized using both scan-paths from chess players and the CAT2000 saliency dataset of natural images, highlights several results. Deep features, pretrained on natural images, were found to be helpful in training visual attention prediction for chess. The proposed neural network architecture is able to generate meaningful saliency maps on unseen chess configurations with good scores on standard metrics. This work provides a baseline for future work on visual attention prediction in similar contexts.
Cyber-Virtual Systems: Simulation, Validation & Visualization<|sep|>We describe our ongoing work and view on simulation, validation and visualization of cyber-physical systems in industrial automation during development, operation and maintenance. System models may represent an existing physical part - for example an existing robot installation - and a software simulated part - for example a possible future extension. We call such systems cyber-virtual systems. In this paper, we present the existing VITELab infrastructure for visualization tasks in industrial automation. The new methodology for simulation and validation motivated in this paper integrates this infrastructure. We are targeting scenarios, where industrial sites which may be in remote locations are modeled and visualized from different sites anywhere in the world. Complementing the visualization work, here, we are also concentrating on software modeling challenges related to cyber-virtual systems and simulation, testing, validation and verification techniques for them. Software models of industrial sites require behavioural models of the components of the industrial sites such as models for tools, robots, workpieces and other machinery as well as communication and sensor facilities. Furthermore, collaboration between sites is an important goal of our work.
Angle Diversity Trasmitter For High Speed Data Center Uplink Communications<|sep|>This paper proposes an uplink optical wireless communication (OWC) link design that can be used by data centers to support communication in spine and leaf architectures between the top of rack leaf switches and large spine switches whose access points are mounted in the ceiling. The use of optical wireless links reduces cabling and allows easy reconfigurability for example when data centres expand. We consider three racks in a data center where each rack contains an Angle Diversity Transmitter (ADT) positioned on the top of the rack to realize the uplink function of a top-of-the-rack (ToR) or a leaf switch. Four receivers are considered to be installed on the ceiling where each is connected to a spine switch. Two types of optical receivers are studied which are a Wide Field-of-View Receiver (WFOVR) and an Angle Diversity Receiver (ADR). The performance of the proposed system is evaluated when the links run at data rates higher than 19 Gbps. The results indicate that the proposed approach achieves excellent performance using simple On-Off Keying (OOK)
Optimal MIMO Combining for Blind Federated Edge Learning with Gradient Sparsification<|sep|>We provide the optimal receive combining strategy for federated learning in multiple-input multiple-output (MIMO) systems. Our proposed algorithm allows the clients to perform individual gradient sparsification which greatly improves performance in scenarios with heterogeneous (non i.i.d.) training data. The proposed method beats the benchmark by a wide margin.
DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language<|sep|>The exponential growths of social media and micro-blogging sites not only provide platforms for empowering freedom of expressions and individual voices, but also enables people to express anti-social behaviour like online harassment, cyberbullying, and hate speech. Numerous works have been proposed to utilize textual data for social and anti-social behaviour analysis, by predicting the contexts mostly for highly-resourced languages like English. However, some languages are under-resourced, e.g., South Asian languages like Bengali, that lack computational resources for accurate natural language processing (NLP). In this paper, we propose an explainable approach for hate speech detection from the under-resourced Bengali language, which we called DeepHateExplainer. Bengali texts are first comprehensively preprocessed, before classifying them into political, personal, geopolitical, and religious hates using a neural ensemble method of transformer-based neural architectures (i.e., monolingual Bangla BERT-base, multilingual BERT-cased/uncased, and XLM-RoBERTa). Important(most and least) terms are then identified using sensitivity analysis and layer-wise relevance propagation(LRP), before providing human-interpretable explanations. Finally, we compute comprehensiveness and sufficiency scores to measure the quality of explanations w.r.t faithfulness. Evaluations against machine learning~(linear and tree-based models) and neural networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines yield F1-scores of 78%, 91%, 89%, and 84%, for political, personal, geopolitical, and religious hates, respectively, outperforming both ML and DNN baselines.
Open-vocabulary Queryable Scene Representations for Real World Planning<|sep|>Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io
The Extremely Young Star Cluster Population In Haro 11<|sep|>We have performed a deep multi-band photometric analysis of the star cluster population of Haro 11. This starburst galaxy (log L_FUV = 10.3 L_sun) is considered a nearby analogue of Lyman break galaxies (LBGs) at high redshift. The study of the numerous star clusters in the systems is an effective way to investigate the formation and evolution of the starburst phase. In fact, the SED fitting models have revealed a surprisingly young star cluster population, with ages between 0.5 and 40 Myr, and estimated masses between 10^3 and 10^7 solar masses. An independent age estimation has been done with the EW(Halpha) analysis of each cluster. This last analysis has confirmed the young ages of the clusters. We noticed that the clusters with ages between 1 and 10 Myr show a flux excess in H (NIC3/F160W) and/or I (WFPC2/F814W) bands with respect to the evolutionary models. Once more Haro 11 represents a challenge to our understanding.
Learning Skill Equivalencies Across Platform Taxonomies<|sep|>Assessment and reporting of skills is a central feature of many digital learning platforms. With students often using multiple platforms, cross-platform assessment has emerged as a new challenge. While technologies such as Learning Tools Interoperability (LTI) have enabled communication between platforms, reconciling the different skill taxonomies they employ has not been solved at scale. In this paper, we introduce and evaluate a methodology for finding and linking equivalent skills between platforms by utilizing problem content as well as the platform's clickstream data. We propose six models to represent skills as continuous real-valued vectors and leverage machine translation to map between skill spaces. The methods are tested on three digital learning platforms: ASSISTments, Khan Academy, and Cognitive Tutor. Our results demonstrate reasonable accuracy in skill equivalency prediction from a fine-grained taxonomy to a coarse-grained one, achieving an average recall@5 of 0.8 between the three platforms. Our skill translation approach has implications for aiding in the tedious, manual process of taxonomy to taxonomy mapping work, also called crosswalks, within the tutoring as well as standardized testing worlds.
A Mobile Application for Smart House Remote Control System<|sep|>At the start of the second decade of 21th century, the time has come to make the Smart Houses a reality for regular use. The different parts of a Smart House are researched but there are still distances from an applicable system, using the modern technology. In this paper we present an overview of the Smart House subsystems necessary for controlling the house using a mobile application efficiently and securely. The sequence diagram of the mobile application connecting to the server application and also the use-cases possible are presented. The challenges faced in designing the mobile application and illustrating the updated house top plane view in that application, are discussed and solutions are adapted for it. Finally the designed mobile application was implemented and the important sections of it were described, such as the interactive house top view map which indicates the status of the devices using predefined icons. The facilities to manage the scheduled tasks and defined rules are also implemented in this mobile application that was developed for use in Windows Mobile platform. This application has the capability of connecting to the main server using GPRS mobile internet and SMS. This system is expected to be an important step towards a unified system structure that can be used efficiently in near future regular houses.
Independence and Matchings in $\sigma$-hypergraphs<|sep|>Let $\sigma$ be a partition of the positive integer $r$. A $\sigma$-hypergraph $H=H(n,r,q|\sigma)$ is an $r$-uniform hypergraph on $nq$ vertices which are partitioned into $n$ classes $V_1, V_2, \ldots, V_n$ each containing $q$ vertices. An $r$-subset $K$ of vertices is an edge of the hypergraph if the partition of $r$ formed by the non-zero cardinalities $|K\cap V_i|, 1\leq i \leq n,$ is $\sigma$. In earlier works we have considered colourings of the vertices of $H$ which are constrained such that any edge has at least $\alpha$ and at most $\beta$ vertices of the same colour, and we have shown that interesting results can be obtained by varying $\alpha, \beta$ and the parameters of $H$ appropriately. In this paper we continue to investigate the versatility of $\sigma$-hypergraphs by considering two classical problems: independence and matchings. We first demonstrate an interesting link between the constrained colourings described above and the $k$-independence number of a hypergraph, that is, the largest cardinality of a subset of vertices of a hypergraph not containing $k+1$ vertices in the same edge. We also give an exact computation of the $k$-independence number of the $\sigma$-hypergraph $H$. We then present results on maximum, and sometimes perfect, matchings in $H$. These results often depend on divisibility relations between the parameters of $H$ and on the highest common factor of the parts of $\sigma$.
On the path integral representation for quantum spin models and its application to the quantum cavity method and to Monte Carlo simulations<|sep|>The cavity method is a well established technique for solving classical spin models on sparse random graphs (mean-field models with finite connectivity). Laumann et al. [arXiv:0706.4391] proposed recently an extension of this method to quantum spin-1/2 models in a transverse field, using a discretized Suzuki-Trotter imaginary time formalism. Here we show how to take analytically the continuous imaginary time limit. Our main technical contribution is an explicit procedure to generate the spin trajectories in a path integral representation of the imaginary time dynamics. As a side result we also show how this procedure can be used in simple heat-bath like Monte Carlo simulations of generic quantum spin models. The replica symmetric continuous time quantum cavity method is formulated for a wide class of models, and applied as a simple example on the Bethe lattice ferromagnet in a transverse field. The results of the methods are confronted with various approximation schemes in this particular case. On this system we performed quantum Monte Carlo simulations that confirm the exactness of the cavity method in the thermodynamic limit.
Properties of infrared extrapolations in a harmonic oscillator basis<|sep|>We continue our studies of infrared (ir) and ultraviolet (uv) regulators of no-core shell model calculations. We extend our results that an extrapolation in the ir cutoff with the uv cutoff above the intrinsic uv scale of the interaction is quite successful, not only for the eigenstates of the Hamiltonian but also for expectation values of operators considered long range. The latter results are obtained with Hamiltonians transformed by the similarity renormalization group (SRG) evolution. On the other hand, a suggested extrapolation in the uv cutoff when the ir cutoff is below the intrinsic ir scale is neither robust nor reliable.
Testing And Hardening IoT Devices Against the Mirai Botnet<|sep|>A large majority of cheap Internet of Things (IoT) devices that arrive brand new, and are configured with out-of-the-box settings, are not being properly secured by the manufactures, and are vulnerable to existing malware lurking on the Internet. Among them is the Mirai botnet which has had its source code leaked to the world, allowing any malicious actor to configure and unleash it. A combination of software assets not being utilised safely and effectively are exposing consumers to a full compromise. We configured and attacked 4 different IoT devices using the Mirai libraries. Our experiments concluded that three out of the four devices were vulnerable to the Mirai malware and became infected when deployed using their default configuration. This demonstrates that the original security configurations are not sufficient to provide acceptable levels of protection for consumers, leaving their devices exposed and vulnerable. By analysing the Mirai libraries and its attack vectors, we were able to determine appropriate device configuration countermeasures to harden the devices against this botnet, which were successfully validated through experimentation.
Image Enhancement via Bilateral Learning<|sep|>Nowadays, due to advanced digital imaging technologies and internet accessibility to the public, the number of generated digital images has increased dramatically. Thus, the need for automatic image enhancement techniques is quite apparent. In recent years, deep learning has been used effectively. Here, after introducing some recently developed works on image enhancement, an image enhancement system based on convolutional neural networks is presented. Our goal is to make an effective use of two available approaches, convolutional neural network and bilateral grid. In our approach, we increase the training data and the model dimensions and propose a variable rate during the training process. The enhancement results produced by our proposed method, while incorporating 5 different experts, show both quantitative and qualitative improvements as compared to other available methods.
Entry and Spectrum Sharing Scheme Selection in Femtocell Markets<|sep|>Focusing on a femtocell communications market, we study the entrant network service provider's (NSP's) long-term decision: whether to enter the market and which spectrum sharing technology to select to maximize its profit. This long-term decision is closely related to the entrant's pricing strategy and the users' aggregate demand, which we model as medium-term and short-term decisions, respectively. We consider two markets, one with no incumbent and the other with one incumbent. For both markets, we show the existence and uniqueness of an equilibrium point in the user subscription dynamics, and provide a sufficient condition for the convergence of the dynamics. For the market with no incumbent, we derive upper and lower bounds on the optimal price and market share that maximize the entrant's revenue, based on which the entrant selects an available technology to maximize its long-term profit. For the market with one incumbent, we model competition between the two NSPs as a non-cooperative game, in which the incumbent and the entrant choose their market shares independently, and provide a sufficient condition that guarantees the existence of at least one pure Nash equilibrium. Finally, we formalize the problem of entry and spectrum sharing scheme selection for the entrant and provide numerical results to complement our analysis.
Towards a determination of the tau lepton dipole moments<|sep|>The tau anomalous magnetic moment (a_tau) and electric dipole moment (d_tau) have not yet been observed. The present bounds on their values are of order 10^-2 and 10^-17 e*cm, respectively. We propose to measure a_tau with a precision of O(10^-3) or better and improve the existing limits on d_tau using precise tau- -> l- nu_tau \bar{nu}_l gamma (l=e or mu) data from high-luminosity B factories. A detailed feasibility study of this method is underway.
Low Rank Regularization: A Review<|sep|>Low rank regularization, in essence, involves introducing a low rank or approximately low rank assumption for matrix we aim to learn, which has achieved great success in many fields including machine learning, data mining and computer version. Over the last decade, much progress has been made in theories and practical applications. Nevertheless, the intersection between them is very slight. In order to construct a bridge between practical applications and theoretical research, in this paper we provide a comprehensive survey for low rank regularization. We first review several traditional machine learning models using low rank regularization, and then show their (or their variants) applications in solving practical issues, such as non-rigid structure from motion and image denoising. Subsequently, we summarize the regularizers and optimization methods that achieve great success in traditional machine learning tasks but are rarely seen in solving practical issues. Finally, we provide a discussion and comparison for some representative regularizers including convex and non-convex relaxations. Extensive experimental results demonstrate that non-convex regularizers can provide a large advantage over the nuclear norm, the regularizer widely used in solving practical issues.
Mean field for Markov Decision Processes: from Discrete to Continuous Optimization<|sep|>We study the convergence of Markov Decision Processes made of a large number of objects to optimization problems on ordinary differential equations (ODE). We show that the optimal reward of such a Markov Decision Process, satisfying a Bellman equation, converges to the solution of a continuous Hamilton-Jacobi-Bellman (HJB) equation based on the mean field approximation of the Markov Decision Process. We give bounds on the difference of the rewards, and a constructive algorithm for deriving an approximating solution to the Markov Decision Process from a solution of the HJB equations. We illustrate the method on three examples pertaining respectively to investment strategies, population dynamics control and scheduling in queues are developed. They are used to illustrate and justify the construction of the controlled ODE and to show the gain obtained by solving a continuous HJB equation rather than a large discrete Bellman equation.
Analysis of complex contagions in random multiplex networks<|sep|>We study the diffusion of influence in random multiplex networks where links can be of $r$ different types, and for a given content (e.g., rumor, product, political view), each link type is associated with a content dependent parameter $c_i$ in $[0,\infty]$ that measures the relative bias type-$i$ links have in spreading this content. In this setting, we propose a linear threshold model of contagion where nodes switch state if their "perceived" proportion of active neighbors exceeds a threshold \tau. Namely, a node connected to $m_i$ active neighbors and $k_i-m_i$ inactive neighbors via type-$i$ links will turn active if $\sum{c_i m_i}/\sum{c_i k_i}$ exceeds its threshold \tau. Under this model, we obtain the condition, probability and expected size of global spreading events. Our results extend the existing work on complex contagions in several directions by i) providing solutions for coupled random networks whose vertices are neither identical nor disjoint, (ii) highlighting the effect of content on the dynamics of complex contagions, and (iii) showing that content-dependent propagation over a multiplex network leads to a subtle relation between the giant vulnerable component of the graph and the global cascade condition that is not seen in the existing models in the literature.
Performance Comparison for Neuroscience Application Benchmarks<|sep|>Researchers within the Human Brain Project and related projects have in the last couple of years expanded their needs for high-performance computing infrastructures. The needs arise from a diverse set of science challenges that range from large-scale simulations of brain models to processing of extreme-scale experimental data sets. The ICEI project, which is in the process of creating a distributed infrastructure optimised for brain research, started to build-up a set of benchmarks that reflect the diversity of applications in this field. In this paper we analyse the performance of some selected benchmarks on an IBM POWER8 and Intel Skylake based systems with and without GPUs.
Emergence of large quantum oscillation frequencies in thin flakes of the kagome superconductor CsV$_{3}$Sb$_{5}$<|sep|>Kagome metals AV$_3$Sb$_5$~(A = K, Rb, Cs) are recently discovered platforms featuring an unusual charge-density-wave (CDW) order and superconductivity. The electronic band structure of a kagome lattice can host both flat bands as well as Dirac-like bands, offering the possibility to stabilize various quantum states. Here, we probe the band structure of CsV$_3$Sb$_5$ via Shubnikov-de Haas quantum oscillations on both bulk single crystals and thin flakes. Although our frequency spectra are broadly consistent with the published data, we unambiguously reveal the existence of new frequencies with large frequencies ranging from $\sim$2085~T to $\sim$2717~T in thin flakes when the magnetic field is along the $c$-axis. These quasi-two-dimensional frequencies correspond to $\sim$52\% to 67\% of the CDW-distorted Brillouin zone volume. The Lifshitz-Kosevich analysis further uncovers surprisingly small cyclotron effective masses, of the order of $\sim$0.1~$m_e$, for these frequencies. Consequently, a large number of high-velocity carriers exists in the thin flake of CsV$_3$Sb$_5$. Comparing with our band structure calculations, we argue that an orbital-selective modification of the band structure is active. Our results provide indispensable information for understanding the fermiology of CsV$_3$Sb$_5$, paving a way for understanding how an orbital-selective mechanism can become an effective means to tune its electronic properties.
Towards Making Deep Learning-based Vulnerability Detectors Robust<|sep|>Automatically detecting software vulnerabilities in source code is an important problem that has attracted much attention. In particular, deep learning-based vulnerability detectors, or DL-based detectors, are attractive because they do not need human experts to define features or patterns of vulnerabilities. However, such detectors' robustness is unclear. In this paper, we initiate the study in this aspect by demonstrating that DL-based detectors are not robust against simple code transformations, dubbed attacks in this paper, as these transformations may be leveraged for malicious purposes. As a first step towards making DL-based detectors robust against such attacks, we propose an innovative framework, dubbed ZigZag, which is centered at (i) decoupling feature learning and classifier learning and (ii) using a ZigZag-style strategy to iteratively refine them until they converge to robust features and robust classifiers. Experimental results show that the ZigZag framework can substantially improve the robustness of DL-based detectors.
Interdiction of a Markovian Evader<|sep|>Shortest path network interdiction is a combinatorial optimization problem on an activity network arising in a number of important security-related applications. It is classically formulated as a bilevel maximin problem representing an "interdictor" and an "evader". The evader tries to move from a source node to the target node along a path of the least cost while the interdictor attempts to frustrate this motion by cutting edges or nodes. The interdiction objective is to find the optimal set of edges to cut given that there is a finite interdiction budget and the interdictor must move first. We reformulate the interdiction problem for stochastic evaders by introducing a model in which the evader follows a Markovian random walk guided by the least-cost path to the target. This model can represent incomplete knowledge about the evader, and the resulting model is a nonlinear 0-1 optimization problem. We then introduce an optimization heuristic based on betweenness centrality that can rapidly find high-quality interdiction solutions by providing a global view of the network.
On Web-based Domain-Specific Language for Internet of Things<|sep|>This paper discusses the challenges of the Internet of Things programming. Sensing and data gathering from the various sources are often the key elements of applications for Smart Cities. So, the effective programming models for them are very important. In this article, we discuss system software models and solutions, rather than network related aspects. In our paper, we present the web-based domain-specific language for Internet of Things applications. Our goal is to present the modern models for data processing in Internet of Things and Smart Cities applications. In our view, the use of this kind of tools should seriously reduce the time to develop new applications.
Some Pragmatic Prevention's Guidelines regarding SARS-CoV-2 and COVID-19 in Latin-America inspired by mixed Machine Learning Techniques and Artificial Mathematical Intelligence. Case Study: Colombia<|sep|>We use an enhanced methodology combining specific forms of AI techniques, opinion mining and artificial mathematical intelligence (AMI), with public data on the spread of the coronavirus SARS-CoV-2 and the incidence of COVID-19 disease in Colombia during the first three months since the first reported positive case. The results obtained, together with conceptual tools coming from the global taxonomy of fundamental cognitive mechanisms emerging in AMI and with suitable contextual information from Colombian public health and mainstream social media, allowed us to stating specific preventive guidelines for a better restructuring of initial safe and stable life conditions in Colombia, and in an extended manner in similar Latin American Countries. More specifically, we describe three major guidelines: 1) regular creative visualization and effective planning, 2) the continuous use of constructive linguistic frameworks, and 3) frequent and moderate use of kinesthetic routines. They should be understood as effective tools from a cognitive and behavioural perspective, rather than from a biological one. Even more, the first two guidelines should be acknowledged in integral cooperation with the third one regarding the global effect of COVID-19 in human beings as a whole, this includes the mind and body.
Reaching for the quantum limits in the simultaneous estimation of phase and phase diffusion<|sep|>Phase diffusion invariably accompanies all phase estimation strategies -- quantum or classical. A precise estimation of the former can often provide valuable understanding of the physics of the phase generating phenomena itself. We theoretically examine the performance of fixed-particle number probe states in the simultaneous estimation of phase and collective phase diffusion. We derive analytical quantum limits associated with the simultaneous local estimation of phase and phase diffusion within the quantum Cramer-Rao bound framework in the regimes of large and small phase diffusive noise. The former is for a general fixed-particle number state and the latter for Holland Burnett states, for which we show quantum-enhanced estimation of phase as well as phase diffusion. We next investigate the simultaneous attainability of these quantum limits using projective measurements acting on a single copy of the state in terms of a trade-off relation. In particular, we are interested how this trade-off varies as a function of the dimension of the state. We derive an analytical bound for this trade-off in the large phase diffusion regime for a particular form of the measurement, and show that the maximum of 2, set by the quantum Cramer-Rao bound, is attainable. Further, we show numerical evidence that as diffusion approaches zero, the optimal trade-off relation approaches 1 for Holland-Burnett states. These numerical results are valid in the small particle number regime and suggest that the trade-off for estimating one parameter with quantum-limited precision leads to a complete lack of precision for the other parameter as the diffusion strength approaches zero. Finally, we provide numerical results showing behaviour of the trade-off for a general value of phase diffusion when using Holland-Burnett probe states.
Trifectas for $T_N$ in 5d<|sep|>The trinions $T_N$ are a class of 5d $\mathcal{N}=1$ superconformal field theories (SCFTs) realized as M-theory on $\mathbb{C}^3/\mathbb{Z}_N \times \mathbb{Z}_N$. We apply to $T_N$, as well as closely-related SCFTs that are obtained by mass deformations, a multitude of recently developed approaches to studying 5d SCFTs and their IR gauge theory descriptions. Thereby we provide a complete picture of the theories both on the Coulomb branch and Higgs branch, from various geometric points of view - toric and gluing of compact surfaces as well as combined fiber diagrams - to magnetic quivers and Hasse diagrams.
DDalphaAMG for Twisted Mass Fermions<|sep|>We present the Adaptive Aggregation-based Domain Decomposition Multigrid method extended to the twisted mass fermion discretization action. We show comparisons of results as a function of tuning the parameters that enter the twisted mass version of the DDalphaAMG library (https://github.com/sbacchio/DDalphaAMG). Moreover, we linked the DDalphaAMG library to the tmLQCD software package and give details on the performance of the multigrid solver during HMC simulations at the physical point.
Symmetry-Based Disentangled Representation Learning requires Interaction with Environments<|sep|>Finding a generally accepted formal definition of a disentangled representation in the context of an agent behaving in an environment is an important challenge towards the construction of data-efficient autonomous agents. Higgins et al. recently proposed Symmetry-Based Disentangled Representation Learning, a definition based on a characterization of symmetries in the environment using group theory. We build on their work and make observations, theoretical and empirical, that lead us to argue that Symmetry-Based Disentangled Representation Learning cannot only be based on static observations: agents should interact with the environment to discover its symmetries. Our experiments can be reproduced in Colab and the code is available on GitHub.
Diagnosing $\Lambda$HDE model with statefinder hierarchy and fractional growth parameter<|sep|>Recently, a new dark energy model called $\Lambda$HDE was proposed. In this model, dark energy consists of two parts: cosmological constant $\Lambda$ and holographic dark energy (HDE). Two key parameters of this model are the fractional density of cosmological constant $\Omega_{\Lambda0}$, and the dimensionless HDE parameter $c$. Since these two parameters determine the dynamical properties of DE and the destiny of universe, it is important to study the impacts of different values of $\Omega_{\Lambda0}$ and $c$ on the $\Lambda$HDE model. In this paper, we apply various DE diagnostic tools to diagnose $\Lambda$HDE models with different values of $\Omega_{\Lambda0}$ and $c$; these tools include statefinder hierarchy \{$S_3^{(1)}, S_4^{(1)}$\}, fractional growth parameter $\epsilon$, and composite null diagnostic (CND), which is a combination of \{$S_3^{(1)}, S_4^{(1)}$\} and $\epsilon$. We find that: (1) adopting different values of $\Omega_{\Lambda0}$ only has quantitative impacts on the evolution of the $\Lambda$HDE model, while adopting different $c$ has qualitative impacts; (2) compared with $S_3^{(1)}$, $S_4^{(1)}$ can give larger differences among the cosmic evolutions of the $\Lambda$HDE model associated with different $\Omega_{\Lambda0}$ or different $c$; (3) compared with the case of using a single diagnostic, adopting a CND pair has much stronger ability to diagnose the $\Lambda$HDE model.
Creating Jackknife and Bootstrap estimates of the covariance matrix for the two-point correlation function<|sep|>We present correction terms that allow delete-one Jackknife and Bootstrap methods to be used to recover unbiased estimates of the data covariance matrix of the two-point correlation function $\xi\left(\mathbf{r}\right)$. We demonstrate the accuracy and precision of this new method using a large set of 1000 QUIJOTE simulations that each cover a comoving volume of $1\rm{\left[h^{-1}Gpc\right]^3}$. The corrected resampling techniques recover the correct amplitude and structure of the data covariance matrix as represented by its principal components to within $\sim10$\%, the level of error achievable with the size of the sample of simulations used for the test. Our corrections for the internal resampling methods are shown to be robust against the intrinsic clustering of the cosmological tracers both in real- and redshift space using two snapshots at $z=0$ and $z=1$ that mimic two samples with significantly different clustering. We also analyse two different slicing of the simulation volume into $n_{\rm sv}=64$ or $125$ sub-samples and show that the main impact of different $n_{\rm sv}$ is on the structure of the covariance matrix due to the limited number of independent internal realisations that can be made given a fixed $n_{\rm sv}$.
Finite field calculations of static polarizabilities and hyperpolarizabilities of In$^{+}$ and Sr<|sep|>The finite field calculations are performed for two heavy frequency-standard candidates In$^+$ and Sr. The progressive hierarchy of electron correlations is implemented by the relativistic coupled-cluster and configuration interaction methods combined with basis set of increasing size. The dipole polarizabilities, dipole hyperpolarizabilities, quadrupole moments, and quadrupole polarizabilities are recommended for the ground state 5s$^2$ $^1S_0$ and low-lying states 5s5p $^3P^{\rm o}_{0,1,2}$ of In$^+$ and Sr. Comparative study of the fully and scalar relativistic electron correlation calculations reveals the effect of the spin-orbit interaction on the dipole polarizabilities of In$^{+}$ and Sr. Finally, the blackbody-radiation shifts due to the dipole polarizability, dipole hyperpolarizability, and quadrupole polarizability are evaluated for the clock transition 5s$^2$ $^1S_0$ - 5s5p $^3P^{\rm o}_0$ of In$^+$ and Sr.
Dynamical Freezing and Scar Points in Strongly Driven Floquet Matter: Resonance vs Emergent Conservation Laws<|sep|>We consider a clean quantum system subject to strong periodic driving. The existence of a dominant energy scale, $h_D^x$, can generate considerable structure in an effective description of a system which, in the absence of the drive, is non-integrable, interacting, and does not host localization. In particular, we uncover points of freezing in the space of drive parameters (frequency and amplitude). At those points, the dynamics is severely constrained due to the emergence of an almost exact local conserved quantity, which scars the {\it entire} Floquet spectrum by preventing the system from heating up ergodically, starting from any generic state, even though it delocalizes over an appropriate subspace. At large drive frequencies, where a na\"ive Magnus expansion would predict a vanishing effective (average) drive, we devise instead a strong-drive Magnus expansion in a moving frame. There, the emergent conservation law is reflected in the appearance of an `integrability' of an effective Hamiltonian. These results hold for a wide variety of Hamiltonians, including the Ising model in a transverse field in {\it any dimension} and for {\it any form of Ising interactions}. The phenomenon is also shown to be robust in the presence of {\it two-body Heisenberg interactions with any arbitrary choice of couplings}. Further, we construct a real-time perturbation theory which captures resonance phenomena where the conservation breaks down, giving way to unbounded heating. This opens a window on the low-frequency regime where the Magnus expansion fails.
Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding<|sep|>Classifying single image patches is important in many different applications, such as road detection or scene understanding. In this paper, we present convolutional patch networks, which are convolutional networks learned to distinguish different image patches and which can be used for pixel-wise labeling. We also show how to incorporate spatial information of the patch as an input to the network, which allows for learning spatial priors for certain categories jointly with an appearance model. In particular, we focus on road detection and urban scene understanding, two application areas where we are able to achieve state-of-the-art results on the KITTI as well as on the LabelMeFacade dataset. Furthermore, our paper offers a guideline for people working in the area and desperately wandering through all the painstaking details that render training CNs on image patches extremely difficult.
Human-Data Interaction: The Human Face of the Data-Driven Society<|sep|>The increasing generation and collection of personal data has created a complex ecosystem, often collaborative but sometimes combative, around companies and individuals engaging in the use of these data. We propose that the interactions between these agents warrants a new topic of study: Human-Data Interaction (HDI). In this paper we discuss how HDI sits at the intersection of various disciplines, including computer science, statistics, sociology, psychology and behavioural economics. We expose the challenges that HDI raises, organised into three core themes of legibility, agency and negotiability, and we present the HDI agenda to open up a dialogue amongst interested parties in the personal and big data ecosystems.
Beating no-go theorems by engineering defects in quantum spin models<|sep|>There exist diverse no-go theorems, ranging from no-cloning to monogamies of quantum correlations and Bell inequality violations, which restrict the processing of information in the quantum world. In a multipartite scenario, monogamy of Bell inequality violation and exclusion principle of dense coding are such theorems, which impede the ability of the system to have quantum advantage between all its parts. In ordered spin systems, the twin restrictions of translation invariance and monogamy of quantum correlations, in general, enforce the bipartite states to be neither Bell inequality violating nor dense-codeable. We show that these quantum characteristics, viz. Bell inequality violation and dense-codeability, can be resurrected, and thereby the no-go theorems overcome, by having quenched disorder in the system parameters leading to quantum spin glass or quantum random field models. We show that the quantum characteristics are regained even though the quenched averaging keeps the disordered spin chains translationally invariant at the physically relevant level of observables. The results show that it is possible to conquer constraints imposed by quantum mechanics in ordered systems by introducing impurities.
Layout-induced Video Representation for Recognizing Agent-in-Place Actions<|sep|>We address the recognition of agent-in-place actions, which are associated with agents who perform them and places where they occur, in the context of outdoor home surveillance. We introduce a representation of the geometry and topology of scene layouts so that a network can generalize from the layouts observed in the training set to unseen layouts in the test set. This Layout-Induced Video Representation (LIVR) abstracts away low-level appearance variance and encodes geometric and topological relationships of places in a specific scene layout. LIVR partitions the semantic features of a video clip into different places to force the network to learn place-based feature descriptions; to predict the confidence of each action, LIVR aggregates features from the place associated with an action and its adjacent places on the scene layout. We introduce the Agent-in-Place Action dataset to show that our method allows neural network models to generalize significantly better to unseen scenes.
Performance of the MAGIC telescopes after the major upgrade<|sep|>MAGIC is a system of two Imaging Atmospheric Cherenkov Telescopes located on the Canary island of La Palma, Spain. During summer 2011 and 2012 it underwent a major upgrade. The main subsystems upgraded were the MAGIC-I camera and its trigger system and the readout system of both telescopes. We use observations of the Crab Nebula taken at low and medium zenith angles to assess the key performance parameters of the MAGIC stereo system. For low zenith angle observations, the standard trigger threshold of the MAGIC telescopes is about 50 GeV. The integral sensitivity for point-like sources with Crab Nebula-like spectra above 220 GeV is (0.66 +/- 0.03)% of Crab Nebula flux in 50 h of observations. The angular resolution, defined as the sigma of a 2-dimensional Gaussian distribution, at energies of a few hundred GeV is below 0.07degree, while the energy resolution is around 16%. We investigate the effect of the systematic uncertainty on the data taken with the MAGIC telescopes after the upgrade. We estimate that the systematic uncertainties can be divided in the following components: < 15% in energy scale, 11 - 18% in flux normalization and +/-0.15 for the slope of the energy spectrum.
Discriminative Optimization: Theory and Applications to Computer Vision Problems<|sep|>Many computer vision problems are formulated as the optimization of a cost function. This approach faces two main challenges: (i) designing a cost function with a local optimum at an acceptable solution, and (ii) developing an efficient numerical method to search for one (or multiple) of these local optima. While designing such functions is feasible in the noiseless case, the stability and location of local optima are mostly unknown under noise, occlusion, or missing data. In practice, this can result in undesirable local optima or not having a local optimum in the expected place. On the other hand, numerical optimization algorithms in high-dimensional spaces are typically local and often rely on expensive first or second order information to guide the search. To overcome these limitations, this paper proposes Discriminative Optimization (DO), a method that learns search directions from data without the need of a cost function. Specifically, DO explicitly learns a sequence of updates in the search space that leads to stationary points that correspond to desired solutions. We provide a formal analysis of DO and illustrate its benefits in the problem of 3D point cloud registration, camera pose estimation, and image denoising. We show that DO performed comparably or outperformed state-of-the-art algorithms in terms of accuracy, robustness to perturbations, and computational efficiency.
Sampling of bosonic qubits<|sep|>The boson sampling problem has brought a lot of attention in the quantum information field because it is not efficiently solvable with a classical computer; nonetheless it can be implemented with linear optical interferometers with single-boson sources. Recently, we introduced a more general problem, the multi-boson correlation sampling problem, which allows us to take advantage of the multi-mode spectral distribution of the bosonic sources together with time-correlated measurements in order to achieve sampling not only over the output ports of the interferometer but also over the joint detection times. This problem was analyzed for both single-photon sources and thermal sources. In this work, we demonstrate that it is possible to use single qubit bosonic sources in order to sample not only over the described "space-time" degree of freedom but also over all the possible exponential number of multi-qubit output states.
Critical Behavior of Low Dimensional Magnetic Systems<|sep|>In this study, critical behavior of low dimensional magnetic systems as cyano-bridged Tb(III)-Cr(III) bimetallic assembly was investigated with the mixed spin $3$- spin $3/2$ Ising model. The mixed spin Ising model is simulated with Cellular Automaton cooling and heating algorithms on one-dimensional lattices in periodic boundary conditions. The Ising model Hamiltonian includes only antiferromagnetic nearest-neighbor interaction ($% J>0$). The mixed spin system behaves like the isolated one-dimensional chain for zero magnetic field ($h=\frac{H}{J}=0$). In the presence of the magnetic field, the magnetization is calculated using zero-field cooling ($ZFC$) and field cooling ($FC$) processes. The one-dimensional Ising model results are compatible with the cyano-bridged Tb(III)-Cr(III) bimetallic quasi-one dimensional assembly [Tb(H_{2}O)_{2}(DMF)_{4}{ Cr(CN)_{6}}].H_{2}O(DMF=dimethylformamide) results.
Surface organization of homoepitaxial InP films grown by metalorganic vapor-phase epitaxy<|sep|>We present a systematic study of the morphology of homoepitaxial InP films grown by metalorganic vapor-phase epitaxy which are imaged with ex situ atomic force microscopy. These films show a dramatic range of different surface morphologies as a function of the growth conditions and substrate (growth temperature, V/III ratio, and miscut angle < 0.6deg and orientation toward A or B sites), ranging from stable step flow to previously unreported strong step bunching, over 10 nm in height. These observations suggest a window of growth parameters for optimal quality epitaxial layers. We also present a theoretical model for these growth modes that takes account of deposition, diffusion, and dissociation of molecular precursors, and the diffusion and step incorporation of atoms released by the precursors. The experimental conditions for step flow and step bunching are reproduced by this model, with the step bunching instability caused by the difference in molecular dissociation from above and below step edges, as was discussed previously for GaAs (001).
Topological Casimir effect in compactified cosmic string spacetime<|sep|>We investigate the Wightman function, the vacuum expectation values of the field squared and the energy-momentum tensor for a massive scalar field with general curvature coupling in the generalized cosmic string geometry with a compact dimension along its axis. The boundary condition along the compactified dimension is taken in general form with an arbitrary phase. The vacuum expectation values are decomposed into two parts. The first one corresponds to the uncompactified cosmic string geometry and the second one is the correction induced by the compactification. The asymptotic behavior of the vacuum expectation values of the field squared, energy density and stresses are investigated near the string and at large distances. We show that the nontrivial topology due to the cosmic string enhances the vacuum polarization effects induced by the compactness of spatial dimension for both the field squared and the vacuum energy density. A simple formula is given for the part of the integrated topological Casimir energy induced by the planar angle deficit. The results are generalized for a charged scalar field in the presence of a constant gauge field. In this case, the vacuum expectation values are periodic functions of the component of the vector potential along the compact dimension.
Red-skewed K$\alpha$ iron lines in GX 13+1<|sep|>Broad, asymmetric, and red-skewed Fe Kalpha emission lines have been observed in the spectra of low-mass X-ray binaries hosting neutron stars (NSs) as a compact object. Because more than one model is able to describe these features, the explanation of where and how the red-skewed Fe lines are produced is still a matter of discussion. It is broadly accepted that the shape of the Fe Kalpha line is strongly determined by the special and general relativistic effects occurring in the innermost part of the accretion disk. In this relativistic framework, the Fe fluorescent lines are produced in the innermost part of the accretion disk by reflection of hard X-ray photons coming from the central source (corona and/or NS surface). We developed an alternative and nonrelativistic model, called the windline model, that is capable to describe the Fe line features. In this nonrelativistic framework, the line photons are produced at the bottom of a partly ionized outflow (wind) shell as a result of illumination by the continuum photons coming from the central source, and the red-skewness of the line profile is explained by repeated electron scattering of the photons in a diverging outflow. Because GX~13+1 is a well-known disk-wind source, it is a perfect target for testing the windline model and comparing it to the relativistic one. In order to access the goodness of the fit and distinguish between the two line models, we used the run-test statistical method in addition to the canonical $\chi^2$ statistical method. The diskline and windline models both fit the asymmetric GX13+1 Fe line well. From a statistical point of view, for the two observations we analyzed, the run-test was not able to distinguish between the two Fe line models, at 5% significance level.
How does the scaling for the polymer chain in the dissipative particle dynamics hold?<|sep|>We performed a series of simulations for a linear polymer chain in a solvent using dissipative particle dynamics to check the scaling relations for the end-to-end distance, radius of gyration and hydrodynamic radius in three dimensions. The polymer chains of up to 80 beads in explicit solvent of various quality are studied. To extract the scaling exponent \nu, the data are analyzed using linear fits, correction-to-scaling forms and analytical fits to the histograms of radius of gyration distribution. For certain combinations of the polymer characteristics and solvent quality, the correction-to-scaling terms are found to be essential while for the others these are negligibly small. In each particular case the final value for the exponent \nu was chosen according to the best least-squares fit. The values of \nu obtained in this way are found within the interval \nu=0.55-0.61 but are concentrated mostly around 0.59, which is very close to the best known theoretical result \nu=0.588. The existence of this interval is attributed both to the peculiarities of the method and to the moderate chain lengths being simulated. Within this shortcoming, the polymer chain in this kind of modeling is found to satisfy the scaling relations for all three radii being considered
Observing the nonclassical nature of ultra-broadband bi-photons at ultrafast speed<|sep|>We observe at record-high speed the nonclassical nature of ultra-broadband bi-photons, reducing the measurement time by four orders of magnitude compared to standard techniques of Hong-Ou-Mandel interference or sum-frequency generation. We measure the quantum state of the broadband bi-photons, amplitude and phase, with a pairwise "Mach-Zehnder" quantum interferometer, where bi-photons that are generated in one nonlinear crystal are enhanced (constructive interference) or diminished (destructive interference) in another crystal, depending on the bi-photon phase. We verify the quantum nature of the interference by observing the dependence of the fringe visibility on internal loss. Since destructive interference is equivalent to an attempt to annihilate in the second crystal (by up-conversion) the bi-photons that were created in the first crystal (by down-conversion), the fringe visibility is a measure for the quantum bi-photon purity of the broadband light. The measurement speed-up is due to the large homodyne-like gain from the strong pump ($\!\sim\!10^{7-9}$) in the up-conversion efficiency of single bi-photons, which enables the use of simple photo-detection of the full, ultra-high photon flux instead of single-photon / coincidence counting.
Risk-Averse Planning Under Uncertainty<|sep|>We consider the problem of designing policies for partially observable Markov decision processes (POMDPs) with dynamic coherent risk objectives. Synthesizing risk-averse optimal policies for POMDPs requires infinite memory and thus undecidable. To overcome this difficulty, we propose a method based on bounded policy iteration for designing stochastic but finite state (memory) controllers, which takes advantage of standard convex optimization methods. Given a memory budget and optimality criterion, the proposed method modifies the stochastic finite state controller leading to sub-optimal solutions with lower coherent risk.
Single and Double Photoionization and Photodissociation of Toluene by Soft X-rays in Circumstellar Environment<|sep|>The formation of polycyclic aromatic hydrocarbons (PAHs) and their methyl derivatives occurs mainly in the dust shells of asymptotic giant branch (AGB) stars. The bands at 3.3 and 3.4 $\mu$m, observed in infrared emission spectra of several objects, are attributed C-H vibrational modes in aromatic and aliphatic structures, respectively. In general, the feature at 3.3 $\mu$m is more intense than the 3.4 $\mu$m. Photoionization and photodissociation processes of toluene, the precursor of methylated PAHs, were studied using synchrotron radiation at soft X-ray energies around the carbon K edge with time-of-flight mass spectrometry. Partial ion yields of a large number of ionic fragments were extracted from single and 2D-spectra, where electron-ion coincidences have revealed the doubly charged parent-molecule and several doubly charged fragments containing seven carbon atoms with considerable abundance. \textit{Ab initio} calculations based on density functional theory were performed to elucidate the chemical structure of these stable dicationic species. The survival of the dications subjected to hard inner shell ionization suggests that they could be observed in the interstellar medium, especially in regions where PAHs are detected. The ionization and destruction of toluene induced by X-rays were examined in the T Dra conditions, a carbon-rich AGB star. In this context, a minimum photodissociation radius and the half-life of toluene subjected to the incidence of the soft X-ray flux emitted from a companion white dwarf star were determined.
Speculate-Correct Error Bounds for k-Nearest Neighbor Classifiers<|sep|>We introduce the speculate-correct method to derive error bounds for local classifiers. Using it, we show that k nearest neighbor classifiers, in spite of their famously fractured decision boundaries, have exponential error bounds with O(sqrt((k + ln n) / n)) error bound range for n in-sample examples.
The HARPS search for Earth-like planets in the habitable zone: I -- Very low-mass planets around HD20794, HD85512 and HD192310<|sep|>In 2009 we started an intense radial-velocity monitoring of a few nearby, slowly-rotating and quiet solar-type stars within the dedicated HARPS-Upgrade GTO program. The goal of this campaign is to gather very-precise radial-velocity data with high cadence and continuity to detect tiny signatures of very-low-mass stars that are potentially present in the habitable zone of their parent stars. Ten stars were selected among the most stable stars of the original HARPS high-precision program that are uniformly spread in hour angle, such that three to four of them are observable at any time of the year. For each star we recorded 50 data points spread over the observing season. The data points consist of three nightly observations with a total integration time of 10 minutes each and are separated by two hours. This is an observational strategy adopted to minimize stellar pulsation and granulation noise. We present the first results of this ambitious program. The radial-velocity data and the orbital parameters of five new and one confirmed low-mass planets around the stars HD20794, HD85512, and HD192310 are reported and discussed, among which is a system of three super-Earths and one that harbors a 3.6 Earth-mass planet at the inner edge of the habitable zone. This result already confirms previous indications that low-mass planets seem to be very frequent around solar-type stars and that this may occur with a frequency higher than 30%
The Erdos-Posa Property for Directed Graphs<|sep|>A classical result by Erdos and Posa states that there is a function $f: {\mathbb N} \rightarrow {\mathbb N}$ such that for every $k$, every graph $G$ contains $k$ pairwise vertex disjoint cycles or a set $T$ of at most $f(k)$ vertices such that $G-T$ is acyclic. The generalisation of this result to directed graphs is known as Younger's conjecture and was proved by Reed, Robertson, Seymour and Thomas in 1996. This so-called Erdos-Posa-property can naturally be generalised to arbitrary graphs and digraphs. Robertson and Seymour proved that a graph $H$ has the Erdos-Posa-property if, and only if, $H$ is planar. In this paper we study the corresponding problem for digraphs. We obtain a complete characterisation of the class of strongly connected digraphs which have the Erdos-Posa-property (both for topological and butterfly minors). We also generalise this result to classes of digraphs which are not strongly connected. In particular, we study the class of vertex-cyclic digraphs (digraphs without trivial strong components). For this natural class of digraphs we obtain a nearly complete characterisation of the digraphs within this class with the Erdos-Posa-property. In particular we give positive and algorithmic examples of digraphs with the Erdos-Posa-property by using directed tree decompositions in a novel way.
Leveraging the ALMA Atacama Compact Array for Cometary Science: An Interferometric Survey of Comet C/2015 ER61 (PanSTARRS) and Evidence for a Distributed Source of Carbon Monosulfide<|sep|>We report the first survey of molecular emission from cometary volatiles using standalone Atacama Compact Array (ACA) observations of the Atacama Large Millimeter/Submillimeter Array (ALMA) toward comet C/2015 ER61 (PanSTARRS) carried out on UT 2017 April 11 and 15, shortly after its April 4 outburst. These measurements of HCN, CS, CH$_3$OH, H$_2$CO, and HNC (along with continuum emission from dust) probed the inner coma of C/2015 ER61, revealing asymmetric outgassing and discerning parent from daughter/distributed source species. This work presents spectrally integrated flux maps, autocorrelation spectra, production rates, and parent scale lengths for each molecule, and a stringent upper limit for CO. HCN is consistent with direct nucleus release in C/2015 ER61, whereas CS, H$_2$CO, HNC, and potentially CH$_3$OH are associated with distributed sources in the coma. Adopting a Haser model, parent scale lengths determined for H$_2$CO (L$_p$ $\sim$ 2200 km) and HNC (L$_p$ $\sim$ 3300 km) are consistent with previous work in comets, whereas significant extended source production (L$_p$ $\sim$ 2000 km) is indicated for CS, suggesting production from an unknown parent in the coma. The continuum presents a point-source distribution, with a flux density implying an excessively large nucleus, inconsistent with other estimates of the nucleus size. It is best explained by the thermal emission of slowly-moving outburst ejectas, with total mass 5--8 $\times$ 10$^{10}$ kg. These results demonstrate the power of the ACA for revealing the abundances, spatial distributions, and locations of molecular production for volatiles in moderately bright comets such as C/2015 ER61.
A spectral survey of CH3CCH in the Hot Molecular Core G331.512-0.103<|sep|>A spectral survey of methyl acetylene (CH3CCH) was conducted toward the hot molecular core/outflow G331.512-0.103. Our APEX observations allowed the detection of 41 uncontaminated rotational lines of CH3CCH in the frequency range between 172-356 GHz. Through an analysis under the local thermodynamic equilibrium assumption, by means of rotational diagrams, we determined Texc = 50 \pm 1 K, N(CH3CCH) = (7.5 \pm 0.4) x 10^{15} cm^{-2}, X[CH3CCH/H2] ~ (0.8-2.8) x 10^{-8} and X[CH3CCH/CH3OH] ~ 0.42 \pm 0.05 for an extended emitting region (~10 arcsec). The relative intensities of the K=2 and K=3 lines within a given K-ladder are strongly negatively correlated to the transitions' upper J quantum-number (r=-0.84). Pure rotational spectra of CH3CCH were simulated at different temperatures, in order to interpret this observation. The results indicate that the emission is characterized by a non-negligible temperature gradient with upper and lower limits of ~45 and ~60 K, respectively. Moreover, the line widths and peak velocities show an overall strong correlation with their rest frequencies, suggesting that the warmer gas is also associated with stronger turbulence effects. The K=0 transitions present a slightly different kinematic signature than the remaining lines, indicating that they might be tracing a different gas component. We speculate that this component is characterized by lower temperatures, and therefore larger sizes. Moreover, we predict and discuss the temporal evolution of the CH3CCH abundance using a two-stage zero-dimensional model of the source constructed with the three-phase Nautilus gas-grain code.
(2,2) and (0,4) Supersymmetric Boundary Conditions in 3d N = 4 Theories and Type IIB Branes<|sep|>The half-BPS boundary conditions preserving $\mathcal{N}=(2,2)$ and $\mathcal{N}=(0,4)$ supersymmetry in 3d $\mathcal{N}=4$ supersymmetric gauge theories are examined. The BPS equations admit decomposition of the bulk supermultiplets into specific boundary supermultiplets of preserved supersymmetry. Nahm-like equations arise in the vector multiplet BPS boundary condition preserving $\mathcal{N}=(0,4)$ supersymmetry and Robin-type boundary conditions appear for the hypermultiplet coupled to vector multiplet when $\mathcal{N}=(2,2)$ supersymmetry is preserved. The half-BPS boundary conditions are realized in the brane configurations of Type IIB string theory.
Bridging Proper Orthogonal Decomposition methods and augmented Newton-Krylov algorithms: an adaptive model order reduction for highly nonlinear mechanical problems<|sep|>This article describes a bridge between POD-based model order reduction techniques and the classical Newton/Krylov solvers. This bridge is used to derive an efficient algorithm to correct, "on-the-fly", the reduced order modelling of highly nonlinear problems undergoing strong topological changes. Damage initiation problems are addressed and tackle via a corrected hyperreduction method. It is shown that the relevancy of reduced order model can be significantly improved with reasonable additional costs when using this algorithm, even when strong topological changes are involved.
Porting CMS Heterogeneous Pixel Reconstruction to Kokkos<|sep|>Programming for a diverse set of compute accelerators in addition to the CPU is a challenge. Maintaining separate source code for each architecture would require lots of effort, and development of new algorithms would be daunting if it had to be repeated many times. Fortunately there are several portability technologies on the market such as Alpaka, Kokkos, and SYCL. These technologies aim to improve the developer productivity by making it possible to use the same source code for many different architectures. In this paper we use heterogeneous pixel reconstruction code from the CMS experiment at the CERNL LHC as a realistic use case of a GPU-targeting HEP reconstruction software, and report experience from prototyping a portable version of it using Kokkos. The development was done in a standalone program that attempts to model many of the complexities of a HEP data processing framework such as CMSSW. We also compare the achieved event processing throughput to the original CUDA code and a CPU version of it.
Discrete symmetries in the Kaluza-Klein-like theories<|sep|>In theories of the Kaluza-Klein kind there are spins or total angular moments in higher dimensions which manifest as charges in the observable $d=(3+1)$. The charge conjugation requirement, if following the prescription in ($3+1$), would transform any particle state out of the Dirac sea into the hole in the Dirac sea, which manifests as an anti-particle having all the spin degrees of freedom in $d$, except $S^{03}$, the same as the corresponding particle state. This is in contradiction with what we observe for the anti-particle. In this paper we redefine the discrete symmetries so that we stay within the subgroups of the starting group of symmetries, while we require that the angular moments in higher dimensions manifest as charges in $d=(3+1)$. We pay attention on spaces with even $d$.
Non-symmetric localized fold of a floating sheet<|sep|>An elastic sheet lying on the surface of a liquid, if axially compressed, shows a transition from a smooth sinusoidal pattern to a well localized fold. This wrinkle-to-fold transition is a manifestation of a localized buckling. The symmetric and antisymmetric shapes of the fold have recently been described by Diamant and Witten (2011), who found two exact solutions of the nonlinear equilibrium equations. In this Note, we show that these solutions can be generalized to a continuous family of solutions, which yields non symmetric shapes of the fold. We prove that non symmetric solutions also describe the shape of a soft strip withdrawn from a liquid bath, a physical problem that allows to easily observe portions of non symmetric profiles.
Exploring the Encoding Layer and Loss Function in End-to-End Speaker and Language Recognition System<|sep|>In this paper, we explore the encoding/pooling layer and loss function in the end-to-end speaker and language recognition system. First, a unified and interpretable end-to-end system for both speaker and language recognition is developed. It accepts variable-length input and produces an utterance level result. In the end-to-end system, the encoding layer plays a role in aggregating the variable-length input sequence into an utterance level representation. Besides the basic temporal average pooling, we introduce a self-attentive pooling layer and a learnable dictionary encoding layer to get the utterance level representation. In terms of loss function for open-set speaker verification, to get more discriminative speaker embedding, center loss and angular softmax loss is introduced in the end-to-end system. Experimental results on Voxceleb and NIST LRE 07 datasets show that the performance of end-to-end learning system could be significantly improved by the proposed encoding layer and loss function.
Modelling Quantum Theoretical Trajectories within Geometric Relativistic Theories<|sep|>Andreka and her colleagues have described various geometrically inspired first-order theories of special and general relativity, while Szekely's PhD dissertation focuses on an intermediate logic of accelerated observers. In this paper we will attempt to incorporate a model of quantum theoretical trajectories that can reasonably claim to be physically meaningful into those theories. We have recently shown that a model of quantum trajectories, based on discrete finitary motion, is logically equivalent to Feynman's path-integral formulation when spacetime is assumed to be Euclidean. In this paper we argue that relativistic observers are subject to the same quantum illusions as in the Euclidean case: even though motion is discrete and respects no built-in 'arrow of time', observers have no choice but to perceive particle trajectories as continuous paths in spacetime. Whereas the relativistic theories presuppose continuous paths as part of their axioms, the illusion of continuity allows us to replace this axiom with a lower-level quantum-inspired axiom concerning discrete jumps in spacetime. We investigate the nature of these jumps, and how far they can be tied to the underlying geometric structure of spacetime. In particular, we consider hops which preserve features of the underlying number field, and investigate the extent to which all hops can be restricted to be of this form. We conclude that hop-based motion can also be regarded as an illusion, one caused by modelling physics in the 'wrong' number system.
Spin-orbit interaction and Dirac cones in d orbital noble metal surface states<|sep|>Band splittings, chiral spin polarization and topological surface states generated by spin-orbit interactions at crystal surfaces are receiving a lot of attention for their potential device applications as well as fascinating physical properties. Most studies have focused on sp states near the Fermi energy, which are relevant for transport and have long lifetimes. Far less explored, though in principle stronger, are spin-orbit interaction effecs within d states, including those deep below the Fermi energy. Here, we report a joint photoemission/ab initio study of spin-orbit effects in the deep d orbital surface states of a 24-layer Au film grown on Ag(111) and a 24-layer Ag film grown on Au(111), singling out a conical intersection (Dirac cone) between two surface states in a large surface-projected gap at the time-reversal symmetric M points. Unlike the often isotropic dispersion at Gamma point Dirac cones, the M point cones are strongly anisotropic. An effective k.p Hamiltonian is derived to describe the anisotropic band splitting and spin polarization near the Dirac cone.
On Model Evaluation under Non-constant Class Imbalance<|sep|>Many real-world classification problems are significantly class-imbalanced to detriment of the class of interest. The standard set of proper evaluation metrics is well-known but the usual assumption is that the test dataset imbalance equals the real-world imbalance. In practice, this assumption is often broken for various reasons. The reported results are then often too optimistic and may lead to wrong conclusions about industrial impact and suitability of proposed techniques. We introduce methods focusing on evaluation under non-constant class imbalance. We show that not only the absolute values of commonly used metrics, but even the order of classifiers in relation to the evaluation metric used is affected by the change of the imbalance rate. Finally, we demonstrate that using subsampling in order to get a test dataset with class imbalance equal to the one observed in the wild is not necessary, and eventually can lead to significant errors in classifier's performance estimate.
The Quijote CMB Experiment<|sep|>We present the current status of the QUIJOTE (Q-U-I JOint TEnerife) CMB Experiment, a new instrument which will start operations early 2009 at Teide Observatory, with the aim of characterizing the polarization of the CMB and other processes of galactic and extragalactic emission in the frequency range 10-30 GHz and at large angular scales. QUIJOTE will be a valuable complement at low frequencies for the PLANCK mission, and will have the required sensitivity to detect a primordial gravitational-wave component if the tensor-to-scalar ratio is larger than r=0.05.
Nonequilibrium phases in hybrid arrays with flux qubits and NV centers<|sep|>We propose a startling hybrid quantum architecture for simulating a localization-delocalization transition. The concept is based on an array of superconducting flux qubits which are coupled to a diamond crystal containing nitrogen-vacancy (NV) centers. The underlying description is a Jaynes-Cummings-lattice in the strong-coupling regime. However, in contrast to well-studied coupled cavity arrays the interaction between lattice sites is mediated here by the qubit rather than by the oscillator degrees of freedom. Nevertheless, we point out that a transition between a localized and a delocalized phase occurs in this system as well. We demonstrate the possibility of monitoring this transition in a non-equilibrium scenario, including decoherence effects. The proposed scheme allows the monitoring of localization-delocalization transitions in Jaynes-Cummings-lattices by use of currently available experimental technology. Contrary to cavity-coupled lattices, our proposed recourse to stylized qubit networks facilitates (i) to investigate localization-delocalization transitions in arbitrary dimensions and (ii) to tune the inter-site coupling in-situ.
Do Comments follow Commenting Conventions? A Case Study in Java and Python<|sep|>Assessing code comment quality is known to be a difficult problem. A number of coding style guidelines have been created with the aim to encourage writing of informative, readable, and consistent comments. However, it is not clear from the research to date which specific aspects of comments the guidelines cover (e.g., syntax, content, structure). Furthermore, the extent to which developers follow these guidelines while writing code comments is unknown. We analyze various style guidelines in Java and Python and uncover that the majority of them address more the content aspect of the comments rather than syntax or formatting, but when considering the different types of information developers embed in comments and the concerns they raise on various online platforms about the commenting practices, existing comment conventions are not yet specified clearly enough, nor do they adequately cover important concerns. We also analyze commenting practices of developers in diverse projects to see the extent to which they follow the guidelines. Our results highlight the mismatch between developer commenting practices and style guidelines, and provide several focal points for the design and improvement of comment quality checking tools.
Fault-Tolerant Spanners: Better and Simpler<|sep|>A natural requirement of many distributed structures is fault-tolerance: after some failures, whatever remains from the structure should still be effective for whatever remains from the network. In this paper we examine spanners of general graphs that are tolerant to vertex failures, and significantly improve their dependence on the number of faults $r$, for all stretch bounds. For stretch $k \geq 3$ we design a simple transformation that converts every $k$-spanner construction with at most $f(n)$ edges into an $r$-fault-tolerant $k$-spanner construction with at most $O(r^3 \log n) \cdot f(2n/r)$ edges. Applying this to standard greedy spanner constructions gives $r$-fault tolerant $k$-spanners with $\tilde O(r^{2} n^{1+\frac{2}{k+1}})$ edges. The previous construction by Chechik, Langberg, Peleg, and Roddity [STOC 2009] depends similarly on $n$ but exponentially on $r$ (approximately like $k^r$). For the case $k=2$ and unit-length edges, an $O(r \log n)$-approximation algorithm is known from recent work of Dinitz and Krauthgamer [arXiv 2010], where several spanner results are obtained using a common approach of rounding a natural flow-based linear programming relaxation. Here we use a different (stronger) LP relaxation and improve the approximation ratio to $O(\log n)$, which is, notably, independent of the number of faults $r$. We further strengthen this bound in terms of the maximum degree by using the \Lovasz Local Lemma. Finally, we show that most of our constructions are inherently local by designing equivalent distributed algorithms in the LOCAL model of distributed computation.
Optimal representation of quantum channels<|sep|>This work shows an approach to reduce the dimensionality of matrix representations of quantum channels. It is achieved by finding a base of the cone of positive semidefinite matrices which represent quantum channels. Next, this is implemented in the Julia programming language as a part of the QuantumInformation.jl package.
Doubly eclipsing systems<|sep|>Aims: Our goal was to increase number of known doubly eclipsing systems such that the resulting dataset would allow to study them via statistical means, as well as prove that they constitute gravitationally bound 2+2 quadruple system. Methods: We analysed photometric data for eclipsing binaries provided by the OGLE survey in the LMC fields. We found a large number of new doubly eclipsing systems (our discoveries are 3x more numerous than previous studies). With a typical orbital period of days for the binaries, we sought eclipse time variations (ETVs) on the timescale of years. In the cases where we were able to detect the ETV period, the difference between the inner and outer periods in the quadruple system is large enough. This allows us to interpret ETVs primarily as the light-time effect, thus providing an interesting constraint on masses of the binaries. Results: In addition to significantly enlarging the database of known doubly eclipsing systems, we performed a thorough analysis of 72 cases. ETVs for 28 of them (39% of the studied cases) showed evidence of relative motion. We note OGLE BLG-ECL-145467 as the most interesting case; it is bright (I=12.6 mag), consists of two detached binaries with periods of about 3.3 d and 4.9 d (making it a candidate for a 3:2 resonant system), mutual period about 1538 d. Distribution of the orbital period ratio P_A/P_B of binaries in 2+2 quadruples shows statistically significant excess at 1 and 1.5. The former is likely a natural statistical preference in weakly interacting systems with periods within the same range. The latter is thought to be evidence of a capture in the 3:2 mean motion resonance of the two binaries. This sets important constraints on evolutionary channels in these systems. The total number of doubly eclipsing systems increased to 146, more than 90% of which are at low declinations on the southern sky.
Beam Test Studies of 3D Pixel Sensors Irradiated Non-Uniformly for the ATLAS Forward Physics Detector<|sep|>Pixel detectors with cylindrical electrodes that penetrate the silicon substrate (so called 3D detectors) offer advantages over standard planar sensors in terms of radiation hardness, since the electrode distance is decoupled from the bulk thickness. In recent years significant progress has been made in the development of 3D sensors, which culminated in the sensor production for the ATLAS Insertable B-Layer (IBL) upgrade carried out at CNM (Barcelona, Spain) and FBK (Trento, Italy). Based on this success, the ATLAS Forward Physics (AFP) experiment has selected the 3D pixel sensor technology for the tracking detector. The AFP project presents a new challenge due to the need for a reduced dead area with respect to IBL, and the in-homogeneous nature of the radiation dose distribution in the sensor. Electrical characterization of the first AFP prototypes and beam test studies of 3D pixel devices irradiated non-uniformly are presented in this paper.
Multi-color balance for color constancy<|sep|>In this paper, we propose a novel multi-color balance adjustment for color constancy. The proposed method, called "n-color balancing," allows us not only to perfectly correct n target colors on the basis of corresponding ground truth colors but also to correct colors other than the n colors. In contrast, although white-balancing can perfectly adjust white, colors other than white are not considered in the framework of white-balancing in general. In an experiment, the proposed multi-color balancing is demonstrated to outperform both conventional white and multi-color balance adjustments including Bradford's model.
STRATA: A Unified Framework for Task Assignments in Large Teams of Heterogeneous Agents<|sep|>Large teams of heterogeneous agents have the potential to solve complex multi-task problems that are intractable for a single agent working independently. However, solving complex multi-task problems requires leveraging the relative strengths of the different kinds of agents in the team. We present Stochastic TRAit-based Task Assignment (STRATA), a unified framework that models large teams of heterogeneous agents and performs effective task assignments. Specifically, given information on which traits (capabilities) are required for various tasks, STRATA computes the assignments of agents to tasks such that the trait requirements are achieved. Inspired by prior work in robot swarms and biodiversity, we categorize agents into different species (groups) based on their traits. We model each trait as a continuous variable and differentiate between traits that can and cannot be aggregated from different agents. STRATA is capable of reasoning about both species-level and agent-level variability in traits. Further, we define measures of diversity for any given team based on the team's continuous-space trait model. We illustrate the necessity and effectiveness of STRATA using detailed experiments based in simulation and in a capture-the-flag game environment.
Steady State of an Active Brownian Particle in Two-Dimensional Harmonic Trap<|sep|>We find an exact series solution for the steady-state probability distribution of a harmonically trapped active Brownian particle in two dimensions, in the presence of translational diffusion. This series solution allows us to efficiently explore the behavior of the system in different parameter regimes. Identifying "active" and "passive" regimes, we predict a surprising re-entrant active-to-passive transition with increasing trap stiffness. Our numerical simulations validate this finding. We discuss various interesting limiting cases wherein closed form expressions for the distributions can be obtained.
Limitations in timing precision due to single-pulse shape variability in millisecond pulsars<|sep|>High-sensitivity radio-frequency observations of millisecond pulsars usually show stochastic, broadband, pulse-shape variations intrinsic to the pulsar emission process. These variations induce jitter noise in pulsar timing observations; understanding the properties of this noise is of particular importance for the effort to detect gravitational waves with pulsar timing arrays. We assess the short-term profile and timing stability of 22 millisecond pulsars that are part of the Parkes Pulsar Timing Array sample by examining intra-observation arrival time variability and single-pulse phenomenology. In 7 of the 22 pulsars, in the band centred at approximately 1400MHz, we find that the brightest observations are limited by intrinsic jitter. We find consistent results, either detections or upper limits, for jitter noise in other frequency bands. PSR J1909-3744 shows the lowest levels of jitter noise, which we estimate to contribute $\sim$10 ns root mean square error to the arrival times for hour-duration observations. Larger levels of jitter noise are found in pulsars with wider pulses and distributions of pulse intensities. The jitter noise in PSR J0437-4715 decorrelates over a bandwidth of $\sim$2 GHz. We show that the uncertainties associated with timing pulsar models can be improved by including physically motivated jitter uncertainties. Pulse-shape variations will limit the timing precision at future, more sensitive, telescopes; it is imperative to account for this noise when designing instrumentation and timing campaigns for these facilities.
HI clouds in the proximity of M33<|sep|>Neutral hydrogen clouds are found in the Milky Way and Andromeda halo both as large complexes and smaller isolated clouds. Here we present a search for Hi clouds in the halo of M33, the third spiral galaxy of the Local Group. We have used two complementary data sets: a 3^o x 3^o map of the area provided by the Arecibo Legacy Fast ALFA (ALFALFA) survey and deeper pointed observations carried out with the Arecibo telescope in two fields that permit sampling of the north eastern and south-western edges of the HI disc. The total amount of Hi around M33 detected by our survey is $\sim 10^7$ M$_{\odot}$. At least 50% of this mass is made of HI clouds that are related both in space and velocity to the galaxy. We discuss several scenarios for the origin of these clouds focusing on the two most interesting ones: $(a)$ dark-matter dominated gaseous satellites, $(b)$ debris from filaments flowing into M33 from the intergalactic medium or generated by a previous interaction with M31. Both scenarios seem to fit with the observed cloud properties. Some structures are found at anomalous velocities, particularly an extended HI complex previously detected by Thilker et al. (2002). Even though the ALFALFA observations seem to indicate that this cloud is possibly connected to M33 by a faint gas bridge, we cannot firmly establish its extragalactic nature or its relation to M33. Taking into account that the clouds associated with M33 are likely to be highly ionised by the extragalactic UV radiation, we predict that the total gas mass associated with them is > 5 x 10^7 M$_{\odot}$. If the gas is steadily falling towards the M33 disc it can provide the fuel needed to sustain a current star formation rate of 0.5 M$_{\odot}$ yr$^{-1}$.
The theory of the quantum kernel-based binary classifier<|sep|>Binary classification is a fundamental problem in machine learning. Recent development of quantum similarity-based binary classifiers and kernel method that exploit quantum interference and feature quantum Hilbert space opened up tremendous opportunities for quantum-enhanced machine learning. To lay the fundamental ground for its further advancement, this work extends the general theory of quantum kernel-based classifiers. Existing quantum kernel-based classifiers are compared and the connection among them is analyzed. Focusing on the squared overlap between quantum states as a similarity measure, the essential and minimal ingredients for the quantum binary classification are examined. The classifier is also extended concerning various aspects, such as data type, measurement, and ensemble learning. The validity of the Hilbert-Schmidt inner product, which becomes the squared overlap for pure states, as a positive definite and symmetric kernel is explicitly shown, thereby connecting the quantum binary classifier and kernel methods.
Short Term Flux and Colour Variations in Low-Energy Peaked Blazars<|sep|>We have measured multi-band optical flux and colour variations for a sample of 12 low energy peaked blazars on short, day-to-month, timescales. Our sample contains six BL Lacertae objects and six flat spectrum radio quasars. These photometric observations, made during September 2008 to June 2009, used five optical telescopes, one in India and four in Bulgaria. We detected short term flux variations in eleven of these blazars and colour variability in eight of them. Our data indicate that six blazars (3C 66A, AO 0235+164, S5 0716+714, PKS 0735+178, OJ 287 and 3C 454.3) were observed in pre- or post-outburst states, that five (PKS 0420-014, 4C 29.45, 3C 279, PKS 1510-089 and BL Lac) were in a low state, while one (3C 273) was in an essentially steady state. The duty cycles for flux and colour variations on short timescales in these low energy peaked blazars are ~ 92 percent and ~ 33 percent, respectively. The colour vs magnitude correlations seen here support the hypothesis that BL Lac objects tend to become bluer with increase in brightness; however, flat spectrum radio quasars may show the opposite trend, and there are exceptions to these trends in both categories of blazar. We briefly discuss emission models for active galactic nuclei that might explain our results.
Dynamic Team Theory of Stochastic Differential Decision Systems with Decentralized Noisy Information Structures via Girsanov's Measure Transformation<|sep|>In this paper, we present two methods which generalize static team theory to dynamic team theory, in the context of continuous-time stochastic nonlinear differential decentralized decision systems, with relaxed strategies, which are measurable to different noisy information structures. For both methods we apply Girsanov's measure transformation to obtain an equivalent dynamic team problem under a reference probability measure, so that the observations and information structures available for decisions, are not affected by any of the team decisions. The first method is based on function space integration with respect to products of Wiener measures, and generalizes Witsenhausen's [1] definition of equivalence between discrete-time static and dynamic team problems. The second method is based on stochastic Pontryagin's maximum principle. The team optimality conditions are given by a "Hamiltonian System" consisting of forward and backward stochastic differential equations, and a conditional variational Hamiltonian with respect to the information structure of each team member, expressed under the initial and a reference probability space via Girsanov's measure transformation. Under global convexity conditions, we show that that PbP optimality implies team optimality. In addition, we also show existence of team and PbP optimal relaxed decentralized strategies (conditional distributions), in the weak$^*$ sense, without imposing convexity on the action spaces of the team members. Moreover, using the embedding of regular strategies into relaxed strategies, we also obtain team and PbP optimality conditions for regular team strategies, which are measurable functions of decentralized information structures, and we use the Krein-Millman theorem to show realizability of relaxed strategies by regular strategies.
Two-Photon Exchange: Future experimental prospects<|sep|>The proton elastic form factor ratio is accessible in unpolarized Rosenbluth-type experiments as well as experiments which make use of polarization degrees of freedom. The extracted values show a distinct discrepancy, growing with $Q^2$. Three recent experiments tested the proposed explanation, two-photon exchange, by measuring the positron-proton to electron-proton cross section ratio. In the results, a small two-photon exchange effect is visible, significantly different from theoretical calculation. Theory at larger momentum transfer remains untested. This paper discusses the possibilities for future measurements at larger momentum transfer.
Preemptive Investment under Uncertainty<|sep|>This paper provides a general characterization of subgame perfect equilibria for strategic timing problems, where two firms have the (real) option to make an irreversible investment. Profit streams are uncertain and depend on the market structure. The analysis is based directly on the inherent economic structure of the model. In particular, the determination of equilibria with preemptive investment is reduced to solving a single class of constrained optimal stopping problems. The general results are applied to typical state-space models, completing commonly insufficient equilibrium arguments, showing when uncertainty leads to qualitatively different behavior, and establishing additional equilibria that are Pareto improvements.
Formation of hypernuclei in high energy reactions within a covariant transport model<|sep|>We investigate the formation of fragments with strangeness degrees of freedom in proton- and heavy-ion-induced reactions at high relativistic energies. The model used is a combination of a dynamical transport model and a statistical approach of fragment formation. We discuss in detail the applicability and limitations of such a hybrid model by comparing data on spectator fragmentation at relativistic $SIS/GSI$-energies. The theoretical results are analyzed in terms of spectator fragmentation with strangeness degrees of freedom such as the production of single-$\Lambda-{}^{3,4,5}He$ hypernuclei. We provide theoretical estimates on the spectra and on inclusive cross sections of light hypernuclei, which could be helpful for future experiments on hypernuclear physics at the new GSI- and J-PARC-facilities.
X-ray flares, plateaus, and chromatic breaks of GRB afterglows from up-scattered forward-shock emission<|sep|>Scattering of the forward-shock synchrotron emission by a relativistic outflow located behind the leading blast-wave may produce an X-ray emission brighter than that coming directly from the forward-shock and may explain four features displayed by Swift X-ray afterglows: flares, plateaus (slow decays), chromatic light-curve breaks, and fast post-plateau decays. For a cold scattering outflow, the reflected flux overshines the primary one if the scattering outflow is nearly baryon-free and highly relativistic. These two requirements can be relaxed if the scattering outflow is energized by weak internal shocks, so that the incident forward-shock photons are also inverse-Compton scattered, in addition to bulk-scattering. Sweeping-up of the photons left behind by the forward shock naturally yields short X-ray flares. Owing to the boost in photon energy produced by bulk-scattering scattering, the reflected emission is more likely to overshine that coming directly from the forward shock at higher photon energies, yielding light-curve plateaus and breaks that appear only in the X-ray. The brightness, shape, and decay of the X-ray light-curve plateau depend on the radial distribution of the scatterer's Lorentz factor and mass-flux. Chromatic X-ray light-curve breaks and sharp post-plateau decays cannot be accommodated by the direct forward-shock emission and argue in favour of the scattering-outflow model proposed here. On the other hand, the X-ray afterglows without plateaus, those with achromatic breaks, and those with very long-lived power-law decays are more naturally accommodated by the standard forward-shock model. Thus the diversity of X-ray light-curves arises from the interplay of the scattered and direct forward-shock emissions.
Human Pose Estimation from RGB Input Using Synthetic Training Data<|sep|>We address the problem of estimating the pose of humans using RGB image input. More specifically, we are using a random forest classifier to classify pixels into joint-based body part categories, much similar to the famous Kinect pose estimator [11], [12]. However, we are using pure RGB input, i.e. no depth. Since the random forest requires a large number of training examples, we are using computer graphics generated, synthetic training data. In addition, we assume that we have access to a large number of real images with bounding box labels, extracted for example by a pedestrian detector or a tracking system. We propose a new objective function for random forest training that uses the weakly labeled data from the target domain to encourage the learner to select features that generalize from the synthetic source domain to the real target domain. We demonstrate on a publicly available dataset [6] that the proposed objective function yields a classifier that significantly outperforms a baseline classifier trained using the standard entropy objective [10].
A polynomial time {\lambda}-calculus with multithreading and side effects<|sep|>The framework of Light Logics has been extensively studied to control the complexity of higher-order functional programs. We propose an extension of this framework to multithreaded programs with side effects, focusing on the case of polynomial time. After introducing a modal \lambda-calculus with parallel composition and regions, we prove that a realistic call-by-value evaluation strategy can be computed in polynomial time for a class of well-formed programs. The result relies on the simulation of call-by-value by a polynomial shallow-first strategy which preserves the evaluation order of side effects. Then, we provide a polynomial type system that guarantees that well-typed programs do not go wrong. Finally, we illustrate the expressivity of the type system by giving a programming example of concurrent iteration producing side effects over an inductive data structure.
Electrokinetic oscillatory flow and energy conversion of viscoelastic fluids in microchannels: a linear analysis<|sep|>We study the electrokinetic flow of viscoelastic fluids subjected to an oscillatory pressure gradient, and particularly focus on the resonance behaviors in the flow. The governing equations are restricted to linear regime so that the velocity and streaming potential fields can be solved analytically. Based on the interaction of viscoelastic shear waves, we explain the mechanism of resonance, and derive a critical Deborah number Dec = 1/4 which dictates the occurrence of resonance. Using the Maxwell fluid model, we show that the resonance enhances electrokinetic effects and results in a dramatic increase of electrokinetic energy conversion efficiency. However, by applying the Oldroyd-B fluid model it reveals that the amplification of efficiency is suppressed even for a very small Newtonian solvent contribution. This may be one of the reasons that experimental verification regarding the high efficiency predicted by Bandopadhyay & Chakraborty (Appl. Phys. Lett., vol. 101, 2012, 043905) is unavailable in the literature. Furthermore, the damping effect of solvent viscosity is more significant for higher-order resonances. Introducing the factor of multiple relaxation times, we show that the occurrence of resonances for the streaming potential field and the flow rate are still dominated by Dec. For the efficiency in the multi-mode case, the occurrence of resonance is dominated by the Deborah number De and the mode number N, and the resonance disappears for small De or large N. In addition, a new type of scaling relation between the streaming potential field and EDL thickness can be identified at large De.
Generated Knowledge Prompting for Commonsense Reasoning<|sep|>It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at https://github.com/liujch1998/GKP
Multi-mode entanglement of N harmonic oscillators coupled to a non-Markovian reservoir<|sep|>Multi-mode entanglement is investigated in the system composed of $N$ coupled identical harmonic oscillators interacting with a common environment. We treat the problem very general by working with the Hamiltonian without the rotating-wave approximation and by considering the environment as a non-Markovian reservoir to the oscillators. We invoke an $N$-mode unitary transformation of the position and momentum operators and find that in the transformed basis the system is represented by a set of independent harmonic oscillators with only one of them coupled to the environment. Working in the Wigner representation of the density operator, we find that the covariance matrix has a block diagonal form that it can be expressed in terms of multiples of $3\times 3$ and $4\times 4$ matrices. This simple property allows to treat the problem to some extend analytically. We illustrate the advantage of working in the transformed basis on a simple example of three harmonic oscillators and find that the entanglement can persists for long times due to presence of constants of motion for the covariance matrix elements. We find that, in contrast to what one could expect, a strong damping of the oscillators leads to a better stationary entanglement than in the case of a weak damping.
CoRe: Color Regression for Multicolor Fashion Garments<|sep|>Developing deep networks that analyze fashion garments has many real-world applications. Among all fashion attributes, color is one of the most important yet challenging to detect. Existing approaches are classification-based and thus cannot go beyond the list of discrete predefined color names. In this paper, we handle color detection as a regression problem to predict the exact RGB values. That's why in addition to a first color classifier, we include a second regression stage for refinement in our newly proposed architecture. This second step combines two attention models: the first depends on the type of clothing, the second depends on the color previously detected by the classifier. Our final prediction is the weighted spatial pooling over the image pixels RGB values, where the illumination has been corrected. This architecture is modular and easily expanded to detect the RGBs of all colors in a multicolor garment. In our experiments, we show the benefits of each component of our architecture.
Fluctuation-driven capacity distribution in complex networks<|sep|>Maximizing robustness and minimizing cost are common objectives in the design of infrastructure networks. However, most infrastructure networks evolve and operate in a highly decentralized fashion, which may significantly impact the allocation of resources across the system. Here, we investigate this question by focusing on the relation between capacity and load in different types of real-world communication and transportation networks. We find strong empirical evidence that the actual capacity of the network elements tends to be similar to the maximum available capacity, if the cost is not strongly constraining. As more weight is given to the cost, however, the capacity approaches the load nonlinearly. In particular, all systems analyzed show larger unoccupied portions of the capacities on network elements subjected to smaller loads, which is in sharp contrast with the assumptions involved in (linear) models proposed in previous theoretical studies. We describe the observed behavior of the capacity-load relation as a function of the relative importance of the cost by using a model that optimizes capacities to cope with network traffic fluctuations. These results suggest that infrastructure systems have evolved under pressure to minimize local failures, but not necessarily global failures that can be caused by the spread of local damage through cascading processes.
Disentangled Human Body Embedding Based on Deep Hierarchical Neural Network<|sep|>Human bodies exhibit various shapes for different identities or poses, but the body shape has certain similarities in structure and thus can be embedded in a low-dimensional space. This paper presents an autoencoder-like network architecture to learn disentangled shape and pose embedding specifically for the 3D human body. This is inspired by recent progress of deformation-based latent representation learning. To improve the reconstruction accuracy, we propose a hierarchical reconstruction pipeline for the disentangling process and construct a large dataset of human body models with consistent connectivity for the learning of the neural network. Our learned embedding can not only achieve superior reconstruction accuracy but also provide great flexibility in 3D human body generation via interpolation, bilinear interpolation, and latent space sampling. The results from extensive experiments demonstrate the powerfulness of our learned 3D human body embedding in various applications.
Moments of nucleon distribution amplitudes from irreducible three-quark operators<|sep|>Semi-exclusive and exclusive processes are becoming more and more important in high energy physics since they are excellently suited to study the internal hadronic structure. To analyze such processes the knowledge of the hadron distribution amplitudes, which are universal for different reactions, is essential. Only rather indirect information on these nonperturbative functions can be obtained from measurements. In this work we report on a lattice QCD computation of moments of nucleon distribution amplitudes using suitable three-quark operators. However, these operators have to be renormalized and the mixing is even more complicated than in the continuum. Using the symmetry group of the hypercubic lattice we therefore derive and implement irreducibly transforming three-quark operators, which allow us to control the mixing pattern and will finally lead to quantitative predictions in the MSbar scheme. We present preliminary results for leading-twist and next-to-leading twist nucleon distribution amplitudes based on the QCDSF/UKQCD simulations with 2 flavors of dynamical clover fermions.
Weingarten calculus for matrix ensembles associated with compact symmetric spaces<|sep|>A method for computing integrals of polynomial functions on compact symmetric spaces is given. Those integrals are expressed as sums of functions on symmetric groups.
Two-electron state from the Floquet scattering matrix perspective<|sep|>Two single-particle sources coupled in series to a chiral electronic waveguide can serve as a probabilistic source of two-particle excitations with tunable properties. The second-order correlation function, characterizing the state of emitted electrons in space-time, is expressed in terms of the Floquet scattering matrix of a source. It is shown that the Fourier transform of the correlation function, characterizing the emitted state in energy space, can be accessed with the help of an energy resolved shot-noise measurement. The two-electron state emitted adiabatically is discussed in detail. In particular, the two-electron wave function is represented via two different sets of single-particle wave functions accessible experimentally.
On-the-fly construction of surrogate constitutive models for concurrent multiscale mechanical analysis through probabilistic machine learning<|sep|>Concurrent multiscale finite element analysis (FE2) is a powerful approach for high-fidelity modeling of materials for which a suitable macroscopic constitutive model is not available. However, the extreme computational effort associated with computing a nested micromodel at every macroscopic integration point makes FE2 prohibitive for most practical applications. Constructing surrogate models able to efficiently compute the microscopic constitutive response is therefore a promising approach in enabling concurrent multiscale modeling. This work presents a reduction framework for adaptively constructing surrogate models based on statistical learning. The nested micromodels are replaced by a machine learning surrogate model based on Gaussian Processes (GP). The need for offline data collection is bypassed by training the GP models online based on data coming from a small set of fully-solved anchor micromodels that undergo the same strain history as their associated macro integration points. The Bayesian formalism inherent to GP models provides a natural tool for uncertainty estimation through which new observations or inclusion of new anchors are triggered. The surrogate constitutive manifold is constructed with as few micromechanical evaluations as possible by enhancing the GP models with gradient information and the solution scheme is made robust through a greedy data selection approach embedded within the conventional finite element solution loop for nonlinear analysis. The sensitivity to model parameters is studied with a tapered bar example with plasticity, while the applicability of the model to more complex cases is demonstrated with the elastoplastic analysis of a plate with multiple cutouts and a crack growth example for mixed-mode bending. Significant efficiency gains are obtained without resorting to offline training.
Dynamical formation of detached trans-Neptunian objects close to the 2:5 and 1:3 mean motion resonances with Neptune<|sep|>Through a semi-analytic approach of the Kozai resonance inside an MMR, we show phase diagrams (e,{\omega}) that suggest the possibility of a scattered particle, after being captured in an MMR with Neptune, to become a detached object. We ran several numerical integrations with thousands of particles perturbed by the four major planets, and there are cases with and without Neptune's residual migration. These were developed to check the semi-analytic approach and to better understand the dynamical mechanisms that produce the detached objects close to an MMR. The numerical simulations with and without a residual migration for Neptune stress the importance of a particular resonance mode, which we name the hibernating mode, on the formation of fossilized detached objects close to MMRs. When considering Neptune's residual migration we are able to show the formation of detached orbits. These objects are fossilized and cannot be trapped in the MMRs again. We find a ratio of the number of fossilized objects with moderate perihelion distance (35 < q < 40 au) to the number of objects with high perihelion distance (q > 40 au) as 3.0/1 for objects close to the 2:5, and 1.7/1 for objects close to the 1:3 resonance. We estimate that the two fossilized population have a total mass between 0.1 and 0.3 Pluto's mass.
Unsupervised Person Re-identification by Deep Asymmetric Metric Embedding<|sep|>Person re-identification (Re-ID) aims to match identities across non-overlapping camera views. Researchers have proposed many supervised Re-ID models which require quantities of cross-view pairwise labelled data. This limits their scalabilities to many applications where a large amount of data from multiple disjoint camera views is available but unlabelled. Although some unsupervised Re-ID models have been proposed to address the scalability problem, they often suffer from the view-specific bias problem which is caused by dramatic variances across different camera views, e.g., different illumination, viewpoints and occlusion. The dramatic variances induce specific feature distortions in different camera views, which can be very disturbing in finding cross-view discriminative information for Re-ID in the unsupervised scenarios, since no label information is available to help alleviate the bias. We propose to explicitly address this problem by learning an unsupervised asymmetric distance metric based on cross-view clustering. The asymmetric distance metric allows specific feature transformations for each camera view to tackle the specific feature distortions. We then design a novel unsupervised loss function to embed the asymmetric metric into a deep neural network, and therefore develop a novel unsupervised deep framework named the DEep Clustering-based Asymmetric MEtric Learning (DECAMEL). In such a way, DECAMEL jointly learns the feature representation and the unsupervised asymmetric metric. DECAMEL learns a compact cross-view cluster structure of Re-ID data, and thus help alleviate the view-specific bias and facilitate mining the potential cross-view discriminative information for unsupervised Re-ID. Extensive experiments on seven benchmark datasets whose sizes span several orders show the effectiveness of our framework.
Omnidirectional Three Module Robot Design and Simulation<|sep|>This paper introduces the Omnidirectional Tractable Three Module Robot for traversing inside complex pipe networks. The robot consists of three omnidirectional modules fixed 120{\deg} apart circumferentially which can rotate about their axis allowing holonomic motion of the robot. Holonomic motion enables the robot to overcome motion singularity when negotiating T-junctions and further allows the robot to arrive in a preferred orientation while taking turns inside a pipe. The singularity region while negotiating T-junctions is analyzed to formulate the geometry of the region. The design and motion capabilities are validated by conducting simulations in MSC ADAMS on a simplified lumped-model of the robot.
D2D Multi-hop Energy Efficiency Toward EMS in B5G<|sep|>Beyond fifth generation (B5G), communication has attracted much attention from academia, industry, and mobile network operators due to network densification, ultra-low latency communication, and enhanced energy and spectrum efficiency. However, a post-disaster emergency management system (EMS), which increasingly relies heavily on wireless communication infrastructure, is falling far behind in innovation and funding. Because the B5G concept represents a telecommunications industry revolution, EMS provisioning is intended to be dispersed, autonomous, and robust to network weaknesses caused by human and natural calamities. When the network is congested, partially functioning, or entirely isolated, we provide multi-device-to-device (D2D) communication to extend the communication coverage area with improved energy efficiency. Furthermore, we examine D2D multi-hop energy efficiency performance in the proposed network. The results demonstrate that the improved D2D multi-hop energy efficiency can improve the EMS effectively and efficiently in extending the coverage area and enhancing energy efficiency. Moreover, the proposed approach has been proven to increase energy efficiency, which acts as a suitable network design to recover from natural disasters and potentially save many lives
The Formation of Spheroids in Early-Type Spirals: Clues From Their Globular Clusters<|sep|>We use deep Hubble Space Telescope images taken with the Advanced Camera for Surveys (ACS) in the F475W and F814W filters to investigate the globular cluster systems in four edge-on Sa spiral galaxies covering a factor of 4 in luminosity. The specific frequencies of the blue globular clusters in the galaxies in our sample fall in the range 0.34 -- 0.84, similar to typical values found for later-type spirals. The number of red globular clusters associated with the bulges generally increases with the bulge luminosity, similar to what is observed for elliptical galaxies, although the specific frequency of bulge clusters is a factor of 2-3 lower for the lowest luminosity bulges than for the higher luminosity bulges. We present a new empirical relation between the fraction of red globular clusters and total bulge luminosity based on the elliptical galaxies studied by ACSVCS (ACS Virgo Cluster Survey), and discuss how this diagram can be used to assess the importance that dissipative processes played in building spiral bulges. Our results suggest a picture where dissipative processes, which are expected during gas-rich major mergers, were more important for building luminous bulges of Sa galaxies, whereas secular evolution may have played a larger role in building lower-luminosity bulges in spirals.
Complex Analysis of the Stellar Binary HD25811; A Subgiant System<|sep|>The visually close binary system HD25811 is analyzed to estimate its physical and geometrical parameters in addition to its spectral type and luminosity class. The method depends on obtaining the best fit between the entire observational spectral energy distribution (SED) of the system and synthetic SEDs created by atmospheric modeling of the individual components, consistent with the system's modified orbital elements. The parameters of the individual components of the system are derived as: $T_{\rm eff}^{\rm a} =6850\pm50$\,K, $T_{\rm eff}^{\rm b} =7000\pm50$\,K, log $g_{\rm a}=4.04\pm0.10$, log $g_{\rm b}=4.15\pm0.10$, $R_{\rm a}=1.96\pm0.20$\,R$_{\odot}$, $R_{\rm b}=1.69\pm0.20$\,R$_{\odot}$, $M_{va}=1.97\pm0.20$, $M_{vb}=2.19\pm0.20$, $L_a= 7.59\pm0.70 L_\odot, L_b= 6.16\pm0.70 L_\odot$ with dynamical parallax $\pi(\textrm{mas})=5.095\pm 0.095$. The analysis shows that the system consists of a $1.55M_{\odot}$ F2 primary star and a less evolved $1.50M_{\odot}$ F1 secondary subgiant star with ages around 2 Gy formed by fragmentation. Synthetic magnitudes of both components were calculated under Johnson-Cousins, Str\"{o}mgren, and Tycho photometrical systems.
Shakeout: A New Approach to Regularized Deep Neural Network Training<|sep|>Recent years have witnessed the success of deep neural networks in dealing with a plenty of practical problems. Dropout has played an essential role in many successful deep neural networks, by inducing regularization in the model training. In this paper, we present a new regularized training approach: Shakeout. Instead of randomly discarding units as Dropout does at the training stage, Shakeout randomly chooses to enhance or reverse each unit's contribution to the next layer. This minor modification of Dropout has the statistical trait: the regularizer induced by Shakeout adaptively combines $L_0$, $L_1$ and $L_2$ regularization terms. Our classification experiments with representative deep architectures on image datasets MNIST, CIFAR-10 and ImageNet show that Shakeout deals with over-fitting effectively and outperforms Dropout. We empirically demonstrate that Shakeout leads to sparser weights under both unsupervised and supervised settings. Shakeout also leads to the grouping effect of the input units in a layer. Considering the weights in reflecting the importance of connections, Shakeout is superior to Dropout, which is valuable for the deep model compression. Moreover, we demonstrate that Shakeout can effectively reduce the instability of the training process of the deep architecture.
A Long-term Dependent and Trustworthy Approach to Reactor Accident Prognosis based on Temporal Fusion Transformer<|sep|>Prognosis of the reactor accident is a crucial way to ensure appropriate strategies are adopted to avoid radioactive releases. However, there is very limited research in the field of nuclear industry. In this paper, we propose a method for accident prognosis based on the Temporal Fusion Transformer (TFT) model with multi-headed self-attention and gating mechanisms. The method utilizes multiple covariates to improve prediction accuracy on the one hand, and quantile regression methods for uncertainty assessment on the other. The method proposed in this paper is applied to the prognosis after loss of coolant accidents (LOCAs) in HPR1000 reactor. Extensive experimental results show that the method surpasses novel deep learning-based prediction methods in terms of prediction accuracy and confidence. Furthermore, the interference experiments with different signal-to-noise ratios and the ablation experiments for static covariates further illustrate that the robustness comes from the ability to extract the features of static and historical covariates. In summary, this work for the first time applies the novel composite deep learning model TFT to the prognosis of key parameters after a reactor accident, and makes a positive contribution to the establishment of a more intelligent and staff-light maintenance method for reactor systems.
Pushing the limits of the CyberGrasp for haptic rendering<|sep|>The CyberGrasp is a well known dataglove-exoskeleton device combination that allows to render haptic feedback to the human fingers. Its design, however, restricts its usability for teleoperation through a limited control bandwidth and position sensor resolution. Therefore the system is restricted to low achievable contact stiffness and feedback gain magnitudes in haptic rendering. Moreover, the system prohibits simple adaption of its controller implementation. In this paper, the ExHand Box is presented, a newly designed back-end to widen the CyberGrasp's bandwidth restrictions and to open it up for fully customized controller implementations. The ExHand Box provides a new computer, interface electronics and motor controllers for the otherwise unmodified CyberGlove and CyberGrasp hand systems. The loop frequency of the new system can be freely varied up to 2 kHz and custom controllers can be implemented through an automatic code generation interface. System performance identification experiments are presented that demonstrate improved behavior in hard contact situations over a range of sampling periods. Maximum contact stiffnesses of up to 50kN/m in a stable condition are demonstrated, which is significantly higher than what could be achieved with the non-customized original system version. Moreover, a bilateral control experiment is conducted to demonstrate the new system's usability for generic teleoperation research. In this experiment a raycasting algorithm is introduced for pre-contact detection in order to compensate for high delay and jitter communication links between master and slave as they appear in an Ethernet network. It is demonstrated that the contact stiffness can be maintained in the order of magnitude of the system performance identification with a demonstrated stiffness of 41kN/m in a stable condition.
Multiferroic FeTe$_2$O$_5$Br: Alternating spin chains with frustrated interchain interactions<|sep|>A combination of density functional theory calculations, many-body model considerations, magnetization and electron spin resonance measurements shows that the multiferroic FeTe$_2$O$_5$Br should be described as a system of alternating antiferromagnetic $S=5/2$ chains with strong Fe-O-Te-O-Fe bridges weakly coupled by two-dimensional frustrated interactions, rather than the previously reported tetramer models. The peculiar temperature dependence of the incommensurate magnetic vector can be explained in terms of interchain exchange striction being responsible for the emergent net electric polarization.
Sign and amplitude representation of the forex networks<|sep|>We decompose the exchange rates returns of 41 currencies (incl. gold) into their sign and amplitude components. Then we group together all exchange rates with a common base currency, construct Minimal Spanning Trees for each group independently, and analyze properties of these trees. We show that both the sign and the amplitude time series have similar correlation properties as far as the core network structure is concerned. There exist however interesting peripheral differences that may open a new perspective to view the Forex dynamics.
Plasmon recombination in narrowgap HgTe quantum wells<|sep|>The dispersion laws of two-dimensional plasmons in narrow-gap HgTe/CdHgTe quantum wells are calculated taking into account the spatial dispersion of the electron susceptibility. At the energy scale of the band gap the dependence of plasmon frequencies on the wave vector is shown to be close to linear that changes significantly the critical concentration of noneqilibrium electron-hole gas corresponding to "switching-on" the carrier recombination with plasmon emission. The recombination rates with the plasmon emission have been calculated.
An Introduction to Deep Visual Explanation<|sep|>The practical impact of deep learning on complex supervised learning problems has been significant, so much so that almost every Artificial Intelligence problem, or at least a portion thereof, has been somehow recast as a deep learning problem. The applications appeal is significant, but this appeal is increasingly challenged by what some call the challenge of explainability, or more generally the more traditional challenge of debuggability: if the outcomes of a deep learning process produce unexpected results (e.g., less than expected performance of a classifier), then there is little available in the way of theories or tools to help investigate the potential causes of such unexpected behavior, especially when this behavior could impact people's lives. We describe a preliminary framework to help address this issue, which we call "deep visual explanation" (DVE). "Deep," because it is the development and performance of deep neural network models that we want to understand. "Visual," because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and "Explanation," because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model. In the exposition of our preliminary framework, we use relatively straightforward image classification examples and a variety of choices on initial configuration of a deep model building scenario. By careful but not complicated instrumentation, we expose classification outcomes of deep models using visualization, and also show initial results for one potential application of interpretability.
On the Capacity of Diffusion-Based Molecular Communications with SiNW FET-Based Receiver<|sep|>Molecular communication (MC) is a bio-inspired communication method based on the exchange of molecules for information transfer among nanoscale devices. Although MC has been extensively studied from various aspects, limitations imposed by the physical design of transceiving units have been largely neglected in the literature. Recently, we have proposed a nanobioelectronic MC receiver architecture based on the nanoscale field effect transistor-based biosensor (bioFET) technology, providing noninvasive and sensitive molecular detection at nanoscale while producing electrical signals at the output. In this paper, we derive analytical closed-form expressions for the capacity and capacity-achieving input distribution for a memoryless MC channel with a silicon nanowire (SiNW) FET-based MC receiver. The resulting expressions could be used to optimize the information flow in MC systems equipped with nanobioelectronic receivers.
Report of the Topical Group on Cosmic Frontier 5 Dark Energy and Cosmic Acceleration: Cosmic Dawn and Before for Snowmass 2021<|sep|>This report summarizes the envisioned research activities as gathered from the Snowmass 2021 CF5 working group concerning Dark Energy and Cosmic Acceleration: Cosmic Dawn and Before. The scientific goals are to study inflation and to search for new physics through precision measurements of relic radiation from the early universe. The envisioned research activities for this decade (2025-35) are constructing and operating major facilities and developing critical enabling capabilities. The major facilities for this decade are the CMB-S4 project, a new Stage-V spectroscopic survey facility, and existing gravitational wave observatories. Enabling capabilities include aligning and investing in theory, computation and model building, and investing in new technologies needed for early universe studies in the following decade (2035+).
Lattices and the Geometry of Numbers<|sep|>In this paper we discuss about properties of lattices and its application in theoretical and algorithmic number theory. This result of Minkowski regarding the lattices initiated the subject of Geometry of Numbers, which uses geometry to study the properties of algebraic numbers. It has application on various other fields of mathematics especially the study of Diophantine equations, analysis of functional analysis etc. This paper will review all the major developments that have occurred in the field of geometry of numbers. In this paper we shall first give a broad overview of the concept of lattice and then discuss about the geometrical properties it has and its applications.
Start-up flow of a viscoelastic fluid in a pipe with fractional Maxwell's model<|sep|>Unidirectional start-up flow of a viscoelastic fluid in a pipe with fractional Maxwell's model is studied. The flow starting from rest is driven by a constant pressure gradient in an infinite long straight pipe. By employing the method of variable separations and Heaviside operational calculus, we obtain the exact solution, from which the flow characteristics are investigated. It is found that the start-up motion of fractional Maxwell's fluid with parameters $\alpha$ and $\beta$, tends to be at rest as time goes to infinity, except the case of $\beta=1$. This observation, which also can be predicted from the mechanics analogue of fractional Maxwell's model, agrees with the classical work of Friedrich and it indicates fractional Maxwell's fluid presents solid-like behavior if $\be\neq 1$ and fluid-like behavior if $\be=1$. For an arbitrary viscoelastic model, a conjecture is proposed to give an intuitive way judging whether it presents fluid-like or solid-like behavior. Also oscillations may occur before the fluid tends to the asymptotic behavior stated above, which is a common phenomenon for viscoelastic fluids.
Quasiparticle velocities in 2D electron/hole liquids with spin-orbit coupling<|sep|>We study the influence of spin-orbit interactions on quasiparticle dispersions in two-dimensional electron and heavy-hole liquids in III-V semiconductors. To obtain closed-form analytical results, we restrict ourselves to spin-orbit interactions with isotropic spectrum and work within the screened Hartree-Fock approximation, valid in the high-density limit. For electrons having a linear-in-momentum Rashba (or, equivalently, Dresselhaus) spin-orbit interaction, we show that the screened Hartree-Fock approximation recovers known results based on the random-phase approximation and we extend those results to higher order in the spin-orbit coupling. While the well-studied case of electrons leads only to a weak modification of quasiparticle properties in the presence of the linear-in-momentum spin-orbit interaction, we find two important distinctions for hole systems (with a leading nonlinear-in-momentum spin-orbit interaction). First, the group velocities associated with the two hole-spin branches acquire a significant difference in the presence of spin-orbit interactions, allowing for the creation of spin-polarized wavepackets in zero magnetic field. Second, we find that the interplay of Coulomb and spin-orbit interactions is significantly more important for holes than for electrons and can be probed through the quasiparticle group velocities. These effects should be directly observable in magnetotransport, Raman scattering, and femtosecond-resolved Faraday rotation measurements. Our results are in agreement with a general argument on the velocities, which we formulate for an arbitrary choice of the spin-orbit coupling.
Structure and evolution of solar supergranulation using SDO/HMI data<|sep|>Context: Studying the motions on the solar surface is fundamental for understanding how turbulent convection transports energy and how magnetic fields are distributed across the solar surface. Aims: From horizontal velocity measurements all over the visible disc of the Sun and using data from the Solar Dynamics Observatory/Helioseismic and Magnetic Imager (SDO/HMI), we investigate the structure and evolution of solar supergranulation. Methods: Horizontal velocity fields were measured by following the proper motions of solar granules using a newly developed version of the coherent structure tracking (CST) code. With this tool, maps of horizontal divergence were computed. We then segmented and identified supergranular cells and followed their histories by using spatio-temporal labelling. With this dataset we derived the fundamental properties of supergranulation, including their motion. Results: We find values of the fundamental parameters of supergranulation similar to previous studies: a mean lifetime of 1.5 days and a mean diameter of 25~Mm. The tracking of individual supergranular cells reveals the solar differential rotation and a poleward circulation trend of the meridional flow. The shape of the derived differential rotation and meridional flow does not depend on the cell size. If there is a background magnetic field, the diverging flows in supergranules are weaker. Conclusions: This study confirms that supergranules are suitable tracers that may be used to investigate the large-scale flows of the solar convection as long as they are detectable enough on the surface.
Phase Crystals<|sep|>Superconductivity owes its properties to the phase of the electron pair condensate that breaks the $U(1)$ symmetry. In the most traditional ground state, the phase is uniform and rigid. The normal state can be unstable towards special inhomogeneous superconducting states: the Abrikosov vortex state, and the Fulde-Ferrell-Larkin-Ovchinnikov state. Here we show that the phase-uniform superconducting state can go into a fundamentally different and more ordered non-uniform ground state, that we denote as a phase crystal. The new state breaks translational invariance through formation of a spatially periodic modulation of the phase, manifested by unusual superflow patterns and circulating currents, that also break time-reversal symmetry. We list the general conditions needed for realization of phase crystals. Using microscopic theory we then derive an analytic expression for the superfluid density tensor for the case of a non-uniform environment in a semi-infinite superconductor. We demonstrate how the surface quasiparticle states enter the superfluid density and identify phase crystallization as the main player in several previous numerical observations in unconventional superconductors, and predict existence of a similar phenomenon in superconductor-ferromagnetic structures. This analytic approach provides a new unifying aspect for the exploration of boundary-induced quasiparticles and collective excitations in superconductors. More generally, we trace the origin of phase crystallization to non-local properties of the gradient energy, which implies existence of similar pattern-forming instabilities in many other contexts.
Photonic Crystal Spectrometer<|sep|>We demonstrate a new kind of optical spectrometer employing photonic crystal patterns to outcouple waveguided light from a transparent substrate. This spectrometer consists of an array of photonic crystal patterns, nanofabricated in a polymer on a glass substrate, combined with a camera. The camera captures an image of the light outcoupled from the patterned substrate; the array of patterns produces a spatially resolved map of intensities for different wavelength bands. The intensity map of the image is converted into a spectrum using the photonic crystal pattern response functions. We present a proof of concept by characterizing a white LED with our photonic crystal spectrometer.
Color graph based wavelet transform with perceptual information<|sep|>In this paper, we propose a numerical strategy to define a multiscale analysis for color and multicomponent images based on the representation of data on a graph. Our approach consists in computing the graph of an image using the psychovisual information and analysing it by using the spectral graph wavelet transform. We suggest introducing color dimension into the computation of the weights of the graph and using the geodesic distance as a means of distance measurement. We thus have defined a wavelet transform based on a graph with perceptual information by using the CIELab color distance. This new representation is illustrated with denoising and inpainting applications. Overall, by introducing psychovisual information in the graph computation for the graph wavelet transform we obtain very promising results. Therefore results in image restoration highlight the interest of the appropriate use of color information.
xView3-SAR: Detecting Dark Fishing Activity Using Synthetic Aperture Radar Imagery<|sep|>Unsustainable fishing practices worldwide pose a major threat to marine resources and ecosystems. Identifying vessels that do not show up in conventional monitoring systems -- known as ``dark vessels'' -- is key to managing and securing the health of marine environments. With the rise of satellite-based synthetic aperture radar (SAR) imaging and modern machine learning (ML), it is now possible to automate detection of dark vessels day or night, under all-weather conditions. SAR images, however, require a domain-specific treatment and are not widely accessible to the ML community. Maritime objects (vessels and offshore infrastructure) are relatively small and sparse, challenging traditional computer vision approaches. We present the largest labeled dataset for training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The images are annotated using a combination of automated and manual analysis. Co-located bathymetry and wind state rasters accompany every SAR image. We also provide an overview of the xView3 Computer Vision Challenge, an international competition using xView3-SAR for ship detection and characterization at large scale. We release the data (\href{https://iuu.xview.us/}{https://iuu.xview.us/}) and code (\href{https://github.com/DIUx-xView}{https://github.com/DIUx-xView}) to support ongoing development and evaluation of ML approaches for this important application.
Context-Aware Small Cell Networks: How Social Metrics Improve Wireless Resource Allocation<|sep|>In this paper, a novel approach for optimizing and managing resource allocation in wireless small cell networks (SCNs) with device-to-device (D2D) communication is proposed. The proposed approach allows to jointly exploit both the wireless and social context of wireless users for optimizing the overall allocation of resources and improving traffic offload in SCNs. This context-aware resource allocation problem is formulated as a matching game in which user equipments (UEs) and resource blocks (RBs) rank one another, based on utility functions that capture both wireless and social metrics. Due to social interrelations, this game is shown to belong to a class of matching games with peer effects. To solve this game, a novel, selforganizing algorithm is proposed, using which UEs and RBs can interact to decide on their desired allocation. The proposed algorithm is then proven to converge to a two-sided stable matching between UEs and RBs. The properties of the resulting stable outcome are then studied and assessed. Simulation results using real social data show that clustering of socially connected users allows to offload a substantially larger amount of traffic than the conventional context-unaware approach. These results show that exploiting social context has high practical relevance in saving resources on the wireless links and on the backhaul.
An experimental study of charge distribution in crystalline and amorphous Si nanoclusters in thin silica films<|sep|>Crystalline and amorphous nanoparticles of silicon in thin silica layers were examined by transmission electron microscopy (TEM), electron energy loss spectroscopy (EELS) and x-ray photoelectron spectroscopy (XPS). We used XPS data in the form of the Auger parameter to separate initial and final state contributions to the Si$_{2p}$ energy shift. The electrostatic charging and electron screening issues as well as initial state effects were also addressed. We show that the chemical shift in the nanocrystals is determined by initial state rather than final state effects, and that the electron screening of silicon core holes in nanocrystals dispersed in SiO$_2$ is inferior to that in pure bulk Si.
Interacting Ghost Dark Energy Models in the Higher Dimensional Cosmology<|sep|>We investigate interacting ghost dark energy models in higher dimensional cosmology. We attempt to model dark matter within a barotropic fluid with $P_{b}=\omega(t)_{b}\rho$. In this work we consider four different models based on choosing equation of state parameter and interaction term. We confirm that our models agree with observational data.
Transient spirals as superposed instabilities<|sep|>We present evidence that recurrent spiral activity, long manifested in simulations of disk galaxies, results from the super-position of a few transient spiral modes. Each mode lasts between 5 and 10 rotations at its corotation radius where its amplitude is greatest. The scattering of stars as each wave decays takes place over narrow ranges of angular momentum, causing abrupt changes to the impedance of the disk to subsequent traveling waves. Partial reflections of waves at these newly created features, allows new standing-wave instabilities to appear that saturate and decay in their turn, scattering particles at new locations, creating a recurring cycle. The spiral activity causes the general level of random motion to rise, gradually decreasing the ability of the disk to support further activity unless the disk contains a dissipative gas component from which stars form on near-circular orbits. We also show that this interpretation is consistent with the behavior reported in other recent simulations with low mass-disks.
Hawking radiation from an evaporating black hole via Bogoliubov transformations<|sep|>We study Hawking radiation on a Vaidya space-time with a gravitational collapse followed by evaporation. The collapsing body is a null thin-shell and the evaporation is induced by a negative energy collapsing null-shell. This mimics the back-reaction to the Hawking radiation. Using Hawking's original method of Bogoliubov transformations we characterize the radiated spectrum as dominated by a thermal emission with an increasing effective temperature. We compute this time dependent temperature and find numerical agreement with results obtained by other techniques. The known divergences at the evaporation time are explained by the divergent nature of the effective temperature. As a consistency check, we re-derived the results from a zero mass limit of a remnant BH scenario.
General Nth order integrals of the motion<|sep|>The general form of an integral of motion that is a polynomial of order N in the momenta is presented for a Hamiltonian system in two-dimensional Euclidean space. The classical and the quantum cases are treated separately, emphasizing both the similarities and the differences between the two. The main application will be to study Nth order superintegrable systems that allow separation of variables in the Hamilton-Jacobi and Schr\"odinger equations, respectively.
M5-branes Probing Flux Backgrounds<|sep|>We analyze the global symmetries and anomalies of 4d $\mathcal{N} = 1$ field theories that arise from a stack of $N$ M5-branes probing a class of flux backgrounds. These backgrounds consist of a resolved $\mathbb{C}^2 / \mathbb{Z}_k$ singularity fibered over a smooth Riemann surface of genus $g \geq 2$, supported by a non-trivial $G_4$-flux configuration labeled by a collection of $2(k-1)$ flux quanta, $\{N_i\}$. For $k=2$, this setup defines a non-trivial superconformal field theory (SCFT) in the IR, which is holographically dual to an explicit $AdS_5$ solution first described by Gauntlett, Martelli, Sparks, and Waldram. The generalization to $k \geq 3$ is hard to tackle directly within holography. Instead, in this paper we lay the groundwork for a systematic analysis of such a generalization by adopting anomaly inflow methods to identify continuous and discrete global symmetries of the 4d field theories. We also compute the 't Hooft anomalies for continuous symmetries at leading order in the limit of large $N$, $N_i$.
Spectrum Bandit Optimization<|sep|>We consider the problem of allocating radio channels to links in a wireless network. Links interact through interference, modelled as a conflict graph (i.e., two interfering links cannot be simultaneously active on the same channel). We aim at identifying the channel allocation maximizing the total network throughput over a finite time horizon. Should we know the average radio conditions on each channel and on each link, an optimal allocation would be obtained by solving an Integer Linear Program (ILP). When radio conditions are unknown a priori, we look for a sequential channel allocation policy that converges to the optimal allocation while minimizing on the way the throughput loss or {\it regret} due to the need for exploring sub-optimal allocations. We formulate this problem as a generic linear bandit problem, and analyze it first in a stochastic setting where radio conditions are driven by a stationary stochastic process, and then in an adversarial setting where radio conditions can evolve arbitrarily. We provide new algorithms in both settings and derive upper bounds on their regrets.
Learning to Match via Inverse Optimal Transport<|sep|>We propose a unified data-driven framework based on inverse optimal transport that can learn adaptive, nonlinear interaction cost function from noisy and incomplete empirical matching matrix and predict new matching in various matching contexts. We emphasize that the discrete optimal transport plays the role of a variational principle which gives rise to an optimization-based framework for modeling the observed empirical matching data. Our formulation leads to a non-convex optimization problem which can be solved efficiently by an alternating optimization method. A key novel aspect of our formulation is the incorporation of marginal relaxation via regularized Wasserstein distance, significantly improving the robustness of the method in the face of noisy or missing empirical matching data. Our model falls into the category of prescriptive models, which not only predict potential future matching, but is also able to explain what leads to empirical matching and quantifies the impact of changes in matching factors. The proposed approach has wide applicability including predicting matching in online dating, labor market, college application and crowdsourcing. We back up our claims with numerical experiments on both synthetic data and real world data sets.
An ABC interpretation of the multiple auxiliary variable method<|sep|>We show that the auxiliary variable method (M{\o}ller et al., 2006; Murray et al., 2006) for inference of Markov random fields can be viewed as an approximate Bayesian computation method for likelihood estimation.
Discrete- versus continuous-state descriptions of the F1-ATPase molecular motor<|sep|>A discrete-state model of the F1-ATPase molecular motor is developed which describes not only the dependences of the rotation and ATP consumption rates on the chemical concentrations of ATP, ADP, and inorganic phosphate, but also on mechanical control parameters such as the friction coefficient and the external torque. The dependence on these mechanical parameters is given to the discrete-state model by fitting its transition rates to the continuous-angle model of P. Gaspard and E. Gerritsma [J. Theor. Biol. 247 (2007) 672-686]. This discrete-state model describes the behavior of the F1 motor in the regime of tight coupling between mechanical motion and chemical reaction. In this way, kinetic and thermodynamic properties of the F1 motor are obtained such as the Michaelis-Menten dependence of the rotation and ATP consumption rates on ATP concentration and its extension in the presence of ADP and Pi, their dependences on friction and external torque, as well as the chemical and mechanical thermodynamic efficiencies.
Non-reciprocal Components Based on Switched Transmission Lines<|sep|>Non-reciprocal components, such as isolators and circulators, are critical to wireless communication and radar applications. Traditionally, non-reciprocal components have been implemented using ferrite materials, which exhibit non-reciprocity under the influence of an external magnetic field. However, ferrite materials cannot be integrated into IC fabrication processes, and consequently are bulky and expensive. In the recent past, there has been strong interest in achieving non-reciprocity in a non-magnetic IC-compatible fashion using spatio-temporal modulation. In this paper, we present a general approach to non-reciprocity based on switched transmission lines. Switched transmission lines enable broadband, lossless and compact non-reciprocity, and a wide range of non-reciprocal functionalities, including non-reciprocal phase shifters, ultra-broadband gyrators and isolators, frequency-conversion isolators, and high-linearity/high-frequency/ultra-broadband circulators. We present a detailed theoretical analysis of the various non-idealities that impact insertion loss and provide design guidelines. The theory is validated by experimental results from discrete-component-based gyrators and isolators, and a 25GHz circulator fabricated in 45nm SOI CMOS technology.
Backward-Propagating MeV Electrons in Ultra-Intense Laser Interactions: Standing Wave Acceleration and Coupling to the Reflected Laser Pulse<|sep|>Laser-accelerated electron beams have been created at a kHz repetition rate from the {\it reflection} of intense ($\sim10^{18}$ W/cm$^2$), $\sim$40 fs laser pulses focused on a continuous water-jet in an experiment at the Air Force Research Laboratory. This paper investigates Particle-in-Cell (PIC) simulations of the laser-target interaction to identify the physical mechanisms of electron acceleration in this experiment. We find that the standing-wave pattern created by the overlap of the incident and reflected laser is particularly important because this standing wave can "inject" electrons into the reflected laser pulse where the electrons are further accelerated. We identify two regimes of standing wave acceleration: a highly relativistic case ($a_0~\geq~1$), and a moderately relativistic case ($a_0~\sim~0.5$) which operates over a larger fraction of the laser period. In previous studies, other groups have investigated the highly relativistic case for its usefulness in launching electrons in the forward direction. We extend this by investigating electron acceleration in the {\it specular (back reflection) direction} and over a wide range of intensities ($10^{17}-10^{19}$ W cm$^{-2}$).
Hunting for open clusters in \textit{Gaia} DR2: $582$ new OCs in the Galactic disc<|sep|>Open clusters are key targets for both Galaxy structure and evolution and stellar physics studies. Since \textit{Gaia} DR2 publication, the discovery of undetected clusters has proven that our samples were not complete. Our aim is to exploit the Big Data capabilities of machine learning to detect new open clusters in \textit{Gaia} DR2, and to complete the open cluster sample to enable further studies on the Galactic disc. We use a machine learning based methodology to systematically search in the Galactic disc, looking for overdensities in the astrometric space and identifying them as open clusters using photometric information. First, we use an unsupervised clustering algorithm, DBSCAN, to blindly search for these overdensities in \textit{Gaia} DR2 $(l,b,\varpi,\mu_{\alpha^*},\mu_\delta)$. After that, we use a deep learning artificial neural network trained on colour-magnitude diagrams to identify isochrone patterns in these overdensities, and to confirm them as open clusters. We find $582$ new open clusters distributed along the Galactic disc, in the region $|b| < 20$. We can detect substructure in complex regions, and identify the tidal tails of a disrupting cluster UBC~$274$ of $\sim 3$ Gyr located at $\sim 2$ kpc. Adapting the methodology into a Big Data environment allows us to target the search driven by physical properties of the open clusters, instead of being driven by its computational requirements. This blind search for open clusters in the Galactic disc increases in a $45\%$ the number of known open clusters.
Exact algorithms for OWA-optimization in multiobjective spanning tree problems<|sep|>This paper deals with the multiobjective version of the optimal spanning tree problem. More precisely, we are interested in determining the optimal spanning tree according to an Ordered Weighted Average (OWA) of its objective values. We first show that the problem is weakly NP-hard. In the case where the weights of the OWA are strictly decreasing, we then propose a mixed integer programming formulation, and provide dedicated optimality conditions yielding an important reduction of the size of the program. Next, we present two bounds that can be used to prune subspaces of solutions either in a shaving phase or in a branch and bound procedure. The validity of these bounds does not depend on specific properties of the weights (apart from non-negativity). All these exact resolution algorithms are compared on the basis of numerical experiments, according to their respective validity scopes.
The interplay between boundary conditions and flow geometries in shear banding: hysteresis, band configurations, and surface transitions<|sep|>We study shear banding flows in models of wormlike micelles or polymer solutions, and explore the effects of different boundary conditions for the viscoelastic stress. These are needed because the equations of motion are inherently non-local and include ``diffusive'' or square-gradient terms. Using the diffusive Johnson-Segalman model and a variant of the Rolie-Poly model for entangled micelles or polymer solutions, we study the interplay between different boundary conditions and the intrinsic stress gradient imposed by the flow geometry. We consider prescribed gradient (Neumann) or value (Dirichlet) of the viscoelastic stress tensor at the boundary, as well as mixed boundary conditions in which an anchoring strength competes with the gradient contribution to the stress dynamics. We find that hysteresis during shear rate sweeps is suppressed if the boundary conditions favor the state that is induced by the sweep. For example, if the boundaries favor the high shear rate phase then hysteresis is suppressed at the low shear rate edges of the stress plateau. If the boundaries favor the low shear rate state, then the high shear rate band can lie in the center of the flow cell, leading to a three-band configuration. Sufficiently strong stress gradients due to curved flow geometries, such as that of cylindrical Couette flow, can convert this to a two-band state by forcing the high shear rate phase against the wall of higher stress, and can suppress the hysteresis loop observed during a shear rate sweep.
Querying Spreadsheets: An Empirical Study<|sep|>One of the most important assets of any company is being able to easily access information on itself and on its business. In this line, it has been observed that this important information is often stored in one of the millions of spreadsheets created every year, due to simplicity in using and manipulating such an artifact. Unfortunately, in many cases it is quite difficult to retrieve the intended information from a spreadsheet: information is often stored in a huge unstructured matrix, with no care for readability or comprehensiveness. In an attempt to aid users in the task of extracting information from a spreadsheet, researchers have been working on models, languages and tools to query. In this paper we present an empirical study evaluating such proposals assessing their usage to query spreadsheets. We investigate the use of the Google Query Function, textual model-driven querying, and visual model-driven querying. To compare these different querying approaches we present an empirical study whose results show that the end-users' productivity increases when using model-driven queries, specially using its visual representation.
Fast resolution of a single factor Heath-Jarrow-Morton model with stochastic volatility<|sep|>This paper considers the single factor Heath-Jarrow-Morton model for the interest rate curve with stochastic volatility. Its natural formulation, described in terms of stochastic differential equations, is solved through Monte Carlo simulations, that usually involve rather large computation time, inefficient from a practical (financial) perspective. This model turns to be Markovian in three dimensions and therefore it can be mapped into a 3D partial differential equations problem. We propose an optimized numerical method to solve the 3D PDE model in both low computation time and reasonable accuracy, a fundamental criterion for practical purposes. The spatial and temporal discretization are performed using finite-difference and Crank-Nicholson schemes respectively, and the computational efficiency is largely increased performing a scale analysis and using Alternating Direction Implicit schemes. Several numerical considerations such as convergence criteria or computation time are analyzed and discussed.
Properties of the linearised functional renormalization group<|sep|>Interactions growing slower than a certain exponential of the square of a scalar field, are well behaved when evolved under the functional renormalization group linearised around the Gaussian fixed point. They satisfy properties usually taken for granted, and reproduce standard perturbative quantisation. However, ever more challenging effects appear the more interactions grow faster than this. We show explicitly that firstly the flow no longer splits uniquely into operators of definite scaling dimension; then (linearised) flows to the infrared can end prematurely in a singularity; and finally new interactions can spontaneously appear at any scale.
Searching for $Z'$ bosons decaying to gluons<|sep|>The production and decay of a new heavy vector boson, a chromophilic $Z'$ vector boson, is described. The chromophilic $Z'$ couples only to two gluons, but its two-body decays are absent, leading to a dominant decay mode of $Z'\rightarrow q\bar{q}g$. The unusual nature of the interaction predicts a cross-section which grows with $m_{Z'}$ for a fixed coupling and an accompanying gluon with a coupling that rises with its energy. We study the $t\bar{t}g$ decay mode, proposing distinct reconstruction techniques for the observation of an excess and for the measurement of $m_{Z'}$. We estimate the sensitivity of current experimental datasets.
PQFabric: A Permissioned Blockchain Secure from Both Classical and Quantum Attacks<|sep|>Hyperledger Fabric is a prominent and flexible solution for building permissioned distributed ledger platforms. Access control and identity management relies on a Membership Service Provider (MSP) whose cryptographic interface only handles standard PKI methods for authentication: RSA and ECDSA classical signatures. Also, MSP-issued credentials may use only one signature scheme, tying the credential-related functions to classical single-signature primitives. RSA and ECDSA are vulnerable to quantum attacks, with an ongoing post-quantum standardization process to identify quantum-safe drop-in replacements. In this paper, we propose a redesign of Fabric's credential-management procedures and related specifications in order to incorporate hybrid digital signatures, protecting against both classical and quantum attacks using one classical and one quantum-safe signature. We create PQFabric, an implementation of Fabric with hybrid signatures that integrates with the Open Quantum Safe (OQS) library. Our implementation offers complete crypto-agility, with the ability to perform live migration to a hybrid quantum-safe blockchain and select any existing OQS signature algorithm for each node. We perform comparative benchmarks of PQFabric with each of the NIST candidates and alternates, revealing that long public keys and signatures lead to an increase in hashing time that is sometimes comparable to the time spent signing or verifying messages itself. This is a new and potentially significant issue in the migration of blockchains to post-quantum signatures.
Fast-forward of quantum adiabatic dynamics in electro-magnetic field<|sep|>We show a method to accelerate quantum adiabatic dynamics of wavefunctions under electro-magnetic field by developing the previous theory (Masuda & Nakamura 2008 and 2010). Firstly we investigate the orbital dynamics of a charged particle. We derive the driving field which accelerates quantum adiabatic dynamics in order to obtain the final adiabatic states except for the spatially uniform phase such as the adiabatic phase in any desired short time. Fast-forward of adiabatic squeezing and transport in the electro-magnetic field is exhibited. Secondly we investigate spin dynamics under the magnetic field, showing the fast-forward of adiabatic spin inversion and of adiabatic dynamics in Landau-Zener model. The connection of the present framework with Kato-Berry's transitionless quantum driving is elucidated in Appendix.
Factorizing the factorization - a spectral-element solver for elliptic equations with linear operation count<|sep|>High-order methods gain more and more attention in computational fluid dynamics. However, the potential advantage of these methods depends critically on the availability of efficient elliptic solvers. With spectral-element methods, static condensation is a common approach to reduce the number of degree of freedoms and to improve the condition of the algebraic equations. The resulting system is block-structured and the face-based operator well suited for matrix-matrix multiplications. However, a straight-forward implementation scales super-linearly with the number of unknowns and, therefore, prohibits the application to high polynomial degrees. This paper proposes a novel factorization technique, which yields a linear operation count of just 13N multiplications, where N is the total number of unknowns. In comparison to previous work it saves a factor larger than 3 and clearly outpaces unfactored variants for all polynomial degrees. Using the new technique as a building block for a preconditioned conjugate gradient method resulted in a runtime scaling linearly with N for polynomial degrees $2 \leq p \leq 32$ . Moreover the solver proved remarkably robust for aspect ratios up to 128.
Cross Layer QoS Support Architecture with Integrated CAC and Scheduling Algorithms for WiMAX BWA Networks<|sep|>In this paper, a new technique for cross layer design, based on present Eb/N0 (bit energy per noise density) ratio of the connections and target values of the Quality of Service (QoS) information parameters from MAC layer, is proposed to dynamically select the Modulation and Coding Scheme (MCS) at the PHY layer for WiMAX Broadband Wireless Access (BWA) networks. The QoS information parameter includes New Connection Blocking Probability (NCBP), Hand off Connection Dropping Probability (HCDP) and Connection Outage Probability (COP). In addition, a Signal to Interference plus Noise Ratio (SINR) based Call Admission Control (CAC) algorithm and Queue based Scheduling algorithm are integrated for the cross layer design. An analytical model using the Continuous Time Markov Chain (CTMC) is developed for performance evaluation of the algorithms under various MCS. The effect of Eb/No is observed for QoS information parameters in order to determine its optimum range. Simulation results show that the integrated CAC and packet Scheduling model maximizes the bandwidth utilization and fair allocation of the system resources for all types of MCS and guarantees the QoS to the connections.
Third quantization: a general method to solve master equations for quadratic open Fermi systems<|sep|>The Lindblad master equation for an arbitrary quadratic system of n fermions is solved explicitly in terms of diagonalization of a 4n x 4n matrix, provided that all Lindblad bath operators are linear in the fermionic variables. The method is applied to the explicit construction of non-equilibrium steady states and the calculation of asymptotic relaxation rates in the far from equilibrium problem of heat and spin transport in a nearest neighbor Heisenberg XY spin 1/2 chain in a transverse magnetic field.
Cubic interaction vertices for massive/massless continuous-spin fields and arbitrary spin fields<|sep|>We use light-cone gauge formalism to study interacting massive and massless continuous-spin fields and finite component arbitrary spin fields propagating in the flat space. Cubic interaction vertices for such fields are considered. We obtain parity invariant cubic vertices for coupling of one continuous-spin field to two arbitrary spin fields and cubic vertices for coupling of two continuous-spin fields to one arbitrary spin field. Parity invariant cubic vertices for self-interacting massive/massless continuous-spin fields are also obtained. We find the complete list of parity invariant cubic vertices for continuous-spin fields and arbitrary spin fields.
Jet quenching within a hybrid strong/weak coupling approach<|sep|>We propose a novel hybrid model for jet quenching, including both strong and weak coupling physics where each seems appropriate. Branching in the parton shower is assumed to be perturbative and described by DGLAP evolution, while interactions with the medium result in each parton in the shower losing energy as at strong coupling, as realized holographically. The medium-modified parton shower is embedded into a hydrodynamic evolution of hot QCD plasma and confronted with LHC jet data.
Hawking Radiation of Black p-Branes from Gravitational Anomaly<|sep|>We investigate the Hawking radiation of black $p$-branes of superstring theories using the method of anomaly cancelation, specially, we use the method of [S. Iso, H. Umetsu and F. Wilczek, {\sl Phys. Rev. Lett.} {\bf 96}, 151302 (2006); {\sl Phys. Rev. D} {\bf 74}, 044017 (2006)]. The metrics of black $p$-branes are spherically symmetric, but not the Schwarzschild type. In order to simplify the calculation, we first make a coordinate transformation to transform the metric to the Schwarzschild type. Then we calculate its energy-momentum flux from the method of anomaly cancelation of the above mentioned references. The obtained energy-momentum flux is equal to a black body radiation, the thermodynamic temperature of the radiation is equal to its Hawking temperature. And we find that the results are not changed for the original non-Schwarzschild type spherically symmetric metric.
The Co-Evolution of a Magnetized Intracluster Medium and Hot Galactic Coronae: Magnetic Field Amplification and Turbulence Generation<|sep|>We use adaptive-mesh magnetohydrodynamic simulations to study the effect of magnetic fields on ram pressure stripping of galaxies in the intracluster medium (ICM). Although the magnetic pressure in typical clusters is not strong enough to affect the gas mass loss rate from galaxies, magnetic fields can affect the morphology of stripped galaxies. ICM magnetic fields are draped around orbiting galaxies and aligned with their stripped tails. Magnetic fields suppress shear instabilities at the galaxy-ICM interface, and magnetized tails are smoother and narrower than tails in comparable hydrodynamic simulations in Vijayaraghavan & Ricker (2015). Orbiting galaxies stretch and amplify ICM magnetic fields, amplifying magnetic power spectra on $10 - 100$ kpc scales. Galaxies inject turbulent kinetic energy into the ICM via their turbulent wakes and $g$-waves. The magnetic energy and kinetic energy in the ICM increase up to $1.5 - 2$ Gyr of evolution, after which galaxies are stripped of most of their gas, and do not have sufficiently large gaseous cross sections to further amplify magnetic fields and inject turbulent kinetic energy. The increase in turbulent pressure due to galaxy stripping and generation of $g$-waves results in an increase in the turbulent volume fraction of the ICM. This turbulent kinetic energy is not a significant contributor to the overall ICM energy budget, but greatly impacts the evolution of the ICM magnetic field. Additionally, the effect of galaxies on magnetic fields can potentially be observed in high resolution Faraday rotation measure (RM) maps as small scale fluctuations in the RM structure.
Optimal grid exploration by asynchronous oblivious robots<|sep|>We consider a team of {\em autonomous weak robots} that are endowed with visibility sensors and motion actuators. Autonomous means that the team cannot rely on any kind of central coordination mechanism or scheduler. By weak we mean that the robots are devoid of (1) any (observable) IDs allowing to differentiate them (anonymous), (2) means of communication allowing them to communicate directly, and (3) any way to remember any previous observation nor computation performed in any previous step (oblivious). Robots asynchronously operate in cycles of three phases: Look, Compute, and Move. Furthermore, the network is an anonymous unoriented grid. In such settings, the robots must collaborate to solve a collective task, here the terminating grid exploration (exploration for short), despite being limited with respect to input from the environment, asymmetry, memory, etc. Exploration requires that robots explore the grid and stop when the task is complete. We propose optimal (w.r.t. the number of robots) solutions for the deterministic terminating exploration of a grid shaped network by a team of $k$ asynchronous oblivious robots in the fully asynchronous and non-atomic model, so called CORDA. In more details, we first assume the ATOM model in which each Look-Compute-Move cycle execution is executed atomically, ie every robot that is activated at instant t instantaneously executes a full cycle between t and t+1. ATOM being strictly stronger than CORDA, all impossibility results in ATOM also hold in CORDA. We show that it is impossible to explore a grid of at least three nodes with less than three robots in ATOM. (This first result holds for both deterministic and probabilistic settings.) Next, we show that it is impossible to deterministically explore a (2,2)-Grid with less than 4 robots, and a (3,3)-Grid with less than 5 robots, respectively. Then, we propose deterministic algorithms in CORDA to exhibit the optimal number of robots allowing to explore of a given grid. Our results show that except in two particular cases, 3 robots are necessary and sufficient to deterministically explore a grid of at least three nodes. The optimal number of robots for the two remaining cases is: 4 for the (2,2)-Grid and 5 for the (3,3)-Grid.
A Comparison of Stereo-Matching Cost between Convolutional Neural Network and Census for Satellite Images<|sep|>Stereo dense image matching can be categorized to low-level feature based matching and deep feature based matching according to their matching cost metrics. Census has been proofed to be one of the most efficient low-level feature based matching methods, while fast Convolutional Neural Network (fst-CNN), as a deep feature based method, has small computing time and is robust for satellite images. Thus, a comparison between fst-CNN and census is critical for further studies in stereo dense image matching. This paper used cost function of fst-CNN and census to do stereo matching, then utilized semi-global matching method to obtain optimized disparity images. Those images are used to produce digital surface model to compare with ground truth points. It addresses that fstCNN performs better than census in the aspect of absolute matching accuracy, histogram of error distribution and matching completeness, but these two algorithms still performs in the same order of magnitude.
Single-Shot 3D Detection of Vehicles from Monocular RGB Images via Geometry Constrained Keypoints in Real-Time<|sep|>In this paper we propose a novel 3D single-shot object detection method for detecting vehicles in monocular RGB images. Our approach lifts 2D detections to 3D space by predicting additional regression and classification parameters and hence keeping the runtime close to pure 2D object detection. The additional parameters are transformed to 3D bounding box keypoints within the network under geometric constraints. Our proposed method features a full 3D description including all three angles of rotation without supervision by any labeled ground truth data for the object's orientation, as it focuses on certain keypoints within the image plane. While our approach can be combined with any modern object detection framework with only little computational overhead, we exemplify the extension of SSD for the prediction of 3D bounding boxes. We test our approach on different datasets for autonomous driving and evaluate it using the challenging KITTI 3D Object Detection as well as the novel nuScenes Object Detection benchmarks. While we achieve competitive results on both benchmarks we outperform current state-of-the-art methods in terms of speed with more than 20 FPS for all tested datasets and image resolutions.
A Note on Computing Extreme Tail Probabilities of the Noncentral T Distribution with Large Noncentrality Parameter<|sep|>The noncentral $t$-distribution is a generalization of the Student's $t$-distribution. In this paper we suggest an alternative approach for computing the cumulative distribution function (CDF) of the noncentral $t$-distribution which is based on a direct numerical integration of a well behaved function. With a double-precision arithmetic, the algorithm provides highly precise and fast evaluation of the extreme tail probabilities of the noncentral $t$-distribution, even for large values of the noncentrality parameter $\delta$ and the degrees of freedom $\nu$. The implementation of the algorithm is available at the MATLAB Central, File Exchange: http://www.mathworks.com/matlabcentral/fileexchange/41790-nctcdfvw.
Push--Pull with Device Sampling<|sep|>We consider decentralized optimization problems in which a number of agents collaborate to minimize the average of their local functions by exchanging over an underlying communication graph. Specifically, we place ourselves in an asynchronous model where only a random portion of nodes perform computation at each iteration, while the information exchange can be conducted between all the nodes and in an asymmetric fashion. For this setting, we propose an algorithm that combines gradient tracking and variance reduction over the entire network. This enables each node to track the average of the gradients of the objective functions. Our theoretical analysis shows that the algorithm converges linearly, when the local objective functions are strongly convex, under mild connectivity conditions on the expected mixing matrices. In particular, our result does not require the mixing matrices to be doubly stochastic. In the experiments, we investigate a broadcast mechanism that transmits information from computing nodes to their neighbors, and confirm the linear convergence of our method on both synthetic and real-world datasets.
AI safety via debate<|sep|>To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.
Superparamagnetic Relaxation Driven by Colored Noise<|sep|>A theoretical investigation of magnetic relaxation processes in single domain particles driven by colored noise is presented. Two approaches are considered; the Landau-Lifshitz-Miyazaki-Seki equation, which is a Langevin dynamics model based on the introduction of an Ornstein-Uhlenbeck correlated noise into the Landau-Lifshitz-Gilbert equation and a Generalized Master Equation approach whereby the ordinary Master Equation is modified through the introduction of an explicit memory kernel. It is found that colored noise is likely to become important for high anisotropy materials where the characteristic system time, in this case the inverse Larmor precession frequency, becomes comparable to the correlation time. When the escape time is much longer than the correlation time, the relaxation profile of the spin has a similar exponential form to the ordinary LLG equation, while for low barrier heights and intermediate damping, for which the correlation time is a sizable fraction of the escape time, an unusual bi-exponential decay is predicted as a characteristic of colored noise. At very high damping and correlation times, the time profile of the spins exhibits a more complicated, noisy trajectory.
Simultaneous Joint and Object Trajectory Templates for Human Activity Recognition from 3-D Data<|sep|>The availability of low-cost range sensors and the development of relatively robust algorithms for the extraction of skeleton joint locations have inspired many researchers to develop human activity recognition methods using the 3-D data. In this paper, an effective method for the recognition of human activities from the normalized joint trajectories is proposed. We represent the actions as multidimensional signals and introduce a novel method for generating action templates by averaging the samples in a "dynamic time" sense. Then in order to deal with the variations in the speed and style of performing actions, we warp the samples to the action templates by an efficient algorithm and employ wavelet filters to extract meaningful spatiotemporal features. The proposed method is also capable of modeling the human-object interactions, by performing the template generation and temporal warping procedure via the joint and object trajectories simultaneously. The experimental evaluation on several challenging datasets demonstrates the effectiveness of our method compared to the state-of-the-arts.
Learning a Discriminative Prior for Blind Image Deblurring<|sep|>We present an effective blind image deblurring method based on a data-driven discriminative prior.Our work is motivated by the fact that a good image prior should favor clear images over blurred images.In this work, we formulate the image prior as a binary classifier which can be achieved by a deep convolutional neural network (CNN).The learned prior is able to distinguish whether an input image is clear or not.Embedded into the maximum a posterior (MAP) framework, it helps blind deblurring in various scenarios, including natural, face, text, and low-illumination images.However, it is difficult to optimize the deblurring method with the learned image prior as it involves a non-linear CNN.Therefore, we develop an efficient numerical approach based on the half-quadratic splitting method and gradient decent algorithm to solve the proposed model.Furthermore, the proposed model can be easily extended to non-uniform deblurring.Both qualitative and quantitative experimental results show that our method performs favorably against state-of-the-art algorithms as well as domain-specific image deblurring approaches.
A Methodology to Generate Crystal-based Molecular Structures for Atomistic Simulations<|sep|>We propose a systematic method to construct crystal-based molecular structures often needed as input for computational chemistry studies. These structures include crystal ``slabs" with periodic boundary conditions (PBCs) and non-periodic solids such as Wulff structures. We also introduce a method to build crystal slabs with orthogonal PBC vectors. These methods are integrated into our code, Los Alamos Crystal Cut (LCC), which is open source and thus fully available to the community. Examples showing the use of these methods are given throughout the manuscript.
Learning to Guide a Saturation-Based Theorem Prover<|sep|>Traditional automated theorem provers have relied on manually tuned heuristics to guide how they perform proof search. Recently, however, there has been a surge of interest in the design of learning mechanisms that can be integrated into theorem provers to improve their performance automatically. In this work, we introduce TRAIL, a deep learning-based approach to theorem proving that characterizes core elements of saturation-based theorem proving within a neural framework. TRAIL leverages (a) an effective graph neural network for representing logical formulas, (b) a novel neural representation of the state of a saturation-based theorem prover in terms of processed clauses and available actions, and (c) a novel representation of the inference selection process as an attention-based action policy. We show through a systematic analysis that these components allow TRAIL to significantly outperform previous reinforcement learning-based theorem provers on two standard benchmark datasets (up to 36% more theorems proved). In addition, to the best of our knowledge, TRAIL is the first reinforcement learning-based approach to exceed the performance of a state-of-the-art traditional theorem prover on a standard theorem proving benchmark (solving up to 17% more problems).
Joint Pose and Shape Estimation of Vehicles from LiDAR Data<|sep|>We address the problem of estimating the pose and shape of vehicles from LiDAR scans, a common problem faced by the autonomous vehicle community. Recent work has tended to address pose and shape estimation separately in isolation, despite the inherent connection between the two. We investigate a method of jointly estimating shape and pose where a single encoding is learned from which shape and pose may be decoded in an efficient yet effective manner. We additionally introduce a novel joint pose and shape loss, and show that this joint training method produces better results than independently-trained pose and shape estimators. We evaluate our method on both synthetic data and real-world data, and show superior performance against a state-of-the-art baseline.
Mass Dependence of the Entropy Product and Sum<|sep|>For black holes with multiple horizons, the area product of all horizons has been proven to be mass independent in many cases. Counterexamples were also found in some occasions. In this paper, we first prove a theorem derived from the first law of black hole thermodynamics and a mathematical lemma related to the Vandermonde determinant. With these arguments, we develop some general criterion for the mass independence of the entropy product as well as the entropy sum. In particular, if a $d$-dimensional spacetime is spherically symmetric and its radial metric function $f(r)$ is a Laurent series in $r$ with the lowest power $-m$ and the highest power $n$, we find the criteria is extremely simple: The entropy product is mass independent if and only if $m\geq d-2$ and $n\geq4-d$. The entropy sum is mass independent if and only if $m\geq d-2$ and $n\geq 2$. Compared to previous works, our method does not require an exact expression of the metric. Our arguments turn out to be useful even for rotating black holes. By applying our theorem and lemma to a Myers-Perry black hole with spacetime dimension $d$, we show that the entropy product/sum is mass independent for all $d>4$, while it is mass dependent only for $d=4$, i.e., the Kerr solution.
The strongest gravitational lenses: I. The statistical impact of cluster mergers<|sep|>For more than a decade now, it has been controversial whether or not the high rate of giant gravitational arcs and the largest observed Einstein radii are consistent with the standard cosmological model. Recent studies indicate that mergers provide an efficient mechanism to substantially increase the strong-lensing efficiency of individual clusters. Based on purely semi-analytic methods, we investigated the statistical impact of cluster mergers on the distribution of the largest Einstein radii and the optical depth for giant gravitational arcs of selected cluster samples. Analysing representative all-sky realizations of clusters at redshifts z < 1 and assuming a constant source redshift of z_s = 2.0, we find that mergers increase the number of Einstein radii above 10 arcsec (20 arcsec) by ~ 35 % (~ 55 %). Exploiting the tight correlation between Einstein radii and lensing cross sections, we infer that the optical depth for giant gravitational arcs with a length-to-width ratio > 7.5 of those clusters with Einstein radii above 10 arcsec (20 arcsec) increases by ~ 45 % (85 %). Our findings suggest that cluster mergers significantly influence in particular the statistical lensing properties of the strongest gravitational lenses. We conclude that semi-analytic studies must inevitably take these events into account before questioning the standard cosmological model on the basis of the largest observed Einstein radii and the statistics of giant gravitational arcs.
Linear complexity of generalized cyclotomic sequences of period $2p^{m}$<|sep|>In this paper, we construct two generalized cyclotomic binary sequences of period $2p^{m}$ based on the generalized cyclotomy and compute their linear complexity, showing that they are of high linear complexity when $m\geq 2$.
Building a comprehensive syntactic and semantic corpus of Chinese clinical texts<|sep|>Objective: To build a comprehensive corpus covering syntactic and semantic annotations of Chinese clinical texts with corresponding annotation guidelines and methods as well as to develop tools trained on the annotated corpus, which supplies baselines for research on Chinese texts in the clinical domain. Materials and methods: An iterative annotation method was proposed to train annotators and to develop annotation guidelines. Then, by using annotation quality assurance measures, a comprehensive corpus was built, containing annotations of part-of-speech (POS) tags, syntactic tags, entities, assertions, and relations. Inter-annotator agreement (IAA) was calculated to evaluate the annotation quality and a Chinese clinical text processing and information extraction system (CCTPIES) was developed based on our annotated corpus. Results: The syntactic corpus consists of 138 Chinese clinical documents with 47,424 tokens and 2553 full parsing trees, while the semantic corpus includes 992 documents that annotated 39,511 entities with their assertions and 7695 relations. IAA evaluation shows that this comprehensive corpus is of good quality, and the system modules are effective. Discussion: The annotated corpus makes a considerable contribution to natural language processing (NLP) research into Chinese texts in the clinical domain. However, this corpus has a number of limitations. Some additional types of clinical text should be introduced to improve corpus coverage and active learning methods should be utilized to promote annotation efficiency. Conclusions: In this study, several annotation guidelines and an annotation method for Chinese clinical texts were proposed, and a comprehensive corpus with its NLP modules were constructed, providing a foundation for further study of applying NLP techniques to Chinese texts in the clinical domain.
Strain localization and failure of disordered particle rafts with tunable ductility during tensile deformation<|sep|>Quasi-static tensile experiments were performed for a model disordered solid consisting of a two-dimensional raft of polydisperse floating granular particles with capillary attractions. The ductility is tuned by controlling the capillary interaction range, which varies with the particle size. During the tensile tests, after an initial period of elastic deformation, strain localization occurs and leads to the formation of a shear band at which the pillar later fails. In this process, small particles with long-ranged interactions can endure large plastic deformations without forming significant voids, while large particles with short-range interactions fail dramatically by fracturing at small deformation. Particle-level structure was measured, and the strain-localized region was found to have higher structural anisotropy than the bulk. Local interactions between anisotropic sites and particle rearrangements were the main mechanisms driving strain localization and the subsequent failure, and significant differences of such interactions exist between ductile and brittle behaviors.
What Happens when Separate and Unequal School Districts Merge?<|sep|>We study the welfare effects of school district consolidation, i.e. the integration of disjoint school districts into a centralised clearinghouse. We show theoretically that, in the worst-case scenario, district consolidation may unambiguously reduce students' welfare, even if the student-optimal stable matching is consistently chosen. However, on average all students experience expected welfare gains from district consolidation, particularly those who belong to smaller and over-demanded districts. Using data from the Hungarian secondary school assignment mechanism, we compute the actual welfare gains from district consolidation in Budapest and compare these to our theoretical predictions. We empirically document substantial welfare gains from district consolidation for students, equivalent to attending a school five kilometres closer to the students' home addresses. As an important building block of our empirical strategy, we describe a method to consistently estimate students' preferences over schools and vice versa that does not fully assume that students report their preferences truthfully in the student-proposing deferred acceptance algorithm.
Spontaneous parametric downconversion in waveguides: What's loss got to do with it?<|sep|>We derive frequency correlation and exit probability expressions for photons generated via spontaneous parametric downconversion (SPDC) in nonlinear waveguides that exhibit linear scattering loss. Such loss is included within a general Hamiltonian formalism by connecting waveguide modes to reservoir modes with a phenomenological coupling Hamiltonian, the parameters of which are later related to the usual loss coefficients. In the limit of a low probability of SPDC pair production, the presence of loss requires that we write the usual lossless generated pair state as a reduced density operator, and we find that this density operator is naturally composed of two photon, one photon, and zero photon contributions. The biphoton probability density, or joint spectral intensity (JSI), associated with the two-photon contribution is determined not only by a phase matching term, but also by a loss matching term. The relative size of the loss coefficients within this term lead to three qualitatively different regimes of SPDC JSIs. If either the pump or generated photon loss is much higher than the other, the side lobes of the phase matching squared sinc function are washed out. On the other hand, if pump and generated photon loss are appropriately balanced, the lossy JSI is identical to the lossless JSI. Finally, if the generated photon loss is frequency dependent, the shape of the JSI can be altered more severely, potentially leading to generated photons that are less frequency correlated though also produced less efficiently when compared to photons generated in low-loss waveguides.
Phylotastic: An Experiment in Creating, Manipulating, and Evolving Phylogenetic Biology Workflows Using Logic Programming<|sep|>Evolutionary Biologists have long struggled with the challenge of developing analysis workflows in a flexible manner, thus facilitating the reuse of phylogenetic knowledge. An evolutionary biology workflow can be viewed as a plan which composes web services that can retrieve, manipulate, and produce phylogenetic trees. The Phylotastic project was launched two years ago as a collaboration between evolutionary biologists and computer scientists, with the goal of developing an open architecture to facilitate the creation of such analysis workflows. While composition of web services is a problem that has been extensively explored in the literature, including within the logic programming domain, the incarnation of the problem in Phylotastic provides a number of additional challenges. Along with the need to integrate preferences and formal ontologies in the description of the desired workflow, evolutionary biologists tend to construct workflows in an incremental manner, by successively refining the workflow, by indicating desired changes (e.g., exclusion of certain services, modifications of the desired output). This leads to the need of successive iterations of incremental replanning, to develop a new workflow that integrates the requested changes while minimizing the changes to the original workflow. This paper illustrates how Phylotastic has addressed the challenges of creating and refining phylogenetic analysis workflows using logic programming technology and how such solutions have been used within the general framework of the Phylotastic project. Under consideration in Theory and Practice of Logic Programming (TPLP).
Finite-Energy Sum Rules in Eta Photoproduction off the Nucleon<|sep|>The reaction ${\gamma}N \to {\eta}N$ is studied in the high-energy regime (with photon lab energies $E_{\gamma}^{\textrm{lab}} > 4$ GeV) using information from the resonance region through the use of finite-energy sum rules (FESR). We illustrate how analyticity allows one to map the t-dependence of the unknown Regge residue functions. We provide predictions for the energy dependence of the beam asymmetry at high energies.
Lightlike Branes as Natural Candidates for Wormhole Throats<|sep|>We first briefly present a consistent world-volume Lagrangian description of lightlike p-branes (LL-branes) in two equivalent forms - a Polyakov-type and a dual to it Nambu-Goto-type formulations. The most important characteristic features of LL-brane dynamics are: (i) the brane tension appears as a non-trivial additional dynamical degree of freedom; (ii) consistency of LL-brane dynamics in a spherically or axially symmetric gravitational background of codimension one requires the presence of an event horizon which is automatically occupied by the LL-brane ("horizon straddling"). Next we consider a bulk Einstein-Maxwell system interacting self-consistently with a codimension one LL-brane. We find spherically symmetric traversable wormhole solutions of Misner-Wheeler type produced by the LL-brane sitting at the wormhole throat with wormhole parameters being functions of the dynamical LL-brane tension.
Phase-referenced Interferometry and Narrow-angle Astrometry with SUSI<|sep|>The Sydney University Stellar Interferometer (SUSI) now incorporates a new beam combiner, called the Microarcsecond University of Sydney Companion Astrometry instrument (MUSCA), for the purpose of high precision differential astrometry of bright binary stars. Operating in the visible wavelength regime where photon-counting and post-processing fringe tracking is possible, MUSCA will be used in tandem with SUSI's primary beam combiner, Precision Astronomical Visible Observations (PAVO), to record high spatial resolution fringes and thereby measure the separation of fringe packets of binary stars. In its current phase of development, the dual beam combiner configuration has successfully demonstrated for the first time a dual-star phase-referencing operation in visible wavelengths. This paper describes the beam combiner optics and hardware, the network of metrology systems employed to measure every non-common path between the two beam combiners and also reports on a recent narrow-angle astrometric observation of $\delta$ Orionis A (HR 1852) as the project enters its on-sky testing phase.
Human Body Orientation Estimation using Convolutional Neural Network<|sep|>Personal robots are expected to interact with the user by recognizing the user's face. However, in most of the service robot applications, the user needs to move himself/herself to allow the robot to see him/her face to face. To overcome such limitations, a method for estimating human body orientation is required. Previous studies used various components such as feature extractors and classification models to classify the orientation which resulted in low performance. For a more robust and accurate approach, we propose the light weight convolutional neural networks, an end to end system, for estimating human body orientation. Our body orientation estimation model achieved 81.58% and 94% accuracy with the benchmark dataset and our own dataset respectively. The proposed method can be used in a wide range of service robot applications which depend on the ability to estimate human body orientation. To show its usefulness in service robot applications, we designed a simple robot application which allows the robot to move towards the user's frontal plane. With this, we demonstrated an improved face detection rate.
Classifying the Arithmetical Complexity of Teaching Models<|sep|>This paper classifies the complexity of various teaching models by their position in the arithmetical hierarchy. In particular, we determine the arithmetical complexity of the index sets of the following classes: (1) the class of uniformly r.e. families with finite teaching dimension, and (2) the class of uniformly r.e. families with finite positive recursive teaching dimension witnessed by a uniformly r.e. teaching sequence. We also derive the arithmetical complexity of several other decision problems in teaching, such as the problem of deciding, given an effective coding $\{\mathcal L_0,\mathcal L_1,\mathcal L_2,\ldots\}$ of all uniformly r.e. families, any $e$ such that $\mathcal L_e = \{L^e_0,L^e_1,\ldots,\}$, any $i$ and $d$, whether or not the teaching dimension of $L^e_i$ with respect to $\mathcal L_e$ is upper bounded by $d$.
On the contribution of nearby sources to the observed cosmic-ray nuclei<|sep|>The presence of nearby discrete cosmic-ray (CR) sources can lead to many interesting effects on the observed properties of CRs. In this paper, we study about the possible effects on the CR primary and secondary spectra and also the subsequent effects on the CR secondary-to-primary ratios. For the study, we assume that CRs undergo diffusive propagation in the Galaxy and we neglect the effect of convection, energy losses and reacceleration. In our model, we assume that there exists a uniform and continuous distribution of CR sources in the Galaxy generating a stationary CR background at the Earth. In addition, we also consider the existence of some nearby sources which inject CRs in a discrete space-time model. Assuming a constant CR source power throughout the Galaxy, our study has found that the presence of nearby supernova remnants (SNRs) produces noticeable variations in the primary fluxes mainly above ~ 100 GeV/n, if CRs are assumed to be released instantaneously after the supernova explosion. The variation reaches a value of ~ 45% at around 10^5 GeV/n. Respect to earlier studies, the variation in the case of the secondaries is found to be almost negligible. We also discuss about the possible effects of the different particle release times from the SNRs. For the particle release time of ~ 10^5 yr, predicted by the diffusive shock acceleration theories in SNRs, we have found that the presence of the nearby SNRs hardly produces any significant effects on the CRs at the Earth.
Towards understanding the ordering behavior of hard needles: New analytical solutions in one dimension<|sep|>We re-examine the ordering behavior of a one-dimensional fluid of freely rotating hard needles, where the centers of mass of the particles are restricted to a line. Analytical equations are obtained for the equation of state, order parameter and orientational correlation functions using the transfer-matrix method if some simplifying assumptions are applied for either the orientational freedom or the contact distance between two needles. The two-state Zwanzig model accounts for the orientational ordering, but it produces unphysical pressure at high densities and there is no orientational correlation. The four-state Zwanzig model gives reasonable results for orientational correlation function, but the pressure is still poorly represented at high densities. In the continuum limit, apart from the orientational correlation length it is managed to reproduce all relevant bulk properties of the hard needles using an approximate formula for the contact distance. The results show that the orientational correlation length diverges at zero and infinite pressures. The high density behavior of needles is not resolved.
Privacy in Data Service Composition<|sep|>In modern information systems different information features, about the same individual, are often collected and managed by autonomous data collection services that may have different privacy policies. Answering many end-users' legitimate queries requires the integration of data from multiple such services. However, data integration is often hindered by the lack of a trusted entity, often called a mediator, with which the services can share their data and delegate the enforcement of their privacy policies. In this paper, we propose a flexible privacy-preserving data integration approach for answering data integration queries without the need for a trusted mediator. In our approach, services are allowed to enforce their privacy policies locally. The mediator is considered to be untrusted, and only has access to encrypted information to allow it to link data subjects across the different services. Services, by virtue of a new privacy requirement, dubbed k-Protection, limiting privacy leaks, cannot infer information about the data held by each other. End-users, in turn, have access to privacy-sanitized data only. We evaluated our approach using an example and a real dataset from the healthcare application domain. The results are promising from both the privacy preservation and the performance perspectives.
Plenoptic microscope based on laser optical feedback imaging (LOFI)<|sep|>We present an overview of the performances of a plenoptic microscope which combines the high sensitivity of a laser optical feedback imaging setup , the high resolution of optical synthetic aperture and a shot noise limited signal to noise ratio by using acoustic photon tagging. By using an adapted phase filtering, this microscope allows phase drift correction and numerical aberration compensation (defocusing, coma, astigmatism ...). This new kind of microscope seems to be well adapted to make deep imaging through scattering and heterogeneous media.
Mobility, traffic and radio channel prediction: 5G and beyond applications<|sep|>Machine learning (ML) is an important component for enabling automation in Radio Access Networks (RANs). The work on applying ML for RAN has been under development for many years and is now also drawing attention in 3GPP and Open-RAN standardization fora. A key component of multiple features, also highlighted in the recent 3GPP specification work, is the use of mobility, traffic and radio channel prediction. These types of predictions form the intelligence enablers to leverage the potentials for ML for RAN, both for current and future wireless networks. This paper provides an overview with evaluation results of current applications that utilize such intelligence enablers, we then discuss how those enablers likely will be a cornerstone for emerging 6G use cases such as wireless energy transmission.
On the use of the Pad\'{e}-Fourier approximation in fast evaluation of the Green's function of layered media<|sep|>Efficient Green's function evaluation in layered media is a holy-grail of wave theory in general and for electromagnetics in particular. While there is a very large amount of knowledge in this context with vast literature, there are yet challenging cases such as the Green's function in thick lossy media and the Green's function at thick media with negative parameters. Here we propose a technique that can nicely tackle these issues. Our approach is based on a rational function approximation of the spectra using the Fourier-Pad\'{e} approximation that is carried out in a conformal mapped spectral plane. We show that this approach can be used in challenging scenarios such as very thick and lossy layers, materials with negative parameters such as in plasmonics, and even to approximate a dominant branch-cut contribution far from the source.
Knowledge Infused Policy Gradients for Adaptive Pandemic Control<|sep|>COVID-19 has impacted nations differently based on their policy implementations. The effective policy requires taking into account public information and adaptability to new knowledge. Epidemiological models built to understand COVID-19 seldom provide the policymaker with the capability for adaptive pandemic control (APC). Among the core challenges to be overcome include (a) inability to handle a high degree of non-homogeneity in different contributing features across the pandemic timeline, (b) lack of an approach that enables adaptive incorporation of public health expert knowledge, and (c) transparent models that enable understanding of the decision-making process in suggesting policy. In this work, we take the early steps to address these challenges using Knowledge Infused Policy Gradient (KIPG) methods. Prior work on knowledge infusion does not handle soft and hard imposition of varying forms of knowledge in disease information and guidelines to necessarily comply with. Furthermore, the models do not attend to non-homogeneity in feature counts, manifesting as partial observability in informing the policy. Additionally, interpretable structures are extracted post-learning instead of learning an interpretable model required for APC. To this end, we introduce a mathematical framework for KIPG methods that can (a) induce relevant feature counts over multi-relational features of the world, (b) handle latent non-homogeneous counts as hidden variables that are linear combinations of kernelized aggregates over the features, and (b) infuse knowledge as functional constraints in a principled manner. The study establishes a theory for imposing hard and soft constraints and simulates it through experiments. In comparison with knowledge-intensive baselines, we show quick sample efficient adaptation to new knowledge and interpretability in the learned policy, especially in a pandemic context.
Nanocriticality in the magnetic phase transition of CoO nanoparticles<|sep|>The universal theory of critical phase transitions describes the critical behavior at second-order phase transitions in infinitely large systems. With the increased contemporary interest in nanoscale materials, we investigated CoO nanoparticles by means of neutron scattering and found how the theory of critical phenomena breaks down in the nanoscale regime. Using CoO as a model system, we have identified a size-dependent nanocritical temperature region close to the antiferromagnetic phase transition where the magnetic correlation length of the nanoparticles converges to a constant value, which is significantly smaller than that of the saturated state found at low temperatures. This is in clear contrast to the divergence around $T_{\rm N}$ observed for bulk systems. Our findings of nanocriticality in the magnetic phase transition is of great importance for the understanding of phase transitions at the nanoscale.
The Effect of Dark Matter Halo Shape on Bar Buckling and Boxy/Peanut Bulges<|sep|>It is well established that bars evolve significantly after they form in galaxy discs, often changing shape both in and out of the disc plane. In some cases they may bend or buckle out of the disc plane resulting in the formation of boxy/peanut/x-shape bulges. In this paper we show that the dark matter halo shape affects bar formation and buckling. We have performed N-body simulations of bar buckling in non-spherical dark matter halos and traced bar evolution for 8 Gyr. We find that bar formation is delayed in oblate halos, resulting in delayed buckling whereas bars form earlier in prolate halos leading to earlier buckling. However, the duration of first buckling remains almost comparable. All the models show two buckling events but the most extreme prolate halo exhibits three distinct buckling features. Bars in prolate halos also show buckling signatures for the longest duration compared to spherical and oblate halos. Since ongoing buckling events are rarely observed, our study suggests that most barred galaxies may have more oblate or spherical halos rather than prolate halos. Our measurement of BPX structures also shows that prolate halos promote bar thickening and disc heating more than oblate and spherical halos.
Mathematical Modelling and Parameter Optimization of Pulsating Heat Pipes<|sep|>Proper heat transfer management is important to key electronic components in microelectronic applications. Pulsating heat pipes (PHP) can be an efficient solution to such heat transfer problems. However, mathematical modelling of a PHP system is still very challenging, due to the complexity and multiphysics nature of the system. In this work, we present a simplified, two-phase heat transfer model, and our analysis shows that it can make good predictions about startup characteristics. Furthermore, by considering parameter estimation as a nonlinear constrained optimization problem, we have used the firefly algorithm to find parameter estimates efficiently. We have also demonstrated that it is possible to obtain good estimates of key parameters using very limited experimental data.
Analytical study of 5G NR eMBB co-existence<|sep|>3GPP release 15 focusing on 5G general outline has been published in December 2017. The major difference with respect to currently deployed LTE is the support of various physical layer numerologies. Making the physical layer scalable allows to properly address new services such as low latency or millimeter communications. However it poses the problem of numerology coexistence. Indeed the orthogonality of the OFDM waveform is broken by the use of different subcarrier spacings and therefore multiplexed communications may interfere with each others. A first and simple solution to limit the distortion is to consider guard bands. In this paper, the authors develop analytical metrics to quantify the level of distortion induced by 5G multi-service multiplexing. Besides, general comments and guard band dimensioning are carried out. It is shown that high interference rejection needs to be associated with side lobe reduction techniques to favor an efficient bandwidth use.
Ring polymers with topological constraints<|sep|>In the first part of this work a summary is provided of some recent experiments and theoretical results which are relevant in the research of systems of polymer rings in nontrivial topological conformations. Next, some advances in modeling the behavior of single polymer knots are presented. The numerical simulations are performed with the help of the Wang-Landau Monte Carlo algorithm. To sample the polymer conformation a set of random transformations called pivot moves is used. The crucial problem of preserving the topology of the knots after each move is tackled with the help of two new techniques which are briefly explained. As an application, the results of an investigation of the effects of topology on the thermal properties of polymer knots is reported. In the end, original results are discussed concerning the use of parallelized codes to study polymers knots composed by a large number of segments within the Wang-Landau approach.
Measuring Systemic Risk: Common Factor Exposures and Tail Dependence Effects<|sep|>We model systemic risk using a common factor that accounts for market-wide shocks and a tail dependence factor that accounts for linkages among extreme stock returns. Specifically, our theoretical model allows for firm-specific impacts of infrequent and extreme events. Using data on the four sectors of the U.S. financial industry from 1996 to 2011, we uncover two key empirical findings. First, disregarding the effect of the tail dependence factor leads to a downward bias in the measurement of systemic risk, especially during weak economic times. Second, when these measures serve as leading indicators of the St. Louis Fed Financial Stress Index, measures that include a tail dependence factor offer better forecasting ability than measures based on a common factor only.
Extensional proofs in a propositional logic modulo isomorphisms<|sep|>System I is a proof language for a fragment of propositional logic where isomorphic propositions, such as $A\wedge B$ and $B\wedge A$, or $A\Rightarrow(B\wedge C)$ and $(A\Rightarrow B)\wedge(A\Rightarrow C)$ are made equal. System I enjoys the strong normalisation property. This is sufficient to prove the existence of empty types, but not to prove the introduction property (every closed term in normal form is an introduction). Moreover, a severe restriction had to be made on the types of the variables in order to obtain the existence of empty types. We show here that adding $\eta$-expansion rules to System I permits to drop this restriction, and yields a strongly normalizing calculus with enjoying the introduction property.
Stochastic Modelling of T-Cell-Activation<|sep|>We investigate a special part of the human immune system, namely the activation of T-Cells, using stochastic tools, especially sharp large deviation results. T-Cells have to distinguish reliably between foreign and self peptides which are both presented to them by antigen presenting cells. Our work is based on a model studied by Zint, Baake, and den Hollander, and originally proposed by van den Berg, Rand, and Burroughs. We are able to dispense with some restrictive distribution assumptions that were used previously, i.e. we establish a higher robustness of the model. A central issue is the analysis of two new perspectives to the scenario (two different quenched systems) in detail. This means that we do not only analyse the total probability of a T-Cell activation (the annealed case) but also consider the probability of an activation of one certain T-Cell type and the probability of a T-Cell activation by a certain antigen presenting cell (the quenched cases). Finally, we see analytically that the probability of T-Cell activation increases with the number of presented foreign peptides in all three cases.
Quantum-Classical Transitions in Complex Networks<|sep|>The inherent properties of specific physical systems can be used as metaphors for investigation of the behavior of complex networks. This insight has already been put into practice in previous work, e.g., studying the network evolution in terms of phase transitions of quantum gases or representing distances among nodes as if they were particle energies. This paper shows that the emergence of different structures in complex networks, such as the scale-free and the winner-takes-all networks, can be represented in terms of a quantum-classical transition for quantum gases. In particular, we propose a model of fermionic networks that allows us to investigate the network evolution and its dependence on the system temperature. Simulations, performed in accordance with the cited model, clearly highlight the separation between classical random and winner-takes-all networks, in full correspondence with the separation between classical and quantum regions for quantum gases. We deem this model useful for the analysis of synthetic and real complex networks.
Unitarity and vacuum stability constraints on the couplings of color octet scalars<|sep|>The recent discovery of a 126 GeV boson at the LHC will be followed by a detailed examination of its couplings in order to determine whether this particle is the Higgs boson of the standard model or one of many particles of an extended scalar sector. One such extension with a rich phenomenology consists of a color octet electroweak doublet scalar. The most general renormalizable scalar potential contains twelve new parameters and it is therefore desirable to constrain them. We present theoretical constraints on these parameters obtained by requiring perturbative unitarity for two-to-two scalar scattering amplitudes at high energy and vacuum stability.
Bernoulli Race Particle Filters<|sep|>When the weights in a particle filter are not available analytically, standard resampling methods cannot be employed. To circumvent this problem state-of-the-art algorithms replace the true weights with non-negative unbiased estimates. This algorithm is still valid but at the cost of higher variance of the resulting filtering estimates in comparison to a particle filter using the true weights. We propose here a novel algorithm that allows for resampling according to the true intractable weights when only an unbiased estimator of the weights is available. We demonstrate our algorithm on several examples.
A noise-driven attractor switching device<|sep|>Problems with artificial neural networks originate from their deterministic nature and inevitable prior learnings, resulting in inadequate adaptability against unpredictable, abrupt environmental change. Here we show that a stochastically excitable threshold unit can be utilized by these systems to partially overcome the environmental change. Using an excitable threshold system, attractors were created that represent quasi-equilibrium states into which a system settles until disrupted by environmental change. Furthermore, noise-driven attractor stabilization and switching were embodied by inhibitory connections. Noise works as a power source to stabilize and switch attractors, and endows the system with hysteresis behavior that resembles that of stereopsis and binocular rivalry in the human visual cortex. A canonical model of the ring network with inhibitory connections composed of class 1 neurons also shows properties that are similar to the simple threshold system.
A generalization of the Becker model in linear viscoelasticity: Creep, relaxation and internal friction<|sep|>We present a new rheological model depending on a real parameter $\nu \in [0,1]$ that reduces to the Maxwell body for $\nu=0$ and to the Becker body for $\nu=1$. The corresponding creep law is expressed in an integral form in which the exponential function of the Becker model is replaced and generalized by a Mittag-Leffler function of order $\nu$. Then, the corresponding non-dimensional creep function and its rate are studied as functions of time for different values of $\nu$ in order to visualize the transition from the classical Maxwell body to the Becker body. Based on the hereditary theory of linear viscoelasticity, we also approximate the relaxation function by solving numerically a Volterra integral equation of the second kind. In turn, the relaxation function is shown versus time for different values of $\nu$ to visualize again the transition from the classical Maxwell body to the Becker body. Furthermore, we provide a full characterization of the new model by computing, in addition to the creep and relaxation functions, the so-called specific dissipation $Q^{-1}$ as a function of frequency, which is of particularly relevance for geophysical applications
A partition-based Summary-Graph-Driven Method for Efficient RDF Query Processing<|sep|>RDF query optimization is a challenging problem. Although considerable factors and their impacts on query efficiency have been investigated, this problem still needs further investigation. We identify that decomposing query into a series of light-weight operations is also effective in boosting query processing. Considering the linked nature of RDF data, the correlations among operations should be carefully handled. In this paper, we present SGDQ, a novel framework that features a partition-based Summary Graph Driven Query for efficient query processing. Basically, SGDQ partitions data and models partitions as a summary graph. A query is decomposed into subqueries that can be answered without inter-partition processing. The final results are derived by perform summary graph matching and join the results generated by all matched subqueries. In essence, SGDQ combines the merits of graph match processing and relational join-based query implementation. It intentionally avoids maintain huge intermediate results by organizing sub-query processing in a summary graph driven fashion. Our extensive evaluations show that SGDQ is an effective framework for efficient RDF query processing. Its query performance consistently outperforms the representative state-of-the-art systems.
Consistent equivalence principle tests with fast radio bursts<|sep|>Fast radio bursts (FRBs) are astrophysical transients of still debated origin. So far several hundred events have been detected, mostly at extragalactic distances, and this number is expected to grow significantly over the next years. The radio signals from the burst experience dispersion as they travel through the free electrons along the line-of-sight characterised by the dispersion measure (DM) of the radio pulse. In addition, each photon also experiences a gravitational Shapiro time delay while travelling through the potentials generated by the large-scale structure. If the equivalence principle (EP) holds, the Shapiro delay is the same for photons of all frequencies. In case the EP is broken, one would expect an additional dispersion to occur which could be either positive or negative for individual sources. Here we suggest to use angular statistics of the DM fluctuations to put constraints on the EP parametrized by the post-Newtonian parameter $\gamma$. Previous studies suffer from the problem that the gravitational potential responsible for the delay diverges in a cosmological setting, which our approach avoids. We carry out a forecast for a population of FRBs observable within the next years and show that any significant detection of the DM angular power spectrum will place constraints on the EP that are by a few orders of magnitude more stringent than current limits.
Multilingual ColBERT-X<|sep|>ColBERT-X is a dense retrieval model for Cross Language Information Retrieval (CLIR). In CLIR, documents are written in one natural language, while the queries are expressed in another. A related task is multilingual IR (MLIR) where the system creates a single ranked list of documents written in many languages. Given that ColBERT-X relies on a pretrained multilingual neural language model to rank documents, a multilingual training procedure can enable a version of ColBERT-X well-suited for MLIR. This paper describes that training procedure. An important factor for good MLIR ranking is fine-tuning XLM-R using mixed-language batches, where the same query is matched with documents in different languages in the same batch. Neural machine translations of MS MARCO passages are used to fine-tune the model.
The K giant stars from the LAMOST survey data I: identification, metallicity, and distance<|sep|>We present a support vector machine classifier to identify the K giant stars from the LAMOST survey directly using their spectral line features. The completeness of the identification is about 75% for tests based on LAMOST stellar parameters. The contamination in the identified K giant sample is lower than 2.5%. Applying the classification method to about 2 million LAMOST spectra observed during the pilot survey and the first year survey, we select 298,036 K giant candidates. The metallicities of the sample are also estimated with uncertainty of $0.13\sim0.29$\,dex based on the equivalent widths of Mg$_{\rm b}$ and iron lines. A Bayesian method is then developed to estimate the posterior probability of the distance for the K giant stars, based on the estimated metallicity and 2MASS photometry. The synthetic isochrone-based distance estimates have been calibrated using 7 globular clusters with a wide range of metallicities. The uncertainty of the estimated distance modulus at $K=11$\,mag, which is the median brightness of the K giant sample, is about 0.6\,mag, corresponding to $\sim30$% in distance. As a scientific verification case, the trailing arm of the Sagittarius stream is clearly identified with the selected K giant sample. Moreover, at about 80\,kpc from the Sun, we use our K giant stars to confirm a detection of stream members near the apo-center of the trailing tail. These rediscoveries of the features of the Sagittarius stream illustrate the potential of the LAMOST survey for detecting substructures in the halo of the Milky Way.
Detectable seismic consequences of the interaction of a primordial black hole with Earth<|sep|>Galaxies observed today are likely to have evolved from density perturbations in the early universe. Perturbations that exceeded some critical threshold are conjectured to have undergone gravitational collapse to form primordial black holes (PBHs) at a range of masses. Such PBHs serve as candidates for cold dark matter and their detection would shed light on conditions in the early universe. Here we propose a mechanism to search for transits of PBHs through/nearby Earth by studying the associated seismic waves. Using a spectral-element method, we simulate and visualize this seismic wave field in Earth's interior. We predict the emergence of two unique signatures, namely, a wave that would arrive almost simultaneously everywhere on Earth's free surface and the excitation of unusual spheroidal modes with a characteristic frequency-spacing in free oscillation spectra. These qualitative characteristics are unaffected by the speed or proximity of the PBH trajectory. The seismic energy deposited by a proximal ${M^{PBH} = 10^{15}}$ g PBH is comparable to a magnitude $M_w=4$ earthquake. The non-seismic collateral damage due to the actual impact of such small PBHs with Earth would be negligible. Unfortunately, the expected collision rate is very low even if PBHs constituted all of dark matter, at ${\sim 10^{-7} {yr}^{-1}}$, and since the rate scales as ${1/M^{PBH}}$, fortunately encounters with larger, Earth-threatening PBHs are exceedingly unlikely. However, the rate at which non-colliding close encounters of PBHs could be detected by seismic activity alone is roughly two orders of magnitude larger --- that is once every hundred thousand years --- than the direct collision rate.
The mass of Beta Pictoris c from Beta Pictoris b orbital motion<|sep|>We aim to demonstrate that the presence and mass of an exoplanet can now be effectively derived from the astrometry of another exoplanet. We combined previous astrometry of $\beta$ Pictoris b with a new set of observations from the GRAVITY interferometer. The orbital motion of $\beta$ Pictoris b is fit using Markov chain Monte Carlo simulations in Jacobi coordinates. The inner planet, $\beta$ Pictoris c, was also reobserved at a separation of 96\,mas, confirming the previous orbital estimations. From the astrometry of planet b only, we can (i) detect the presence of $\beta$ Pictoris c and (ii) constrain its mass to $10.04^{+4.53}_{-3.10}\,M_{\rm Jup}$. If one adds the astrometry of $\beta$ Pictoris c, the mass is narrowed down to $9.15^{+1.08}_{-1.06}\,M_{\rm Jup}$. The inclusion of radial velocity measurements does not affect the orbital parameters significantly, but it does slightly decrease the mass estimate to $8.89^{+0.75}_{-0.75}\,M_{\rm Jup}$. With a semimajor axis of $2.68\pm0.02$\,au, a period of $1221\pm15$ days, and an eccentricity of $0.32\pm0.02$, the orbital parameters of $\beta$ Pictoris c are now constrained as precisely as those of $\beta$ Pictoris b. The orbital configuration is compatible with a high-order mean-motion resonance (7:1). The impact of the resonance on the planets' dynamics would then be negligible with respect to the secular perturbations, which might have played an important role in the eccentricity excitation of the outer planet.
Jahn-Teller distortions and the magnetic order in the perovskite manganites<|sep|>We introduce an effective model for $e_g$ electrons to describe three-dimensional perovskite (La$_{1-x}$Sr$_{x}$MnO$_3$ and La$_{1-x}$Ca$_{x}$MnO$_3$) manganites and study the magnetic and orbital order on $4\times 4\times 4$ cluster using correlated wave functions. The model includes the kinetic energy, and on-site Coulomb interactions for $e_g$ electrons, antiferromagnetic superexchange interaction between $S=3/2$ core spins, and the coupling between $e_g$ electrons and Jahn-Teller modes. The model reproduces the experimentally observed magnetic order: ($i$) $A$-type antiferromagnetic phase in the undoped insulator LaMnO$_3$, with alternating $e_g$ orbitals and with small Jahn-Teller distortions, changing to a conducting phase at 32 GPa pressure, and ($ii$) ferromagnetic order in one-eight doped La$_{7/8}$Sr$_{1/8}$MnO$_3$ and in quarter doped La$_{3/4}$Sr$_{1/4}$MnO$_3$ compounds. For half-doped La$_{1/2}$Ca$_{1/2}$MnO$_3$ one finds a competition between a ferromagnetic conductor and the CE insulating phase; the latter is stabilized by the Jahn-Teller coupling being twice larger than for the strontium-doped compound. Altogether, there is a subtle balance between all Hamiltonian parameters and the phase diagram is quite sensitive to the precise values they take.
Mass gap in the 2D O(3) non-linear sigma model with a theta=pi term<|sep|>By analytic continuation to real theta of data obtained from numerical simulation at imaginary theta we study the Haldane conjecture and show that the O(3) non-linear sigma model with a theta term in 2 dimensions becomes massless at theta=3.10(5). A modified cluster algorithm has been introduced to simulate the model with imaginary theta. Two different definitions of the topological charge on the lattice have been used; one of them needs renormalization to match the continuum operator. Our work also offers a successful test for numerical methods based on analytic continuation.
Gribov copies and confinement<|sep|>We analyse the role of the Gribov ambiguity in the construction of physical charges in gauge theories. It is shown explicitly how the Gribov copies prevent the construction of physical coloured charges beyond perturbation theory. We also present a new and manifestly non-perturbative class of Coulomb gauge Gribov copies.
Summarizing text to embed qualitative data into visualizations<|sep|>Qualitative data can be conveyed with strings of text. Fitting longer text into visualizations requires a) space to place the text inside the visualization; and b) appropriate text to fit the space available. For quantitative visualizations, space is available in area marks; or within visualization layouts where the marks have an implied space (e.g. bar charts). For qualitative visualizations, space is defined in common text layouts such as prose paragraphs. To fit text within these layouts is a function for emerging NLP capabilities such as summarization.
Unsupervised Conversation Disentanglement through Co-Training<|sep|>Conversation disentanglement aims to separate intermingled messages into detached sessions, which is a fundamental task in understanding multi-party conversations. Existing work on conversation disentanglement relies heavily upon human-annotated datasets, which are expensive to obtain in practice. In this work, we explore to train a conversation disentanglement model without referencing any human annotations. Our method is built upon a deep co-training algorithm, which consists of two neural networks: a message-pair classifier and a session classifier. The former is responsible for retrieving local relations between two messages while the latter categorizes a message to a session by capturing context-aware information. Both networks are initialized respectively with pseudo data built from an unannotated corpus. During the deep co-training process, we use the session classifier as a reinforcement learning component to learn a session assigning policy by maximizing the local rewards given by the message-pair classifier. For the message-pair classifier, we enrich its training data by retrieving message pairs with high confidence from the disentangled sessions predicted by the session classifier. Experimental results on the large Movie Dialogue Dataset demonstrate that our proposed approach achieves competitive performance compared to the previous supervised methods. Further experiments show that the predicted disentangled conversations can promote the performance on the downstream task of multi-party response selection.
Solitary, explosive, rational and elliptic doubly periodic solutions for nonlinear electron-acoustic waves in the earth's magnetotail region<|sep|>A theoretical investigation has been made of electron acoustic wave propagating in unmagnetized collisionless plasma consisting of a cold electron fluid and isothermal ions with two different temperatures obeying Boltzmann type distributions. Based on the pseudo-potential approach, large amplitude potential structures and the existence of Solitary waves are discussed. The reductive perturbation method has been employed to derive the Korteweg-de Vries (KdV) equation for small but finite amplitude electrostatic waves. An algebraic method with computerized symbolic computation, which greatly exceeds the applicability of the existing tanh, extended tanh methods in obtaining a series of exact solutions of the KdV equation, is used here. Numerical studies have been made using plasma parameters close to those values corresponding to Earth's plasma sheet boundary layer region reveals different solutions i.e., bell-shaped solitary pulses and singularity solutions at a finite point which called "blowup" solutions, Jacobi elliptic doubly periodic wave, a Weierstrass elliptic doubly periodic type solutions, in addition to the propagation of an explosive pulses. The result of the present investigation may be applicable to some plasma environments, such as earth's magnetotail region and terrestrial magnetosphere.
Flavor Physics in an SO(10) Grand Unified Model<|sep|>In supersymmetric grand-unified models, the lepton mixing matrix can possibly affect flavor-changing transitions in the quark sector. We present a detailed analysis of a model proposed by Chang, Masiero and Murayama, in which the near-maximal atmospheric neutrino mixing angle governs large new b -> s transitions. Relating the supersymmetric low-energy parameters to seven new parameters of this SO(10) GUT model, we perform a correlated study of several flavor-changing neutral current (FCNC) processes. We find the current bound on B(tau -> mu gamma) more constraining than B(B -> X_s gamma). The LEP limit on the lightest Higgs boson mass implies an important lower bound on tan beta, which in turn limits the size of the new FCNC transitions. Remarkably, the combined analysis does not rule out large effects in B_s-B_s-bar mixing and we can easily accomodate the large CP phase in the B_s-B_s-bar system which has recently been inferred from a global analysis of CDF and DO data. The model predicts a particle spectrum which is different from the popular Constrained Minimal Supersymmetric Standard Model (CMSSM). B(tau -> mu gamma) enforces heavy masses, typically above 1 TeV, for the sfermions of the degenerate first two generations. However, the ratio of the third-generation and first-generation sfermion masses is smaller than in the CMSSM and a (dominantly right-handed) stop with mass below 500 GeV is possible.
Blow up on a curve for a nonlinear Schr\"odinger equation on Riemannian surfaces<|sep|>We consider the focusing quintic nonlinear Schr\"odinger equation posed on a rotationally symmetric surface, typically the sphere $S^2$ or the two dimensional hyperbolic space $H^2$. We prove the existence and the stability of solutions blowing up on a suitable curve with the log log speed. The Euclidean case is handled in Rapha\"el (2006) and our result shows that the log log rate persists in other geometries with the assumption of a radial symmetry of the manifold.
Fast R-CNN<|sep|>This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.
MEKF Ignoring Initial Conditions for Attitude Estimation Using Vector Observations<|sep|>In this paper, the well-known multiplicative extended Kalman filter (MEKF) is re-investigated for attitude estimation using vector observations. From the Lie group theory, it is shown that the attitude estimation model is group affine and its error state model should be trajectory-independent. Moreover, with such trajectory-independent error state model, the linear Kalman filter is still effective for large initialization errors. However, the measurement model of the traditional MEKF is dependent on the attitude prediction, which is therefore trajectory-dependent. This is also the main reason why the performance of traditional MEKF is degraded for large initialization errors. Through substitution of the attitude prediction related term with the vector observation in body frame, a trajectory-independent measurement model is derived for MEKF. Meanwhile, the MEKFs with reference attitude error definition and with global state formulating on special Euclidean group have also been studied, with main focus on derivation of the trajectory-independent measurement models. Extensive Monte Carlo simulations and field test of attitude estimation implementations demonstrate that the performance of MEKFs can be much improved with trajectory-independent measurement models.
Quantum Request-Answer Game with Buffer Model for Online Algorithms<|sep|>We consider online algorithms as a request-answer game. An adversary that generates input requests, and an online algorithm answers. We consider a generalized version of the game that has a buffer of limited size. The adversary loads data to the buffer, and the algorithm has random access to elements of the buffer. We consider quantum and classical (deterministic or randomized) algorithms for the model. In the paper, we provide a specific problem (The Most Frequent Keyword Problem) and a quantum algorithm that works better than any classical (deterministic or randomized) algorithm in terms of competitive ratio. At the same time, for the problem, classical online algorithms in the standard model are equivalent to the classical algorithms in the request-answer game with buffer model.
Giant Rings in the CMB Sky<|sep|>We find a unique direction in the CMB sky around which giant rings have an anomalous mean temperature profile. This direction is in very close alignment with the afore measured anomalously large bulk flow direction. Using Monte Carlo simulations, we estimate the significance of the giant rings at the $3\sigma$ level and the alignment with the bulk flow at $2.5\sigma$. We argue that a cosmic defect seeded by a pre-inflationary particle could explain the giant rings, the large bulk flow and their alignment.
Meson and Baryon resonances<|sep|>In this talk I review recent advances on the structure of the meson and baryon resonances which can be dynamically generated from the interaction of mesons or mesons and baryons. Particular emphasis is put on results involving vector mesons, which bring new light into the nature of some of the observed higher mass mesons and baryons and make predictions for new states.
Scale invariance vs conformal invariance<|sep|>In this review article, we discuss the distinction and possible equivalence between scale invariance and conformal invariance in relativistic quantum field theories. Under some technical assumptions, we can prove that scale invariant quantum field theories in $d=2$ dimension necessarily possess the enhanced conformal symmetry. The use of the conformal symmetry is well appreciated in the literature, but the fact that all the scale invariant phenomena in $d=2$ dimension enjoy the conformal property relies on the deep structure of the renormalization group. The outstanding question is whether this feature is specific to $d=2$ dimension or it holds in higher dimensions, too. As of January 2014, our consensus is that there is no known example of scale invariant but non-conformal field theories in $d=4$ dimension under the assumptions of (1) unitarity, (2) Poincar\'e invariance (causality), (3) discrete spectrum in scaling dimension, (4) existence of scale current and (5) unbroken scale invariance in the vacuum. We have a perturbative proof of the enhancement of conformal invariance from scale invariance based on the higher dimensional analogue of Zamolodchikov's $c$-theorem, but the non-perturbative proof is yet to come. As a reference we have tried to collect as many interesting examples of scale invariance in relativistic quantum field theories as possible in this article. We give a complementary holographic argument based on the energy-condition of the gravitational system and the space-time diffeomorphism in order to support the claim of the symmetry enhancement. We believe that the possible enhancement of conformal invariance from scale invariance reveals the sublime nature of the renormalization group and space-time with holography.
Maximum a Posteriori Joint State Path and Parameter Estimation in Stochastic Differential Equations<|sep|>A wide variety of phenomena of engineering and scientific interest are of a continuous-time nature and can be modeled by stochastic differential equations (SDEs), which represent the evolution of the uncertainty in the states of a system. For systems of this class, some parameters of the SDE might be unknown and the measured data often includes noise, so state and parameter estimators are needed to perform inference and further analysis using the system state path. The distributions of SDEs which are nonlinear or subject to non-Gaussian measurement noise do not admit tractable analytic expressions, so state and parameter estimators for these systems are often approximations based on heuristics, such as the extended and unscented Kalman smoothers, or the prediction error method using nonlinear Kalman filters. However, the Onsager Machlup functional can be used to obtain fictitious densities for the parameters and state-paths of SDEs with analytic expressions. In this thesis, we provide a unified theoretical framework for maximum a posteriori (MAP) estimation of general random variables, possibly infinite-dimensional, and show how the Onsager--Machlup functional can be used to construct the joint MAP state-path and parameter estimator for SDEs. We also prove that the minimum energy estimator, which is often thought to be the MAP state-path estimator, actually gives the state paths associated to the MAP noise paths. Furthermore, we prove that the discretized MAP state-path and parameter estimators, which have emerged recently as powerful alternatives to nonlinear Kalman smoothers, converge hypographically as the discretization step vanishes. Their hypographical limit, however, is the MAP estimator for SDEs when the trapezoidal discretization is used and the minimum energy estimator when the Euler discretization is used, associating different interpretations to each discretized estimate.
Condensation Transition in Polydisperse Hard Rods<|sep|>We study a mass transport model, where spherical particles diffusing on a ring can stochastically exchange volume $v$, with the constraint of a fixed total volume $V=\sum_{i=1}^N v_i$, $N$ being the total number of particles. The particles, referred to as $p$-spheres, have a linear size that behaves as $v_i^{1/p}$ and our model thus represents a gas of polydisperse hard rods with variable diameters $v_i^{1/p}$. We show that our model admits a factorized steady state distribution which provides the size distribution that minimizes the free energy of a polydisperse hard rod system, under the constraints of fixed $N$ and $V$. Complementary approaches (explicit construction of the steady state distribution on the one hand ; density functional theory on the other hand) completely and consistently specify the behaviour of the system. A real space condensation transition is shown to take place for $p>1$: beyond a critical density a macroscopic aggregate is formed and coexists with a critical fluid phase. Our work establishes the bridge between stochastic mass transport approaches and the optimal polydispersity of hard sphere fluids studied in previous articles.
Fragmenting protostellar disks: properties and observational signatures<|sep|>Using numerical hydrodynamic simulations, we study the gravitational fragmentation of an unstable protostellar disc formed during the collapse of a pre-stellar core with a mass of 1.2 M_sun. The forming fragments span a mass range from about a Jupiter mass to very-low-mass protostars and are located at distances from a few tens to a thousand AU, with a dearth of objects at < 100 AU. We explore the possibility of observational detection of the fragments in discs viewed through the outflow cavity at a distance of 250 pc. We demonstrate that one hour of integration time with the Atacama Large Millimeter/sub-millimeter Array (ALMA) is sufficient to detect the fragments with masses as low as 1.5 M_Jup at orbital distances up to 800 AU from the protostar. The ALMA resolution sets the limit on the minimum orbital distance of detectable fragments. For the adopted resolution of our simulated ALMA images of 0.1", the fragments can be detected at distances down to 50 AU. At smaller distances, the fragments usually merge with the central density peak. The likelihood for detecting the fragments reduces significantly for a lower resolution of 0.5". Some of the most massive fragments, regardless of their orbital distance, can produce characteristic peaks at approximately 5 micron and hence their presence can be indirectly inferred from the observed spectral energy distributions of protostars.
The Cardy-Verlinde equation in a spherical symmetric gravitational collapse<|sep|>The Cardy-Verlinde formula is analyzed in the contest of the gravitational collapse. Starting from the holographic principle, we show how the equations for a homogeneous and isotropic gravitational collapse describe the formation of the black hole entropy. Some comments on the role of the entangled entropy and the connection with the c-theorem are made.
Quantization of Calogero-Painlev\'e System and Multi-Particle Quantum Painlev\'e Equations II-VI<|sep|>We consider the isomonodromic formulation of the Calogero-Painlev\'e multi-particle systems and proceed to their canonical quantization. We then proceed to the quantum Hamiltonian reduction on a special representation to radial variables, in analogy with the classical case and also with the theory of quantum Calogero equations. This quantized version is compared to the generalization of a result of Nagoya on integral representations of certain solutions of the quantum Painlev\'e equations. We also provide multi-particle generalizations of these integral representations.
Sufficient Conditions for Tuza's Conjecture on Packing and Covering Triangles<|sep|>Given a simple graph $G=(V,E)$, a subset of $E$ is called a triangle cover if it intersects each triangle of $G$. Let $\nu_t(G)$ and $\tau_t(G)$ denote the maximum number of pairwise edge-disjoint triangles in $G$ and the minimum cardinality of a triangle cover of $G$, respectively. Tuza conjectured in 1981 that $\tau_t(G)/\nu_t(G)\le2$ holds for every graph $G$. In this paper, using a hypergraph approach, we design polynomial-time combinatorial algorithms for finding small triangle covers. These algorithms imply new sufficient conditions for Tuza's conjecture on covering and packing triangles. More precisely, suppose that the set $\mathscr T_G$ of triangles covers all edges in $G$. We show that a triangle cover of $G$ with cardinality at most $2\nu_t(G)$ can be found in polynomial time if one of the following conditions is satisfied: (i) $\nu_t(G)/|\mathscr T_G|\ge\frac13$, (ii) $\nu_t(G)/|E|\ge\frac14$, (iii) $|E|/|\mathscr T_G|\ge2$. Keywords: Triangle cover, Triangle packing, Linear 3-uniform hypergraphs, Combinatorial algorithms
Boundary Giant Magnons and Giant Gravitons<|sep|>We construct the full set of boundary giant magnons on $\mathbb{R}\times S^{2}$ attached to the maximal $Z=0$ giant graviton by mapping from the general solution to static sine-Gordon theory on the interval and compute the values of $\Delta-J$ at finite $J$, including the leading order corrections when $J$ is large. We then consider the Born-Infeld theory of the giant graviton itself to construct BIon spike solutions that correspond to the world volume description of the boundary giant magnons at finite $J$.
Developing a Temporal Bibliographic Data Set for Entity Resolution<|sep|>Entity resolution is the process of identifying groups of records within or across data sets where each group represents a real-world entity. Novel techniques that consider temporal features to improve the quality of entity resolution have recently attracted significant attention. However, there are currently no large data sets available that contain both temporal information as well as ground truth information to evaluate the quality of temporal entity resolution approaches. In this paper, we describe the preparation of a temporal data set based on author profiles extracted from the Digital Bibliography and Library Project (DBLP). We completed missing links between publications and author profiles in the DBLP data set using the DBLP public API. We then used the Microsoft Academic Graph (MAG) to link temporal affiliation information for DBLP authors. We selected around 80K (1%) of author profiles that cover 2 million (50%) publications using information in DBLP such as alternative author names and personal web profile to improve the reliability of the resulting ground truth, while at the same time keeping the data set challenging for temporal entity resolution research.
An artificial intelligence tool for heterogeneous team formation in the classroom<|sep|>Nowadays, there is increasing interest in the development of teamwork skills in the educational context. This growing interest is motivated by its pedagogical effectiveness and the fact that, in labour contexts, enterprises organize their employees in teams to carry out complex projects. Despite its crucial importance in the classroom and industry, there is a lack of support for the team formation process. Not only do many factors influence team performance, but the problem becomes exponentially costly if teams are to be optimized. In this article, we propose a tool whose aim it is to cover such a gap. It combines artificial intelligence techniques such as coalition structure generation, Bayesian learning, and Belbin's role theory to facilitate the generation of working groups in an educational context. This tool improves current state of the art proposals in three ways: i) it takes into account the feedback of other teammates in order to establish the most predominant role of a student instead of self-perception questionnaires; ii) it handles uncertainty with regard to each student's predominant team role; iii) it is iterative since it considers information from several interactions in order to improve the estimation of role assignments. We tested the performance of the proposed tool in an experiment involving students that took part in three different team activities. The experiments suggest that the proposed tool is able to improve different teamwork aspects such as team dynamics and student satisfaction.
Correlation properties of a three-body bosonic mixture in a harmonic trap<|sep|>We make use of a simple pair correlated wave function approach to obtain results for the ground-state densities and momentum distribution of a one-dimensional three-body bosonic system with different interactions in a harmonic trap. For equal interactions this approach is able to reproduce the known analytical cases of zero and infinite repulsion. We show that our results for the correlations agree with the exact diagonalization in all interaction regimes and with analytical results for the strongly repulsive impurity. This method also enables us to access the more complicated cases of mixed interactions, and the probability densities of these systems are analyzed.
Transmit Covariance and Waveform Optimization for Non-orthogonal CP-FBMA System<|sep|>Filter bank multiple access (FBMA) without subbands orthogonality has been proposed as a new candidate waveform to better meet the requirements of future wireless communication systems and scenarios. It has the ability to process directly the complex symbols without any fancy preprocessing. Along with the usage of cyclic prefix (CP) and wide-banded subband design, CP-FBMA can further improve the peak-to-average power ratio and bit error rate performance while reducing the length of filters. However, the potential gain of removing the orthogonality constraint on the subband filters in the system has not been fully exploited from the perspective of waveform design, which inspires us to optimize the subband filters for CP-FBMA system to maximizing the achievable rate. Besides, we propose a joint optimization algorithm to optimize both the waveform and the covariance matrices iteratively. Furthermore, the joint optimization algorithm can meet the requirements of filter design in practical applications in which the available spectrum consists of several isolated bandwidth parts. Both general framework and detailed derivation of the algorithms are presented. Simulation results show that the algorithms converge after only a few iterations and can improve the sum rate dramatically while reducing the transmission delay of information symbols.
The Bright X-Ray Source in NGC 3413<|sep|>The emission-line dwarf galaxy NGC 3413 is known to host a bright X-ray source near its optical center. The 0.3-10 keV luminosity of this source is estimated to be approximately 10$^{39}$ erg$s^{-1}$ potentially qualifying it as an ultra-luminous X-ray (ULX) source. A recent XMM-Newton observation suggests that the source is not point-like, and instead, is more likely a composite of point-like sources with extended and/or diffuse emission. The spectral and temporal features of the bright region are similar to those associated with the so-called broadened disk state of ULXs. Based on a multi-color blackbody spectral fit, we estimate the mass of the bright source to be in the range 3 - 20M$_{\odot}$. Potential optical counterparts are also explored with the aid of available SDSS and PanStars data.
Dynamical screening in monolayer transition-metal dichalcogenides and its manifestations in the exciton spectrum<|sep|>Monolayer transition-metal dichalcogenides (ML-TMDs) offer exciting opportunities to test the manifestations of many-body interactions through changes in the charge density. Tuning the charge density by a gate voltage leads to profound changes in the optical spectra of excitons in ML-TMDs. We review the band-gap renormalization and dynamical screening as a function of charge density, and then incorporate these effects through various approximations that model long-wavelength charge excitations in the Bethe-Salpeter Equation (BSE). We then show that coupling between excitons and shortwave charge excitations is essential to resolve several experimental puzzles. Unlike ubiquitous and well-studied plasmons, driven by collective oscillations of the background charge density in the long-wavelength limit, we discuss the emergence of shortwave plasmons that originate from the short-range Coulomb interaction through which electrons transition between the $\mathbf{K}$ and $-\mathbf{K}$ valleys. We study the coupling between the shortwave plasmons and the neutral exciton through the self-energy of the latter. We then elucidate how this coupling as well as the spin ordering in the conduction band give rise to an experimentally observed optical sideband in electron-doped W-based MLs, conspicuously absent in electron-doped Mo-based MLs or any hole-doped ML-TMDs. While the focus of this review is on the optical manifestations of many-body effects in ML-TMDs, a systematic description of the dynamical screening and its various approximations allow one to revisit other phenomena, such as nonequilibrium transport or superconducting pairing, where the use of the BSE or the emergence of shortwave plasmons can play an important role.
Occlusion Resistant Object Rotation Regression from Point Cloud Segments<|sep|>Rotation estimation of known rigid objects is important for robotic applications such as dexterous manipulation. Most existing methods for rotation estimation use intermediate representations such as templates, global or local feature descriptors, or object coordinates, which require multiple steps in order to infer the object pose. We propose to directly regress a pose vector from raw point cloud segments using a convolutional neural network. Experimental results show that our method can potentially achieve competitive performance compared to a state-of-the-art method, while also showing more robustness against occlusion. Our method does not require any post processing such as refinement with the iterative closest point algorithm.
Convex programming with fast proximal and linear operators<|sep|>We present Epsilon, a system for general convex programming using fast linear and proximal operators. As with existing convex programming frameworks, users specify convex optimization problems using a natural grammar for mathematical expressions, composing functions in a way that is guaranteed to be convex by the rules of disciplined convex programming. Given such an input, the Epsilon compiler transforms the optimization problem into a mathematically equivalent form consisting only of functions with efficient proximal operators---an intermediate representation we refer to as prox-affine form. By reducing problems to this form, Epsilon enables solving general convex problems using a large library of fast proximal and linear operators; numerical examples on many popular problems from statistics and machine learning show that this often improves running times by an order of magnitude or more vs. existing approaches based on conic solvers.
A Calculus for Modular Loop Acceleration<|sep|>Loop acceleration can be used to prove safety, reachability, runtime bounds, and (non-)termination of programs operating on integers. To this end, a variety of acceleration techniques has been proposed. However, all of them are monolithic: Either they accelerate a loop successfully or they fail completely. In contrast, we present a calculus that allows for combining acceleration techniques in a modular way and we show how to integrate many existing acceleration techniques into our calculus. Moreover, we propose two novel acceleration techniques that can be incorporated into our calculus seamlessly. An empirical evaluation demonstrates the applicability of our approach.
Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders<|sep|>Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For examples, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model. In this work, we propose a regularization framework for variational autoencoders as a step toward semantic validity. We focus on the matrix representation of graphs and formulate penalty terms that regularize the output distribution of the decoder to encourage the satisfaction of validity constraints. Experimental results confirm a much higher likelihood of sampling valid graphs in our approach, compared with others reported in the literature.
A non-intrusive reduced-order modeling method using polynomial chaos expansion<|sep|>We propose a non-intrusive reduced-order modeling method based on proper orthogonal decomposition (POD) and polynomial chaos expansion (PCE) for stochastic representations in uncertainty quantification (UQ) analysis. Firstly, POD provides an optimally ordered basis from a set of selected full-order snapshots. Truncating this optimal basis, we construct a reduced-order model with undetermined coefficients. Then, PCE is utilized to approximate the coefficients of the truncated basis. In the proposed method, we construct a PCE using a non-intrusive regression-based method. Combined with the model reduction ability of POD, the proposed method efficiently provides stochastic representations in UQ analysis. To investigate the performance of the proposed method, we provide three numerical examples, i.e., a highly nonlinear analytical function with three uncertain parameters, two-dimensional (2D) heat-driven cavity flow with a stochastic boundary temperature, and 2D heat diffusion with stochastic conductivity. The results demonstrate that the proposed method significantly reduces the computational costs and storage requirements that arise due to high-dimensional physical and random spaces. While demonstrating a similar accuracy with that of the classical full-PCE in predicting statistical quantities. Furthermore, the proposed method reasonably predict the outputs of the full order model using only a few snapshots.
LIGHTEN: Learning Interactions with Graph and Hierarchical TEmporal Networks for HOI in videos<|sep|>Analyzing the interactions between humans and objects from a video includes identification of the relationships between humans and the objects present in the video. It can be thought of as a specialized version of Visual Relationship Detection, wherein one of the objects must be a human. While traditional methods formulate the problem as inference on a sequence of video segments, we present a hierarchical approach, LIGHTEN, to learn visual features to effectively capture spatio-temporal cues at multiple granularities in a video. Unlike current approaches, LIGHTEN avoids using ground truth data like depth maps or 3D human pose, thus increasing generalization across non-RGBD datasets as well. Furthermore, we achieve the same using only the visual features, instead of the commonly used hand-crafted spatial features. We achieve state-of-the-art results in human-object interaction detection (88.9% and 92.6%) and anticipation tasks of CAD-120 and competitive results on image based HOI detection in V-COCO dataset, setting a new benchmark for visual features based approaches. Code for LIGHTEN is available at https://github.com/praneeth11009/LIGHTEN-Learning-Interactions-with-Graphs-and-Hierarchical-TEmporal-Networks-for-HOI
Fast and accurate mock catalogue generation for low-mass galaxies<|sep|>We present an accurate and fast framework for generating mock catalogues including low-mass halos, based on an implementation of the COmoving Lagrangian Acceleration (COLA) technique. Multiple realisations of mock catalogues are crucial for analyses of large-scale structure, but conventional N-body simulations are too computationally expensive for the production of thousands of realisations. We show that COLA simulations can produce accurate mock catalogues with a moderate computation resource for low- to intermediate- mass galaxies in $10^{12} M_\odot$ haloes, both in real and redshift space. COLA simulations have accurate peculiar velocities, without systematic errors in the velocity power spectra for k < 0.15 h/Mpc, and with only 3-per-cent error for k < 0.2 h/Mpc. We use COLA with 10 time steps and a Halo Occupation Distribution to produce 600 mock galaxy catalogues of the WiggleZ Dark Energy Survey. Our parallelized code for efficient generation of accurate halo catalogues is publicly available at github.com/junkoda/cola_halo.
Theory of two-dimensional macroscopic quantum tunneling in YBa$_2$ Cu$_3$ O$_{7-\delta}$ Josephson junctions coupled to an LC circuit<|sep|>We investigate classical thermal activation (TA) and macroscopic quantum tunneling (MQT) for a YBa$_2$Cu$_3$O$_{7-\delta}$ (YBCO) Josephson junction coupled to an LC circuit theoretically. Due to the coupling between the junction and the LC circuit, the macroscopic phase dynamics can be described as the escape process of a fictitious particle with an anisotropic mass moving in a two-dimensional potential. We analytically calculate the escape rate including both the TA and MQT regime by taking into account the peculiar dynamical nature of the system. In addtion to large suppression of the MQT rate at zero temperature, we study details of the temperature dependece of the escape rate across a crossover region. These results are in an excellent agreement with recent experimental data for the MQT and TA rate in a YBCO biepitaxial Josephson junction. Therefore the coupling to the LC circuit is essential in understanding the macroscopic quantum dynamics and the qubit operation based on the YBCO biepitaxial Josephson junctions.
Observable characteristics of the charged black hole surrounded by thin disk accretion in Rastall gravity<|sep|>The observable characteristics of the charged black hole (BH) surrounded by a thin disk accretion are investigated in the Rastall gravity. We found that the radii of the direct emission, lensing ring, and photon ring dramatically increased as the radiation field parameter increases, but they only weakly depend on the BH charge. Three positions of the radiation accretion disk relative to the BH are considered, i.e., the innermost accretion disk is closed to the radii of the innermost stable circular orbit, the photon ring of the BH, and the event horizon of the BH. The observed images in three cases respectively are obtained. It is found that the total observed flux is dominated by the direct emission, the lensing ring provides a small contribution, and the photon ring is negligible. The lensing and photon rings could not be observed in the blurred image with the EHT resolution. Our results suggest that the observable characteristics of the charged BH surrounded by the thin disk accretion in the Rastall gravity depend on both the BH space-time structure and the position of the radiating accretion disk with respect to the BH. The research of these BH images may serve as a probe for the BH-disk structure in M87$^{*}$ like nearby active galactic nuclei.
Stability of traveling, pre-tensioned, heavy cables<|sep|>We study the dynamics of an inclined tensioned, heavy cable traveling with a constant speed in the vertical plane. The cable is modeled as a beam resisting bending and shear. The governing equation for the transverse in-plane vibrations of the cable are derived through the Newton-Euler method. The cable dynamics is also studied in the limit of zero bending stiffness. In all cases, application of en- ergy balance reveals that the total energy of the system fluctuates even though the oscillations are small and bounded in time, indicating that the system is nonconser- vative. A comprehensive stability analysis is carried out in the parameter space of inclination, traveling speed, pre-tension, bending rigidity and the slenderness of the cable. Effect of damping is also considered. We conclude that, while pre-tension, rigidity and slenderness enhance the stability of the traveling cable, the angle of inclination affects the stability adversely. These results may act as guidelines for safer design and operation.
Accurate IMU Preintegration Using Switched Linear Systems For Autonomous Systems<|sep|>Employing an inertial measurement unit (IMU) as an additional sensor can dramatically improve both reliability and accuracy of visual/Lidar odometry (VO/LO). Different IMU integration models are introduced using different assumptions on the linear acceleration from the IMU. In this paper, a novel IMU integration model is proposed by using switched linear systems. The proposed approach assumes that both the linear acceleration and the angular velocity in the body frame are constant between two consecutive IMU measurements. This is more realistic in real world situation compared to existing approaches which assume that linear acceleration is constant in the world frame while angular velocity is constant in the body frame between two successive IMU measurements. Experimental results show that the proposed approach outperforms the state-of-the-art IMU integration model. The proposed model is thus important for localization of high speed autonomous vehicles in GPS denied environments.
Knowledge Map: Toward a New Approach Supporting the Knowledge Management in Distributed Data Mining<|sep|>Distributed data mining (DDM) deals with the problem of finding patterns or models, called knowledge, in an environment with distributed data and computations. Today, a massive amounts of data which are often geographically distributed and owned by different organisation are being mined. As consequence, a large mount of knowledge are being produced. This causes problems of not only knowledge management but also visualization in data mining. Besides, the main aim of DDM is to exploit fully the benefit of distributed data analysis while minimising the communication. Existing DDM techniques perform partial analysis of local data at individual sites and then generate a global model by aggregating these local results. These two steps are not independent since naive approaches to local analysis may produce an incorrect and ambiguous global data model. The integrating and cooperating of these two steps need an effective knowledge management, concretely an efficient map of knowledge in order to take the advantage of mined knowledge to guide mining the data. In this paper, we present "knowledge map", a representation of knowledge about mined knowledge. This new approach aims to manage efficiently mined knowledge in large scale distributed platform such as Grid. This knowledge map is used to facilitate not only the visualization, evaluation of mining results but also the coordinating of local mining process and existing knowledge to increase the accuracy of final model.
Equivariant Contrastive Learning for Sequential Recommendation<|sep|>Contrastive learning (CL) benefits the training of sequential recommendation models with informative self-supervision signals. Existing solutions apply general sequential data augmentation strategies to generate positive pairs and encourage their representations to be invariant. However, due to the inherent properties of user behavior sequences, some augmentation strategies, such as item substitution, can lead to changes in user intent. Learning indiscriminately invariant representations for all augmentation strategies might be sub-optimal. Therefore, we propose Equivariant Contrastive Learning for Sequential Recommendation (ECL-SR), which endows SR models with great discriminative power, making the learned user behavior representations sensitive to invasive augmentations (e.g., item substitution) and insensitive to mild augmentations (e.g., feature-level dropout masking). In detail, we use the conditional discriminator to capture differences in behavior due to item substitution, which encourages the user behavior encoder to be equivariant to invasive augmentations. Comprehensive experiments on four benchmark datasets show that the proposed ECL-SR framework achieves competitive performance compared to state-of-the-art SR models. The source code will be released.
Dense Uncertainty Estimation via an Ensemble-based Conditional Latent Variable Model<|sep|>Uncertainty estimation has been extensively studied in recent literature, which can usually be classified as aleatoric uncertainty and epistemic uncertainty. In current aleatoric uncertainty estimation frameworks, it is often neglected that the aleatoric uncertainty is an inherent attribute of the data and can only be correctly estimated with an unbiased oracle model. Since the oracle model is inaccessible in most cases, we propose a new sampling and selection strategy at train time to approximate the oracle model for aleatoric uncertainty estimation. Further, we show a trivial solution in the dual-head based heteroscedastic aleatoric uncertainty estimation framework and introduce a new uncertainty consistency loss to avoid it. For epistemic uncertainty estimation, we argue that the internal variable in a conditional latent variable model is another source of epistemic uncertainty to model the predictive distribution and explore the limited knowledge about the hidden true model. We validate our observation on a dense prediction task, i.e., camouflaged object detection. Our results show that our solution achieves both accurate deterministic results and reliable uncertainty estimation.
Population games and Discrete optimal transport<|sep|>We propose a new evolutionary dynamics for population games with a discrete strategy set, inspired by the theory of optimal transport and Mean field games. The dynamics can be described as a Fokker-Planck equation on a discrete strategy set. The derived dynamics is the gradient flow of a free energy and the transition density equation of a Markov process. Such process provides models for the behavior of the individual players in population, which is myopic, greedy and irrational. The stability of the dynamics is governed by optimal transport metric, entropy and Fisher information.
Minimal Z' models and the 125 GeV Higgs boson<|sep|>The 1-loop renormalization group equations for the minimal Z' models encompassing a type-I seesaw mechanism are studied in the light of the 125 GeV Higgs boson discovery. This model is taken as a benchmark for the general case of singlet extensions of the standard model. The most important result is that negative scalar mixing angles are favoured with respect to positive values. Further, a minimum value for the latter exists, as well as a maximum value for the masses of the heavy neutrinos, depending on the vacuum expectation value of the singlet scalar.
Immigration Document Classification and Automated Response Generation<|sep|>In this paper, we consider the problem of organizing supporting documents vital to U.S. work visa petitions, as well as responding to Requests For Evidence (RFE) issued by the U.S.~Citizenship and Immigration Services (USCIS). Typically, both processes require a significant amount of repetitive manual effort. To reduce the burden of mechanical work, we apply machine learning methods to automate these processes, with humans in the loop to review and edit output for submission. In particular, we use an ensemble of image and text classifiers to categorize supporting documents. We also use a text classifier to automatically identify the types of evidence being requested in an RFE, and used the identified types in conjunction with response templates and extracted fields to assemble draft responses. Empirical results suggest that our approach achieves considerable accuracy while significantly reducing processing time.
Quantitative sum rule analysis of low-temperature spectral functions<|sep|>We analyze QCD and Weinberg-type sum rules in a low-temperature pion gas using vector and axial-vector spectral functions following from the model-independent chiral-mixing scheme. Toward this end we employ recently constructed vacuum spectral functions with ground and first-excited states in both channels and a universal perturbative continuum; they quantitatively describe hadronic tau-decay data and satisfy vacuum sum rules. These features facilitate the implementation of chiral mixing without further assumptions, and lead to in-medium spectral functions which exhibit a mutual tendency of compensating resonance and dip structures, suggestive for an approach toward structureless distributions. In the sum rule analysis, we account for pion mass corrections, which turn out to be significant. While the Weinberg sum rules remain satisfied even at high temperatures, the numerical evaluation of the QCD sum rules for vector and axial-vector channels reveals significant deviations setting in for temperatures beyond ~140 MeV, suggestive of additional physics beyond low-energy chiral pion dynamics.
Quantum Public-Key Encryption with Information Theoretic Security<|sep|>We propose a definition for the information theoretic security of a quantum public-key encryption scheme, and present bit-oriented and two-bit-oriented encryption schemes satisfying our security definition via the introduction of a new public-key algorithm structure. We extend the scheme to a multi-bitoriented one, and conjecture that it is also information theoretically secure, depending directly on the structure of our new algorithm.
Exact ground state Monte Carlo method for Bosons without importance sampling<|sep|>Generally ``exact'' Quantum Monte Carlo computations for the ground state of many Bosons make use of importance sampling. The importance sampling is based, either on a guiding function or on an initial variational wave function. Here we investigate the need of importance sampling in the case of Path Integral Ground State (PIGS) Monte Carlo. PIGS is based on a discrete imaginary time evolution of an initial wave function with a non zero overlap with the ground state, that gives rise to a discrete path which is sampled via a Metropolis like algorithm. In principle the exact ground state is reached in the limit of an infinite imaginary time evolution, but actual computations are based on finite time evolutions and the question is whether such computations give unbiased exact results. We have studied bulk liquid and solid 4He with PIGS by considering as initial wave function a constant, i.e. the ground state of an ideal Bose gas. This implies that the evolution toward the ground state is driven only by the imaginary time propagator, i.e. there is no importance sampling. For both the phases we obtain results converging to those obtained by considering the best available variational wave function (the Shadow wave function) as initial wave function. Moreover we obtain the same results even by considering wave functions with the wrong correlations, for instance a wave function of a strongly localized Einstein crystal for the liquid phase. This convergence is true not only for diagonal properties such as the energy, the radial distribution function and the static structure factor, but also for off-diagonal ones, such as the one--body density matrix. From this analysis we conclude that zero temperature PIGS calculations can be as unbiased as those of finite temperature Path Integral Monte Carlo.
Magnetic Phases of Spatially-Modulated Spin-1 Chains in Rydberg Excitons: Classical and Quantum Simulations<|sep|>In this work, we study the magnetic phases of a spatially-modulated chain of spin-1 Rydberg excitons. Using the Density Matrix Renormalization Group (DMRG) technique we study various magnetic and topologically nontrivial phases using both single-particle properties like local magnetization and quantum entropy as well as many-body ones like pair-wise N\'eel and long-range string correlations. In particular, we investigate the emergence and robustness of Haldane phase, a topological phase of anti-ferromagnetic spin-1 chains. Further, we devise a hybrid quantum algorithm employing Restricted Boltzmann Machine to simulate the ground state of such a system which shows very good agreement with the results of exact diagonalization (ED) and DMRG.
Split Learning Meets Koopman Theory for Wireless Remote Monitoring and Prediction<|sep|>Remote state monitoring over wireless is envisaged to play a pivotal role in enabling beyond 5G applications ranging from remote drone control to remote surgery. One key challenge is to identify the system dynamics that is non-linear with a large dimensional state. To obviate this issue, in this article we propose to train an autoencoder whose encoder and decoder are split and stored at a state sensor and its remote observer, respectively. This autoencoder not only decreases the remote monitoring payload size by reducing the state representation dimension, but also learns the system dynamics by lifting it via a Koopman operator, thereby allowing the observer to locally predict future states after training convergence. Numerical results under a non-linear cart-pole environment demonstrate that the proposed split learning of a Koopman autoencoder can locally predict future states, and the prediction accuracy increases with the representation dimension and transmission power.
Decentralized Optimal Control for Connected Automated Vehicles at Intersections Including Left and Right Turns<|sep|>In prior work, we addressed the problem of optimally controlling on line connected and automated vehicles crossing two adjacent intersections in an urban area to minimize fuel consumption while achieving maximal throughput without any explicit traffic signaling and without considering left and right turns. In this paper, we extend the solution of this problem to account for left and right turns under hard safety constraints. Furthermore, we formulate and solve another optimization problem to minimize a measure of passenger discomfort while the vehicle turns at the intersection and we investigate the associated tradeoff between minimizing fuel consumption and passenger discomfort.
Classical product code constructions for quantum Calderbank-Shor-Steane codes<|sep|>Several notions of code products are known in quantum error correction, such as hyper-graph products, homological products, lifted products, balanced products, to name a few. In this paper we introduce a new product code construction which is a natural generalisation of classical product codes to quantum codes: starting from a set of component Calderbank-Shor-Steane (CSS) codes, a larger CSS code is obtained where both $X$ parity checks and $Z$ parity checks are associated to classical product codes. We deduce several properties of product CSS codes from the properties of the component codes, including bounds on the code distance, and show that built-in redundancies in the parity checks result in so-called meta-checks which can be exploited to correct syndrome read-out errors. We then specialise to the case of single-parity-check (SPC) product codes which in the classical domain are a common choice for constructing product codes. Logical error rate simulations of a SPC $3$-fold product CSS code having parameters $[[512,174,8]]$ are shown under both a maximum likelihood decoder for the erasure channel and belief propagation decoding for depolarising noise. We compare the results with other codes of comparable block length and rate, including a code from the family of asymptotically good quantum Tanner codes. We observe that our reference product CSS code outperforms all other examined codes.
Magnetic relaxation phenomena in Cu$_2$OSeO$_3$ and phase diagram<|sep|>We present an investigation of the magnetic field-temperature phase diagram of Cu$_2$OSeO$_3$ based on DC magnetisation and AC susceptibility measurements covering a broad frequency range of four orders of magnitude, from very low frequencies reaching 0.1 Hz up to 1 kHz. The experiments were performed in the vicinity of $T_C=58.2$ K and around the skyrmion lattice A-phase. At the borders between the different phases the characteristic relaxation times reach several milliseconds and the relaxation is non-exponential. Consequently the borders between the different phases depend on the specific criteria and frequency used and an unambiguous determination is not possible.
Multilingual Translation with Extensible Multilingual Pretraining and Finetuning<|sep|>Recent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext. In this work, we show that multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one direction, a pretrained model is finetuned on many directions at the same time. Compared to multilingual models trained from scratch, starting from pretrained models incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is not available. We demonstrate that pretrained models can be extended to incorporate additional languages without loss of performance. We double the number of languages in mBART to support multilingual machine translation models of 50 languages. Finally, we create the ML50 benchmark, covering low, mid, and high resource languages, to facilitate reproducible research by standardizing training and evaluation data. On ML50, we demonstrate that multilingual finetuning improves on average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while improving 9.3 BLEU on average over bilingual baselines from scratch.
Light charged Higgs boson scenario in 3HDMs<|sep|>The measurement of the $B\to X_s\gamma$ process gives important constraints on physics related to charged Higgs bosons ($H^\pm$). In 2-Higgs Doublet Models (2HDMs) with a softly-broken $Z_2$ symmetry, a light $H^\pm$ scenario, in which $H^\pm$ can be produced via the top decay, is possible in two of four types of Yukawa interactions (the so-called Type-I and Type-X). In these types of 2HDMs, the $H^\pm \to \tau^\pm \nu$ decay mode is dominant in wide regions of the parameter space. In this report, we discuss the other possibility of a light charged Higgs boson scenario in 3-Higgs-Doublet Models (3HDMs) based on the results obtained in Ref. [1]. We show that charged Higgs bosons can mainly decay into $cb$ without contradiction with the $B\to X_s\gamma$ data and the direct searches for charged Higgs bosons at the LHC, and this scenario cannot be realized in the 2HDMs.
Discovery of ultra-fast outflows in a sample of Broad Line Radio Galaxies observed with Suzaku<|sep|>We present the results of a uniform and systematic search for blue-shifted Fe K absorption lines in the X-ray spectra of five bright Broad-Line Radio Galaxies (BLRGs) observed with Suzaku. We detect, for the first time at X-rays in radio-loud AGN, several absorption lines at energies greater than 7 keV in three out of five sources, namely 3C 111, 3C 120 and 3C 390.3. The lines are detected with high significance according to both the F-test and extensive Monte Carlo simulations. Their likely interpretation as blue-shifted Fe XXV and Fe XXVI K-shell resonance lines implies an origin from highly ionized gas outflowing with mildly relativistic velocities, in the range 0.04-0.15c. A fit with specific photo-ionization models gives ionization parameters in the range log_xi~4-5.6 and column densities of N_H~10^22-10^23 cm^-2. These characteristics are very similar to those of the Ultra-Fast Outflows (UFOs) previously observed in radio-quiet AGN. Their estimated location within ~0.01-0.3pc from the central super-massive black hole suggests a likely origin related with accretion disk winds/outflows. Depending on the absorber covering fraction, the mass outflow rate of these UFOs can be comparable to the accretion rate and their kinetic power can correspond to a significant fraction of the bolometric luminosity and is comparable to their typical jet power. Therefore, these UFOs can play a significant role in the expected feedback from the AGN on the surrounding environment and can give us further clues on the relation between the accretion disk and the formation of winds/jets in both radio-quiet and radio-loud AGN.
Global stability analysis of the axisymmetric wake past a spinning bullet-shaped body<|sep|>We analyze the global linear stability of the axisymmetric flow around a spinning bullet-shaped body as a function of the Reynolds number, $Re=w_{\infty}D/\nu$, and of the rotation parameter $\Omega=\omega D/(2 w_{\infty})$, in the ranges $Re<450$ and $0\leq\Omega\leq 1$. Here, $w_{\infty}$ and $\omega$ are the free-stream and the body rotation velocities respectively, and $\nu$ is the fluid kinematic viscosity. The spectrum and the eigenfunctions obtained allow us to explain the different bifurcations from the axisymmetric state observed in previous numerical studies. Our results reveal that three global eigenmodes, denoted Low-Frequency (LF), Medium-Frequency (MF) and High-Frequency (HF) modes, become unstable in different regions of the $Re-\Omega$ parameter plane. We provide precise computations of the corresponding neutral curves, that divide the $Re-\Omega$ plane into four different regions: the stable axisymmetric flow prevails for small enough values of $Re$ and $\Omega$, while three different frozen states, where the wake structures co-rotate with the body at different angular velocities, take place as a consequence of the destabilization of the LF, MF and HF modes. Several direct numerical simulations of the nonlinear state associated to the MF mode, identified here for the first time, are also reported to complement the linear stability results. Finally, we point out the important fact that, since the axisymmetric base flow is $SO(2)$-symmetric, the theory of equivariant bifurcations implies that the weakly non-linear regimes that emerge close to criticality must necessarily take the form of rotating-wave states. These states, previously referred to as frozen wakes in the literature, are thus shown to result from the base-flow symmetry.
Large-Margin Determinantal Point Processes<|sep|>Determinantal point processes (DPPs) offer a powerful approach to modeling diversity in many applications where the goal is to select a diverse subset. We study the problem of learning the parameters (the kernel matrix) of a DPP from labeled training data. We make two contributions. First, we show how to reparameterize a DPP's kernel matrix with multiple kernel functions, thus enhancing modeling flexibility. Second, we propose a novel parameter estimation technique based on the principle of large margin separation. In contrast to the state-of-the-art method of maximum likelihood estimation, our large-margin loss function explicitly models errors in selecting the target subsets, and it can be customized to trade off different types of errors (precision vs. recall). Extensive empirical studies validate our contributions, including applications on challenging document and video summarization, where flexibility in modeling the kernel matrix and balancing different errors is indispensable.
Achievable Degrees of Freedom on K-user MIMO Multi-way Relay Channel with Common and Private Messages<|sep|>This paper investigates the achievable total degrees of freedom (DoF) of the MIMO multi-way relay channel that consists of K users, where each user is equipped with M antennas, and a decode-and-forward relay equipped with N antennas. In this channel, each user wants to convey K-1 private messages to the other users in addition to a common message to all of them. Due to the absence of direct links between the users, communication occurs through the relay in two phases; a multiple access channel phase (MAC) and a broadcast (BC) phase. We drive cut-set bounds on the total DoF of the network, and show that the network has DoF less than or equal to K min(N,M). Achievability of the upper bound is shown by using signal space alignment for network coding in the MAC phase, and zero-forcing precoding in the BC phase. We show that introducing the common messages besides the private messages leads to achieving higher total DoF than using the private messages only.
Surfactant-induced migration of a spherical drop in Stokes flow<|sep|>In Stokes flows, symmetry considerations dictate that a neutrally-buoyant spherical particle will not migrate laterally with respect to the local flow direction. We show that a loss of symmetry due to flow-induced surfactant redistribution leads to cross-stream drift of a spherical drop in Poiseuille flow. We derive analytical expressions for the migration velocity in the limit of small non-uniformities in the surfactant distribution, corresponding to weak-flow conditions or a high-viscosity drop. The analysis predicts that the direction of migration is always towards the flow centerline.
Buoyancy and Marangoni Effects on Horizontal Ribbon Growth<|sep|>Unsteady simulations of horizontal ribbon growth of silicon were performed that included both Marangoni and buoyancy effects. A chaotic flow was observed dominated by strong Marangoni-driven jets emerging near the local temperature minima on the free surface. This oscillatory flow caused the vertical position of the leading edge of the sheet to fluctuate, resulting in corrugations on the top surface of the ribbon. Additionally, larger amplitude and wavelength nonuniformities appeared on the bottom of the sheet resulting in a sheet with varying thickness. Lastly, the unsteady flow caused temporal variations in growth rate, which when converted to distance using the pull speed, matched the wavelengths observed on the top surface. All three of these phenomena have been observed experimentally: The median of the surface wavelengths and amplitudes decreased with increasing temperature sensitivity of surface tension and had wavelengths on the same order as experiments for a sensitivity corresponding to uncontaminated silicon. Oscillations in growth rate have been observed using passive antimony demarcation and thickness variations have been measured after sheet removal. These results indicate that the chaotic flow makes producing thin uniform sheets using HRG challenging.
Mass formulas for single-charm tetraquarks with Fermi-Breit hyperfine interaction<|sep|>In this paper we present the main results of our investigation of the $cq \bar{q} \bar{q}$ single-charm scalar tetraquarks and their SU(3)$_\mathrm{F}$ representations: $\bar{15}_S$, $\bar{3}_S$, $6_A$ and $\bar{3}_A$. We use the Fermi-Breit interaction Hamiltonian with SU(3) flavor symmetry breaking to determine the masses of the single-charm tetraquarks. We also discuss mass spectra obtained from meson and baryon mass fits. The mass spectra are very similar to those obtained with Glozman-Riska hyperfine interaction, and they indicate that some of the experimentally detected states may have tetraquark nature.
Population study of Galactic supernova remnants at very high $\gamma$-ray energies with H.E.S.S.<|sep|>Shell-type supernova remnants (SNRs) are considered prime candidates for the acceleration of Galactic cosmic rays (CRs) up to the knee of the CR spectrum at $\mathrm{E} \approx \mathrm{3}\times \mathrm{10}^\mathrm{15}$ eV. Our Milky Way galaxy hosts more than 350 SNRs discovered at radio wavelengths and at high energies, of which 220 fall into the H.E.S.S. Galactic Plane Survey (HGPS) region. Of those, only 50 SNRs are coincident with a H.E.S.S source and in 8 cases the very high-energy (VHE) emission is firmly identified as an SNR. The H.E.S.S. GPS provides us with a legacy for SNR population study in VHE $\gamma$-rays and we use this rich data set to extract VHE flux upper limits from all undetected SNRs. Overall, the derived flux upper limits are not in contradiction with the canonical CR paradigm. Assuming this paradigm holds true, we can constrain typical ambient density values around shell-type SNRs to $n\leq 7~\textrm{cm}^\textrm{-3}$ and electron-to-proton energy fractions above 10~TeV to $\epsilon_\textrm{ep} \leq 5\times 10^{-3}$. Furthermore, comparisons of VHE with radio luminosities in non-interacting SNRs reveal a behaviour that is in agreement with the theory of magnetic field amplification at shell-type SNRs.
Modelling uncertainty of the radiation energy emitted by extensive air showers<|sep|>Recently, the energy determination of extensive air showers using radio emission has been shown to be both precise and accurate. In particular, radio detection offers the opportunity for an independent measurement of the absolute energy of cosmic rays, since the radiation energy (the energy radiated in the form of radio signals) can be predicted using first-principle calculations involving no free parameters, and the measurement of radio waves is not subject to any significant absorption or scattering in the atmosphere. Here, we verify the implementation of radiation-energy calculations from microscopic simulation codes by comparing Monte Carlo simulations made with the two codes CoREAS and ZHAireS. To isolate potential differences in the radio-emission calculation from differences in the air-shower simulation, the simulations are performed with equivalent settings, especially the same model for the hadronic interactions and the description of the atmosphere. Comparing a large set of simulations with different primary energies and shower directions we observe differences amounting to a total of only 3.3 %. This corresponds to an uncertainty of only 1.6 % in the determination of the absolute energy scale and thus opens the potential of using the radiation energy as an accurate calibration method for cosmic ray experiments.
On the Sample Size of Random Convex Programs with Structured Dependence on the Uncertainty (Extended Version)<|sep|>The "scenario approach" provides an intuitive method to address chance constrained problems arising in control design for uncertain systems. It addresses these problems by replacing the chance constraint with a finite number of sampled constraints (scenarios). The sample size critically depends on Helly's dimension, a quantity always upper bounded by the number of decision variables. However, this standard bound can lead to computationally expensive programs whose solutions are conservative in terms of cost and violation probability. We derive improved bounds of Helly's dimension for problems where the chance constraint has certain structural properties. The improved bounds lower the number of scenarios required for these problems, leading both to improved objective value and reduced computational complexity. Our results are generally applicable to Randomized Model Predictive Control of chance constrained linear systems with additive uncertainty and affine disturbance feedback. The efficacy of the proposed bound is demonstrated on an inventory management example.
Testing dark matter interactions with CMB spectral distortions<|sep|>Possible interactions of dark matter (DM) with Standard Model (SM) particles can be tested with spectral distortions (SDs) of the cosmic microwave background (CMB). In particular, a non-relativistic DM particle that scatters elastically with photons, electrons or nuclei imprints a negative chemical potential $\mu$ to the CMB spectrum. This article revisits the first study of this effect, with an accurate treatment of heat exchange between DM and SM particles. We show that the instantaneous-decoupling approximation made in the original study systematically and significantly underestimates the amplitude of SDs. As a consequence, we derive tighter upper bounds to the DM-SM elastic-scattering cross section for DM masses $m_\chi \lesssim 0.1$ MeV, from the non-detection of $\mu$-distortions by FIRAS. We also show that a future instrument like PIXIE, sensitive to $|\mu| \sim 10^{-8}$, would be able to probe DM-SM cross sections much smaller than first forecasted, and orders of magnitude below current upper limits from CMB-anisotropy data, up to DM masses of $\sim 1$ GeV. Lastly, we study the sensitivity of SDs to the electric and magnetic dipole moments of the DM. Although SDs can place non-trivial constraints on these models, we find that even future SD experiments are unlikely to improve upon the best current bounds. This article is accompanied by the public code DMDIST, which allows one to compute CMB SDs for generic particle-DM models, specified by their cross sections for elastic scattering with and annihilation into SM particles.
Photometric Properties of Six Local Volume Dwarf Galaxies from Deep Near-Infrared Observations<|sep|>We have obtained deep near-infrared $J$- (1.25 $\mu$m), $H$- (1.65$ \mu$m) and $K_s$-band (2.15 $\mu$m) imaging for a sample of six dwarf galaxies ($M_B\ga-17$ mag) in the Local Volume (LV, $D\la10$ Mpc). The sample consists mainly of early-type dwarf galaxies found in various environments in the LV. Two galaxies (LEDA 166099 and UGCA 200) in the sample are detected in the near-infrared for the first time. The deep near-infrared images allow for a detailed study of the photometric and structural properties of each galaxy. The surface brightness profiles of the galaxies are detected down to the ~$24 mag arcsec^{-2}$ isophote in the $J$- and $H$-bands, and $23 mag arcsec^{-2}$ in the $K_s$-band. The total magnitudes of the galaxies are derived in the three wavelength bands. For the brightest galaxies ($M_B\la-15.5$ mag) in the sample, we find that the Two Micron All Sky Survey (2MASS) underestimates the total magnitudes of these systems by up to $\la0.5$ mag. The radial surface brightness profiles of the galaxies are fitted with an exponential (for those galaxies having a stellar disk) or S\'ersic law to derive the structure of the underlying stellar component. In particular, the effective surface brightness ($\mu_e$) and effective radius ($r_e$) are determined from the analytic fits to the surface brightness profile. The $J$-$K_s$ colours for the galaxies have been measured to explore the luminosity-metallicity relation for early-type dwarfs. In addition, the $B$-$K_s$ colours of the galaxies are used to assess their evolutionary state relative to other galaxy morphologies. The total stellar masses of the dwarf galaxies are derived from the $H$-band photometric measurements. These will later be compared to the dynamical mass estimates for the galaxies to determine their dark matter content.
Bayesian compressed sensing with new sparsity-inducing prior<|sep|>Sparse Bayesian learning (SBL) is a popular approach to sparse signal recovery in compressed sensing (CS). In SBL, the signal sparsity information is exploited by assuming a sparsity-inducing prior for the signal that is then estimated using Bayesian inference. In this paper, a new sparsity-inducing prior is introduced and efficient algorithms are developed for signal recovery. The main algorithm is shown to produce a sparser solution than existing SBL methods while preserving their desirable properties. Numerical simulations with one-dimensional synthetic signals and two-dimensional images verify our analysis and show that for sparse signals the proposed algorithm outperforms its SBL peers in both the signal recovery accuracy and computational speed. Its improved performance is also demonstrated in comparison with other state-of-the-art methods in CS.
Pipeline Reduction of Binary Light Curves from Large-Scale Surveys<|sep|>One of the most important changes in observational astronomy of the 21st Century is a rapid shift from classical object-by-object observations to extensive automatic surveys. As CCD detectors are getting better and their prices are getting lower, more and more small and medium-size observatories are refocusing their attention to detection of stellar variability through systematic sky-scanning missions. This trend is aditionally powered by the success of pioneering surveys such as ASAS, DENIS, OGLE, TASS, their space counterpart Hipparcos and others. Such surveys produce massive amounts of data and it is not at all clear how these data are to be reduced and analysed. This is especially striking in the eclipsing binary (EB) field, where most frequently used tools are optimized for object-by-object analysis. A clear need for thorough, reliable and fully automated approaches to modeling and analysis of EB data is thus obvious. This task is very difficult because of limited data quality, non-uniform phase coverage and solution degeneracy. This paper reviews recent advancements in putting together semi-automatic and fully automatic pipelines for EB data processing. Automatic procedures have already been used to process Hipparcos data, LMC/SMC observations, OGLE and ASAS catalogs etc. We discuss the advantages and shortcomings of these procedures.
On the Behavior of the Distributed Coordination Function of IEEE 802.11 with Multirate Capability under General Transmission Conditions<|sep|>The aim of this paper is threefold. First, it presents a multi-dimensional Markovian state transition model characterizing the behavior of the IEEE 802.11 protocol at the Medium Access Control layer which accounts for packet transmission failures due to channel errors modeling both saturated and non-saturated traffic conditions. Second, it provides a throughput analysis of the IEEE 802.11 protocol at the data link layer in both saturated and non-saturated traffic conditions taking into account the impact of both the physical propagation channel and multirate transmission in Rayleigh fading environment. The general traffic model assumed is M/M/1/K. Finally, it shows that the behavior of the throughput in non-saturated traffic conditions is a linear combination of two system parameters; the payload size and the packet rates, $\lambda^{(s)}$, of each contending station. The validity interval of the proposed model is also derived. Simulation results closely match the theoretical derivations, confirming the effectiveness of the proposed models.
Multi-stage Speaker Extraction with Utterance and Frame-Level Reference Signals<|sep|>Speaker extraction requires a sample speech from the target speaker as the reference. However, enrolling a speaker with a long speech is not practical. We propose a speaker extraction technique, that performs in multiple stages to take full advantage of short reference speech sample. The extracted speech in early stages is used as the reference speech for late stages. For the first time, we use frame-level sequential speech embedding as the reference for target speaker. This is a departure from the traditional utterance-based speaker embedding reference. In addition, a signal fusion scheme is proposed to combine the decoded signals in multiple scales with automatically learned weights. Experiments on WSJ0-2mix and its noisy versions (WHAM! and WHAMR!) show that SpEx++ consistently outperforms other state-of-the-art baselines.
Decoupling Long- and Short-Term Patterns in Spatiotemporal Inference<|sep|>Sensors are the key to sensing the environment and imparting benefits to smart cities in many aspects, such as providing real-time air quality information throughout an urban area. However, a prerequisite is to obtain fine-grained knowledge of the environment. There is a limit to how many sensors can be installed in the physical world due to non-negligible expenses. In this paper, we propose to infer real-time information of any given location in a city based on historical and current observations from the available sensors (termed spatiotemporal inference). Our approach decouples the modeling of short-term and long-term patterns, relying on two major components. Firstly, unlike previous studies that separated the spatial and temporal relation learning, we introduce a joint spatiotemporal graph attention network that learns the short-term dependencies across both the spatial and temporal dimensions. Secondly, we propose an adaptive graph recurrent network with a time skip for capturing long-term patterns. The adaptive adjacency matrices are learned inductively first as the inputs of a recurrent network to learn dynamic dependencies. Experimental results on four public real-world datasets show that our method reduces state-of-the-art baseline mean absolute errors by 5%~12%.
Optimal Participation of Price-Maker Battery Energy Storage Systems (BESSs) in Energy, Reserve and Pay as Performance Regulation Markets<|sep|>Motivated by the need of assessing the optimal allocation of battery energy storage services across various markets and the corresponding impact on market operations, an optimization framework is proposed in this work to coordinate the operation of an independent utility-scale price-maker battery energy storage system (BESS) in the energy, spinning reserve and performance-based regulation markets. The entire problem is formulated as a bi-level optimization process, where the structure of all markets is modeled considering the joint operation limits. The strategic bidding behavior of a price-maker BESS in a pay as performance regulation market is investigated. Additionally, a specific approach is introduced for modeling automatic generation control (AGC) signals in the optimization. Although the formulated problem is non-linear, it is converted to mixed-integer linear programming (MILP) to find the optimum solution. The proposed framework is evaluated using test case scenarios created from real-world market data. Case study results show the impact of BESS's price-making behavior on the joint operation of energy, reserve, and regulation markets.
Cross-correlating 2D and 3D galaxy surveys<|sep|>Galaxy surveys probe both structure formation and the expansion rate, making them promising avenues for understanding the dark universe. Photometric surveys accurately map the 2D distribution of galaxy positions and shapes in a given redshift range, while spectroscopic surveys provide sparser 3D maps of the galaxy distribution. We present a way to analyse overlapping 2D and 3D maps jointly and without loss of information. We represent 3D maps using spherical Fourier-Bessel (sFB) modes, which preserve radial coverage while accounting for the spherical sky geometry, and we decompose 2D maps in a spherical harmonic basis. In these bases, a simple expression exists for the cross-correlation of the two fields. One very powerful application is the ability to simultaneously constrain the redshift distribution of the photometric sample, the sample biases, and cosmological parameters. We use our framework to show that combined analysis of DESI and LSST can improve cosmological constraints by factors of ${\sim}1.2$ to ${\sim}1.8$ on the region where they overlap relative to identically sized disjoint regions. We also show that in the overlap of DES and SDSS-III in Stripe 82, cross-correlating improves photo-$z$ parameter constraints by factors of ${\sim}2$ to ${\sim}12$ over internal photo-$z$ reconstructions.
Deep Generative Model for Periodic Graphs<|sep|>Periodic graphs are graphs consisting of repetitive local structures, such as crystal nets and polygon mesh. Their generative modeling has great potential in real-world applications such as material design and graphics synthesis. Classical models either rely on domain-specific predefined generation principles (e.g., in crystal net design), or follow geometry-based prescribed rules. Recently, deep generative models has shown great promise in automatically generating general graphs. However, their advancement into periodic graphs have not been well explored due to several key challenges in 1) maintaining graph periodicity; 2) disentangling local and global patterns; and 3) efficiency in learning repetitive patterns. To address them, this paper proposes Periodical-Graph Disentangled Variational Auto-encoder (PGD-VAE), a new deep generative models for periodic graphs that can automatically learn, disentangle, and generate local and global graph patterns. Specifically, we develop a new periodic graph encoder consisting of global-pattern encoder and local-pattern encoder that ensures to disentangle the representation into global and local semantics. We then propose a new periodic graph decoder consisting of local structure decoder, neighborhood decoder, and global structure decoder, as well as the assembler of their outputs that guarantees periodicity. Moreover, we design a new model learning objective that helps ensure the invariance of local-semantic representations for the graphs with the same local structure. Comprehensive experimental evaluations have been conducted to demonstrate the effectiveness of the proposed method. The code of proposed PGD-VAE is availabe at https://github.com/shi-yu-wang/PGD-VAE.
Security of decoy-state quantum key distribution with correlated intensity fluctuations<|sep|>One of the most prominent techniques to enhance the performance of practical quantum key distribution (QKD) systems with laser sources is the decoy-state method. Current decoy-state QKD setups operate at GHz repetition rates, a regime where memory effects in the modulators and electronics that control them create correlations between the intensities of the emitted pulses. This translates into information leakage about the selected intensities, which cripples a crucial premise of the decoy-state method, thus invalidating the use of standard security analyses. To overcome this problem, a novel security proof that exploits the Cauchy-Schwarz constraint has been introduced recently. Its main drawback is, however, that the achievable key rate is significantly lower than that of the ideal scenario without intensity correlations. Here, we improve this security proof technique by combining it with a fine-grained decoy-state analysis, which can deliver a tight estimation of the relevant parameters that determine the secret key rate. This results in a notable performance enhancement, being now the attainable distance double than that of previous analyses for certain parameter regimes. Also, we show that when the probability density function of the intensity fluctuations, conditioned on the current and previous intensity choices, is known, our approach provides a key rate very similar to the ideal scenario, which highlights the importance of an accurate experimental characterization of the correlations.
Modeling Musical Context with Word2vec<|sep|>We present a semantic vector space model for capturing complex polyphonic musical context. A word2vec model based on a skip-gram representation with negative sampling was used to model slices of music from a dataset of Beethoven's piano sonatas. A visualization of the reduced vector space using t-distributed stochastic neighbor embedding shows that the resulting embedded vector space captures tonal relationships, even without any explicit information about the musical contents of the slices. Secondly, an excerpt of the Moonlight Sonata from Beethoven was altered by replacing slices based on context similarity. The resulting music shows that the selected slice based on similar word2vec context also has a relatively short tonal distance from the original slice.
Hiding its age: the case for a younger bulge<|sep|>The determination of the age of the bulge has led to two contradictory results. On the one side, the color-magnitude diagrams in different bulge fields seem to indicate a uniformly old ($>$10 Gyr) population. On the other side, individual ages derived from dwarfs observed through microlensing events seem to indicate a large spread, from $\sim$ 2 to $\sim$ 13 Gyr. Because the bulge is now recognised as being mainly a boxy peanut-shaped bar, it is suggested that disk stars are one of its main constituents, and therefore also stars with ages significantly younger than 10 Gyr. Other arguments as well point to the fact that the bulge cannot be exclusively old, and in particular cannot be a burst population, as it is usually expected if the bulge was the fossil remnant of a merger phase in the early Galaxy. In the present study, we show that given the range of metallicities observed in the bulge, a uniformly old population would be reflected into a significant spread in color at the turn-off which is not observed. Inversely, we demonstrate that the correlation between age and metallicity expected to hold for the inner disk would conspire to form a color-magnitude diagram with a remarkably small spread in color, thus mimicking the color-magnitude diagram of a uniformly old population. If stars younger than 10 Gyr are part of the bulge, as must be the case if the bulge has been mainly formed through dynamical instabilities in the disk, then a very small spread at the turn-off is expected, as seen in the observations.
Charge asymmetries of top quarks at hadron colliders revisited<|sep|>A sizeable difference in the differential production cross section of top- compared to antitop-quark production, denoted charge asymmetry, has been observed at the Tevatron. The experimental results seem to exceed the theory predictions based on the Standard Model by a significant amount and have triggered a large number of suggestions for "new physics". In the present paper the Standard Model predictions for Tevatron and LHC experiments are revisited. This includes a reanalysis of electromagnetic as well as weak corrections, leading to a shift of the asymmetry by roughly a factor 1.1 when compared to the results of the first papers on this subject. The impact of cuts on the transverse momentum of the top-antitop system is studied. Restricting the ttbar system to a transverse momentum less than 20 GeV leads to an enhancement of the asymmetries by factors between 1.3 and 1.5, indicating the importance of an improved understanding of the $t\bar t$-momentum distribution. Predictions for similar measurements at the LHC are presented, demonstrating the sensitivity of the large rapidity region both to the Standard Model contribution and effects from "new physics".
Stellar Evolutionary Effects on the Abundances of PAH and SN-Condensed Dust in Galaxies<|sep|>Spectral and photometric observations of nearby galaxies show a correlation between the strength of their mid-IR aromatic features, attributed to PAH molecules, and their metal abundance, leading to a deficiency of these features in low-metallicity galaxies. In this paper, we suggest that the observed correlation represents a trend of PAH abundance with galactic age, reflecting the delayed injection of carbon dust into the ISM by AGB stars in the final post-AGB phase of their evolution. AGB stars are the primary sources of PAHs and carbon dust in galaxies, and recycle their ejecta back to the interstellar medium only after a few hundred million years of evolution on the main sequence. In contrast, more massive stars that explode as Type II supernovae inject their metals and dust almost instantaneously after their formation. We first determined the PAH abundance in galaxies by constructing detailed models of UV-to-radio SED of galaxies that estimate the contribution of dust in PAH-free HII regions, and PAHs and dust from photodissociation regions, to the IR emission. All model components: the galaxies' stellar content, properties of their HII regions, and their ionizing and non-ionizing radiation fields and dust abundances, are constrained by their observed multiwavelength spectrum. After determining the PAH and dust abundances in 35 nearby galaxies using our SED model, we use a chemical evolution model to show that the delayed injection of carbon dust by AGB stars provides a natural explanation to the dependence of the PAH content in galaxies with metallicity. We also show that larger dust particles giving rise to the far-IR emission follow a distinct evolutionary trend closely related to the injection of dust by massive stars into the ISM.
A boundary integral algorithm for the Laplace Dirichlet-Neumann mixed eigenvalue problem<|sep|>We present a novel integral-equation algorithm for evaluation of Zaremba eigenvalues and eigenfunctions}, that is, eigenvalues and eigenfunctions of the Laplace operator with mixed Dirichlet-Neumann boundary conditions; of course, (slight modifications of) our algorithms are also applicable to the pure Dirichlet and Neumann eigenproblems. Expressing the eigenfunctions by means of an ansatz based on the single layer boundary operator, the Zaremba eigenproblem is transformed into a nonlinear equation for the eigenvalue $\mu$. For smooth domains the singular structure at Dirichlet-Neumann junctions is incorporated as part of our corresponding numerical algorithm---which otherwise relies on use of the cosine change of variables, trigonometric polynomials and, to avoid the Gibbs phenomenon that would arise from the solution singularities, the Fourier Continuation method (FC). The resulting numerical algorithm converges with high order accuracy without recourse to use of meshes finer than those resulting from the cosine transformation. For non-smooth (Lipschitz) domains, in turn, an alternative algorithm is presented which achieves high-order accuracy on the basis of graded meshes. In either case, smooth or Lipschitz boundary, eigenvalues are evaluated by searching for zero minimal singular values of a suitably stabilized discrete version of the single layer operator mentioned above. (The stabilization technique is used to enable robust non-local zero searches.) The resulting methods, which are fast and highly accurate for high- and low-frequencies alike, can solve extremely challenging two-dimensional Dirichlet, Neumann and Zaremba eigenproblems with high accuracies in short computing times---enabling, in particular, evaluation of thousands of eigenvalues and corresponding eigenfunctions for a given smooth or non-smooth geometry with nearly full double-precision accuracy
Demonstration of Low Emittance in the Cornell Energy Recovery Linac Injector Prototype<|sep|>We present a detailed study of the six-dimensional phase space of the electron beam produced by the Cornell Energy Recovery Linac Photoinjector, a high-brightness, high repetition rate (1.3 GHz) DC photoemission source designed to drive a hard x-ray energy recovery linac (ERL). A complete simulation model of the injector has been constructed, verified by measurement, and optimized. Both the horizontal and vertical 2D transverse phase spaces, as well as the time-resolved (sliced) horizontal phase space, were simulated and directly measured at the end of the injector for 19 pC and 77 pC bunches at roughly 8 MeV. These bunch charges were chosen because they correspond to 25 mA and 100 mA average current if operating at the full 1.3 GHz repetition rate. The resulting 90% normalized transverse emittances for 19 (77) pC/bunch were 0.23 +/- 0.02 (0.51 +/- 0.04) microns in the horizontal plane, and 0.14 +/- 0.01 (0.29 +/- 0.02) microns in the vertical plane, respectively. These emittances were measured with a corresponding bunch length of 2.1 +/- 0.1 (3.0 +/- 0.2) ps, respectively. In each case the rms momentum spread was determined to be on the order of 1e-3. Excellent overall agreement between measurement and simulation has been demonstrated. Using the emittances and bunch length measured at 19 pC/bunch, we estimate the electron beam quality in a 1.3 GHz, 5 GeV hard x-ray ERL to be at least a factor of 20 times better than that of existing storage rings when the rms energy spread of each device is considered. These results represent a milestone for the field of high-brightness, high-current photoinjectors.
What Makes Agile Software Development Agile?<|sep|>Together with many success stories, promises such as the increase in production speed and the improvement in stakeholders' collaboration have contributed to making agile a transformation in the software industry in which many companies want to take part. However, driven either by a natural and expected evolution or by contextual factors that challenge the adoption of agile methods as prescribed by their creator(s), software processes in practice mutate into hybrids over time. Are these still agile? In this article, we investigate the question: what makes a software development method agile? We present an empirical study grounded in a large-scale international survey that aims to identify software development methods and practices that improve or tame agility. Based on 556 data points, we analyze the perceived degree of agility in the implementation of standard project disciplines and its relation to used development methods and practices. Our findings suggest that only a small number of participants operate their projects in a purely traditional or agile manner (under 15%). That said, most project disciplines and most practices show a clear trend towards increasing degrees of agility. Compared to the methods used to develop software, the selection of practices has a stronger effect on the degree of agility of a given discipline. Finally, there are no methods or practices that explicitly guarantee or prevent agility. We conclude that agility cannot be defined solely at the process level. Additional factors need to be taken into account when trying to implement or improve agility in a software company. Finally, we discuss the field of software process-related research in the light of our findings and present a roadmap for future research.
Decentralized Robust Control for Damping Inter-area Oscillations in Power Systems<|sep|>As power systems become more and more interconnected, the inter-area oscillations has become a serious factor limiting large power transfer among different areas. Underdamped (Undamped) inter-area oscillations may cause system breakup and even lead to large-scale blackout. Traditional damping controllers include Power System Stabilizer (PSS) and Flexible AC Transmission System (FACTS) controller, which adds additional damping to the inter-area oscillation modes by affecting the real power in an indirect manner. However, the effectiveness of these controllers is restricted to the neighborhood of a prescribed set of operating conditions. In this paper, decentralized robust controllers are developed to improve the damping ratios of the inter-area oscillation modes by directly affecting the real power through the turbine governing system. The proposed control strategy requires only local signals and is robust to the variations in operation condition and system topology. The effectiveness of the proposed robust controllers is illustrated by detailed case studies on two different test systems.
On Variants of Facility Location Problem with Outliers<|sep|>In this work, we study the extension of two variants of the facility location problem (FL) to make them robust towards a few distantly located clients. First, $k$-facility location problem ($k$FL), a common generalization of FL and $k$ median problems, is a well studied problem in literature. In the second variant, lower bounded facility location (LBFL), we are given a bound on the minimum number of clients that an opened facility must serve. Lower bounds are required in many applications like profitability in commerce and load balancing in transportation problem. In both the cases, the cost of the solution may be increased grossly by a few distantly located clients, called the outliers. Thus, in this work, we extend $k$FL and LBFL to make them robust towards the outliers. For $k$FL with outliers ($k$FLO) we present the first (constant) factor approximation violating the cardinality requirement by +1. As a by-product, we also obtain the first approximation for FLO based on LP-rounding. For LBFLO, we present a tri-criteria solution with a trade-off between the violations in lower bounds and the number of outliers. With a violation of $1/2$ in lower bounds, we get a violation of $2$ in outliers.
Automated classification of periodic variable stars{Improved methodology for the automated classification of periodic variable stars}<|sep|>We present a novel automated methodology to detect and classify periodic variable stars in a large database of photometric time series. The methods are based on multivariate Bayesian statistics and use a multi-stage approach. We applied our method to the ground-based data of the TrES Lyr1 field, which is also observed by the Kepler satellite, covering ~26000 stars. We found many eclipsing binaries as well as classical non-radial pulsators, such as slowly pulsating B stars, Gamma Doradus, Beta Cephei and Delta Scuti stars. Also a few classical radial pulsators were found.
A Practical Dynamic Programming Approach to Datalog Provenance Computation<|sep|>We establish a translation between a formalism for dynamic programming over hypergraphs and the computation of semiring-based provenance for Datalog programs. The benefit of this translation is a new method for computing provenance for a specific class of semirings. Theoretical and practical optimizations lead to an efficient implementation using \textsc{Souffl\'e}, a state-of-the-art Datalog interpreter. Experimental results on real-world data suggest this approach to be efficient in practical contexts, even competing with our previous dedicated solutions for computing provenance in annotated graph databases. The cost overhead compared to plain Datalog evaluation is fairly moderate in many cases of interest.
The Fine-Tuning Argument<|sep|>Our laws of nature and our cosmos appear to be delicately fine-tuned for life to emerge, in a way that seems hard to attribute to chance. In view of this, some have taken the opportunity to revive the scholastic Argument from Design, whereas others have felt the need to explain this apparent fine-tuning of the clockwork of the Universe by proposing the existence of a `Multiverse'. We analyze this issue from a sober perspective. Having reviewed the literature and having added several observations of our own, we conclude that cosmic fine-tuning supports neither Design nor a Multiverse, since both of these fail at an explanatory level as well as in a more quantitative context of Bayesian confirmation theory (although there might be other reasons to believe in these ideas, to be found in religion and in inflation and/or string theory, respectively). In fact, fine-tuning and Design even seem to be at odds with each other, whereas the inference from fine-tuning to a Multiverse only works if the latter is underwritten by an additional metaphysical hypothesis we consider unwarranted. Instead, we suggest that fine-tuning requires no special explanation at all, since it is not the Universe that is fine-tuned for life, but life that has been fine-tuned to the Universe.
Bounded Synthesis and Reinforcement Learning of Supervisors for Stochastic Discrete Event Systems with LTL Specifications<|sep|>In this paper, we consider supervisory control of stochastic discrete event systems (SDESs) under linear temporal logic specifications. Applying the bounded synthesis, we reduce the supervisor synthesis into a problem of satisfying a safety condition. First, we consider a synthesis problem of a directed controller using the safety condition. We assign a negative reward to the unsafe states and introduce an expected return with a state-dependent discount factor. We compute a winning region and a directed controller with the maximum satisfaction probability using a dynamic programming method, where the expected return is used as a value function. Next, we construct a permissive supervisor via the optimal value function. We show that the supervisor accomplishes the maximum satisfaction probability and maximizes the reachable set within the winning region. Finally, for an unknown SDES, we propose a two-stage model-free reinforcement learning method for efficient learning of the winning region and the directed controllers with the maximum satisfaction probability. We also demonstrate the effectiveness of the proposed method by simulation.
2D Gauge Field Theory<|sep|>We show from the action integral that under the assumption of longitudinal dominance and transverse confinement, QCD4 in (3+1) dimensional space-time can be approximately compactified into QCD2 in (1+1) dimensional space-time. In such a process, we find the relation between the coupling constant g(2D) in QCD2 and the coupling constant $g(4D)$ in QCD4. We also show that quarks and gluons in QCD2 acquire masses as a result of the compactification.
Gender Differences in Undergraduate Physics Courses: A Comparative Study of Persistence<|sep|>We have investigated the difference in persistence between male and female students while taking undergraduatephysics courses. To quantify the persistence of a certain group of students, we have defined 'persistence index' as the inverseof the decrease rate of the number of that group of students while taking a specific course. We have collected the data fromthree consecutive workshops on various topics of physics. After plotting the number of participations against the number ofdays attended, we have calculated the decrease rates and persistence indices for both male and female student groups on eachworkshop and compared the persistence indices on a bar diagram. The comparative statistics show that the persistence indicesof female student groups are significantly higher than that of male student groups. This leads us to the conclusion that thefemale students are more persistent than male students while taking an undergraduate physics course.
A PSF-based approach to Kepler/K2 data. II. Exoplanet candidates in Praesepe (M 44)<|sep|>In this work we keep pushing K2 data to a high photometric precision, close to that of the Kepler main mission, using a PSF-based, neighbour-subtraction technique, which also overcome the dilution effects in crowded environments. We analyse the open cluster M 44 (NGC 2632), observed during the K2 Campaign 5, and extract light curves of stars imaged on module 14, where most of the cluster lies. We present two candidate exoplanets hosted by cluster members and five by field stars. As a by-product of our investigation, we find 1680 eclipsing binaries and variable stars, 1071 of which are new discoveries. Among them, we report the presence of a heartbeat binary star. Together with this work, we release to the community a catalogue with the variable stars and the candidate exoplanets found, as well as all our raw and detrended light curves.
Constraining the geometry and kinematics of the quasar broad emission line region using gravitational microlensing. I. Models and simulations<|sep|>Recent studies have shown that line profile distortions are commonly observed in gravitationally lensed quasar spectra. We investigate the effect of gravitational microlensing on quasar broad emission line profiles and their underlying continuum, combining the emission from simple representative BLR models with generic microlensing magnification maps. Specifically, we considered Keplerian disk, polar, and equatorial wind BLR models of various sizes. The effect of microlensing has been quantified with four observables: $\mu^{BLR}$, the total magnification of the broad emission line; $\mu^{cont}$, the magnification of the underlying continuum; as well as red/blue, RBI and wings/core, WCI, indices that characterize the line profile distortions. The simulations showed that distortions of line profiles, such as those recently observed in lensed quasars, can indeed be reproduced and attributed to the differential effect of microlensing on spatially separated regions of the BLR. While the magnification of the emission line $\mu^{BLR}$ sets an upper limit on the BLR size and, similarly, the magnification of the continuum $\mu^{cont}$ sets an upper limit on the size of the continuum source, the line profile distortions mainly depend on the BLR geometry and kinematics. We thus built (WCI,RBI) diagrams that can serve as diagnostic diagrams to discriminate between the various BLR models on the basis of quantitative measurements. It appears that a strong microlensing effect puts important constraints on the size of the BLR and on its distance to the high-magnification caustic. In that case, BLR models with different geometries and kinematics are more prone to produce distinctive line profile distortions for a limited number of caustic configurations, which facilitates their discrimination.
Sonic booms and diffusion wakes generated by a heavy quark in thermal AdS/CFT<|sep|>We evaluate the Poynting vector generated by a heavy quark moving through a thermal state of N=4 gauge theory using AdS/CFT. A significant diffusion wake is observed as well as a Mach cone. We discuss the ratio of the energy going into sound modes to the energy coming in from the wake.
Summary Workbench: Unifying Application and Evaluation of Text Summarization Models<|sep|>This paper presents Summary Workbench, a new tool for developing and evaluating text summarization models. New models and evaluation measures can be easily integrated as Docker-based plugins, allowing to examine the quality of their summaries against any input and to evaluate them using various evaluation measures. Visual analyses combining multiple measures provide insights into the models' strengths and weaknesses. The tool is hosted at \url{https://tldr.demo.webis.de} and also supports local deployment for private resources.
On Characterizing the Data Access Complexity of Programs<|sep|>Technology trends will cause data movement to account for the majority of energy expenditure and execution time on emerging computers. Therefore, computational complexity will no longer be a sufficient metric for comparing algorithms, and a fundamental characterization of data access complexity will be increasingly important. The problem of developing lower bounds for data access complexity has been modeled using the formalism of Hong & Kung's red/blue pebble game for computational directed acyclic graphs (CDAGs). However, previously developed approaches to lower bounds analysis for the red/blue pebble game are very limited in effectiveness when applied to CDAGs of real programs, with computations comprised of multiple sub-computations with differing DAG structure. We address this problem by developing an approach for effectively composing lower bounds based on graph decomposition. We also develop a static analysis algorithm to derive the asymptotic data-access lower bounds of programs, as a function of the problem size and cache size.
DRAT-based Bit-Vector Proofs in CVC4<|sep|>Many state-of-the-art Satisfiability Modulo Theories (SMT) solvers for the theory of fixed-size bit-vectors employ an approach called bit-blasting, where a given formula is translated into a Boolean satisfiability (SAT) problem and delegated to a SAT solver. Consequently, producing bit-vector proofs in an SMT solver requires incorporating SAT proofs into its proof infrastructure. In this paper, we describe three approaches for integrating DRAT proofs generated by an off-the-shelf SAT solver into the proof infrastructure of the SMT solver CVC4 and explore their strengths and weaknesses. We implemented all three approaches using cryptominisat as the SAT back-end for its bit-blasting engine and evaluated performance in terms of proof-production and proof-checking.
NEPTUNE: Network- and GPU-aware Management of Serverless Functions at the Edge<|sep|>Nowadays a wide range of applications is constrained by low-latency requirements that cloud infrastructures cannot meet. Multi-access Edge Computing (MEC) has been proposed as the reference architecture for executing applications closer to users and reduce latency, but new challenges arise: edge nodes are resource-constrained, the workload can vary significantly since users are nomadic, and task complexity is increasing (e.g., machine learning inference). To overcome these problems, the paper presents NEPTUNE, a serverless-based framework for managing complex MEC solutions. NEPTUNE i) places functions on edge nodes according to user locations, ii) avoids the saturation of single nodes, iii) exploits GPUs when available, and iv) allocates resources (CPU cores) dynamically to meet foreseen execution times. A prototype, built on top of K3S, was used to evaluate NEPTUNE on a set of experiments that demonstrate a significant reduction in terms of response time, network overhead, and resource consumption compared to three state-of-the-art approaches.
An SDN hybrid architecture for vehicular networks: Application to Intelligent Transport System<|sep|>Vehicular networks are one of the cornerstone of an Intelligent Transportation System (ITS). They are expected to provide ubiquitous network connectivity to moving vehicles while supporting various ITS services, some with very stringent requirements in terms of latency and reliability. Two vehicular networking technologies are envisioned to jointly support the full range of ITS services : DSRC (Dedicated Short Range Communication) for direct vehicle to vehicle/Road Side Units (RSU) communications and cellular technologies. To the best of our knowledge, approaches from the literature usually divide ITS services on each of these networks according to their requirements and one single network is in charge of supporting the each service. Those that consider both network technologies to offer multi-path routing, load balancing or path splitting for a better quality of experience of ITS services assume obviously separately controlled networks. Under the umbrella of SDN (Software Defined Networking), we propose in this paper a hybrid network architecture that enables the joint control of the networks providing connectivity to multi-homed vehicles and, also, explore the opportunities brought by such an architecture. We show through some use cases, that in addition to the flexibility and fine-grained programmability brought by SDN, it opens the way towards the development of effective network control algorithms that are the key towards the successful support of ITS services and especially those with stringent QoS. We also show how these algorithms could also benefit from information related to the environment or context in which vehicles evolve (traffic density, planned trajectory, ..), which could be easily collected by data providers and made available via the cloud.
Hint at an axion-like particle from the redshift dependence of blazar spectra<|sep|>We consider the largest observed sample including all intermediate-frequency peaked (IBL) and high-frequency peaked (HBL) flaring blazars above 100 GeV up to redshift $z = 0.6$. We show that the best-fit regression line of the emitted spectral indices $\Gamma_{\rm em} (z)$ is a concave parabola decreasing as $z$ increases, thereby implying a statistical correlation between the $\{\Gamma_{\rm em} (z) \}$ distribution and $z$. This result contradicts our expectation that such a distribution should be $z$-independent. We argue that the above correlation does not arise from any selection bias. We show that our expectation naturally emerges provided that axion-like particles (ALPs) are put into the game. Moreover, ALPs can also explain why flat spectrum radio quasars emit up to 400 GeV, in sharp contradiction with conventional physics. So, the combination of the two very different but consistent results -- taken at face value -- leads to a hint at an ALP with mass $m = {\cal O} (10^{-10} \, {\rm eV})$ and two-photon coupling in the range $2.94 \times 10^{- 12} \, {\rm GeV}^{- 1} < g_{a \gamma \gamma} < 0.66 \times 10^{- 10} \, {\rm GeV}^{- 1}$. As a bonus, the Universe would become considerably more transparent above energies $E \gtrsim 1 \, {\rm TeV}$ than dictated by conventional physics. Our prediction can be checked not only by the new generation of observatories like CTA, HAWC, GAMMA-400, LHAASO, TAIGA-HiSCORE and HERD, but also thanks to the planned laboratory experiments ALPS II (upgraded), STAX, IAXO and with other techniques now being developed by Avignone and collaborators.
Uncovering the Temporal Dynamics of Diffusion Networks<|sep|>Time plays an essential role in the diffusion of information, influence and disease over networks. In many cases we only observe when a node copies information, makes a decision or becomes infected -- but the connectivity, transmission rates between nodes and transmission sources are unknown. Inferring the underlying dynamics is of outstanding interest since it enables forecasting, influencing and retarding infections, broadly construed. To this end, we model diffusion processes as discrete networks of continuous temporal processes occurring at different rates. Given cascade data -- observed infection times of nodes -- we infer the edges of the global diffusion network and estimate the transmission rates of each edge that best explain the observed data. The optimization problem is convex. The model naturally (without heuristics) imposes sparse solutions and requires no parameter tuning. The problem decouples into a collection of independent smaller problems, thus scaling easily to networks on the order of hundreds of thousands of nodes. Experiments on real and synthetic data show that our algorithm both recovers the edges of diffusion networks and accurately estimates their transmission rates from cascade data.
Successors of Singular Cardinals and Coloring Theorems II<|sep|>We investigate negative square-brackets partition relations at successors of singular cardinals of countable cofinality. Along the way we prove some club-guessing results.
INGESTBASE: A Declarative Data Ingestion System<|sep|>Big data applications have fast arriving data that must be quickly ingested. At the same time, they have specific needs to preprocess and transform the data before it could be put to use. The current practice is to do these preparatory transformations once the data is already ingested, however, this is expensive to run and cumbersome to manage. As a result, there is a need to push data preprocessing down to the ingestion itself. In this paper, we present a declarative data ingestion system, called INGESTBASE, to allow application developers to plan and specify their data ingestion logic in a more systematic manner. We introduce the notion of ingestions plans, analogous to query plans, and present a declarative ingestion language to help developers easily build sophisticated ingestion plans. INGESTBASE provides an extensible ingestion optimizer to rewrite and optimize ingestion plans by applying rules such as operator reordering and pipelining. Finally, the INGESTBASE runtime engine runs the optimized ingestion plan in a distributed and fault-tolerant manner. Later, at query processing time, INGESTBASE supports ingestion-aware data access and interfaces with upstream query processors, such as Hadoop MapReduce and Spark, to post- process the ingested data. We demonstrate through a number of experiments that INGESTBASE: (i) is flexible enough to express a variety of ingestion techniques, (ii) incurs a low ingestion overhead, (iii) provides efficient access to the ingested data, and (iv) has much better performance, up to 6 times, than preparing data as an afterthought, via a query processor.
Learning an Integrated Distance Metric for Comparing Structure of Complex Networks<|sep|>Graph comparison plays a major role in many network applications. We often need a similarity metric for comparing networks according to their structural properties. Various network features - such as degree distribution and clustering coefficient - provide measurements for comparing networks from different points of view, but a global and integrated distance metric is still missing. In this paper, we employ distance metric learning algorithms in order to construct an integrated distance metric for comparing structural properties of complex networks. According to natural witnesses of network similarities (such as network categories) the distance metric is learned by the means of a dataset of some labeled real networks. For evaluating our proposed method which is called NetDistance, we applied it as the distance metric in K-nearest-neighbors classification. Empirical results show that NetDistance outperforms previous methods, at least 20 percent, with respect to precision.
Experimental and First principle calculation of Co_xNi_{(1-x)}Si solid solution structural stability<|sep|>We report the investigation of the structural stability of Co$_{(1-x)}$Ni$_x$Si monosilicides for $0<x<1$. As CoSi crystallizes in the FeSi-type structure (B20) and NiSi is stable in the MnP-type structure (B31), a complete set of samples has been synthesized and a systematic study of phase formation under different annealing conditions were carried out in order to understand the reason of such a structural transition when x goes from 0 to 1. This study has revealed a limit in the solubility of Ni in CoSi B20 structure of about 17.5 at.% and of Co in NiSi B31 phase of about 13 at.%. For $0.35<x<0.74$ both B20 and B31 phases are present in the sample at there respective limits of solubility. The temperature dependence of the magnetic susceptibility has also been measured revealing diamagnetic behaviors. Optimal structural parameters and phase stability of the solid solution have been investigated using self-consistent full-potential linearized augmented plane wave method (FP-LAPW) based on the density functional theory (DFT). This calculation well predicts the structural instability observed experimentally.
Generalized Slow Roll in the Unified Effective Field Theory of Inflation<|sep|>We provide a compact and unified treatment of power spectrum observables for the effective field theory (EFT) of inflation with the complete set of operators that lead to second-order equations of motion in metric perturbations in both space and time derivatives, including Horndeski and GLPV theories. We relate the EFT operators in ADM form to the four additional free functions of time in the scalar and tensor equations. Using the generalized slow roll formalism, we show that each power spectrum can be described by an integral over a single source that is a function of its respective sound horizon. With this correspondence, existing model independent constraints on the source function can be simply reinterpreted in the more general inflationary context. By expanding these sources around an optimized freeze-out epoch, we also provide characterizations of these spectra in terms of five slow-roll hierarchies whose leading order forms are compact and accurate as long as EFT coefficients vary only on timescales greater than an efold. We also clarify the relationship between the unitary gauge observables employed in the EFT and the comoving gauge observables of the post-inflationary universe.
M82 - A radio continuum and polarisation study I. Data reduction and cosmic ray propagation<|sep|>The potential role of magnetic fields and cosmic ray propagation for feedback processes in the early Universe can be probed by studies of local starburst counterparts with an equivalent star-formation rate. Archival data from the WSRT was reduced and a new calibration technique introduced to reach the high dynamic ranges needed for the complex source morphology of M82. This data was combined with archival VLA data, yielding total power maps at 3cm, 6cm, 22cm and 92cm. The data shows a confinement of the emission at wavelengths of 3/6cm to the core region and a largely extended halo reaching up to 4kpc away from the galaxy midplane at wavelengths of 22/92cm up to a sensitivity limit of 90muJy and 1.8mJy respectively. The results are used to calculate the magnetic field strength in the core region to 98muG and to 24muG in the halo regions. From the observation of free-free losses the filling factor of the ionised medium could be estimated to 2%. We find that the radio emission from the core region is dominated by very dense HII-regions and supernova remnants, while the surrounding medium is filled with hot X-ray and neutral gas. Cosmic rays radiating at frequencies higher than 1.4 GHz are suffering from high synchrotron and inverse Compton losses in the core region and are not able to reach the halo. Even the cosmic rays radiating at longer wavelengths are only able to build up the observed kpc sized halo, when several starbursting periods are assumed where the photon field density varies by an order of magnitude. These findings together with the strong correlation between Halpha, PAH+, and our radio continuum data suggests a magnetic field which is frozen into the ionised medium and driven out of the galaxy kinematically.
DIAPHANE: a Portable Radiation Transport Library for Astrophysical Applications<|sep|>One of the most computationally demanding aspects of the hydrodynamical modelling of Astrophysical phenomena is the transport of energy by radiation or relativistic particles. Physical processes involving energy transport are ubiquitous and of capital importance in many scenarios ranging from planet formation to cosmic structure evolution, including explosive events like core collapse supernova or gamma-ray bursts. Moreover, the ability to model and hence understand these processes has often been limited by the approximations and incompleteness in the treatment of radiation and relativistic particles. The DIAPHANE project has focused in developing a portable and scalable library that handles the transport of radiation and particles (in particular neutrinos) independently of the underlying hydrodynamic code. In this work, we present the computational framework and the functionalities of the first version of the DIAPHANE library, which has been successfully ported to three different smoothed-particle hydrodynamic codes, GADGET2, GASOLINE and SPHYNX. We also present validation of different modules solving the equations of radiation and neutrino transport using different numerical schemes.
Distance approximation using Isolation Forests<|sep|>This work briefly explores the possibility of approximating spatial distance (alternatively, similarity) between data points using the Isolation Forest method envisioned for outlier detection. The logic is similar to that of isolation: the more similar or closer two points are, the more random splits it will take to separate them. The separation depth between two points can be standardized in the same way as the isolation depth, transforming it into a distance metric that is limited in range, centered, and in compliance with the axioms of distance. This metric presents some desirable properties such as being invariant to the scales of variables or being able to account for non-linear relationships between variables, which other metrics such as Euclidean or Mahalanobis distance do not. Extensions to the Isolation Forest method are also proposed for handling categorical variables and missing values, resulting in a more generalizable and robust metric.
The Lyman alpha morphology of local starburst galaxies: release of calibrated images<|sep|>We present reduced and calibrated high resolution Lyman-alpha (Lya) images for a sample of 6 local star forming galaxies. Targets were selected to represent a range in luminosity and metallicity and to include both known Lya emitters and non-emitters. Far ultraviolet imaging was carried out with the Solar Blind Channel of the ACS on HST in the F122M (Lya on-line) and F140LP (continuum) filters. The resulting Lya images are the product of careful modeling of both the stellar and nebular continua, facilitated by supporting HST imaging in Ha and 5 continuum bands, combined with Starburst99 evolutionary synthesis models, and prescriptions for dust extinction on the continuum. In all, the resulting morphologies in Lya, Ha, and UV-continuum are qualitatively very different and we show that the bulk of Lya emerges in a diffuse component resulting from resonant scattering events. Lya escape fractions, computed from integrated Ha luminosities and recombination theory, are found never to exceed 14%. Internal dust extinction is estimated in each pixel and used to correct Lya fluxes. However, the extinction corrections are far too small (factors from 2.6 to infinity) to reconcile the global Lya luminosities with recombination theory. Surprisingly, when comparing the global equivalent widths of Lya and Ha, the two quantities appear anti-correlated, which may be due to the evolution of mechanical feedback. This calls for caution in the interpretation of Lya observations. The images presented have a physical resolution 3 orders of magnitude better than attainable at high-z from the ground with current instrumentation and our images may therefore serve as useful templates for comparing with observations and modeling of primeval galaxy formation. We therefore provide the reduced Lya, Ha, and continuum images to the community.
Secular resonance of inner test particles in hierarchical planetary systems<|sep|>The present work studies the secular resonance associated with the critical argument $\sigma = \varpi$ ($\varpi$ is the longitude of pericentre) for inner test particles moving in low-eccentricity region with inclination $i$ smaller than $39^{\circ}$. To formulate the dynamical model, the double-averaged Hamiltonian is formulated up to an arbitrary order in the semimajor axis ratio, and then those high-order periodic terms are removed from the double-averaged Hamiltonian by means of Hori--Deprit transformation technique. The resulting Hamiltonian determines a resonant model with a single degree of freedom. Based on the resonant model, it becomes possible to explore the phase-space structure, resonant centre, and resonant width in an analytical manner. In particular, an excellent correspondence is found between the resonant width in terms of the eccentricity variation and the maximum variation of eccentricity ($\Delta e$) for test particles initially placed on quasi-circular orbits. It means that the secular dynamics in the low-eccentricity space with $i < 39^{\circ}$ is dominantly governed by the secular resonance associated with $\sigma = \varpi$.
Sky reconstruction from transit visibilities: PAON-4 and Tianlai Dish Array<|sep|>The spherical harmonics $m$-mode decomposition is a powerful sky map reconstruction method suitable for radio interferometers operating in transit mode. It can be applied to various configurations, including dish arrays and cylinders. We describe the computation of the instrument response function, the point spread function (PSF), transfer function, the noise covariance matrix and noise power spectrum. The analysis in this paper is focused on dish arrays operating in transit mode. We show that arrays with regular spacing have more pronounced side lobes as well as structures in their noise power spectrum, compared to arrays with irregular spacing, specially in the north-south direction. A good knowledge of the noise power spectrum $C^{\mathrm{noise}}(\ell)$ is essential for intensity mapping experiments as non uniform $C^{\mathrm{noise}}(\ell)$ is a potential problem for the measurement of the HI power spectrum. Different configurations have been studied to optimise the PAON-4 and Tianlai dish array layouts. We present their expected performance and their sensitivities to the 21-cm emission of the Milky Way and local extragalactic HI clumps
Fairness in Multiterminal Data Compression: A Splitting Method for The Egalitarian Solution<|sep|>This paper proposes a novel splitting (SPLIT) algorithm to achieve fairness in the multiterminal lossless data compression problem. It finds the egalitarian solution in the Slepian-Wolf region and completes in strongly polynomial time. We show that the SPLIT algorithm adaptively updates the source coding rates to the optimal solution, while recursively splitting the terminal set, enabling parallel and distributed computation. The result of an experiment demonstrates a significant reduction in computation time by the parallel implementation when the number of terminals becomes large. The achieved egalitarian solution is also shown to be superior to the Shapley value in distributed networks, e.g., wireless sensor networks, in that it best balances the nodes' energy consumption and is far less computationally complex to obtain.
From the Integer to the Fractional Quantum Hall Effect in Graphene<|sep|>The fractional quantum Hall effect is a very particular manifestation of electronic correlations in two-dimensional systems in a strong perpendicular magnetic field. It arises as a consequence of a strong Coulomb repulsion between electrons in the same Landau level that conspires with a particular chirality of the electronic states. This chirality is inherited from the classical cyclotron motion, i.e. a particular sense of electronic rotation due to the orientation of the magnetic field. The specificity of the FQHE in graphene consists of a four-fold spin-valley degeneracy inherited from the electronic bands in the vicinity of the Fermi level. The relevant Coulomb interaction respects this SU(4) symmetry, and one is therefore confronted with a generic four-component fractional quantum Hall effect, as well as with other correlated four-component phases, such as spin-valley ferromagnetic states. The present article aims at a -- mainly theoretical -- discussion of these exotic phases, in comparison with experimental evidence for them.
Controlling Epidemic Spread using Probabilistic Diffusion Models on Networks<|sep|>The spread of an epidemic is often modeled by an SIR random process on a social network graph. The MinINF problem for optimal social distancing involves minimizing the expected number of infections, when we are allowed to break at most $B$ edges; similarly the MinINFNode problem involves removing at most $B$ vertices. These are fundamental problems in epidemiology and network science. While a number of heuristics have been considered, the complexity of these problems remains generally open. In this paper, we present two bicriteria approximation algorithms for MinINF, which give the first non-trivial approximations for this problem. The first is based on the cut sparsification result of Karger \cite{karger:mathor99}, and works when the transmission probabilities are not too small. The second is a Sample Average Approximation (SAA) based algorithm, which we analyze for the Chung-Lu random graph model. We also extend some of our results to tackle the MinINFNode problem.
In Situ Cane Toad Recognition<|sep|>Cane toads are invasive, toxic to native predators, compete with native insectivores, and have a devastating impact on Australian ecosystems, prompting the Australian government to list toads as a key threatening process under the Environment Protection and Biodiversity Conservation Act 1999. Mechanical cane toad traps could be made more native-fauna friendly if they could distinguish invasive cane toads from native species. Here we designed and trained a Convolution Neural Network (CNN) starting from the Xception CNN. The XToadGmp toad-recognition CNN we developed was trained end-to-end using heat-map Gaussian targets. After training, XToadGmp required minimum image pre/post-processing and when tested on 720x1280 shaped images, it achieved 97.1% classification accuracy on 1863 toad and 2892 not-toad test images, which were not used in training.
Finite top-mass effects in gluon-induced Higgs production with a jet-veto at NNLO<|sep|>Effects from a finite top quark mass on the H+n-jet cross section through gluon fusion are studied for $n=0/n\ge 1$ at NNLO/NLO QCD. For this purpose, sub-leading terms in $1/m_t$ are calculated. We show that the asymptotic expansion of the jet-vetoed cross section at NNLO is very well behaved and that the heavy-top approximation is valid at the five permille level up to jet-veto cuts of 300 GeV. For the inclusive Higgs+jet rate, we introduce a matching procedure that allows for a reliable prediction of the top-mass effects using the expansion in $1/m_t$. The quality of the effective field theory to evaluate differential K-factors for the distribution of the hardest jet is found to be better than 1-2% as long as the transverse momentum of the jet is integrated out or remains below about 150 GeV.
Layer Adaptive Node Selection in Bayesian Neural Networks: Statistical Guarantees and Implementation Details<|sep|>Sparse deep neural networks have proven to be efficient for predictive model building in large-scale studies. Although several works have studied theoretical and numerical properties of sparse neural architectures, they have primarily focused on the edge selection. Sparsity through edge selection might be intuitively appealing; however, it does not necessarily reduce the structural complexity of a network. Instead pruning excessive nodes leads to a structurally sparse network with significant computational speedup during inference. To this end, we propose a Bayesian sparse solution using spike-and-slab Gaussian priors to allow for automatic node selection during training. The use of spike-and-slab prior alleviates the need of an ad-hoc thresholding rule for pruning. In addition, we adopt a variational Bayes approach to circumvent the computational challenges of traditional Markov Chain Monte Carlo (MCMC) implementation. In the context of node selection, we establish the fundamental result of variational posterior consistency together with the characterization of prior parameters. In contrast to the previous works, our theoretical development relaxes the assumptions of the equal number of nodes and uniform bounds on all network weights, thereby accommodating sparse networks with layer-dependent node structures or coefficient bounds. With a layer-wise characterization of prior inclusion probabilities, we discuss the optimal contraction rates of the variational posterior. We empirically demonstrate that our proposed approach outperforms the edge selection method in computational complexity with similar or better predictive performance. Our experimental evidence further substantiates that our theoretical work facilitates layer-wise optimal node recovery.
Python script used as simulator for the teaching of electric field in electromagnetism course<|sep|>We present this work like software tool developed in Python, based on a methodology to obtain the electric field produced by n charges. The tool was developed and implemented in courses of electromagnetism and laboratory in three institutions of higher education. The aim for this work is to incorporate information and communication technologies (ICTs) at the university, in accordance with the programs promoted by the Colombian Ministry of Education. We wanted to connect the students with sensitives experiences of the physical phenomena that allow them to improve their experience of learning of subjects traditionally studied through the board course.
A Statistical Survey of Peculiar L and T Dwarfs in SDSS, 2MASS, and WISE<|sep|>We present the final results from a targeted search for brown dwarfs with unusual near-infrared colors. From a positional cross-match of SDSS, 2MASS and WISE, we have identified 144 candidate peculiar L and T dwarfs. Spectroscopy confirms that 20 of the objects are peculiar or are candidate binaries. Nine of the 420 objects in our sample are young ($\lesssim$200 Myr; 2.1%) and another 8 (1.9%) are unusually red with no signatures of youth. With a spectroscopic $J-K_s$ color of 2.58 $\pm$ 0.11 mag, one of the new objects, the L6 dwarf 2MASS J03530419+0418193, is among the reddest field dwarfs currently known and is one of the reddest objects with no signatures of youth known to date. We have also discovered another potentially very low gravity object, the L1 dwarf 2MASS J00133470+1109403, and independently identified the young L7 dwarf 2MASS J00440332+0228112, first reported by Schneider and collaborators. Our results confirm that signatures of low gravity are no longer discernible in low to moderate resolution spectra of objects older than $\sim$200 Myr. The 1.9% of unusually red L dwarfs that do not show other signatures of youth could be slightly older, up to $\sim$400 Myr. In this case a red $J-K_s$ color may be more diagnostic of moderate youth than individual spectral features. However, its is also possible that these objects are relatively metal-rich, and so have an enhanced atmospheric dust content.
SIM: A Slot-Independent Neural Model for Dialogue State Tracking<|sep|>Dialogue state tracking is an important component in task-oriented dialogue systems to identify users' goals and requests as a dialogue proceeds. However, as most previous models are dependent on dialogue slots, the model complexity soars when the number of slots increases. In this paper, we put forward a slot-independent neural model (SIM) to track dialogue states while keeping the model complexity invariant to the number of dialogue slots. The model utilizes attention mechanisms between user utterance and system actions. SIM achieves state-of-the-art results on WoZ and DSTC2 tasks, with only 20% of the model size of previous models.
Mean-free path effects in magnetoresistance of ferromagnetic nanocontacts<|sep|>We investigated the mean-free path effects on the magnetoresistance of ferromagnetic nanocontacts. For most combinations of parameters the magnetoresistance monotonously decreases with increasing the contact cross-section. However, for a certain choice of parameters the calculations show non-monotonous behavior of the magnetoresistance in the region in which the diameter of the contact becomes comparable with the mean-free path of electrons. We attribute this effect to different conduction regimes in the vicinity of the nanocontact: ballistic for electrons of one spin projection, and simultaneously diffusive for the other. Furthermore, at certain combinations of spin asymmetries of the bulk mean-free paths in a heterocontact, the magnetoresistance can be almost constant, or may even grow as the contact diameter increases. Thus, our calculations suggest a way to search for combinations of material parameters, for which high magnetoresistances can be achieved not only at the nanometric size of the contact, but also at much larger cross-sections of nanocontacts which can be easier for fabrication with current technologies. The trial calculations of the magnetoresistance with material parameters close to those for the Mumetal-Ni heterocontacts agree satisfactorily with the available experimental data.
On the resurgent structure of quantum periods<|sep|>Quantum periods appear in many contexts, from quantum mechanics to local mirror symmetry. They can be described in terms of topological string free energies and Wilson loops, in the so-called Nekrasov-Shatashvili limit. We consider the trans-series extension of the holomorphic anomaly equations satisfied by these quantities, and we obtain exact multi-instanton solutions for these trans-series. Building on this result, we propose a unified perspective on the resurgent structure of quantum periods. We show for example that the Delabaere-Pham formula, which was originally obtained in quantum mechanical examples, is a generic feature of quantum periods. We illustrate our general results with explicit calculations for the double-well in quantum mechanics, and for the quantum mirror curve of local $\mathbb{P}^2$.
Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model<|sep|>With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.
Scaling and crossover behaviour in a truncated long range quantum walk<|sep|>We consider a discrete time quantum walker in one dimension, where at each step, the step length $\ell$ is chosen from a distribution $P(\ell) \propto \ell^{-\delta -1}$ with $\ell \leq \ell_{max}$. We evaluate the probability $f(x,t)$ that the walker is at position $x$ at time $t$ and its first two moments. As expected, the disorder effectively localizes the walk even for large values of $\delta$. Asymptotically, $\langle x^2 \rangle \propto t^{3/2}$ and $\langle x \rangle \propto t^{1/2}$ independent of $\delta$ and $\ell$, both finite. The scaled distribution $f(x,t)t^{1/2}$ plotted versus $x/t^{1/2}$ shows a data collapse for $x/t < \alpha(\delta,\ell_{max}) \sim \mathcal O(1) $ indicating the existence of a universal scaling function. The scaling function is shown to have a crossover behaviour at $\delta = \delta^* \approx 4.0$ beyond which the results are independent of $\ell_{max}$. We also calculate the von Neumann entropy of entanglement which gives a larger asymptotic value compared to the quantum walk with unique step length even for large $\delta$, with negligible dependence on the initial condition.
Tunable single photon emission from dipolaritons<|sep|>We study a system comprising of a double quantum well embedded in a micropillar optical cavity, where strong coupling between a direct exciton, indirect exciton, and cavity photon is achieved. We show that the resulting hybrid quasiparticles - dipolaritons - can induce strong photon correlations and lead to anti-bunched behaviour of the cavity output field. The origin of the observed single photon emission is attributed to unconventional photon blockade. Moreover, we find that the second-order equal time correlation function $g^{(2)}(0)$ can be tuned over a large range using an electric field applied to the structure, or changing the frequency of the pump. This allows for an on-the-flight control of cavity output properties, and is important for the future generation of tunable single photon emission sources.
Precise radial velocities of giant stars IX. HD 59686 Ab: a massive circumstellar planet orbiting a giant star in a ~13.6 au eccentric binary system<|sep|>Context: For over 12 yr, we have carried out a precise radial velocity survey of a sample of 373 G and K giant stars using the Hamilton \'Echelle Spectrograph at Lick Observatory. There are, among others, a number of multiple planetary systems in our sample as well as several planetary candidates in stellar binaries. Aims: We aim at detecting and characterizing substellar+stellar companions to the giant star HD 59686 A (HR 2877, HIP 36616). Methods: We obtained high precision radial velocity (RV) measurements of the star HD 59686 A. By fitting a Keplerian model to the periodic changes in the RVs, we can assess the nature of companions in the system. In order to discriminate between RV variations due to non-radial pulsation or stellar spots we used infrared RVs taken with the CRIRES spectrograph at the Very Large Telescope. Additionally, to further characterize the system, we obtain high-resolution images with LMIRCam at the Large Binocular Telescope. Results: We report the likely discovery of a giant planet with a mass of $m_{p}~\sin i=6.92_{-0.24}^{+0.18}~M_{Jup}$ orbiting at $a_{p}=1.0860_{-0.0007}^{+0.0006}$ au from the giant star HD 59686 A. Besides the planetary signal, we discover an eccentric ($e_{B}=0.729_{-0.003}^{+0.004}$) binary companion with a mass of $m_{B}~\sin i=0.5296_{-0.0008}^{+0.0011}~M_{Sun}$ orbiting at a semi-major axis of just $a_{B}=13.56_{-0.14}^{+0.18}$ au. Conclusions: The existence of the planet HD 59686 Ab in a tight eccentric binary system severely challenges standard giant planet formation theories and requires substantial improvements to such theories in tight binaries. Otherwise, alternative planet formation scenarios such as second generation planets or dynamical interactions in an early phase of the system's lifetime should be seriously considered in order to better understand the origin of this enigmatic planet.
Structure Based Extended Resolution for Constraint Programming<|sep|>Nogood learning is a powerful approach to reducing search in Constraint Programming (CP) solvers. The current state of the art, called Lazy Clause Generation (LCG), uses resolution to derive nogoods expressing the reasons for each search failure. Such nogoods can prune other parts of the search tree, producing exponential speedups on a wide variety of problems. Nogood learning solvers can be seen as resolution proof systems. The stronger the proof system, the faster it can solve a CP problem. It has recently been shown that the proof system used in LCG is at least as strong as general resolution. However, stronger proof systems such as \emph{extended resolution} exist. Extended resolution allows for literals expressing arbitrary logical concepts over existing variables to be introduced and can allow exponentially smaller proofs than general resolution. The primary problem in using extended resolution is to figure out exactly which literals are useful to introduce. In this paper, we show that we can use the structural information contained in a CP model in order to introduce useful literals, and that this can translate into significant speedups on a range of problems.
Continual BatchNorm Adaptation (CBNA) for Semantic Segmentation<|sep|>Environment perception in autonomous driving vehicles often heavily relies on deep neural networks (DNNs), which are subject to domain shifts, leading to a significantly decreased performance during DNN deployment. Usually, this problem is addressed by unsupervised domain adaptation (UDA) approaches trained either simultaneously on source and target domain datasets or even source-free only on target data in an offline fashion. In this work, we further expand a source-free UDA approach to a continual and therefore online-capable UDA on a single-image basis for semantic segmentation. Accordingly, our method only requires the pre-trained model from the supplier (trained in the source domain) and the current (unlabeled target domain) camera image. Our method Continual BatchNorm Adaptation (CBNA) modifies the source domain statistics in the batch normalization layers, using target domain images in an unsupervised fashion, which yields consistent performance improvements during inference. Thereby, in contrast to existing works, our approach can be applied to improve a DNN continuously on a single-image basis during deployment without access to source data, without algorithmic delay, and nearly without computational overhead. We show the consistent effectiveness of our method across a wide variety of source/target domain settings for semantic segmentation. Code is available at https://github.com/ifnspaml/CBNA.
Curvature Perturbations and Anomaly explain Dark Energy<|sep|>We investigate the history of dark energy to explain the present magnitude. We assume the dark energy is the residual cosmological constant. The most important channel in the reheating process is the gluon pair productions by QCD trace anomaly. We argue dark energy decays rapidly by gluon pair emissions during the reheating and after the big bang. The reheating temperature is determined by the decay width of dark energy Gamma and the Planck mass M_p as sqrt{M_P Gamma} ~ 10^6GeV. It is the consequence of Friedmann's equation and an equilibrium condition Gamma~ H. As the Universe cools below the hadronic scale, dark energy density is almost frozen. Nevertheless the dark energy further decreases by emitting two photons. We have estimated the current decay rate of dark energy from the QED trace anomaly. The consistent solution of Friedmann equation is in an excellent agreement with the observations. The suppression factor of dark energy scale is the product of fine structure constant alpha and curvature perturbation P as 10^{-30}=(\alpha^2P/4\pi)^2. We argue the conformal symmetry breaking in the both UV and IR are necessary unless dark energy is subtracted. We also investigated lepto-genesis by adding massive right handed neutrinos. The realistic lepto-genesis takes place during reheating process.
Luminous X-ray AGN in Clusters of Galaxies<|sep|>We present a study of X-ray AGN overdensities in 16 Abell clusters, within the redshift range 0.073<z<0.279, in order to investigate the effect of the hot inter-cluster environment on the triggering of the AGN phenomenon. The X-ray AGN overdensities, with respect to the field expectations, were estimated for sources with L_x>= 10^{42} erg s^{-1} (at the redshift of the clusters) and within an area of 1 h^{-1}_{72} Mpc radius (excluding the core). To investigate the presence or not of a true enhancement of luminous X-ray AGN in the cluster area, we also derived the corresponding optical galaxy overdensities, using a suitable range of $r$-band magnitudes. We always find the latter to be significantly higher (and only in two cases roughly equal) with respect to the corresponding X-ray overdensities. Over the whole cluster sample, the mean X-ray point-source overdensity is a factor of ~4 less than that corresponding to bright optical galaxies, a difference which is significant at a >0.995 level, as indicated by an appropriate t-student test. We conclude that the triggering of luminous X-ray AGN in rich clusters is strongly suppressed. Furthermore, searching for optical SDSS counterparts of all the X-ray sources, associated with our clusters, we found that about half appear to be background QSOs, while others are background and foreground AGN or stars. The true overdensity of X-ray point sources, associated to the clusters, is therefore even smaller than what our statistical approach revealed.
Inverse-square law violation and reactor antineutrino anomaly<|sep|>We discuss a possibility that the so-called reactor antineutrino anomaly can be, at least in part, explained by applying a quantum field-theoretical approach to neutrino oscillations, which in particular predicts a small deviation from the classical inverse-square law at short but macroscopic distances between the neutrino source and detector. An extensive statistical analysis of the reactor data is performed to examine this speculation.
Fast and Accurate Camera Covariance Computation for Large 3D Reconstruction<|sep|>Estimating uncertainty of camera parameters computed in Structure from Motion (SfM) is an important tool for evaluating the quality of the reconstruction and guiding the reconstruction process. Yet, the quality of the estimated parameters of large reconstructions has been rarely evaluated due to the computational challenges. We present a new algorithm which employs the sparsity of the uncertainty propagation and speeds the computation up about ten times \wrt previous approaches. Our computation is accurate and does not use any approximations. We can compute uncertainties of thousands of cameras in tens of seconds on a standard PC. We also demonstrate that our approach can be effectively used for reconstructions of any size by applying it to smaller sub-reconstructions.
Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform<|sep|>We propose a lightweight end-to-end text-to-speech model using multi-band generation and inverse short-time Fourier transform. Our model is based on VITS, a high-quality end-to-end text-to-speech model, but adopts two changes for more efficient inference: 1) the most computationally expensive component is partially replaced with a simple inverse short-time Fourier transform, and 2) multi-band generation, with fixed or trainable synthesis filters, is used to generate waveforms. Unlike conventional lightweight models, which employ optimization or knowledge distillation separately to train two cascaded components, our method enjoys the full benefits of end-to-end optimization. Experimental results show that our model synthesized speech as natural as that synthesized by VITS, while achieving a real-time factor of 0.066 on an Intel Core i7 CPU, 4.1 times faster than VITS. Moreover, a smaller version of the model significantly outperformed a lightweight baseline model with respect to both naturalness and inference speed. Code and audio samples are available from https://github.com/MasayaKawamura/MB-iSTFT-VITS.
A rain induced landslide 3D model based on molecular dynamics with fractal and fractional water diffusion<|sep|>We present a three-dimensional model, based on cohesive spherical particles, of rain-induced landslides. The rainwater infiltration into the soil follow the either the fractional or the fractal diffusion equations. We solve analytically the fractal diffusion partial differential equation (PDE) with particular boundary conditions to simulate a rainfall event. Then, for the PDE, we developed a numerical integration scheme that we integrate with MD (Molecular Dynamics) algorithm for the triggering and propagation of the simulated landslide. Therefore we test the numerical integration scheme of fractal diffusion equation with the analytical solution. We adopt the fractal diffusion equation in term of gravimetric water content that we use as input of triggering scheme based on Mohr-Coulomb limit-equilibrium criterion, adapted to particle level. Moreover, taking into account an interacting force Lennard-Jones inspired, we use a standard MD algorithm to update particle positions and velocities. Then we present results for homogeneous and heterogeneous systems (i.e. composed by particles with same or different radius respectively). Interestingly, in the heterogeneous case, we observe segregation effects due to the different volume of the particles. Finally we show the parameter sensibility analysis both for triggering and propagation phase. Our simulations confirm the results of our previous two-dimensional model and therefore the feasible applicability to real cases.
Darker than Black-Box: Face Reconstruction from Similarity Queries<|sep|>Several methods for inversion of face recognition models were recently presented, attempting to reconstruct a face from deep templates. Although some of these approaches work in a black-box setup using only face embeddings, usually, on the end-user side, only similarity scores are provided. Therefore, these algorithms are inapplicable in such scenarios. We propose a novel approach that allows reconstructing the face querying only similarity scores of the black-box model. While our algorithm operates in a more general setup, experiments show that it is query efficient and outperforms the existing methods.
Time evolution of high-energy emissions of low-mass stars:I. Age determination using stellar chronology with white dwarfs in wide binaries<|sep|>Stellar ages are extremely difficult to determine and often subject to large uncertainties, especially for field low-mass stars. We plan to carry out a calibration of the decrease in high-energy emissions of low-mass GKM stars with time, and therefore precise age determination is a key ingredient. The overall goal of our research is to study the time evolution of these high-energy emissions as an essential input to studing exoplanetary atmospheres. We propose to determine stellar ages with a methodology based on wide binaries. We are interested in systems composed of a low-mass star and a white dwarf (WD), where the latter serves as a stellar chronometer for the system. We aim at obtaining reliable ages for a sample of late-type stars older than 1 Gyr. We selected a sample of wide binaries composed by a DA type WD and a GKM companion. High signal-to-noise, low-resolution spectroscopic observations were obtained for most of the WD members of the sample. Atmospheric parameters were determined by fitting the spectroscopic data to appropiate WD models. The total ages of the systems were derived by using cooling sequences, an initial-final mass relationship and evolutionary tracks, to account for the progenitor life. The spectroscopic observations have allowed us to determine ages for the binary systems using WDs as cosmochronometers. We obtained reliable ages for 27 stars between 1 and 5 Gyr, which is a range where age determination becomes difficult for field objects.
Computational study of boron nitride nanotube synthesis: how catalyst morphology stabilizes the boron nitride bond<|sep|>In an attempt to understand why catalytic methods for the growth of boron nitride nanotubes work much worse than for their carbon counterparts, we use first-principles calculations to study the energetics of elemental reactions forming N2, B2 and BN molecules on an iron catalyst. We observe that in the case of these small molecules, the catalytic activity is hindered by the formation of B2 on the iron surface. We also observe that the local morphology of a step edge present in our nanoparticle model stabilizes the boron nitride molecule with respect to B2 due to the ability of the step edge to offer sites with different coordination simultaneously for nitrogen and boron. Our results emphasize the importance of atomic steps for a high yield chemical vapor deposition growth of BN nanotubes and may outline new directions for improving the efficiency of the method.
Estimating initial conditions for dynamical systems with incomplete information<|sep|>In this paper we study the problem of inferring the initial conditions of a dynamical system under incomplete information. Studying several model systems, we infer the latent microstates that best reproduce an observed time series when the observations are sparse,noisy and aggregated under a (possibly) nonlinear observation operator. This is done by minimizing the least-squares distance between the observed time series and a model-simulated time series using gradient-based methods. We validate this method for the Lorenz and Mackey-Glass systems by making out-of-sample predictions. Finally, we analyze the predicting power of our method as a function of the number of observations available. We find a critical transition for the Mackey-Glass system, beyond which it can be initialized with arbitrary precision.
I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image<|sep|>Most of the previous image-based 3D human pose and mesh estimation methods estimate parameters of the human mesh model from an input image. However, directly regressing the parameters from the input image is a highly non-linear mapping because it breaks the spatial relationship between pixels in the input image. In addition, it cannot model the prediction uncertainty, which can make training harder. To resolve the above issues, we propose I2L-MeshNet, an image-to-lixel (line+pixel) prediction network. The proposed I2L-MeshNet predicts the per-lixel likelihood on 1D heatmaps for each mesh vertex coordinate instead of directly regressing the parameters. Our lixel-based 1D heatmap preserves the spatial relationship in the input image and models the prediction uncertainty. We demonstrate the benefit of the image-to-lixel prediction and show that the proposed I2L-MeshNet outperforms previous methods. The code is publicly available https://github.com/mks0601/I2L-MeshNet_RELEASE.
Bias voltage Control of Avalanche Photo-Diode Using a Window Comparator<|sep|>This work aims at controlling the bias voltage of APDs, used for single photon detection, with a micro-controller through pulse height comparison.
Graph-Based Manifold Frequency Analysis for Denoising<|sep|>We propose a new framework for manifold denoising based on processing in the graph Fourier frequency domain, derived from the spectral decomposition of the discrete graph Laplacian. Our approach uses the Spectral Graph Wavelet transform in order to per- form non-iterative denoising directly in the graph frequency domain, an approach inspired by conventional wavelet-based signal denoising methods. We theoretically justify our approach, based on the fact that for smooth manifolds the coordinate information energy is localized in the low spectral graph wavelet sub-bands, while the noise affects all frequency bands in a similar way. Experimental results show that our proposed manifold frequency denoising (MFD) approach significantly outperforms the state of the art denoising meth- ods, and is robust to a wide range of parameter selections, e.g., the choice of k nearest neighbor connectivity of the graph.
Event shape engineering for inclusive spectra and elliptic flow in Pb-Pb collisions at $\sqrt{s_\rm{NN}}=2.76$ TeV<|sep|>We report on results obtained with the Event Shape Engineering technique applied to Pb-Pb collisions at $\sqrt{s_\rm{NN}}=2.76$ TeV. By selecting events in the same centrality interval, but with very different average flow, different initial state conditions can be studied. We find the effect of the event-shape selection on the elliptic flow coefficient $v_2$ to be almost independent of transverse momentum $p_\rm{T}$, as expected if this effect is due to fluctuations in the initial geometry of the system. Charged hadron, pion, kaon, and proton transverse momentum distributions are found to be harder in events with higher-than-average elliptic flow, indicating an interplay between radial and elliptic flow.
Face Expression Recognition and Analysis: The State of the Art<|sep|>The automatic recognition of facial expressions has been an active research topic since the early nineties. There have been several advances in the past few years in terms of face detection and tracking, feature extraction mechanisms and the techniques used for expression classification. This paper surveys some of the published work since 2001 till date. The paper presents a time-line view of the advances made in this field, the applications of automatic face expression recognizers, the characteristics of an ideal system, the databases that have been used and the advances made in terms of their standardization and a detailed summary of the state of the art. The paper also discusses facial parameterization using FACS Action Units (AUs) and MPEG-4 Facial Animation Parameters (FAPs) and the recent advances in face detection, tracking and feature extraction methods. Notes have also been presented on emotions, expressions and facial features, discussion on the six prototypic expressions and the recent studies on expression classifiers. The paper ends with a note on the challenges and the future work. This paper has been written in a tutorial style with the intention of helping students and researchers who are new to this field.
Realistic 3D hydrodynamics simulations find significant turbulent entrainment in massive stars<|sep|>Our understanding of stellar structure and evolution coming from one-dimensional (1D) stellar models is limited by uncertainties related to multi-dimensional processes taking place in stellar interiors. 1D models, however, can now be tested and improved with the help of detailed three-dimensional (3D) hydrodynamics models, which can reproduce complex multi-dimensional processes over short timescales, thanks to the recent advances in computing resources. Among these processes, turbulent entrainment leading to mixing across convective boundaries is one of the least understood and most impactful. Here we present the results from a set of hydrodynamics simulations of the neon-burning shell in a massive star, and interpret them in the framework of the turbulent entrainment law from geophysics. Our simulations differ from previous studies in their unprecedented degree of realism in reproducing the stellar environment. Importantly, the strong entrainment found in the simulations highlights the major flaws of the current implementation of convective boundary mixing in 1D stellar models. This study therefore calls for major revisions of how convective boundaries are modelled in 1D, and in particular the implementation of entrainment in these models. This will have important implications for supernova theory, nucleosynthesis, neutron stars and black holes physics.
Constraining Planetary Migration and Tidal Dissipation with Coeval Hot Jupiters<|sep|>We investigate the constraints on the formation of, and tidal dissipation processes in, hot Jupiters (HJs) that can be inferred based on reliable knowledge of the age of a system or population. Particular attention is paid to the role of young systems (such as those in open clusters or star-forming regions) in such studies. For an ensemble of coeval HJ (or proto-HJ) systems, we quantify the effect of age on the distribution of orbital eccentricities with respect to orbital periods as well as the location of the observed "pile-up" feature. We expect the effects of pre-main-sequence stellar evolution to be important only if a substantial fraction of HJs approach their current orbits early in protostellar contraction (ages less than 10 Myr). Application to the HJs presently known in the cluster M67 yields constraints on the dissipation roughly consistent with those gleaned from planets in the field; for those in the Hyades and Praesepe, our results suggest a higher degree of dissipation at early times than that inferred from other populations.
Deep Learning in Beyond 5G Networks with Image-based Time-Series Representation<|sep|>Towards the network innovation, the Beyond Five-Generation (B5G) networks envision the use of machine learning (ML) methods to predict the network conditions and performance indicators in order to best make decisions and allocate resources. In this paper, we propose a new ML approach to accomplish predictions in B5G networks. Instead of handling the time-series in the network domain of values, we transform them into image thus allowing to apply advanced ML methods of Computer Vision field to reach better predictions in B5G networks. Particularly, we analyze different techniques to transform time-series of network measures into image representation, e.g., Recurrence Plots, Markov Transition Fields, and Gramian Angular Fields. Then, we apply deep neural networks with convolutional layers to predict different 5G radio signal quality indicators. When comparing with other ML-based solutions, experimental results from 5G transmission datasets showed the feasibility and small prediction errors of the proposed approach.
Energy-Delay Tradeoffs in a Load-Balanced Router<|sep|>The Load-Balanced Router architecture has received a lot of attention because it does not require centralized scheduling at the internal switch fabrics. In this paper we reexamine the architecture, motivated by its potential to turn off multiple components and thereby conserve energy in the presence of low traffic. We perform a detailed analysis of the queue and delay performance of a Load-Balanced Router under a simple random routing algorithm. We calculate probabilistic bounds for queue size and delay, and show that the probabilities drop exponentially with increasing queue size or delay. We also demonstrate a tradeoff in energy consumption against the queue and delay performance.
On the ratio of $t\bar{t}\gamma$ and $t\bar{t}$ cross sections at the LHC<|sep|>We study the ratio of the cross sections for $t\bar{t}\gamma$ and $t\bar{t}$ production at the LHC. The presence of correlations between theoretical uncertainties of the two processes makes possible a precise determination of this observable. This can help to evidentiate effects of new physics that might reveal themselves only when sufficiently precise theoretical predictions are available. Our analysis is based on fully realistic simulations of $t\bar{t}\gamma$ and $t\bar{t}$ production in the dilepton decay channel, including complete off-shell and non-resonant effects at NLO QCD accuracy. We discuss Standard Model predictions for the LHC Run II at both inclusive and differential level, also quantifying the impact of the theoretical uncertainties related to variation of scales and parton distribution functions.
Theoretical Aspects of Cosmic Acceleration<|sep|>Efforts to understand and map the possible explanations for the late time acceleration of the universe have led to a broad range of suggestions, ranging from the cosmological constant and straightforward dark energy, to exotically coupled models, to infrared modifications of General Relativity. If we are to uncover which, if any, of these approaches might provide a serious answer to the problem, it is crucial to understand the constraints that theoretical consistency places on the models, and on the regimes in which they make predictions. In this talk, delivered as an invited plenary lecture at the Dark Side of the Universe conference in Kyoto, Japan, I briefly describe some modern attempts to carry out this program and some of the more interesting ideas that have emerged. As an example, I use the Galileon model, discussing how the Vainshtein mechanism occurs, and how a number of these theoretical problems arise around such backgrounds.
A Decentralized IoT Data Marketplace<|sep|>This paper proposes an architecture for dynamic decentralized marketplace for trading of Internet of Things data. To this end, we introduce a 3-tier framework which consists of provider, consumer and broker. The framework is realized using multiple trustless broker which matches and selects potential data provider based on the consumers requirements. Rather than using a centralized server to manage the contract between provider and consumer, the framework leverages smart contract-based agreement for automatically enforcing the terms of the contract to the involved parties.
On polymorphic logical gates in sub-excitable chemical medium<|sep|>In a sub-excitable light-sensitive Belousov-Zhabotinsky chemical medium an asymmetric disturbance causes the formation of localized traveling wave-fragments. Under the right conditions these wave-fragment can conserve their shape and velocity vectors for extended time periods. The size and life span of a fragment depend on the illumination level of the medium. When two or more wave-fragments collide they annihilate or merge into a new wave-fragment. In computer simulations based on the Oregonator model we demonstrate that the outcomes of inter-fragment collisions can be controlled by varying the illumination level applied to the medium. We interpret these wave-fragments as values of Boolean variables and design collision-based polymorphic logical gates. The gate implements operation XNOR for low illumination, and it acts as NOR gate for high illumination. As a NOR gate is a universal gate then we are able to demonstrate that a simulated light sensitive BZ medium exhibits computational universality.
Streaming instabilities in accreting and magnetized laminar protoplanetary disks<|sep|>The streaming instability is one of the most promising pathways to the formation of planetesimals from pebbles. Understanding how this instability operates under realistic conditions expected in protoplanetary disks is therefore crucial to assess the efficiency of planet formation. Contemporary models of protoplanetary disks show that magnetic fields are key to driving gas accretion through large-scale, laminar magnetic stresses. However, the effect of such magnetic fields on the streaming instability has not been examined in detail. To this end, we study the stability of dusty, magnetized gas in a protoplanetary disk. We find the streaming instability can be enhanced by passive magnetic torques and even persist in the absence of a global radial pressure gradient. In this case, instability is attributed to the azimuthal drift between dust and gas, unlike the classical streaming instability, which is driven by radial drift. This suggests that the streaming instability can remain effective inside dust-trapping pressure bumps in accreting disks. When a live vertical field is considered, we find the magneto-rotational instability can be damped by dust feedback, while the classic streaming instability can be stabilized by magnetic perturbations. We also find that Alfv\'en waves can be destabilized by dust-gas drift, but this instability requires nearly ideal conditions. We discuss the possible implications of these results for dust dynamics and planetesimal formation in protoplanetary disks.
Probing exotic Higgs sectors from the precise measurement of Higgs boson couplings<|sep|>We study coupling constants of the standard model like Higgs boson with the gauge bosons $hZZ$ and $hWW$ and fermions $hf\bar{f}$ in the general Higgs sector which contains higher isospin representations with arbitrary hypercharge. In Higgs sectors with exotic Higgs representations, the $hZZ$ and $hWW$ coupling constants can be larger than those in the standard model. We calculate deviations in the Higgs boson couplings from standard model values in the model with a real or complex triplet field, the Georgi-Machacek model and the model with a septet scalar field. We also study deviations in the event rates of $h\to ZZ^*$, $h\to WW^*$, $h\to \gamma\gamma$, $h\to b\bar{b}$ and $h\to \tau\tau$ channels.
Towards exact symplectic integrators from Liouvillian forms<|sep|>In this article we introduce a low order implicit symplectic integrator designed to follow the Hamiltonian flow as close as possible. This integrator is obtained by the method of Liouvillian forms and does not require particular hypotheses on the Hamiltonian. The numerical scheme introduced in this paper is a modification of the symplectic mid-point rule, it is symmetric and it is obtained by an isotopy of the deformation of the exact Hamiltonian flow to the straight line passing by two consecutive points of the discretized flow. This isotopy generates an alternative vector field on the flow lines transversal to the Hamiltonian vector field. We consider only the line arising from the mid-point to construct the symplectic integrator.
Scaling Rrelation in two situations of extreme mergers<|sep|>Clusters of galaxies are known to be dynamically active systems, yet X-ray studies of the low redshift population exhibit tight scaling laws. In this work, we extend previous studies of this apparent paradox using numerical simulations of two extreme merger cases, one is a high Mach number (above 2.5) satellite merger similar to the "bullet cluster" and the other a merger of nearly equal mass progenitors. Creating X-ray images densely sampled in time, we construct TX, Mgas, and YX measures within R500 and compare to the calibrations of Kravtsov et al. (2006). We find that these extreme merger cases respect the scaling relations, for both intrinsic measures and for measures derived from appropriately masked, synthetic Chandra X-ray images. The masking procedure plays a critical role in the X-ray temperature calculation while it is irrelevant in the X-ray gas mass derivation. Mis-centering up to 100 kpc does not influence the result. The observationally determined radius R500 might conduce to systematic shifts in Mgas, and YX which increase the total mass scatter.
A Reversible Data hiding Scheme in Encrypted Domain for Secret Image Sharing based on Chinese Remainder Theorem<|sep|>Reversible data hiding in encrypted domain (RDH-ED) schemes based on symmetric or public key encryption are mainly applied to the security of end-to-end communication. Aimed at providing reliable technical supports for multi-party security scenarios, a separable RDH-ED scheme for secret image sharing based on Chinese remainder theorem (CRT) is presented. In the application of (t, n) secret image sharing, the image is first shared into n different shares of ciphertext. Only when not less than t shares obtained, can the original image be reconstructed. In our scheme, additional data could be embedded into the image shares. To realize data extraction from the image shares and the reconstructed image separably, two data hiding methods are proposed: one is homomorphic difference expansion in encrypted domain (HDE-ED) that supports data extraction from the reconstructed image by utilizing the addition homomorphism of CRT secret sharing; the other is difference expansion in image shares (DE-IS) that supports the data extraction from the marked shares before image reconstruction. Experimental results demonstrate that the proposed scheme could not only maintain the security and the threshold function of secret sharing system, but also obtain a better reversibility and efficiency compared with most existing RDH-ED algorithms. The maximum embedding rate of HDE-ED could reach 0.5000 bits per pixel and the average embedding rate of DE-IS is 0.0545 bits per bit of ciphertext.
The Nature of the Atmosphere of the Transiting Super-Earth GJ 1214b<|sep|>The newly discovered planet GJ 1214b is the first known transiting super-Earth requiring a significant atmosphere to explain its observed mass and radius. Models for the structure of this planet predict that it likely possesses a H-He envelope of at least 0.05% of the total mass of the planet. However, models without a significant H-He atmosphere are not entirely ruled out by the available data. Here we explore a range of possible atmospheres for the planet, ranging from solar composition gas, to pure CO_2 or water (steam). We present transmission and emission spectra for each of these cases. We find that, if GJ 1214b possesses a hydrogen-rich atmosphere as expected, then the primary transit depth for such an atmosphere would vary at a level of up to 0.3% as a function of wavelength, relative to the background light of its M-dwarf host star. Observations at this level of precision are potentially obtainable with current space-based instrumentation. Successful detection of the transmission signature of this planet at the ~0.1% level would therefore provide confirmation of the hydrogen-rich nature of the planetary atmosphere. It follows that transmission spectroscopy at this level of precision could provide a first glimpse into answering the question of whether planets in the super-Earth mass regime (1 - 10 M_Earth) more closely resemble large terrestrial planets or small gas giant planets.
Future weak lensing constraints in a dark coupled universe<|sep|>Coupled cosmologies can predict values for the cosmological parameters at low redshifts which may differ substantially from the parameters values within non-interacting cosmologies. Therefore, low redshift probes, as the growth of structure and the dark matter distribution via galaxy and weak lensing surveys constitute a unique tool to constrain interacting dark sector models. We focus here on weak lensing forecasts from future Euclid and LSST-like surveys combined with the ongoing Planck cosmic microwave background experiment. We find that these future data could constrain the dimensionless coupling to be smaller than a few $\times 10^{-2}$. The coupling parameter $\xi$ is strongly degenerate with the cold dark matter energy density $\Omega_{c}h^2$ and the Hubble constant $H_0$.These degeneracies may cause important biases in the cosmological parameter values if in the universe there exists an interaction among the dark matter and dark energy sectors.
CME Magnetic Structure and IMF Preconditioning Affecting SEP Transport<|sep|>Coronal mass ejections (CMEs) and solar energetic particles (SEPs) are two phenomena that can cause severe space weather effects throughout the heliosphere. The evolution of CMEs, especially in terms of their magnetic structure, and the configuration of the interplanetary magnetic field (IMF) that influences the transport of SEPs are currently areas of active research. These two aspects are not necessarily independent of each other, especially during solar maximum when multiple eruptive events can occur close in time. Accordingly, we present the analysis of a CME that erupted on 2012 May 11 (SOL2012-05-11) and an SEP event following an eruption that took place on 2012 May 17 (SOL2012-05-17). After observing the May 11 CME using remote-sensing data from three viewpoints, we evaluate its propagation through interplanetary space using several models. Then, we analyse in-situ measurements from five predicted impact locations (Venus, Earth, the Spitzer Space Telescope, the Mars Science Laboratory en route to Mars, and Mars) in order to search for CME signatures. We find that all in-situ locations detect signatures of an SEP event, which we trace back to the May 17 eruption. These findings suggest that the May 11 CME provided a direct magnetic connectivity for the efficient transport of SEPs. We discuss the space weather implications of CME evolution, regarding in particular its magnetic structure, and CME-driven IMF preconditioning that facilitates SEP transport. Finally, this work remarks the importance of using data from multiple spacecraft, even those that do not include space weather research as their primary objective.
Constrained Superfields in Dynamical Background<|sep|>We study the nonlinear realization of supersymmetry in a dynamical/cosmological background in which derivative terms like kinetic terms are finite. Starting from linearly realized theories, we integrate out heavy modes without neglecting derivative terms to obtain algebraic constraints on superfields. Thanks to the supersymmetry breaking contribution by the kinetic energy, the validity of constrained superfields can be extended to cosmological regimes and phenomena such as reheating after inflation, kinetic-energy domination, and the kinetic and standard misalignment of axion.
Cyclotron line formation in the magnetized atmospheres of compact stars: I. The transfer equations for polarized radiation<|sep|>We find the forms of the transfer equations for polarized cyclotron radiation in the atmospheres of compact stars, which are simple enough to allow practical implementation and still preserve all important physical effects. We take into account a frequency redistribution of radiation within the cyclotron line as well as the relativistic and quantum-electrodynamic effects. Our analysis is valid for the magnetic fields up to $10^{13}$G and for temperatures well below 500keV.} We present and compare two forms of the radiation transfer equations. The first form, for the intensities of ordinary and extraordinary modes, is applicable for the compact stars with a moderate magnetic field strength up to $10^{11}$G for typical neutron star and up to $10^9$G for magnetic white dwarfs. The second form, for the Stokes parameters, is more complex, but applicable even if a linear mode coupling takes place somewhere in the scattering-dominated atmosphere. Analysing dispersion properties of a magnetized plasma {in the latter case, we describe a range of parameters where the linear mode coupling is possible and essential.
Redirection of a crack driven by viscous fluid taking into account plastic effects in the process zone<|sep|>In this paper the problem of redirection of a crack driven by viscous fluid under mixed mode loading is considered. The loading includes the classical Modes I - III and the hydraulically induced tangential traction on the fracture walls. The effect of the plastic deformation of the near tip zone is accounted for. Different criteria to determine the fracture deflection angle are examined and compared.
A Fully Abstract Symbolic Semantics for Psi-Calculi<|sep|>We present a symbolic transition system and bisimulation equivalence for psi-calculi, and show that it is fully abstract with respect to bisimulation congruence in the non-symbolic semantics. A psi-calculus is an extension of the pi-calculus with nominal data types for data structures and for logical assertions representing facts about data. These can be transmitted between processes and their names can be statically scoped using the standard pi-calculus mechanism to allow for scope migrations. Psi-calculi can be more general than other proposed extensions of the pi-calculus such as the applied pi-calculus, the spi-calculus, the fusion calculus, or the concurrent constraint pi-calculus. Symbolic semantics are necessary for an efficient implementation of the calculus in automated tools exploring state spaces, and the full abstraction property means the semantics of a process does not change from the original.
Scattering of Scalar Field by an Extended Black Hole in F(R) gravity<|sep|>In this work we have studied the scattering of scalar field around an extended black hole in F(R) gravity using WKB method. We have obtained the wave function in different regions such as near the horizon region, away from horizon and far away from horizon and the absorption cross section are calculated. We find that the absorption cross section is inversely proportional to the cube of Hawking temperature. We have also evaluated the Hawking temperature of the black hole via tunneling method.
One Size Does not Fit All: When to Use Signature-based Pruning to Improve Template Matching for RDF graphs<|sep|>Signature-based pruning is broadly accepted as an effective way to improve query performance of graph template matching on general labeled graphs. Most existing techniques which utilize signature-based pruning claim its benefits on all datasets and queries. However, the effectiveness of signature-based pruning varies greatly among different RDF datasets and highly related with their dataset characteristics. We observe that the performance benefits from signature-based pruning depend not only on the size of the RDF graphs, but also the underlying graph structure and the complexity of queries. This motivates us to propose a flexible RDF querying framework, called RDF-h, which selectively utilizes signature-based pruning by evaluating the characteristics of RDF datasets and query templates. Scalability and efficiency of RDF-h is demonstrated in experimental results using both real and synthetic datasets. Keywords: RDF, Graph Template Matching, Signature-based Pruning
Optimal location problems with routing cost<|sep|>In the paper a model problem for the location of a given number $N$ of points in a given region $\Omega$ and with a given resources density $\rho(x)$ is considered. The main difference between the usual location problems and the present one is that in addition to the location cost an extra {\it routing cost} is considered, that takes into account the fact that the resources have to travel between the locations on a point-to-point basis. The limit problem as $N\to\infty$ is characterized and some applications to airfreight systems are shown.
Universality of dark matter haloes shape over six decades in mass: Insights from the Millennium XXL and SBARBINE simulations<|sep|>For the last 30 years many observational and theoretical evidences have shown that galaxy clusters are not spherical objects, and that their shape is much better described by a triaxial geometry. With the advent of multi-wavelength data of increasing quality, triaxial investigations of galaxy clusters is gathering a growing interest from the community, especially in the time of "precision cosmology". In this work, we aim to provide the first statistically significant predictions in the unexplored mass range above 3x10^14 Mo/h, using haloes from two redshifts (z=0 and z=1) of the Millennium XXL simulation. The size of this cosmological dark matter only simulation (4.1 Gpc) allows the formation of a statistically significant number of massive cluster scale haloes (about 500 with M>2x10^15 Mo/h and 780000 with M>10^14 Mo/h). Besides, we aim to extend this investigation to lower masses in order to look for universal predictions across nearly six orders of magnitude in mass, from 10^10 to almost 10^16 Mo/h. For this purpose we use the SBARBINE simulations, allowing to model haloes of masses starting from 10^10 Mo/h. We use an elliptical overdensity method to select haloes and compute the shapes of the unimodal ones (approximately 50%), while we discard the unrelaxed. The minor to major and intermediate to major axis ratio are found to be well described by simple functional forms. For a given mass we can fully characterize the shape of a halo and give predictions about the distribution of axis ratios for a given cosmology and redshift. Moreover, these results are in some disagreement with the findings of Jing & Suto (2002) which are widely used in the community even though they have to be extrapolated far beyond their original mass range. This "recipe" is made available to the community in this paper and in a dedicated web page.
Strong competition between $\Theta_{II}$-loop-current order and $d$-wave charge order along the diagonal direction in a two-dimensional hot spot model<|sep|>We study the fate of the so-called $\Theta_{II}$-loop-current order that breaks both time-reversal and parity symmetries in a two-dimensional hot spot model with antiferromagnetically mediated interactions, using Fermi surfaces relevant to the phenomenology of the cuprate superconductors. We start from a three-band Emery model describing the hopping of holes in the CuO$_{2}$ plane that includes two hopping parameters $t_{pp}$ and $t_{pd}$, local on-site Coulomb interactions $U_{d}$ and $U_{p}$ and nearest-neighbor $V_{pd}$ couplings between the fermions in the copper [Cu$(3d_{x^{2}-y^{2}})$] and oxygen [O$(2p_{x})$ and O$(2p_{y})$] orbitals. By focusing on the lowest-energy band, we proceed to decouple the local interaction $U_{d}$ of the Cu orbital in the spin channel using a Hubbard-Stratonovich transformation to arrive at the interacting part of the so-called spin-fermion model. We also decouple the nearest-neighbor interaction $V_{pd}$ to introduce the order parameter of the $\Theta_{II}$-loop-current order. In this way, we are able to construct a consistent mean-field theory that describes the strong competition between the composite order parameter made of a quadrupole-density-wave and $d$-wave pairing fluctuations proposed in Efetov \emph{et al.} [Nat. Phys. \textbf{9}, 442 (2013)] with the $\Theta_{II}$-loop-current order parameter that is argued to be relevant for explaining important aspects of the physics of the pseudogap phase displayed in the underdoped cuprates.
Automated Smart Wick System-Based Microfarm Using Internet of Things<|sep|>This paper presents a study conducted to allow urban farmers to remotely monitor their farm through the design and development of an Internet of Things-based (IoT) microfarm prototype which utilized wick system as planting method. The system involves the detection of three environmental parameters namely, light intensity, soil moisture and temperature through the use of respective sensors which were connected to the Arduino microcontroller, the sensor node of the system. Irregularities in the aforementioned parameters were neutralized through the use of parameter regulators such as LED growlight strips, water pump and air cooler. The data collected by these sensors were gathered by the Arduino microcontroller and were sent to the Web database through the IoT gateway which was the Raspberry Pi computer chip. These data were also sent to an Android unit installed with the Microfarm Companion application which was capable of monitoring and controlling the environmental parameters observed in the microfarm. The application allows the user to view the current value of the parameter involved and to choose whether to control the parameter regulators automatically or manually. The microfarm system runs autonomously which reduces the labor required to produce healthy plants and crops. Mustard greens samples were used in testing the system. After a month of monitoring the height of the samples, it was observed that the average height of the samples is about 0.23 cm taller than the standard height. The proponents has also tested the system functionality by evaluating the sensor data log that provides the values gathered by the sensors and the turn-on times of the parameter regulators. From these data, it can be observed that whenever the values obtained by the sensors fall outside the threshold range, the parameter regulators turns on, indicating that the system is working properly.
Effects of a single impurity in a Luttinger liquid with spin-orbit coupling<|sep|>In quasi-1D conducting nanowires spin-orbit coupling destructs spin-charge separation, intrinsic to Tomonaga-Luttinger liquid (TLL). We study renormalization of a single scattering impurity in a such liquid. Performing bosonization of low-energy excitations and exploiting perturbative renormalization analysis we extend the phase portrait in $K_s$ - $K_c$ space, obtained previously for TLL with decoupled spin-charge channels.
Estimating Graphlet Statistics via Lifting<|sep|>Exploratory analysis over network data is often limited by the ability to efficiently calculate graph statistics, which can provide a model-free understanding of the macroscopic properties of a network. We introduce a framework for estimating the graphlet count---the number of occurrences of a small subgraph motif (e.g. a wedge or a triangle) in the network. For massive graphs, where accessing the whole graph is not possible, the only viable algorithms are those that make a limited number of vertex neighborhood queries. We introduce a Monte Carlo sampling technique for graphlet counts, called {\em Lifting}, which can simultaneously sample all graphlets of size up to $k$ vertices for arbitrary $k$. This is the first graphlet sampling method that can provably sample every graphlet with positive probability and can sample graphlets of arbitrary size $k$. We outline variants of lifted graphlet counts, including the ordered, unordered, and shotgun estimators, random walk starts, and parallel vertex starts. We prove that our graphlet count updates are unbiased for the true graphlet count and have a controlled variance for all graphlets. We compare the experimental performance of lifted graphlet counts to the state-of-the art graphlet sampling procedures: Waddling and the pairwise subgraph random walk.
Non-fragile Finite-time Stabilization for Discrete Mean-field Stochastic Systems<|sep|>In this paper, the problem of non-fragile finite-time stabilization for linear discrete mean-field stochastic systems is studied. The uncertain characteristics in control parameters are assumed to be random satisfying the Bernoulli distribution. A new approach called the ``state transition matrix method" is introduced and some necessary and sufficient conditions are derived to solve the underlying stabilization problem. The Lyapunov theorem based on the state transition matrix also makes a contribution to the discrete finite-time control theory. One practical example is provided to validate the effectiveness of the newly proposed control strategy.
Finite temperature dynamical correlations for the dimerized spin-1/2 chain<|sep|>We use the density matrix renormalization group method (DMRG) to compute the frequency and momentum resolved spin-spin correlation functions of a dimerized spin-1/2 chain under a magnetic field at finite temperature. The spectral features strongly depend on the regime of the magnetic field. For increasing magnetic fields, the transitions from a gapped spin liquid phase to a Tomonaga-Luttinger liquid, and then to a totally polarized phase, can be identified in the spectra. Compared to the zero temperature case, the finite temperature excitations give rise to additional spectral features that we compute numerically and identify analytically as transitions from thermally excited states. We compute quantitatively the broadening of the dispersion of a single spin-flip excitation due to the temperature and find a strong asymmetric broadening. We discuss the consequences of these findings for neutron experiments on dimerized one dimensional quantum chains.
Model-based clustering in networks with Stochastic Community Finding<|sep|>In the model-based clustering of networks, blockmodelling may be used to identify roles in the network. We identify a special case of the Stochastic Block Model (SBM) where we constrain the cluster-cluster interactions such that the density inside the clusters of nodes is expected to be greater than the density between clusters. This corresponds to the intuition behind community-finding methods, where nodes tend to clustered together if they link to each other. We call this model Stochastic Community Finding (SCF) and present an efficient MCMC algorithm which can cluster the nodes, given the network. The algorithm is evaluated on synthetic data and is applied to a social network of interactions at a karate club and at a monastery, demonstrating how the SCF finds the 'ground truth' clustering where sometimes the SBM does not. The SCF is only one possible form of constraint or specialization that may be applied to the SBM. In a more supervised context, it may be appropriate to use other specializations to guide the SBM.
Simulation Analysis of Medium Access Techniques<|sep|>This paper presents comparison of Access Techniques used in Medium Access Control (MAC) protocol for Wireless Body Area Networks (WBANs). Comparison is performed between Time Division Multiple Access (TDMA), Frequency Division Multiple Access (FDMA), Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA), Pure ALOHA and Slotted ALOHA (S-ALOHA). Performance metrics used for comparison are throughput (T), delay (D) and offered load (G). The main goal for comparison is to show which technique gives highest Throughput and lowest Delay with increase in Load. Energy efficiency is major issue in WBAN that is why there is need to know which technique performs best for energy conservation and also gives minimum delay.
Improving Iris Recognition Accuracy By Score Based Fusion Method<|sep|>Iris recognition technology, used to identify individuals by photographing the iris of their eye, has become popular in security applications because of its ease of use, accuracy, and safety in controlling access to high-security areas. Fusion of multiple algorithms for biometric verification performance improvement has received considerable attention. The proposed method combines the zero-crossing 1 D wavelet Euler number, and genetic algorithm based for feature extraction. The output from these three algorithms is normalized and their score are fused to decide whether the user is genuine or imposter. This new strategies is discussed in this paper, in order to compute a multimodal combined score.
Maser Flares Driven by Variations in Pumping and Background Radiation<|sep|>We simulate maser flares by varying either the pump rate or the background level of radiation in a 3D model of a maser cloud. We investigate the effect of different cloud shapes, saturation levels and viewpoints. Results are considered for clouds with both uniform and internally variable unsaturated inversion. Pumping and background variations are represented by several different driving functions, some of which are light curves drawn from observations. We summarise the pumping variability results in terms of three observable parameters, the maximum flux density achieved, a variability index and duty cycle. We demonstrate typical ranges of the flux density that may result from viewing an aspherical object from random viewpoints. The best object for a flare is a prolate cloud, viewed close to its long axis and driven from unsaturated conditions to at least modest saturation. Results for variation of the background level are qualitatively different from the variable pumping results in that they tend to produce short intervals of low flux density under conditions of moderate saturation and sufficient variability to be consistent with strong flaring. Variable background models typically have a significantly higher duty cycle than those with variable pumping.
PM4Py-GPU: a High-Performance General-Purpose Library for Process Mining<|sep|>Open-source process mining provides many algorithms for the analysis of event data which could be used to analyze mainstream processes (e.g., O2C, P2P, CRM). However, compared to commercial tools, they lack the performance and struggle to analyze large amounts of data. This paper presents PM4Py-GPU, a Python process mining library based on the NVIDIA RAPIDS framework. Thanks to the dataframe columnar storage and the high level of parallelism, a significant speed-up is achieved on classic process mining computations and processing activities.
Ground-state properties of a supersymmetric fermion chain<|sep|>We analyze the ground state of a strongly interacting fermion chain with a supersymmetry. We conjecture a number of exact results, such as a hidden duality between weak and strong couplings. By exploiting a scale free property of the perturbative expansions, we find exact expressions for the order parameters, yielding the critical exponents. We show that the ground state of this fermion chain and another model in the same universality class, the XYZ chain along a line of couplings, are both written in terms of the same polynomials. We demonstrate this explicitly for up to N = 24 sites, and provide consistency checks for large N. These polynomials satisfy a recursion relation related to the Painlev\'e VI differential equation, and using a scale-free property of these polynomials, we derive a simple and exact formula for their limit as N goes to infinity.
Performance simulation of a MRPC-based PET Imaging System<|sep|>The low cost and high resolution gas-based Multi-gap Resistive Plate Chamber (MRPC) opens a new possibility to find an efficient alternative detector for Time of Flight (TOF) based Positron Emission Tomography, where the sensitivity of the system depends largely on the time resolution of the detector. Suitable converters can be used to increase the efficiency of detection of photons from annihilation. In this work, we perform a detailed GEANT4 simulation to optimize the converter thickness thereby improving the efficiency of photon conversion. Also we have developed a Monte Carlo based simulation of MRPC response thereby obtaining the intrinsic time resolution of the detector, making it possible to simulate the final response of MRPC-based systems for PET imaging. The result of the cosmic ray test of a four-gap Bakelite-based MRPC operating in streamer mode is discussed.
Microring resonators on a membrane optical circuit for atom-light interactions<|sep|>We describe the design and fabrication of a scalable atom-light photonic interface based on a silicon nitride microring resonator on a transparent silicon dioxide-nitride multi-layer membrane. This new photonic platform is fully compatible with freespace cold atom laser cooling, stable trapping, and sorting at around $100~$nm from the microring surface, permitting the formation of an organized, strongly interacting atom-photonic hybrid lattice. We demonstrate small radius ($R\sim$16$\mu$m) microring and racetrack resonators with a high quality factor $Q=3.2\times 10^5$, projecting a single atom cooperativity parameter of $C=25$ and a vacuum Rabi frequency of $2g= 2\pi\times 340~$MHz for trapped cesium atoms interacting with a microring resonator mode. We show that the quality factor is currently limited by the surface roughness of the multi-layer membrane, grown using low pressure chemical vapor deposition (LPCVD) processes. We discuss possible further improvements to a quality factor above $Q>5\times10^6$, potentially achieving single atom cooperativity parameter of $C > 500$ for strong single atom-photon coupling.
Quasars as standard candles II: The non linear relation between UV and X-ray emission at high redshifts<|sep|>A tight non-linear relation between the X-ray and the optical-ultraviolet (UV) emission has been observed in Active Galactic Nuclei (AGN) over a wide range of redshift and several orders of magnitude in luminosity, suggesting the existence of an ubiquitous physical mechanism regulating the energy transfer between the accretion disc and the X-ray emitting corona. Recently, our group developed a method to use this relation in the observational cosmology, turning quasars into standardizable candles. This work has the main aim to investigate the potential evolution of this correction at high redshifts. We thus studied the $L_{\rm X}-L_{\rm UV}$ relation for a sample of quasars in the redshift range 4<$z$<7, adopting the selection criteria proposed in our previous work regarding their spectral properties. The resulting sample consists of 53 Type 1 (unobscured) quasars, observed either with Chandra or XMM-Newton, for which we performed a full spectral analysis, determining the rest-frame 2 keV flux density, as well as more general X-ray properties such as the estimate of photon index, and the soft (0.5-2 keV) and hard (2-10 keV) unabsorbed luminosities. We find that the relation shows no evidence for evolution with redshift. The intrinsic dispersion of the L$_X$-L$_{UV}$ for a sample free of systematics/contaminants is of the order of 0.22 dex, which is consistent with previous estimates from our group on quasars at lower redshift.
Sem-LSD: A Learning-based Semantic Line Segment Detector<|sep|>In this paper, we introduces a new type of line-shaped image representation, named semantic line segment (Sem-LS) and focus on solving its detection problem. Sem-LS contains high-level semantics and is a compact scene representation where only visually salient line segments with stable semantics are preserved. Combined with high-level semantics, Sem-LS is more robust under cluttered environment compared with existing line-shaped representations. The compactness of Sem-LS facilitates its use in large-scale applications, such as city-scale SLAM (simultaneously localization and mapping) and LCD (loop closure detection). Sem-LS detection is a challenging task due to its significantly different appearance from existing learning-based image representations such as wireframes and objects. For further investigation, we first label Sem-LS on two well-known datasets, KITTI and KAIST URBAN, as new benchmarks. Then, we propose a learning-based Sem-LS detector (Sem-LSD) and devise new module as well as metrics to address unique challenges in Sem-LS detection. Experimental results have shown both the efficacy and efficiency of Sem-LSD. Finally, the effectiveness of the proposed Sem-LS is supported by two experiments on detector repeatability and a city-scale LCD problem. Labeled datasets and code will be released shortly.
The effect of confinement and stiffness on the conformational change of a semiflexible homopolymer chain<|sep|>We analyse the nature of the confinement of an infinitely long (and finite) linear semiflexible homo-polymer chain confined in between two geometrical constraints (A&B) under good solvent condition in two dimensions. The constraints are stair shaped impenetrable lines. A lattice model of fully directed self avoiding walk is used to list information of walks of the confined chain and the exact enumeration technique is used for the canonical ensemble of conformations of the confined chain to discuss equilibrium statistics of the chain. We obtain the probability of polymerization of the confined flexible chain segments with either one end (polymer trains) or both the ends of the confined chain lying on the stair shaped constraints (polymer bridge and arc). We have also calculated the force of confinement exerted by the constraints on to the chain or the force exerted by the chain on the geometrical constraints using grand canonical ensemble theory and discuss nature of variation of the force.
Information content of the weak-charge form factor<|sep|>Parity-violating electron scattering provides a model-independent determination of the nuclear weak-charge form factor that has widespread implications across such diverse areas as fundamental symmetries, nuclear structure, heavy-ion collisions, and neutron-star structure. We assess the impact of precise measurements of the weak-charge form factor of ${}^{48}$Ca and ${}^{208}$Pb on a variety of nuclear observables, such as the neutron skin and the electric-dipole polarizability. We use the nuclear Density Functional Theory with several accurately calibrated non-relativistic and relativistic energy density functionals. To assess the degree of correlation between nuclear observables and to explore systematic and statistical uncertainties on theoretical predictions, we employ the chi-square statistical covariance technique. We find a strong correlation between the weak-charge form factor and the neutron radius, that allows for an accurate determination of the neutron skin of neutron-rich nuclei. We determine the optimal range of the momentum transfer $q$ that maximizes the information content of the measured weak-charge form factor and quantify the uncertainties associated with the strange quark contribution. Moreover, we confirm the role of the electric-dipole polarizability as a strong isovector indicator. Accurate measurements of the weak-charge form factor of ${}^{48}$Ca and ${}^{208}$Pb will have a profound impact on many aspects of nuclear theory and hadronic measurements of neutron skins of exotic nuclei at radioactive-beam facilities.
Wearable camera-based human absolute localization in large warehouses<|sep|>In a robotised warehouse, as in any place where robots move autonomously, a major issue is the localization or detection of human operators during their intervention in the work area of the robots. This paper introduces a wearable human localization system for large warehouses, which utilize preinstalled infrastructure used for localization of automated guided vehicles (AGVs). A monocular down-looking camera is detecting ground nodes, identifying them and computing the absolute position of the human to allow safe cooperation and coexistence of humans and AGVs in the same workspace. A virtual safety area around the human operator is set up and any AGV in this area is immediately stopped. In order to avoid triggering an emergency stop because of the short distance between robots and human operators, the trajectories of the robots have to be modified so that they do not interfere with the human. The purpose of this paper is to demonstrate an absolute visual localization method working in the challenging environment of an automated warehouse with low intensity of light, massively changing environment and using solely monocular camera placed on the human body.
Gravitational Lensing by Spherical Lenses<|sep|>In this work we introduced a new proposal to study the gravitational lensing theory by spherical lenses, starting from its surface mass density $\Sigma(x)$ written in terms of a decreasing function $f$ of a dimensionless coordinate $x$ on the lens plane. The main result is the use of the function $f(x)$ to find directly the lens properties, at the same time that the lens problem is described by a first order differential equation which encodes all information about the lens. SIS and NIS profiles are used as examples to find their functions $f(x)$. Using the Poisson equation we find that the deflection angle is directly proportional to $f(x)$, and therefore the lens equation can be written in terms of the function and the parameters of the lens. The critical and caustic curves, as well as image formation and magnification generated by the lens are analyzed. As an example of this method, the properties of a lens modeled by a NFW profile are determined. Altough the puntual mass is spherically symmetric, its mass density is not continuous so that its $f(x)$ function is discussed in the Appendix A.
Progenitor constraints for core-collapse supernovae from Chandra X-ray observations<|sep|>The progenitors of hydrogen-poor core-collapse supernovae (SNe) of types Ib, Ic and IIb are believed to have shed their outer hydrogen envelopes either by extremely strong stellar winds, characteristic of classical Wolf-Rayet stars, or by binary interaction with a close companion star. The exact nature of the progenitors and the relative importance of these processes are still open questions. One relatively unexplored method to constrain the progenitors is to search for high-mass X-ray binaries (HMXB) at SN locations in pre-explosion X-ray observations. In a HMXB, one star has already exploded as a core-collapse SN, producing a neutron star or a stellar-mass black hole. It is likely that the second star in the system will also explode as a supernova, which should cause a detectable long-term change in the system's X-ray luminosity. In particular, a pre-explosion detection of a HMXB coincident with a SN could be informative about the progenitor's nature. In this paper we analyze pre-explosion ACIS observations of 18 nearby type Ib, Ic and IIb supernovae from the Chandra X-ray observatory public archive. Two sources that could potentially be associated with the supernova are identified in the sample. Additionally we make similar post-explosion measurements for 46 SNe. Although our modelling indicates that progenitor systems with compact binary companions are probably quite rare, studies of this type can in the future provide more stringent constraints as the number of discovered nearby SNe and suitable pre-explosion X-ray data are both increasing.
Abelian Fibrations, String Junctions, and Flux/Geometry Duality<|sep|>In previous work, it was argued that the type IIB T^6/Z_2 orientifold with a choice of flux preserving N=2 supersymmetry is dual to a class of purely geometric type IIA compactifications on abelian surface (T^4) fibered Calabi-Yau threefolds. We provide two explicit constructions of the resulting Calabi-Yau duals. The first is a monodromy based description, analogous to F-theory encoding of Calabi-Yau geometry via 7-branes and string junctions, except for T^4 rather than T^2 fibers. The second is an explicit algebro-geometric construction in which the T^4 fibers arise as the Jacobian tori of a family of genus-2 curves. This improved description of the duality map will be a useful tool to extend our understanding of warped compactifications. We sketch applications to related work to define warped Kaluza-Klein reduction in toroidal orientifolds, and to check the modified rules for D-brane instanton zero mode counting due to the presence of flux and other D-branes. The nontrivial fundamental groups of the Calabi-Yau manifolds constructed also have potential applications to heterotic model building.
On post-Newtonian orbits and the Galactic-center stars<|sep|>Stars near the Galactic center reach a few percent of light speed during pericenter passage, which makes post-Newtonian effects potentially detectable. We formulate the orbit equations in Hamiltonian form such that the $O(v^2/c^2)$ and $O(v^3/c^3)$ post-Newtonian effects of the Kerr metric appear as a simple generalization of the Kepler problem. A related perturbative Hamiltonian applies to photon paths. We then derive a symplectic integrator with adaptive time-steps, for fast and accurate numerical calculation of post-Newtonian effects. Using this integrator, we explore relativistic effects. Taking the star S2 as an example, we find that general relativity would contribute tenths of mas in astrometry and tens of $\rm km s^{-1}$ in kinematics. (For eventual comparison with observations, redshift and time-delay contributions from the gravitational field on light paths will need to be calculated, but we do attempt these in the present paper.) The contribution from stars, gas, and dark matter in the Galactic center region is still poorly constrained observationally, but current models suggest that the resulting Newtonian perturbation on the orbits could plausibly be of the same order as the relativistic effects for stars with semi-major axes $\gtrsim 0.01$ pc (or 250 mas). Nevertheless, the known and distinctive {\it time dependence} of the relativistic perturbations may make it possible to disentangle and extract both effects from observations.
Exploring the Suitability of BLE Beacons to Track Poacher Vehicles in Harsh Jungle Terrains<|sep|>Our overall aim is focused on exploring whether we could use Bluetooth Low Energy (BLE) technology to track poacher vehicles in remote and rural areas such as Sabah, in Malaysia, especially deep inside the jungle terrain with little or no communication technologies exists. Tracking technologies are currently limited to relying on satellites or cellular towers, for environments that do not permit access to these signals, very few viable alternatives exist. This paper explores the use of BLE as a method to track vehicles. It works by mounting Bluetooth beacons beside a road and placing a receiver concealed somewhere inside the vehicle. As the vehicle drives past the beacon, the receiver and beacon are momentarily in range, the receiver then stores a unique ID from the beacon and when the vehicle is then in an area with GSM signal, an SMS is sent containing the unique IDs of the beacons that have been detected. This project is prototyped and tested in collaboration with the Danau Girang Field Centre in Sabah, Malaysia. The results offer insights for how effective BLE beacons are in a tracking situation for where the beacon and receiver are in range for a short period of time as well as how different obstructions will affect the range and strength of the signal. It is important to note that our objective is not to catch the poacher, instead to understand how they move around within jungle terrain, as we can use such information to develop a comprehensive plan against poaching activities.
Spiral patterns in planetesimal circumbinary disks<|sep|>Planet formation scenarios and the observed planetary dynamics in binaries pose a number of theoretical challenges, especially in what concerns circumbinary planetary systems. We explore the dynamical stirring of a planetesimal circumbinary disk in the epoch when the gas component disappears. For this purpose, following theoretical approaches by Heppenheimer (1978) and Moriwaki and Nakagawa (2004), we develop a secular theory for the dynamics of planetesimals in circumbinary disks. If the binary is eccentric and its components have unequal masses, a spiral density wave is generated, engulfing the disk on the secular timescale, which may exceed 10^7 yr, depending on the problem parameters. The spiral pattern is transient; thus, its observed presence may betray system's young age. We explore the pattern both analytically and in numerical experiments. The derived analytical spiral is a modified lituus; it matches the numerical density wave in the gas-free case perfectly. Using the SPH scheme, we explore the effect of residual gas on the wave propagation.
Autoregressive Score Matching<|sep|>Autoregressive models use chain rule to define a joint probability distribution as a product of conditionals. These conditionals need to be normalized, imposing constraints on the functional families that can be used. To increase flexibility, we propose autoregressive conditional score models (AR-CSM) where we parameterize the joint distribution in terms of the derivatives of univariate log-conditionals (scores), which need not be normalized. To train AR-CSM, we introduce a new divergence between distributions named Composite Score Matching (CSM). For AR-CSM models, this divergence between data and model distributions can be computed and optimized efficiently, requiring no expensive sampling or adversarial training. Compared to previous score matching algorithms, our method is more scalable to high dimensional data and more stable to optimize. We show with extensive experimental results that it can be applied to density estimation on synthetic data, image generation, image denoising, and training latent variable models with implicit encoders.
Attentional Biased Stochastic Gradient for Imbalanced Classification<|sep|>In this paper, we present a simple yet effective method (ABSGD) for addressing the data imbalance issue in deep learning. Our method is a simple modification to momentum SGD where we leverage an attentional mechanism to assign an individual importance weight to each gradient in the mini-batch. Unlike many existing heuristic-driven methods for tackling data imbalance, our method is grounded in {\it theoretically justified distributionally robust optimization (DRO)}, which is guaranteed to converge to a stationary point of an information-regularized DRO problem. The individual-level weight of a sampled data is systematically proportional to the exponential of a scaled loss value of the data, where the scaling factor is interpreted as the regularization parameter in the framework of information-regularized DRO. Compared with existing class-level weighting schemes, our method can capture the diversity between individual examples within each class. Compared with existing individual-level weighting methods using meta-learning that require three backward propagations for computing mini-batch stochastic gradients, our method is more efficient with only one backward propagation at each iteration as in standard deep learning methods. To balance between the learning of feature extraction layers and the learning of the classifier layer, we employ a two-stage method that uses SGD for pretraining followed by ABSGD for learning a robust classifier and finetuning lower layers. Our empirical studies on several benchmark datasets demonstrate the effectiveness of the proposed method.
Reasoning about Entailment with Neural Attention<|sep|>While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.
Generalized Pareto Processes and Liquidity<|sep|>Motivated by the modeling of liquidity risk in fund management in a dynamic setting, we propose and investigate a class of time series models with generalized Pareto marginals: the autoregressive generalized Pareto process (ARGP), a modified ARGP (MARGP) and a thresholded ARGP (TARGP). These models are able to capture key data features apparent in fund liquidity data and reflect the underlying phenomena via easily interpreted, low-dimensional model parameters. We establish stationarity and ergodicity, provide a link to the class of shot-noise processes, and determine the associated interarrival distributions for exceedances. Moreover, we provide estimators for all relevant model parameters and establish consistency and asymptotic normality for all estimators (except the threshold parameter, which as usual must be dealt with separately). Finally, we illustrate our approach using real-world fund redemption data, and we discuss the goodness-of-fit of the estimated models.
Focus Demo: CANFAR+Skytree: A Cloud Computing and Data Mining System for Astronomy<|sep|>This is a companion Focus Demonstration article to the CANFAR+Skytree poster (Ball 2012), demonstrating the usage of the Skytree machine learning software on the Canadian Advanced Network for Astronomical Research (CANFAR) cloud computing system. CANFAR+Skytree is the world's first cloud computing system for data mining in astronomy.
Normalized Attention Without Probability Cage<|sep|>Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.
Target echo strength modelling at FOI, including results from the BeTSSi II workshop<|sep|>An overview of the target echo strength (TS) modelling capacity at the Swedish Defense Research Agency (FOI) is presented. The modelling methods described range from approximate ones, such as raytracing and Kirchhoff approximation codes, to high accuracy full field codes including boundary integral equation methods and finite elements methods. Illustrations of the applicability of the codes are given for a few simple cases tackled during the BeTTSi II (Benchmark Target Echo Strength Simulation) workshop held in Kiel 2014.
Nonlinear 1-Bit Precoding for Massive MU-MIMO with Higher-Order Modulation<|sep|>Massive multi-user (MU) multiple-input multiple- output (MIMO) is widely believed to be a core technology for the upcoming fifth-generation (5G) wireless communication standards. The use of low-precision digital-to-analog converters (DACs) in MU-MIMO base stations is of interest because it reduces the power consumption, system costs, and raw baseband data rates. In this paper, we develop novel algorithms for downlink precoding in massive MU-MIMO systems with 1-bit DACs that support higher-order modulation schemes such as 8-PSK or 16-QAM. Specifically, we present low-complexity nonlinear precoding algorithms that achieve low error rates when combined with blind or training-based channel-estimation algorithms at the user equipment. These results are in stark contrast to linear-quantized precoding algorithms, which suffer from a high error floor if used with high-order modulation schemes and 1-bit DACs.
Metal-mass-to-light ratios of the Perseus cluster out to the virial radius<|sep|>We analyzed XMM-Newton data of the Perseus cluster out to $\sim$1 Mpc, or approximately half the virial radius. Using the flux ratios of Lyalpha lines of H-like Si and S to Kalpha line of He-like Fe, the abundance ratios of Si/Fe and S/Fe of the intracluster medium (ICM) were derived using the APEC plasma code v2.0.1. The temperature dependence of the line ratio limits the systematic uncertainty in the derived abundance ratio. The Si/Fe and S/Fe in the ICM of the Perseus cluster show no radial gradient. The emission-weighted averages of the Si/Fe and S/Fe ratios outside the cool core are 0.91 +- 0.08 and 0.93 +- 0.10, respectively, in solar units according to the solar abundance table of Lodders (2003). These ratios indicate that most Fe was synthesized by supernovae Ia. We collected K-band luminosities of galaxies and calculated the ratio of Fe and Si mass in the ICM to K-band luminosity, iron-mass-to-light ratio (IMLR) and silicon-mass-to-light ratio (SMLR). Within $\sim$1 Mpc, the cumulative IMLR and SMLR increase with radius. Using Suzaku data for the northwest and east directions, we also calculated the IMLR out to $\sim$ 1.8 Mpc, or about the virial radius. We constrained the SMLR out to this radius and discussed the slope of the initial mass function of stars in the cluster. Using the cumulative IMLR profile, we discuss the past supernova Ia rate.
Inhomogeneities in the Universe and the Fitting Problem<|sep|>Observational cosmology provides us with a large number of high precision data which are used to derive models trying to reproduce ``on the mean'' our observable patch of the Universe. Most of these attempts are achieved in the framework of a Friedmann-Lema\^itre cosmology where large scale homogeneity is assumed. However, we know, from the observation of structures at increasing scales, that these models are only approximations of a smoothed or averaged inhomogeneous underlying patern. Anyhow, when modelling the Universe, the usual method is to use continuous functions representing the kinematical scalars of the velocity field, implicitly assuming that they represent volume averages of the corresponding fine-scale inhomogeneous quantities, then put them into the Einstein equations which are solved to give the model and its dependance upon a number of parameters arbitrarily defined. In General Relativity, such a method is very much involved since the equations which determine the metric tensor and the quantities calculated from it are highly nonlinear. The question raised by the method consisting of determining the parameters of an a priori assumed FLRW model from observational data is the ``fitting problem'' brought to general attention by Ellis and Stoeger in the 80's. This problem has recently experienced a reniewed attention due to the amount of available data and the increase of the minimum scale at which homogeneity can be assumed. We propose a discussion of this issue in the light of the latest developments of observational and theoretical cosmology.
BSTree: an Incremental Indexing Structure for Similarity Search and Real Time Monitoring of Data Streams<|sep|>In this work, a new indexing technique of data streams called BSTree is proposed. This technique uses the method of data discretization, SAX [4], to reduce online the dimensionality of data streams. It draws on Btree to build the index and finally uses an LRV (least Recently visited) pruning technique to rid the index structure from data whose last visit time exceeds a threshold value and thus minimizes response time for similarity search queries.
Preventive and Reactive Cyber Defense Dynamics Is Globally Stable<|sep|>The recently proposed {\em cybersecurity dynamics} approach aims to understand cybersecurity from a holistic perspective by modeling the evolution of the global cybersecurity state. These models describe the interactions between the various kinds of cyber defenses and the various kinds of cyber attacks. We study a particular kind of cybersecurity dynamics caused by the interactions between preventive and reactive defenses (e.g., filtering and malware detection) against push- and pull-based cyber attacks (e.g., malware spreading and "drive-by download" attacks). The dynamics was previously shown to be globally stable in a {\em special} regime of the parameter universe, but little is known beyond this special regime. In this paper, we resolve an open problem in this domain by proving that the dynamics is globally stable in the {\em entire} parameter universe (i.e., the dynamics always converges to a unique equilibrium). We discuss the cybersecurity meanings and implications of this theoretic result. We also prove that the dynamics converges {\em exponentially} to the equilibrium except for a special parameter regime, in which case the dynamics converges {\em polynomially}. Since it is often difficult to compute the equilibrium, we propose new bounds of the equilibrium and numerically show that these bounds are tighter than those proposed in the literature.
A New Non-MDS Hash Function Resisting Birthday Attack and Meet-in-the-middle Attack<|sep|>To examine the integrity and authenticity of an IP address efficiently and economically, this paper proposes a new non-Merkle-Damgard structural (non-MDS) hash function called JUNA that is based on a multivariate permutation problem and an anomalous subset product problem to which no subexponential time solutions are found so far. JUNA includes an initialization algorithm and a compression algorithm, and converts a short message of n bits which is regarded as only one block into a digest of m bits, where 80 <= m <= 232 and 80 <= m <= n <= 4096. The analysis and proof show that the new hash is one-way, weakly collision-free, and strongly collision-free, and its security against existent attacks such as birthday attack and meet-in-the- middle attack is to O(2 ^ m). Moreover, a detailed proof that the new hash function is resistant to the birthday attack is given. Compared with the Chaum-Heijst-Pfitzmann hash based on a discrete logarithm problem, the new hash is lightweight, and thus it opens a door to convenience for utilization of lightweight digital signing schemes.
Traffic flow splitting from crowdsourced digital route choice support<|sep|>Digital technology is fundamentally transforming human mobility. Route choices in particular are greatly affected by the availability of traffic data, increased connectivity of data sources and cheap access to computational resources. Digital routing technologies promise more efficient route choices for the individual and a reduction of congestion for cities. Yet, it is unclear how widespread adoption of such technologies actually alters the collective traffic flow dynamics on complex street networks. Here, we answer this question for the dynamics of urban commuting under digital route choice support. Building on the class of congestion games we study the evolution of commuting behavior as a fraction of the population relies on, but also contributes to, crowdsourced traffic information. The remainder of the population makes their route choices based on personal experience. We show how digital route choice support may cause a separation of commuter flows into technology and non-technology users along different routes. This collective behavior may fuel systemic inefficiencies and lead to an increase of congestion as a consequence. These results highlight new research directions in the field of algorithmic design of route choice decision support protocols to help fight congestion, emissions and other systemic inefficiencies in the course of increasing urbanization and digitization.
GHTraffic: A Dataset for Reproducible Research in Service-Oriented Computing<|sep|>We present GHTraffic, a dataset of significant size comprising HTTP transactions extracted from GitHub data and augmented with synthetic transaction data. The dataset facilitates reproducible research on many aspects of service-oriented computing. This paper discusses use cases for such a dataset and extracts a set of requirements from these use cases. We then discuss the design of GHTraffic, and the methods and tool used to construct it. We conclude our contribution with some selective metrics that characterise GHTraffic.
Analyzing the Forgetting Problem in the Pretrain-Finetuning of Dialogue Response Models<|sep|>In this work, we study how the finetuning stage in the pretrain-finetune framework changes the behavior of a pretrained neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. Our major finding is that after standard finetuning, the model forgets some of the important language generation skills acquired during large-scale pretraining. We demonstrate the forgetting phenomenon through a set of detailed behavior analysis from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named "mix-review". We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.
Detection of OH absorption against PSR B1718-35<|sep|>OH absorption against PSR B1718-35 at (l,b) =351.688,+0.671 has been discovered at 1665 and 1667 MHz using the Green Bank Telescope. The absorption appears to arise at the interface of an HII region and a molecular cloud which are likely associated with the high mass star forming region NGC 6334. Beam dilution is found to be the cause of differences in the opacity of the OH against the Galactic background continuum emission and against the pulsar. The OH cloud is approximately 3 by 1.3 pc and is located behind the HII region.
Reconfiguration of a satellite constellation in circular formation orbit with decentralized model predictive control<|sep|>Satellite constellation missions, consisting of a large number of spacecraft, are increasingly being launched or planned. Such missions require novel control approaches, in particular for what concerns orbital phasing maneuvers. In this context, we consider the problem of reconfiguration of a satellite constellation in a circular formation. In our scenario, a formation of equally spaced spacecraft need to undergo an autonomous reconfiguration due to the deorbiting of a satellite in the formation. The remaining spacecraft have to reconfigure to form again an equidistant formation. To achieve this goal, we consider two decentralized strategies that rely on different sets of information about the neighboring spacecraft in the formation. In the fully decentralized case, each controller knows only the current states of each spacecraft, i.e. position and velocity, while in the second decentralized strategy with with information sharing, the entire planned nominal trajectory of each spacecraft is available to its neighbors. Our numerical simulation results show that, by increasing the amount of information available to each spacecraft, faster reconfiguration maneuvers with smaller fuel consumption can be achieved.
The study of thermonuclear X-ray bursts in accreting millisecond pulsar MAXI J1816-195 with NuSTAR and NICER<|sep|>The millisecond pulsar MAXI J1816--195 was recently discovered by MAXI in 2022 May. We have studied different properties of the pulsar using data from NuSTAR and NICER observations. The position of the source is measured by NuSTAR as RA = $18^h 16^m 52^s.40$, Dec = $-19^o37^{'} 58^{''}.35$. The unstable burning of accreted material on the surface of neutron stars induces thermonuclear (Type-I) bursts. Several thermonuclear bursts have been detected from the source during the outburst. We study the evolution of burst profile with flux and energy using NuSTAR and NICER observations. During the NuSTAR observation, a total of four bursts were detected from the source. The duration of each burst was around $\sim$ 30 s and the ratio of peak to persistent count rate is $\sim$ 26 as seen from the NuSTAR data. The thermonuclear bursts are modeled to determine the burst timing parameters using a sharp linear rise and exponential decay function. The burst profiles show a relatively long tail in lower energies. The hardness ratio during the thermonuclear bursts shows significant variation as observed by NuSTAR. We successfully model the broadband burst-resolved spectra with a combination of an absorbed blackbody along with a non-thermal component to account for the persistent emission. The burst-resolved spectral parameters show significant evolution during the burst. During the peak of the burst, the Eddington luminosity is found to be $\sim 3.7 \times 10^{38}$ erg s$^{-1}$. The burst-resolved spectral parameters provide a source distance of $8.5\pm1.2$ kpc for isotropic burst emission.
Unsupervised Graph Poisoning Attack via Contrastive Loss Back-propagation<|sep|>Graph contrastive learning is the state-of-the-art unsupervised graph representation learning framework and has shown comparable performance with supervised approaches. However, evaluating whether the graph contrastive learning is robust to adversarial attacks is still an open problem because most existing graph adversarial attacks are supervised models, which means they heavily rely on labels and can only be used to evaluate the graph contrastive learning in a specific scenario. For unsupervised graph representation methods such as graph contrastive learning, it is difficult to acquire labels in real-world scenarios, making traditional supervised graph attack methods difficult to be applied to test their robustness. In this paper, we propose a novel unsupervised gradient-based adversarial attack that does not rely on labels for graph contrastive learning. We compute the gradients of the adjacency matrices of the two views and flip the edges with gradient ascent to maximize the contrastive loss. In this way, we can fully use multiple views generated by the graph contrastive learning models and pick the most informative edges without knowing their labels, and therefore can promisingly support our model adapted to more kinds of downstream tasks. Extensive experiments show that our attack outperforms unsupervised baseline attacks and has comparable performance with supervised attacks in multiple downstream tasks including node classification and link prediction. We further show that our attack can be transferred to other graph representation models as well.
Studying springs in series using a single spring<|sep|>Springs are used for a wide range of applications in physics and engineering. Possibly, one of its most common uses is to study the nature of restoring forces in oscillatory systems. While experiments that verify the Hooke's law using springs are abundant in the physics literature, those that explore the combination of several springs together are very rare. In this paper, an experiment designed to study the static properties of a combination of springs in series using only one single spring is presented. Paint marks placed on the coils of the spring allowed us to divide it into segments, and considered it as a collection of springs connected in series. The validity of Hooke's law for the system and the relationship between the spring constant of the segments with the spring constant of the entire spring is verified experimentally. The easy setup, accurate results, and educational benefits make this experiment attractive and useful for high school and first-year college students.
Structured Model Pruning of Convolutional Networks on Tensor Processing Units<|sep|>The deployment of convolutional neural networks is often hindered by high computational and storage requirements. Structured model pruning is a promising approach to alleviate these requirements. Using the VGG-16 model as an example, we measure the accuracy-efficiency trade-off for various structured model pruning methods and datasets (CIFAR-10 and ImageNet) on Tensor Processing Units (TPUs). To measure the actual performance of models, we develop a structured model pruning library for TensorFlow2 to modify models in place (instead of adding mask layers). We show that structured model pruning can significantly improve model memory usage and speed on TPUs without losing accuracy, especially for small datasets (e.g., CIFAR-10).
Detecting small low emission radiating sources<|sep|>The article addresses the possibility of robust detection of geometrically small, low emission sources on a significantly stronger background. This problem is important for homeland security. A technique of detecting such sources using Compton type cameras is developed, which is shown on numerical examples to have high sensitivity and specificity and also allows to assign confidence probabilities of the detection. 2D case is considered in detail.
Estimating the Probability that a Function Observed with Noise is Convex<|sep|>Consider a real-valued function that can only be observed with stochastic noise at a finite set of design points within a Euclidean space. We wish to determine whether there exists a convex function that goes through the true function values at the design points. We develop an asymptotically consistent Bayesian sequential sampling procedure that estimates the posterior probability of this being true. In each iteration, the posterior probability is estimated using Monte Carlo simulation. We offer three variance reduction methods -- change of measure, acceptance-rejection, and conditional Monte Carlo. Numerical experiments suggest that the conditional Monte Carlo method should be preferred.
Study of Dalitz decay phi -> eta e+e- with KLOE detector<|sep|>We have studied the vector to pseudoscalar conversion decay phi -> eta e+e-, with eta -> pi0pi0pi0, with the KLOE detector at DAPHNE. The data set of 1.7 fb-1 of e+e- collisions at sqrt(s)~Mphi contains a clear conversion decay signal of ~31,000 events from which we measured a value of BR(phi -> eta e+e-)=(1.075+-0.007+-0.038)x10-4. The same sample is used to determine the transition form factor by a fit to the e+e- invariant mass spectrum, obtaining b(phi eta) =(1.17 +- 0.10 + 0.07) GeV-2, that improves by a factor of five the precision of the previous measurement and is in good agreement with VMD expectations.
The 125 GeV Higgs signal at the LHC in the CP Violating MSSM<|sep|>The ATLAS and CMS collaborations have observed independently at the Large Hadron Collider (LHC) a new Higgs-like particle with a mass $M_h \sim$ 125 GeV and properties similar to that predicted by the Standard Model (SM). Although the measurements indicate that this Higgs-like boson is compatible with the SM hypothesis, however due to large uncertainties in some of the Higgs detection channels, one still has the possibility of testing this object as being a candidate for some Beyond the SM (BSM) physics scenarios, for example, the Minimal Supersymmetric Standard Model (MSSM), in the CP-conserving version (CPC-MSSM). In this paper, we evaluate the modifications of these CPC-MSSM results when CP-violating (CPV) phases are turned on explicitly, leading to the CP-violating MSSM (CPV-MSSM). We investigate the role of the CPV phases in (some of) the soft Supersymmetry (SUSY) terms on both the mass of the lightest Higgs boson $h_1$, and the rates for the processes $gg \rightarrow h_1 \rightarrow \gamma \gamma$, $gg \rightarrow h_1 \rightarrow ZZ^*\rightarrow 4l$, $gg \rightarrow h_1 \rightarrow WW^*\rightarrow l \nu l \nu$, $pp \rightarrow V h_1 \rightarrow V b\bar b$ and $pp \rightarrow V h_1 \rightarrow V \tau^+\tau^-$, ($V \equiv W^\pm, Z$) at the LHC, considering the impact of the flavor constraints as well as the constraints coming from the Electric Dipole Moment (EDM) measurements. We find that the imaginary part of the top and bottom Yukawa couplings can take very small but non-zero values even after satisfying the recent updates from LHC. Our study shows that the CPV-MSSM provides an equally potential solution (like its CP-conserving (CPC) counterpart) to the recent LHC Higgs data. Improvement in different Higgs coupling measurements is necessary in order to test the possibility of probing the small dependence on these CPV phases in the Higgs sector of the MSSM.
Heavy-Flavor-Conserving Hadronic Weak Decays of Heavy Baryons<|sep|>More than two decades ago, we studied heavy-flavor-conserving weak decays of heavy baryons within the framework that incorporates both heavy-quark and chiral symmetries. In view of the first observation of $\Xi_b^-\to\Lambda_b^0\pi^-$ by LHCb recently, we have reexamined these decays and presented updated predictions. The predicted rates for $\Xi_b^-\to\Lambda_b^0\pi^-$ in the MIT bag and diquark models are consistent with experiment. The major theoretical uncertainty stems from the evaluation of baryon matrix elements. The branching fraction of $\Xi_c\to\Lambda_c\pi$ is predicted to be of order $10^{-4}$. It is suppressed relative to $Br(\Xi_b\to\Lambda_b\pi)$ owing to the shorter lifetime of $\Xi_c$ relative to $\Xi_b$ and the destructive nonspectator $W$-exchange contribution. The kinematically accessible weak decays of the sextet heavy baryon $\Omega_Q$ are $\Omega_Q\to\Xi_Q\pi$. Due to the absence of the $B_6-B_{\bar 3}$ transition in the heavy quark limit and the $B_6-B_6$ transition in the model calculations, $\Omega_Q\to\Xi_Q\pi$ vanish in the heavy quark limit.
Enhanced Neck Feature Representation for Object Detection in Aerial Images<|sep|>Object detection in aerial images is a fundamental research topic in the geoscience and remote sensing domain. However, the advanced approaches on this topic mainly focus on designing the elaborate backbones or head networks but ignore neck networks. In this letter, we first underline the importance of the neck network in object detection from the perspective of information bottleneck. Then, to alleviate the information deficiency problem in the current approaches, we propose a global semantic network (GSNet), which acts as a bridge from the backbone network to the head network in a bidirectional global pattern. Compared to the existing approaches, our model can capture the rich and enhanced image features with less computational costs. Besides, we further propose a feature fusion refinement module (FRM) for different levels of features, which are suffering from the problem of semantic gap in feature fusion. To demonstrate the effectiveness and efficiency of our approach, experiments are carried out on two challenging and representative aerial image datasets (i.e., DOTA and HRSC2016). Experimental results in terms of accuracy and complexity validate the superiority of our method. The code has been open-sourced at GSNet.
The Inconvenient Truths of Ground Truth for Binary Analysis<|sep|>The effectiveness of binary analysis tools and techniques is often measured with respect to how well they map to a ground truth. We have found that not all ground truths are created equal. This paper challenges the binary analysis community to take a long look at the concept of ground truth, to ensure that we are in agreement with definition(s) of ground truth, so that we can be confident in the evaluation of tools and techniques. This becomes even more important as we move to trained machine learning models, which are only as useful as the validity of the ground truth in the training.
Estimating Structural Disparities for Face Models<|sep|>In machine learning, disparity metrics are often defined by measuring the difference in the performance or outcome of a model, across different sub-populations (groups) of datapoints. Thus, the inputs to disparity quantification consist of a model's predictions $\hat{y}$, the ground-truth labels for the predictions $y$, and group labels $g$ for the data points. Performance of the model for each group is calculated by comparing $\hat{y}$ and $y$ for the datapoints within a specific group, and as a result, disparity of performance across the different groups can be calculated. In many real world scenarios however, group labels ($g$) may not be available at scale during training and validation time, or collecting them might not be feasible or desirable as they could often be sensitive information. As a result, evaluating disparity metrics across categorical groups would not be feasible. On the other hand, in many scenarios noisy groupings may be obtainable using some form of a proxy, which would allow measuring disparity metrics across sub-populations. Here we explore performing such analysis on computer vision models trained on human faces, and on tasks such as face attribute prediction and affect estimation. Our experiments indicate that embeddings resulting from an off-the-shelf face recognition model, could meaningfully serve as a proxy for such estimation.
QuSBT: Search-Based Testing of Quantum Programs<|sep|>Generating a test suite for a quantum program such that it has the maximum number of failing tests is an optimization problem. For such optimization, search-based testing has shown promising results in the context of classical programs. To this end, we present a test generation tool for quantum programs based on a genetic algorithm, called QuSBT (Search-based Testing of Quantum Programs). QuSBT automates the testing of quantum programs, with the aim of finding a test suite having the maximum number of failing test cases. QuSBT utilizes IBM's Qiskit as the simulation framework for quantum programs. We present the tool architecture in addition to the implemented methodology (i.e., the encoding of the search individual, the definition of the fitness function expressing the search problem, and the test assessment w.r.t. two types of failures). Finally, we report results of the experiments in which we tested a set of faulty quantum programs with QuSBT to assess its effectiveness. Repository (code and experimental results): https://github.com/Simula-COMPLEX/qusbt-tool Video: https://youtu.be/3apRCtluAn4
Scintillating bolometers based on ZnMoO$_4$ and Zn$^{100}$MoO$_4$ crystals to search for 0$\nu$2$\beta$ decay of $^{100}$Mo (LUMINEU project): first tests at the Modane Underground Laboratory<|sep|>The technology of scintillating bolometers based on zinc molybdate (ZnMoO$_4$) crystals is under development within the LUMINEU project to search for 0$\nu$2$\beta$ decay of $^{100}$Mo with the goal to set the basis for large scale experiments capable to explore the inverted hierarchy region of the neutrino mass pattern. Advanced ZnMoO$_4$ crystal scintillators with mass of $\sim$~0.3 kg were developed and Zn$^{100}$MoO$_4$ crystal from enriched $^{100}$Mo was produced for the first time by using the low-thermal-gradient Czochralski technique. One ZnMoO$_4$ scintillator and two samples (59 g and 63 g) cut from the enriched boule were tested aboveground at milli-Kelvin temperature as scintillating bolometers showing a high detection performance. The first results of the low background measurements with three ZnMoO$_4$ and two enriched detectors installed in the EDELWEISS set-up at the Modane Underground Laboratory (France) are presented.
The accreted stellar halo as a window on halo assembly in L* galaxies<|sep|>Theory and observations agree that the accreted stellar halos (ASHs) of Milky Way-like galaxies display significant scatter. I take advantage of this stochasticity to invert the link between halo assembly history (HAH) and ASH, using mock ASHs corresponding to 750 $\Lambda$CDM HAHs, sharing a final virial mass of $M_{h}(z=0)=10^{12.25}M_\odot$. Hosts with poor/rich ASHs assemble following orthogonal growth-patterns. Hosts with rich ASHs experience accretion events (AEs) with high virial mass ratios (HVMRs, $M_s/M_h\gtrsim 0.1$) at $0.5\lesssim z_{infall}\lesssim1.5$, in a phase of fast growth. This maximizes the accreted stellar mass under the condition these satellites are disrupted by $z=0$. At similar times, hosts with poor ASHs grow slowly through minor mergers, with only very recent HVMR AEs: this results in a globally more abundant satellite population and in distinctive surviving massive satellites (stellar mass $\log M_{s,*}/M_\odot\gtrsim 9$). Several properties of the Milky Way are in agreement with the predictions of this framework for hosts with poor, concentrated ASHs, including: i) the recent infall of Sagittarius and Magellanic Clouds, ii) the likely higher-than-average concentration of its dark halo, iii) the signatures of fast chemical enrichment of a sizable fraction of its halo stellar populations.
Chaos in a non-autonomous nonlinear system describing asymmetric water wheels<|sep|>We use physical principles to derive a water wheel model under the assumption of an asymmetric water wheel for which the water inflow rate is in general unsteady (modeled by an arbitrary function of time). Our model allows one to recover the asymmetric water wheel with steady flow rate, as well as the symmetric water wheel, as special cases. Under physically reasonable assumptions we then reduce the underlying model into a non-autonomous nonlinear system. In order to determine parameter regimes giving chaotic dynamics in this non-autonomous nonlinear system, we consider an application of competitive modes analysis. In order to apply this method to a non-autonomous system, we are required to generalize the competitive modes analysis so that it is applicable to non-autonomous systems. The non-autonomous nonlinear water wheel model is shown to satisfy competitive modes conditions for chaos in certain parameter regimes, and we employ the obtained parameter regimes to construct the chaotic attractors. As anticipated, the asymmetric unsteady water wheel exhibits more disorder than does the asymmetric steady water wheel, which in turn is less regular than the symmetric steady state water wheel. Our results suggest that chaos should be fairly ubiquitous in the asymmetric water wheel model with unsteady inflow of water.
Dense molecular cloud cores as a source of micrometer-sized grains in galaxies<|sep|>Coreshine in dense molecular cloud cores (dense cores) is interpreted as evidence for micrometer-sized grains (referred to as very large grains, VLGs). VLGs may have a significant influence on the total dust amount and the extinction curve. We estimate the total abundance of VLGs in the Galaxy, assuming that dense cores are the site of VLG formation. We find that the VLG abundance relative to the total dust mass is roughly $\phi_\mathrm{VLG}\sim 0.01(1-\epsilon )/\epsilon (\tau_\mathrm{SF}/5\times 10^9~\mathrm{yr})^{-1} (f_\mathrm{VLG}/0.5)(t_\mathrm{shat}/10^8~\mathrm{yr})$, where $\epsilon$ is the star formation efficiency in dense cores, $\tau_\mathrm{SF}$ the timescale of gas consumption by star formation, $f_\mathrm{VLG}$ the fraction of dust mass eventually coagulated into VLGs in dense cores, and $t_\mathrm{shat}$ the lifetime of VLGs (determined by shattering). Adopting their typical values for the Galaxy, we obtain $\phi_\mathrm{VLG}\sim 0.02$--0.09. This abundance is well below the value detected in the heliosphere by Ulysses and Galileo, which means that local enhancement of VLG abundance in the solar neighborhood is required if the VLGs originate from dense cores. We also show that the effects of VLGs on the extinction curve are negligible even with the upper value of the above range, $\phi_\mathrm{VLG}\sim 0.09$. If we adopt an extreme value, $\phi_\mathrm{VLG}\sim 0.5$, close to that inferred from the above spacecraft data, the extinction curve is still in the range of the variation in Galactic extinction curves, but is not typical of the diffuse ISM.
Performance Bounds for the Scenario Approach and an Extension to a Class of Non-convex Programs<|sep|>We consider the Scenario Convex Program (SCP) for two classes of optimization problems that are not tractable in general: Robust Convex Programs (RCPs) and Chance-Constrained Programs (CCPs). We establish a probabilistic bridge from the optimal value of SCP to the optimal values of RCP and CCP in which the uncertainty takes values in a general, possibly infinite dimensional, metric space. We then extend our results to a certain class of non-convex problems that includes, for example, binary decision variables. In the process, we also settle a measurability issue for a general class of scenario programs, which to date has been addressed by an assumption. Finally, we demonstrate the applicability of our results on a benchmark problem and a problem in fault detection and isolation.
Vertical kinematics of the thick disc at 4.5 < R < 9.5 kpc<|sep|>We explored a method to reconstruct the distribution function of the Galactic thick disc within the action space where nearby thick-disc stars are distributed. By applying this method to 127 chemically-selected thick-disc stars in the Solar neighbourhood, we found that the vertical velocity dispersion that corresponds to the reconstructed distribution function declines approximately as $\exp (-R/R_s)$ at 4.5 kpc < R < 9.5 kpc, with $R_s$ = 8.3 $\pm$ 1.1 (rand.) $\pm$ 1.6 (sys.) kpc. Also, we found that the vertical velocity dispersion $\sigma_z$ of our local thick-disc stars shows only weak dependency on radial and azimuthal velocities $(v_R, v_\phi)$. We discuss possible implications of these results on the global structure of the Milky Way thick disc.
Capture of Irregular Satellites at Jupiter<|sep|>The irregular satellites of outer planets are thought to have been captured from heliocentric orbits. The exact nature of the capture process, however, remains uncertain. We examine the possibility that irregular satellites were captured from the planetesimal disk during the early Solar System instability when encounters between the outer planets occurred (Nesvorny, Vokrouhlicky & Morbidelli 2007, AJ 133; hereafter NVM07). NVM07 already showed that the irregular satellites of Saturn, Uranus and Neptune were plausibly captured during planetary encounters. Here we find that the current instability models present favorable conditions for capture of irregular satellites at Jupiter as well, mainly because Jupiter undergoes a phase of close encounters with an ice giant. We show that the orbital distribution of bodies captured during planetary encounters provides a good match to the observed distribution of irregular satellites at Jupiter. The capture efficiency for each particle in the original transplanetary disk is found to be (1.3-3.6)x10^-8. This is roughly enough to explain the observed population of jovian irregular moons. We also confirm NVM07's results for the irregular satellites of Saturn, Uranus and Neptune.
When perceptual time stands still: Long stable memory in binocular rivalry<|sep|>We have carried out binocular rivalry experiments with a large number of subjects to obtain high quality statistics on probability distribution of dominance duration (PDDD) for two cases where (a) the rival stimulus is continuously presented and (b) the rival stimulus is periodically removed, with stimulus-on and stimulus-off intervals Ton and Toff respectively. It is shown that the PDDD obtained for the latter case can be reproduced to a reasonable degree of approximation by simply using the PDDD of part (a) and slicing it at pieces of time extent Ton and by introducing intervals of length Toff between the on-intervals where the PDDD is set to zero. This suggests that the variables representing the perceptual state do not change significantly during long blank intervals. We argue that these findings impose challenges to theoretical models which aim at describing visual perception.
Towards The Development of a Bishnupriya Manipuri Corpus<|sep|>For any deep computational processing of language we need evidences, and one such set of evidences is corpus. This paper describes the development of a text-based corpus for the Bishnupriya Manipuri language. A Corpus is considered as a building block for any language processing tasks. Due to the lack of awareness like other Indian languages, it is also studied less frequently. As a result the language still lacks a good corpus and basic language processing tools. As per our knowledge this is the first effort to develop a corpus for Bishnupriya Manipuri language.
Resummation in (F)APT<|sep|>We present new results on the summation of nonpower-series expansions in QCD Analytic Perturbation Theory (APT) in the one-loop approximation. We show how to generalize the approach suggested by one of us earlier to the cases of APT and Fractional APT (FAPT) with heavy-quark thresholds. Using this approach we analyze the Higgs boson decay $H^0\to\bar{b}b$. We produce estimations of the higher-order corrections importance in (F)APT and present a very transparent interpretation of the resummation results in terms of $\Lambda_\text{QCD}$ shifts.
Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D Object Detection<|sep|>As a crucial task of autonomous driving, 3D object detection has made great progress in recent years. However, monocular 3D object detection remains a challenging problem due to the unsatisfactory performance in depth estimation. Most existing monocular methods typically directly regress the scene depth while ignoring important relationships between the depth and various geometric elements (e.g. bounding box sizes, 3D object dimensions, and object poses). In this paper, we propose to learn geometry-guided depth estimation with projective modeling to advance monocular 3D object detection. Specifically, a principled geometry formula with projective modeling of 2D and 3D depth predictions in the monocular 3D object detection network is devised. We further implement and embed the proposed formula to enable geometry-aware deep representation learning, allowing effective 2D and 3D interactions for boosting the depth estimation. Moreover, we provide a strong baseline through addressing substantial misalignment between 2D annotation and projected boxes to ensure robust learning with the proposed geometric formula. Experiments on the KITTI dataset show that our method remarkably improves the detection performance of the state-of-the-art monocular-based method without extra data by 2.80% on the moderate test setting. The model and code will be released at https://github.com/YinminZhang/MonoGeo.
Can ROS be used securely in industry? Red teaming ROS-Industrial<|sep|>With its growing use in industry, ROS is rapidly becoming a standard in robotics. While developments in ROS 2 show promise, the slow adoption cycles in industry will push widespread ROS 2 industrial adoption years from now. ROS will prevail in the meantime which raises the question: can ROS be used securely for industrial use cases even though its origins didn't consider it? The present study analyzes this question experimentally by performing a targeted offensive security exercise in a synthetic industrial use case involving ROS-Industrial and ROS packages. Our exercise results in four groups of attacks which manage to compromise the ROS computational graph, and all except one take control of most robotic endpoints at desire. To the best of our knowledge and given our setup, results do not favour the secure use of ROS in industry today, however, we managed to confirm that the security of certain robotic endpoints hold and remain optimistic about securing ROS industrial deployments.
Evaluating Perceptual Bias During Geometric Scaling of Scatterplots<|sep|>Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.
Nucleon and nucleon-pair momentum distributions in A <= 12 nuclei<|sep|>We report variational Monte Carlo calculations of single-nucleon momentum distributions for A <= 12 nuclei and nucleon-pair and nucleon-cluster momentum distributions for A <= 8. The wave functions have been generated for a Hamiltonian containing the Argonne v18 two-nucleon and Urbana X three-nucleon potentials. The single-nucleon and nucleon-pair momentum distributions exhibit universal features attributable to the one-pion-exchange tensor interaction. The single-nucleon distributions are broken down into proton and neutron components and spin-up and spin-down components where appropriate. The nucleon-pair momentum distributions are given separately for pp and pn pairs. The nucleon-cluster momentum distributions include dp in 3He, tp and dd in 4He, alpha-d in 6Li, alpha-t in 7Li, and alpha-alpha in 8Be. Detailed tables are provided on-line for download.
A preserved high-z compact progenitor in the heart of NGC3311 revealed with MUSE 2D stellar population analysis<|sep|>Massive early-type galaxies are believed to be the end result of an extended mass accretion history. The stars formed in situ very early on in the initial phase of the assembly might have originated from an extremely intense star formation burst, and may still be found within the cores of such galaxies today. We investigate the presence of a surviving high-$z$ compact progenitor component in the brightest galaxy of the Hydra I cluster, NGC 3311, by mapping its 2D kinematics and stellar population out to 2 effective radii, combining MUSE observations, extended EMILES models, and a newly developed parametric fully Bayesian framework using full-spectrum fitting. We present 2D maps and radial profiles of the stellar velocity dispersion, age, total metallicity, $\alpha$-element, sodium abundance ([Na/Fe]), and the initial mass function (IMF) slope. All properties have significant gradients, confirming the existence of multiple structural components, including a young, metal-rich "blue spot". We find that the component dominating the light budget of NGC 3311 within $R\lesssim 2.0$ kpc is the surviving $z=0$ analog of a high-$z$ compact core. This concentrated structure has a relatively small velocity dispersion ($\sigma_*\approx 180$ km s$^{-1}$), is very old (ages$\gtrsim 11$ Gyr), metal-rich ([Z/H]$\sim0.2$ and [Na/Fe]$\sim0.4$), and has a bottom-heavy IMF (with slope $\Gamma_b\sim2.4$). In the outer region, stars become increasingly hotter, younger, metal and sodium poorer, $\alpha$-element richer, and the IMF slope becomes Chabrier-like. The multiple structural components in NGC 3311 confirm the predictions from the two-phase formation scenario for NGC 3311. Interestingly, the outer stellar population has an overabundant [$\alpha$/Fe], most likely because NGC 3311, located at the center of the galaxy cluster, accreted stars from rapidly quenched satellites.[Abridged]
Interplay of fixed points in scalar models<|sep|>We performed the renormalization group analysis of scalar models exhibiting spontaneous symmetry breaking. It is shown that an infrared fixed point appears in the broken symmetric phase of the models, which induces a dynamical scale, that can be identified with the correlation length. This enables one to identify the type of the phase transition which shows similarity to the one appearing in the crossover scale. The critical exponent $\nu$ of the correlation length also proved to be equal in the crossover and the infrared scaling regimes.
Fast solution of Sylvester-structured systems for spatial source separation of the Cosmic Microwave Background<|sep|>Implementation of many statistical methods for large, multivariate data sets requires one to solve a linear system that, depending on the method, is of the dimension of the number of observations or each individual data vector. This is often the limiting factor in scaling the method with data size and complexity. In this paper we illustrate the use of Krylov subspace methods to address this issue in a statistical solution to a source separation problem in cosmology where the data size is prohibitively large for direct solution of the required system. Two distinct approaches are described: one that uses the method of conjugate gradients directly to the Kronecker-structured problem and another that reformulates the system as a Sylvester matrix equation. We show that both approaches produce an accurate solution within an acceptable computation time and with practical memory requirements for the data size that is currently available.
Reconstruction of potential energy profiles from multiple rupture time distributions<|sep|>We explore the mathematical and numerical aspects of reconstructing a potential energy profile of a molecular bond from its rupture time distribution. While reliable reconstruction of gross attributes, such as the height and the width of an energy barrier, can be easily extracted from a single first passage time (FPT) distribution, the reconstruction of finer structure is ill-conditioned. More careful analysis shows the existence of optimal bond potential amplitudes (represented by an effective Peclet number) and initial bond configurations that yield the most efficient numerical reconstruction of simple potentials. Furthermore, we show that reconstruction of more complex potentials containing multiple minima can be achieved by simultaneously using two or more measured FPT distributions, obtained under different physical conditions. For example, by changing the effective potential energy surface by known amounts, additional measured FPT distributions improve the reconstruction. We demonstrate the possibility of reconstructing potentials with multiple minima, motivate heuristic rules-of-thumb for optimizing the reconstruction, and discuss further applications and extensions.
Markov processes and generalized Schroedinger equations<|sep|>Starting from the forward and backward infinitesimal generators of bilateral, time-homogeneous Markov processes, the self-adjoint Hamiltonians of the generalized Schroedinger equations are first introduced by means of suitable Doob transformations. Then, by broadening with the aid of the Dirichlet forms the results of the Nelson stochastic mechanics, we prove that it is possible to associate bilateral, and time-homogeneous Markov processes to the wave functions stationary solutions of our generalized Schroedinger equations. Particular attention is then paid to the special case of the Levy-Schroedinger equations and to their associated Levy-type Markov processes, and to a few examples of Cauchy background noise.
SymbioCity: Smart Cities for Smarter Networks<|sep|>The "Smart City" (SC) concept revolves around the idea of embodying cutting-edge ICT solutions in the very fabric of future cities, in order to offer new and better services to citizens while lowering the city management costs, both in monetary, social, and environmental terms. In this framework, communication technologies are perceived as subservient to the SC services, providing the means to collect and process the data needed to make the services function. In this paper, we propose a new vision in which technology and SC services are designed to take advantage of each other in a symbiotic manner. According to this new paradigm, which we call "SymbioCity", SC services can indeed be exploited to improve the performance of the same communication systems that provide them with data. Suggestive examples of this symbiotic ecosystem are discussed in the paper. The dissertation is then substantiated in a proof-of-concept case study, where we show how the traffic monitoring service provided by the London Smart City initiative can be used to predict the density of users in a certain zone and optimize the cellular service in that area.
Size matters? Or not: A/B testing with limited sample in automotive embedded software<|sep|>A/B testing is gaining attention in the automotive sector as a promising tool to measure causal effects from software changes. Different from the web-facing businesses, where A/B testing has been well-established, the automotive domain often suffers from limited eligible users to participate in online experiments. To address this shortcoming, we present a method for designing balanced control and treatment groups so that sound conclusions can be drawn from experiments with considerably small sample sizes. While the Balance Match Weighted method has been used in other domains such as medicine, this is the first paper to apply and evaluate it in the context of software development. Furthermore, we describe the Balance Match Weighted method in detail and we conduct a case study together with an automotive manufacturer to apply the group design method in a fleet of vehicles. Finally, we present our case study in the automotive software engineering domain, as well as a discussion on the benefits and limitations of the A/B group design method.
Using Deep Learning for price prediction by exploiting stationary limit order book features<|sep|>The recent surge in Deep Learning (DL) research of the past decade has successfully provided solutions to many difficult problems. The field of quantitative analysis has been slowly adapting the new methods to its problems, but due to problems such as the non-stationary nature of financial data, significant challenges must be overcome before DL is fully utilized. In this work a new method to construct stationary features, that allows DL models to be applied effectively, is proposed. These features are thoroughly tested on the task of predicting mid price movements of the Limit Order Book. Several DL models are evaluated, such as recurrent Long Short Term Memory (LSTM) networks and Convolutional Neural Networks (CNN). Finally a novel model that combines the ability of CNNs to extract useful features and the ability of LSTMs' to analyze time series, is proposed and evaluated. The combined model is able to outperform the individual LSTM and CNN models in the prediction horizons that are tested.
Representing and decomposing genomic structural variants as balanced integer flows on sequence graphs<|sep|>The study of genomic variation has provided key insights into the functional role of mutations. Predominantly, studies have focused on single nucleotide variants (SNV), which are relatively easy to detect and can be described with rich mathematical models. However, it has been observed that genomes are highly plastic, and that whole regions can be moved, removed or duplicated in bulk. These structural variants (SV) have been shown to have significant impact on the phenotype, but their study has been held back by the combinatorial complexity of the underlying models. We describe here a general model of structural variation that encompasses both balanced rearrangements and arbitrary copy-numbers variants (CNV). In this model, we show that the space of possible evolutionary histories that explain the structural differences between any two genomes can be sampled ergodically.
B -> tau nu: Opening up the Charged Higgs Parameter Space with R-parity Violation<|sep|>The theoretically clean channel B+ -> tau+ nu shows a close to 3sigma discrepancy between the Standard Model prediction and the data. This in turn puts a strong constraint on the parameter space of a two-Higgs doublet model, including R-parity conserving supersymmetry. The constraint is so strong that it almost smells of fine-tuning. We show how the parameter space opens up with the introduction of suitable R-parity violating interactions, and release the tension between data and theory.
An Empirical Study on Post-processing Methods for Word Embeddings<|sep|>Word embeddings learnt from large corpora have been adopted in various applications in natural language processing and served as the general input representations to learning systems. Recently, a series of post-processing methods have been proposed to boost the performance of word embeddings on similarity comparison and analogy retrieval tasks, and some have been adapted to compose sentence representations. The general hypothesis behind these methods is that by enforcing the embedding space to be more isotropic, the similarity between words can be better expressed. We view these methods as an approach to shrink the covariance/gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimising an objective in the semi-Riemannian manifold with Centralised Kernel Alignment (CKA), we are able to search for the optimal shrinkage parameter, and provide a post-processing method to smooth the spectrum of learnt word vectors which yields improved performance on downstream tasks.
Alquist 3.0: Alexa Prize Bot Using Conversational Knowledge Graph<|sep|>The third version of the open-domain dialogue system Alquist developed within the Alexa Prize 2020 competition is designed to conduct coherent and engaging conversations on popular topics. The main novel contribution is the introduction of a system leveraging an innovative approach based on a conversational knowledge graph and adjacency pairs. The conversational knowledge graph allows the system to utilize knowledge expressed during the dialogue in consequent turns and across conversations. Dialogue adjacency pairs divide the conversation into small conversational structures, which can be combined and allow the system to react to a wide range of user inputs flexibly. We discuss and describe Alquist's pipeline, data acquisition and processing, dialogue manager, NLG, knowledge aggregation, and a hierarchy of adjacency pairs. We present the experimental results of the individual parts of the system.
Low-traffic limit and first-passage times for a simple model of the continuous double auction<|sep|>We consider a simplified model of the continuous double auction where prices are integers varying from $1$ to $N$ with limit orders and market orders, but quantity per order limited to a single share. For this model, the order process is equivalent to two $M/M/1$ queues. We study the behaviour of the auction in the low-traffic limit where limit orders are immediately transformed into market orders. In this limit, the distribution of prices can be computed exactly and gives a reasonable approximation of the price distribution when the ratio between the rate of order arrivals and the rate of order executions is below $1/2$. This is further confirmed by the analysis of the first passage time in $1$ or $N$.
A Continuous-Time Markov Chain Model for the Spread of COVID-19<|sep|>Since late 2019 the novel coronavirus, also known as COVID-19, has caused a pandemic that persists. This paper shows how a continuous-time Markov chain model for the spread of COVID-19 can be used to explain, and justify to undergraduate students, strategies now being used in attempts to control the virus. The material in the paper is written at the level of students who are taking an introductory course on the theory and applications of stochastic processes.
Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling<|sep|>The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of sub-images of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models. To address the latter challenge, we propose to use Multidimensional Upscaling to grow an image in both size and depth via intermediate stages utilising distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets.
Thermal entropy of a quark-antiquark pair above and below deconfinement from a dynamical holographic QCD model<|sep|>We discuss the entropy carried by a quark-antiquark pair, in particular across the deconfinement transition. We therefore rely on a self-consistent solution to Einstein-Maxwell-dilaton gravity, capable of mimicking essential features of QCD. In particular we introduce a novel model that still captures well the QCD confinement and deconfinement phases, while allowing the introduction of a temperature in a phase which resembles the confined phase, this thanks to it being dual to a small black hole. We pay due attention to some subtleties of such model. We confirm the lattice picture of a strong build-up of thermal entropy towards the critical temperature T_c, both coming from below or above T_c. We also include a chemical potential, confirming this entropic picture and we consider its effect on the speed of sound. Moreover, the temperature dependent confinement phase from the holography side allows us to find a string tension that does not vanish at T_c, a finding also supported by lattice QCD.
Diffusing-Horizon Model Predictive Control<|sep|>We analyze a time-coarsening strategy for model predictive control (MPC) that we call diffusing-horizon MPC. This strategy seeks to overcome the computational challenges associated with optimal control problems that span multiple timescales. The coarsening approach uses a time discretization grid that becomes exponentially more sparse as one moves forward in time. This design is motivated by a recently established property of optimal control problems that is known as exponential decay of sensitivity. This property states that the impact of a parametric perturbation at a future time decays exponentially as one moves backward in time. We establish conditions under which this property holds for a constrained MPC formulation with linear dynamics and costs. Moreover, we show that the proposed coarsening scheme can be cast as a parametric perturbation of the MPC problem and thus the exponential decay condition holds. We use a heating, ventilation, and air conditioning plant case study with real data to demonstrate the proposed approach. Specifically, we show that computational times can be reduced by two orders of magnitude while increasing the closed-loop cost by only 3%.
Unsteady Aerodynamics and Vortex-sheet Formation of A Two-dimensional Airfoil<|sep|>Unsteady inviscid flow models of wings and airfoils have been developed to study the aerodynamics of natural and man-made flyers. Vortex methods have been extensively applied to reduce the dimensionality of these aerodynamic models, based on the proper estimation of the strength and distribution of the vortices in the wake. In such modeling approaches, one of the most fundamental questions is how the vortex sheets are generated and released from sharp edges. To determine the formation of the trailing-edge vortex sheet, the classical Kutta condition can be extended to unsteady situations by realizing that a flow cannot turn abruptly around a sharp edge. This condition can be readily applied to a flat plate or an airfoil with cusped trailing edge since the direction of the forming vortex sheet is known to be tangential to the trailing edge. However, for a finite-angle trailing edge, or in the case of flow separation away from a sharp corner, the direction of the forming vortex sheet is ambiguous. To remove any ad-hoc implementation, the unsteady Kutta condition, the conservation of circulation, as well as the conservation laws of mass and momentum are coupled to analytically solve for the angle, strength, and relative velocity of the trailing-edge vortex sheet. The two-dimensional aerodynamic model together with the proposed vortex-sheet formation condition is verified by comparing flow structures and force calculations with experimental results for airfoils in steady and unsteady background flows.
Extracting Effective Subnetworks with Gumebel-Softmax<|sep|>Large and performant neural networks are often overparameterized and can be drastically reduced in size and complexity thanks to pruning. Pruning is a group of methods, which seeks to remove redundant or unnecessary weights or groups of weights in a network. These techniques allow the creation of lightweight networks, which are particularly critical in embedded or mobile applications. In this paper, we devise an alternative pruning method that allows extracting effective subnetworks from larger untrained ones. Our method is stochastic and extracts subnetworks by exploring different topologies which are sampled using Gumbel Softmax. The latter is also used to train probability distributions which measure the relevance of weights in the sampled topologies. The resulting subnetworks are further enhanced using a highly efficient rescaling mechanism that reduces training time and improves performance. Extensive experiments conducted on CIFAR show the outperformance of our subnetwork extraction method against the related work.
Training Complex Models with Multi-Task Weak Supervision<|sep|>As machine learning models continue to increase in complexity, collecting large hand-labeled training sets has become one of the biggest roadblocks in practice. Instead, weaker forms of supervision that provide noisier but cheaper labels are often used. However, these weak supervision sources have diverse and unknown accuracies, may output correlated labels, and may label different tasks or apply at different levels of granularity. We propose a framework for integrating and modeling such weak supervision sources by viewing them as labeling different related sub-tasks of a problem, which we refer to as the multi-task weak supervision setting. We show that by solving a matrix completion-style problem, we can recover the accuracies of these multi-task sources given their dependency structure, but without any labeled data, leading to higher-quality supervision for training an end model. Theoretically, we show that the generalization error of models trained with this approach improves with the number of unlabeled data points, and characterize the scaling with respect to the task and dependency structures. On three fine-grained classification problems, we show that our approach leads to average gains of 20.2 points in accuracy over a traditional supervised approach, 6.8 points over a majority vote baseline, and 4.1 points over a previously proposed weak supervision method that models tasks separately.
Multiple ferromagnetic transitions and structural distortion in the van-der-Waals ferromagnet VI$_3$ at ambient and finite pressures<|sep|>We present a combined study of zero-field $^{51}$V and $^{127}$I NMR at ambient pressure and specific heat and magnetization measurements under pressure up to 2.08 GPa on bulk single crystals of the van-der-Waals ferromagnet VI$_3$. At ambient pressure, our results consistently demonstrate that VI$_3$ undergoes a structural transition at $T_s \approx $78 K, followed by two subsequent ferromagnetic transitions at $T_{FM1} \approx $50 K and $T_{FM2} \approx $36 K upon cooling. At lowest temperature ($T < T_{FM2}$), two magnetically-ordered V sites exist, whereas only one magnetically-ordered V site is observed for $T_{FM1} < T\,< T_{FM2}$. Whereas $T_{FM1}$ is almost unaffected by external pressure, $T_{FM2}$ is highly responsive to pressure and merges with the $T_{FM1}$ line at $p \approx 0.6 $GPa. At even higher pressures ($p \approx $1.25\,GPa), the $T_{FM2}$ line merges with the structural transition at $T_s$ which becomes moderately suppressed with $p$ for $p < 1.25$ GPa. Taken together, our data point towards a complex magnetic structure and an interesting interplay of magnetic and structural degrees of freedom in VI$_3$.
Studying the nature of the unidentified gamma-ray source HESS J1841-055 with the MAGIC telescopes<|sep|>We investigate the physical nature and origin of the gamma-ray emission from the extended source HESS J1841-055 observed at TeV and GeV energies. We observed HESS J1841-055 at TeV energies for a total effective time of 43 hours with the MAGIC telescopes, in 2012 and 2013. Additionally, we analysed the GeV counterpart making use of about 10 years of Fermi-LAT data. Using both Fermi-LAT and MAGIC, we study both the spectral and energy-dependent morphology of the source for almost four decades of energy. The origin of the gamma-ray emission from this region is investigated using multi-waveband information on sources present in this region, suggested to be associated with this unidentified gamma-ray source. We find that the extended emission at GeV-TeV energies is best described by more than one source model. We also perform the first energy-dependent analysis of the HESS J1841-055 region at GeV-TeV. We find that the emission at lower energies comes from a diffuse or extended component, while the major contribution of gamma rays above 1 TeV arises from the southern part of the source. Moreover, we find that a significant curvature is present in the combined observed spectrum of MAGIC and Fermi-LAT. The first multi-wavelength spectral energy distribution of this unidentified source shows that the emission at GeV-TeV energies can be well explained with both leptonic and hadronic models. For the leptonic scenario, bremsstrahlung is the dominant emission compared to inverse Compton. On the other hand, for the hadronic model, gamma-ray resulting from the decay of neutral pions ($\pi^0$) can explain the observed spectrum. The presence of dense molecular clouds overlapping with HESS J1841-055 makes both bremsstrahlung and $\pi^0$-decay processes the dominant emission mechanisms for the source.
Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization<|sep|>Stochastic compositional optimization generalizes classic (non-compositional) stochastic optimization to the minimization of compositions of functions. Each composition may introduce an additional expectation. The series of expectations may be nested. Stochastic compositional optimization is gaining popularity in applications such as reinforcement learning and meta learning. This paper presents a new Stochastically Corrected Stochastic Compositional gradient method (SCSC). SCSC runs in a single-time scale with a single loop, uses a fixed batch size, and guarantees to converge at the same rate as the stochastic gradient descent (SGD) method for non-compositional stochastic optimization. This is achieved by making a careful improvement to a popular stochastic compositional gradient method. It is easy to apply SGD-improvement techniques to accelerate SCSC. This helps SCSC achieve state-of-the-art performance for stochastic compositional optimization. In particular, we apply Adam to SCSC, and the exhibited rate of convergence matches that of the original Adam on non-compositional stochastic optimization. We test SCSC using the portfolio management and model-agnostic meta-learning tasks.
Modular categories, integrality and Egyptian fractions<|sep|>It is a well-known result of Etingof, Nikshych and Ostrik that there are finitely many inequivalent integral modular categories of any fixed rank $n$. This follows from a double-exponential bound on the maximal denominator in an Egyptian fraction representation of 1. A na\"ive computer search approach to the classification of rank $n$ integral modular categories using this bound quickly overwhelms the computer's memory (for $n\geq 7$). We use a modified strategy: find general conditions on modular categories that imply integrality and study the classification problem in these limited settings. The first such condition is that the order of the twist matrix is 2,3,4 or 6 and we obtain a fairly complete description of these classes of modular categories. The second condition is that the unit object is the only simple non-self-dual object, which is equivalent to odd-dimensionality. In this case we obtain a (linear) improvement on the bounds and employ number-theoretic techniques to obtain a classification for rank at most 11 for odd-dimensional modular categories.
Which tone-mapping operator is the best? A comparative study of perceptual quality<|sep|>Tone-mapping operators (TMO) are designed to generate perceptually similar low-dynamic range images from high-dynamic range ones. We studied the performance of fifteen TMOs in two psychophysical experiments where observers compared the digitally generated tone-mapped images to their corresponding physical scenes. All experiments were performed in a controlled environment and the setups were designed to emphasise different image properties: in the first experiment we evaluated the local relationships among intensity-levels, and in the second one we evaluated global visual appearance among physical scenes and tone-mapped images, which were presented side by side. We ranked the TMOs according to how well they reproduce the results obtained in the physical scene. Our results show that ranking position clearly depends on the adopted evaluation criteria, which implies that, in general, these tone-mapping algorithms consider either local or global image attributes but rarely both. We conclude that a more thorough and standardized evaluation criteria are needed to study all the characteristics of TMOs, as there is ample room for improvement in future developments.
Extended silicate dust emission in PG QSOs<|sep|>This paper addresses the origin of the silicate emission observed in PG QSOs, based on observations with the Spitzer Space Telescope. Scenarios based on the unified model suggest that silicate emission in AGN arises mainly from the illuminated faces of the clouds in the torus at temperatures near sublimation. However, detections of silicate emission in Type 2 QSOs, and the estimated cool dust temperatures, argue for a more extended emission region.To investigate this issue we present the mid-infrared spectra of 23 QSOs. These spectra, and especially the silicate emission features at ~10 and ~18 mu can be fitted using dusty narrow line region (NLR) models and a combination of black bodies. The bolometric luminosities of the QSOs allow us to derive the radial distances and covering factors for the silicate-emitting dust. The inferred radii are 100-200 times larger than the dust sublimation radius, much larger than the expected dimensions of the inner torus. Our QSO mid-IR spectra are consistent with the bulk of the silicate dust emission arising from the dust in the innermost parts of the NLR.
Future Experiments in Relativistic Heavy Ion Collisions<|sep|>The measurements at RHIC have revealed a new state of matter, which needs to be further characterized in order to better understand its implications for the early evolution of the universe and QCD. I will show that, in the near future, complementary key measurements can be performed at RHIC, LHC, and FAIR. I will focus on results than can be obtained using identified particles, a probe which has been the basis for this conference over the past three decades. The sophisticated detectors, built and planned, for all three accelerator facilities enable us to measure leptons, photons, muons as well as hadrons and resonances of all flavors almost equally well, which makes these experiments unprecedented precision tools for the comprehensive understanding of the physics of the early universe.
Prospects for detection of intermediate-mass black holes in globular clusters using integrated-light spectroscopy<|sep|>The detection of intermediate mass black holes (IMBHs) in Galactic globular clusters (GCs) has so far been controversial. In order to characterize the effectiveness of integrated-light spectroscopy through integral field units, we analyze realistic mock data generated from state-of-the-art Monte Carlo simulations of GCs with a central IMBH, considering different setups and conditions varying IMBH mass, cluster distance, and accuracy in determination of the center. The mock observations are modeled with isotropic Jeans models to assess the success rate in identifying the IMBH presence, which we find to be primarily dependent on IMBH mass. However, even for a IMBH of considerable mass (3% of the total GC mass), the analysis does not yield conclusive results in 1 out of 5 cases, because of shot noise due to bright stars close to the IMBH line-of-sight. This stochastic variability in the modeling outcome grows with decreasing BH mass, with approximately 3 failures out of 4 for IMBHs with 0.1% of total GC mass. Finally, we find that our analysis is generally unable to exclude at 68% confidence an IMBH with mass of $10^3~M_\odot$ in snapshots without a central BH. Interestingly, our results are not sensitive to GC distance within 5-20 kpc, nor to mis-identification of the GC center by less than 2'' (<20% of the core radius). These findings highlight the value of ground-based integral field spectroscopy for large GC surveys, where systematic failures can be accounted for, but stress the importance of discrete kinematic measurements that are less affected by stochasticity induced by bright stars.
Contamination of early-type galaxy alignments to galaxy lensing-CMB lensing cross-correlation<|sep|>Galaxy shapes are subject to distortions due to the tidal field of the Universe. The cross-correlation of galaxy lensing with the lensing of the Cosmic Microwave Background (CMB) cannot easily be separated from the cross-correlation of galaxy intrinsic shapes with CMB lensing. Previous work suggested that the intrinsic alignment contamination can be $15\%$ of this cross-spectrum for the CFHT Stripe 82 (CS82) and Atacama Cosmology Telescope surveys. Here we re-examine these estimates using up-to-date observational constraints of intrinsic alignments at a redshift more similar to that of CS82 galaxies. We find a $\approx$ $10\%$ contamination of the cross-spectrum from red galaxies, with $\approx$ $3\%$ uncertainty due to uncertainties in the redshift distribution of source galaxies and the modelling of the spectral energy distribution. Blue galaxies are consistent with being unaligned, but could contaminate the cross-spectrum by an additional $9.5\%$ within current $95\%$ confidence levels. While our fiducial estimate of alignment contamination is similar to previous work, our work suggests that the relevance of alignments for CMB lensing-galaxy lensing cross-correlation remains largely unconstrained. Little information is currently available about alignments at $z>1.2$. We consider the upper limiting case where all $z>1.2$ galaxies are aligned with the same strength as low redshift luminous red galaxies, finding as much as $\approx$ $60\%$ contamination.
Optimal Nonstationary Reproduction Distribution for Nonanticipative RDF on Abstract Alphabets<|sep|>In this paper we introduce a definition for nonanticipative Rate Distortion Function (RDF) on abstract alphabets, and we invoke weak convergence of probability measures to show various of its properties, such as, existence of the optimal reproduction conditional distribution, compactness of the fidelity set, lower semicontinuity of the RDF functional, etc. Further, we derive the closed form expression of the optimal nonstationary reproduction distribution. This expression is computed recursively backward in time. Throughout the paper we point out an operational meaning of the nonanticipative RDF by recalling the coding theorem derive in \cite{tatikonda2000}, and we state relations to Gorbunov-Pinsker's nonanticipatory $\epsilon-$entropy \cite{gorbunov-pinsker}.
Axionic extension of the Proca action<|sep|>In the context of Cartan theory, we will show that the Proca action can be obtained from the Gauss-Bonnet action for a special choice of the torsion tensor. This in fact equivalent to the special case of the 4th vector Galileon Lagrangian. The theory will then be promoted to contain an axion field. It will be proved that the model admits de Sitter expanding phase with healthy tensor and vector fluctuations. The scalar sector has 4 degrees of freedom, but only one of them remains dynamical in the limit $k\rightarrow\infty$. We will analyze the scalar fluctuations in the small scales limit and obtain the parameter space of the theory in which all the perturbations remain healthy.
Perturbations in Bouncing and Cyclic Models, a General Study<|sep|>Being able to reliably track perturbations across bounces and turnarounds in cyclic and bouncing cosmology lies at the heart of being able to compare the predictions of these models with the Cosmic Microwave Background observations. This has been a challenging task due to the unknown nature of the physics involved during the bounce as well as the technical challenge of matching perturbations precisely between the expansion and contraction phases. In this paper, we will present general techniques (analytical and numerical) that can be applied to understand the physics of the fluctuations, especially those with "long" wavelengths, and test its validity in some simple bouncing/cyclic toy models where the physics is well understood. We will then apply our techniques to more interesting cosmological models such as the bounce inflation and cyclic inflation.
A Novel Hexpyramid Pupil Slicer for an ExAO Parallel DM for the Giant Magellan Telescope<|sep|>The 25.4m Giant Magellan Telescope (GMT) will be amongst the first in a new series of segmented extremely large telescopes (ELTs). The 25.4 m pupil is segmented into seven 8.4 m circular segments in a flower petal pattern. At the University of Arizona we have developed a novel pupil slicer that will be used for ELT extreme adaptive optics (ExAO) on the up and coming ExAO instrument, GMagAO-X. This comes in the form of a six-sided reflective pyramid with a hole through the center known as a "hexpyramid". By passing the GMT pupil onto this reflective optic, the six outer petals will be sent outward in six different directions while the central segment passes through the center. Each segment will travel to its own polarization independent flat fold mirror mounted on a piezoelectric piston/tip/tilt controller then onto its own commercial 3,000 actuator deformable mirror (DM) that will be employed for extreme wavefront control. This scheme of seven DMs working in parallel to produce a 21,000 actuator DM is a new ExAO architecture that we named a "parallel DM," in which the hexpyramid is a key optical component. This significantly surpasses any current or near future actuator count for any monolithic DM architecture. The optical system is designed for high-quality wavefront (lambda/10 surface PV) with no polarization errors and no vignetting. The design and fabrication of the invar mechanical mounting structure for this complex optical system is described in this paper.
Cryptanalysis of a computer cryptography scheme based on a filter bank<|sep|>This paper analyzes the security of a recently-proposed signal encryption scheme based on a filter bank. A very critical weakness of this new signal encryption procedure is exploited in order to successfully recover the associated secret key.
Learning Kernel-Smoothed Machine Translation with Retrieved Examples<|sep|>How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Example Retrieval (KSTER), an effective approach to adapt neural machine translation models online. Experiments on domain adaptation and multi-domain machine translation datasets show that even without expensive retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over the best existing online adaptation methods. The code and trained models are released at https://github.com/jiangqn/KSTER.
Constraints on time variation of fine structure constant from WMAP-3yr data<|sep|>This paper presents the constraints on the time variation of the fine structure constant at recombination relative to its present value, Delta_alpha = (alpha_rec-alpha_0) / alpha_0, obtained from the analysis of the WMAP-3yr Cosmic Microwave Background (CMB) data, with an additional prior on the Hubble expansion rate from HST Hubble Key Project. I found out that -0.039 < Delta_alpha < 0.010 at 95% C.L., which brings a 30% improvement to the previous limits from WMAP-1yr data. The corresponding recombination redshift, 1012 < z_rec < 1115, shows a delayed recombination epoch compared with the results from WMAP-1yr data.
Gap Solitons and Bloch Waves in Nonlinear Periodic Systems<|sep|>We comprehensively investigate gap solitons and Bloch waves in one-dimensional nonlinear periodic systems. Our results show that there exists a composition relation between them: Bloch waves at either the center or edge of the Brillouin zone are infinite chains composed of fundamental gap solitons(FGSs). We argue that such a relation is related to the exact relation between nonlinear Bloch waves and nonlinear Wannier functions. With this composition relation, many conclusions can be drawn for gap solitons without any computation. For example, for the defocusing nonlinearity, there are $n$ families of FGS in the $n$th linear Bloch band gap; for the focusing case, there are infinite number of families of FGSs in the semi-infinite gap and other gaps. In addition, the stability of gap solitons is analyzed. In literature there are numerical results showing that some FGSs have cutoffs on propagation constant (or chemical potential), i.e. these FGSs do not exist for all values of propagation constant (or chemical potential) in the linear band gap. We offer an explanation for this cutoff.
ATLsc with partial observation<|sep|>Alternating-time temporal logic with strategy contexts (ATLsc) is a powerful formalism for expressing properties of multi-agent systems: it extends CTL with strategy quantifiers, offering a convenient way of expressing both collaboration and antagonism between several agents. Incomplete observation of the state space is a desirable feature in such a framework, but it quickly leads to undecidable verification problems. In this paper, we prove that uniform incomplete observation (where all players have the same observation) preserves decidability of the model-checking problem, even for very expressive logics such as ATLsc.
SOFIA/FORCAST Observations of Warm Dust in S106: A Fragmented Environment<|sep|>We present mid-IR (19 - 37 microns) imaging observations of S106 from SOFIA/FORCAST, complemented with IR observations from Spitzer/IRAC (3.6 - 8.0 microns), IRTF/MIRLIN (11.3 and 12.5 microns), and Herschel/PACS (70 and 160 microns). We use these observations, observations in the literature, and radiation transfer modeling to study the heating and composition of the warm (~ 100 K) dust in the region. The dust is heated radiatively by the source S106 IR, with little contributions from grain-electron collisions and Ly-alpha radiation. The dust luminosity is >~ (9.02 +/- 1.01) x 10^4 L_sun, consistent with heating by a mid- to late-type O star. We find a temperature gradient (~ 75 - 107 K) in the lobes, which is consistent with a dusty equatorial geometry around S106 IR. Furthermore, the SOFIA observations resolve several cool (~ 65 - 70 K) lanes and pockets of warmer (~ 75 - 90 K) dust in the ionization shadow, indicating that the environment is fragmented. We model the dust mass as a composition of amorphous silicates, amorphous carbon, big grains, very small grains, and PAHs. We present the relative abundances of each grain component for several locations in S106.
Simulations of Overstable Inertial-acoustic Modes in Black-Hole Accretion Discs<|sep|>We present two-dimensional inviscid hydrodynamic simulations of overstable inertial-acoustic oscillation modes (p-modes) in black-hole accretion discs. These global spiral waves are trapped in the inner-most region of the disc, and are driven overstable by wave absorption at the corotation resonance ($r_c$) when the gradient of the background disc vortensity (vorticity divided by surface density) at $r_c$ is positive and the disc inner boundary is sufficiently reflective. Previous linear calculations have shown that the growth rates of these modes can be as high as 10% of the rotation frequency at the disc inner edge. We confirm these linear growth rates and the primary disc oscillation frequencies in our simulations when the mode amplitude undergoes exponential growth. We show that the mode growth saturates when the radial velocity perturbation becomes comparable to the disc sound speed. During the saturation stage, the primary disc oscillation frequency differs only slightly (by less than a few percent) from the linear mode frequency. Sharp features in the fluid velocity profiles at this stage suggest that the saturation results from nonlinear wave steepening and mode-mode interactions.
Universal recovery maps and approximate sufficiency of quantum relative entropy<|sep|>The data processing inequality states that the quantum relative entropy between two states $\rho$ and $\sigma$ can never increase by applying the same quantum channel $\mathcal{N}$ to both states. This inequality can be strengthened with a remainder term in the form of a distance between $\rho$ and the closest recovered state $(\mathcal{R} \circ \mathcal{N})(\rho)$, where $\mathcal{R}$ is a recovery map with the property that $\sigma = (\mathcal{R} \circ \mathcal{N})(\sigma)$. We show the existence of an explicit recovery map that is universal in the sense that it depends only on $\sigma$ and the quantum channel $\mathcal{N}$ to be reversed. This result gives an alternate, information-theoretic characterization of the conditions for approximate quantum error correction.
Positive and negative electrocaloric effect in BaTiO$_3$ in the presence of defect dipoles<|sep|>The influence of defect dipoles on the electrocaloric effect (ECE) in acceptor doped BaTiO$_3$ is studied by means of lattice-based Monte-Carlo simulations. A Ginzburg-Landau type effective Hamiltonian is used. Oxygen vacancy-acceptor associates are described by fixed defect dipoles with orientation parallel or anti-parallel to the external field. By a combination of canonical and microcanoncial simulations the ECE is directly evaluated. Our results show that in the case of anti-parallel defect dipoles the ECE can be positive or negative depending on the density of defect dipoles. Moreover, a transition from a negative to positive ECE can be observed from a certain density of anti-parallel dipoles on when the external field increases. These transitions are due to the delicate interplay of internal and external fields, and are explained by the domain structure evolution and related field-induced entropy changes. The results are compared to those obtained by MD simulations employing an {\it{ab initio}} based effective Hamiltonian, and a good qualitative agreement is found. In addition, a novel electrocaloric cycle, which makes use of the negative ECE and defect dipoles, is proposed to enhance the cooling effect.
An improved approximation scheme for the centrifugal term and the Hulthen potential<|sep|>We present a new approximation scheme for the centrifugal term to solve the Schrodinger equation with the Hulthen potential for any arbitrary l state by means of a mathematical Nikiforov-Uvarov (NU) method. We obtain the bound state energy eigenvalues and the normalized corresponding eigenfunctions expressed in terms of the Jacobi polynomials or hypergeometric functions for a particle exposed to this potential field. Our numerical results of the energy eigenvalues are found to be in high agreement with those results obtained by using the program based on a numerical integration procedure. The s-wave (l=0) analytic solution for the binding energies and eigenfunctions of a particle are also calculated. The physical meaning of the approximate analytical solution is discussed. The present approximation scheme is systematic and accurate.
Think Global, Act Local: The Influence of Environment Age and Host Mass on Type Ia Supernova Light Curves<|sep|>The reliability of Type Ia supernovae (SNIa) may be limited by the imprint of their galactic origins. To investigate the connection between supernovae and their host characteristics, we developed an improved method to estimate the stellar population age of the host as well as the local environment around the site of the supernova. We use a Bayesian method to estimate the star formation history and mass weighted age of a supernova's environment by matching observed spectral energy distributions to a synthesized stellar population. Applying this age estimator to both the photometrically and spectroscopically classified Sloan Digital Sky Survey II supernovae (N=103) we find a $0.114 \pm 0.039~{\rm mag}$ `step' in the average Hubble residual at a stellar age of $\sim 8~\text{Gyr}$; it is nearly twice the size of the currently popular mass step. We then apply a principal component analysis on the SALT2 parameters, host stellar mass, and local environment age. We find that a new parameter, PC$_1$, consisting of a linear combination of stretch, host stellar mass, and local age, shows a very significant ($4.7\sigma$) correlation with Hubble residuals. There is a much broader range of PC$_1$ values found in the Hubble flow sample when compared with the Cepheid calibration galaxies. These samples have mildly statistically different average PC$_1$ values, at $\sim 2.5\sigma$, resulting in at most a 1.3% reduction in the evaluation of H$_0$. Despite accounting for the highly significant trend in SNIa Hubble residuals, there remains a 9% discrepancy between the most recent precision estimates of H$_0$ using SNIa and the CMB.
Analytical properties and exact solutions of the Lotka--Volterra competition system<|sep|>The Lotka--Volterra competition system with diffusion is considered. The Painlev\'e property of this system is investigated. Exact traveling wave solutions of the Lotka--Volterra competition system are found. Periodic solutions expressed in terms of the Weierstrass elliptic function are also given.
Superconductivity in Engineered Two-Dimensional Electron Gases<|sep|>We consider Kohn-Luttinger mechanism for superconductivity in a two-dimensional electron gas confined to a narrow well between two metallic planes with two occupied subbands with Fermi momenta $k_{FL} > k_{FS}$. On the basis of a perturbative analysis, we conclude that non-s-wave superconductivity emerges even when the bands are parabolic. We analyze the conditions that maximize $T_c$ as a function of the distance to the metallic planes, the ratio $k_{FL}/k_{FS}$, and $r_s$, which measures the strength of Coulomb correlations. The largest attraction is in p-wave and d-wave channels, of which p-wave is typically the strongest. For $r_s = O(1)$ we estimate that the dimensionless coupling $\lambda \approx 10^{-1}$, but it likely continues increasing for larger $r_s$ (where we lose theoretical control).
Neutrino-driven Turbulent Convection and Standing Accretion Shock Instability in Three-Dimensional Core-Collapse Supernovae<|sep|>We conduct a series of numerical experiments into the nature of three-dimensional (3D) hydrodynamics in the postbounce stalled-shock phase of core-collapse supernovae using 3D general-relativistic hydrodynamic simulations of a $27$-$M_\odot$ progenitor star with a neutrino leakage/heating scheme. We vary the strength of neutrino heating and find three cases of 3D dynamics: (1) neutrino-driven convection, (2) initially neutrino-driven convection and subsequent development of the standing accretion shock instability (SASI), (3) SASI dominated evolution. This confirms previous 3D results of Hanke et al. 2013, ApJ 770, 66 and Couch & Connor 2014, ApJ 785, 123. We carry out simulations with resolutions differing by up to a factor of $\sim$4 and demonstrate that low resolution is artificially favorable for explosion in the 3D convection-dominated case, since it decreases the efficiency of energy transport to small scales. Low resolution results in higher radial convective fluxes of energy and enthalpy, more fully buoyant mass, and stronger neutrino heating. In the SASI-dominated case, lower resolution damps SASI oscillations. In the convection-dominated case, a quasi-stationary angular kinetic energy spectrum $E(\ell)$ develops in the heating layer. Like other 3D studies, we find $E(\ell) \propto \ell^{-1}$ in the "inertial range," while theory and local simulations argue for $E(\ell) \propto \ell^{-5/3}$. We argue that current 3D simulations do not resolve the inertial range of turbulence and are affected by numerical viscosity up to the energy containing scale, creating a "bottleneck" that prevents an efficient turbulent cascade.
Results on Searches for New Physics at B Factories<|sep|>We summarize recent results on B+ -> tau+ nu setting constraints on the charged Higgs mass, discuss the CP puzzle in B -> K pi decays and present searches for a light neutral Higgs in radiative Upsilon(2S) and Upsilon(3S)decays.
Broadband Linear Polarization of Jupiter Trojans<|sep|>Trojan asteroids orbit in the Lagrange points of the system Sun-planet-asteroid. Their dynamical stability make their physical properties important proxies for the early evolution of our solar system. To study their origin, we want to characterize the surfaces of Jupiter Trojan asteroids and check possible similarities with objects of the main belt and of the Kuiper Belt. We have obtained high-accuracy broad-band linear polarization measurements of six Jupiter Trojans of the L4 population and tried to estimate the main features of their polarimetric behaviour. We have compared the polarimetric properties of our targets among themselves, and with those of other atmosphere-less bodies of our solar system. Our sample show approximately homogeneous polarimetric behaviour, although some distinct features are found between them. In general, the polarimetric properties of Trojan asteroids are similar to those of D- and P-type main-belt asteroids. No sign of coma activity is detected in any of the observed objects. An extended polarimetric survey may help to further investigate the origin and the surface evolution of Jupiter Trojans.
Cosmological Perturbations from a Group Theoretical Point of View<|sep|>We present a new approach to cosmological perturbations based on the theory of Lie groups and their representations. After re-deriving the standard covariant formalism from SO(3) considerations, we provide a new expansion of the perturbed Friedmann-Lemaitre-Robertson-Walker (FLRW) metric in terms of irreducible representations of the Lorentz group. The resulting decomposition splits into (scalar, scalar), (scalar, vector) and (vector, vector) terms. These equations directly correspond to the standard Lifshitz classification of cosmological perturbations using scalar, vector and tensor modes which arise from the irreducible SO(3) representation of the spatial part of the metric. While the Lorentz group basis matches the underlying local symmetries of the FLRW spacetime better than the SO(3), the new equations do not provide further simplification compared to the standard cosmological perturbation theory. We conjecture that this is due to the fact that the so(3,1) ~ su(2) x su(2) Lorentz algebra has no pair of commuting generators commuting with any of the translation group generators.
RMS and charge radii in a potential model<|sep|>The Dalgarno's method of perturbation is used to solve the Schrodinger's equation with the Cornell potential $V(r)=-\frac{4\alpha_s}{3r}+br+c$. The short range and long range effect of the potential is incorporated in the same wave function by using two scales $r^S$ and $r^L$ as an integration limit. The results for bounds on r.m.s. radii of various heavy flavored mesons are reported. We have also showed the relation between r.m.s. and charge radius of mesons.
Learning Kernels for Structured Prediction using Polynomial Kernel Transformations<|sep|>Learning the kernel functions used in kernel methods has been a vastly explored area in machine learning. It is now widely accepted that to obtain 'good' performance, learning a kernel function is the key challenge. In this work we focus on learning kernel representations for structured regression. We propose use of polynomials expansion of kernels, referred to as Schoenberg transforms and Gegenbaur transforms, which arise from the seminal result of Schoenberg (1938). These kernels can be thought of as polynomial combination of input features in a high dimensional reproducing kernel Hilbert space (RKHS). We learn kernels over input and output for structured data, such that, dependency between kernel features is maximized. We use Hilbert-Schmidt Independence Criterion (HSIC) to measure this. We also give an efficient, matrix decomposition-based algorithm to learn these kernel transformations, and demonstrate state-of-the-art results on several real-world datasets.
Deterministic nonclassicality for quantum mechanical oscillators in thermal states<|sep|>Quantum nonclassicality is the basic building stone for the vast majority of quantum information applications and methods of its generation are at the forefront of research. One of the obstacles any method needs to clear is the looming presence of decohorence and noise which act against the nonclassicality and often erase it completely. In this paper we show that nonclassical states of a quantum harmonic oscillators initially in thermal equilibrium states can be deterministically created by coupling it to a single two level system. This can be achieved even in the absorption regime in which the two level system is initially in the ground state. The method is resilient to noise and it may actually benefit from it, as witnessed by the systems with higher thermal energy producing more nonclassical states.
Corner-Impact Bifurcations: a novel class of discontinuity-induced bifurcations in Cam-Follower Systems<|sep|>This paper is concerned with the analysis of a class of impacting systems of relevance in applications: cam-follower systems. We show that these systems, which can be modelled as discontinuously forced impact oscillators, can exhibit complex behaviour due to the detachment at high rotational speeds between the follower and the cam. We propose that the observed phenomena can be explained in terms of a novel type of discontinuity-induced bifurcation, termed as corner-impact. We present a complete analysis of this bifurcation in the case of non-autonomous impact oscillator and explain the transition to chaos observed in a representative cam-follower example. The theoretical findings are validated numerically.
Degrees of Freedom Rate Region of the $K$-user Interference Channel with Blind CSIT Using Staggered Antenna Switching<|sep|>In this paper, we consider the problem of the interference alignment for the $K$-user SISO interference channel with blind channel state information at transmitters (CSIT). Our achievement in contrast to popular $K-$user interference alignment (IA) scheme has more practical notions. In this case every receiver is equipped with one reconfigurable antenna which tries to place its desired signal in a subspace which is linearly independent from interference signals. We show that if the channel values are known to the receivers only, the sum degrees-of-freedom (DOF) rate region of the linear BIA with staggered antenna switching is $\frac{Kr}{r^2-r+K}$, where $r = \left \lceil{\frac{\sqrt{1+4K}-1}{2}} \right \rceil$. The result indicates that the optimum DoF rate region of the $K-$user interference channel is to achieve the DoF of $\frac{\sqrt{K}}{2}$ for an asymptotically large network. Thus, the DoF of the $K$-user interference channel using staggered antenna switching grows sub-linearly with the number of the users, whereas it grows linearly in the case where transmitters access the CSI. In addition we propose both achievability and converse proof so as to show that this is the DoF rate region of blind interference alignment (BIA) with staggered antenna switching.
Linearity of quantum probability measure and Hardy's model<|sep|>We re-examine d=4 hidden-variables-models for a system of two spin-$1/2$ particles in view of the concrete model of Hardy, who analyzed the criterion of entanglement without referring to inequality. The basis of our analysis is the linearity of the probability measure related to the Born probability interpretation, which excludes non-contextual hidden-variables models in $d\geq 3$. To be specific, we note the inconsistency of the non-contextual hidden-variables model in $d=4$ with the linearity of the quantum mechanical probability measure in the sense $\langle\psi|{\bf a}\cdot {\bf \sigma}\otimes{\bf b}\cdot {\bf \sigma}|\psi\rangle+\langle\psi|{\bf a}\cdot {\bf \sigma}\otimes{\bf b}^{\prime}\cdot {\bf \sigma}|\psi\rangle=\langle\psi|{\bf a}\cdot {\bf \sigma}\otimes ({\bf b}+{\bf b}^{\prime})\cdot {\bf \sigma}|\psi\rangle$ for non-collinear ${\bf b}$ and ${\bf b}^{\prime}$. It is then shown that Hardy's model in $d=4$ does not lead to a unique mathematical expression in the demonstration of the discrepancy of local realism (hidden-variables model) with entanglement and thus his proof is incomplete. We identify the origin of this non-uniqueness with the non-uniqueness of translating quantum mechanical expressions into expressions in hidden-variables models, which results from the failure of the above linearity of the probability measure. In contrast, if the linearity of the probability measure is strictly imposed, which is tantamount to asking that the non-contextual hidden-variables model in $d=4$ gives the CHSH inequality $|\langle B\rangle|\leq 2$ uniquely, it is shown that the hidden-variables model can describe only separable quantum mechanical states; this conclusion is in perfect agreement with the so-called Gisin's theorem which states that $|\langle B\rangle|\leq 2$ implies separable states.
Using Detailed Access Trajectories for Learning Behavior Analysis<|sep|>Student learning activity in MOOCs can be viewed from multiple perspectives. We present a new organization of MOOC learner activity data at a resolution that is in between the fine granularity of the clickstream and coarse organizations that count activities, aggregate students or use long duration time units. A detailed access trajectory (DAT) consists of binary values and is two dimensional with one axis that is a time series, e.g. days and the other that is a chronologically ordered list of a MOOC component type's instances, e.g. videos in instructional order. Most popular MOOC platforms generate data that can be organized as detailed access trajectories (DATs).We explore the value of DATs by conducting four empirical mini-studies. Our studies suggest DATs contain rich information about students' learning behaviors and facilitate MOOC learning analyses.
Nonlocal network dynamics via fractional graph Laplacians<|sep|>We introduce nonlocal dynamics on directed networks through the construction of a fractional version of a nonsymmetric Laplacian for weighted directed graphs. Furthermore, we provide an analytic treatment of fractional dynamics for both directed and undirected graphs, showing the possibility of exploring the network employing random walks with jumps of arbitrary length. We also provide some examples of the applicability of the proposed dynamics, including consensus over multi-agent systems described by directed networks.
Minimum-Length Scheduling with Finite Queues: Solution Characterization and Algorithmic Framework<|sep|>We consider a set of transmitter-receiver pairs, or links, that share a common channel and address the problem of emptying backlogged queues at the transmitters in minimum time. The problem amounts to determining activation subsets of links and their time durations to form a minimum-length schedule. The problem of scheduling has been studied under various formulations before. In this paper, we present fundamental insights and solution characterizations that include: (i) showing that the complexity of the problem remains high for any continuous and increasing rate function, (ii) formulating and proving sufficient and necessary optimality conditions of two base scheduling strategies that correspond to emptying the queues using "one-at-a-time" or "all-at-once" strategies, (iii) presenting and proving the tractability of the special case in which the transmission rates are functions only of the cardinality of the link activation sets. These results are independent of physical-layer system specifications and are valid for any form of rate function. We then develop an algorithmic framework. The framework encompasses exact as well as sub-optimal, but fast, scheduling algorithms, all under a unified principle design. Through computational experiments we finally investigate the performance of several specific algorithms.
The luminosity and stellar mass Fundamental Plane of early-type galaxies<|sep|>From a sample of ~50000 early-type galaxies from the SDSS, we measured the traditional Fundamental Plane in four bands. We then replaced luminosity with stellar mass, and measured the "stellar mass" FP. The FP steepens slightly as one moves from shorter to longer wavelengths: the orthogonal fit has slope 1.40 in g and 1.47 in z. The FP is thinner at longer wavelengths: scatter is 0.062 dex in g, 0.054 dex in z. The scatter is larger at small galaxy sizes/masses; at large masses measurement errors account for essentially all of the observed scatter. The FP steepens further when luminosity is replaced with stellar mass, to slope ~ 1.6. The intrinsic scatter also reduces further, to 0.048 dex. Since color and stellar mass-to-light ratio are closely related, this explains why color can be thought of as the fourth FP parameter. However, the slope of the stellar mass FP remains shallower than the value of 2 associated with the virial theorem. This is because the ratio of dynamical to stellar mass increases at large masses as M_d^0.17. The face-on view of the stellar mass kappa-space suggests that there is an upper limit to the stellar density for a given dynamical mass, and this decreases at large masses: M_*/R_e^3 ~ M_d^-4/3. We also study how the estimated coefficients a and b of the FP are affected by other selection effects (e.g. excluding small sigma biases a high; excluding fainter L biases a low). These biases are seen in FPs which have no intrinsic curvature, so the observation that a and b scale with L and sigma is not, by itself, evidence that the Plane is warped. We show that the FP appears to curve sharply downwards at the small mass end, and more gradually downwards towards larger masses. Whereas the drop at small sizes is real, most of the latter effect is due to correlated errors.
Stationary Wigner Equation with Inflow Boundary Conditions: Will a Symmetric Potential Yield a Symmetric Solution?<|sep|>Based on the well-posedness of the stationary Wigner equation with inflow boundary conditions given in (A. Arnold, H et al. J. Math. Phys., 41, 2000), we prove without any additional prerequisite conditions that the solution of the Wigner equation with symmetric potential and inflow boundary conditions will be symmetric. This improve the result in (D. Taj et al. Europhys. Lett., 74, 2006) which depends on the convergence of solution formulated in the Neumann series. By numerical studies, we present the convergence of the numerical solution to the symmetric profile for three different numerical schemes. This implies that the upwind schemes can also yield a symmetric numerical solution, on the contrary to the argument given in (D. Taj et al. Europhys. Lett., 74, 2006).
A cumulative approach to quantification for sentiment analysis<|sep|>We estimate sentiment categories proportions for retrieval within large retrieval sets. In general, estimates are produced by counting the classification outcomes and then by adjusting such category sizes taking into account misclassification error matrix. However, both the accuracy of the classifier and the precision of the retrieval produce a large number of errors that makes difficult the application of an aggregative approach to sentiment analysis as a reliable and efficient estimation of proportions for sentiment categories. The challenge for real time analytics during retrieval is thus to overcome misclassification errors, and more importantly, to apply sentiment classification or any other similar post-processing analytics at retrieval time. We present a non-aggregative approach that can be applied to very large retrieval sets of queries.
GRB 090510: a short burst from a massive star ?<|sep|>GRB afterglow 090510 is (so far) the best-monitored afterglow in the optical, X-ray, and above 100 MeV, measurements covering 2-3 decades in time at each frequency. Owing to its power-law temporal decay and power-law spectrum, it seems very likely that the highest energy emission is from the forward-shock energizing the ambient medium (the standard blast-wave model for GRB afterglows), the GeV flux and its decay rate being consistent with that model's expectations. However, the synchrotron emission from a collimated outflow (the standard jet model) has difficulties in accounting for the lower-energy afterglow emission, where a simultaneous break occurs at 2 ks in the optical and X-ray light-curves, but with the optical flux decay (before and after the break) being much slower than in the X-rays (at same time). The measured X-ray and GeV fluxes are incompatible with the higher-energy afterglow emission being from same spectral component as the lower-energy afterglow emission, which suggests a synchrotron self-Compton model for this afterglow. Cessation of energy injection in the blast-wave and an ambient medium with a wind-like n ~ r^{-2} density can explain all features of the optical and X-ray light-curves of GRB afterglow 090510. Such an ambient medium radial structure is incompatible with this short-GRB originating from the merger of two compact stars.
Inter-layer Transition in Neural Architecture Search<|sep|>Differential Neural Architecture Search (NAS) methods represent the network architecture as a repetitive proxy directed acyclic graph (DAG) and optimize the network weights and architecture weights alternatively in a differential manner. However, existing methods model the architecture weights on each edge (i.e., a layer in the network) as statistically independent variables, ignoring the dependency between edges in DAG induced by their directed topological connections. In this paper, we make the first attempt to investigate such dependency by proposing a novel Inter-layer Transition NAS method. It casts the architecture optimization into a sequential decision process where the dependency between the architecture weights of connected edges is explicitly modeled. Specifically, edges are divided into inner and outer groups according to whether or not their predecessor edges are in the same cell. While the architecture weights of outer edges are optimized independently, those of inner edges are derived sequentially based on the architecture weights of their predecessor edges and the learnable transition matrices in an attentive probability transition manner. Experiments on five benchmarks confirm the value of modeling inter-layer dependency and demonstrate the proposed method outperforms state-of-the-art methods.
Quantum wormhole as a Ricci flow<|sep|>The idea is considered that a quantum wormhole in a spacetime foam can be described as a Ricci flow. In this interpretation the Ricci flow is a statistical system and every metric in the Ricci flow is a microscopical state. The probability density of the microscopical state is connected with a Perelman's functional of a rescaled Ricci flow.
Parameter Estimation for Partially Observed Hypoelliptic Diffusions<|sep|>Hypoelliptic diffusion processes can be used to model a variety of phenomena in applications ranging from molecular dynamics to audio signal analysis. We study parameter estimation for such processes in situations where we observe some components of the solution at discrete times. Since exact likelihoods for the transition densities are typically not known, approximations are used that are expected to work well in the limit of small inter-sample times $\Delta t$ and large total observation times $N\Delta t$. Hypoellipticity together with partial observation leads to ill-conditioning requiring a judicious combination of approximate likelihoods for the various parameters to be estimated. We combine these in a deterministic scan Gibbs sampler alternating between missing data in the unobserved solution components, and parameters. Numerical experiments illustrate asymptotic consistency of the method when applied to simulated data. The paper concludes with application of the Gibbs sampler to molecular dynamics data.
Spin polarized current in a junction of zigzag carbon nanotube<|sep|>We investigated spin-resolved electronic transport through a junction composed of a nonmagnetic metal electrode and a zigzag carbon nanotube by means of self-consistent Green's function method in the tight binding approximation and the unrestricted Hartree-Fock approximation. Our results show that the electric current can be spin-polarized if the coupling of the junction is weak. Further calculations on spin-spin correlation and local density of states reveal the existence of magnetic edge states in zigzag carbon nanotubes, which is responsible for the observed spin-polarized current and can be controlled by applying a gate voltage. We also studied the influence of the nearest-neighbor Coulomb interaction and the junction coupling strength on the spin-polarization of the current.
The Concept of Few-Parameter Modelling of Eclipsing Binary and Exoplanet Transit Light Curves<|sep|>We present a new few-parameter phenomenological model of light curves of eclipsing binaries and stars with transiting planets that is able to fit the observed light curves with the accuracy better than 1\% of their amplitudes. The model can be used namely for appropriate descriptions of light curve shapes, classification, mid-eclipse time determination, and fine period analyses.
HR-CAM: Precise Localization of Pathology Using Multi-level Learning in CNNs<|sep|>We propose a CNN based technique that aggregates feature maps from its multiple layers that can localize abnormalities with greater details as well as predict pathology under consideration. Existing class activation mapping (CAM) techniques extract feature maps from either the final layer or a single intermediate layer to create the discriminative maps and then interpolate to upsample to the original image resolution. In this case, the subject specific localization is coarse and is unable to capture subtle abnormalities. To mitigate this, our method builds a novel CNN based discriminative localization model that we call high resolution CAM (HR-CAM), which accounts for layers from each resolution, therefore facilitating a comprehensive map that can delineate the pathology for each subject by combining low-level, intermediate as well as high-level features from the CNN. Moreover, our model directly provides the discriminative map in the resolution of the original image facilitating finer delineation of abnormalities. We demonstrate the working of our model on a simulated abnormalities data where we illustrate how the model captures finer details in the final discriminative maps as compared to current techniques. We then apply this technique: (1) to classify ependymomas from grade IV glioblastoma on T1-weighted contrast enhanced (T1-CE) MRI and (2) to predict Parkinson's disease from neuromelanin sensitive MRI. In all these cases we demonstrate that our model not only predicts pathologies with high accuracies, but also creates clinically interpretable subject specific high resolution discriminative localizations. Overall, the technique can be generalized to any CNN and carries high relevance in a clinical setting.
How much can we trust high-resolution spectroscopic stellar chemical abundances?<|sep|>To study stellar populations, it is common to combine chemical abundances from different spectroscopic surveys/studies where different setups were used. These inhomogeneities can lead us to inaccurate scientific conclusions. In this work, we studied one aspect of the problem: When deriving chemical abundances from high-resolution stellar spectra, what differences originate from the use of different radiative transfer codes?
Automated Detection of Doxing on Twitter<|sep|>Doxing refers to the practice of disclosing sensitive personal information about a person without their consent. This form of cyberbullying is an unpleasant and sometimes dangerous phenomenon for online social networks. Although prior work exists on automated identification of other types of cyberbullying, a need exists for methods capable of detecting doxing on Twitter specifically. We propose and evaluate a set of approaches for automatically detecting second- and third-party disclosures on Twitter of sensitive private information, a subset of which constitutes doxing. We summarize our findings of common intentions behind doxing episodes and compare nine different approaches for automated detection based on string-matching and one-hot encoded heuristics, as well as word and contextualized string embedding representations of tweets. We identify an approach providing 96.86% accuracy and 97.37% recall using contextualized string embeddings and conclude by discussing the practicality of our proposed methods.
Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation<|sep|>Visual navigation is a task of training an embodied agent by intelligently navigating to a target object (e.g., television) using only visual observations. A key challenge for current deep reinforcement learning models lies in the requirements for a large amount of training data. It is exceedingly expensive to construct sufficient 3D synthetic environments annotated with the target object information. In this paper, we focus on visual navigation in the low-resource setting, where we have only a few training environments annotated with object information. We propose a novel unsupervised reinforcement learning approach to learn transferable meta-skills (e.g., bypass obstacles, go straight) from unannotated environments without any supervisory signals. The agent can then fast adapt to visual navigation through learning a high-level master policy to combine these meta-skills, when the visual-navigation-specified reward is provided. Evaluation in the AI2-THOR environments shows that our method significantly outperforms the baseline by 53.34% relatively on SPL, and further qualitative analysis demonstrates that our method learns transferable motor primitives for visual navigation.
Thermally stable magnetic skyrmions in multilayer synthetic antiferromagnetic racetracks<|sep|>A magnetic skyrmion is a topological magnetization structure with a nanometric size and a well-defined swirling spin distribution, which is anticipated to be an essential building block for novel skyrmion-based device applications. We study the motion of magnetic skyrmions in multilayer synthetic antiferromagnetic (SAF) racetracks as well as in conventional monolayer ferromagnetic (FM) racetracks at finite temperature. There is an odd-even effect of the constituent FM layer number on the skyrmion Hall effect (SkHE). Namely, due to the suppression of the SkHE, the magnetic skyrmion has no transverse motion in multilayer SAF racetracks packed with even FM layers. It is shown that a moving magnetic skyrmion is stable even at room temperature ($T=300$ K) in a bilayer SAF racetrack but it is destructed at $T=100$ K in a monolayer FM racetrack. Our results indicate that the SAF structures are reliable and promising candidates for future applications in skyrmion-electronics and skyrmion-spintronics.
The mutually normalizing regular subgroups of the holomorph of a cyclic group of prime power order<|sep|>Let $G=C_{p^n}$ be a finite cyclic p-group, and let $Hol(G)$ denote its holomorph. In this work, we find and characterize the regular subgroups of $Hol(G)$ that are mutually normalizing each other in the permutation group $Sym(G)$. We represent such regular subgroups as vertices of a graph, and we connect a pair of them by an edge when they mutually normalize each other. The approach to construct this local normalizing graph relies on the theory of gamma functions, and the final result will contain all the information about the regular subgroups of $Hol(G)$ in a compact form.
Direct Imaging of Planet Transit Events<|sep|>Exoplanet transit events are attractive targets for the ultrahigh-resolution capabilities afforded by optical interferometers. The intersection of two developments in astronomy enable direct imaging of exoplanet transits: first, improvements in sensitivity and precision of interferometric instrumentation; and second, identification of ever-brighter host stars. Efforts are underway for the first direct high-precision detection of closure phase signatures with the CHARA Array and Navy Precision Optical Interferometer. When successful, these measurements will enable recovery of the transit position angle on the sky, along with characterization of other system parameters, such as stellar radius, planet radius, and other parameters of the transit event. This technique can directly determine the planet's radius independent of any outside observations, and appears able to improve substantially upon other determinations of that radius; it will be possible to extract wavelength dependence of that radius determination, for connection to characterization of planetary atmospheric composition & structure. Additional directly observed parameters - also not dependent on transit photometry or spectroscopy - include impact parameter, transit ingress time, and transit velocity.
Nonlinear electromagnetic formulation for particle simulation of lower hybrid waves in toroidal geometry<|sep|>Electromagnetic particle simulation model has been formulated and verified for nonlinear processes of lower hybrid (LH) waves in fusion plasmas. Electron dynamics is described by the drift kinetic equation using either kinetic momentum or canonical momentum. Ion dynamics is treated as the fluid system or by the Vlasov equation. Compressible magnetic perturbation is retained to simulate both the fast and slow LH waves. Numerical properties are greatly improved by using electron continuity equation to enforce consistency between electrostatic potential and vector potential, and by using the importance sampling technique. The simulation model has been implemented in the gyrokinetic toroidal code (GTC), and verified for the dispersion relation and nonlinear particle trapping of the electromagnetic LH waves.
Green's formula and singularity at a triple contact line. Example of finite-displacement solution<|sep|>The various equations at the surfaces and triple contact lines of a deformable body are obtained from a variational condition, by applying Green's formula in the whole space and on the Riemannian surfaces. The surface equations are similar to the Cauchy's equations for the volume, but involve a special definition of the 'divergence' (tensorial product of the covariant derivatives on the surface and the whole space). The normal component of the divergence equation generalizes the Laplace's equation for a fluid-fluid interface. Assuming that Green's formula remains valid at the contact line (despite the singularity), two equations are obtained at this line. The first one expresses that the fluid-fluid surface tension is equilibrated by the two surface stresses (and not by the volume stresses of the body) and suggests a finite displacement at this line (contrary to the infinite-displacement solution of classical elasticity, in which the surface properties are not taken into account). The second equation represents a strong modification of Young's capillary equation. The validity of Green's formula and the existence of a finite-displacement solution are justified with an explicit example of finite-displacement solution in the simple case of a half-space elastic solid bounded by a plane. The solution satisfies the contact line equations and its elastic energy is finite (whereas it is infinite for the classical elastic solution). The strain tensor components generally have different limits when approaching the contact line under different directions. Although Green's formula cannot be directly applied, because the stress tensor components do not belong to the Sobolev space H1(V), it is shown that this formula remains valid. As a consequence, there is no contribution of the volume stresses at the contact line. The validity of Green's formula plays a central role in the theory.
Spin-Singlet Quantum Hall States and Jack Polynomials with a Prescribed Symmetry<|sep|>We show that a large class of bosonic spin-singlet Fractional Quantum Hall model wave-functions and their quasi-hole excitations can be written in terms of Jack polynomials with a prescribed symmetry. Our approach describes new spin-singlet quantum Hall states at filling fraction nu = 2k/(2r-1) and generalizes the (k,r) spin-polarized Jack polynomial states. The NASS and Halperin spin singlet states emerge as specific cases of our construction. The polynomials express many-body states which contain configurations obtained from a root partition through a generalized squeezing procedure involving spin and orbital degrees of freedom. The corresponding generalized Pauli principle for root partitions is obtained, allowing for counting of the quasihole states. We also extract the central charge and quasihole scaling dimension, and propose a conjecture for the underlying CFT of the (k, r) spin-singlet Jack states.
Gravitational and electromagnetic radiation from an electrically charged black hole in general nonlinear electrodynamics<|sep|>We derive the equations for the odd and even parity perturbations of coupled electromagnetic and gravitational fields of a black hole with an electric charge within the context of general nonlinear electrodynamics. The Lagrangian density is a generic function of the Lorentz invariant scalar quantities of the electromagnetic fields. We include the Hodge dual of the electromagnetic field tensor and the cosmological constant in our calculations. For each type of parity, we reduce the system of Einstein field equations coupled to nonlinear electrodynamics to two coupled Schr\"odinger-type wave equations, one for the gravitational field and one for the electromagnetic field. The stability conditions in the presence of the Hodge dual of the electromagnetic field are derived.
Spectral variability of ultraluminous X-ray sources<|sep|>We study spectral variability of 11 ultraluminous X-ray sources (ULX) using archived XMM-Newton and Chandra observations. We use three models to describe the observed spectra: a power-law, a multi-colour disc (MCD) and a combination of these two models. We find that 7 ULXs show a correlation between the luminosity Lx and the photon index Gamma. Furthermore, 4 out of these 7 ULXs also show spectral pivoting in the observed energy band. We also find that two ULXs show an Lx-Gamma anti-correlation. The spectra of 4 ULXs in the sample can be adequately fitted with a MCD model. We compare these sources to known black hole binaries (BHB) and find that they follow similar paths in their luminosity-temperature diagrams. Finally we show that the `soft excess' reported for many of these ULXs at about 0.2 keV seems to roughly follow a trend Lsoft \propto T^{-3.5} when modelled with a power-law plus a `cool' MCD model. This is contrary to the L \propto T^4 relation that is expected from theory and what is seen for many accreting BHBs. The observed trend could instead arise from disc emission beamed by an outflowing wind around a about 10 solar mass black hole.
iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images<|sep|>We propose iSegFormer, a memory-efficient transformer that combines a Swin transformer with a lightweight multilayer perceptron (MLP) decoder. With the efficient Swin transformer blocks for hierarchical self-attention and the simple MLP decoder for aggregating both local and global attention, iSegFormer learns powerful representations while achieving high computational efficiencies. Specifically, we apply iSegFormer to interactive 3D medical image segmentation.
Capturing collective conflict dynamics with sparse social circuits<|sep|>We discuss a set of computational techniques, called Inductive Game Theory, for extracting strategic decision-making rules from time series data and constructing probabilistic social circuits. We construct these circuits by connecting component individuals and groups with strategies in a game and propose an inductive approach to reconstructing the edges. We demonstrate this approach with conflict behavior in a society of pigtailed macaques by identifying significant patterns in decision-making by individuals. With the constructed circuit, we then capture macroscopic features of the system that were not specified in the construction of the initial circuit, providing a mapping between individual level behaviors to collective behaviors over the scale of the group. We extend on previous work in Inductive Game Theory by more efficiently searching the space of possible strategies by grouping individuals into socially relevant sets to produce a more efficient, parsimonious specification of the underlying interactions between components. We discuss how we reduce the dimensionality of these circuits using coarse-graining or compression to build cognitive effective theories for collective behavior.
A robust algorithm for explaining unreliable machine learning survival models using the Kolmogorov-Smirnov bounds<|sep|>A new robust algorithm based of the explanation method SurvLIME called SurvLIME-KS is proposed for explaining machine learning survival models. The algorithm is developed to ensure robustness to cases of a small amount of training data or outliers of survival data. The first idea behind SurvLIME-KS is to apply the Cox proportional hazards model to approximate the black-box survival model at the local area around a test example due to the linear relationship of covariates in the model. The second idea is to incorporate the well-known Kolmogorov-Smirnov bounds for constructing sets of predicted cumulative hazard functions. As a result, the robust maximin strategy is used, which aims to minimize the average distance between cumulative hazard functions of the explained black-box model and of the approximating Cox model, and to maximize the distance over all cumulative hazard functions in the interval produced by the Kolmogorov-Smirnov bounds. The maximin optimization problem is reduced to the quadratic program. Various numerical experiments with synthetic and real datasets demonstrate the SurvLIME-KS efficiency.
Cumulative Oxygen Abundances of Spiral Galaxies<|sep|>Studying the global evolution of spiral galaxies requires determining their overall chemical compositions. However, since spirals tend to possess gradients in their chemical compositions, determining their overall chemical abundances poses a challenge. In this study, the framework for a newly proposed method for determining the overall oxygen abundance of a disk is established. By separately integrating the absolute amounts of hydrogen and oxygen out to large radii, the cumulative oxygen abundance is shown to approach an asymptotic value. In this manner, a reliable account of the overall chemical state of a disk is revealed.
Controlled Manipulation of Mode Splitting in an Optical Microcavity by Two Rayleigh Scatterers<|sep|>We report controlled manipulation of mode splitting in an optical microresonator coupled to two nanoprobes. It is demonstrated that, by controlling the positions of the nanoprobes, the split modes can be tuned simultaneously or individually and experience crossing or anti-crossing in frequency and linewidth. A tunable transition between standing wave mode and travelling wave mode is also observed. Underlying physics is discussed by developing a two-scatterer model which can be extended to multiple scatterers. Observed rich dynamics and tunability of split modes in a single microresonator will find immediate applications in optical sensing, opto-mechanics, filters and will provide a platform to study strong light-matter interactions in two-mode cavities.
Pareto Robust optimization on Euclidean vector spaces<|sep|>Pareto efficiency for robust linear programs was introduced by Iancu and Trichakis in [9]. We generalize their approach and theoretical results to robust optimization problems in Euclidean spaces with affine uncertainty. Additionally, we demonstrate the value of this approach in an exemplary manner in the area of robust semidefinite programming (SDP). In particular, we prove that computing a Pareto robustly optimal solution for a robust SDP is tractable and illustrate the benefit of such solutions at the example of the maximal eigenvalue problem. Furthermore, we modify the famous algorithm of Goemans and Williamson [8] in order to compute cuts for the robust max-cut problem that yield an improved approximation guarantee in non-worst-case scenarios.
Capital requirements with defaultable securities<|sep|>We study capital requirements for bounded financial positions defined as the minimum amount of capital to invest in a chosen eligible asset targeting a pre-specified acceptability test. We allow for general acceptance sets and general eligible assets, including defaultable bonds. Since the payoff of these assets is not necessarily bounded away from zero the resulting risk measures cannot be transformed into cash-additive risk measures by a change of numeraire. However, extending the range of eligible assets is important because, as exemplified by the recent financial crisis, assuming the existence of default-free bonds may be unrealistic. We focus on finiteness and continuity properties of these general risk measures. As an application, we discuss capital requirements based on Value-at-Risk and Tail-Value-at-Risk acceptability, the two most important acceptability criteria in practice. Finally, we prove that there is no optimal choice of the eligible asset. Our results and our examples show that a theory of capital requirements allowing for general eligible assets is richer than the standard theory of cash-additive risk measures.
Microscopic theory of type-1.5 superconductivity in multiband systems<|sep|>We report a self-consistent microscopic theory of characteristic length scales, vortex structure and type-1.5 superconducting state in two-band systems using two-band Eilenberger formalism.
Increasing the Efficiency of 6-DoF Visual Localization Using Multi-Modal Sensory Data<|sep|>Localization is a key requirement for mobile robot autonomy and human-robot interaction. Vision-based localization is accurate and flexible, however, it incurs a high computational burden which limits its application on many resource-constrained platforms. In this paper, we address the problem of performing real-time localization in large-scale 3D point cloud maps of ever-growing size. While most systems using multi-modal information reduce localization time by employing side-channel information in a coarse manner (eg. WiFi for a rough prior position estimate), we propose to inter-weave the map with rich sensory data. This multi-modal approach achieves two key goals simultaneously. First, it enables us to harness additional sensory data to localise against a map covering a vast area in real-time; and secondly, it also allows us to roughly localise devices which are not equipped with a camera. The key to our approach is a localization policy based on a sequential Monte Carlo estimator. The localiser uses this policy to attempt point-matching only in nodes where it is likely to succeed, significantly increasing the efficiency of the localization process. The proposed multi-modal localization system is evaluated extensively in a large museum building. The results show that our multi-modal approach not only increases the localization accuracy but significantly reduces computational time.
Cosmological models with the spinor and non-minimally interacting scalar field<|sep|>The solution to the current extending Universe problem, and the description of all stages of evolution compels scientists to consider various cosmological models. Scalar - tensor models are rather simple and also allow us to clearly define the separate stages of evolution. Furthermore, other cosmological models are reduced. Our work takes into consideration the non-minimally interacted scalar field and the spinor field. The spinor field has been considered to establish a better understanding of the stages of evolution in our Universe.
Universality of the ion flux to the JET outer wall<|sep|>Universality in the ion flux to the JET outer-wall is observed in outerwall limiter mounted Langmuir probe (OLP) time-series across a large range of plasma current and line-averaged density during Ohmically heated horizontal target L-mode plasmas. The mean, M, and the standard deviation, sigma, of the ion-saturation current measured by the OLP show systematic variation with plasma current and density. Both increase as either plasma current decreases and/or density increases. Upon renormalization, achieved by subtraction of M and rescaling by sigma, the probability distribution functions (PDFs) of each signal collapse approximately onto a single curve. The shape of the curve deviates from a distribution in the tail of the PDF and is better described by a log-normal distribution. The collapse occurs over 4 decades of the ordinate which, given the wide parameter space over which the data spans, is a strong indication of universality.
Securing Password Authentication for Web-based Applications<|sep|>The use of passwords and the need to protect passwords are not going away. The majority of websites that require authentication continue to support password authentication. Even high-security applications such as Internet Banking portals, which deploy 2-factor authentication, rely on password authentication as one of the authentication factors. However phishing attacks continue to plague password-based authentication despite aggressive efforts in detection and takedown as well as comprehensive user awareness and training programs. There is currently no foolproof mechanism even for security-conscious websites to prevent users from being directed to fraudulent websites and having their passwords phished. In this paper, we apply a threat analysis on the web password login process, and uncover a design vulnerability in the HTML<inputtype="password"> field. This vulnerability can be exploited for phishing attacks as the web authentication process is not end-to-end secured from each input password field to the web server. We identify four properties that encapsulate the requirements to stop web-based password phishing, and propose a secure protocol to be used with a new credential field that complies with the four properties. We further analyze the proposed protocol through an abuse-case evaluation, discuss various deployment issues, and also perform a test implementation to understand its data and execution overheads
Numerical Solution of an Inverse Problem in Size-Structured Population Dynamics<|sep|>We consider a size-structured model for cell division and address the question of determining the division (birth) rate from the measured stable size distribution of the population. We propose a new regularization technique based on a filtering approach. We prove convergence of the algorithm and validate the theoretical results by implementing numerical simulations, based on classical techniques. We compare the results for direct and inverse problems, for the filtering method and for the quasi-reversibility method proposed in [Perthame-Zubelli].
SpiderNet: Hybrid Differentiable-Evolutionary Architecture Search via Train-Free Metrics<|sep|>Neural Architecture Search (NAS) algorithms are intended to remove the burden of manual neural network design, and have shown to be capable of designing excellent models for a variety of well-known problems. However, these algorithms require a variety of design parameters in the form of user configuration or hard-coded decisions which limit the variety of networks that can be discovered. This means that NAS algorithms do not eliminate model design tuning, they instead merely shift the burden of where that tuning needs to be applied. In this paper, we present SpiderNet, a hybrid differentiable-evolutionary and hardware-aware algorithm that rapidly and efficiently produces state-of-the-art networks. More importantly, SpiderNet is a proof-of-concept of a minimally-configured NAS algorithm; the majority of design choices seen in other algorithms are incorporated into SpiderNet's dynamically-evolving search space, minimizing the number of user choices to just two: reduction cell count and initial channel count. SpiderNet produces models highly-competitive with the state-of-the-art, and outperforms random search in accuracy, runtime, memory size, and parameter count.
The Low-High-Low Trend of Type III Radio Burst Starting Frequencies and Solar Flare Hard X-rays<|sep|>Using simultaneous X-ray and radio observations from solar flares, we investigate the link between the type III radio burst starting frequency and hard X-ray spectral index. For a proportion of events the relation derived between the starting height (frequency) of type III radio bursts and the electron beam velocity spectral index (deduced from X-rays) is used to infer the spatial properties (height and size) of the electron beam acceleration region. Both quantities can be related to the distance travelled before an electron beam becomes unstable to Langmuir waves. To obtain a list of suitable events we considered the RHESSI catalogue of X-ray flares and the Phoenix 2 catalogue of type III radio bursts. From the 200 events that showed both type III and X-ray signatures, we selected 30 events which had simultaneous emission in both wavelengths, good signal to noise in the X-ray domain and > 20 seconds duration. We find that > 50 % of the selected events show a good correlation between the starting frequencies of the groups of type III bursts and the hard X-ray spectral indices. A low-high-low trend for the starting frequency of type III bursts is frequently observed. Assuming a background electron density model and the thick target approximation for X-ray observations, this leads to a correlation between starting heights of the type III emission and the beam electron spectral index. Using this correlation we infer the altitude and vertical extents of the flare acceleration regions. We find heights from 183 Mm down to 25 Mm while the sizes range from 13 Mm to 2 Mm. These values agree with previous work that places an extended flare acceleration region high in the corona. We analyse the assumptions required and explore possible extensions to our assumed model. We discuss these results with respect to the acceleration heights and sizes derived from X-ray observations alone.
DeepID3: Face Recognition with Very Deep Neural Networks<|sep|>The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.
Distance-Based Independence Screening for Canonical Analysis<|sep|>This paper introduces a new method named Distance-based Independence Screening for Canonical Analysis (DISCA) to reduce dimensions of two random vectors with arbitrary dimensions. The objective of our method is to identify the low dimensional linear projections of two random vectors, such that any dimension reduction based on linear projection with lower dimensions will surely affect some dependent structure -- the removed components are not independent. The essence of DISCA is to use the distance correlation to eliminate the "redundant" dimensions until infeasible. Unlike the existing canonical analysis methods, DISCA does not require the dimensions of the reduced subspaces of the two random vectors to be equal, nor does it require certain distributional assumption on the random vectors. We show that under mild conditions, our approach does undercover the lowest possible linear dependency structures between two random vectors, and our conditions are weaker than some sufficient linear subspace-based methods. Numerically, DISCA is to solve a non-convex optimization problem. We formulate it as a difference-of-convex (DC) optimization problem, and then further adopt the alternating direction method of multipliers (ADMM) on the convex step of the DC algorithms to parallelize/accelerate the computation. Some sufficient linear subspace-based methods use potentially numerically-intensive bootstrap method to determine the dimensions of the reduced subspaces in advance; our method avoids this complexity. In simulations, we present cases that DISCA can solve effectively, while other methods cannot. In both the simulation studies and real data cases, when the other state-of-the-art dimension reduction methods are applicable, we observe that DISCA performs either comparably or better than most of them. Codes and an R package can be found in GitHub https://github.com/ChuanpingYu/DISCA.
Stability of Topic Modeling via Matrix Factorization<|sep|>Topic models can provide us with an insight into the underlying latent structure of a large corpus of documents. A range of methods have been proposed in the literature, including probabilistic topic models and techniques based on matrix factorization. However, in both cases, standard implementations rely on stochastic elements in their initialization phase, which can potentially lead to different results being generated on the same corpus when using the same parameter values. This corresponds to the concept of "instability" which has previously been studied in the context of $k$-means clustering. In many applications of topic modeling, this problem of instability is not considered and topic models are treated as being definitive, even though the results may change considerably if the initialization process is altered. In this paper we demonstrate the inherent instability of popular topic modeling approaches, using a number of new measures to assess stability. To address this issue in the context of matrix factorization for topic modeling, we propose the use of ensemble learning strategies. Based on experiments performed on annotated text corpora, we show that a K-Fold ensemble strategy, combining both ensembles and structured initialization, can significantly reduce instability, while simultaneously yielding more accurate topic models.
Past and present cosmic structure in the SDSS DR7 main sample<|sep|>We present a chrono-cosmography project, aiming at the inference of the four dimensional formation history of the observed large scale structure from its origin to the present epoch. To do so, we perform a full-scale Bayesian analysis of the northern galactic cap of the Sloan Digital Sky Survey (SDSS) Data Release 7 main galaxy sample, relying on a fully probabilistic, physical model of the non-linearly evolved density field. Besides inferring initial conditions from observations, our methodology naturally and accurately reconstructs non-linear features at the present epoch, such as walls and filaments, corresponding to high-order correlation functions generated by late-time structure formation. Our inference framework self-consistently accounts for typical observational systematic and statistical uncertainties such as noise, survey geometry and selection effects. We further account for luminosity dependent galaxy biases and automatic noise calibration within a fully Bayesian approach. As a result, this analysis provides highly-detailed and accurate reconstructions of the present density field on scales larger than $\sim~3$ Mpc$/h$, constrained by SDSS observations. This approach also leads to the first quantitative inference of plausible formation histories of the dynamic large scale structure underlying the observed galaxy distribution. The results described in this work constitute the first full Bayesian non-linear analysis of the cosmic large scale structure with the demonstrated capability of uncertainty quantification. Some of these results will be made publicly available along with this work. The level of detail of inferred results and the high degree of control on observational uncertainties pave the path towards high precision chrono-cosmography, the subject of simultaneously studying the dynamics and the morphology of the inhomogeneous Universe.
Constructing Synchronously Rotating Double White Dwarf Binaries<|sep|>We have developed a self-consistent-field technique similar to the one described by Hachisu, Eriguchi, & Nomoto (1986b) that can be used to construct detailed force-balanced models of synchronously rotating, double white dwarf (DWD) binaries that have a wide range of total masses, mass ratios, and separations. In addition to providing a computational tool that can be used to provide quiet initial starts for dynamical studies of the onset of mass transfer in DWD systems, we show that this SCF technique can be used to construct model sequences that mimic the last portion of the detached inspiral phase of DWD binary evolutions, and semi-detached model sequences that mimic a phase of conservative mass transfer.
Weakly Supervised Named Entity Tagging with Learnable Logical Rules<|sep|>We study the problem of building entity tagging systems by using a few rules as weak supervision. Previous methods mostly focus on disambiguation entity types based on contexts and expert-provided rules, while assuming entity spans are given. In this work, we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner. Specifically, we introduce compound rules that are composed from simple rules to increase the precision of boundary detection and generate more diverse pseudo labels. We further design a dynamic label selection strategy to ensure pseudo label quality and therefore avoid overfitting the neural tagger. Experiments on three datasets demonstrate that our method outperforms other weakly supervised methods and even rivals a state-of-the-art distantly supervised tagger with a lexicon of over 2,000 terms when starting from only 20 simple rules. Our method can serve as a tool for rapidly building taggers in emerging domains and tasks. Case studies show that learned rules can potentially explain the predicted entities.
Master Stability Functions for metacommunities with two types of habitats<|sep|>Current questions in ecology revolve around instabilities in the dynamics on spatial networks and particularly the effect of node heterogeneity. We extend the Master Stability Function formalism to inhomogeneous biregular networks having two types of spatial nodes. Notably, this class of systems also allows the investigation of certain types of dynamics on higher-order networks. Combined with the Generalized Modelling approach to study the linear stability of steady states, this is a powerful tool to numerically asses the stability of large ensembles of systems. We analyze the stability of ecological metacommunities with two distinct types of habitats analytically and numerically in order to identify several sets of conditions under which the dynamics can become stabilized by dispersal. Our analytical approach allows general insights into stabilizing and destabilizing effects in metapopulations. Specifically, we show that maladaptive dispersal may be stable under certain conditions.
On quantumness in multi-parameter quantum estimation<|sep|>In this article we derive a measure of quantumness in quantum multi-parameter estimation problems. We can show that the ratio between the mean Uhlmann Curvature and the Fisher Information provides a figure of merit which estimates the amount of incompatibility arising from the quantum nature of the underlying physical system. This ratio accounts for the discrepancy between the attainable precision in the simultaneous estimation of multiple parameters and the precision predicted by the Cram\'er-Rao bound. As a testbed for this concept, we consider a quantum many-body system in thermal equilibrium, and explore the quantum compatibility of the model across its phase diagram.
A Consistent Orbital Stability Analysis for the GJ 581 System<|sep|>We apply a combination of N-body modeling techniques and automated data fitting with Monte Carlo Markov Chain uncertainty analysis of Keplerian orbital models to radial velocity data to determine long term stability of the planetary system GJ 581. We find that while there are stability concerns with the 4-planet model as published by Forveille et al. (2011), when uncertainties in the system are accounted for, particularly stellar jitter, the hypothesis that the 4-planet model is gravitationally unstable is not statistically significant. Additionally, the system including proposed planet g by Vogt et al. (2012) also shows some stability concerns when eccentricities are allowed to float in the orbital fit, yet when uncertainties are included in the analysis the system including planet g also can not be proven to be unstable. We present revised reduced chi-squared values for Keplerian astrocentric orbital fits assuming 4-planet and 5-planet models for GJ~581 under the condition that best fits must be stable, and find no distinguishable difference by including planet g in the model. Additionally we present revised orbital element estimates for each assuming uncertainties due to stellar jitter under the constraint of the system being gravitationally stable.
Measurement and Analysis of Radio-frequency Radiation Exposure Level from Different Mobile Base Transceiver Stations in Ajaokuta and Environs, Nigeria<|sep|>We present the result of a preliminary assessment of radio-frequency radiation exposure from selected mobile base stations in Ajaokuta environs. The Power density of RF radiation within a radial distance of 125m was measured. Although values fluctuated due to the influence of other factors, including wave interference from other electromagnetic sources around reference base stations, we show from analysis that radiation exposure level is below the standard limit (4.5W/sqm for 900MHz and 9W/sqm for 18000MHz) set by the International Commission on Non-ionizing Radiation Protection (ICNIRP) and other regulatory agencies.
Mix and Mask Actor-Critic Methods<|sep|>Shared feature spaces for actor-critic methods aims to capture generalized latent representations to be used by the policy and value function with the hopes for a more stable and sample-efficient optimization. However, such a paradigm present a number of challenges in practice, as parameters generating a shared representation must learn off two distinct objectives, resulting in competing updates and learning perturbations. In this paper, we present a novel feature-sharing framework to address these difficulties by introducing the mix and mask mechanisms and the distributional scalarization technique. These mechanisms behaves dynamically to couple and decouple connected latent features variably between the policy and value function, while the distributional scalarization standardizes the two objectives using a probabilistic standpoint. From our experimental results, we demonstrate significant performance improvements compared to alternative methods using separate networks and networks with a shared backbone.
Robust Mechanisms Under Common Valuation<|sep|>We study robust mechanisms to sell a common-value good. We assume that the mechanism designer knows the prior distribution of the buyers' common value but is unsure of the buyers' information structure about the common value. We use linear programming duality to derive mechanisms that guarantee a good revenue among all information structures and all equilibria. Our mechanism maximizes the revenue guarantee when there is one buyer. As the number of buyers tends to infinity, the revenue guarantee of our mechanism converges to the full surplus.
Abell 611. II. X-ray and strong lensing analyses<|sep|>We present the results of our analyses of the X-ray emission and of the strong lensing systems in the relaxed galaxy cluster Abell 611 (z=0.288). We infer the X-ray mass estimate deriving the density and temperature profiles of the intra-cluster medium within the radius r ~ 700 kpc through a non-parametric approach; assuming that the cluster is in hydrostatic equilibrium and adopting a matter density profile, we can recover the total mass distribution of Abell 611 via the X-ray data. We derive the total projected mass in the central region of Abell 611 performing a parametric analysis of its strong lensing features through the publicly available analysis software Lenstool. As a final step we compare the results obtained with both methods. We derive a good agreement between the X-ray and strong lensing total mass estimates in the central region (i.e. within the radius r ~100 kpc), while a marginal disagreement is found between the two mass estimates when extrapolating the strong lensing results in the outer spatial range. We suggest that in this case the X-ray/strong lensing mass disagreement can be explained by an incorrect estimate of the relative contributions of the baryonic component and of the dark matter, caused by the intrinsic degeneracy between the different mass components in the strong lensing analysis. We discuss the effect of some possible systematic errors that influence both mass estimates. We find a slight dependence of the measurements of the X-ray temperatures (and therefore of the X-ray total masses) on the background adopted in the spectral analysis, with total deviations on the value of M_200 of the order of the 1-sigma statistical error. The strong lensing mass results are instead sensitive to the parameterisation of the galactic halo mass in the central regions, in particular to the modelling of the Brightest Cluster Galaxy (BCG) baryonic component.
Bifurcations of periodic and chaotic attractors in pinball billiards with focusing boundaries<|sep|>We study the dynamics of billiard models with a modified collision rule: the outgoing angle from a collision is a uniform contraction, by a factor lambda, of the incident angle. These pinball billiards interpolate between a one-dimensional map when lambda=0 and the classical Hamiltonian case of elastic collisions when lambda=1. For all lambda<1, the dynamics is dissipative, and thus gives rise to attractors, which may be periodic or chaotic. Motivated by recent rigorous results of Markarian, Pujals and Sambarino, we numerically investigate and characterise the bifurcations of the resulting attractors as the contraction parameter is varied. Some billiards exhibit only periodic attractors, some only chaotic attractors, and others have coexistence of the two types.
Evolution of swarming behavior is shaped by how predators attack<|sep|>Animal grouping behaviors have been widely studied due to their implications for understanding social intelligence, collective cognition, and potential applications in engineering, artificial intelligence, and robotics. An important biological aspect of these studies is discerning which selection pressures favor the evolution of grouping behavior. In the past decade, researchers have begun using evolutionary computation to study the evolutionary effects of these selection pressures in predator-prey models. The selfish herd hypothesis states that concentrated groups arise because prey selfishly attempt to place their conspecifics between themselves and the predator, thus causing an endless cycle of movement toward the center of the group. Using an evolutionary model of a predator-prey system, we show that how predators attack is critical to the evolution of the selfish herd. Following this discovery, we show that density-dependent predation provides an abstraction of Hamilton's original formulation of ``domains of danger.'' Finally, we verify that density-dependent predation provides a sufficient selective advantage for prey to evolve the selfish herd in response to predation by coevolving predators. Thus, our work corroborates Hamilton's selfish herd hypothesis in a digital evolutionary model, refines the assumptions of the selfish herd hypothesis, and generalizes the domain of danger concept to density-dependent predation.
An Order Statistics Approach to the Halo Model for Galaxies<|sep|>We use the Halo Model to explore the implications of assuming that galaxy luminosities in groups are randomly drawn from an underlying luminosity function. We show that even the simplest of such order statistics models -- one in which this luminosity function $p(L)$ is universal -- naturally produces a number of features associated with previous analyses based on the `central plus Poisson satellites' hypothesis. These include the monotonic relation of mean central luminosity with halo mass, the Lognormal distribution around this mean, and the tight relation between the central and satellite mass scales. In stark contrast to observations of galaxy clustering, however, this model predicts $\textit{no}$ luminosity dependence of large scale clustering. We then show that an extended version of this model, based on the order statistics of a $\textit{halo mass dependent}$ luminosity function $p(L|m)$, is in much better agreement with the clustering data as well as satellite luminosities, but systematically under-predicts central luminosities. This brings into focus the idea that central galaxies constitute a distinct population that is affected by different physical processes than are the satellites. We model this physical difference as a statistical brightening of the central luminosities, over and above the order statistics prediction. The magnitude gap between the brightest and second brightest group galaxy is predicted as a by-product, and is also in good agreement with observations. We propose that this order statistics framework provides a useful language in which to compare the Halo Model for galaxies with more physically motivated galaxy formation models
Two regimes in conductivity and the Hall coefficient of underdoped cuprates in strong magnetic fields<|sep|>We address recent experiments shedding light on the energy spectrum of under- and optimally doped cuprates at temperatures above superconducting transition. Angle resolved photoemission reveals coherent excitation only near nodal points on parts of the "bare" Fermi surface known as the Fermi arcs. The question debated in the literature is whether the small normal pocket, seen via quantum oscillations exists at higher temperatures or forms below a charge order transition in strong magnetic fields. Assuming the former case as possibility, expressions derived for the resistivity and the Hall coefficient (in weak and strong magnetic fields) with both types of carriers participating in transport. There are two regimes. At higher temperatures (at a fixed field) electrons are dragged by the Fermi arcs' holes. The pocket being small its contribution into conductivity and the Hall coefficient is negligible. At lower temperatures electrons decouple from holes behaving as a Fermi gas in the magnetic field. Mobility of holes on the arcs decreasing in strong fields with decrease of temperature, below a crossover point the pocket electrons prevail changing sign of the Hall coefficient in the low temperature limit. Such behavior finds its confirmation in recent high-field experiments.
Small air showers in IceTop<|sep|>IceTop is an air shower array that is part of the IceCube Observatory currently under construction at the geographic South Pole. When completed, it will consist of 80 stations covering an area of 1 km2. Previous analyzes done with IceTop studied the events that triggered five or more stations, leading to an effective energy threshold of about 0.5 PeV. The goal of this study is to push this threshold lower, into the region where it will overlap with direct measurements of cosmic rays which currently have an upper limit around 300TeV.We select showers that trigger exactly three or exactly four adjacent surface stations that are not on the periphery of the detector (contained events). This extends the energy threshold down to 150TeV.
The chemistry of the most metal-rich damped Lyman $\alpha$ systems at z$\sim2$ II. Context with the Local Group<|sep|>Using our sample of the most metal-rich damped Lyman $\alpha$ systems (DLAs) at z$\sim2$, and two literature compilations of chemical abundances in 341 DLAs and 2818 stars, we present an analysis of the chemical composition of DLAs in the context of the Local Group. The metal-rich sample of DLAs at z$\sim2$ probes metallicities as high as the Galactic disc and the most metal-rich dwarf spheroidals (dSphs), permitting an analysis of many elements typically observed in DLAs (Fe, Zn, Cr, Mn, Si, and S) in comparison to stellar abundances observed in the Galaxy and its satellites (in particular dSphs). Our main conclusions are: (1) non-solar [Zn/Fe] abundances in metal-poor Galactic stars and in dSphs over the full metallicity range probed by DLAs, suggest that Zn is not a simple proxy for Fe in DLAs and therefore not a suitable indicator of dust depletion. After correcting for dust depletion, the majority of DLAs have subsolar [Zn/Fe] similar to dSphs; (2) at [Fe/H]$\sim-0.5$, a constant [Mn/Fe]$\sim-0.5$ and near-solar [$\alpha$/Fe] (requiring an assumption about dust depletion) are in better agreement with dwarf galaxies than Galactic disc stars; (3) [$\alpha$/Zn] is usually solar or subsolar in DLAs. However, although low ratios of [$\alpha$/Fe] are usually considered more `dwarf-like' than `Milky Way-like', subsolar [Zn/Fe] in Local Group dwarfs leads to supersolar [$\alpha$/Zn] in the dSphs, in contrast with the DLAs. Therefore, whilst DLAs exhibit some similarities with the Local Group dwarf population, there are also notable differences.
Survey geometry and the internal consistency of recent cosmic shear measurements<|sep|>We explore the impact of an update to the typical approximation for the shape noise term in the analytic covariance matrix for cosmic shear experiments that assumes the absence of survey boundary and mask effects. We present an exact expression for the number of galaxy pairs in this term based on the the survey mask, which leads to more than a factor of three increase in the shape noise on the largest measured scales for the Kilo-Degree Survey (KIDS-450) real-space cosmic shear data. We compare the result of this analytic expression to several alternative methods for measuring the shape noise from the data and find excellent agreement. This update to the covariance resolves any internal model tension evidenced by the previously large cosmological best-fit $\chi^2$ for the KiDS-450 cosmic shear data. The best-fit $\chi^2$ is reduced from 161 to 121 for 118 degrees of freedom. We also apply a correction to how the multiplicative shear calibration uncertainty is included in the covariance. This change, along with a previously known update to the reported effective angular values of the data vector, jointly shift the inferred amplitude of the correlation function to higher values. We find that this improves agreement of the KiDS-450 cosmic shear results with Dark Energy Survey Year 1 and Planck results.
Morphological Star-Galaxy Separation<|sep|>We discuss the statistical foundations of morphological star-galaxy separation. We show that many of the star-galaxy separation metrics in common use today (e.g. by SDSS or SExtractor) are closely related both to each other, and to the model odds ratio derived in a Bayesian framework by Sebok (1979). While the scaling of these algorithms with the noise properties of the sources varies, these differences do not strongly differentiate their performance. We construct a model of the performance of a star-galaxy separator in a realistic survey to understand the impact of observational signal-to-noise ratio (or equivalently, 5-sigma limiting depth) and seeing on classification performance. The model quantitatively demonstrates that, assuming realistic densities and angular sizes of stars and galaxies, 10% worse seeing can be compensated for by approximately 0.4 magnitudes deeper data to achieve the same star-galaxy classification performance. We discuss how to probabilistically combine multiple measurements, either of the same type (e.g., subsequent exposures), or differing types (e.g., multiple bandpasses), or differing methodologies (e.g., morphological and color-based classification). These methods are increasingly important for observations at faint magnitudes, where the rapidly rising number density of small galaxies makes star-galaxy classification a challenging problem. However, because of the significant role that the signal-to-noise ratio plays in resolving small galaxies, surveys with large-aperture telescopes, such as LSST, will continue to see improving star-galaxy separation as they push to these fainter magnitudes.
Complex-valued Iris Recognition Network<|sep|>In this work, we design a fully complex-valued neural network for the task of iris recognition. Unlike the problem of general object recognition, where real-valued neural networks can be used to extract pertinent features, iris recognition depends on the extraction of both phase and magnitude information from the input iris texture in order to better represent its biometric content. This necessitates the extraction and processing of phase information that cannot be effectively handled by a real-valued neural network. In this regard, we design a fully complex-valued neural network that can better capture the multi-scale, multi-resolution, and multi-orientation phase and amplitude features of the iris texture. We show a strong correspondence of the proposed complex-valued iris recognition network with Gabor wavelets that are used to generate the classical IrisCode; however, the proposed method enables a new capability of automatic complex-valued feature learning that is tailored for iris recognition. We conduct experiments on three benchmark datasets - ND-CrossSensor-2013, CASIA-Iris-Thousand and UBIRIS.v2 - and show the benefit of the proposed network for the task of iris recognition. We exploit visualization schemes to convey how the complex-valued network, when compared to standard real-valued networks, extracts fundamentally different features from the iris texture.
Isomorphism Testing for Graphs of Bounded Rank Width<|sep|>We give an algorithm that, for every fixed k, decides isomorphism of graphs of rank width at most k in polynomial time. As the clique width of a graph is bounded in terms of its rank width, we also obtain a polynomial time isomorphism test for graph classes of bounded clique width.
Spatial storage of discrete dark solitons<|sep|>The interaction between a mobile discrete dark soliton (DDS) and impurities in one-dimensional nonlinear (Kerr) photonic lattices is studied. We found that the scattering is an inelastic process where the DDS can be reflected or transmitted depending on its transversal speed and the strength of the impurities. In particular, in the reflection regime, the DDS increases its transversal speed after each scattering. A method for spatial storage of DDS solutions using two impurities is discussed, where the soliton can be trapped within a storage region until it reaches the critical speed needed to be transmitted. We show, numerically, that this method allows the storage of multiple DDS simultaneously.
Assessing 3D scan quality in Virtual Reality through paired-comparisons psychophysics test<|sep|>Consumer 3D scanners and depth cameras are increasingly being used to generate content and avatars for Virtual Reality (VR) environments and avoid the inconveniences of hand modeling; however, it is sometimes difficult to evaluate quantitatively the mesh quality at which 3D scans should be exported, and whether the object perception might be affected by its shading. We propose using a paired-comparisons test based on psychophysics of perception to do that evaluation. As psychophysics is not subject to opinion, skill level, mental state, or economic situation it can be considered a quantitative way to measure how people perceive the mesh quality. In particular, we propose using the psychophysical measure for the comparison of four different levels of mesh quality (1K, 5K, 10K and 20K triangles). We present two studies within subjects: in one we investigate the quality perception variations of seeing an object in a regular screen monitor against an stereoscopic Head Mounted Display (HMD); while in the second experiment we aim at detecting the effects of shading into quality perception. At each iteration of the pair-test comparisons participants pick the mesh that they think had higher quality; by the end of the experiment we compile a preference matrix. The matrix evidences the correlation between real quality and assessed quality. Regarding the shading mode, we find an interaction with quality and shading when the model has high definition. Furthermore, we assess the subjective realism of the most/least preferred scans using an Immersive Augmented Reality (IAR) video-see-through setup. Results show higher levels of realism were perceived through the HMD than when using a monitor, although the quality was similarly perceived in both systems.
WISH VI. Constraints on UV and X-ray irradiation from a survey of hydrides in low- to high-mass YSOs<|sep|>Hydrides are simple compounds containing one or a few hydrogen atoms bonded to a heavier atom. They are fundamental precursor molecules in cosmic chemistry and many hydride ions have become observable in high quality for the first time thanks to the Herschel Space Observatory. Ionized hydrides, such as CH+ and OH+, and also HCO+ that affect the chemistry of molecules such as water, provide complementary information on irradiation by far UV (FUV) or X-rays and gas temperature. The targeted lines of CH+, OH+, H2O+, C+ and CH are detected mostly in blue-shifted absorption. H3O+ and SH+ are detected in emission and only toward some high-mass objects. The observed line parameters and correlations suggest two different origins, related to gas entrained by the outflows and to the circumstellar envelope. The column density ratios of CH+/OH+ are estimated from chemical slab models, assuming that the H2 density is given by the specific density model of each object at the beam radius. For the low-mass YSOs the observed ratio can be reproduced for an FUV flux of 2-400 times the ISRF at the location of the molecules. In two high-mass objects, the UV flux is 20-200 times the ISRF derived from absorption lines, and 300-600 ISRF using emission lines. If the FUV flux required for low-mass objects originates at the central protostar, a substantial FUV luminosity, up to 1.5 L_sun, is required. There is no molecular evidence for X-ray induced chemistry in the low-mass objects on the observed scales of a few 1000 AU. For high-mass regions, the FUV flux required to produce the observed molecular ratios is smaller than the unattenuated flux expected from the central object(s) at the Herschel beam radius. This is consistent with an FUV flux reduced by circumstellar extinction or by bloating of the protostar.
Arbitrary Sequence RAMs<|sep|>It is known that in some cases a Random Access Machine (RAM) benefits from having an additional input that is an arbitrary number, satisfying only the criterion of being sufficiently large. This is known as the ARAM model. We introduce a new type of RAM, which we refer to as the Arbitrary Sequence RAM (ASRAM), that generalises the ARAM by allowing the generation of additional arbitrary large numbers at will during execution time. We characterise the power contribution of this ability under several RAM variants. In particular, we demonstrate that an arithmetic ASRAM is more powerful than an arithmetic ARAM, that a sufficiently equipped ASRAM can recognise any language in the arithmetic hierarchy in constant time (and more, if it is given more time), and that, on the other hand, in some cases the ASRAM is no more powerful than its underlying RAM.
Matrix Variate RBM and Its Applications<|sep|>Restricted Boltzmann Machine (RBM) is an importan- t generative model modeling vectorial data. While applying an RBM in practice to images, the data have to be vec- torized. This results in high-dimensional data and valu- able spatial information has got lost in vectorization. In this paper, a Matrix-Variate Restricted Boltzmann Machine (MVRBM) model is proposed by generalizing the classic RBM to explicitly model matrix data. In the new RBM model, both input and hidden variables are in matrix forms which are connected by bilinear transforms. The MVRBM has much less model parameters, resulting in a faster train- ing algorithm while retaining comparable performance as the classic RBM. The advantages of the MVRBM have been demonstrated on two real-world applications: Image super- resolution and handwritten digit recognition.
Adaptive Control of Uncertain Pure-feedback Nonlinear Systems<|sep|>A novel adaptive control approach is proposed to solve the globally asymptotic state stabilization problem for uncertain pure-feedback nonlinear systems which can be transformed into the pseudo-affine form. The pseudo-affine pure-feedback nonlinear system under consideration is with non-linearly parameterised uncertainties and possibly unknown control coefficients. Based on the parameter separation technique, a backstepping controller is designed by adopting the adaptive high gain idea. The rigorous stability analysis shows that the proposed controller could guarantee, for any initial system condition, boundedness of the closed-loop signals and globally asymptotic stabilization of the state. A numerical and a realistic examples are employed to demonstrate the effectiveness of the proposed control method.
SoGCN: Second-Order Graph Convolutional Networks<|sep|>Graph Convolutional Networks (GCN) with multi-hop aggregation is more expressive than one-hop GCN but suffers from higher model complexity. Finding the shortest aggregation range that achieves comparable expressiveness and minimizes this side effect remains an open question. We answer this question by showing that multi-layer second-order graph convolution (SoGC) is sufficient to attain the ability of expressing polynomial spectral filters with arbitrary coefficients. Compared to models with one-hop aggregation, multi-hop propagation, and jump connections, SoGC possesses filter representational completeness while being lightweight, efficient, and easy to implement. Thereby, we suggest that SoGC is a simple design capable of forming the basic building block of GCNs, playing the same role as $3 \times 3$ kernels in CNNs. We build our Second-Order Graph Convolutional Networks (SoGCN) with SoGC and design a synthetic dataset to verify its filter fitting capability to validate these points. For real-world tasks, we present the state-of-the-art performance of SoGCN on the benchmark of node classification, graph classification, and graph regression datasets.
Waves in screeching jets<|sep|>The interaction between various wavelike structures in screeching jets is considered via both experimental measurements and linear stability theory. Velocity snapshots of screeching jets are used to produce a reduced order model of the screech cycle via proper orthogonal decomposition. Streamwise Fourier filtering is then applied to isolate the negative and positive wavenumber components, which for the waves of interest in this jet correspond to upstream and downstream-travelling waves. A global stability analysis on an experimentally derived base flow is conducted, demonstrating a close match to the results obtained via experiment, indicating that the mechanisms considered here are well represented in a linear framework. In both analyses, three distinct wavelike structures are evident. These three waves are those first shown by Tam & Hu (1989) to be supported by a cylindrical vortex sheet. One is the Kelvin-Helmholtz mode. Another is the upstream-travelling guided jet mode that has been a topic of recent discussion. The third component, with positive phase velocity, has not previously been identified in screeching jets. We provide evidence that this downstream-travelling wave is a duct-like mode similar to that recently identified in high-subsonic jets by Towne et al. (2017). We further demonstrate that both of the latter two waves are generated by the interaction between the Kelvin-Helmholtz wavepacket and the shock cells in the flow, according to a theory first proposed in Tam & Tanna (1982). Finally, we consider the periodic spatial modulation of the coherent velocity fluctuation evident in screeching jets, and show that this modulation is the result of the superposition of the three wavelike structures, with no evidence that the shocks in the flow modulate the growth of the Kelvin-Helmholtz wavepacket.
Resolving structural variability in network models and the brain<|sep|>Large-scale white matter pathways crisscrossing the cortex create a complex pattern of connectivity that underlies human cognitive function. Generative mechanisms for this architecture have been difficult to identify in part because little is known about mechanistic drivers of structured networks. Here we contrast network properties derived from diffusion spectrum imaging data of the human brain with 13 synthetic network models chosen to probe the roles of physical network embedding and temporal network growth. We characterize both the empirical and synthetic networks using familiar diagnostics presented in statistical form, as scatter plots and distributions, to reveal the full range of variability of each measure across scales in the network. We focus on the degree distribution, degree assortativity, hierarchy, topological Rentian scaling, and topological fractal scaling---in addition to several summary statistics, including the mean clustering coefficient, shortest path length, and network diameter. The models are investigated in a progressive, branching sequence, aimed at capturing different elements thought to be important in the brain, and range from simple random and regular networks, to models that incorporate specific growth rules and constraints. We find that synthetic models that constrain the network nodes to be embedded in anatomical brain regions tend to produce distributions that are similar to those extracted from the brain. We also find that network models hardcoded to display one network property do not in general also display a second, suggesting that multiple neurobiological mechanisms might be at play in the development of human brain network architecture. Together, the network models that we develop and employ provide a potentially useful starting point for the statistical inference of brain network structure from neuroimaging data.
Terrestrial Planet Formation in Extra-Solar Planetary Systems<|sep|>Terrestrial planets form in a series of dynamical steps from the solid component of circumstellar disks. First, km-sized planetesimals form likely via a combination of sticky collisions, turbulent concentration of solids, and gravitational collapse from micron-sized dust grains in the thin disk midplane. Second, planetesimals coalesce to form Moon- to Mars-sized protoplanets, also called "planetary embryos". Finally, full-sized terrestrial planets accrete from protoplanets and planetesimals. This final stage of accretion lasts about 10-100 Myr and is strongly affected by gravitational perturbations from any gas giant planets, which are constrained to form more quickly, during the 1-10 Myr lifetime of the gaseous component of the disk. It is during this final stage that the bulk compositions and volatile (e.g., water) contents of terrestrial planets are set, depending on their feeding zones and the amount of radial mixing that occurs. The main factors that influence terrestrial planet formation are the mass and surface density profile of the disk, and the perturbations from giant planets and binary companions if they exist. Simple accretion models predicts that low-mass stars should form small, dry planets in their habitable zones. The migration of a giant planet through a disk of rocky bodies does not completely impede terrestrial planet growth. Rather, "hot Jupiter" systems are likely to also contain exterior, very water-rich Earth-like planets, and also "hot Earths", very close-in rocky planets. Roughly one third of the known systems of extra-solar (giant) planets could allow a terrestrial planet to form in the habitable zone.
Thermal conductivity and stability of commercial MgB$_2$ conductors<|sep|>This paper presents a study of the thermal transport properties of MgB$_2$ tapes differing in architecture, stabilization and constituent materials. The temperature and field dependence of thermal conductivity, $\kappa(T,B)$, was investigated both along the conductor and in the direction perpendicular to the tape. These data provide fundamental input parameters to describe the 3D heat diffusion process in a winding. Thermal transport properties - even in field - are typically deduced using semi-empirical formulas based on the residual resistivity ratio of the stabilizer measured in absence of magnetic field. The accuracy of these procedures was evaluated comparing the calculated $\kappa$ values with the measured ones. Based on the experimental thermal conduction properties $\kappa(T,B)$ and critical current surface $J_C(T,B)$ we determined the dependence of minimum quench energy and normal zone propagation velocity on the operating parameters of the conductor. The correlation between thermal properties and tape layout allowed us to provide information on how to optimize the thermal stability of MgB$_2$ conductors.
Production of vector bosons in association with jets in ATLAS<|sep|>Measurements of the production of jets in association with a W/Z boson in proton-proton collisions are presented using data collected by the ATLAS experiment at LHC at sqrt s = 8 and 13 TeV. Several kinematic regimes are explored with various approaches to probe different aspects of these processes. The differential cross sections of a Z boson in association with jets with pT > 30 GeV and |y|< 2.5 at sqrt s = 13 TeV are measured in a fiducial phase space, probing strong interactions that completely dominate in these processes, while measurements of a W boson in association with at least two jets at high pT and high di-jet invariant mass, where the electroweak production is enhanced, are performed with sqrt s = 8 TeV data. Angular distributions in W+jets events with high pT jets are also measured at sqrt s = 8 TeV focusing on small angular separation between the jets and the W decay products, where contributions from real W emission are expected large. Finally a measurement of the splitting scales occurring in the kt jet-clustering algorithm is presented for final states containing a Z boson at sqrt s = 8 TeV. This measurement based on charged-particle track information constitutes a complementary approach to study jet properties. Data distributions of all the measurements are corrected for detector effects and compared with state-of-art predictions.
Quantum-enhanced learning of rotations about an unknown direction<|sep|>We design machines that learn how to rotate a quantum bit about an initially unknown direction, encoded in the state of a spin-j particle. We show that a machine equipped with a quantum memory of O(log j) qubits can outperform all machines with purely classical memory, even if the size of their memory is arbitrarily large. The advantage is present for every finite j and persists as long as the quantum memory is accessed for no more than O(j) times. We establish these results by deriving the ultimate performance achievable with purely classical memories, thus providing a benchmark that can be used to experimentally demonstrate the implementation of quantum-enhanced learning.
Approximating Scheduling Machines with Capacity Constraints<|sep|>In the Scheduling Machines with Capacity Constraints problem, we are given k identical machines, each of which can process at most m_i jobs. M jobs are also given, where job j has a non-negative processing time length t_j >= 0. The task is to find a schedule such that the makespan is minimized and the capacity constraints are met. In this paper, we present a 3-approximation algorithm using an extension of Iterative Rounding Method introduced by Jain. To the best of the authors' knowledge, this is the first attempt to apply Iterative Rounding Method to scheduling problem with capacity constraints.
Unsupervised Image-to-Image Translation Networks<|sep|>Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit .
Probing the physical and chemical structure of the CS core in LDN 673. Multitransitional and continuum observations<|sep|>High-angular resolution observations of dense molecular cores show that these cores can be clumpier at smaller scales, and that some of these clumps can also be unbound or transient. The use of chemical models of the evolution of the molecular gas provides a way to probe the physical properties of the clouds. We study the properties of the clump and inter-clump medium in the starless CS core in LDN 673 by carrying out a molecular line survey with the IRAM 30-m telescope toward two clumps and two inter-clump positions. We also observed the 1.2-mm continuum with the MAMBO-II bolometer at IRAM. The dust continuum map shows four condensations, three of them centrally peaked, coinciding with previously identified sub-millimetre sources. We confirm that the denser clump of the region, $n\sim3.6 \times10^5$\cmt, is also the more chemically evolved, and it could still undergo further fragmentation. The inter-clump medium positions are denser than previously expected, likely $n\sim1\times10^3$--1$\times10^4$\cmt\ due to contamination, and are chemically young, similar to the gas in the lower density clump position. We argue that the density contrast between these positions and their general young chemical age would support the existence of transient clumps in the lower density material of the core. We were also able to find reasonable fits of the observationally derived chemical abundances to models of the chemistry of transient clumps.
A fully-spectroscopic triple-source-plane lens: the Jackpot completed<|sep|>We present a spectroscopic determination of the redshift of the second source in the Jackpot gravitational lens system J0946+1006, for which only a photometric estimate of $z_{\rm phot}$ = 2.41$^{+0.04}_{-0.21}$ has previously been available. By visually inspecting an archival VLT X-Shooter observation, we located a single emission line from the source in the H-band. Among the possible options we find that this line is most likely to be [OIII] 5007 Ang at $z_{\rm spec} $ = 2.035. Guided by this proposal, we were able to detect the faint CIII] 1907,1909 Ang emission doublet in a deep VLT MUSE datacube. The CIII] emission is spatially coincident with the brightest parts of the second Einstein ring, and strongly supports the redshift identification. The spectroscopic redshift is only marginally consistent with the photometric estimate. Re-examining the cosmological constraints from J0946+1006, the revised measurement favours less negative values of the dark energy equation-of-state parameter $w$; when combined with a cosmic microwave background prior, we infer $w$ = $-1.04\pm0.20$. The revised redshift does not significantly help to reconcile the small discrepancy in the image positions for the even more distant third source in J0946+1006.
Stellar evolution in real time: Period variations in galactic RR Lyr stars<|sep|>The times of maximum brightness collected in the GEOS RR Lyr database allowed us to trace the period variations of a sample of 123 galactic RRab variables. These data span a time baseline exceeding 100 years. Clear evidence of period increases or decreases at constant rates has been found, suggesting evolutionary effects. The observed rates are slightly larger than those predicted by theoretical models; moreover, there is an unexpected large percentage of RRab stars showing a period decrease. The new possibilities offered by the use of robotic telecopes (TAROTs, REM) and of data from satellite (CoRoT) are expected to speed up the project to measure stellar evolution in real time. It is noteworthy that the outlines of this project have been sketched during several GEOS meetings, where the different knowledge of amateur and professional astronomers found a very profitable synthesis.
Branching ratios of $\alpha$-decay to excited states of even-even nuclei<|sep|>Branching ratios of $\alpha $-decay to members of the ground state rotational band and excited 0$^{+}$ states of even-even nuclei are calculated in the framework of the generalized liquid drop model (GLDM) by taking into account the angular momentum of the $\alpha$-particle and the excitation probability of the daughter nucleus. The calculation covers isotopic chains from Hg to Fm in the mass regions $180< A <202$ and A$\geq 224$. The calculated branching ratios of the $\alpha $-transitions are in good agreement with the experimental data and some useful predictions are provided for future experiments.
rp-Process weak-interaction mediated rates of waiting-point nuclei<|sep|>Electron capture and positron decay rates are calculated for neutron-deficient Kr and Sr waiting point nuclei in stellar matter. The calculation is performed within the framework of pn-QRPA model for rp-process conditions. Fine tuning of particle-particle, particle-hole interaction parameters and a proper choice of the deformation parameter resulted in an accurate reproduction of the measured half-lives. The same model parameters were used to calculate stellar rates. Inclusion of measured Gamow-Teller strength distributions finally led to a reliable calculation of weak rates that reproduced the measured half-lives well under limiting conditions. For the rp-process conditions, electron capture and positron decay rates on $^{72}$Kr and $^{76}$Sr are of comparable magnitude whereas electron capture rates on $^{78}$Sr and $^{74}$Kr are 1--2 orders of magnitude bigger than the corresponding positron decay rates. The pn-QRPA calculated electron capture rates on $^{74}$Kr are bigger than previously calculated. The present calculation strongly suggests that, under rp-process conditions, electron capture rates form an integral part of weak-interaction mediated rates and should not be neglected in nuclear reaction network calculations as done previously.
Channel adversarial training for speaker verification and diarization<|sep|>Previous work has encouraged domain-invariance in deep speaker embedding by adversarially classifying the dataset or labelled environment to which the generated features belong. We propose a training strategy which aims to produce features that are invariant at the granularity of the recording or channel, a finer grained objective than dataset- or environment-invariance. By training an adversary to predict whether pairs of same-speaker embeddings belong to the same recording in a Siamese fashion, learned features are discouraged from utilizing channel information that may be speaker discriminative during training. Experiments for verification on VoxCeleb and diarization and verification on CALLHOME show promising improvements over a strong baseline in addition to outperforming a dataset-adversarial model. The VoxCeleb model in particular performs well, achieving a $4\%$ relative improvement in EER over a Kaldi baseline, while using a similar architecture and less training data.
Asymmetric silicate dust distribution toward the silicate carbon star BM Gem<|sep|>Silicate carbon stars show the 10 micron silicate emission, despite their carbon-rich photospheres. They are considered to have circumbinary or circum-companion disks, which serve as a reservoir of oxygen-rich material shed by mass loss in the past. We present N-band spectro-interferometric observations of the silicate carbon star BM Gem using MIDI at the Very Large Telescope Interferometer (VLTI). Our aim is to probe the spatial distribution of oxygen-rich dust with high spatial resolution. BM Gem was observed with VLTI/MIDI at 44--62 m baselines using the UT2-UT3 and UT3-UT4 baseline configurations. The N-band visibilities observed for BM Gem show a steep decrease from 8 to ~10 micron and a gradual increase longward of ~10 micron, reflecting the optically thin silicate emission feature emanating from sub-micron-sized amorphous silicate grains. The differential phases obtained at baselines of ~44--46 m show significant non-zero values (~ -70 degrees) in the central part of the silicate emission feature between ~9 and 11 micron, revealing a photocenter shift and the asymmetric nature of the silicate emitting region. The observed N-band visibilities and differential phases can be fairly explained by a simple geometrical model in which the unresolved star is surrounded by a ring with azimuthal brightness modulation. The best-fit model is characterized by a broad ring (~70 mas across at 10 micron) with a bright region which is offset from the unresolved star by ~20 mas at a position angle of ~280 degrees. This model can be interpreted as a system with a circum-companion disk and is consistent with the spectroscopic signatures of an accretion disk around an unseen companion recently discovered in the violet spectrum of BM Gem.
Patient-independent Epileptic Seizure Prediction using Deep Learning Models<|sep|>Objective: Epilepsy is one of the most prevalent neurological diseases among humans and can lead to severe brain injuries, strokes, and brain tumors. Early detection of seizures can help to mitigate injuries, and can be used to aid the treatment of patients with epilepsy. The purpose of a seizure prediction system is to successfully identify the pre-ictal brain stage, which occurs before a seizure event. Patient-independent seizure prediction models are designed to offer accurate performance across multiple subjects within a dataset, and have been identified as a real-world solution to the seizure prediction problem. However, little attention has been given for designing such models to adapt to the high inter-subject variability in EEG data. Methods: We propose two patient-independent deep learning architectures with different learning strategies that can learn a global function utilizing data from multiple subjects. Results: Proposed models achieve state-of-the-art performance for seizure prediction on the CHB-MIT-EEG dataset, demonstrating 88.81% and 91.54% accuracy respectively. Conclusions: The Siamese model trained on the proposed learning strategy is able to learn patterns related to patient variations in data while predicting seizures. Significance: Our models show superior performance for patient-independent seizure prediction, and the same architecture can be used as a patient-specific classifier after model adaptation. We are the first study that employs model interpretation to understand classifier behavior for the task for seizure prediction, and we also show that the MFCC feature map utilized by our models contains predictive biomarkers related to interictal and pre-ictal brain states.
Serverless End Game: Disaggregation enabling Transparency<|sep|>For many years, the distributed systems community has struggled to smooth the transition from local to remote computing. Transparency means concealing the complexities of distributed programming like remote locations, failures or scaling. For us, full transparency implies that we can compile, debug and run unmodified single-machine code over effectively unlimited compute, storage, and memory resources. We elaborate in this article why resource disaggregation in serverless computing is the definitive catalyst to enable full transparency in the Cloud. We demonstrate with two experiments that we can achieve transparency today over disaggregated serverless resources and obtain comparable performance to local executions. We also show that locality cannot be neglected for many problems and we present five open research challenges: granular middleware and locality, memory disaggregation, virtualization, elastic programming models, and optimized deployment. If full transparency is possible, who needs explicit use of middleware if you can treat remote entities as local ones? Can we close the curtains of distributed systems complexity for the majority of users?
Cause-Effect Preservation and Classification using Neurochaos Learning<|sep|>Discovering cause-effect from observational data is an important but challenging problem in science and engineering. In this work, a recently proposed brain inspired learning algorithm namely-\emph{Neurochaos Learning} (NL) is used for the classification of cause-effect from simulated data. The data instances used are generated from coupled AR processes, coupled 1D chaotic skew tent maps, coupled 1D chaotic logistic maps and a real-world prey-predator system. The proposed method consistently outperforms a five layer Deep Neural Network architecture for coupling coefficient values ranging from $0.1$ to $0.7$. Further, we investigate the preservation of causality in the feature extracted space of NL using Granger Causality (GC) for coupled AR processes and and Compression-Complexity Causality (CCC) for coupled chaotic systems and real-world prey-predator dataset. This ability of NL to preserve causality under a chaotic transformation and successfully classify cause and effect time series (including a transfer learning scenario) is highly desirable in causal machine learning applications.
Theory and Phenomenology of Composite 2-Higgs Doublet Models<|sep|>In this talk, we report unitarity constraints and phenomenological studies at the Large Hadron Collider for the extra Higgs bosons of a Composite 2-Higgs Doublet Model.
Structure, Energy, and Thermal Transport Properties of Si-SiO$_2$ Nanostructures using an Ab initio based Parameterization of a Charge-Optimized Many-Body Forcefield<|sep|>In an effort to extend the reach of current ab initio calculations to simulations requiring millions of configurations for complex systems such as heterostructures, we have parameterized the third-generation Charge Optimized Many-Body (COMB3) potential using solely ab initio total energies, forces, and stress tensors as input. The quality and the predictive power of the new forcefield is assessed by computing properties including the cohesive energy and density of SiO$_2$ polymorphs, surface energies of alpha-quartz, and phonon densities of states of crystalline and amorphous phases of SiO$_2$. Comparison with data from experiments, ab initio calculations, and molecular dynamics simulations using published forcefields including BKS (van Beest, Kramer, and van Santen), ReaxFF, and COMB2 demonstrate an overall improvement of the new parameterization. The computed temperature dependence of the thermal conductivity of crystalline alpha-quartz and the Kapitza resistance of the interface between crystalline Si(001) and amorphous silica are in excellent agreement with experiment, setting the stage for simulations of complex nanoscale heterostructures.
Transport Studies in a Gate-Tunable Three-Terminal Josephson Junction<|sep|>Josephson junctions with three or more superconducting leads have been predicted to exhibit topological effects in the presence of few conducting modes within the interstitial normal material. Such behavior, of relevance for topologically-protected quantum bits, would lead to specific transport features measured between terminals, with topological phase transitions occurring as a function of phase and voltage bias. Although conventional, two-terminal Josephson junctions have been studied extensively, multi-terminal devices have received relatively little attention to date. Motivated in part by the possibility to ultimately observe topological phenomena in multi-terminal Josephson devices, as well as their potential for coupling gatemon qubits, here we describe the superconducting features of a top-gated mesoscopic three-terminal Josephson device. The device is based on an InAs two-dimensional electron gas (2DEG) proximitized by epitaxial aluminum. We map out the transport properties of the device as a function of bias currents, top gate voltage and magnetic field. We find a very good agreement between the zero-field experimental phase diagram and a resistively and capacitively shunted junction (RCSJ) computational model.
Three-dimensional simulations of multiple protoplanets embedded in a protostellar disc<|sep|>Protoplanet eccentricities of e >~ H/r can slow or reverse migration, but previous 2D studies have shown that gravitational scattering cannot maintain significant planet eccentricities against disc-induced damping. We simulate the evolution of low-mass protoplanetary swarms in three dimensions. The aim is to examine both protoplanet survival rates and the dynamical structure of the resulting planetary systems, and to compare them with 2D simulations. We present results from a 3D hydrodynamic simulation of eight protoplanets embedded in a protoplanetary disc. We also present a suite of simulations performed using an N-body code, modified to include prescriptions for planetary migration and for eccentricity and inclination damping. These prescriptions were obtained by fitting analytic formulae to hydrodynamic simulations of planets embedded in discs with initially eccentric and/or inclined orbits. As was found in two dimensions, differential migration produces groups of protoplanets in stable, multiple mean-motion resonances that migrate in lockstep, preventing prolonged periods of gravitational scattering. In almost all simulations, this leads to large-scale migration of the protoplanet swarm into the central star in the absence of a viable stopping mechanism. The evolution involves mutual collisions, occasional instances of large-scale scattering, and the frequent formation of the long-lived, co-orbital planet systems that arise in > 30% of all runs. Disc-induced damping overwhelms eccentricity and inclination growth due to planet-planet interactions. Co-orbital planets are a natural outcome of dynamical relaxation in a strongly dissipative environment, and if observed in nature would imply that such a period of evolution commonly arises during planetary formation.
Kinetic Monte Carlo Simulation of Single-Electron Multiple-Trapping Transport in Disordered Media<|sep|>The conventional single-particle Monte Carlo simulation of charge transport in disordered media is based on the truncated density of localized states (DOLS) which benefits from very short time execution. Although this model successfully clarifies the properties of electron transport in moderately disordered media, it overestimates the electron diffusion coefficient for strongly disordered media. The origin of this deviation is discussed in terms of zero-temperature approximation in the truncated DOLS and the ignorance of spatial occupation of localized states. Here, based on the multiple-trapping regime we introduce a modified single-particle kinetic Monte Carlo model that can be used to investigate the electron transport in any disordered media independent from the value of disorder parameter. In the proposed model, instead of using a truncated DOLS we imply the raw DOLS. In addition, we have introduced an occupation index for localized states to consider the effect of spatial occupation of trap sites. The proposed model is justified in a simple cubic lattice of trap sites for broad interval of disorder parameters, Fermi levels, and temperatures.
Inquiry-based learning in a first-year honors course<|sep|>We describe a case study of a problem-solving section, using the "Harkness" discussion method, of an honors multivariable calculus course. Students in the problem-solving section had equivalent outcomes on exams, reported higher ratings in self-assessments of skills, and took more math classes in the following year, compared to students in the lecture-based sections.
Sweeping Away the Mysteries of Dusty Continuous Winds in AGN<|sep|>An integral part of the Unified Model for Active Galactic Nuclei (AGNs) is an axisymmetric obscuring medium, which is commonly depicted as a torus of gas and dust surrounding the central engine. However, a robust, dynamical model of the torus is required in order to understand the fundamental physics of AGNs and interpret their observational signatures. Here we explore self-similar, dusty disk-winds, driven by both magnetocentrifugal forces and radiation pressure, as an explanation for the torus. Using these models, we make predictions of AGN infrared (IR) spectral energy distributions (SEDs) from 2-100 microns by varying parameters such as: the viewing angle; the base column density of the wind; the Eddington ratio; the black hole mass; and the amount of power in the input spectrum emitted in the X-ray relative to that emitted in the UV/optical. We find that models with N_H,0 = 10^25 cm^-2, L/L_Edd = 0.1, and M_BH >= 10^8 Msun are able to adequately approximate the general shape and amount of power expected in the IR as observed in a composite of optically luminous Sloan Digital Sky Survey (SDSS) quasars. The effect of varying the relative power coming out in X-rays relative to the UV is a change in the emission below ~5 micron from the hottest dust grains; this arises from the differing contributions to heating and acceleration of UV and X-ray photons. We see mass outflows ranging from ~1-4 Msun/yr, terminal velocities ranging from ~1900-8000 km/s, and kinetic luminosities ranging from ~1x10^42-8x10^43 erg/s. Further development of this model holds promise for using specific features of observed IR spectra in AGNs to infer fundamental physical parameters of the systems.
Strong decays of the bottom mesons $B_1(5721)$, $B_2(5747)$, $B_{s1}(5830)$, $B_{s2}(5840)$ and $B(5970)$<|sep|>In this article, we study the two-body strong decays of the bottom mesons with the heavy meson effective theory in the leading order approximation, and obtain all the analytical expressions of the decay widths of the light pseudoscalar mesons transitions among the S-wave, P-wave and D-wave bottom mesons. As an application, we tentatively assign the bottom meson $B(5970)$ as the $2{\rm S}\,1^-$, $1{\rm D}\,1^-$ and $1{\rm D}\,3^-$ states, respectively, and calculate the decay widths of the $B_1(5721)$, $B_2(5747)$, $B_{s1}(5830)$, $B_{s2}(5840)$ and $B(5970)$, which can be confronted with the experimental data in the future.
Lifelong Neural Topic Learning in Contextualized Autoregressive Topic Models of Language via Informative Transfers<|sep|>Topic models such as LDA, DocNADE, iDocNADEe have been popular in document analysis. However, the traditional topic models have several limitations including: (1) Bag-of-words (BoW) assumption, where they ignore word ordering, (2) Data sparsity, where the application of topic models is challenging due to limited word co-occurrences, leading to incoherent topics and (3) No Continuous Learning framework for topic learning in lifelong fashion, exploiting historical knowledge (or latent topics) and minimizing catastrophic forgetting. This thesis focuses on addressing the above challenges within neural topic modeling framework. We propose: (1) Contextualized topic model that combines a topic and a language model and introduces linguistic structures (such as word ordering, syntactic and semantic features, etc.) in topic modeling, (2) A novel lifelong learning mechanism into neural topic modeling framework to demonstrate continuous learning in sequential document collections and minimizing catastrophic forgetting. Additionally, we perform a selective data augmentation to alleviate the need for complete historical corpora during data hallucination or replay.
Initial guesses for multi-shift solvers<|sep|>I will present a method for providing initial guesses to a linear solver for systems with multiple shifts. This can also be extended to the case of multiple sources each with a different shift.
Disc-corona interaction in the heartbeat state of GRS 1915+105<|sep|>Timing analysis provides information about the dynamics of matter accreting on to neutron stars and black holes, and hence is crucial for studying the physics of the accretion flow around these objects. It is difficult, however, to associate the different variability components with each of the spectral components of the accretion flow. We apply several new methods to two Rossi X-ray Timing Explorer observations of the black hole binary GRS 1915+105 during its heartbeat state to explore the origin of the X-ray variability and the interactions of the accretion-flow components. We offer a promising window into the disc--corona interaction through analysing the formation regions of the disc aperiodic variabilities with different time-scales via comparing the corresponding transition energies of the amplitude-ratio spectra. In a previous paper, we analysed the Fourier power density as a function of energy and frequency to study the origin of the aperiodic variability, and combined that analysis with the phase lag as a function of frequency to derive a picture of the disc--corona interaction in this source. We here, for the first time, investigate the phase lag as a function of energy and frequency, and display some interesting details of the disc--corona interaction. Besides, the results from the shape of amplitude-ratio spectrum and from several other aspects suggest that the quasi-periodic oscillation originates from the corona.
Inverse Seesaw in NMSSM and 126 GeV Higgs Boson<|sep|>We consider extensions of the next-to-minimal supersymmetric model (NMSSM) in which the observed neutrino masses are generated through a TeV scale inverse seesaw mechanism. The new particles associated with this mechanism can have sizable couplings to the Higgs field which can yield a large contribution to the mass of the lightest CP-even Higgs boson. With this new contribution, a 126 GeV Higgs is possible along with order of 200 GeV masses for the stop quarks for a broad range of \tan\beta. The Higgs production and decay in the diphoton channel can be enhanced due to this new contribution. It is also possible to solve the little hierarchy problem in this model without invoking a maximal value for the NMSSM trilinear coupling and without severe restrictions on the value of \tan\beta.
Modifying the Standard Disk Model for the Ultraviolet Spectral Analysis of Disk-dominated Cataclysmic Variables. I. The Novalikes MV Lyrae, BZ Camelopardalis, and V592 Cassiopeiae<|sep|>The standard disk is often inadequate to model disk-dominated cataclysmic variables (CVs) and generates a spectrum that is bluer than the observed UV spectra [Puebla et al 2007]. X-ray observations of these systems reveal an optically thin boundary layer (BL) expected to appear as an inner hole in the disk. Consequently, we truncate the inner disk. However, instead of removing the inner disk, we impose the no-shear boundary condition at the truncation radius, thereby lowering the disk temperature and generating a spectrum that better fits the UV data. With our modified disk, we analyze the archival UV spectra of three novalikes that cannot be fitted with standard disks. For the VY Scl systems MV Lyr and BZ Cam, we fit a hot inflated white dwarf WD with a cold modified disk ($\dot{M} \sim $ a few $10^{-9}M_{\odot}$/yr). For V592 Cas, the slightly modified disk ($\dot{M} \sim 6 \times 10^{-9}M_{\odot}$/yr) completely dominates the UV. These results are consistent with Swift X-ray observations of these systems [Balman et al 2014], revealing BLs merged with ADAF-like flows and/or hot coronae, where the advection of energy is likely launching an outflow and heating the WD, thereby explaining the high WD temperature in VY Scl systems. This is further supported by the fact that the X-ray hardness ratio increases with the shallowness of the UV slope in a small CV sample we examine. Furthermore, for 105 disk-dominated systems, the International Ultraviolet Explorer (IUE) spectra UV slope decreases in the same order as the ratio of the X-ray flux to optical/UV flux: from SU UMa's, to U Gem's, Z Cam's, UX UMa's, and VY Scl's.
A possible relation between flare activity in superluminous supernovae and gamma-ray bursts<|sep|>Significant undulations appear in the light curve of a recently discovered super-luminous supernova (SLSN) SN 2015bn after the first peak, while the underlying profile of the light curve can be well explained by a continuous energy supply from a central engine, possibly the spin-down of a millisecond magnetar. We propose that these undulations are caused by an intermittent pulsed energy supply, indicating an energetic flare activity of the central engine of the SLSN. Many post-burst flares were discovered during X-ray afterglow observations of Gamma-Ray Bursts (GRBs). We find that the SLSN flares described here approximately obey the empirical correlation between the luminosity and time scale of GRB flares, extrapolated to the relevant longer time scales of SLSN flares. This confirms the possible connection between these two different phenomena as previously suggested.
Regularization of an autoconvolution problem in ultrashort laser pulse characterization<|sep|>An ill-posed inverse problem of autoconvolution type is investigated. This inverse problem occurs in nonlinear optics in the context of ultrashort laser pulse characterization. The novelty of the mathematical model consists in a physically required extension of the deautoconvolution problem beyond the classical case usually discussed in literature: (i) For measurements of ultrashort laser pulses with the self-diffraction SPIDER method, a stable approximate solution of an autocovolution equation with a complex-valued kernel function is needed. (ii) The considered scenario requires complex functions both, in the solution and the rhs of the integral equation. Since, however, noisy data are available not only for amplitude and phase functions of the rhs, but also for the amplitude of the solution, the stable approximate reconstruction of the associated smooth phase function represents the main goal of the paper. An iterative regularization approach is described that is specifically adapted to the physical situation in pulse characterization, using a non-standard stopping rule for the iteration process of computing regularized solutions. Our approach is illustrated by several case studies for synthetic noisy data and physically realistic complex-valued kernel functions. Based on an example with focus on amplitude perturbations, we show that the autoconvolution equation is locally ill-posed everywhere. To date, the analytical treatment of the impact of noisy data on phase perturbations remains an open question. However, we show its influence with the help of numerical experiments. Moreover, we formulate assertions on the non-uniqueness of the complex-valued autoconvolution problem, at least for the simplified case of a constant kernel. The presented results and figures associated with case studies illustrate the ill-posedness phenomena also for the case of non-trivial complex kernel functions.
Shaping oscillations via mixed feedback<|sep|>We study the problem of controlling oscillations in closed loop by combining positive and negative feedback in a mixed configuration. We develop a complete design procedure to set the relative strength of the two feedback loops to achieve steady oscillations. The proposed design takes advantage of dominance theory and adopts classical harmonic balance and fast/slow analysis to regulate the frequency of oscillations. The design is illustrated on a simple two-mass system, a setting that reveals the potential of the approach for locomotion, mimicking approaches based on central pattern generators.
Rate Splitting for Massive MIMO Multi-carrier system using Full Duplex Decode and Forward Relay with Hardware Impairments<|sep|>In this paper, we address the power allocation problem for a decode and forward (DF) relay system, where a massive multiple-input-multiple-output (mMIMO) multi-carrier (MC) base station (BS) node communicates with a MC single antenna node directly and also through the single antenna full duplex (FD) MC relay, using rate splitting (RS) approach. Successive interference cancellation approach is adopted at the destination. We consider orthogonal frequency division multiplexing (OFDM) as our MC strategy. We take into account the impact of hardware distortions resulting in residual self-interference and inter-carrier leakage (ICL), and also imperfect channel state information (CSI). We formulate a joint sub-carrier and power allocation problem to maximize the total sum rate. An iterative optimization method is proposed, which follows successive inner approximation (SIA) framework to reach the convergence point that satisfies the Karush-Kuhn-Tucker (KKT) conditions. Numerical results show the significance of distortion-aware design for such systems, and also the significant gain in terms of sum rate compared to its half duplex (HD) and also non-rate splitting scheme.
Adapted sampling for 3D X-ray computed tomography<|sep|>In this paper, we introduce a method to build an adapted mesh representation of a 3D object for X-Ray tomography reconstruction. Using this representation, we provide means to reduce the computational cost of reconstruction by way of iterative algorithms. The adapted sampling of the reconstruction space is directly obtained from the projection dataset and prior to any reconstruction. It is built following two stages : firstly, 2D structural information is extracted from the projection images and is secondly merged in 3D to obtain a 3D pointcloud sampling the interfaces of the object. A relevant mesh is then built from this cloud by way of tetrahedralization. Critical parameters selections have been automatized through a statistical framework, thus avoiding dependence on users expertise. Applying this approach on geometrical shapes and on a 3D Shepp-Logan phantom, we show the relevance of such a sampling - obtained in a few seconds - and the drastic decrease in cells number to be estimated during reconstruction when compared to the usual regular voxel lattice. A first iterative reconstruction of the Shepp-Logan using this kind of sampling shows the relevant advantages in terms of low dose or sparse acquisition sampling contexts. The method can also prove useful for other applications such as finite element method computations.
The Latin American Giant Observatory: a successful collaboration in Latin America based on Cosmic Rays and computer science domains<|sep|>In this work the strategy of the Latin American Giant Observatory (LAGO) to build a Latin American collaboration is presented. Installing Cosmic Rays detectors settled all around the Continent, from Mexico to the Antarctica, this collaboration is forming a community that embraces both high energy physicist and computer scientists. This is so because the data that are measured must be analytical processed and due to the fact that \textit{a priori} and \textit{a posteriori} simulations representing the effects of the radiation must be performed. To perform the calculi, customized codes have been implemented by the collaboration. With regard to the huge amount of data emerging from this network of sensors and from the computational simulations performed in a diversity of computing architectures and e-infrastructures, an effort is being carried out to catalog and preserve a vast amount of data produced by the water-Cherenkov Detector network and the complete LAGO simulation workflow that characterize each site. Metadata, Permanent Identifiers and the facilities from the LAGO Data Repository are described in this work jointly with the simulation codes used. These initiatives allow researchers to produce and find data and to directly use them in a code running by means of a Science Gateway that provides access to different clusters, Grid and Cloud infrastructures worldwide.
The Newton-Shamanskii method for solving a quadratic matrix equation arising in quasi-birth-death problems<|sep|>In order to determine the stationary distribution for discrete time quasi-birth-death Markov chains, it is necessary to find the minimal nonnegative solution of a quadratic matrix equation. We apply the Newton-Shamanskii method for solving the equation. We show that the sequence of matrices generated by the Newton-Shamanskii method is monotonically increasing and converges to the minimal nonnegative solution of the equation. Numerical experiments show the effectiveness of our method.
Improving tag recommendation by folding in more consistency<|sep|>Tag recommendation is a major aspect of collaborative tagging systems. It aims to recommend tags to a user for tagging an item. In this paper we present a part of our work in progress which is a novel improvement of recommendations by re-ranking the output of a tag recommender. We mine association rules between candidates tags in order to determine a more consistent list of tags to recommend. Our method is an add-on one which leads to better recommendations as we show in this paper. It is easily parallelizable and morever it may be applied to a lot of tag recommenders. The experiments we did on five datasets with two kinds of tag recommender demonstrated the efficiency of our method.
Fully Convolutional Crowd Counting On Highly Congested Scenes<|sep|>In this paper we advance the state-of-the-art for crowd counting in high density scenes by further exploring the idea of a fully convolutional crowd counting model introduced by (Zhang et al., 2016). Producing an accurate and robust crowd count estimator using computer vision techniques has attracted significant research interest in recent years. Applications for crowd counting systems exist in many diverse areas including city planning, retail, and of course general public safety. Developing a highly generalised counting model that can be deployed in any surveillance scenario with any camera perspective is the key objective for research in this area. Techniques developed in the past have generally performed poorly in highly congested scenes with several thousands of people in frame (Rodriguez et al., 2011). Our approach, influenced by the work of (Zhang et al., 2016), consists of the following contributions: (1) A training set augmentation scheme that minimises redundancy among training samples to improve model generalisation and overall counting performance; (2) a deep, single column, fully convolutional network (FCN) architecture; (3) a multi-scale averaging step during inference. The developed technique can analyse images of any resolution or aspect ratio and achieves state-of-the-art counting performance on the Shanghaitech Part B and UCF CC 50 datasets as well as competitive performance on Shanghaitech Part A.
SwinIQA: Learned Swin Distance for Compressed Image Quality Assessment<|sep|>Image compression has raised widespread interest recently due to its significant importance for multimedia storage and transmission. Meanwhile, a reliable image quality assessment (IQA) for compressed images can not only help to verify the performance of various compression algorithms but also help to guide the compression optimization in turn. In this paper, we design a full-reference image quality assessment metric SwinIQA to measure the perceptual quality of compressed images in a learned Swin distance space. It is known that the compression artifacts are usually non-uniformly distributed with diverse distortion types and degrees. To warp the compressed images into the shared representation space while maintaining the complex distortion information, we extract the hierarchical feature representations from each stage of the Swin Transformer. Besides, we utilize cross attention operation to map the extracted feature representations into a learned Swin distance space. Experimental results show that the proposed metric achieves higher consistency with human's perceptual judgment compared with both traditional methods and learning-based methods on CLIC datasets.
Enhanced Seebeck effect in graphene devices by strain and doping engineering<|sep|>In this work, we investigate the possibility of enhancing the thermoelectric power (Seebeck coefficient) in graphene devices by strain and doping engineering. While a local strain can result in the misalignment of Dirac cones of different graphene sections in the k-space, doping engineering leads to their displacement in energy. By combining these two effects, we demonstrate that a conduction gap as large as a few hundreds meV can be achieved and hence the enhanced Seebeck coefficient can reach a value higher than 1.4 mV/K in graphene doped heterojunctions with a locally strained area. Such hetero-channels appear to be very promising for enlarging the applications of graphene devices as in strain and thermal sensors.
Rare event failure test case generation in Learning-Enabled-Controllers<|sep|>Machine learning models have prevalent applications in many real-world problems, which increases the importance of correctness in the behaviour of these trained models. Finding a good test case that can reveal the potential failure in these trained systems can help to retrain these models to increase their correctness. For a well-trained model, the occurrence of a failure is rare. Consequently, searching these rare scenarios by evaluating each sample in input search space or randomized search would be costly and sometimes intractable due to large search space, limited computational resources, and available time. In this paper, we tried to address this challenge of finding these failure scenarios faster than traditional randomized search. The central idea of our approach is to separate the input data space in region of high failure probability and region of low/minimal failure probability based on the observation made by training data, data drawn from real-world statistics, and knowledge from a domain expert. Using these information, we can design a generative model from which we can generate scenarios that have a high likelihood to reveal the potential failure. We evaluated this approach on two different experimental scenarios and able to speed up the discovery of such failures a thousand-fold faster than the traditional randomized search.
CO(1-0) line imaging of massive star-forming disc galaxies at z=1.5-2.2<|sep|>We present detections of the CO(J= 1-0) emission line in a sample of four massive star-forming galaxies at z~1.5-2.2 obtained with the Karl G. Jansky Very Large Array (VLA). Combining these observations with previous CO(2-1) and CO(3-2) detections of these galaxies, we study the excitation properties of the molecular gas in our sample sources. We find an average line brightness temperature ratios of R_{21}=0.70+\-0.16 and R_{31}=0.50+\-0.29, based on measurements for three and two galaxies, respectively. These results provide additional support to previous indications of sub-thermal gas excitation for the CO(3-2) line with a typically assumed line ratio R_{31}~0.5. For one of our targets, BzK-21000, we present spatially resolved CO line maps. At the resolution of 0.18 arcsec (1.5 kpc), most of the emission is resolved out except for some clumpy structure. From this, we attempt to identify molecular gas clumps in the data cube, finding 4 possible candidates. We estimate that <40 % of the molecular gas is confined to giant clumps (~1.5 kpc in size), and thus most of the gas could be distributed in small fainter clouds or in fairly diffuse extended regions of lower brightness temperatures than our sensitivity limit.
Support Recovery of Sparse Signals in the Presence of Multiple Measurement Vectors<|sep|>This paper studies the problem of support recovery of sparse signals based on multiple measurement vectors (MMV). The MMV support recovery problem is connected to the problem of decoding messages in a Single-Input Multiple-Output (SIMO) multiple access channel (MAC), thereby enabling an information theoretic framework for analyzing performance limits in recovering the support of sparse signals. Sharp sufficient and necessary conditions for successful support recovery are derived in terms of the number of measurements per measurement vector, the number of nonzero rows, the measurement noise level, and especially the number of measurement vectors. Through the interpretations of the results, in particular the connection to the multiple output communication system, the benefit of having MMV for sparse signal recovery is illustrated providing a theoretical foundation to the performance improvement enabled by MMV as observed in many existing simulation results. In particular, it is shown that the structure (rank) of the matrix formed by the nonzero entries plays an important role on the performance limits of support recovery.
Rehearsal: A Configuration Verification Tool for Puppet<|sep|>Large-scale data centers and cloud computing have turned system configuration into a challenging problem. Several widely-publicized outages have been blamed not on software bugs, but on configuration bugs. To cope, thousands of organizations use system configuration languages to manage their computing infrastructure. Of these, Puppet is the most widely used with thousands of paying customers and many more open-source users. The heart of Puppet is a domain-specific language that describes the state of a system. Puppet already performs some basic static checks, but they only prevent a narrow range of errors. Furthermore, testing is ineffective because many errors are only triggered under specific machine states that are difficult to predict and reproduce. With several examples, we show that a key problem with Puppet is that configurations can be non-deterministic. This paper presents Rehearsal, a verification tool for Puppet configurations. Rehearsal implements a sound, complete, and scalable determinacy analysis for Puppet. To develop it, we (1) present a formal semantics for Puppet, (2) use several analyses to shrink our models to a tractable size, and (3) frame determinism-checking as decidable formulas for an SMT solver. Rehearsal then leverages the determinacy analysis to check other important properties, such as idempotency. Finally, we apply Rehearsal to several real-world Puppet configurations.
Ultraviolet and X-ray variability of active galactic nuclei with Swift<|sep|>We analyse a sample of 21 active galactic nuclei (AGN) using data from the Swift satellite to study the variability properties of the population in the X-ray, UV and optical band. We find that the variable part of the UV-optical emission has a spectrum consistent with a powerlaw, with an average index of $-2.21\pm0.13$, as would be expected from central illumination of a thin disc (index of -7/3). We also calculate the slope of a powerlaw from UV to X-ray variable emission, $\alpha_{\rm OX,Var}$; the average for this sample is $\alpha_{\rm OX,Var} = -1.06 \pm 0.04$. The anticorrelation of $\alpha_{\rm OX}$ with the UV luminosity, $L_{\rm UV}$, previously found in the average emission is also present in the variable part: $\alpha_{\rm OX,Var} = (-0.177 \pm 0.083) {\rm log} (L_{\rm \nu,Var} (2500\,\AA)) + (3.88 \pm 2.33)$. Correlated variability between the emission in X-rays and UV is detected significantly for 9 of the 21 sources. All these cases are consistent with the UV lagging the X-rays, as would be seen if the correlated UV variations were produced by the reprocessing of X-ray emission. The observed UV lags are tentatively longer than expected for a standard thin disc.
Decomposing Digital Paintings into Layers via RGB-space Geometry<|sep|>In digital painting software, layers organize paintings. However, layers are not explicitly represented, transmitted, or published with the final digital painting. We propose a technique to decompose a digital painting into layers. In our decomposition, each layer represents a coat of paint of a single paint color applied with varying opacity throughout the image. Our decomposition is based on the painting's RGB-space geometry. In RGB-space, a geometric structure is revealed due to the linear nature of the standard Porter-Duff "over" pixel compositing operation. The vertices of the convex hull of pixels in RGB-space suggest paint colors. Users choose the degree of simplification to perform on the convex hull, as well as a layer order for the colors. We solve a constrained optimization problem to find maximally translucent, spatially coherent opacity for each layer, such that the composition of the layers reproduces the original image. We demonstrate the utility of the resulting decompositions for re-editing.
A Log Auditing Approach for Trust Management in Peer-to-Peer Collaboration<|sep|>Nowadays we are faced with an increasing popularity of social software including wikis, blogs, micro-blogs and online social networks such as Facebook and MySpace. Unfortunately, the mostly used social services are centralized and personal information is stored at a single vendor. This results in potential privacy problems as users do not have much control over how their private data is disseminated. To overcome this limitation, some recent approaches envisioned replacing the single authority centralization of services by a peer-to-peer trust-based approach where users can decide with whom they want to share their private data. In this peer-to-peer collaboration it is very difficult to ensure that after data is shared with other peers, these peers will not misbehave and violate data privacy. In this paper we propose a mechanism that addresses the issue of data privacy violation due to data disclosure to malicious peers. In our approach trust values between users are adjusted according to their previous activities on the shared data. Users share their private data by specifying some obligations the receivers must follow. We log modifications done by users on the shared data as well as the obligations that must be followed when data is shared. By a log-auditing mechanism we detect users that misbehaved and we adjust their associated trust values by using any existing decentralized trust model.
Informational Neurobayesian Approach to Neural Networks Training. Opportunities and Prospects<|sep|>A study of the classification problem in context of information theory is presented in the paper. Current research in that field is focused on optimisation and bayesian approach. Although that gives satisfying results, they require a vast amount of data and computations to train on. Authors propose a new concept named Informational Neurobayesian Approach (INA), which allows to solve the same problems, but requires significantly less training data as well as computational power. Experiments were conducted to compare its performance with the traditional one and the results showed that capacity of the INA is quite promising.
Temperature-Dependent Band Structure of SrTiO$_3$ Interfaces<|sep|>We build a theoretical model for the electronic properties of the two-dimensional (2D) electron gas that forms at the interface between insulating SrTiO$_3$ and a number of polar cap layers, including LaTiO$_3$, LaAlO$_3$, and GdTiO$_3$. The model treats conduction electrons within a tight-binding approximation, and the dielectric polarization via a Landau-Devonshire free energy that incorporates strontium titanate's strongly nonlinear, nonlocal, and temperature-dependent dielectric response. The self-consistent band structure comprises a mix of quantum 2D states that are tightly bound to the interface, and quasi-three-dimensional (3D) states that extend hundreds of unit cells into the SrTiO$_3$ substrate. We find that there is a substantial shift of electrons away from the interface into the 3D tails as temperature is lowered from 300 K to 10 K. This shift is least important at high electron densities ($\sim 10^{14}$ cm$^{-2}$), but becomes substantial at low densities; for example, the total electron density within 4~nm of the interface changes by a factor of two for 2D electron densities $\sim 10^{13}$ cm$^{-2}$. We speculate that the quasi-3D tails form the low-density high-mobility component of the interfacial electron gas that is widely inferred from magnetoresistance measurements.
Super stellar clusters with a bimodal hydrodynamic solution: an Approximate Analytic Approach<|sep|>We look for a simple analytic model to distinguish between stellar clusters undergoing a bimodal hydrodynamic solution from those able to drive only a stationary wind. Clusters in the bimodal regime undergo strong radiative cooling within their densest inner regions, which results in the accumulation of the matter injected by supernovae and stellar winds and eventually in the formation of further stellar generations, while their outer regions sustain a stationary wind. The analytic formulae are derived from the basic hydrodynamic equations. Our main assumption, that the density at the star cluster surface scales almost linearly with that at the stagnation radius, is based on results from semi-analytic and full numerical calculations. The analytic formulation allows for the determination of the threshold mechanical luminosity that separates clusters evolving in either of the two solutions. It is possible to fix the stagnation radius by simple analytic expressions and thus to determine the fractions of the deposited matter that clusters evolving in the bimodal regime blow out as a wind or recycle into further stellar generations.
Superfluid Effective Field Theory for Dark Matter Direct Detection<|sep|>We develop an effective field theory (EFT) framework for superfluid ${}^4$He to model the interactions among quasiparticles, helium atoms and probe particles. Our effective field theory approach brings together symmetry arguments and power-counting and matches to classical fluid dynamics. We then present the decay and scattering rates for the relevant processes involving quasiparticles and helium atoms. The presented EFT framework and results can be used to understand the dynamics of thermalization in the superfluid, and can be further applied to sub-GeV dark matter direct detection with superfluid ${}^4$He.
System of equations and staggered solution algorithm for immiscible two-phase flow coupled with linear poromechanics<|sep|>This document is a presentation of the equations of simultaneous water and oil flow in deformable porous medium and linear poromechanics as well as the staggered solution algorithm to solve the coupled system of equations.
Dynamic physical layer equalization in optical communication networks<|sep|>In optical transport networks, signal lightpaths between two terminal nodes can be different due to current network conditions. Thus the transmission distance and accumulated dispersion in the lightpath cannot be predicted. Therefore, the adaptive compensation of dynamic dispersion is necessary in such networks to enable flexible routing and switching. In this paper, we present a detailed analysis on the adaptive dispersion compensation using the least-mean-square (LMS) algorithm in coherent optical communication networks. It is found that the variable-step-size LMS equalizer can achieve the same performance with a lower complexity, compared to the traditional LMS algorithm.
Cycles and Clustering in Multiplex Networks<|sep|>In multiplex networks, cycles cannot be characterized only by their length, as edges may occur in different layers in different combinations. We define a classification of cycles by the number of edges in each layer and the number of switches between layers. We calculate the expected number of cycles of each type in the configuration model of a large sparse multiplex network. Our method accounts for the full degree distribution including correlations between degrees in different layers. In particular, we obtain the numbers of cycles of length 3 of all possible types. Using these, we give a complete set of clustering coefficients and their expected values. We show that correlations between the degrees of a vertex in different layers strongly affect the number of cycles of a given type, and the number of switches between layers. Both increase with assortative correlations and are strongly decreased by disassortative correlations. The effect of correlations on clustering coefficients is equally pronounced.
On the dependence of galaxy morphologies on galaxy mergers<|sep|>The distribution of galaxy morphological types is a key test for models of galaxy formation and evolution, providing strong constraints on the relative contribution of different physical processes responsible for the growth of the spheroidal components. In this paper, we make use of a suite of semi-analytic models to study the efficiency of galaxy mergers in disrupting galaxy discs and building galaxy bulges. In particular, we compare standard prescriptions usually adopted in semi-analytic models, with new prescriptions proposed by Kannan et al., based on results from high-resolution hydrodynamical simulations, and we show that these new implementations reduce the efficiency of bulge formation through mergers. In addition, we compare our model results with a variety of observational measurements of the fraction of spheroid-dominated galaxies as a function of stellar and halo mass, showing that the present uncertainties in the data represent an important limitation to our understanding of spheroid formation. Our results indicate that the main tension between theoretical models and observations does not stem from the survival of purely disc structures (i.e. bulgeless galaxies), rather from the distribution of galaxies of different morphological types, as a function of their stellar mass.
Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model<|sep|>In this work, we quantize a trained Transformer machine language translation model leveraging INT8/VNNI instructions in the latest Intel$^\circledR$ Xeon$^\circledR$ Cascade Lake processors to improve inference performance while maintaining less than 0.5$\%$ drop in accuracy. To the best of our knowledge, this is the first attempt in the industry to quantize the Transformer model. This has high impact as it clearly demonstrates the various complexities of quantizing the language translation model. We present novel quantization techniques directly in TensorFlow to opportunistically replace 32-bit floating point (FP32) computations with 8-bit integers (INT8) and transform the FP32 computational graph. We also present a bin-packing parallel batching technique to maximize CPU utilization. Overall, our optimizations with INT8/VNNI deliver 1.5X improvement over the best FP32 performance. Furthermore, it reveals the opportunities and challenges to boost performance of quantized deep learning inference and establishes best practices to run inference with high efficiency on Intel CPUs.
Social Welfare Maximization and Conformism via Information Design in Linear-Quadratic-Gaussian Games<|sep|>We consider linear-quadratic Gaussian (LQG) games in which players have quadratic payoffs that depend on the players' actions and an unknown payoff-relevant state, and signals on the state that follow a Gaussian distribution conditional on the state realization. An information designer decides the fidelity of information revealed to the players in order to maximize the social welfare of the players or reduce the disagreement among players' actions. Leveraging the semi-definiteness of the information design problem, we derive analytical solutions for these objectives under specific LQG games. We show that full information disclosure maximizes social welfare when there is a common payoff-relevant state, when there is strategic substitutability in the actions of players, or when the signals are public. Numerical results show that as strategic substitution increases, the value of the information disclosure increases. When the objective is to induce conformity among players' actions, hiding information is optimal. Lastly, we consider the information design objective that is a weighted combination of social welfare and cohesiveness of players' actions. We obtain an interval for the weights where full information disclosure is optimal under public signals for games with strategic substitutability. Numerical solutions show that the actual interval where full information disclosure is optimal gets close to the analytical interval obtained as substitution increases.
Probing for Understanding of English Verb Classes and Alternations in Large Pre-trained Language Models<|sep|>We investigate the extent to which verb alternation classes, as described by Levin (1993), are encoded in the embeddings of Large Pre-trained Language Models (PLMs) such as BERT, RoBERTa, ELECTRA, and DeBERTa using selectively constructed diagnostic classifiers for word and sentence-level prediction tasks. We follow and expand upon the experiments of Kann et al. (2019), which aim to probe whether static embeddings encode frame-selectional properties of verbs. At both the word and sentence level, we find that contextual embeddings from PLMs not only outperform non-contextual embeddings, but achieve astonishingly high accuracies on tasks across most alternation classes. Additionally, we find evidence that the middle-to-upper layers of PLMs achieve better performance on average than the lower layers across all probing tasks.
Nematic quantum phase transition of composite Fermi liquids in half-filled Landau levels and their geometric response<|sep|>We present a theory of the isotropic-nematic quantum phase transition in the composite Fermi liquid arising in half-filled Landau levels. We show that the quantum phase transition between the isotropic and the nematic phase is triggered by an attractive quadrupolar interaction between electrons, as in the case of conventional Fermi liquids. We derive the theory of the nematic state and of the phase transition. This theory is based on the flux attachment procedure which maps an electron liquid in half-filled Landau levels into the composite Fermi liquid close to a nematic transition. We show that the local fluctuations of the nematic order parameters act as an effective dynamical metric interplaying with the underlying Chern-Simons gauge fields associated with the flux attachment. Both the fluctuations of the Chern-Simons gauge field and the nematic order parameter can destroy the composite fermion quasiparticles and drive the system into a non-Fermi liquid state. The effective field theory for the isotropic-nematic phase transition has $z = 3$ dynamical exponent due to Landau damping effects. We show that there is a Berry phase type term which governs the effective dynamics of the nematic order parameter fluctuations, which can be interpreted as a non-universal "Hall viscosity" of the dynamical metric. We show that the effective field theory has a Wen-Zee-type term. Both terms originate from the time-reversal breaking fluctuation of the Chern-Simons gauge fields. We present a perturbative computation of the Hall viscosity and also show that this term is also obtained by a Ward identity. We show that the disclination of the nematic fluid, carries an electric charge. We show that a resonance observed in radio-frequency conductivity experiments can be interpreted as a Goldstone nematic mode gapped by lattice effects.
FrameProv: Towards End-To-End Video Provenance<|sep|>Video feeds are often deliberately used as evidence, as in the case of CCTV footage; but more often than not, the existence of footage of a supposed event is perceived as proof of fact in the eyes of the public at large. This reliance represents a societal vulnerability given the existence of easy-to-use editing tools and means to fabricate entire video feeds using machine learning. And, as the recent barrage of fake news and fake porn videos have shown, this isn't merely an academic concern, it is actively been exploited. I posit that this exploitation is only going to get more insidious. In this position paper, I introduce a long term project that aims to mitigate some of the most egregious forms of manipulation by embedding trustworthy components in the video transmission chain. Unlike earlier works, I am not aiming to do tamper detection or other forms of forensics -- approaches I think are bound to fail in the face of the reality of necessary editing and compression -- instead, the aim here is to provide a way for the video publisher to prove the integrity of the video feed as well as make explicit any edits they may have performed. To do this, I present a novel data structure, a video-edit specification language and supporting infrastructure that provides end-to-end video provenance, from the camera sensor to the viewer. I have implemented a prototype of this system and am in talks with journalists and video editors to discuss the best ways forward with introducing this idea to the mainstream.
Security against false data injection attack in cyber-physical systems<|sep|>In this paper, secure, remote estimation of a linear Gaussian process via observations at multiple sensors is considered. Such a framework is relevant to many cyber-physical systems and internet-of-things applications. Sensors make sequential measurements that are shared with a fusion center; the fusion center applies a certain filtering algorithm to make its estimates. The challenge is the presence of a few unknown malicious sensors which can inject anomalous observations to skew the estimates at the fusion center. The set of malicious sensors may be time-varying. The problems of malicious sensor detection and secure estimation are considered. First, an algorithm for secure estimation is proposed. The proposed estimation scheme uses a novel filtering and learning algorithm, where an optimal filter is learnt over time by using the sensor observations in order to filter out malicious sensor observations while retaining other sensor measurements. Next, a novel detector to detect injection attacks on an unknown sensor subset is developed. Numerical results demonstrate up to 3 dB gain in the mean squared error and up to 75% higher attack detection probability under a small false alarm rate constraint, against a competing algorithm that requires additional side information.
Assimilating X- and S-band Radar Data for a Heavy Precipitation Event in Italy<|sep|>During the night between 9 and 10 September 2017, multiple flash floods associated to a heavy-precipitation event affected the town of Livorno, located in Tuscany, Italy. Accumulated precipitation exceeding 200 mm in two hours, associated with a return period higher than 200 years, caused all the largest streams of the Livorno municipality to flood several areas of the town. We used the limited-area Weather Research and Forecasting (WRF) model, in a convection-permitting setup, to reconstruct the extreme event leading to the flash floods. We evaluated possible forecasting improvements emerging from the assimilation of local ground stations and X- and S-band radar data into the WRF, using the configuration operational at the meteorological center of Tuscany region (LaMMA) at the time of the event. Simulations were verified against weather station observations, through an innovative method aimed at disentangling the positioning and intensity errors of precipitation forecasts. By providing more accurate descriptions of the low-level flow and a better assessment of the atmospheric water vapour, the results demonstrate that assimilating radar data improved the quantitative precipitation forecasts.
Automatic Testing and Validation of Level of Detail Reductions Through Supervised Learning<|sep|>Modern video games are rapidly growing in size and scale, and to create rich and interesting environments, a large amount of content is needed. As a consequence, often several thousands of detailed 3D assets are used to create a single scene. As each asset's polygon mesh can contain millions of polygons, the number of polygons that need to be drawn every frame may exceed several billions. Therefore, the computational resources often limit how many detailed objects that can be displayed in a scene. To push this limit and to optimize performance one can reduce the polygon count of the assets when possible. Basically, the idea is that an object at farther distance from the capturing camera, consequently with relatively smaller screen size, its polygon count may be reduced without affecting the perceived quality. Level of Detail (LOD) refers to the complexity level of a 3D model representation. The process of removing complexity is often called LOD reduction and can be done automatically with an algorithm or by hand by artists. However, this process may lead to deterioration of the visual quality if the different LODs differ significantly, or if LOD reduction transition is not seamless. Today the validation of these results is mainly done manually requiring an expert to visually inspect the results. However, this process is slow, mundane, and therefore prone to error. Herein we propose a method to automate this process based on the use of deep convolutional networks. We report promising results and envision that this method can be used to automate the process of LOD reduction testing and validation.
A Case For Noisy Shallow Gate-Based Circuits In Quantum Machine Learning<|sep|>There is increasing interest in the development of gate-based quantum circuits for the training of machine learning models. Yet, little is understood concerning the parameters of circuit design, and the effects of noise and other measurement errors on the performance of quantum machine learning models. In this paper, we explore the practical implications of key circuit design parameters (number of qubits, depth etc.) using several standard machine learning datasets and IBM's Qiskit simulator. In total we evaluate over 6500 unique circuits with $n \approx 120700$ individual runs. We find that in general shallow (low depth) wide (more qubits) circuit topologies tend to outperform deeper ones in settings without noise. We also explore the implications and effects of different notions of noise and discuss circuit topologies that are more / less robust to noise for classification machine learning tasks. Based on the findings we define guidelines for circuit topologies that show near-term promise for the realisation of quantum machine learning algorithms using gate-based NISQ quantum computer.
A full-fledged micromagnetic code in less than 70 lines of NumPy<|sep|>We present a complete micromagnetic finite-difference code in less than 70 lines of Python. The code makes largely use of the NumPy library and computes the exchange field by finite differences and the demagnetization field with a fast convolution algorithm. Since the magnetization in finite-difference micromagnetics is represented by a multi-dimensional array and the NumPy library features a rich interface for this data structure, the presented code is a good starting point for the development of novel algorithms.
Knowledge Management Concepts For Training By Project An observation of the case of project management education<|sep|>Project management education programmes are often proposed in higher education to give students competences in project planning (Gantt's chart), project organizing, human and technical resource management, quality control and also social competences (collaboration, communication), emotional ones (empathy, consideration of the other, humour, ethics), and organizational ones (leadership, political vision, and so on). This training is often given according a training-by-project type of learning with case studies. This article presents one course characterized by a pedagogical organization based upon Knowledge Management (KM) concepts: knowledge transfer and construction throughout a learning circle and social interactions. The course is supported by a rich and complex tutor organization. We have observed this course by using another KM method inspired from KADS with various return of experience formalized into cards and charts. Our intention is, according to the model of Argyris and Sch\"on (Smith, 2001), to gain feedback information about local and global processes and about actors' experience in order to improve the course. This paper describes precisely the course (pedagogical method and tutor activity) and the KM observation method permitting to identify problem to solve. In our case, we observe problem of pedacogical coordination and skills acquisition. We propose to design a metacognitive tool for tutors and students, usable for improving knowledge construction and learning process organisation
Non-linear corrections to inflationary power spectrum<|sep|>We study non-linear contributions to the power spectrum of the curvature perturbation on super-horizon scales, produced during slow-roll inflation driven by a canonical single scalar field. We find that on large scales the linear power spectrum completely dominates and leading non-linear corrections remain totally negligible, indicating that we can safely rely on linear perturbation theory to study inflationary power spectrum. We also briefly comment on the infrared and ultraviolet behaviour of the non-linear corrections.
Comparing theories: the dynamics of changing vocabulary. A case-study in relativity theory<|sep|>There are several first-order logic (FOL) axiomatizations of special relativity theory in the literature, all looking essentially different but claiming to axiomatize the same physical theory. In this paper, we elaborate a comparison, in the framework of mathematical logic, between these FOL theories for special relativity. For this comparison, we use a version of mathematical definability theory in which new entities can also be defined besides new relations over already available entities. In particular, we build an interpretation of the reference-frame oriented theory SpecRel into the observationally oriented Signalling theory of James Ax. This interpretation provides SpecRel with an operational/experimental semantics. Then we make precise, "quantitative" comparisons between these two theories via using the notion of definitional equivalence. This is an application of logic to the philosophy of science and physics in the spirit of Johan van Benthem's work.
Construction and enumeration of circuits capable of guiding a miniature vehicle<|sep|>In contrast to traditional toy tracks, a patented system allows the creation of a large number of tracks with a minimal number of pieces, and whose loops always close properly. These circuits strongly resemble traditional self-avoiding polygons (whose explicit enumeration has not yet been resolved for an arbitrary number of squares) yet there are numerous differences, notably the fact that the geometric constraints are different than those of self-avoiding polygons. We present the methodology allowing the construction and enumeration of all of the possible tracks containing a given number of pieces. For small numbers of pieces, several variants are proposed which allow the consideration or not of the fact that the obtained circuits are identical up to a given isometry. For greater numbers of pieces, only an estimation will be offered. In the latter case, a randomly construction of circuits is also given. We will give some routes for generalizations for similar problems.
Why many theories of shock waves are necessary. Convergence error in formally path-consistent schemes<|sep|>We are interested in nonlinear hyperbolic systems in nonconservative form arising in fluid dynamics, and, for solutions containing shock waves, we investigate the convergence of finite difference schemes applied to such systems. According to Dal Maso, LeFloch, and Murat's theory, a shock wave theory for a given nonconservative system requires prescribing a priori a family of paths in the phase space. In the present paper, we consider schemes that are formally consistent with a given family of paths, and we investigate their limiting behavior as the mesh is refined. We generalize to systems a property established earlier by Hou and LeFloch for scalar conservation laws, and we prove that nonconservative schemes generate, at the level of the limiting hyperbolic system, a "convergence error" source-term which, provided the total variation of the approximations remains uniformly bounded, is a locally bounded measure. We discuss the role of the equivalent equation associated with a difference scheme; here, the distinction between scalar equations and systems appears most clearly since, for systems, the equivalent equation of a scheme that is formally path-consistent depends upon the prescribed family of paths. The core of this paper is devoted to investigate numerically the approximation of several models arising in fluid dynamics. For systems having nonconservative products associated with linearly degenerate characteristic fields, the convergence error vanishes. For some other models, this measure is evaluated very accurately, especially by plotting the shock curves associated with each scheme under consideration.
Contact-less Material Probing with Distributed Sensors: Joint Sensing and Communication Optimization<|sep|>The utilization of RF signals to probe material properties of objects is of huge interest both in academia as well as industry. To this end, a setup is investigated, in which a transmitter equipped with a two-dimensional multi-antenna array dispatches a signal, which hits objects in the environment and the reflections from the objects are captured by distributed sensors. The received signal at those sensors are then amplified and forwarded to a multiple antenna fusion center, which performs space-time post-processing in order to optimize the information extraction. In this process, optimal design of power allocation per object alongside sensors amplifications is of crucial importance. Here, the power allocation and sensors amplifications is jointly optimized, given maximum-ratio combining (MRC) at the fusion center. We formulate this challenge as a sum-power minimization under per-object SINR constraints, a sum-power constraint at the transmitter and individual power constraints at the sensors. Moreover, the advantage of deploying zero-forcing (ZF) and minimum mean-squared error (MMSE) at the fusion center is discussed. Asymptotic analysis is also provided for the case that large number of sensors are deployed in the sensing environment.
Radio Synchrotron Emission from Secondary Leptons in the Vicinity of Sgr A*<|sep|>A point-like source of ~TeV gamma-rays has recently been seen towards the Galactic center by HESS and other air Cerenkov telescopes. In recent work (Ballantyne et al. 2007), we demonstrated that these gamma-rays can be attributed to high-energy protons that (i) are accelerated close to the event horizon of the central black hole, Sgr A*, (ii) diffuse out to ~pc scales, and (iii) finally interact to produce gamma-rays. The same hadronic collision processes will necessarily lead to the creation of electrons and positrons. Here we calculate the synchrotron emissivity of these secondary leptons in the same magnetic field configuration through which the initiating protons have been propagated in our model. We compare this emission with the observed ~GHz radio spectrum of the inner few pc region which we have assembled from archival data and new measurements we have made with the Australia Telescope Compact Array. We find that our model predicts secondary synchrotron emission with a steep slope consistent with the observations but with an overall normalization that is too large by a factor of ~ 2. If we further constrain our theoretical gamma-ray curve to obey the implicit EGRET upper limit on emission from this region we predict radio emission that is consistent with observations, i.e., the hadronic model of gamma ray emission can, simultaneously and without fine-tuning, also explain essentially all the diffuse radio emission detected from the inner few pc of the Galaxy.
Optical cavity resonator in an expanding universe<|sep|>We study evolution of frequency of a standing electromagnetic (EM) wave in a resonant optical cavity placed to the expanding manifold described by the Robertson-Walker metric. One builds a local coordinate system in which spacetime is locally Minkowskian. However, due to the conformal nature of the Robertson-Walker metric the conventional transformation to the local inertial coordinates introduces ambiguity in the physical interpretation of the local time coordinate. Therefore, contrary to a common-sense expectation, a straightforward implementation of EEP alone does not allow us to decide whether atomic clocks ticks at the same rate as the clocks based on EM modes of a cavity. To resolve the ambiguity we analyzed the cavity rigidity and the oscillation of its EM modes in an expanding universe by employing the Maxwell equations. We found out that both the size of the cavity and the EM frequency experience an adiabatic drift in conformal coordinates as the universe expands. We set up the oscillation equation for the EM modes, solve it by the WKB approximation, and reduce the coordinate-dependent quantities to their counterparts measured by a local observer who counts time with atomic clock. The solution shows that there is a perfect cancellation of the adiabatic drift of cavity's frequency by the transformation to local coordinates, and the time counted by the clocks based on EM modes of cavity has the same rate as that of atomic clocks. We conclude that there should be no cosmological drift of frequency of a standing EM wave oscillating in the cavity resonator as compared to the frequency of atomic clocks. Continuous comparison of the frequency of the optical cavity resonator against that of atomic clock yields a powerful null test of the local isotropy of the Hubble expansion and the Einstein equivalence principle in cosmology.
Domain-Wall Standard Model in non-compact 5D and LHC phenomenology<|sep|>We propose a framework to construct "Domain-Wall Standard Model" in a non compact 5-dimensional space-time, where all the Standard Model (SM) fields are localized in certain domains of the 5th dimension and the SM is realized as a 4-dimensional effective theory without any compactification for the 5th dimension. In this context, we investigate the collider phenomenology of the Kaluza-Klein (KK) modes of the SM gauge bosons and the current constraints from the search for a new gauge boson resonance at the LHC Run-2. The couplings of the SM fermions with the KK-mode gauge bosons depend on the configuration of the SM fermions in the 5-dimensional bulk. This "geometry" of the model can be tested at the future Large Hadron Collider experiment, once a KK-mode of the SM gauge boson is discovered.
Quantum Gravity Corrections to the Mean Field Theory of Nucleons<|sep|>In this paper, we analyze the correction to the mean field theory potential for a system of nucleons. It will be argued that these corrections can be obtained by deforming the Schr\"{o}dinger's equation describing a system of nucleons by a minimal length in the background geometry of space-time. This is because such a minimal length occurs due to quantum gravitational effects, and modifies the low energy quantum mechanical systems. In fact, as the mean field potential for the nucleons is represented by the Woods-Saxon potential, we will explicitly analyze such corrections to this potential. We will obtain the corrections to the energy eigenvalues of the deformed Schr\"{o}dinger's equation for the Woods-Saxon potential. We will also construct the wave function for the deformed Schr\"{o}dinger's equation.
Realization of associative products in terms of Moyal and tomographic symbols<|sep|>The quantizer-dequantizer method allows to construct associative products on any measure space. Here we consider an inverse problem: given an associative product is it possible to realize it within the quantizer-dequantizer framework? The answer is positive in finite dimensions and we give a few examples in infinite dimensions.
DropSample: A New Training Method to Enhance Deep Convolutional Neural Networks for Large-Scale Unconstrained Handwritten Chinese Character Recognition<|sep|>Inspired by the theory of Leitners learning box from the field of psychology, we propose DropSample, a new method for training deep convolutional neural networks (DCNNs), and apply it to large-scale online handwritten Chinese character recognition (HCCR). According to the principle of DropSample, each training sample is associated with a quota function that is dynamically adjusted on the basis of the classification confidence given by the DCNN softmax output. After a learning iteration, samples with low confidence will have a higher probability of being selected as training data in the next iteration; in contrast, well-trained and well-recognized samples with very high confidence will have a lower probability of being involved in the next training iteration and can be gradually eliminated. As a result, the learning process becomes more efficient as it progresses. Furthermore, we investigate the use of domain-specific knowledge to enhance the performance of DCNN by adding a domain knowledge layer before the traditional CNN. By adopting DropSample together with different types of domain-specific knowledge, the accuracy of HCCR can be improved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1, and ICDAR 2013 online HCCR competition datasets yield outstanding recognition rates of 97.33%, 97.06%, and 97.51% respectively, all of which are significantly better than the previous best results reported in the literature.
Development of Wearable Systems for Ubiquitous Healthcare Service Provisioning<|sep|>This paper reports on the development of a wearable system using wireless biomedical sensors for ubiquitous healthcare service provisioning. The prototype system is developed to address current healthcare challenges such as increasing cost of services, inability to access diverse services, low quality services and increasing population of elderly as experienced globally. The biomedical sensors proactively collect physiological data of remote patients to recommend diagnostic services. The prototype system is designed to monitor oxygen saturation level (SpO2), Heart Rate (HR), activity and location of the elderly. Physiological data collected are uploaded to a Health Server (HS) via GPRS/Internet for analysis.
Multi-critical Behavior in Topological Phase Transitions<|sep|>Topological phase transitions can be described by the theory of critical phenomena and identified by critical exponents that define their universality classes. This is a consequence of the existence of a diverging length at the transition that has been identified as the penetration depth of the surface modes in the non-trivial topological phase. In this paper, we characterize different universality classes of topological transitions by determining their correlation length exponents directly from numerical calculations of the penetration length of the edge modes as a function of the distance to the topological transition. We consider generalizations of the topological non-trivial Su-Schrieefer-Heeger (SSH) model, for the case of next nearest neighbors hopping and in the presence of a synthetic potential. The latter allows the system to transit between two universality classes with different correlation length and dynamic critical exponents. It presents a multi-critical point in its phase diagram since the behavior of the Berry connection depends on the path it is approached. We compare our results with those obtained from a scaling approach to the Berry connection.
Simultaneously constraining the astrophysics of reionisation and the epoch of heating with 21CMMC<|sep|>The cosmic 21 cm signal is set to revolutionise our understanding of the early Universe, allowing us to probe the 3D temperature and ionisation structure of the intergalactic medium (IGM). It will open a window onto the unseen first galaxies, showing us how their UV and X-ray photons drove the cosmic milestones of the epoch of reionisation (EoR) and epoch of heating (EoH). To facilitate parameter inference from the 21 cm signal, we previously developed 21CMMC: a Monte Carlo Markov Chain sampler of 3D EoR simulations. Here we extend 21CMMC to include simultaneous modelling of the EoH, resulting in a complete Bayesian inference framework for the astrophysics dominating the observable epochs of the cosmic 21 cm signal. We demonstrate that second generation interferometers, the Hydrogen Epoch of Reionisation Array (HERA) and Square Kilometre Array (SKA) will be able to constrain ionising and X-ray source properties of the first galaxies with a fractional precision of order $\sim1$-10 per cent (1$\sigma$). The ionisation history of the Universe can be constrained to within a few percent. Using our extended framework, we quantify the bias in EoR parameter recovery incurred by the common simplification of a saturated spin temperature in the IGM. Depending on the extent of overlap between the EoR and EoH, the recovered astrophysical parameters can be biased by $\sim3-10\sigma$.
Flavour constraints on beyond the Standard Model scenarios<|sep|>The interplay of flavour and collider physics is entering a new era with the start-up of the LHC. During the past few years rare B decays and in particular b -> s gamma transitions have been extensively used and provided exciting opportunities for mapping possible routes beyond the SM. Flavour constraints play in this manner a complementary role to the direct searches. Here we present an overview of the existing flavour constraints on various models and show examples of comparison with the LHC discovery potentials. The SuperIso program which is dedicated to flavour physics observable calculations is also described briefly.
Network Coding-Based Cooperative ARQ Scheme<|sep|>In this paper we introduce a novel Automatic Repeat reQuest (ARQ) scheme for cooperative wireless networks. Our scheme adopts network coding techniques in order to enhance the total bandwidth of the network by minimizing the total number of transmissions. The performance of the proposed approach is evaluated by means of computer simulations and compared to other cooperative schemes, while an analytical solution is provided to validate the results.
DNS of compressible multiphase flows through the Eulerian approach<|sep|>In this paper we present three multiphase flow models suitable for the study of the dynamics of compressible dispersed multiphase flows. We adopt the Eulerian approach because we focus our attention to dispersed (concentration smaller than 0.001) and small particles (the Stokes number has to be smaller than 0.2). We apply these models to the compressible ($\text{Ma} = 0.2,\,0.5$) homogeneous and isotropic decaying turbulence inside a periodic three-dimensional box ($256^3$ cells) using a numerical solver based on the OpenFOAM$^{R}$ C++ libraries. In order to validate our simulations in the single-phase case we compare the energy spectrum obtained with our code with the one computed by an eighth order scheme getting a very good result (the relative error is very small $4*10^{-4}$). Moving to the bi-phase case, initially we insert inside the box an homogeneous distribution of particles leaving unchanged the initial velocity field. Because of the centrifugal force, turbulence induce particle preferential concentration and we study the evolution of the solid-phase density. Moreover, we do an {\em a-priori} test on the new sub-grid term of the multiphase equations comparing them with the standard sub-grid scale term of the Navier-Stokes equations.
Effect of Pixelation on the Parameter Estimation of Single Molecule Trajectories<|sep|>The advent of single molecule microscopy has revolutionized biological investigations by providing a powerful tool for the study of intercellular and intracellular trafficking processes of protein molecules which was not available before through conventional microscopy. In practice, pixelated detectors are used to acquire the images of fluorescently labeled objects moving in cellular environments. Then, the acquired fluorescence microscopy images contain the numbers of the photons detected in each pixel, during an exposure time interval. Moreover, instead of having the exact locations of detection of the photons, we only know the pixel areas in which the photons impact the detector. These challenges make the analysis of single molecule trajectories, from pixelated images, a complex problem. Here, we investigate the effect of pixelation on the parameter estimation of single molecule trajectories. In particular, we develop a stochastic framework to calculate the maximum likelihood estimates of the parameters of a stochastic differential equation that describes the motion of the molecule in living cells. We also calculate the Fisher information matrix for this parameter estimation problem. The analytical results are complicated through the fact that the observation process in a microscope prohibits the use of standard Kalman filter type approaches. The analytical framework presented here is illustrated with examples of low photon count scenarios for which we rely on Monte Carlo methods to compute the associated probability distributions.
Combinatorial and Asymptotical Results on the Neighborhood Grid<|sep|>In 2009, Joselli et al introduced the Neighborhood Grid data structure for fast computation of neighborhood estimates in point clouds. Even though the data structure has been used in several applications and shown to be practically relevant, it is theoretically not yet well understood. The purpose of this paper is to present a polynomial-time algorithm to build the data structure. Furthermore, it is investigated whether the presented algorithm is optimal. This investigations leads to several combinatorial questions for which partial results are given. Finally, we present several limits and experiments regarding the quality of the obtained neighborhood relation.
Flows, scaling, and the control of moment hierarchies for stochastic chemical reaction networks<|sep|>Stochastic chemical reaction networks (CRNs) are complex systems which combine the features of concurrent transformation of multiple variables in each elementary reaction event, and nonlinear relations between states and their rates of change. Most general results concerning CRNs are limited to restricted cases where a topological characteristic known as deficiency takes value 0 or 1. Here we derive equations of motion for fluctuation moments at all orders for stochastic CRNs at general deficiency. We show, for the case of the mass-action rate law, that the generator of the stochastic process acts on the hierarchy of factorial moments with a finite representation. Whereas simulation of high-order moments for many-particle systems is costly, this representation reduces solution of moment hierarchies to a complexity comparable to solving a heat equation. At steady states, moment hierarchies for finite CRNs interpolate between low-order and high-order scaling regimes, which may be approximated separately by distributions similar to those for deficiency-0 networks, and connected through matched asymptotic expansions. In CRNs with multiple stable or metastable steady states, boundedness of high-order moments provides the starting condition for recursive solution downward to low-order moments, reversing the order usually used to solve moment hierarchies. A basis for a subset of network flows defined by having the same mean-regressing property as the flows in deficiency-0 networks gives the leading contribution to low-order moments in CRNs at general deficiency, in a $1/n$-expansion in large particle numbers. Our results give a physical picture of the different informational roles of mean-regressing and non-mean-regressing flows, and clarify the dynamical meaning of deficiency not only for first-moment conditions but for all orders in fluctuations.
F(R) supergravity<|sep|>We review the F(R) supergravity recently proposed in Phys. Lett. B674 (2009) 59 and Class. Quantum Grav. 26 (2009) 135006. Our construction supersymmetrizes popular f(R) theories of modified gravity in four spacetime dimensions. We use curved superspace of N=1 Poincar'e supergravity in its minimal (2nd order) formulation so that our F(R) supergravity action is manifestly invariant under local N=1 supersymmetry. We prove that the F(R) supergravity is classically equivalent to the standard N=1 Poincar'e supergravity (minimally) coupled to a dynamical chiral superfield, via a Legendre-Weyl transform in superspace. A K"ahler potential, a superpotential and a scalar potential of the chiral superfield are governed by a single holomorphic function. We find the conditions of vanishing cosmological constant without fine-tuning, which define a no-scale F(R) supergravity.
My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em Competition<|sep|>The first ever human vs. computer no-limit Texas hold 'em competition took place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this article I present my thoughts on the competition design, agent architecture, and lessons learned.
Procedural Crowd Generation for Semantically Augmented Virtual Cities<|sep|>Authoring realistic behaviors to populate a large virtual city can be a cumbersome, time-consuming and error-prone task. Believable crowds require the effort of storytellers and programming experts working together for long periods of time. In this work, we present a new framework to allow users to generate populated environments in an easier and faster way, by relying on the use of procedural techniques. Our framework consists of the procedural generation of semantically-augmented virtual cities to drive the procedural generation and simulation of crowds. The main novelty lies in the generation of agendas for each individual inhabitant (alone or as part of a family) by using a rule-based grammar that combines city semantics with the autonomous persons' characteristics. Real-world data can be used to accommodate the generation of a virtual population, thus enabling the recreation of more realistic scenarios. Users can author a new population or city by editing rule files with the flexibility of re-using, combining or extending the rules of previous populations. The results show how logical and consistent behaviors can be easily generated for a large crowd providing a good starting point to bring virtual cities to life.
BRVST: Efficient and Content-Expressive Information Matching Overlay in Wireless Networks<|sep|>Efficient and flexible information matching over wireless networks has become increasingly important and challenging with the popularity of smart devices and the growth of social-network-based applications. Some existing approaches designed for wired networks are not applicable to wireless networks, due to their overwhelming control overheads. In this paper, we propose a reliable and scalable binary range vector summary tree (BRVST) infrastructure for flexible information expression support, effective content matching and timely information dissemination over the dynamic wireless network. A novel attribute range vector structure has been introduced for efficient and accurate content representation and a summary tree structure to facilitate information aggregation. For robust and scalable operations over dynamic wireless network, the proposed overlay system exploits a virtual hierarchical geographic management framework. Extensive simulations demonstrate that BRVST has a significantly faster event matching speed, while incurs very low storage and traffic overhead, as compared with peer schemes tested.
NaturalProver: Grounded Mathematical Proof Generation with Language Models<|sep|>Theorem proving in natural mathematical language - the mixture of symbolic and natural language used by humans - plays a central role in mathematical advances and education, and tests aspects of reasoning that are core to intelligence. Yet it has remained underexplored with modern generative models. We study large-scale language models on two new generation tasks: suggesting the next step in a mathematical proof, and full proof generation. We develop NaturalProver, a language model that generates proofs by conditioning on background references (e.g. theorems and definitions that are either retrieved or human-provided), and optionally enforces their presence with constrained decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves the quality of next-step suggestions and generated proofs over fine-tuned GPT-3, according to human evaluations from university-level mathematics students. NaturalProver is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40% of the time, which is to our knowledge the first demonstration of these capabilities using neural language models.
A Combined View of Sterile-Neutrino Constraints from CMB and Neutrino Oscillation Measurements<|sep|>We perform a comparative analysis of constraints on sterile neutrinos from the Planck experiment and from current and future neutrino oscillation experiments (MINOS, IceCube, SBN). For the first time, we express the Planck constraints on $N_{\rm eff}$ and $m_{\rm eff}^{\rm sterile}$ from the Cosmic Microwave Background in the parameter space used by oscillation experiments using both mass-squared differences and mixing angles. In a model with a single sterile neutrino species and using standard assumptions, we find that the Planck data and the oscillation experiments measuring muon-neutrino disappearance have similar sensitivity.
Efficient and Robust Shape Correspondence via Sparsity-Enforced Quadratic Assignment<|sep|>In this work, we introduce a novel local pairwise descriptor and then develop a simple, effective iterative method to solve the resulting quadratic assignment through sparsity control for shape correspondence between two approximate isometric surfaces. Our pairwise descriptor is based on the stiffness and mass matrix of finite element approximation of the Laplace-Beltrami differential operator, which is local in space, sparse to represent, and extremely easy to compute while containing global information. It allows us to deal with open surfaces, partial matching, and topological perturbations robustly. To solve the resulting quadratic assignment problem efficiently, the two key ideas of our iterative algorithm are: 1) select pairs with good (approximate) correspondence as anchor points, 2) solve a regularized quadratic assignment problem only in the neighborhood of selected anchor points through sparsity control. These two ingredients can improve and increase the number of anchor points quickly while reducing the computation cost in each quadratic assignment iteration significantly. With enough high-quality anchor points, one may use various pointwise global features with reference to these anchor points to further improve the dense shape correspondence. We use various experiments to show the efficiency, quality, and versatility of our method on large data sets, patches, and point clouds (without global meshes).
Subexponential Parameterized Algorithm for Minimum Fill-in<|sep|>The Minimum Fill-in problem is to decide if a graph can be triangulated by adding at most k edges. Kaplan, Shamir, and Tarjan [FOCS 1994] have shown that the problem is solvable in time O(2^(O(k)) + k2 * nm) on graphs with n vertices and m edges and thus is fixed parameter tractable. Here, we give the first subexponential parameterized algorithm solving Minimum Fill-in in time O(2^(O(\sqrt{k} log k)) + k2 * nm). This substantially lower the complexity of the problem. Techniques developed for Minimum Fill-in can be used to obtain subexponential parameterized algorithms for several related problems including Minimum Chain Completion, Chordal Graph Sandwich, and Triangulating Colored Graph.
On the Consistency of Optimal Bayesian Feature Selection in the Presence of Correlations<|sep|>Optimal Bayesian feature selection (OBFS) is a multivariate supervised screening method designed from the ground up for biomarker discovery. In this work, we prove that Gaussian OBFS is strongly consistent under mild conditions, and provide rates of convergence for key posteriors in the framework. These results are of enormous importance, since they identify precisely what features are selected by OBFS asymptotically, characterize the relative rates of convergence for posteriors on different types of features, provide conditions that guarantee convergence, justify the use of OBFS when its internal assumptions are invalid, and set the stage for understanding the asymptotic behavior of other algorithms based on the OBFS framework.
On the Security of Key Extraction from Measuring Physical Quantities<|sep|>Key extraction via measuring a physical quantity is a class of information theoretic key exchange protocols that rely on the physical characteristics of the communication channel to enable the computation of a shared key by two (or more) parties that share no prior secret information. The key is supposed to be information theoretically hidden to an eavesdropper. Despite the recent surge of research activity in the area, concrete claims about the security of the protocols typically rely on channel abstractions that are not fully experimentally substantiated. In this work, we propose a novel methodology for the {\em experimental} security analysis of these protocols. The crux of our methodology is a falsifiable channel abstraction that is accompanied by an efficient experimental approximation algorithm of the {\em conditional min-entropy} available to the two parties given the view of the eavesdropper. We focus on the signal strength between two wirelessly communicating transceivers as the measured quantity and we use an experimental setup to compute the conditional min-entropy of the channel given the view of the attacker which we find to be linearly increasing. Armed with this understanding of the channel, we showcase the methodology by providing a general protocol for key extraction in this setting that is shown to be secure for a concrete parameter selection. In this way we provide a first comprehensively analyzed wireless key extraction protocol that is demonstrably secure against passive adversaries. Our methodology uses hidden Markov models as the channel model and a dynamic programming approach to approximate conditional min-entropy but other possible instantiations of the methodology can be motivated by our work.
Reinforced Imitation: Sample Efficient Deep Reinforcement Learning for Map-less Navigation by Leveraging Prior Demonstrations<|sep|>This work presents a case study of a learning-based approach for target driven map-less navigation. The underlying navigation model is an end-to-end neural network which is trained using a combination of expert demonstrations, imitation learning (IL) and reinforcement learning (RL). While RL and IL suffer from a large sample complexity and the distribution mismatch problem, respectively, we show that leveraging prior expert demonstrations for pre-training can reduce the training time to reach at least the same level of performance compared to plain RL by a factor of 5. We present a thorough evaluation of different combinations of expert demonstrations, different RL algorithms and reward functions, both in simulation and on a real robotic platform. Our results show that the final model outperforms both standalone approaches in the amount of successful navigation tasks. In addition, the RL reward function can be significantly simplified when using pre-training, e.g. by using a sparse reward only. The learned navigation policy is able to generalize to unseen and real-world environments.
Microscopic mechanism responsible for radiation-enhanced diffusion of impurity atoms<|sep|>Modeling of radiation-enhanced diffusion of boron and phosphorus atoms during irradiation of silicon substrates respectively with high- and low-energy protons was carried out. The results obtained confirm the previously arrived conclusion that impurity diffusion occurs by means of the "impurity atom - intrinsic point defect" pairs and that the condition of the local thermodynamic equilibrium between substitutional impurity atoms, nonequilibrium point defects created by irradiation, and the pairs is valid. It is shown that using radiation-enhanced diffusion, one can form a special impurity distribution in the semiconductor substrate including retrograde profiles with increasing impurity concentration into the bulk of a semiconductor. The calculations performed give clear evidence in favor of further investigation of various doping processes based on radiation-enhanced diffusion, especially the processes of plasma doping, to develop a cheap method for formation of specific impurity distributions in the near surface region.
Multiple scattering Sunyaev-Zeldovich signal II: relativistic effects<|sep|>We study the multiple scattering Sunyaev-Zeldovich (SZ) signature, extending our previous analysis to high-temperature clusters. We consistently treat the anisotropy of the ambient radiation field caused by the first scattering and also consider lowest order kinematic terms. We show that due to temperature corrections monopole through octupole anisotropy of the singly scattered SZ signal attain different spectra in the second scattering. The difference becomes more pronounced at high temperature, and thus could be used to constrain individual line of sight moments of the electron density and temperature profiles. While very challenging from the observational point of view, this further extends the list of possible SZ observables that will be important for 3D cluster-profile reconstruction, possibly helping to break geometric degeneracies caused by projection effects. We also briefly discuss the scattering of primordial CMB anisotropies by SZ clusters.
Finite temperature quantum statistics of H$_3^+$ molecular ion<|sep|>Full quantum statistical $NVT$ simulation of the five-particle system H$_3^+$ has been carried out using the path integral Monte Carlo method. Structure and energetics is evaluated as a function of temperature up to the thermal dissociation limit. The weakly density dependent dissociation temperature is found to be around $4000$ K. Contributions from the quantum dynamics and thermal motion are sorted out by comparing differences between simulations with quantum and classical nuclei. The essential role of the quantum description of the protons is established.
Studying the multi-wavelength signals from short GRBs<|sep|>Since the first host galaxies and afterglows of short GRBs were identified, they have remained very difficult to study: their multiwavelenth afterglows are notoriously faint and host galaxy identification often relies upon minimalising a chance alignment probability. Despite these observational challenges, there is now a sufficiently large sample to constrain the properties of the wider population and, in this review talk, I will summarise the current multi-wavelength observations of short GRBs. Additionally, I will describe how these observed data are able to both support and challenge the standard theoretical models of the progenitors and central engines. Looking towards the future, due to technological and theoretical advances, we are about to enter an exciting era for the study of short GRBs. We will be able to search for predicted counterparts in wide-field multi-wavelength transient searches and have the tantalising prospect of finding the very first ``smoking gun'' signal from the progenitor via the detection of gravitational waves.
Energy repartition for a harmonic chain with local reservoirs<|sep|>We exactly analyze the vibrational properties of a chain of harmonic oscillators in contact with local Langevin heat baths. Nonequilibrium steady-state fluctuations are found to be described by a set of mode-temperatures, independent of the strengths of both the harmonic interaction and the viscous damping. Energy is equally distributed between the conjugate variables of a given mode but differently among different modes, in a manner which depends exclusively on the bath temperatures and on the boundary conditions. We outline how bath-temperature profiles can be designed to enhance or reduce fluctuations at specific frequencies in the power spectrum of the chain length.
Multimodal deep learning approach for joint EEG-EMG data compression and classification<|sep|>In this paper, we present a joint compression and classification approach of EEG and EMG signals using a deep learning approach. Specifically, we build our system based on the deep autoencoder architecture which is designed not only to extract discriminant features in the multimodal data representation but also to reconstruct the data from the latent representation using encoder-decoder layers. Since autoencoder can be seen as a compression approach, we extend it to handle multimodal data at the encoder layer, reconstructed and retrieved at the decoder layer. We show through experimental results, that exploiting both multimodal data intercorellation and intracorellation 1) Significantly reduces signal distortion particularly for high compression levels 2) Achieves better accuracy in classifying EEG and EMG signals recorded and labeled according to the sentiments of the volunteer.
Total Domishold Graphs: a Generalization of Threshold Graphs, with Connections to Threshold Hypergraphs<|sep|>A total dominating set in a graph is a set of vertices such that every vertex of the graph has a neighbor in the set. We introduce and study graphs that admit non-negative real weights associated to their vertices such that a set of vertices is a total dominating set if and only if the sum of the corresponding weights exceeds a certain threshold. We show that these graphs, which we call total domishold graphs, form a non-hereditary class of graphs properly containing the classes of threshold graphs and the complements of domishold graphs, and are closely related to threshold Boolean functions and threshold hypergraphs. We present a polynomial time recognition algorithm of total domishold graphs, and characterize graphs in which the above property holds in a hereditary sense. Our characterization is obtained by studying a new family of hypergraphs, defined similarly as the Sperner hypergraphs, which may be of independent interest.
Thermodynamics and the Structure of Quantum Theory as a Generalized Probabilistic Theory<|sep|>This thesis investigates the connection between quantum theory, thermodynamics and information theory. Theories with structure similar to that of quantum theory are considered, mathematically described by the framework of "Generalized Probabilistic Theories". For these theories, a thought experiment by von Neumann is adapted to obtain a natural thermodynamic entropy definition, following a proposal by J. Barrett. Mathematical properties of this entropy are compared to physical consequences of the thought experiment. The validity of the second law of thermodynamics is investigated. In that context, observables and projective measurements are generalized to prove an entropy increase for projective measurements of ensembles. Information-theoretically motivated definitions of the entropy are compared to the entropy from the thermodynamic thought experiment. The conditions for the thermodynamic entropy to be well-defined are considered in greater detail. Several further properties of the theories under consideration (e.g. whether there is higher order interference, Pfister's state discrimination principle) and their relation to entropy are investigated.
Microcontroller Based Testing of Digital IP-Core<|sep|>Testing core based System on Chip is a challenge for the test engineers. To test the complete SOC at one time with maximum fault coverage, test engineers prefer to test each IP-core separately. At speed testing using external testers is more expensive because of gigahertz processor. The purpose of this paper is to develop cost efficient and flexible test methodology for testing digital IP-cores . The prominent feature of the approach is to use microcontroller to test IP-core. The novel feature is that there is no need of test pattern generator and output response analyzer as microcontroller performs the function of both. This approach has various advantages such as at speed testing, low cost, less area overhead and greater flexibility since most of the testing process is based on software.
Directional Clustering Tests Based on Nearest Neighbor Contingency Tables<|sep|>Spatial interaction between two or more classes or species has important implications in various fields and causes multivariate patterns such as segregation or association. Segregation occurs when members of a class or species are more likely to be found near members of the same class or conspecifics; while association occurs when members of a class or species are more likely to be found near members of another class or species. The null patterns considered are random labeling (RL) and complete spatial randomness (CSR) of points from two or more classes, which is called \emph{CSR independence}, henceforth. The clustering tests based on nearest neighbor contingency tables (NNCTs) that are in use in literature are two-sided tests. In this article, we consider the directional (i.e., one-sided) versions of the cell-specific NNCT-tests and introduce new directional NNCT-tests for the two-class case. We analyze the distributional properties; compare the empirical significant levels and empirical power estimates of the tests using extensive Monte Carlo simulations. We demonstrate that the new directional tests have comparable performance with the currently available NNCT-tests in terms of empirical size and power. We use four example data sets for illustrative purposes and provide guidelines for using these NNCT-tests.
The Dual Frequency Anisotropic Magneto-Optical Trap<|sep|>The cloud of cold atoms produced by a Magneto-Optical Trap is known to exhibit instabilities. We examine in this paper in which limits it could be possible to realize an experimental trap similar to the configurations studied theoretically, i.e. mainly traps where one direction is privileged. We study the static behavior of an anisotropic trap, where anisotropy results essentially from the use of two different laser frequencies for the arms of the trap. Such a trap has very surprising behaviors, in particular the cloud disappears for some laser frequencies, while it exists for smaller and larger frequencies. A model is build to explain these behaviors. We show in particular that, to reproduce the experimental observations, the model has to take into account the cross saturation effects. Moreover, the couplings between the different directions cannot be neglected.
How fast do Jupiters grow? Signatures of the snowline and growth rate in the distribution of gas giant planets<|sep|>We present here observational evidence that the snowline plays a significant role in the formation and evolution of gas giant planets. When considering the population of observed exoplanets, we find a boundary in mass-semimajor axis space that suggests planets are preferentially found beyond the snowline prior to undergoing gap-opening inward migration and associated gas accretion. This is consistent with theoretical models suggesting that sudden changes in opacity -- as would occur at the snowline -- can influence core migration. Furthermore, population synthesis modelling suggests that this boundary implies that gas giant planets accrete ~ 70 % of the inward flowing gas, allowing ~ 30$ % through to the inner disc. This is qualitatively consistent with observations of transition discs suggesting the presence of inner holes, despite there being ongoing gas accretion.
Private Synthetic Data for Multitask Learning and Marginal Queries<|sep|>We provide a differentially private algorithm for producing synthetic data simultaneously useful for multiple tasks: marginal queries and multitask machine learning (ML). A key innovation in our algorithm is the ability to directly handle numerical features, in contrast to a number of related prior approaches which require numerical features to be first converted into {high cardinality} categorical features via {a binning strategy}. Higher binning granularity is required for better accuracy, but this negatively impacts scalability. Eliminating the need for binning allows us to produce synthetic data preserving large numbers of statistical queries such as marginals on numerical features, and class conditional linear threshold queries. Preserving the latter means that the fraction of points of each class label above a particular half-space is roughly the same in both the real and synthetic data. This is the property that is needed to train a linear classifier in a multitask setting. Our algorithm also allows us to produce high quality synthetic data for mixed marginal queries, that combine both categorical and numerical features. Our method consistently runs 2-5x faster than the best comparable techniques, and provides significant accuracy improvements in both marginal queries and linear prediction tasks for mixed-type datasets.
Control of nonlinear switched systems based on validated simulation<|sep|>We present an algorithm of control synthesis for nonlinear switched systems, based on an existing procedure of state-space bisection and made available for nonlinear systems with the help of validated simulation. The use of validated simulation also permits to take bounded perturbations and varying parameters into account. It is particularly interesting for safety critical applications, such as in aeronautical, military or medical fields. The whole approach is entirely guaranteed and the induced controllers are correct-by-design.
A Generalized Framework for Analytic Regularization of Uniform Cubic B-spline Displacement Fields<|sep|>Image registration is an inherently ill-posed problem that lacks the constraints needed for a unique mapping between voxels of the two images being registered. As such, one must regularize the registration to achieve physically meaningful transforms. The regularization penalty is usually a function of derivatives of the displacement-vector field, and can be calculated either analytically or numerically. The numerical approach, however, is computationally expensive depending on the image size, and therefore a computationally efficient analytical framework has been developed. Using cubic B-splines as the registration transform, we develop a generalized mathematical framework that supports five distinct regularizers: diffusion, curvature, linear elastic, third-order, and total displacement. We validate our approach by comparing each with its numerical counterpart in terms of accuracy. We also provide benchmarking results showing that the analytic solutions run significantly faster -- up to two orders of magnitude -- than finite differencing based numerical implementations.
Optimal Decomposition of Belief Networks<|sep|>In this paper, optimum decomposition of belief networks is discussed. Some methods of decomposition are examined and a new method - the method of Minimum Total Number of States (MTNS) - is proposed. The problem of optimum belief network decomposition under our framework, as under all the other frameworks, is shown to be NP-hard. According to the computational complexity analysis, an algorithm of belief network decomposition is proposed in (Wee, 1990a) based on simulated annealing.
Topological BF description of 2D accelerated chiral edge modes<|sep|>We consider the topological abelian BF theory with radial boundary on a generic 3D manifold. Our aim is to study if, where and how the boundary keeps memory of the details of the background metric. We find that some features are topologically protected and do not depend on the bulk metric. The 2D action holographically induced on the boundary depends on two scalar fields, and can be decoupled in two Luttinger actions describing two chiral bosons moving on the edge of the 3D bulk. The outcome is that these edge excitations are accelerated, as a direct consequence of the non-flat nature of the bulk spacetime. The chiral velocities of the edge modes, indeed, acquire a local dependence through the determinant of the induced metric on the boundary. We find three possibilities for the motion of the edge quasiparticles: same directions, opposite directions and a single-moving mode. But, requiring that the Hamiltonian of the 2D theory is bounded by below, the case of edge modes moving in the same direction is ruled out: systems involving parallel Hall currents (for instance Fractional Quantum Hall Effect with $\nu=2/5$) cannot be described by a BF theory with boundary, independently from the geometry of the bulk spacetime, because of positive energy considerations. We are therefore left with physical situations characterized by edge excitations moving with opposite velocities (examples are FQHE with $\nu=1-1/n$, with $n$ positive integer, and Helical Luttinger Liquids phenomena) or a single-moving mode (Quantum Anomalous Hall). A strong restriction is obtained by requiring Time Reversal symmetry, which uniquely identifies modes with equal and opposite velocities, and we know that this is the case of Topological Insulators. The novelty, with respect to the flat bulk background, is that the modes have local velocities, which corresponds to Topological Insulators with accelerated edge modes.
Spatially Resolved Dark Count Rate of SiPMs<|sep|>The Silicon Photomultiplier (SiPM) is a promising photo-detector for a variety of applications. However, the high dark count rate (DCR) of the SiPM is still a contemporary problem. Decreasing the DCR would significantly broaden the range of possible applications. In this work we present a novel method for the spatially resolved characterization of crystal defects in SiPMs. The contribution of crystal defects to the DCR is evaluated by exploiting the effect of "hot carrier luminescence" (HCL), which is light that is emitted during the Geiger mode operation of avalanche photodiodes (SiPM micro-cells). Spatially confined regions with an enhanced light emission intensity (hotspots) are identified within the active areas of SiPM micro-cells. By correlating the detected light intensity and the DCR, a significant contribution of up to 56 % of the DCR can be attributed to less than 5 % of the micro-cells. The analysis of the temperature dependence of the emitted light identifies the Shockley-Read-Hall-Generation to be the dominant mechanism responsible for the occurrence of hotspots. The motivation of this work is to generate a deeper understanding of the origin of hotspots in order to suppress their contribution to the DCR of SiPMs.
Ising-like models on arbitrary graphs : The Hadamard way<|sep|>We propose a generic framework to describe classical Ising-like models defined on arbitrary graphs. The energy spectrum is shown to be the Hadamard transform of a suitably defined sparse "coding" vector associated with the graph. We expect that the existence of a fast Hadamard transform algorithm (used for instance in image ccompression processes), together with the sparseness of the coding vector, may provide ways to fasten the spectrum computation.. Applying this formalism to regular graphs, such as hypercubic graphs, we obtain a simple recurrence relation for the spectrum, which significantly speeds up its determination. First attempts to analyse partition functions and transfer matrices are also presented.
Instability of Non-uniform Toroidal Magnetic Fields in Accretion Disks<|sep|>A new type of instability that is expected to drive magnetohydrodynamic (MHD) turbulence from a purely toroidal magnetic field in an accretion disk is presented. It is already known that in a differentially rotating system, the uniform toroidal magnetic field is unstable due to a magnetorotational instability (MRI) under a non-axisymmetric and vertical perturbation, while it is stable under a purely vertical perturbation. Contrary to the previous study, this paper proposes an unstable mode completely confined to the equatorial plane, driven by the expansive nature of the magnetic pressure gradient force under a non-uniform toroidal field. The basic nature of this growing eigenmode, to which we give a name "magneto-gradient driven instability", is studied using linear analysis, and the corresponding nonlinear evolution is then investigated using two-dimensional ideal MHD simulations. Although a single localized magnetic field channel alone cannot provide sufficient Maxwell stress to contribute significantly to the angular momentum transport, we find that the mode coupling between neighboring toroidal fields under multiple localized magnetic field channels drastically generates a highly turbulent state and leads to the enhanced transport of angular momentum, comparable to the efficiency seen in previous studies on MRIs. This horizontally confined mode may play an important role in the saturation of an MRI through complementray growth with the toroidal MHDs and coupling with magnetic reconnection.
Josephson Effect between Conventional and Rashba Superconductors<|sep|>We study the Josephson effect between a conventional s-wave superconductor and a non-centrosymmetric superconductor with Rashba spin-orbit coupling. Rashba spin-orbit coupling affects the Josephson pair tunneling in a characteristic way. The Josephson coupling can be decomposed into two parts, a `spin-singlet-like' and a `spin-triplet-like' component. The latter component can lead to shift of the Josephson phase by \pi relative to the former coupling. This has important implications on interference effects and may explain some recent experimental results for the Al/CePt3Si junction.
Observational Testability of Kerr bound in X-ray Spectrum of Black-Hole Candidates<|sep|>The specific angular momentum of a Kerr black hole must not be larger than its mass. The observational confirmation of this bound which we call a Kerr bound directly suggests the existence of a black hole. In order to investigate observational testability of this bound by using the X-ray energy spectrum of black hole candidates, we calculate energy spectra for a super-spinning object (or a naked singularity) which is described by a Kerr metric but whose specific angular momentum is larger than its mass, and then compare the spectra of this object with those of a black hole. We assume an optically thick and geometrically thin disc around the super-spinning object and calculate its thermal energy spectrum seen by a distant observer by solving general relativistic radiative transfer equations including usual special and general relativistic effects such as Doppler boosting, gravitational redshift, light bending and frame-dragging. Surprisingly, for a given black hole, we can always find its super-spinning counterpart with its spin $a_*$ in the range $5/3<a_*<8\sqrt{6}/3$ whose observed spectrum is very similar to and practically indistinguishable from that of the black hole. As a result, we conclude that to confirm the Kerr bound we need more than the X-ray thermal spectrum of the black hole candidates.
Faster RER-CNN: application to the detection of vehicles in aerial images<|sep|>Detecting small vehicles in aerial images is a difficult job that can be challenging even for humans. Rotating objects, low resolution, small inter-class variability and very large images comprising complicated backgrounds render the work of photo-interpreters tedious and wearisome. Unfortunately even the best classical detection pipelines like Faster R-CNN cannot be used off-the-shelf with good results because they were built to process object centric images from day-to-day life with multi-scale vertical objects. In this work we build on the Faster R-CNN approach to turn it into a detection framework that deals appropriately with the rotation equivariance inherent to any aerial image task. This new pipeline (Faster Rotation Equivariant Regions CNN) gives, without any bells and whistles, state-of-the-art results on one of the most challenging aerial imagery datasets: VeDAI and give good results w.r.t. the baseline Faster R-CNN on two others: Munich and GoogleEarth .
Effects of co-ordination number on the nucleation behaviour in many-component self-assembly<|sep|>We report canonical and grand-canonical lattice Monte Carlo simulations of the self-assembly of addressable structures comprising hundreds of distinct component types. The nucleation behaviour, in the form of free-energy barriers to nucleation, changes significantly as the co-ordination number of the building blocks is changed from 4 to 8 to 12. Unlike tetrahedral structures - which roughly correspond to DNA bricks that have been studied in experiment - the shapes of the free-energy barriers of higher co-ordination structures depend strongly on the supersaturation, and such structures require a very significant driving force for structure growth before nucleation becomes thermally accessible. Although growth at high supersaturation results in more defects during self-assembly, we show that high co-ordination number structures can still be assembled successfully in computer simulations and that they exhibit self-assembly behaviour analogous to DNA bricks. In particular, the self-assembly remains modular, enabling in principle a wide variety of nanostructures to be assembled, with a greater spatial resolution than is possible in low co-ordination structures.
From liquid to solid bonding in cohesive granular media<|sep|>We study the transition of a granular packing from liquid to solid bonding in the course of drying. The particles are initially wetted by a liquid brine and the cohesion of the packing is ensured by capillary forces, but the crystallization of the solute transforms the liquid bonds into partially cemented bonds. This transition is evidenced experimentally by measuring the compressive strength of the samples at regular intervals of times. Our experimental data reveal three regimes: 1) Up to a critical degree of saturation, no solid bonds are formed and the cohesion remains practically constant; 2) The onset of cementation occurs at the surface and a front spreads towards the center of the sample with a nonlinear increase of the cohesion; 3) All bonds are partially cemented when the cementation front reaches the center of the sample, but the cohesion increases rapidly due to the consolidation of cemented bonds. We introduce a model based on a parametric cohesion law at the bonds and a bond crystallization parameter. This model predicts correctly the phase transition and the relation between microscopic and macroscopic cohesion.
Reasoning by Cases in Structured Argumentation<|sep|>We extend the $ASPIC^+$ framework for structured argumentation so as to allow applications of the reasoning by cases inference scheme for defeasible arguments. Given an argument with conclusion `$A$ or $B$', an argument based on $A$ with conclusion $C$, and an argument based on $B$ with conclusion $C$, we allow the construction of an argument with conclusion $C$. We show how our framework leads to different results than other approaches in non-monotonic logic for dealing with disjunctive information, such as disjunctive default theory or approaches based on the OR-rule (which allows to derive a defeasible rule `If ($A$ or $B$) then $C$', given two defeasible rules `If $A$ then $C$' and `If $B$ then $C$'). We raise new questions regarding the subtleties of reasoning defeasibly with disjunctive information, and show that its formalization is more intricate than one would presume.
CAWS - Security Algorithms for Wireless Sensor Networks: A Cellular Automata Based Approach<|sep|>Security in the Wireless Sensor Networks (WSN) is a very challenging task because of their dissimilarities with the conventional wireless networks. The related works so far have been done have tried to solve the problem keeping in the mind the constraints of WSNs. In this paper we have proposed a set of cellular automata based security algorithms (CAWS) which consists of CAKD, a Cellular Automata (CA) based key management algorithm and CASC, a CA based secure data communication algorithm, which require very small amount of memory as well as simple computation.
A sensitive 3 mm line survey of L483: a broad view of the chemical composition of a core around a Class 0 object<|sep|>We present an IRAM 30m line survey of the dense core L483 in the 3 mm band. We detected 71 molecules (140 including different isotopologs), most of which are present in the cold and quiescent ambient cloud. Of particular interest among the detected molecules are the cis isomer of HCOOH, the complex organic molecules HCOOCH3, CH3OCH3, and C2H5OH, a wide variety of carbon chains, nitrogen oxides like N2O, and saturated molecules like CH3SH. In general, fractional molecular abundances in L483 are systematically lower than in TMC-1 (especially for carbon chains), tend to be higher than in L1544 and B1-b, and are similar to those in L1527. Apart from the overabundance of carbon chains in TMC-1, we find that L483 does not have a marked chemical differentiation with respect to starless/prestellar cores like TMC-1 and L1544, although it does chemically differentiate from Class 0 hot corino sources like IRAS 16293-2422. This fact suggests that the chemical composition of the ambient cloud of some Class 0 sources could be largely inherited from the dark cloud starless/prestellar phase. We also derived isotopic ratios for a variety of molecules, some of which show isotopic anomalies like an extreme depletion in 13C for one of the two isotopologs of c-C3H2, a drastic enrichment in 18O for SO and HNCO (SO being also largely enriched in 17O), and different abundances for the two 13C substituted species of C2H and the two 15N substituted species of N2H+. We report the first detection in space of some minor isotopologs like c-C3D. The exhaustive chemical characterization of L483 presented here, together with similar studies of other prestellar and protostellar sources, should allow us to identify the main factors that regulate the chemical composition of cores along the process of formation of low-mass protostars.
Towards Auditable Distributed Systems<|sep|>The emerging trend towards distributed (cloud) systems (DS) has widely arrived whether in the automotive, public or the financial sector, but the execution of services of heterogeneous service providers is exposed to several risks. Beside hardware/software faults or cyber attacks that can influence the correctness of the system, fraud is also an issue. In such case it is not only important to verify the correctness of the system, but also have evidence which component and participant behaves faulty. This makes it possible, e.g. to claim for compensation after systems execution but also to assure information for verification can be trusted. The main goal of our research is to assure the monitoring of DS based on auditable information. We follow a decentralized monitoring strategy and envision a distributed monitoring approach of system properties based on distributedlogic programs that consider auditability. The expected contribution of this work is to establish with the application of our framework the mutual trust of distributed parties, as well as trust of clients in the systems execution. We showcase our ideas on a DS for booking services with unmanned air vehicles.
Performance and Stability of the Chelonia Storage Cloud<|sep|>In this paper we present the Chelonia storage cloud middleware. It was designed to fill the requirements gap between those of large, sophisticated scientific collaborations which have adopted the grid paradigm for their distributed storage needs, and of corporate business communities which are gravitating towards the cloud paradigm. The similarities to and differences between Chelonia and several well-known grid- and cloud-based storage solutions are commented. The design of Chelonia has been chosen to optimize high reliability and scalability of an integrated system of heterogeneous, geographically dispersed storage sites and the ability to easily expand the system dynamically. The architecture and implementation in term of web-services running inside the Advanced Resource Connector Hosting Environment Dameon (ARC HED) are described. We present results of tests in both local-area and wide-area networks that demonstrate the fault-tolerance, stability and scalability of Chelonia.
Verbal Characterization of Probabilistic Clusters using Minimal Discriminative Propositions<|sep|>In a knowledge discovery process, interpretation and evaluation of the mined results are indispensable in practice. In the case of data clustering, however, it is often difficult to see in what aspect each cluster has been formed. This paper proposes a method for automatic and objective characterization or "verbalization" of the clusters obtained by mixture models, in which we collect conjunctions of propositions (attribute-value pairs) that help us interpret or evaluate the clusters. The proposed method provides us with a new, in-depth and consistent tool for cluster interpretation/evaluation, and works for various types of datasets including continuous attributes and missing values. Experimental results with a couple of standard datasets exhibit the utility of the proposed method, and the importance of the feedbacks from the interpretation/evaluation step.
Inexpensive discrete atomistic model technique for studying excitations on infinite disordered media: the case of orientational glass ArN$_2$<|sep|>Excitations of disordered systems such as glasses are of fundamental and practical interest but computationally very expensive to solve. Here we introduce a technique for modeling these excitations in an infinite disordered medium with a reasonable computational cost. The technique relies on a discrete atomic model to simulate the low-energy behavior of an atomic lattice with molecular impurities. The interaction between different atoms is approximated using a spring like interaction based on the Lennard Jones potential but can be easily adapted to other potentials. The technique allows to solve a statistically representative number of samples with a minimum of computational expense, and uses a Monte-Carlo approach to achieve a state corresponding to any given temperature. This technique has already been applied successfully to a problem with interest in condensed matter physics: the solid solution of N$_2$ in Ar.
Mountain formation by repeated, inhomogeneous crustal failure in a neutron star<|sep|>The elastic crust of a neutron star fractures repeatedly as it spins down electromagnetically. An idealised, macroscopic model of inhomogeneous crustal failure is presented based on a cellular automaton with nearest-neighbour tectonic interactions involving strain redistribution and thermal dissipation. Predictions are made of the size and waiting-time distributions of failure events, as well as the rate of failure as the star spins down. The last failure event typically occurs when the star spins down to approximately 1% of its birth frequency with implications for rotational glitch activity. Neutron stars are commonly suggested as sources of continuous gravitational waves. The output of the automaton is converted into predictions of the star's mass ellipticity and gravitational wave strain as functions of its age, with implications for future observations with instruments such as the Laser Interferometer Gravitational Wave Observatory (LIGO), the Virgo interferometer, or the Kamioka Gravitational Wave Detector (KAGRA).
Robustness Meets Deep Learning: An End-to-End Hybrid Pipeline for Unsupervised Learning of Egomotion<|sep|>In this work, we propose a method that combines unsupervised deep learning predictions for optical flow and monocular disparity with a model based optimization procedure for instantaneous camera pose. Given the flow and disparity predictions from the network, we apply a RANSAC outlier rejection scheme to find an inlier set of flows and disparities, which we use to solve for the relative camera pose in a least squares fashion. We show that this pipeline is fully differentiable, allowing us to combine the pose with the network outputs as an additional unsupervised training loss to further refine the predicted flows and disparities. This method not only allows us to directly regress relative pose from the network outputs, but also automatically segments away pixels that do not fit the rigid scene assumptions that many unsupervised structure from motion methods apply, such as on independently moving objects. We evaluate our method on the KITTI dataset, and demonstrate state of the art results, even in the presence of challenging independently moving objects.
The role of magnetic fields in the formation of protostellar discs<|sep|>Truncated abstract: The formation of a protostellar disc is a natural outcome during the star formation process. As gas in a molecular cloud core collapses under self-gravity, the angular momentum of the gas will slow its collapse on small scales and promote the formation of a protostellar disc. Although the angular momenta of dense star-forming cores remain to be fully characterized observationally, existing data indicates that typical cores have enough angular momenta to form relatively large, rotationally supported discs. However, molecular clouds are observed to be permeated by magnetic fields, which can strongly affect the evolution of angular momentum through magnetic braking. Indeed, in the ideal MHD limit, magnetic braking has been shown to be so efficient as to remove essentially all of the angular momentum of the material close to the forming star such that disc formation is suppressed. This is known as the magnetic braking catastrophe. The catastrophe must be averted in order for the all-important rotationally supported discs to appear, but when and how this happens remains debated. We review the resolutions proposed to date, with emphasis on misalignment, turbulence and especially non-ideal effects. The dissipative non-ideal effects weaken the magnetic field, and the dispersive term redirects it to promote or hinder disc formation. When self-consistently applying non-ideal processes, rotationally supported discs of at least tens of au form, thus preventing the magnetic braking catastrophe. The non-ideal processes are sensitive to the magnetic field strength, cosmic ray ionization rate, and gas and dust grain properties, thus a complete understanding of the host molecular cloud is required. Therefore, the properties of the host molecular cloud -- and especially its magnetic field -- cannot be ignored when numerically modelling the formation and evolution of protostellar discs.
Hurricanes on tidally locked terrestrial planets: Fixed surface temperature experiments<|sep|>In this work, we study the presence of hurricanes on exoplanets. Tidally locked terrestrial planets around M dwarfs are the main targets of space missions looking to discover habitable exoplanets. The question of whether hurricanes can form on this kind of planet is important for determining their climate and habitability. Using a high-resolution global atmospheric circulation model, we investigated whether there are hurricanes on tidally locked terrestrial planets under fixed surface temperatures. The relevant effects of the planetary rotation rate, surface temperature, and bulk atmospheric compositions were examined. We find that hurricanes can form on the planets but not on all of them. For planets near the inner edge of the habitable zone of late M dwarfs, there are more numerous and stronger hurricanes on both day and night sides. For planets in the middle and outer ranges of the habitable zone, the possibility of hurricane formation is low or even close to zero, as has been suggested in recent studies. Earth-based hurricane theories are applicable to tidally locked planets only when the atmospheric compositions are similar to that of Earth. However, if the background atmosphere is lighter than H2O, hurricanes can hardly be produced because convection is always inhibited due to the effect of the mean molecular weight, similarly to the case of Saturn. These results have broad implications on the precipitation, ocean mixing, climate, and atmospheric characterization of tidally locked planets. Finally, A test with a coupled slab ocean and an Earth-like atmosphere in a tide-locked orbit of ten Earth days demonstrates that there are also hurricanes present in the experiment.
Incremental Import Vector Machines for Classifying Hyperspectral Data<|sep|>In this paper we propose an incremental learning strategy for import vector machines (IVM), which is a sparse kernel logistic regression approach. We use the procedure for the concept of self-training for sequential classification of hyperspectral data. The strategy comprises the inclusion of new training samples to increase the classification accuracy and the deletion of non-informative samples to be memory- and runtime-efficient. Moreover, we update the parameters in the incremental IVM model without re-training from scratch. Therefore, the incremental classifier is able to deal with large data sets. The performance of the IVM in comparison to support vector machines (SVM) is evaluated in terms of accuracy and experiments are conducted to assess the potential of the probabilistic outputs of the IVM. Experimental results demonstrate that the IVM and SVM perform similar in terms of classification accuracy. However, the number of import vectors is significantly lower when compared to the number of support vectors and thus, the computation time during classification can be decreased. Moreover, the probabilities provided by IVM are more reliable, when compared to the probabilistic information, derived from an SVM's output. In addition, the proposed self-training strategy can increase the classification accuracy. Overall, the IVM and the its incremental version is worthwhile for the classification of hyperspectral data.
Analyzing Interference from Static Cellular Cooperation using the Nearest Neighbour Model<|sep|>The problem of base station cooperation has recently been set within the framework of Stochastic Geometry. Existing works consider that a user dynamically chooses the set of stations that cooperate for his/her service. However, this assumption often does not hold. Cooperation groups could be predefined and static, with nodes connected by fixed infrastructure. To analyse such a potential network, in this work we propose a grouping method based on proximity. It is a variation of the so called Nearest Neighbour Model. We restrict ourselves to the simplest case where only singles and pairs of base stations are allowed to be formed. For this, two new point processes are defined from the dependent thinning of a Poisson Point Process, one for the singles and one for the pairs. Structural characteristics for the two are provided, including their density, Voronoi surface, nearest neighbour, empty space and J-function. We further make use of these results to analyse their interference fields and give explicit formulas to their expected value and their Laplace transform. The results constitute a novel toolbox towards the performance evaluation of networks with static cooperation.
Solving Nonlinear Parabolic Equations by a Strongly Implicit Finite-Difference Scheme<|sep|>We discuss the numerical solution of nonlinear parabolic partial differential equations, exhibiting finite speed of propagation, via a strongly implicit finite-difference scheme with formal truncation error $\mathcal{O}\left[(\Delta x)^2 + (\Delta t)^2 \right]$. Our application of interest is the spreading of viscous gravity currents in the study of which these type of differential equations arise. Viscous gravity currents are low Reynolds number (viscous forces dominate inertial forces) flow phenomena in which a dense, viscous fluid displaces a lighter (usually immiscible) fluid. The fluids may be confined by the sidewalls of a channel or propagate in an unconfined two-dimensional (or axisymmetric three-dimensional) geometry. Under the lubrication approximation, the mathematical description of the spreading of these fluids reduces to solving the so-called thin-film equation for the current's shape $h(x,t)$. To solve such nonlinear parabolic equations we propose a finite-difference scheme based on the Crank--Nicolson idea. We implement the scheme for problems involving a single spatial coordinate (i.e., two-dimensional, axisymmetric or spherically-symmetric three-dimensional currents) on an equispaced but staggered grid. We benchmark the scheme against analytical solutions and highlight its strong numerical stability by specifically considering the spreading of non-Newtonian power-law fluids in a variable-width confined channel-like geometry (a "Hele-Shaw cell") subject to a given mass conservation/balance constraint. We show that this constraint can be implemented by re-expressing it as nonlinear flux boundary conditions on the domain's endpoints. Then, we show numerically that the scheme achieves its full second-order accuracy in space and time. We also highlight through numerical simulations how the proposed scheme accurately respects the mass conservation/balance constraint.
Dimensional Regularization in Position Space and a Forest Formula for Regularized Epstein-Glaser Renormalization<|sep|>The present work contains a consistent formulation of the methods of dimensional regularization (DimReg) and minimal subtraction (MS) in Minkowski position space. The methods are implemented into the framework of perturbative Algebraic Quantum Field Theory (pAQFT). The developed methods are used to solve the Epstein-Glaser recursion for the construction of time-ordered products in all orders of causal perturbation theory. A solution is given in terms of a forest formula in the sense of Zimmermann. A relation to the alternative approach to renormalization theory using Hopf algebras is established.
Characterizing the red optical sky background fluctuations from narrow-band imaging<|sep|>The detection and characterization of the physical properties of very distant galaxies will be one the prominent science case of all future Extremely Large Telescopes, including the 39m E-ELT. Multi-Object Spectroscopic instruments are potentially very important tools for studying these objects, and in particular fiber-based concepts. However, detecting and studying such faint and distant sources will require subtraction of the sky background signal (i.e., between OH airglow lines) with an accuracy of ~1%. This requires a precise and accurate knowledge of the sky background temporal and spatial fluctuations. Using FORS2 narrow-band filter imaging data, we are currently investigating what are the fluctuations of the sky background at ~9000A. We present preliminary results of sky background fluctuations from this study over spatial scales reaching ~4 arcmin, as well as first glimpses into the temporal variations of such fluctuations over timescales of the order of the hour. This study (and other complementary on-going studies) will be essential in designing the next-generation fiber-fed instruments for the E-ELT.
Fermionic instantons<|sep|>We demonstrate the existence of a broad class of non-perturbative fermionic solutions to the Euclidean supergravity equations of motion, which are half-BPS and nonsingular, possess zero action, and obey an (anti)self-duality condition. These are identified as fermionic instantons associated to the status of the gravitino as the gauge field of local supersymmetry. By explicitly constructing these configurations from combinations of (anti)self-dual Yang-Mills gauge fields and Killing spinors, we may leverage the ADHM method and generalisations thereof to provide all possible solutions on certain gravitational backgrounds. As one may expect, these solutions generate and are in turn intrinsically dependent upon spacetime torsion.
Design of a Transport Triggered Architecture Processor for Flexible Iterative Turbo Decoder<|sep|>In order to meet the requirement of high data rates for the next generation wireless systems, the efficient implementation of receiver algorithms is essential. On the other hand, the rapid development of technology motivates the investigation of programmable implementations. This paper summarizes the design of a programmable turbo decoder as an applicationspecific instruction-set processor (ASIP) using Transport Triggered Architecture (TTA). The processor architecture is designed in such manner that it can be programmed to support other receiver algorithms, for example, decoding based on the Viterbi algorithm. Different suboptimal maximum a posteriori (MAP) algorithms are used and compared to one another for the softinput soft-output (SISO) component decoders in a single TTA processor. The max-log-MAP algorithm outperforms the other suboptimal algorithms in terms of latency. The design enables the designer to change the suboptimal algorithms according to the bit error rate (BER) performance requirement. Unlike many other programmable turbo decoder implementations, quadratic polynomial permutation (QPP) interleaver is used in this work for contention-free memory access and to make the processor 3GPP LTE compliant. Several optimization techniques to enable real time processing on programmable platforms are introduced. Using our method, with a single iteration 31.32 Mbps throughput is achieved for the max-log-MAP algorithm for a clock frequency of 200 MHz.
Compressing Low Precision Deep Neural Networks Using Sparsity-Induced Regularization in Ternary Networks<|sep|>A low precision deep neural network training technique for producing sparse, ternary neural networks is presented. The technique incorporates hard- ware implementation costs during training to achieve significant model compression for inference. Training involves three stages: network training using L2 regularization and a quantization threshold regularizer, quantization pruning, and finally retraining. Resulting networks achieve improved accuracy, reduced memory footprint and reduced computational complexity compared with conventional methods, on MNIST and CIFAR10 datasets. Our networks are up to 98% sparse and 5 & 11 times smaller than equivalent binary and ternary models, translating to significant resource and speed benefits for hardware implementations.
A Generalization of Gauge Invariance<|sep|>We consider perturbative quantum field theory in the causal framework. Gauge invariance is, in this framework, an identity involving chronological products of the interaction Lagrangian; it express the fact that the scattering matrix must leave invariant the sub-space of physical states. We are interested in generalizations of such identity involving Wick sub-monomials of the interaction Lagrangian. The analysis can be performed by direct computation in the lower orders of perturbation theory; guided by these computations we conjecture a generalization for arbitrary orders.
A direct Numerov sixth order numerical scheme to accurately solve the unidimensional Poisson equation with Dirichlet boundary conditions<|sep|>In this article, we present an analytical direct method, based on a Numerov three-point scheme, which is sixth order accurate and has a linear execution time on the grid dimension, to solve the discrete one-dimensional Poisson equation with Dirichlet boundary conditions. Our results should improve numerical codes used mainly in self-consistent calculations in solid state physics.
$D\overline{D}$ momentum correlations versus relative azimuth as a sensitive probe for thermalization<|sep|>In high-energy nuclear collisions at LHC, where a QGP might be created, the degree of thermalization at the partonic level is a key issue. Due to their large mass, heavy quarks are a powerful tool to probe thermalization. We propose to measure azimuthal correlations of heavy-quark hadrons and their decay products. Changes or even the complete absence of these initially existing azimuthal correlations in $Pb-Pb$ collisions might indicate thermalization at the partonic level. We present studies with PYTHIA for $p-p$ collisions at 14 TeV using the two-particle transverse momentum correlator ${<\overline{\Delta}p_{t,1}\overline{\Delta}p_{t,2}>}$ as a sensitive measure of potential changes in these azimuthal correlations. Contributions from transverse radial flow are estimated.
NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field<|sep|>In this paper, we present an efficient and robust deep learning solution for novel view synthesis of complex scenes. In our approach, a 3D scene is represented as a light field, i.e., a set of rays, each of which has a corresponding color when reaching the image plane. For efficient novel view rendering, we adopt a two-plane parameterization of the light field, where each ray is characterized by a 4D parameter. We then formulate the light field as a 4D function that maps 4D coordinates to corresponding color values. We train a deep fully connected network to optimize this implicit function and memorize the 3D scene. Then, the scene-specific model is used to synthesize novel views. Different from previous light field approaches which require dense view sampling to reliably render novel views, our method can render novel views by sampling rays and querying the color for each ray from the network directly, thus enabling high-quality light field rendering with a sparser set of training images. Per-ray depth can be optionally predicted by the network, thus enabling applications such as auto refocus. Our novel view synthesis results are comparable to the state-of-the-arts, and even superior in some challenging scenes with refraction and reflection. We achieve this while maintaining an interactive frame rate and a small memory footprint.
Precision Calculation of Inflation Correlators at One Loop<|sep|>We initiate a systematic study of precision calculation of the inflation correlators at the 1-loop level, starting in this paper with bosonic 1-loop bispectrum with chemical-potential enhancement. Such 1-loop processes could lead to important cosmological collider observables but are notoriously difficult to compute due to the lack of symmetries. We attack the problem from a direct numerical approach based on the real-time Schwinger-Keldysh formalism and show full numerical results for arbitrary kinematics containing both the oscillatory "signals" and the "backgrounds". Our results show that, while the non-oscillatory part can be one to two orders of magnitude larger, the oscillatory signal can be separated out by applying appropriate high-pass filters. We have also compared the result with analytic estimates typically adopted in the literature. While the amplitude is comparable, there is a non-negligible deviation in the frequency of the oscillatory part away from the extreme squeezed limit.
On Coding for Cooperative Data Exchange<|sep|>We consider the problem of data exchange by a group of closely-located wireless nodes. In this problem each node holds a set of packets and needs to obtain all the packets held by other nodes. Each of the nodes can broadcast the packets in its possession (or a combination thereof) via a noiseless broadcast channel of capacity one packet per channel use. The goal is to minimize the total number of transmissions needed to satisfy the demands of all the nodes, assuming that they can cooperate with each other and are fully aware of the packet sets available to other nodes. This problem arises in several practical settings, such as peer-to-peer systems and wireless data broadcast. In this paper, we establish upper and lower bounds on the optimal number of transmissions and present an efficient algorithm with provable performance guarantees. The effectiveness of our algorithms is established through numerical simulations.
maxsmooth: Rapid Maximally Smooth Function Fitting With Applications in Global 21-cm Cosmology<|sep|>Maximally Smooth Functions (MSFs) are a form of constrained functions in which there are no inflection points or zero crossings in high order derivatives. Consequently, they have applications to signal recovery in experiments where signals of interest are expected to be non-smooth features masked by larger smooth signals or foregrounds. They can also act as a powerful tool for diagnosing the presence of systematics. The constrained nature of MSFs makes fitting these functions a non-trivial task. We introduce maxsmooth, an open source package that uses quadratic programming to rapidly fit MSFs. We demonstrate the efficiency and reliability of maxsmooth by comparison to commonly used fitting routines and show that we can reduce the fitting time by approximately two orders of magnitude. We introduce and implement with maxsmooth Partially Smooth Functions, which are useful for describing elements of non-smooth structure in foregrounds. This work has been motivated by the problem of foreground modelling in 21-cm cosmology. We discuss applications of maxsmooth to 21-cm cosmology and highlight this with examples using data from the Experiment to Detect the Global Epoch of Reionization Signature (EDGES) and the Large-aperture Experiment to Detect the Dark Ages (LEDA) experiments. We demonstrate the presence of a sinusoidal systematic in the EDGES data with a log-evidence difference of $86.19\pm0.12$ when compared to a pure foreground fit. MSFs are applied to data from LEDA for the first time in this paper and we identify the presence of sinusoidal systematics. maxsmooth is pip installable and available for download at: https://github.com/htjb/maxsmooth
$X(3872)$, $X_b$, and the $\chi_{b1}(3P)$ state<|sep|>We discuss the possible production and discovery channels in $e^+e^-$ and $pp$ machines of the $X_b$, the bottomonium counterpart of $X(3872)$ and the putative isoscalar analogue of the charged bottomonium-like states $Z_b$ discovered by Belle. We suggest that the $X_b$ may be close in mass to the bottomonium state $\chi_{b1}(3P)$, mixing with it and sharing its decay channels, just as $X(3872)$ is likely a mixture of a $\bar D D^*$ molecule and $\chi_{c1}(2P)$. Consequently, the experiments which reported observing $\chi_{b1}(3P)$ might have actually discovered the $X_b$, or a mixture of the two states.
t-tbar Pair production cross section measurement at the LHC<|sep|>Measurement of $t\bar{t}$ pair production cross sections with an integrated luminosity of around 1 fb$^{-1}$ at $\sqrt{s}$ = 7 TeV obtained with the ATLAS and CMS detectors are reported. The inclusive cross sections in dilepton (ee, $e\mu$, $\mu\mu$ and $\mu\tau$), lepton+jets (e, $\mu$) and all hadronic decay modes are measured. In addition to inclusive cross section measurement, the study of jet multiplicity with additional jets are also presented, which is important to constrain the initial state radiation. Measurement of the charge asymmetry at the LHC is also presented. All measurements are compatible with Standard Model predictions.
Classical kinematics and Finsler structures for nonminimal Lorentz-violating fermions<|sep|>In the current paper the Lagrangian of a classical, relativistic point particle is obtained whose conjugate momentum satisfies the dispersion relation of a quantum wave packet that is subject to Lorentz violation based on a particular coefficient of the nonminimal Standard-Model Extension (SME). The properties of this Lagrangian are analyzed and two corresponding Finsler structures are obtained. One structure describes a scaled Euclidean geometry, whereas the other is neither a Riemann nor a Randers or Kropina structure. The results of the article provide some initial understanding of classical Lagrangians of the nonminimal SME fermion sector.
SoK: Decentralized Exchanges (DEX) with Automated Market Maker (AMM) Protocols<|sep|>As an integral part of the decentralized finance (DeFi) ecosystem, decentralized exchanges (DEX) with automated market maker (AMM) protocols have gained massive traction with the recently revived interest in blockchain and distributed ledger technology (DLT) in general. Instead of matching the buy and sell sides, AMMs employ a peer-to-pool method and determine asset price algorithmically through a so-called conservation function. To facilitate the improvement and development of AMM-based DEX, we create the first systematization of knowledge in this area. We first establish a general AMM framework describing the economics and formalizing the system's state-space representation. We then employ our framework to systematically compare the top AMM protocols' mechanics, illustrating their conservation functions, as well as slippage and divergence loss functions. We further discuss security and privacy concerns, how they are enabled by AMM-based DEX's inherent properties, and explore mitigating solutions. Finally, we conduct a comprehensive literature review on related work covering both DeFi and conventional market microstructure.
PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image<|sep|>This paper proposes a deep neural network (DNN) for piece-wise planar depthmap reconstruction from a single RGB image. While DNNs have brought remarkable progress to single-image depth prediction, piece-wise planar depthmap reconstruction requires a structured geometry representation, and has been a difficult task to master even for DNNs. The proposed end-to-end DNN learns to directly infer a set of plane parameters and corresponding plane segmentation masks from a single RGB image. We have generated more than 50,000 piece-wise planar depthmaps for training and testing from ScanNet, a large-scale RGBD video database. Our qualitative and quantitative evaluations demonstrate that the proposed approach outperforms baseline methods in terms of both plane segmentation and depth estimation accuracy. To the best of our knowledge, this paper presents the first end-to-end neural architecture for piece-wise planar reconstruction from a single RGB image. Code and data are available at https://github.com/art-programmer/PlaneNet.
On the Effectiveness of Discretizing Quantitative Attributes in Linear Classifiers<|sep|>Learning algorithms that learn linear models often have high representation bias on real-world problems. In this paper, we show that this representation bias can be greatly reduced by discretization. Discretization is a common procedure in machine learning that is used to convert a quantitative attribute into a qualitative one. It is often motivated by the limitation of some learners to qualitative data. Discretization loses information, as fewer distinctions between instances are possible using discretized data relative to undiscretized data. In consequence, where discretization is not essential, it might appear desirable to avoid it. However, it has been shown that discretization often substantially reduces the error of the linear generative Bayesian classifier naive Bayes. This motivates a systematic study of the effectiveness of discretizing quantitative attributes for other linear classifiers. In this work, we study the effect of discretization on the performance of linear classifiers optimizing three distinct discriminative objective functions --- logistic regression (optimizing negative log-likelihood), support vector classifiers (optimizing hinge loss) and a zero-hidden layer artificial neural network (optimizing mean-square-error). We show that discretization can greatly increase the accuracy of these linear discriminative learners by reducing their representation bias, especially on big datasets. We substantiate our claims with an empirical study on $42$ benchmark datasets.
Rainbow's Stars<|sep|>In recent years, a growing interest on the equilibrium of compact astrophysical objects like white dwarf and neutron stars has been manifested. In particular, various modifications due to Planck scale energy effects have been considered. In this paper we analyze the modification induced by Gravity's Rainbow on the equilibrium configurations described by the Tolman-Oppenheimer-Volkoff (TOV) equation. Our purpose is to explore the possibility that the Rainbow Planck-scale deformation of space-time could support the existence of different compact stars.
First results on the interactions of relativistic $^9$C nuclei in nuclear track emulsion<|sep|>\indent First results of the exposure of nuclear track emulsions in a secondary beam enriched by $^9$C nuclei at energy of 1.2 A GeV are described. The presented statistics corresponds to the most peripheral $^9$C interactions. For the first time a dissociation $^9$C $\to3^3$He not accompanied by target fragments and mesons is identified.\par
Diffractive Dijet Production in Antiproton-Proton Collisions at $\sqrt{s}$=1.96 TeV<|sep|>We report on a study of diffractive dijet production in $\bar{p}p$ collisions at $\sqrt{s}=1.96$ TeV using the CDF II detector at the Fermilab Tevatron $\bar{p}p$ collider. A data sample from 310 pb$^{-1}$ of integrated luminosity collected by triggering on a high transverse energy jet, $E_T^{jet}$, in coincidence with a recoil antiproton detected in a Roman pot spectrometer is used to measure the ratio of single-diffractive to inclusive-dijet event rates as a function of $x^{\bar p}$ of the interacting parton in the antiproton, the Bjorken-$x$, $x^{\bar p}_{Bj}$, and a $Q^2\approx (E_T^{jet})^2$ in the ranges $10^{-3}<x^{\bar p}_{Bj}<10^{-1}$ and $10^2<Q^2 <10^4$ GeV$^2$, respectively. Results are presented for the region of $\bar p$-momentum-loss fraction $0.03<\xi_{\bar p}<0.09$ and a four-momentum transfer squared $t_{\bar p}>-4$ GeV$^2$. The $t_{\bar p}$ dependence is measured as a function of $Q^2$ and $x_{Bj}^{\bar p}$ and compared with that of inclusive single diffraction dissociation. We find weak $x^{\bar p}_{Bj}$ and $Q^2$ dependencies in the ratio of single diffractive to inclusive event rates, and no significant $Q^2$ dependence in the diffractive $t_{\bar p}$ distributions.
Learning Abstract Models for Strategic Exploration and Fast Reward Transfer<|sep|>Model-based reinforcement learning (RL) is appealing because (i) it enables planning and thus more strategic exploration, and (ii) by decoupling dynamics from rewards, it enables fast transfer to new reward functions. However, learning an accurate Markov Decision Process (MDP) over high-dimensional states (e.g., raw pixels) is extremely challenging because it requires function approximation, which leads to compounding errors. Instead, to avoid compounding errors, we propose learning an abstract MDP over abstract states: low-dimensional coarse representations of the state (e.g., capturing agent position, ignoring other objects). We assume access to an abstraction function that maps the concrete states to abstract states. In our approach, we construct an abstract MDP, which grows through strategic exploration via planning. Similar to hierarchical RL approaches, the abstract actions of the abstract MDP are backed by learned subpolicies that navigate between abstract states. Our approach achieves strong results on three of the hardest Arcade Learning Environment games (Montezuma's Revenge, Pitfall!, and Private Eye), including superhuman performance on Pitfall! without demonstrations. After training on one task, we can reuse the learned abstract MDP for new reward functions, achieving higher reward in 1000x fewer samples than model-free methods trained from scratch.
Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary Framework<|sep|>Enforcing orthogonality in neural networks is an antidote for gradient vanishing/exploding problems, sensitivity by adversarial perturbation, and bounding generalization errors. However, many previous approaches are heuristic, and the orthogonality of convolutional layers is not systematically studied: some of these designs are not exactly orthogonal, while others only consider standard convolutional layers and propose specific classes of their realizations. To address this problem, we propose a theoretical framework for orthogonal convolutional layers, which establishes the equivalence between various orthogonal convolutional layers in the spatial domain and the paraunitary systems in the spectral domain. Since there exists a complete spectral factorization of paraunitary systems, any orthogonal convolution layer can be parameterized as convolutions of spatial filters. Our framework endows high expressive power to various convolutional layers while maintaining their exact orthogonality. Furthermore, our layers are memory and computationally efficient for deep networks compared to previous designs. Our versatile framework, for the first time, enables the study of architecture designs for deep orthogonal networks, such as choices of skip connection, initialization, stride, and dilation. Consequently, we scale up orthogonal networks to deep architectures, including ResNet, WideResNet, and ShuffleNet, substantially increasing the performance over the traditional shallow orthogonal networks.
Resolving orientation-specific diffusion-relaxation features via Monte-Carlo density-peak clustering in heterogeneous brain tissue<|sep|>Characterizing the properties and orientations of sub-voxel fiber populations, although essential to study white-matter architecture, microstructure and connectivity, remains one of the main challenges faced by the MRI microstructure community. While some progress has been made in overcoming this challenge using models, signal representations and tractography algorithms, these approaches are ultimately limited by their key assumptions or by the lack of specificity of the diffusion signal alone. In order to alleviate these limitations, we combine diffusion-relaxation MR acquisitions incorporating tensor-valued diffusion encoding, Monte-Carlo signal inversions that extract non-parametric intra-voxel distributions of diffusion tensors and relaxation rates, and density-based clustering techniques. This new approach, called "Monte-Carlo density-peak clustering" (MC-DPC), first delineates clusters in the diffusion-orientation subspace of the fiber-like diffusion-relaxation components output by Monte-Carlo signal inversions and then draws from the statistical aspect of these inversion algorithms to compute the median and interquartile range of orientation-resolved means of diffusivities and relaxation rates. Evaluating MC-DPC on tensor-valued diffusion-encoded and T2-weighted correlated datasets in silico and in vivo, we demonstrate its ability to simultaneously capture sub-voxel fiber orientations and cones of uncertainty, and measure fiber-specific diffusion-relaxation properties that are consistent with the known anatomy and existing literature. Straightforwardly translatable to other diffusion-relaxation correlation experiments probing $T_1$ and $T_2^*$, MC-DPC shows potential in tracking bundle-specific patient-control group differences and longitudinal microstructural changes, enabling new tools for microstructure-informed tractography, and mapping tract-specific myelination states.
Fast embedding of jets in heavy-ion collisions for background studies with ALICE<|sep|>Jet reconstruction in heavy-ion collisions is strongly affected by soft background from the underlying event. For an appropriate interpretation of the jet observables it is essential to understand the influence of the background and its fluctuations on the reconstructed jets. With this purpose we study random cones and the response of a known probe embedded in a heavy-ion events. The embedded probe can be a single high-p_T track or a jet from a simulated or real pp event. This allows a detailed study of background fluctuations and verification of the performance of background subtraction methods.
Proposal for High-harmonic EEHG Lasing at Shanghai Deep Ultra-Violet Free-electron Laser<|sep|>The echo-enabled harmonic generation (EEHG) free-electron laser (FEL) has been already demonstrated at lower harmonics and the first lasing at third harmonic also has been achieved at Shanghai deep ultra-violet FEL (SDUV-FEL). While the great advantage of much higher harmonic up-conversion efficiency of EEHG over other seeded FELs only shows evidently at much higher harmonics. In this paper, we investigate the possibility of EEHG lasing at 10-th harmonic of the seed laser at SDUV-FEL, both physical designs and numerical simulations have been studied carefully. Two proposals of EEHG at 10-th harmonic have been studied respectively, i.e. with the seed lasers of the same color and two difference colors, the simulation results indicate that both approaches could be the candidate for EEHG lasing at 10-th harmonic at SDUV-FEL, meanwhile the coherent synchrotron radiation does not affect the performance of EEHG-FEL but only slightly shifts the central radiation frequency.
Determination of a space-dependent force function in the one-dimensional wave equation<|sep|>The determination of an unknown spacewice dependent force function acting on a vibrating string from over-specified Cauchy boundary data is investigated numerically using the boundary element method (BEM) combined with a regularized method of separating variables. This linear inverse problem is ill-posed since small errors in the input data cause large errors in the output force solution. Consequently, when the input data is contaminated with noise we use the Tikhonov regularization method in order to obtain a stable solution. The choice of the regularization parameter is based on the L-curve method. Numerical results show that the solution is accurate for exact data and stable for noisy data
Effective potentials and kink spectra in non-integrable perturbed conformal field theories<|sep|>We analyze the evolution of the effective potential and the particle spectrum of two-parameter families of non-integrable quantum field theories. These theories are defined by deformations of conformal minimal models M_m by using the operators Phi_{1,3}, Phi_{1,2} and Phi_{2,1}. This study extends to all minimal models the analysis previously done for the classes of universality of the Ising, the Tricritical Ising and the RSOS models. We establish the symmetry and the duality properties of the various models also identifying the limiting theories that emerge when m goes to infinity.
An Analysis of Unsupervised Pre-training in Light of Recent Advances<|sep|>Convolutional neural networks perform well on object recognition because of a number of recent advances: rectified linear units (ReLUs), data augmentation, dropout, and large labelled datasets. Unsupervised data has been proposed as another way to improve performance. Unfortunately, unsupervised pre-training is not used by state-of-the-art methods leading to the following question: Is unsupervised pre-training still useful given recent advances? If so, when? We answer this in three parts: we 1) develop an unsupervised method that incorporates ReLUs and recent unsupervised regularization techniques, 2) analyze the benefits of unsupervised pre-training compared to data augmentation and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised samples, 3) verify our findings on STL-10. We discover unsupervised pre-training, as expected, helps when the ratio of unsupervised to supervised samples is high, and surprisingly, hurts when the ratio is low. We also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on STL-10.
New Protocols and Negative Results for Textual Entailment Data Collection<|sep|>Natural language inference (NLI) data has proven useful in benchmarking and, especially, as pretraining data for tasks requiring language understanding. However, the crowdsourcing protocol that was used to collect this data has known issues and was not explicitly optimized for either of these purposes, so it is likely far from ideal. We propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples. Using these alternatives and a fifth baseline protocol, we collect and compare five new 8.5k-example training sets. In evaluations focused on transfer learning applications, our results are solidly negative, with models trained on our baseline dataset yielding good transfer performance to downstream tasks, but none of our four new methods (nor the recent ANLI) showing any improvements over that baseline. In a small silver lining, we observe that all four new protocols, especially those where annotators edit pre-filled text boxes, reduce previously observed issues with annotation artifacts.
Boundary conditions for the Baumgarte-Shapiro-Shibata-Nakamura formulation of Einstein's field equations<|sep|>We discuss the initial-boundary value problem for the Baumgarte-Shapiro-Shibata-Nakamura evolution system of Einstein's field equations which has been used extensively in numerical simulations of binary black holes and neutron stars. We specify nine boundary conditions for this system with the following properties: (i) they impose the momentum constraint at the boundary, which is shown to preserve all the constraints throughout evolution, (ii) they approximately control the incoming gravitational degrees of freedom by specifying the Weyl scalar Psi_0 at the boundary, (iii) they control the gauge freedom by requiring a Neumann boundary condition for the lapse, by setting the normal component of the shift to zero, and by imposing a Sommerfeld-like condition on the tangential components of the shift, (iv) they are shown to yield a well-posed problem in the limit of weak gravity. Possible numerical applications of our results are also discussed briefly.
On Multi-source Networks: Enumeration, Rate Region Computation, and Hierarchy<|sep|>Recent algorithmic developments have enabled computers to automatically determine and prove the capacity regions of small hypergraph networks under network coding. A structural theory relating network coding problems of different sizes is developed to make best use of this newfound computational capability. A formal notion of network minimality is developed which removes components of a network coding problem that are inessential to its core complexity. Equivalence between different network coding problems under relabeling is formalized via group actions, an algorithm which can directly list single representatives from each equivalence class of minimal networks up to a prescribed network size is presented. This algorithm, together with rate region software, is leveraged to create a database containing the rate regions for all minimal network coding problems with five or fewer sources and edges, a collection of 744119 equivalence classes representing more than 9 million networks. In order to best learn from this database, and to leverage it to infer rate regions and their characteristics of networks at scale, a hierarchy between different network coding problems is created with a new theory of combinations and embedding operators.
Learning safety in model-based Reinforcement Learning using MPC and Gaussian Processes<|sep|>In this work, we propose a method to encourage safety in a Model Predictive Control (MPC)-based Reinforcement Learning (RL) agent via Gaussian Process (GP) regression. This framework consists of 1) a parametric MPC scheme that is employed as model-based controller with approximate knowledge on the real system's dynamics, 2) an episodic RL algorithm tasked with adjusting the MPC parametrization in order to increase its performance, and lastly, 3) GP regressors used to estimate, directly from data, constraints on the MPC parameters capable of predicting, up to some probability, whether the parametrization is likely to yield a safe or unsafe policy. These constraints are then enforced onto the RL updates in an effort to enhance the learning method with a probabilistic safety mechanism. Compared to other recent publications combining safe RL with MPC, our method does not require further assumptions on, e.g., the prediction model in order to retain computational tractability. We illustrate the results of our method in a numerical example on the control of a quadrotor drone in a safety-critical environment.
The presence of a critical end point in the QCD phase diagram from the net-baryon number fluctuations<|sep|>The net-baryon number fluctuations for three-flavor quark matter are computed within the Polyakov extended Nambu$-$Jona-Lasinio model. Two models with vanishing and nonvanishing vector interactions are considered. While the former predicts a critical end point (CEP) in the phase diagram, the latter predicts no CEP. We show that the nonmonotonic behavior of the susceptibilities in the phase diagram is still present even in the absence of a CEP. Therefore, from the nonmonotonic behavior of the susceptibilities solely, one cannot assume the existence of a CEP. We analyze other possible properties that may distinguish the two scenarios, and determine the behavior of the net-baryon number fluctuations and the velocity of sound along several isentropes, with moderate and small values. It is shown that the value of the susceptibilities ratios and the velocity of sound at two or three isentropic lines could possibly allow to distinguish both scenarios, a phase diagram with or without CEP. Smoother behaviors of these quantities may indicate the nonexistence of a CEP. We also discuss the critical behavior of the strange sector.
Progressive Collapse Mechanisms of Brittle and Ductile Framed Structures<|sep|>In this paper, we study the progressive collapse of 3D framed structures made of reinforced concrete after the sudden loss of a column. The structures are represented by elasto-plastic Euler Bernoulli beams with elongation-rotation failure threshold. We performed simulations using the Discrete Element Method considering inelastic collisions between the structural elements. The results show what collapse initiation and impact-driven propagation mechanisms are activated in structures with different geometric and mechanical features. Namely, we investigate the influence of the cross sectional size and reinforcement $\alpha$ and of the plastic capacity $\beta$ of the structural elements. We also study the final collapse extent and the fragment size distribution and their relation to $\alpha$, $\beta$ and to the observed collapse mechanisms. Finally, we compare the damage response of structures with symmetric and asymmetric reinforcement in the beams.
Multi-Temporal Spatial-Spectral Comparison Network for Hyperspectral Anomalous Change Detection<|sep|>Hyperspectral anomalous change detection has been a challenging task for its emphasis on the dynamics of small and rare objects against the prevalent changes. In this paper, we have proposed a Multi-Temporal spatial-spectral Comparison Network for hyperspectral anomalous change detection (MTC-NET). The whole model is a deep siamese network, aiming at learning the prevalent spectral difference resulting from the complex imaging conditions from the hyperspectral images by contrastive learning. A three-dimensional spatial spectral attention module is designed to effectively extract the spatial semantic information and the key spectral differences. Then the gaps between the multi-temporal features are minimized, boosting the alignment of the semantic and spectral features and the suppression of the multi-temporal background spectral difference. The experiments on the "Viareggio 2013" datasets demonstrate the effectiveness of proposed MTC-NET.
Fairness-aware Personalized Ranking Recommendation via Adversarial Learning<|sep|>Recommendation algorithms typically build models based on historical user-item interactions (e.g., clicks, likes, or ratings) to provide a personalized ranked list of items. These interactions are often distributed unevenly over different groups of items due to varying user preferences. However, we show that recommendation algorithms can inherit or even amplify this imbalanced distribution, leading to unfair recommendations to item groups. Concretely, we formalize the concepts of ranking-based statistical parity and equal opportunity as two measures of fairness in personalized ranking recommendation for item groups. Then, we empirically show that one of the most widely adopted algorithms -- Bayesian Personalized Ranking -- produces unfair recommendations, which motivates our effort to propose the novel fairness-aware personalized ranking model. The debiased model is able to improve the two proposed fairness metrics while preserving recommendation performance. Experiments on three public datasets show strong fairness improvement of the proposed model versus state-of-the-art alternatives. This is paper is an extended and reorganized version of our SIGIR 2020~\cite{zhu2020measuring} paper. In this paper, we re-frame the studied problem as `item recommendation fairness' in personalized ranking recommendation systems, and provide more details about the training process of the proposed model and details of experiment setup.
Hindered M1 Radiative Decay of $\Upsilon(2S)$ from Lattice NRQCD<|sep|>We present a calculation of the hindered M$1$ $\Upsilon(2S) \to \eta_b(1S) \gamma$ decay rate using lattice non-relativistic QCD. The calculation includes spin-dependent relativistic corrections to the NRQCD action through $\mathcal{O}(v^6)$ in the quark's relative velocity, relativistic corrections to the leading order current which mediates the transition through the quark's magnetic moment, radiative corrections to the leading spin-magnetic coupling and for the first time a full error budget. We also use gluon field ensembles at multiple lattice spacing values, all of which include $u$, $d$, $s$ and $c$ quark vacuum polarisation. Our result for the branching fraction is $\mathcal{B}(\Upsilon(2S)\to\eta_b(1S)\gamma) = 5.4(1.8)\times 10^{-4} $, which agrees with the current experimental value.
Multirate Synchronous Sampling of Sparse Multiband Signals<|sep|>Recent advances in optical systems make them ideal for undersampling multiband signals that have high bandwidths. In this paper we propose a new scheme for reconstructing multiband sparse signals using a small number of sampling channels. The scheme, which we call synchronous multirate sampling (SMRS), entails gathering samples synchronously at few different rates whose sum is significantly lower than the Nyquist sampling rate. The signals are reconstructed by solving a system of linear equations. We have demonstrated an accurate and robust reconstruction of signals using a small number of sampling channels that operate at relatively high rates. Sampling at higher rates increases the signal to noise ratio in samples. The SMRS scheme enables a significant reduction in the number of channels required when the sampling rate increases. We have demonstrated, using only three sampling channels, an accurate sampling and reconstruction of 4 real signals (8 bands). The matrices that are used to reconstruct the signals in the SMRS scheme also have low condition numbers. This indicates that the SMRS scheme is robust to noise in signals. The success of the SMRS scheme relies on the assumption that the sampled signals are sparse. As a result most of the sampled spectrum may be unaliased in at least one of the sampling channels. This is in contrast to multicoset sampling schemes in which an alias in one channel is equivalent to an alias in all channels. We have demonstrated that the SMRS scheme obtains similar performance using 3 sampling channels and a total sampling rate 8 times the Landau rate to an implementation of a multicoset sampling scheme that uses 6 sampling channels with a total sampling rate of 13 times the Landau rate.
A Temporal Module for Logical Frameworks<|sep|>In artificial intelligence, multi agent systems constitute an interesting typology of society modeling, and have in this regard vast fields of application, which extend to the human sciences. Logic is often used to model such kind of systems as it is easier to verify than other approaches, and provides explainability and potential validation. In this paper we define a time module suitable to add time to many logic representations of agents.
Subdivision based snakes for contour detection<|sep|>In this paper we propose a method for computing the contour of an object in an image using a snake represented as a subdivision curve. The evolution of the snake is driven by its control points which are computed minimizing an energy that pushes the snake towards the boundary of the interest region. Our method profits from the hierarchical nature of subdivision curves, since the unknowns of the optimization process are the few control points of the subdivision curve in the coarse representation and, at the same time, good approximations of the energies and their derivatives are obtained from the fine representation. We introduce a new region energy that guides the snake maximizing the contrast between the average intensity of the image within the snake and over the complement of the snake in a bounding box that does not change during the optimization. To illustrate the performance of our method we discuss the snakes associated with two classical subdivision schemes: the four point scheme and the cubic B-spline. Our experiments using synthetic and real images confirm that the proposed method is fast and robust.
Experimental Status of Supersymmetry after the LHC Run-I<|sep|>The ATLAS and CMS experiments at the Large Hadron Collider (LHC) at CERN have searched for signals of new physics, in particular for supersymmetry. The data collected until 2012 at center-of-mass energies of 7 and 8 TeV and integrated luminosities of 5 fb^-1 and 20 fb^-1, respectively, agree with the expectation from standard model processes. Constraints on supersymmetry have been calculated and interpreted in different models. Limits on supersymmetry particle masses at the TeV scale have been derived and interpreted generally in the context of simplified model spectra. The constrained minimal supersymmetric standard model is disfavored by the experimental results. Natural supersymmetry scenarios with low supersymmetry particle masses remain possible in multiple regions, for example in those with compressed spectra, that are difficult to access experimentally. The upgraded LHC operating at 13 TeV is gaining sensitivity to the remaining unexplored SUSY parameter space.
Separability criteria for continuous variable systems<|sep|>A general separability condition on the second moment (covariance matrix) for continuous variable two-party systems is derived by an analysis analogous to the derivation of the Kennard's uncertainty relation without referring to the non-negativity of the partially transposed density matrix. This separability criterion is generally more stringent than that used by Simon which is based on the non-negativity of partially transposed density matrix, and thus this criterion may be useful in the analysis of general continuous two-party systems. Another separability criterion used by Duan et al. is shown to be generally weaker than that of Simon. We thus have a hierarchy of separability criteria, but all these criteria when combined with suitable squeezing become equivalent at the boundary of the P-representation condition and thus turned out to be sufficient to analyze the separability of two-party Gaussian systems.
Numerical Study of Statistical Properties of the Galactic Center Distance Estimate from the Geometry of Spiral Arm Segments<|sep|>The influence of various factors on the statistical properties of the Galactic center distance ($R_0$) estimate obtained by solving the general problem of determining the geometric parameters of a Galactic spiral arm from its segment with the inclusion of the distance to the spiral pole, i.e., $R_0$, in the set of parameters has been studied by the Monte Carlo method. Our numerical simulations have been performed for the model segments representing the Perseus and Scutum arms based on masers in high-mass star forming regions. We show that the uncertainty in the present-day parallax measurements for these objects systematically decreases (!) with increasing heliocentric distance, while the relative uncertainty in the parallaxes is approximately constant. This lucky circumstance increases by a factor of 1.4-1.7 the accuracy of estimating $R_0$ from the arm segment traced by masers. Our numerical experiments provide evidence for the consistency of the $R_0$ estimate from the spiral-segment geometry. The significant biases of the estimate detected only for the Scutum arm are caused mainly by the random parallax errors, the small angular extent of the segment, and the small number of objects representing it. The dispersion of the $R_0$ estimate depends most strongly on the angular extent of the segment and the parallax uncertainty if the latter, on average, does not depend on the distance. When the data on 3-8 segments are processed simultaneously, the predicted standard error of the final estimate is $\sigma_{R_0} \simeq 0.5$-$0.3$ kpc, respectively. The accuracy can be improved by increasing the extent of the identified segments and the number of objects belonging to them. A more complex variant of the method taking into account the measuring and natural dispersions of objects relative to the arm center line will avoid the biases of the parameter estimates.
SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping<|sep|>Neural vocoder using denoising diffusion probabilistic model (DDPM) has been improved by adaptation of the diffusion noise distribution to given acoustic features. In this study, we propose SpecGrad that adapts the diffusion noise so that its time-varying spectral envelope becomes close to the conditioning log-mel spectrogram. This adaptation by time-varying filtering improves the sound quality especially in the high-frequency bands. It is processed in the time-frequency domain to keep the computational cost almost the same as the conventional DDPM-based neural vocoders. Experimental results showed that SpecGrad generates higher-fidelity speech waveform than conventional DDPM-based neural vocoders in both analysis-synthesis and speech enhancement scenarios. Audio demos are available at wavegrad.github.io/specgrad/.
Steady X-Ray Synchrotron Emission in the Northeastern Limb of SN 1006<|sep|>We investigate time variations and detailed spatial structures of X-ray synchrotron emission in the northeastern limb of SN 1006, using two Chandra observations taken in 2000 and 2008. We extract spectra from a number of small (about 10") regions. After taking account of proper motion and isolating the synchrotron from the thermal emission, we study time variations in the synchrotron emission in the small regions. We find that there are no regions showing strong flux variations. Our analysis shows an apparent flux decline in the overall synchrotron flux of about 4% at high energies, but we suspect that this is mostly a calibration effect, and that flux is actually constant to about 1%. This is much less than the variation found in other remnants where it was used to infer magnetic-field strengths up to 1 mG. We attribute the lack of variability to the smoothness of the synchrotron morphology, in contrast to the small-scale knots found to be variable in other remnants. The smoothness is to be expected for a Type Ia remnant encountering uniform material. Finally we find a spatial correlation between the flux and the cut-off frequency in synchrotron emission. The simplest interpretation is that the cut-off frequency depends on the magnetic-field strength. This would require that the maximum energy of accelerated electrons is not limited by synchrotron losses, but by some other effect. Alternatively, the rate of particle injection and acceleration may vary due to some effect not yet accounted for, such as a dependence on shock obliquity.
Risks of Friendships on Social Networks<|sep|>In this paper, we explore the risks of friends in social networks caused by their friendship patterns, by using real life social network data and starting from a previously defined risk model. Particularly, we observe that risks of friendships can be mined by analyzing users' attitude towards friends of friends. This allows us to give new insights into friendship and risk dynamics on social networks.
Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners<|sep|>Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as Flipped, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped outperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on average by 8.4% and 9.7% points, respectively. Flipped gives particularly large improvements on unseen labels, outperforming T0-11B by up to +20% average F1 score. This indicates that the strong task generalization of Flipped comes from improved generalization to novel labels. We release our code at https://github.com/seonghyeonye/Flipped-Learning.
Hadron Collider Production of Massive Color-Octet Vector Bosons at Next-to-Leading Order<|sep|>This paper completes the study of the next-to-leading order (NLO) QCD corrections to massive color-octet vector boson production at the LHC and Tevatron. The massive color-octet vector bosons are generically referred to as colorons. Building on our previous calculation of quark-initiated coloron production at NLO, we use the pinch technique to investigate coloron production via gluon fusion. We demonstrate that this one-loop production amplitude is finite, and find that its numerical contribution to coloron production is typically four orders of magnitude smaller than the contribution from quark annihilation. Coloron production via gluon fusion is therefore only relevant if the colorons are (nearly) fermiophobic. We then present extensive plots and tables of our full results for NLO coloron production at the Tevatron and the LHC.
The normalization of citation counts based on classification systems<|sep|>If we want to assess whether the paper in question has had a particularly high or low citation impact compared to other papers, the standard practice in bibliometrics is to normalize citations in respect of the subject category and publication year. A number of proposals for an improved procedure in the normalization of citation impact have been put forward in recent years. Against the background of these proposals this study describes an ideal solution for the normalization of citation impact: in a first step, the reference set for the publication in question is collated by means of a classification scheme, where every publication is associated with a single principal research field or subfield entry (e. g. via Chemical Abstracts sections) and a publication year. In a second step, percentiles of citation counts are calculated for this set and used to assign the normalized citation impact score to the publications (and also to the publication in question).
Confinement: $G_2$ group case<|sep|>The gauge group being centreless, $G_2$ gauge theory is a good laboratory for studying the role of the centre of the group for colour confinement in Yang-Mills gauge theories. In this paper, we investigate $G_2$ pure gauge theory at finite temperature on the lattice. By studying the finite size scaling of the plaquette, the Polyakov loop and their susceptibilities, we show that a deconfinement phase transition takes place. The analysis of the pseudocritical exponents give strong evidence of the deconfinement transition being first order. Implications of our findings for scenarios of colour confinement are discussed.
Online Human Activity Recognition Employing Hierarchical Hidden Markov Models<|sep|>In the last few years there has been a growing interest in Human Activity Recognition~(HAR) topic. Sensor-based HAR approaches, in particular, has been gaining more popularity owing to their privacy preserving nature. Furthermore, due to the widespread accessibility of the internet, a broad range of streaming-based applications such as online HAR, has emerged over the past decades. However, proposing sufficiently robust online activity recognition approach in smart environment setting is still considered as a remarkable challenge. This paper presents a novel online application of Hierarchical Hidden Markov Model in order to detect the current activity on the live streaming of sensor events. Our method consists of two phases. In the first phase, data stream is segmented based on the beginning and ending of the activity patterns. Also, on-going activity is reported with every receiving observation. This phase is implemented using Hierarchical Hidden Markov models. The second phase is devoted to the correction of the provided label for the segmented data stream based on statistical features. The proposed model can also discover the activities that happen during another activity - so-called interrupted activities. After detecting the activity pane, the predicted label will be corrected utilizing statistical features such as time of day at which the activity happened and the duration of the activity. We validated our proposed method by testing it against two different smart home datasets and demonstrated its effectiveness, which is competing with the state-of-the-art methods.
A Computer Algorithm For Engineering Off-Shell Multiplets With Four Supercharges On The World Sheet<|sep|>We present an adinkra-based computer algorithm implemented in a Mathematica code and use it in a limited demonstration of how to engineer off-shell, arbitrary N-extended world-sheet supermultiplets. Using one of the outputs from this algorithm, we present evidence for the unexpected discovery of a previously unknown 8 - 8 representation of N = 2 world sheet supersymmetry. As well, we uncover a menagerie of (p, q) = (3, 1) world sheet supermultiplets.
Polar Broad Absorption Line Quasars: An Open Question<|sep|>It has been argued that certain broad absorption line quasars are viewed within 35 degrees of the axis of a relativistic radio jet, based on two-epoch radio flux density variability. It is true if the surface brightness of a radio source is observed to change by a sufficiently large amount, the inferred brightness temperature will exceed 10^12 K and Doppler beaming in our direction must be invoked to avoid a Compton cooling catastrophe. However, flux density changes cannot be linked to surface brightness changes without knowledge of the size of the source. If an optically thick source changes in projected area but not surface brightness, its brightness temperature is constant and its flux variability yields no constraint on its orientation. Moreover, as pointed out by Rees, spherical expansion of an emission source at relativistic speeds yields an apparently superluminal increase in its projected area, which can explain short-timescale flux density variability without requiring a relativistic jet oriented near to our line of sight. Therefore, two-epoch radio flux density variability by itself cannot unambiguously identify sources with jets directed towards us. Only VLBI imaging can robustly determine the fraction of broad absorption line quasars which are polar.
Mean-field universality class induced by weak hyperbolic curvatures<|sep|>Order-disorder phase transition of the ferromagnetic Ising model is investigated on a series of two-dimensional lattices that have negative Gaussian curvatures. Exceptional lattice sites of coordination number seven are distributed on the triangular lattice, where the typical distance between the nearest exceptional sites is proportional to an integer parameter $n$. Thus, the corresponding curvature is asymptotically proportional to $- n^{-2}_{~}$. Spontaneous magnetization and specific heat are calculated by means of the corner transfer matrix renormalization group method. For all the finite $n$ cases, we observe the mean-field-like phase transition. It is confirmed that the entanglement entropy at the transition temperature is linear in $(c / 6) \ln n$, where $c = 1 / 2$ is the central charge of the Ising model. The fact agrees with the presence of the typical length scale $n$ being proportional to the curvature radius.
Hybrid Attention Networks for Flow and Pressure Forecasting in Water Distribution Systems<|sep|>Multivariate geo-sensory time series prediction is challenging because of the complex spatial and temporal correlation. In urban water distribution systems (WDS), numerous spatial-correlated sensors have been deployed to continuously collect hydraulic data. Forecasts of monitored flow and pressure time series are of vital importance for operational decision making, alerts and anomaly detection. To address this issue, we proposed a hybrid dual-stage spatial-temporal attention-based recurrent neural networks (hDS-RNN). Our model consists of two stages: a spatial attention-based encoder and a temporal attention-based decoder. Specifically, a hybrid spatial attention mechanism that employs inputs along temporal and spatial axes is proposed. Experiments on a real-world dataset are conducted and demonstrate that our model outperformed 9 baseline models in flow and pressure series prediction in WDS.
Transverse momentum spectra of charged hadrons in jets at the Tevatron<|sep|>The hadronic $\kt$-spectrum inside a high energy jet is determined including corrections of relative magnitude $\cO{\sqrt{\alpha_s}}$ with respect to the Modified Leading Logarithmic Approximation (MLLA), in the limiting spectrum approximation (assuming an infrared cut-off $Q_0 =\lqcd$) and beyond ($Q_0\ne\lqcd$). The results in the limiting spectrum approximation are found to be, after normalization, in impressive agreement with measurements by the CDF collaboration. A new integral representation for the "hump-backed" plateau is also reported.
Nonlocal resistance and its fluctuations in microstructures of band-inverted HgTe/(Hg,Cd)Te quantum wells<|sep|>We investigate experimentally transport in gated microsctructures containing a band-inverted HgTe/Hg_{0.3}Cd_{0.7}Te quantum well. Measurements of nonlocal resistances using many contacts prove that in the depletion regime the current is carried by the edge channels, as expected for a two-dimensional topological insulator. However, high and non-quantized values of channel resistances show that the topological protection length (i.e. the distance on which the carriers in helical edge channels propagate without backscattering) is much shorter than the channel length, which is ~100 micrometers. The weak temperature dependence of the resistance and the presence of temperature dependent reproducible quasi-periodic resistance fluctuations can be qualitatively explained by the presence of charge puddles in the well, to which the electrons from the edge channels are tunnel-coupled.
A Ternary Bi-Directional LSTM Classification for Brain Activation Pattern Recognition Using fNIRS<|sep|>Functional near-infrared spectroscopy (fNIRS) is a non-invasive, low-cost method used to study the brain's blood flow pattern. Such patterns can enable us to classify performed by a subject. In recent research, most classification systems use traditional machine learning algorithms for the classification of tasks. These methods, which are easier to implement, usually suffer from low accuracy. Further, a complex pre-processing phase is required for data preparation before implementing traditional machine learning methods. The proposed system uses a Bi-Directional LSTM based deep learning architecture for task classification, including mental arithmetic, motor imagery, and idle state using fNIRS data. Further, this system will require less pre-processing than the traditional approach, saving time and computational resources while obtaining an accuracy of 81.48\%, which is considerably higher than the accuracy obtained using conventional machine learning algorithms for the same data set.
Flexible Shrinkage Estimation in High-Dimensional Varying Coefficient Models<|sep|>We consider the problem of simultaneous variable selection and constant coefficient identification in high-dimensional varying coefficient models based on B-spline basis expansion. Both objectives can be considered as some type of model selection problems and we show that they can be achieved by a double shrinkage strategy. We apply the adaptive group Lasso penalty in models involving a diverging number of covariates, which can be much larger than the sample size, but we assume the number of relevant variables is smaller than the sample size via model sparsity. Such so-called ultra-high dimensional settings are especially challenging in semiparametric models as we consider here and has not been dealt with before. Under suitable conditions, we show that consistency in terms of both variable selection and constant coefficient identification can be achieved, as well as the oracle property of the constant coefficients. Even in the case that the zero and constant coefficients are known a priori, our results appear to be new in that it reduces to semivarying coefficient models (a.k.a. partially linear varying coefficient models) with a diverging number of covariates. We also theoretically demonstrate the consistency of a semiparametric BIC-type criterion in this high-dimensional context, extending several previous results. The finite sample behavior of the estimator is evaluated by some Monte Carlo studies.
Facilitating Satellite-Airborne-Terrestrial Integration for Dynamic and Infrastructure-less Networks<|sep|>This paper studies the potential improvement in the achievable data rate available to ground users by integrating satellite, airborne, and terrestrial networks. The goal is to establish dynamic wireless services in remote or infrastructure-less areas. This integration uses high-altitude platforms in the exosphere, stratosphere, and troposphere for better altitude reuse coupled with emerging optical or other high-frequency directional transceivers. Hence they offer a significant increases in the scarce spectrum aggregate efficiency. However, managing resource allocation with deployment in this integrated system still has some difficulties. This paper aims to tackle resource management challenges by (i) providing wireless services to ground users in remote areas and connecting them with metropolitan and rural areas, (ii) employing high-altitude platforms (HAPs) equipped with free-space-optical communication modules for back-hauling backbone. Finally, we show how our results illustrate the advantages of using the proposed scheme.
An anisotropic bouncing universe in non-local gravity<|sep|>We show that it is possible to realize a cosmological bouncing solution in an anisotropic but homogeneous Bianchi-I background in a class of non-local, infinite derivative theories of gravity. We show that the anisotropic shear grows slower than in general relativity during the contraction phase, peaks to a finite value at the bounce point, and then decreases as the universe asymptotes towards isotropy and homogeneity, and ultimately to de Sitter. Along with a cosmological constant, the matter sector required to drive such a bounce is found to consist of three components - radiation, stiff matter and $k$-matter (whose energy density decays like the inverse square of the average scale factor). Generically, $k$-matter exerts anisotropic pressures. We will test the bouncing solution in local and non-local gravity and show that in the latter case it is possible to simultaneously satisfy positivity of energy density and, at least in the late time de Sitter phase, avoid the introduction of propagating ghost/tachyonic modes.
A Type-coherent, Expressive Representation as an Initial Step to Language Understanding<|sep|>A growing interest in tasks involving language understanding by the NLP community has led to the need for effective semantic parsing and inference. Modern NLP systems use semantic representations that do not quite fulfill the nuanced needs for language understanding: adequately modeling language semantics, enabling general inferences, and being accurately recoverable. This document describes underspecified logical forms (ULF) for Episodic Logic (EL), which is an initial form for a semantic representation that balances these needs. ULFs fully resolve the semantic type structure while leaving issues such as quantifier scope, word sense, and anaphora unresolved; they provide a starting point for further resolution into EL, and enable certain structural inferences without further resolution. This document also presents preliminary results of creating a hand-annotated corpus of ULFs for the purpose of training a precise ULF parser, showing a three-person pairwise interannotator agreement of 0.88 on confident annotations. We hypothesize that a divide-and-conquer approach to semantic parsing starting with derivation of ULFs will lead to semantic analyses that do justice to subtle aspects of linguistic meaning, and will enable construction of more accurate semantic parsers.
Critical Behaviour in Trapped Strongly Interacting Fermi Gases<|sep|>We investigate the width of the Ginzburg critical region and experimental signatures of critical behavior in strongly interacting trapped Fermi gases close to unitarity, where the s-wave scattering length diverges. Despite the fact that the width of the critical region is of the order unity, evidence of critical behavior in the bulk thermodynamics of trapped gases is strongly suppressed by their inhomogeneity. The specific heat of a harmonically confined gas, for instance, is \textit{linear} in the reduced temperature $t = (T-T_{\mathrm{c}})/T_{\mathrm{c}}$ above $T_{\mathrm{c}}$. We also discuss the prospects of observing critical behavior in the local compressibility from measurements of the density profile.
Nonstochastic Multi-Armed Bandits with Graph-Structured Feedback<|sep|>We present and study a partial-information model of online learning, where a decision maker repeatedly chooses from a finite set of actions, and observes some subset of the associated losses. This naturally models several situations where the losses of different actions are related, and knowing the loss of one action provides information on the loss of other actions. Moreover, it generalizes and interpolates between the well studied full-information setting (where all losses are revealed) and the bandit setting (where only the loss of the action chosen by the player is revealed). We provide several algorithms addressing different variants of our setting, and provide tight regret bounds depending on combinatorial properties of the information feedback structure.
Conspiracy of BSM physics and cosmology<|sep|>The lack of experimental evidence at the LHC for physics beyond the Standard model (BSM) of elementary particles together with necessity of its existence to provide solutions of internal problems of the Standard model (SM) as well as of physical nature of the basic elements of the modern cosmology demonstrates the conspiracy of BSM physics. Simultaneously the data of precision cosmology only tighten the constraints on the deviations from the now standard LambdaCDM model and thus exhibit conspiracy of the nonstandard cosmological scenarios. We show that studying new physics in combination of its physical, astrophysical and cosmological probes, can not only unveil the conspiracy of BSM physics but will also inevitably reveal nonstandard features in the cosmological scenario.
Modes of an elliptical cylindrical resonant cavity -- analytical solution<|sep|>An analytical solution of the Helmholtz equation for electromagnetic field distribution in a resonant cavity with elliptic cross-section is found. We compare the frequencies of the eigenmodes with numerical and experimental values for a metallic cavity and find an excellent matching. We focus our analysis on the microwave frequency region, and show how the ellipticity of the cavity (ratio of the minor and major axes length $b/a$) influences several mode frequencies and also the $Q$-factor of the cavity. By doing so, we demonstrate how the elliptic geometry splits the degeneracy of certain modes of the circular cylindric cavity.
Recent Progress of Heterostructures Based on Two Dimensional Materials and Wide Bandgap Semiconductors<|sep|>Recent progress in the synthesis and assembly of two-dimensional (2D) materials has laid the foundation for various applications of atomically thin layer films. These 2D materials possess rich and diverse properties such as layer-dependent band gaps, interesting spin degrees of freedom, and variable crystal structures. They exhibit broad application prospects in micro-nano devices. In the meantime, the wide bandgap semiconductors (WBS) with an elevated breakdown voltage, high mobility, and high thermal conductivity have shown important applications in high-frequency microwave devices, high-temperature and high-power electronic devices. Beyond the study on single 2D materials or WBS materials, the multi-functional 2D/WBS heterostructures can promote the carrier transport at the interface, potentially providing novel physical phenomena and applications, and improving the performance of electronic and optoelectronic devices. In this review, we overview the advantages of the heterostructures of 2D materials and WBS materials, and introduce the construction methods of 2D/WBS heterostructures. Then, we present the diversity and recent progress in the applications of 2D/WBS heterostructures, including photodetectors, photocatalysis, sensors, and energy related devices. Finally, we put forward the current challenges of 2D/WBS heterostructures and propose the promising research directions in the future.
Non-redundant random generation algorithms for weighted context-free languages<|sep|>We address the non-redundant random generation of $k$ words of length $n$ in a context-free language. Additionally, we want to avoid a predefined set of words. We study a rejection-based approach, whose worst-case time complexity is shown to grow exponentially with $k$ for some specifications and in the limit case of a coupon collector. We propose two algorithms respectively based on the recursive method and on an unranking approach. We show how careful implementations of these algorithms allow for a non-redundant generation of $k$ words of length $n$ in $\mathcal{O}(k\cdot n\cdot \log{n})$ arithmetic operations, after a precomputation of $\Theta(n)$ numbers. The overall complexity is therefore dominated by the generation of $k$ words, and the non-redundancy comes at a negligible cost.
A Generic Framework for Task Offloading in mmWave MEC Backhaul Networks<|sep|>With the emergence of millimeter-Wave (mmWave) communication technology, the capacity of mobile backhaul networks can be significantly increased. On the other hand, Mobile Edge Computing (MEC) provides an appropriate infrastructure to offload latency-sensitive tasks. However, the amount of resources in MEC servers is typically limited. Therefore, it is important to intelligently manage the MEC task offloading by optimizing the backhaul bandwidth and edge server resource allocation in order to decrease the overall latency of the offloaded tasks. This paper investigates the task allocation problem in MEC environment, where the mmWave technology is used in the backhaul network. We formulate a Mixed Integer NonLinear Programming (MINLP) problem with the goal to minimize the total task serving time. Its objective is to determine an optimized network topology, identify which server is used to process a given offloaded task, find the path of each user task, and determine the allocated bandwidth to each task on mmWave backhaul links. Because the problem is difficult to solve, we develop a two-step approach. First, a Mixed Integer Linear Program (MILP) determining the network topology and the routing paths is optimally solved. Then, the fractions of bandwidth allocated to each user task are optimized by solving a quasi-convex problem. Numerical results illustrate the obtained topology and routing paths for selected scenarios and show that optimizing the bandwidth allocation significantly improves the total serving time, particularly for bandwidth-intensive tasks.
Coronal loop physical parameters from the analysis of multiple observed transverse oscillations<|sep|>The analysis of quickly damped transverse oscillations of solar coronal loops using magneto-hydrodynamic seismology allow us to infer physical parameters that are difficult to measure otherwise. Under the assumption that such damped oscillations are due to the resonant conversion of global modes into Alfven oscillations of the tube surface, we carry out a global seismological analysis of a large set of coronal loops. A Bayesian hierarchical method is used to obtain distributions for coronal loop physical parameters by means of a global analysis of a large number of observations. The resulting distributions summarise global information and constitute data-favoured information that can be used for the inversion of individual events. The results strongly suggest that internal Alfven travel times along the loop are larger than 100 s and smaller than 540 s with 95% probability. Likewise, the density contrast between the loop interior and the surrounding is larger than 2.3 and below 6.9 with 95% probability.
Malware Classification with Word Embedding Features<|sep|>Malware classification is an important and challenging problem in information security. Modern malware classification techniques rely on machine learning models that can be trained on features such as opcode sequences, API calls, and byte $n$-grams, among many others. In this research, we consider opcode features. We implement hybrid machine learning techniques, where we engineer feature vectors by training hidden Markov models -- a technique that we refer to as HMM2Vec -- and Word2Vec embeddings on these opcode sequences. The resulting HMM2Vec and Word2Vec embedding vectors are then used as features for classification algorithms. Specifically, we consider support vector machine (SVM), $k$-nearest neighbor ($k$-NN), random forest (RF), and convolutional neural network (CNN) classifiers. We conduct substantial experiments over a variety of malware families. Our experiments extend well beyond any previous work in this field.
Microscopic study of neutron-rich Dysprosium isotopes<|sep|>Microscopic studies in heavy nuclei are very scarce due to large valence spaces involved. This computational problem can be avoided by means of the use of symmetry based models. Ground-state, gamma and beta-bands, and their B(E2) transition strengths in 160-168Dy isotopes, are studied in the framework of the pseudo-SU(3) model which includes the preserving symmetry Q.Q term and the symmetry-breaking Nilsson and pairing terms, systematically parametrized. Additionally, three rotor-like terms are considered whose free parameters, fixed for all members of the chain are used to fine tune the moment of inertia of rotational bands and the band-head of gamma and beta-bands. The model succesfully describes in a systematic way rotational features in these nuclei and allows to extrapolate toward the midshell nucleus 170Dy. The results presented show that it is possible to study full chain of isotopes or isotones in the region with the present model.
Numerically exact counting statistics of energy current in the Kondo regime<|sep|>We use the inchworm Quantum Monte Carlo method to investigate the full counting statistics of particle and energy currents in a strongly correlated quantum dot. Our method is used to extract the heat fluctuations and entropy production of a quantum thermoelectric device, as well as cumulants of the particle and energy currents. The energy--particle current cross correlations reveal information on the preparation of the system and the interplay of thermal and electric currents. We furthermore demonstrate the signature of a crossover from Coulomb blockade to Kondo physics in the energy current fluctuations, and show how the conventional master equation approach to full counting statistics systematically fails to capture this crossover.
Non-terrestrial Communications Assisted by Reconfigurable Intelligent Surfaces<|sep|>Non-terrestrial communications have emerged as a key enabler for seamless connectivity in the upcoming generation networks. This kind of network can support high data rate communications among aerial platforms (i.e., unmanned aerial vehicles (UAVs), high-altitude platforms (HAPs), and satellites) and cellular networks, achieving anywhere and anytime connections. However, there are many practical implementation limitations, especially overload power consumption, high probability of blockage, and dynamic propagation environment. Fortunately, the recent technology reconfigurable intelligent surface (RIS) is expected to be one of the most cost-efficient solutions to address such issues. RIS with low-cost elements can bypass blockages and create multiple line-of-sight (LoS) links, and provide controllable communication channels. In this paper, we present a comprehensive literature review on the RIS-assisted non-terrestrial networks (RANTNs). Firstly, the framework of the RANTNs is introduced with detailed discussion about distinct properties of RIS in NTNs and the two types of RIS, that is, terrestrial RISs (TRISs), and aerial RISs (ARISs), and the classification of RANTNs including RIS-assisted air-to-ground (A2G)/ground-to-air (G2A), ARIS-assisted ground-to-ground (G2G), and RIS-assisted air-to-air (A2A) communications. In combination with next-generation communication technologies, the advanced technologies in RANTNs are discussed. Then we overview the literature related to RANTNs from the perspectives of performance analysis and optimization, followed by the widely used methodologies. Finally, open challenges and future research direction in the context of the RANTNs are highlighted.
GraphLab: A New Framework for Parallel Machine Learning<|sep|>Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.
Soft Photon Theorem for High Energy Amplitudes in Yukawa and Scalar Theories<|sep|>We study the emission of soft photons coupling to high energy fixed angle scattering processes at first order in the electromagnetic coupling but to all loop orders in a class of theories without soft divergences, including massive and massless Yukawa and scalar theories. We adapt a method introduced by del Duca for quantum electrodynamics to show that subleading corrections to the soft photon theorem are sensitive to the structure of non leading external jets of collinear lines. Our techniques are based on a power counting analysis of loop integrals and an application of jet Ward identities. We also apply Grammer and Yennie's decomposition to isolate separately gauge invariant contributions to the soft photon expansion. These are interpreted as infrared sensitive matrix elements coupling to a field strength tensor.
Will quantum cosmology resurrect chaotic inflation model?<|sep|>The single field chaotic inflation model with a monomial power greater than one seems to be ruled out by the recent Planck and WMAP CMB data while Starobinsky model with a higher curvature term seems to be a viable model. Higher curvature terms being originated from quantum fluctuations, we revisit the quantum cosmology of the Wheeler-DeWitt equation for the chaotic inflation model. The semiclassical cosmology emerges from quantum cosmology with fluctuations of spacetimes and matter when the wave function is peaked around the semiclassical trajectory with quantum corrections a la the de Broglie-Bohm pilot theory.
Sketch-Driven Regular Expression Generation from Natural Language and Examples<|sep|>Recent systems for converting natural language descriptions into regular expressions (regexes) have achieved some success, but typically deal with short, formulaic text and can only produce simple regexes. Realworld regexes are complex, hard to describe with brief sentences, and sometimes require examples to fully convey the user's intent. We present a framework for regex synthesis in this setting where both natural language (NL) and examples are available. First, a semantic parser (either grammar-based or neural) maps the natural language description into an intermediate sketch, which is an incomplete regex containing holes to denote missing components. Then a program synthesizer searches over the regex space defined by the sketch and finds a regex that is consistent with the given string examples. Our semantic parser can be trained purely from weak supervision based on correctness of the synthesized regex, or it can leverage heuristically-derived sketches. We evaluate on two prior datasets (Kushman and Barzilay, 2013; Locascio et al., 2016) and a real-world dataset from Stack Overflow. Our system achieves state-of-the-art performance on the prior datasets and solves 57% of the real-world dataset, which existing neural systems completely fail on.
Influence of Planck foreground masks in the large angular scale quadrant CMB asymmetry<|sep|>The measured CMB angular distribution shows a great consistency with the LCDM model. However, isotropy violations were reported in CMB temperature maps of both WMAP and Planck data. We investigate the influence of different masks employed in the analysis of CMB angular distribution, in particular in the excess of power in the Southeastern quadrant (SEQ) and the lack of power in the Northeastern quadrant (NEQ). We compare the two-point correlation function (TPCF) computed for each quadrant of the CMB foreground-cleaned temperature maps to 1000 simulations generated assuming the LCDM best-fit power spectrum using four different masks. In addition to the quadrants, we computed the TPCF for circular regions in the map where the excess and lack of power are present. We also compare the effect of Galactic cuts in the TPCF calculations as compared to the simulations. We found consistent results for three masks, namely mask-rulerminimal, U73 and U66. The results indicate that the excess of power in the SEQ tends to vanish as the portion of the sky covered by the mask increases and the lack of power in the NEQ remains virtually unchanged. When UT78 mask is applied, the NEQ becomes no longer anomalous and the excess of power in the SEQ becomes the most significant one among the masks. Nevertheless, the asymmetry between the SEQ and NEQ is independent of the mask and it is in disagreement with the isotropic model with at least 95% C.L. We find that UT78 is in disagreement with the other analysed masks, specially considering the SEQ and the NEQ individual analysis. Most importantly, the use of UT78 washes out the anomaly in the NEQ. Furthermore, we found excess of kurtosis, compared with simulations, in the NEQ for the regions not masked by UT78 but masked by the other masks, indicating that the previous result could be due to non-removed residual foregrounds by UT78.
Solar neutrinos and neutrino physics<|sep|>Solar neutrino studies triggered and largely motivated the major developments in neutrino physics in the last 50 years. Theory of neutrino propagation in different media with matter and fields has been elaborated. It includes oscillations in vacuum and matter, resonance flavor conversion and resonance oscillations, spin and spin-flavor precession, etc. LMA MSW has been established as the true solution of the solar neutrino problem. Parameters theta12 and Delta_m21^2 have been measured; theta13 extracted from the solar data is in agreement with results from reactor experiments. Solar neutrino studies provide a sensitive way to test theory of neutrino oscillations and conversion. Characterized by long baseline, huge fluxes and low energies they are a powerful set-up to search for new physics beyond the standard 3nu paradigm: new neutrino states, sterile neutrinos, non-standard neutrino interactions, effects of violation of fundamental symmetries, new dynamics of neutrino propagation, probes of space and time. These searches allow us to get stringent, and in some cases unique bounds on new physics. We summarize the results on physics of propagation, neutrino properties and physics beyond the standard model obtained from studies of solar neutrinos.
Deformable Slice-to-Volume Registration for Motion Correction in Fetal Body MRI<|sep|>In in-utero MRI, motion correction for fetal body and placenta poses a particular challenge due to the presence of local non-rigid transformations of organs caused by bending and stretching. The existing slice-to-volume registration (SVR) reconstruction methods are widely employed for motion correction of fetal brain that undergoes only rigid transformation. However, for reconstruction of fetal body and placenta, rigid registration cannot resolve the issue of misregistrations due to deformable motion, resulting in degradation of features in the reconstructed volume. We propose a Deformable SVR (DSVR), a novel approach for non-rigid motion correction of fetal MRI based on a hierarchical deformable SVR scheme to allow high resolution reconstruction of the fetal body and placenta. Additionally, a robust scheme for structure-based rejection of outliers minimises the impact of registration errors. The improved performance of DSVR in comparison to SVR and patch-to-volume registration (PVR) methods is quantitatively demonstrated in simulated experiments and 20 fetal MRI datasets from 28-31 weeks gestational age (GA) range with varying degree of motion corruption. In addition, we present qualitative evaluation of 100 fetal body cases from 20-34 weeks GA range.
Sketch-based 3D Shape Modeling from Sparse Point Clouds<|sep|>3D modeling based on point clouds is an efficient way to reconstruct and create detailed 3D content. However, the geometric procedure may lose accuracy due to high redundancy and the absence of an explicit structure. In this work, we propose a human-in-the-loop sketch-based point cloud reconstruction framework to leverage users cognitive abilities in geometry extraction. We present an interactive drawing interface for 3D model creation from point cloud data with the help of user sketches. We adopt an optimization method in which the user can continuously edit the contours extracted from the obtained 3D model and retrieve the model iteratively. Finally, we verify the proposed user interface for modeling from sparse point clouds. see video here https://www.youtube.com/watch?v=0H19NyXDRJE .
Locally-Oriented Programming: A Simple Programming Model for Stencil-Based Computations on Multi-Level Distributed Memory Architectures<|sep|>Emerging hybrid accelerator architectures for high performance computing are often suited for the use of a data-parallel programming model. Unfortunately, programmers of these architectures face a steep learning curve that frequently requires learning a new language (e.g., OpenCL). Furthermore, the distributed (and frequently multi-level) nature of the memory organization of clusters of these machines provides an additional level of complexity. This paper presents preliminary work examining how programming with a local orientation can be employed to provide simpler access to accelerator architectures. A locally-oriented programming model is especially useful for the solution of algorithms requiring the application of a stencil or convolution kernel. In this programming model, a programmer codes the algorithm by modifying only a single array element (called the local element), but has read-only access to a small sub-array surrounding the local element. We demonstrate how a locally-oriented programming model can be adopted as a language extension using source-to-source program transformations.
A subspace code of size $333$ in the setting of a binary $q$-analog of the Fano plane<|sep|>We show that there is a binary subspace code of constant dimension 3 in ambient dimension 7, having minimum distance 4 and cardinality 333, i.e., $333 \le A_2(7,4;3)$, which improves the previous best known lower bound of 329. Moreover, if a code with these parameters has at least 333 elements, its automorphism group is in one of $31$ conjugacy classes. This is achieved by a more general technique for an exhaustive search in a finite group that does not depend on the enumeration of all subgroups.
Non-Keplerian effects in precision radial velocity measurements of double-line spectroscopic binary stars: numerical simulations<|sep|>Current precision in radial velocity (RV) measurements of binary stars reaches $\sim$2 ms$^{-1}$. This level of precision means that RV models have to take into account additional non-Keplerian effects such as tidal and rotational distortion of the components of a binary star, relativistic effects and orbital precession. We generate synthetic binaries using Yonsei-Yale stellar models. For typical representatives we investigate the impact of various orbital orientations and different non-Keplerian effects on the RV curves. To this end we simulate RV observations with an added white noise of different scale. Subsequently we try to reconstruct the input orbital parameters and their errors by fitting a model using a standard least-squares method. In particular we investigate the connection between the tidal distortion of the shape of the stars and the best-fit orbital eccentricity, the possibility of deriving orbital inclination of a non-eclipsing binary star by exploiting relativistic effects and the circumstances in which the orbital precession can be detected. We confirm that the method proposed by to obtain orbital inclination with use of the relativistic effect does work in favourable cases and that it can be used even for orbital configurations far from an edge-on orientation. We show that the RV variations imposed by tidally distorted stars can mimic non-zero eccentricity in some binaries. The scale of such an effect depends on the RV accuracy. Finally, we demonstrate that the apsidal precession can be easily detected with precision RVs. In particular we can detect orbital precession of $10^{-4}$ rad yr$^{-1}$, $10^{-3}$ rad yr$^{-1}$ for precision of RVs of 1 ms$^{-1}$ and 10 ms$^{-1}$ respectively.
Protected string spectrum in AdS3/CFT2 from worldsheet integrability<|sep|>We derive the protected closed-string spectra of AdS3/CFT2 dual pairs with 16 supercharges at arbitrary values of the string tension and of the three-form fluxes. These follow immediately from the all-loop Bethe equations for the spectra of the integrable worldsheet theories. Further, representing the underlying integrable systems as spin chains, we find that their dynamics involves length-changing interactions and that protected states correspond to gapless excitations above the Berenstein-Maldacena-Nastase vacuum. In the case of AdS3 x S3 x T4 the degeneracies of such operators precisely match those of the dual CFT2 and the supergravity spectrum. On the other hand, we find that for AdS3 x S3 x S3 x S1 there are fewer protected states than previous supergravity calculations had suggested. In particular, protected states have the same su(2) charge with respect to the two three-spheres.
Origin and evolution of two-component debris discs and an application to the q$^1$ Eridani system<|sep|>Many debris discs reveal a two-component structure, with an outer Kuiper-belt analogue and a warm inner component whose origin is still a matter of debate. One possibility is that warm emission stems from an "asteroid belt" closer in to the star. We consider a scenario in which a set of giant planets is formed in an initially extended planetesimal disc. These planets carve a broad gap around their orbits, splitting up the disc into the outer and the inner belts. After the gas dispersal, both belts undergo collisional evolution in a steady-state regime. This scenario is explored with detailed collisional simulations involving realistic physics to describe a long-term collisional depletion of the two-component disc. We find that the inner disc may be able to retain larger amounts of material at older ages than thought before on the basis of simplified analytic models. We show that the proposed scenario is consistent with a suite of thermal emission and scattered light observational data for a bright two-temperature debris disc around a nearby solar-type star q$^1$ Eridani. This implies a Solar System-like architecture of the system, with an outer massive "Kuiper belt", an inner "asteroid belt", and a few Neptune- to Jupiter-mass planets in between.
Adaptive Surface Normal Constraint for Depth Estimation<|sep|>We present a novel method for single image depth estimation using surface normal constraints. Existing depth estimation methods either suffer from the lack of geometric constraints, or are limited to the difficulty of reliably capturing geometric context, which leads to a bottleneck of depth estimation quality. We therefore introduce a simple yet effective method, named Adaptive Surface Normal (ASN) constraint, to effectively correlate the depth estimation with geometric consistency. Our key idea is to adaptively determine the reliable local geometry from a set of randomly sampled candidates to derive surface normal constraint, for which we measure the consistency of the geometric contextual features. As a result, our method can faithfully reconstruct the 3D geometry and is robust to local shape variations, such as boundaries, sharp corners and noises. We conduct extensive evaluations and comparisons using public datasets. The experimental results demonstrate our method outperforms the state-of-the-art methods and has superior efficiency and robustness.
Few-Shot Anomaly Detection for Polyp Frames from Colonoscopy<|sep|>Anomaly detection methods generally target the learning of a normal image distribution (i.e., inliers showing healthy cases) and during testing, samples relatively far from the learned distribution are classified as anomalies (i.e., outliers showing disease cases). These approaches tend to be sensitive to outliers that lie relatively close to inliers (e.g., a colonoscopy image with a small polyp). In this paper, we address the inappropriate sensitivity to outliers by also learning from inliers. We propose a new few-shot anomaly detection method based on an encoder trained to maximise the mutual information between feature embeddings and normal images, followed by a few-shot score inference network, trained with a large set of inliers and a substantially smaller set of outliers. We evaluate our proposed method on the clinical problem of detecting frames containing polyps from colonoscopy video sequences, where the training set has 13350 normal images (i.e., without polyps) and less than 100 abnormal images (i.e., with polyps). The results of our proposed model on this data set reveal a state-of-the-art detection result, while the performance based on different number of anomaly samples is relatively stable after approximately 40 abnormal training images.
A Tale of Two Fractals: The Hofstadter Butterfly and The Integral Apollonian Gaskets<|sep|>This paper unveils a mapping between a quantum fractal that describes a physical phenomena, and an abstract geometrical fractal. The quantum fractal is the Hofstadter butterfly discovered in 1976 in an iconic condensed matter problem of electrons moving in a two-dimensional lattice in a transverse magnetic field. The geometric fractal is the integer Apollonian gasket characterized in terms of a 300 BC problem of mutually tangent circles. Both of these fractals are made up of integers. In the Hofstadter butterfly, these integers encode the topological quantum numbers of quantum Hall conductivity. In the Apollonian gaskets an infinite number of mutually tangent circles are nested inside each other, where each circle has integer curvature. The mapping between these two fractals reveals a hidden threefold symmetry embedded in the kaleidoscopic images that describe the asymptotic scaling properties of the butterfly. This paper also serves as a mini review of these fractals, emphasizing their hierarchical aspects in terms of Farey fractions.
Generalising Deep Learning MRI Reconstruction across Different Domains<|sep|>We look into robustness of deep learning based MRI reconstruction when tested on unseen contrasts and organs. We then propose to generalise the network by training with large publicly-available natural image datasets with synthesised phase information to achieve high cross-domain reconstruction performance which is competitive with domain-specific training. To explain its generalisation mechanism, we have also analysed patch sets for different training datasets.
InstructableCrowd: Creating IF-THEN Rules for Smartphones via Conversations with the Crowd<|sep|>Natural language interfaces have become a common part of modern digital life. Chatbots utilize text-based conversations to communicate with users; personal assistants on smartphones such as Google Assistant take direct speech commands from their users; and speech-controlled devices such as Amazon Echo use voice as their only input mode. In this paper, we introduce InstructableCrowd, a crowd-powered system that allows users to program their devices via conversation. The user verbally expresses a problem to the system, in which a group of crowd workers collectively respond and program relevant multi-part IF-THEN rules to help the user. The IF-THEN rules generated by InstructableCrowd connect relevant sensor combinations (e.g., location, weather, device acceleration, etc.) to useful effectors (e.g., text messages, device alarms, etc.). Our study showed that non-programmers can use the conversational interface of InstructableCrowd to create IF-THEN rules that have similar quality compared with the rules created manually. InstructableCrowd generally illustrates how users may converse with their devices, not only to trigger simple voice commands, but also to personalize their increasingly powerful and complicated devices.
Refraction-Based Alternative Explanation for: Bending of Light Near a Star, Gravitational Red/Blue Shift and Black-Hole<|sep|>In this research-paper, many of the general-relativity-tests such as bending of light near a star and gravitational red/blue shift are explained without general-relativity & even without Newtonian-approach. The authors first raise questions on the validity of both, the Newtonian and the relativistic approach; and then propose a novel alternative-explanation. The new alternative explanation is based on refraction-phenomenon of optics. Estimation of results with new approach are in agreement with known values. Though physics is different, but it is argued that general-relativity based gravitational-bending and refraction based bending have more in common than is generally realized. Also discussed are black-hole and gravitational-lensing in the new perspective of refraction. The new refraction-based theory makes a few new predictions and also suggests a few tests.
Schwinger mechanism in the SU(3) Nambu--Jona-Lasinio model with an electric field<|sep|>In this work we study the electrized quark matter under finite temperature and density conditions in the context of the SU(2) and SU(3) Nambu--Jona-Lasinio models. To this end, we evaluate the effective quark masses and the Schwinger quark-antiquark pair production rate. For the SU(3) NJL model we incorporate in the Lagrangian the 't Hooft determinant and we present a set of analytical expressions more convenient for numerical evaluations. We predict a decrease of the pseudocritical electric field with the increase of the temperature for both models and a more prominent production rate for the SU(3) model when compared to the SU(2).
Sampling Spatially Correlated Clutter<|sep|>Correlated ${\cal G}$ distributions can be used to describe the clutter seen in images obtained with coherent illumination, as is the case of B-scan ultrasound, laser, sonar and synthetic aperture radar (SAR) imagery. These distributions are derived using the square root of the generalized inverse Gaussian distribution for the amplitude backscatter within the multiplicative model. A two-parameters particular case of the amplitude ${\mathcal G}$ distribution, called ${\mathcal G}_{A}^{0}$, constitutes a modeling improvement with respect to the widespread ${\mathcal K}_{A}$ distribution when fitting urban, forested and deforested areas in remote sensing data. This article deals with the modeling and the simulation of correlated ${\mathcal G}_{A}^{0}$-distributed random fields. It is accomplished by means of the Inverse Transform method, applied to Gaussian random fields with spatial correlation. The main feature of this approach is its generality, since it allows the introduction of negative correlation values in the resulting process, necessary for the proper explanation of the shadowing effect in many SAR images.
Concrete Score Matching: Generalized Score Matching for Discrete Data<|sep|>Representing probability distributions by the gradient of their density functions has proven effective in modeling a wide range of continuous data modalities. However, this representation is not applicable in discrete domains where the gradient is undefined. To this end, we propose an analogous score function called the "Concrete score", a generalization of the (Stein) score for discrete settings. Given a predefined neighborhood structure, the Concrete score of any input is defined by the rate of change of the probabilities with respect to local directional changes of the input. This formulation allows us to recover the (Stein) score in continuous domains when measuring such changes by the Euclidean distance, while using the Manhattan distance leads to our novel score function in discrete domains. Finally, we introduce a new framework to learn such scores from samples called Concrete Score Matching (CSM), and propose an efficient training objective to scale our approach to high dimensions. Empirically, we demonstrate the efficacy of CSM on density estimation tasks on a mixture of synthetic, tabular, and high-dimensional image datasets, and demonstrate that it performs favorably relative to existing baselines for modeling discrete data.
On Affinity Measures for Artificial Immune System Movie Recommenders<|sep|>We combine Artificial Immune Systems 'AIS', technology with Collaborative Filtering 'CF' and use it to build a movie recommendation system. We already know that Artificial Immune Systems work well as movie recommenders from previous work by Cayzer and Aickelin 3, 4, 5. Here our aim is to investigate the effect of different affinity measure algorithms for the AIS. Two different affinity measures, Kendalls Tau and Weighted Kappa, are used to calculate the correlation coefficients for the movie recommender. We compare the results with those published previously and show that Weighted Kappa is more suitable than others for movie problems. We also show that AIS are generally robust movie recommenders and that, as long as a suitable affinity measure is chosen, results are good.
ViP: Video Platform for PyTorch<|sep|>This work presents the Video Platform for PyTorch (ViP), a deep learning-based framework designed to handle and extend to any problem domain based on videos. ViP supports (1) a single unified interface applicable to all video problem domains, (2) quick prototyping of video models, (3) executing large-batch operations with reduced memory consumption, and (4) easy and reproducible experimental setups. ViP's core functionality is built with flexibility and modularity in mind to allow for smooth data flow between different parts of the platform and benchmarking against existing methods. In providing a software platform that supports multiple video-based problem domains, we allow for more cross-pollination of models, ideas and stronger generalization in the video understanding research community.
K-Deep Simplex: Deep Manifold Learning via Local Dictionaries<|sep|>We propose K-Deep Simplex (KDS), a unified optimization framework for nonlinear dimensionality reduction that combines the strengths of manifold learning and sparse dictionary learning. Our approach learns local dictionaries that represent a data point with reconstruction coefficients supported on the probability simplex. The dictionaries are learned using algorithm unrolling, an increasingly popular technique for structured deep learning. KDS enjoys tremendous computational advantages over related approaches and is both interpretable and flexible. In particular, KDS is quasilinear in the number of data points with scaling that depends on intrinsic geometric properties of the data. We apply KDS to the unsupervised clustering problem and prove theoretical performance guarantees. Experiments show that the algorithm is highly efficient and performs competitively on synthetic and real data sets.
Incorporation of charge- and pair-density-wave states into the one-band model of d-wave superconductivity<|sep|>We study the coexistence of pair- (PDW) and charge-density-wave (CDW) states within the single-band $t$-$J$-$U$ and Hubbard models of $d$-$wave$ superconductivity and discuss our results in the context of the experimental observations for the copper-based compounds. In order to take into account the correlation effects with a proper precision, we use the approach based on the diagrammatic expansion of the Gutzwiller wave function (DE-GWF), that goes beyond the renormalized mean field theory (RMFT) in a systematic manner. According to our analysis of the $t$-$J$-$U$ model, the transition between the pure $d$-$wave$ superconducting phase (SC) and the coexistent CDW+PDW phase takes place at $\delta\approx 0.18$ (close to the optimal doping), with the modulated phase located in the underdoped regime. The situation is slightly different for the case of the Hubbard model, where a narrow stability regime of a precursor nematic phase sets in preceding the formation of the modulated CDW+PDW state, with the decreasing hole doping. The results complete our discussion of the standard phase diagram for high-T$_C$ superconducting compounds within the DE-GWF variational approach in the single narrow-band case.
Subspace Thresholding Pursuit: A Reconstruction Algorithm for Compressed Sensing<|sep|>We propose a new iterative greedy algorithm for reconstructions of sparse signals with or without noisy perturbations in compressed sensing. The proposed algorithm, called \emph{subspace thresholding pursuit} (STP) in this paper, is a simple combination of subspace pursuit and iterative hard thresholding. Firstly, STP has the theoretical guarantee comparable to that of $\ell_1$ minimization in terms of restricted isometry property. Secondly, with a tuned parameter, on the one hand, when reconstructing Gaussian signals, it can outperform other state-of-the-art reconstruction algorithms greatly; on the other hand, when reconstructing constant amplitude signals with random signs, it can outperform other state-of-the-art iterative greedy algorithms and even outperform $\ell_1$ minimization if the undersampling ratio is not very large. In addition, we propose a simple but effective method to improve the empirical performance further if the undersampling ratio is large. Finally, it is showed that other iterative greedy algorithms can improve their empirical performance by borrowing the idea of STP.
Beyond just "flattening the curve": Optimal control of epidemics with purely non-pharmaceutical interventions<|sep|>When effective medical treatment and vaccination are not available, non-pharmaceutical interventions such as social distancing, home quarantine and far-reaching shutdown of public life are the only available strategies to prevent the spread of epidemics. Based on an extended SEIR (susceptible-exposed-infectious-recovered) model and continuous-time optimal control theory, we compute the optimal non-pharmaceutical intervention strategy for the case that a vaccine is never found and complete containment (eradication of the epidemic) is impossible. In this case, the optimal control must meet competing requirements: First, the minimization of disease-related deaths, and, second, the establishment of a sufficient degree of natural immunity at the end of the measures, in order to exclude a second wave. Moreover, the socio-economic costs of the intervention shall be kept at a minimum. The numerically computed optimal control strategy is a single-intervention scenario that goes beyond heuristically motivated interventions and simple "flattening of the curve." Careful analysis of the computed control strategy reveals, however, that the obtained solution is in fact a tightrope walk close to the stability boundary of the system, where socio-economic costs and the risk of a new outbreak must be constantly balanced against one another. The model system is calibrated to reproduce the initial exponential growth phase of the COVID-19 pandemic in Germany.
Group-Sparse Signal Denoising: Non-Convex Regularization, Convex Optimization<|sep|>Convex optimization with sparsity-promoting convex regularization is a standard approach for estimating sparse signals in noise. In order to promote sparsity more strongly than convex regularization, it is also standard practice to employ non-convex optimization. In this paper, we take a third approach. We utilize a non-convex regularization term chosen such that the total cost function (consisting of data consistency and regularization terms) is convex. Therefore, sparsity is more strongly promoted than in the standard convex formulation, but without sacrificing the attractive aspects of convex optimization (unique minimum, robust algorithms, etc.). We use this idea to improve the recently developed 'overlapping group shrinkage' (OGS) algorithm for the denoising of group-sparse signals. The algorithm is applied to the problem of speech enhancement with favorable results in terms of both SNR and perceptual quality.
The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization<|sep|>We present a novel benchmark and associated evaluation metrics for assessing the performance of text anonymization methods. Text anonymization, defined as the task of editing a text document to prevent the disclosure of personal information, currently suffers from a shortage of privacy-oriented annotated text resources, making it difficult to properly evaluate the level of privacy protection offered by various anonymization methods. This paper presents TAB (Text Anonymization Benchmark), a new, open-source annotated corpus developed to address this shortage. The corpus comprises 1,268 English-language court cases from the European Court of Human Rights (ECHR) enriched with comprehensive annotations about the personal information appearing in each document, including their semantic category, identifier type, confidential attributes, and co-reference relations. Compared to previous work, the TAB corpus is designed to go beyond traditional de-identification (which is limited to the detection of predefined semantic categories), and explicitly marks which text spans ought to be masked in order to conceal the identity of the person to be protected. Along with presenting the corpus and its annotation layers, we also propose a set of evaluation metrics that are specifically tailored towards measuring the performance of text anonymization, both in terms of privacy protection and utility preservation. We illustrate the use of the benchmark and the proposed metrics by assessing the empirical performance of several baseline text anonymization models. The full corpus along with its privacy-oriented annotation guidelines, evaluation scripts and baseline models are available on: https://github.com/NorskRegnesentral/text-anonymisation-benchmark
Mass-loss histories of Type IIn supernova progenitors within decades before their explosion<|sep|>We present results of a systematic study of the mass-loss properties of Type IIn supernova progenitors within decades before their explosion. We apply an analytic light curve model to 11 Type IIn supernova bolometric light curves to derive the circumstellar medium properties. We reconstruct the mass-loss histories based on the estimated circumstellar medium properties. The estimated mass-loss rates are mostly higher than 1e-3 Msun/yr and they are consistent with those obtained by other methods. The mass-loss rates are often found to be constantly high within decades before their explosion. This indicates that there exists some mechanism to sustain the high mass-loss rates of Type IIn supernova progenitors for at least decades before their explosion. Thus, the shorter eruptive mass loss events observed in some Type IIn supernova progenitors are not always responsible for creating their dense circumstellar media. In addition, we find that Type IIn supernova progenitors may tend to increase their mass-loss rates as they approach to the time of their explosion. We also show a detailed comparison between our analytic prediction and numerical results.
The VLA Nascent Disk And Multiplicity Survey of Perseus Protostars (VANDAM). III. Extended Radio Emission from Protostars in Perseus<|sep|>Centimeter continuum emission from protostars offers insight into the innermost part of the outflows, as shock-ionized gas produces free-free emission. We observed a complete population of Class 0 and I protostars in the Perseus molecular cloud at 4.1 cm and 6.4 cm with resolution and sensitivity superior to previous surveys. From a total of 71 detections, 8 sources exhibit resolved emission at 4.1 cm and/or 6.4 cm. In this paper we focus on this sub-sample, analyzing their spectral indices along the jet, and their alignment with respect to the large-scale molecular outflow. Spectral indices for fluxes integrated toward the position of the protostar are consistent with free-free thermal emission. The value of the spectral index along a radio jet decreases with distance from the protostar. For six sources, emission is well aligned with the outflow central axis, showing that we observe the ionized base of the jet. This is not the case for two sources, where we note misalignment of the emission with respect to the large-scale outflow. This might indicate that the emission does not originate in the radio jet, but rather in an ionized outflow cavity wall or disk surface. For five of the sources, the spectral indices along the jet decrease well below the thermal free-free limit of -0.1 with $>2\sigma$ significance. This is indicative of synchrotron emission, meaning that high energy electrons are being produced in the outflows close to the disk. This result can have far-reaching implications for the chemical composition of the embedded disks.
Estimation and simulation of the transaction arrival process in intraday electricity markets<|sep|>We examine the novel problem of the estimation of transaction arrival processes in the intraday electricity markets. We model the inter-arrivals using multiple time-varying parametric densities based on the generalized F distribution estimated by maximum likelihood. We analyse both the in-sample characteristics and the probabilistic forecasting performance. In a rolling window forecasting study, we simulate many trajectories to evaluate the forecasts and gain significant insights into the model fit. The prediction accuracy is evaluated by a functional version of the MAE (mean absolute error), RMSE (root mean squared error) and CRPS (continuous ranked probability score) for the simulated count processes. This paper fills the gap in the literature regarding the intensity estimation of transaction arrivals and is a major contribution to the topic, yet leaves much of the field for further development. The study presented in this paper is conducted based on the German Intraday Continuous electricity market data, but this method can be easily applied to any other continuous intraday electricity market. For the German market, a specific generalized gamma distribution setup explains the overall behaviour significantly best, especially as the tail behaviour of the process is well covered.
Self organized mode locking effect in superconductor / ferromagnet hybrids<|sep|>The vortex dynamics in a low temperature superconductor deposited on top of a rectangular array of micrometer size permalloy triangles is investigated experimentally. The rectangular unit cell is such that neighboring triangles physically touch each other along one direction. This design stabilizes remanent states which differ from the magnetic vortex state typical of individual non-interacting triangles. Magnetic Force Microscopy images have revealed that the magnetic landscape of the template can be switched to an ordered configuration after magnetizing the sample with an in-plane field. The ordered phase exhibits a broad flux flow regime with relatively low critical current and a highly anisotropic response. This behavior is caused by the spontaneous formation of two separated rows of vortices and antivortices along each line of connected triangles. The existence of a clear flux flow regime even for zero external field supports this interpretation. The density of induced vortex-antivortex pairs is directly obtained using a high frequency measurement technique which allows us to resolve the discrete motion of vortices. Strikingly, the presence of vortex-antivortex rows gives rise to a self organized synchronized motion of vortices which manifests itself as field independent Shapiro steps in the current-voltage characteristics.
Spatially-Coupled MacKay-Neal Codes and Hsu-Anastasopoulos Codes<|sep|>Kudekar et al. recently proved that for transmission over the binary erasure channel (BEC), spatial coupling of LDPC codes increases the BP threshold of the coupled ensemble to the MAP threshold of the underlying LDPC codes. One major drawback of the capacity-achieving spatially-coupled LDPC codes is that one needs to increase the column and row weight of parity-check matrices of the underlying LDPC codes. It is proved, that Hsu-Anastasopoulos (HA) codes and MacKay-Neal (MN) codes achieve the capacity of memoryless binary-input symmetric-output channels under MAP decoding with bounded column and row weight of the parity-check matrices. The HA codes and the MN codes are dual codes each other. The aim of this paper is to present an empirical evidence that spatially-coupled MN (resp. HA) codes with bounded column and row weight achieve the capacity of the BEC. To this end, we introduce a spatial coupling scheme of MN (resp. HA) codes. By density evolution analysis, we will show that the resulting spatially-coupled MN (resp. HA) codes have the BP threshold close to the Shannon limit.
Neural Synchronization and Cryptography<|sep|>Neural networks can synchronize by learning from each other. In the case of discrete weights full synchronization is achieved in a finite number of steps. Additional networks can be trained by using the inputs and outputs generated during this process as examples. Several learning rules for both tasks are presented and analyzed. In the case of Tree Parity Machines synchronization is much faster than learning. Scaling laws for the number of steps needed for full synchronization and successful learning are derived using analytical models. They indicate that the difference between both processes can be controlled by changing the synaptic depth. In the case of bidirectional interaction the synchronization time increases proportional to the square of this parameter, but it grows exponentially, if information is transmitted in one direction only. Because of this effect neural synchronization can be used to construct a cryptographic key-exchange protocol. Here the partners benefit from mutual interaction, so that a passive attacker is usually unable to learn the generated key in time. The success probabilities of different attack methods are determined by numerical simulations and scaling laws are derived from the data. They show that the partners can reach any desired level of security by just increasing the synaptic depth. Then the complexity of a successful attack grows exponentially, but there is only a polynomial increase of the effort needed to generate a key. Further improvements of security are possible by replacing the random inputs with queries generated by the partners.
Estimating the standard error of cross-Validation-Based estimators of classifier performance<|sep|>First, we analyze the variance of the Cross Validation (CV)-based estimators used for estimating the performance of classification rules. Second, we propose a novel estimator to estimate this variance using the Influence Function (IF) approach that had been used previously very successfully to estimate the variance of the bootstrap-based estimators. The motivation for this research is that, as the best of our knowledge, the literature lacks a rigorous method for estimating the variance of the CV-based estimators. What is available is a set of ad-hoc procedures that have no mathematical foundation since they ignore the covariance structure among dependent random variables. The conducted experiments show that the IF proposed method has small RMS error with some bias. However, surprisingly, the ad-hoc methods still work better than the IF-based method. Unfortunately, this is due to the lack of enough smoothness if compared to the bootstrap estimator. This opens the research for three points: (1) more comprehensive simulation study to clarify when the IF method win or loose; (2) more mathematical analysis to figure out why the ad-hoc methods work well; and (3) more mathematical treatment to figure out the connection between the appropriate amount of "smoothness" and decreasing the bias of the IF method.
Object Detection Through Exploration With A Foveated Visual Field<|sep|>We present a foveated object detector (FOD) as a biologically-inspired alternative to the sliding window (SW) approach which is the dominant method of search in computer vision object detection. Similar to the human visual system, the FOD has higher resolution at the fovea and lower resolution at the visual periphery. Consequently, more computational resources are allocated at the fovea and relatively fewer at the periphery. The FOD processes the entire scene, uses retino-specific object detection classifiers to guide eye movements, aligns its fovea with regions of interest in the input image and integrates observations across multiple fixations. Our approach combines modern object detectors from computer vision with a recent model of peripheral pooling regions found at the V1 layer of the human visual system. We assessed various eye movement strategies on the PASCAL VOC 2007 dataset and show that the FOD performs on par with the SW detector while bringing significant computational cost savings.
Variational Inference for Sparse and Undirected Models<|sep|>Undirected graphical models are applied in genomics, protein structure prediction, and neuroscience to identify sparse interactions that underlie discrete data. Although Bayesian methods for inference would be favorable in these contexts, they are rarely used because they require doubly intractable Monte Carlo sampling. Here, we develop a framework for scalable Bayesian inference of discrete undirected models based on two new methods. The first is Persistent VI, an algorithm for variational inference of discrete undirected models that avoids doubly intractable MCMC and approximations of the partition function. The second is Fadeout, a reparameterization approach for variational inference under sparsity-inducing priors that captures a posteriori correlations between parameters and hyperparameters with noncentered parameterizations. We find that, together, these methods for variational inference substantially improve learning of sparse undirected graphical models in simulated and real problems from physics and biology.
Assimilation of wall-pressure measurements in high-speed flow over a cone<|sep|>A nonlinear ensemble-variational (EnVar) data assimilation is performed in order to estimate the unknown flow field over a slender cone at Mach-6, from isolated wall-pressure measurements. The cost functional accounts for discrepancies in wall-pressure spectra and total intensity between the experiment and the prediction using direct numerical simulations (DNS), as well as our relative confidence in the measurements and the estimated state. We demonstrate the robustness of the predicted flow by direct propagation of posterior statistics. The approach provides a unique first look at the flow beyond the sensor data, and rigorously accounts for the role of nonlinearity unlike previous efforts that adopted ad-hoc inflow syntheses. Away from the wall, two- and three-dimensional assimilated states both show rope-like structures, qualitatively similar to independent schlieren visualizations. Despite this resemblance, and even though the planar second modes are the most unstable upstream, three-dimensional (3D) waves must be included in the assimilation in order to accurately reproduce the wall-pressure measurements recorded in the Ludwieg-Tube facility. The results highlight the importance of three-dimensionality of the field and of the base-state distortion on the instability waves in this experiment, and motivate future measurements that probe the 3D nature of the flow field.
Discovery of Crystallized Water Ice in a Silhouette Disk in the M43 Region<|sep|>We present the 1.9--4.2um spectra of the five bright (L<11.2) young stars associated with silhouette disks with moderate to high inclination angle of 39--80deg in the M42 and M43 regions. The water ice absorption is seen toward d121-1925 and d216-0939, while the spectra of d182-316, d183-405, and d218-354 show no water ice feature around 3.1um within the detection limits. By comparing the water ice features toward nearby stars, we find that the water ice absorption toward d121-1925 and d216-0939 most likely originates from the foreground material and the surrounding disk, respectively. The angle of the disk inclination is found to be mainly responsible for the difference of the optical depth of the water ice among the five young stars. Our results suggest that there is a critical inclination angle between 65deg and 75deg for the circumstellar disk where the water ice absorption becomes strong. The average density at the disk surface of d216-0939 was found to be 6.38x10^(-18) g cm^(-3). The water ice absorption band in the d216-0939 disk is remarkable in that the maximum optical depth of the water ice band is at a longer wavelength than detected before. It indicates that the primary carrier of the feature is purely crystallized water ice at the surface of the d216-0939 disk with characteristic size of ~0.8um, which suggests grain growth. This is the first direct detection of purely crystallized water ice in a silhouette disk.
Dynamic Power Splitting Policies for AF Relay Networks with Wireless Energy Harvesting<|sep|>Wireless energy harvesting (WEH) provides an exciting way to supply energy for relay nodes to forward information for the source-destination pairs. In this paper, we investigate the problem on how the relay node dynamically adjusts the power splitting ratio of information transmission (IT) and energy harvesting (EH) in order to achieve the optimal outage performance. According to the knowledge of channel state information (CSI) at the relay, optimal dynamic power splitting policy with full CSI and partial CSI are both provided. Finally, through simulations, the proposed power splitting policies can improve the outage performances and the policy with full CSI achieves the best performance. It is also shown that the policy with partial CSI can approach the policy with full CSI closely and incurs far less system overhead.
Cache Discovery Policies of MANET<|sep|>In situations where establishing a network infrastructure is impossible, Ad-hoc networks are considered particularly important. Most of the previous research in Ad-hoc networks concentrated on the development and enhancement of dynamic routing protocols, which could efficiently discover routes between two communicating nodes. Although routing strategies is an important topic in MANETs, other topics such as data access are also crucial since the final goal of using Ad-hoc networks is to provide data access to mobile nodes. One of the most attractive techniques used to improve the data access performance in MANET environment is cooperative caching; which means multiple caching nodes share and cooperatively manage the cached contents. It is lead the research to important questions, what data should be cached, where, when, and how? A cooperative caching addressed into two basic issues: cache discovery and cache management, in other words, how to find requested data efficiently and how to manage an individual cache to improve the overall capacity of a cooperated cache. In this paper we have made a review of the existing cache discovery algorithms to address four stages after application request and before server response, using an historical file to record the previous data requests, and proposed cluster architecture with data cluster head election to store efficient information for future use and reducing the cost of flooding. In addition, this paper suggests some alternative techniques for cache discovery. Finally, the paper concludes with a discussion on future research directions.
Longitudinal dielectric permeability of the quantum degenerate collisional plasmas<|sep|>Dielectric permeability of the degenerate electronic gas for the collisional plasmas is found. The kinetic equation of Wigner -- Vlasov -- Boltzmann with integral of collisions in relaxation form in coordinate space is used. We will notice that dielectric permeability with using of the relaxation equation in the momentum space has been received by Mermin.
Virtual Windshields: Merging Reality and Digital Content to Improve the Driving Experience<|sep|>In recent years, the use of the automobile as the primary mode of transportation has been increasing and driving has become an important part of daily life. Driving is a multi-sensory experience as drivers rely on their senses to provide them with important information. In a vehicular context human senses are all too often limited and obstructed. Today, road accidents constitute the eighth leading cause of death. The escalation of technology has propelled new ways in which driver's senses may be augmented. The enclosed aspect of a car, allied with the configuration of the controls and displays directed towards the driver, offer significant advantages for augmented reality (AR) systems when considering the amount of immersion it can provide to the user. In addition, the inherent mobility and virtually unlimited power autonomy transform cars into perfect mobile computing platforms. However, automobiles currently present limited network connectivity and thus the created augmented objects are merely providing information captured by in-vehicle sensors, cameras and other databases. By combining the new paradigm of Vehicular Ad Hoc Networking (VANET) with AR human machine interfaces, we show that it is possible to design novel cooperative Advanced Driver Assistance Systems (ADAS), that base the creation of AR content on the information collected from neighbouring vehicles or roadside infrastructures. As such we implement prototypes of both visual and acoustic AR systems, which can significantly improve the driving experience. We believe our results contribute to the formulation of a vision where the vehicle is perceived as an extension of the body which permeates the human senses to the world outside the vessel, where the car is used as a better, multi-sensory immersive version of a mobile phone that integrates touch, vision and sound enhancements, leveraging unique properties of VANET.
Two-photon IR pumped UV-Vis transient absorption spectroscopy of Dirac fermions in the 2D and 3D topological insulator Bi2Se3<|sep|>It is often taken for granted that in pump-probe experiments on the topological insulator (TI) Bi2Se3 using IR pumping with a commercial Ti:Sapphire laser [~800 nm (1.55 eV photon energy)], the electrons are excited in the one-photon absorption regime, even when pumped with absorbed fluences in the mJ cm-2 range. Here, using UV-Vis transient absorption (TA) spectroscopy, we show that even at low-power IR pumping with absorbed fluences in the mkJ cm-2 range, the TA spectra of the TI Bi2Se3 extend across a part of the UV and the entire visible region. This observation suggests unambiguously that the two-photon pumping regime accompanies the usual one-photon pumping regime even at low laser powers applied. We attribute the high efficiency of two-photon pumping to the giant nonlinearity of Dirac fermions in the Dirac surface states (SS). On the contrary, one-photon pumping is associated with the excitation of bound valence electrons in the bulk into the conduction band. Two mechanisms of absorption bleaching were also revealed since they manifest themselves in different spectral regions of probing. These two mechanisms were assigned to the filling of the phase-space in the Dirac SS and bulk states and the corresponding Pauli blocking.
Understanding Coarsening for Embedding Large-Scale Graphs<|sep|>A significant portion of the data today, e.g, social networks, web connections, etc., can be modeled by graphs. A proper analysis of graphs with Machine Learning (ML) algorithms has the potential to yield far-reaching insights into many areas of research and industry. However, the irregular structure of graph data constitutes an obstacle for running ML tasks on graphs such as link prediction, node classification, and anomaly detection. Graph embedding is a compute-intensive process of representing graphs as a set of vectors in a d-dimensional space, which in turn makes it amenable to ML tasks. Many approaches have been proposed in the literature to improve the performance of graph embedding, e.g., using distributed algorithms, accelerators, and pre-processing techniques. Graph coarsening, which can be considered a pre-processing step, is a structural approximation of a given, large graph with a smaller one. As the literature suggests, the cost of embedding significantly decreases when coarsening is employed. In this work, we thoroughly analyze the impact of the coarsening quality on the embedding performance both in terms of speed and accuracy. Our experiments with a state-of-the-art, fast graph embedding tool show that there is an interplay between the coarsening decisions taken and the embedding quality.
General formulation of Luria-Delbr{\"u}ck distribution of the number of mutants<|sep|>The Luria-Delbr{\"u}ck experiment is a cornerstone of evolutionary theory, demonstrating the randomness of mutations before selection. The distribution of the number of mutants in this experiment has been the subject of intense investigation during the last 70 years. Despite this considerable effort, most of the results have been obtained under the assumption of constant growth rate, which is far from the experimental condition. We derive here the properties of this distribution for arbitrary growth function, for both the deterministic and stochastic growth of the mutants. The derivation we propose uses the number of wild type bacteria as the independent variable instead of time. The derivation is surprisingly simple and versatile, allowing many generalizations to be taken easily into account.
Analytic computation of the secular effects of encounters on a binary: features arising from second-order perturbation theory<|sep|>Binary-single interactions play a crucial role in the evolution of dense stellar systems such as globular clusters. In addition, they are believed to drive black hole (BH) binary mergers in these systems. A subset of binary-single interactions are secular encounters, for which the third body approaches the binary on a relatively wide orbit, and such that it is justified to average the equations of motion over the binary's orbital phase. Previous works used first-order perturbation theory to compute the effects of such secular encounters on the binary. However, this approach can break down for highly eccentric binaries, which are important for BH binary mergers and gravitational wave sources. Here, we present an analytic computation using second-order perturbation techniques, valid to the quadrupole-order approximation. In our calculation, we take into account the instantaneous back-reaction of the binary to the third body, and compute corrections to previous first-order results. Using singly-averaged and direct 3-body integrations, we demonstrate the validity of our expressions. In particular, we show that the eccentricity change for highly eccentric binaries can reach a plateau, associated with a large inclination change, and can even reverse sign. These effects are not captured by previous first-order results. We provide a simple script to conveniently evaluate our analytic expressions, including routines for numerical integration and verification.
Construction of type II blow-up solutions for the energy-critical wave equation in dimension 5<|sep|>We consider the semilinear wave equation with focusing energy-critical nonlinearity in space dimension 5 with radial data. It is known that a solution $(u, \partial_t u)$ which blows up at $t = 0$ in a neighborhood (in the energy norm) of the family of solitons $W_\lambda$, asymptotically decomposes in the energy space as a sum of a bubble $W_\lambda$ and an asymptotic profile $(u_0^*, u_1^*)$, where $\lim_{t\to 0}\lambda(t)/t = 0$ and $(u^*_0, u^*_1) \in \dot H^1\times L^2$. We construct a blow-up solution of this type such that $(u^*_0, u^*_1)$ is any pair of sufficiently regular functions with $u_0^*(0) > 0$. For these solutions the concentration rate is $\lambda(t) \sim t^4$. We also provide examples of solutions with concentration rate $\lambda(t) \sim t^{\nu + 1}$ for $\nu > 8$, related to the behaviour of the asymptotic profile near the origin.
Euclidean Dynamical Triangulation revisited: is the phase transition really first order?<|sep|>The transition between the two phases of 4D Euclidean Dynamical Triangulation [1] was long believed to be of second order until in 1996 first order behavior was found for sufficiently large systems [3,4]. However, one may wonder if this finding was affected by the numerical methods used: to control volume fluctuations, in both studies [3,4] an artificial harmonic potential was added to the action; in [4] measurements were taken after a fixed number of accepted instead of attempted moves which introduces an additional error. Finally the simulations suffer from strong critical slowing down which may have been underestimated. In the present work, we address the above weaknesses: we allow the volume to fluctuate freely within a fixed interval; we take measurements after a fixed number of attempted moves; and we overcome critical slowing down by using an optimized parallel tempering algorithm [6]. With these improved methods, on systems of size up to 64k 4-simplices, we confirm that the phase transition is first order.
Out-of-Distribution Detection for Automotive Perception<|sep|>Neural networks (NNs) are widely used for object classification in autonomous driving. However, NNs can fail on input data not well represented by the training dataset, known as out-of-distribution (OOD) data. A mechanism to detect OOD samples is important for safety-critical applications, such as automotive perception, to trigger a safe fallback mode. NNs often rely on softmax normalization for confidence estimation, which can lead to high confidences being assigned to OOD samples, thus hindering the detection of failures. This paper presents a method for determining whether inputs are OOD, which does not require OOD data during training and does not increase the computational cost of inference. The latter property is especially important in automotive applications with limited computational resources and real-time constraints. Our proposed approach outperforms state-of-the-art methods on real-world automotive datasets.
How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild<|sep|>Successful active speaker detection requires a three-stage pipeline: (i) audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation modeling between a reference speaker and the background speakers within each frame, and (iii) temporal modeling for the reference speaker. Each stage of this pipeline plays an important role for the final performance of the created architecture. Based on a series of controlled experiments, this work presents several practical guidelines for audio-visual active speaker detection. Correspondingly, we present a new architecture called ASDNet, which achieves a new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5% outperforming the second best with a large margin of 4.7%. Our code and pretrained models are publicly available.
Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection<|sep|>Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.
Exact Free Energies of Statistical Systems on Random Networks<|sep|>Statistical systems on random networks can be formulated in terms of partition functions expressed with integrals by regarding Feynman diagrams as random networks. We consider the cases of random networks with bounded but generic degrees of vertices, and show that the free energies can be exactly evaluated in the thermodynamic limit by the Laplace method, and that the exact expressions can in principle be obtained by solving polynomial equations for mean fields. As demonstrations, we apply our method to the ferromagnetic Ising models on random networks. The free energy of the ferromagnetic Ising model on random networks with trivalent vertices is shown to exactly reproduce that of the ferromagnetic Ising model on the Bethe lattice. We also consider the cases with heterogeneity with mixtures of orders of vertices, and derive the known formula of the Curie temperature.
Redshift Evolution of the Black Hole Merger Rate from Globular Clusters<|sep|>As the sensitivity of current and future gravitational-wave detectors improves, it will become possible to measure the evolution of the binary black hole merger rate with redshift. Here, we combine detailed fits to state-of-the-art dynamical models of binary black hole formation in dense star clusters with a cosmological model of cluster formation across cosmic time. We find a typical merger rate of 14 $\rm{Gpc}^{-3} \rm{yr}^{-1}$ in the local universe, with a reasonable range of 4-18 $\rm{Gpc}^{-3} \rm{yr}^{-1}$, depending on the rate of cluster disruption and the cluster initial mass function. This rate increases by a factor of 6 to redshift $z=2.7$ before declining at higher redshifts. We compare the merger rate from binaries produced in clusters to similar estimates from isolated binaries and triples in galactic fields, and discuss various ways that these different formation channels could add up to the current merger rate observed by LIGO/Virgo.
Logic programs with propositional connectives and aggregates<|sep|>Answer set programming (ASP) is a logic programming paradigm that can be used to solve complex combinatorial search problems. Aggregates are an ASP construct that plays an important role in many applications. Defining a satisfactory semantics of aggregates turned out to be a difficult problem, and in this paper we propose a new approach, based on an analogy between aggregates and propositional connectives. First, we extend the definition of an answer set/stable model to cover arbitrary propositional theories; then we define aggregates on top of them both as primitive constructs and as abbreviations for formulas. Our definition of an aggregate combines expressiveness and simplicity, and it inherits many theorems about programs with nested expressions, such as theorems about strong equivalence and splitting.
The Friedmann cosmological models revisited as an harmonic motion and new exact solutions<|sep|>A new approach for arbitrary dimension to the Friedmann cosmological models is presented. Taking suitable changes of the parameters of the spacetime the harmonic motion equations appear, where the curvature determines the angular frequency. Some physical interpretations are also explained. As a consequence of our approach, we give new exact solutions to the Einstein's equation when the observer is not comoving with the perfect fluid, as well as, when the pressure not vanishes, including a new procedure to acquire exact solutions for cases of universes at the dark energy dominated stage.
Accelerating Discrete Wavelet Transforms on Parallel Architectures<|sep|>The 2-D discrete wavelet transform (DWT) can be found in the heart of many image-processing algorithms. Until recently, several studies have compared the performance of such transform on various shared-memory parallel architectures, especially on graphics processing units (GPUs). All these studies, however, considered only separable calculation schemes. We show that corresponding separable parts can be merged into non-separable units, which halves the number of steps. In addition, we introduce an optional optimization approach leading to a reduction in the number of arithmetic operations. The discussed schemes were adapted on the OpenCL framework and pixel shaders, and then evaluated using GPUs of two biggest vendors. We demonstrate the performance of the proposed non-separable methods by comparison with existing separable schemes. The non-separable schemes outperform their separable counterparts on numerous setups, especially considering the pixel shaders.
Rheology finds distinct glass and jamming transitions in emulsions<|sep|>We study the rheology of monodisperse and bidisperse emulsions with various droplet sizes (1 $\mu$m -- 2 $\mu$m diameter). Above a critical volume fraction $\phi_c$, these systems exhibit solid-like behavior and a yield stress can be detected. Previous experiments suggest that for small thermal particles, rheology will see a glass transition at $\phi_c = \phi_g =0.58$; for large athermal systems, rheology will see a jamming transition at $\phi_c = \phi_J =0.64$. However, simulations point out that at the crossover of thermal and athermal regimes, the glass and jamming transitions may both be observed in the same sample. Here we conduct an experiment by shearing four oil-in-water emulsions with a rheometer. We observe both a glass and a jamming transition for our smaller diameter droplets, and only a jamming transition for our larger diameter droplets. The bidisperse sample behaves similarly to the small droplet sample, with two transitions observed. Our rheology data are well-fit by both the Herschel-Bulkley model and the Three Component model. Based on the fitting parameters, our raw rheological data would not collapse onto a master curve. Our results suggest that liquid-solid transitions in dispersions may not be universal, but depend on particle type.
Distributed Evaluation of Graph Queries using Recursive Relational Algebra<|sep|>We present a system called Dist-$\mu$-RA for the distributed evaluation of recursive graph queries. Dist-$\mu$-RA builds on the recursive relational algebra and extends it with evaluation plans suited for the distributed setting. The goal is to offer expressivity for high-level queries while providing efficiency at scale and reducing communication costs. Experimental results on both real and synthetic graphs show the effectiveness of the proposed approach compared to existing systems.
Deep CNNs for large scale species classification<|sep|>Large Scale image classification is a challenging problem within the field of computer vision. As the real world contains billions of different objects, understanding the performance of popular techniques and models is vital in order to apply them to real world tasks. In this paper, we evaluate techniques and popular CNN based deep learning architectures to perform large scale species classification on the dataset from iNaturalist 2019 Challenge. Methods utilizing dataset pruning and transfer learning are shown to outperform models trained without either of the two techniques. The ResNext based classifier outperforms other model architectures over 10 epochs and achieves a top-one validation error of 0.68 when classifying amongst the 1,010 species.
Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation<|sep|>Topic taxonomies display hierarchical topic structures of a text corpus and provide topical knowledge to enhance various NLP applications. To dynamically incorporate new topic information, several recent studies have tried to expand (or complete) a topic taxonomy by inserting emerging topics identified in a set of new documents. However, existing methods focus only on frequent terms in documents and the local topic-subtopic relations in a taxonomy, which leads to limited topic term coverage and fails to model the global topic hierarchy. In this work, we propose a novel framework for topic taxonomy expansion, named TopicExpan, which directly generates topic-related terms belonging to new topics. Specifically, TopicExpan leverages the hierarchical relation structure surrounding a new topic and the textual content of an input document for topic term generation. This approach encourages newly-inserted topics to further cover important but less frequent terms as well as to keep their relation consistency within the taxonomy. Experimental results on two real-world text corpora show that TopicExpan significantly outperforms other baseline methods in terms of the quality of output taxonomies.
Online Multi-Cell Coordinated MIMO Wireless Network Virtualization with Imperfect CSI<|sep|>We consider online coordinated precoding design for downlink wireless network virtualization (WNV) in a multi-cell multiple-input multiple-output (MIMO) network with imperfect channel state information (CSI). In our WNV framework, an infrastructure provider (InP) owns each base station that is shared by several service providers (SPs) oblivious of each other. The SPs design their precoders as virtualization demands for user services, while the InP designs the actual precoding solution to meet the service demands from the SPs. Our aim is to minimize the long-term time-averaged expected precoding deviation over MIMO fading channels, subject to both per-cell long-term and short-term transmit power limits. We propose an online coordinated precoding algorithm for virtualization, which provides a fully distributed semi-closed-form precoding solution at each cell, based only on the current imperfect CSI without any CSI exchange across cells. Taking into account the two-fold impact of imperfect CSI on both the InP and the SPs, we show that our proposed algorithm is within an $O(\delta)$ gap from the optimum over any time horizon, where $\delta$ is a CSI inaccuracy indicator. Simulation results validate the performance of our proposed algorithm under two commonly used precoding techniques in a typical urban micro-cell network environment.
The compositional and evolutionary logic of metabolism<|sep|>Metabolism displays striking and robust regularities in the forms of modularity and hierarchy, whose composition may be compactly described. This renders metabolic architecture comprehensible as a system, and suggests the order in which layers of that system emerged. Metabolism also serves as the foundation in other hierarchies, at least up to cellular integration including bioenergetics and molecular replication, and trophic ecology. The recapitulation of patterns first seen in metabolism, in these higher levels, suggests metabolism as a source of causation or constraint on many forms of organization in the biosphere. We identify as modules widely reused subsets of chemicals, reactions, or functions, each with a conserved internal structure. At the small molecule substrate level, module boundaries are generally associated with the most complex reaction mechanisms and the most conserved enzymes. Cofactors form a structurally and functionally distinctive control layer over the small-molecule substrate. Complex cofactors are often used at module boundaries of the substrate level, while simpler ones participate in widely used reactions. Cofactor functions thus act as "keys" that incorporate classes of organic reactions within biochemistry. The same modules that organize the compositional diversity of metabolism are argued to have governed long-term evolution. Early evolution of core metabolism, especially carbon-fixation, appears to have required few innovations among a small number of conserved modules, to produce adaptations to simple biogeochemical changes of environment. We demonstrate these features of metabolism at several levels of hierarchy, beginning with the small-molecule substrate and network architecture, continuing with cofactors and key conserved reactions, and culminating in the aggregation of multiple diverse physical and biochemical processes in cells.
Using H(z) data as a probe of the concordance model<|sep|>Direct observations of the Hubble rate, from cosmic chronometers and the radial baryon acoustic oscillation scale, can out-perform supernovae observations in understanding the expansion history, because supernovae observations need to be differentiated to extract H(z). We use existing H(z) data and smooth the data using a new Gaussian Processes package, GaPP, from which we can also estimate derivatives. The obtained Hubble rate and its derivatives are used to reconstruct the equation of state of dark energy and to perform consistency tests of the LCDM model, some of which are newly devised here. Current data is consistent with the concordance model, but is rather sparse. Future observations will provide a dramatic improvement in our ability to constrain or refute the concordance model of cosmology. We produce simulated data to illustrate how effective H(z) data will be in combination with Gaussian Processes.
Learning Model-Based Vehicle-Relocation Decisions for Real-Time Ride-Sharing: Hybridizing Learning and Optimization<|sep|>Large-scale ride-sharing systems combine real-time dispatching and routing optimization over a rolling time horizon with a model predictive control (MPC) component that relocates idle vehicles to anticipate the demand. The MPC optimization operates over a longer time horizon to compensate for the inherent myopic nature of the real-time dispatching. These longer time horizons are beneficial for the quality of relocation decisions but increase computational complexity. Consequently, the ride-sharing operators are often forced to use a relatively short time horizon. To address this computational challenge, this paper proposes a hybrid approach that combines machine learning and optimization. The machine-learning component learns the optimal solution to the MPC on the aggregated level to overcome the sparsity and high-dimensionality of the solution. The optimization component transforms the machine-learning prediction back to the original granularity through a tractable transportation model. As a consequence, the original NP-hard MPC problem is reduced to a polynomial time prediction and optimization, which allows the ride-sharing operators to consider a longer time horizon. Experimental results show that the hybrid approach achieves significantly better service quality than the MPC optimization in terms of average rider waiting time, due to its ability to model a longer horizon.
BriNet: Towards Bridging the Intra-class and Inter-class Gaps in One-Shot Segmentation<|sep|>Few-shot segmentation focuses on the generalization of models to segment unseen object instances with limited training samples. Although tremendous improvements have been achieved, existing methods are still constrained by two factors. (1) The information interaction between query and support images is not adequate, leaving intra-class gap. (2) The object categories at the training and inference stages have no overlap, leaving the inter-class gap. Thus, we propose a framework, BriNet, to bridge these gaps. First, more information interactions are encouraged between the extracted features of the query and support images, i.e., using an Information Exchange Module to emphasize the common objects. Furthermore, to precisely localize the query objects, we design a multi-path fine-grained strategy which is able to make better use of the support feature representations. Second, a new online refinement strategy is proposed to help the trained model adapt to unseen classes, achieved by switching the roles of the query and the support images at the inference stage. The effectiveness of our framework is demonstrated by experimental results, which outperforms other competitive methods and leads to a new state-of-the-art on both PASCAL VOC and MSCOCO dataset.
Hybrid functional study of non-linear elasticity and internal strain in zincblende III-V materials<|sep|>We investigate the elastic properties of selected zincblende III-V semiconductors. Using hybrid functional density functional theory we calculate the second and third order elastic constants, and first and second-order internal strain tensor components for Ga, In and Al containing III-V compounds. For many of these parameters, there are no available experimental measurements, and this work is the first to predict their values. The stricter convergence criteria for the calculation of higher order elastic constants are demonstrated, and arguments are made based on this for extracting these constants via the calculated stresses, rather than the energies, in the context of plane-wave-based calculations. The calculated elastic properties are used to determine the strain regime at which higher order elasticity becomes important by comparing the stresses predicted by a lower and a higher order elasticity theory. Finally, the results are compared with available experimental literature data and previous theory.
Benchmarking Graph Neural Networks on Link Prediction<|sep|>In this paper, we benchmark several existing graph neural network (GNN) models on different datasets for link predictions. In particular, the graph convolutional network (GCN), GraphSAGE, graph attention network (GAT) as well as variational graph auto-encoder (VGAE) are implemented dedicated to link prediction tasks, in-depth analysis are performed, and results from several different papers are replicated, also a more fair and systematic comparison are provided. Our experiments show these GNN architectures perform similarly on various benchmarks for link prediction tasks.
Ligand-protein interactions in lysozyme investigated through a dual-resolution model<|sep|>A fully atomistic modelling of biological macromolecules at relevant length- and time-scales is often cumbersome or not even desirable, both in terms of computational effort required and it a posteriori analysis. This difficulty can be overcome with the use of multi-resolution models, in which different regions of the same system are concurrently described at different levels of detail. In enzymes, computationally expensive atomistic detail is crucial in the modelling of the active site in order to capture e.g. the chemically subtle process of ligand binding. In contrast, important yet more collective properties of the remainder of the protein can be reproduced with a coarser description. In the present work, we demonstrate the effectiveness of this approach through the calculation of the binding free energy of hen egg white lysozyme (HEWL) with the inhibitor di-N-acetylchitotriose. Particular attention is posed to the impact of the mapping, i.e. the selection of atomistic and coarse-grained residues, on the binding free energy. It is shown that, in spite of small variations of the binding free energy with respect to the active site resolution, the separate contributions coming from different energetic terms (such as electrostatic and van der Waals interactions) manifest a stronger dependence on the mapping, thus pointing to the existence of an optimal level of intermediate resolution.
Resource Usage Analysis of Logic Programs via Abstract Interpretation Using Sized Types<|sep|>We present a novel general resource analysis for logic programs based on sized types. Sized types are representations that incorporate structural (shape) information and allow expressing both lower and upper bounds on the size of a set of terms and their subterms at any position and depth. They also allow relating the sizes of terms and subterms occurring at different argument positions in logic predicates. Using these sized types, the resource analysis can infer both lower and upper bounds on the resources used by all the procedures in a program as functions on input term (and subterm) sizes, overcoming limitations of existing resource analyses and enhancing their precision. Our new resource analysis has been developed within the abstract interpretation framework, as an extension of the sized types abstract domain, and has been integrated into the Ciao preprocessor, CiaoPP. The abstract domain operations are integrated with the setting up and solving of recurrence equations for inferring both size and resource usage functions. We show that the analysis is an improvement over the previous resource analysis present in CiaoPP and compares well in power to state of the art systems.
Computing Graph Neural Networks: A Survey from Algorithms to Accelerators<|sep|>Graph Neural Networks (GNNs) have exploded onto the machine learning scene in recent years owing to their capability to model and learn from graph-structured data. Such an ability has strong implications in a wide variety of fields whose data is inherently relational, for which conventional neural networks do not perform well. Indeed, as recent reviews can attest, research in the area of GNNs has grown rapidly and has lead to the development of a variety of GNN algorithm variants as well as to the exploration of groundbreaking applications in chemistry, neurology, electronics, or communication networks, among others. At the current stage of research, however, the efficient processing of GNNs is still an open challenge for several reasons. Besides of their novelty, GNNs are hard to compute due to their dependence on the input graph, their combination of dense and very sparse operations, or the need to scale to huge graphs in some applications. In this context, this paper aims to make two main contributions. On the one hand, a review of the field of GNNs is presented from the perspective of computing. This includes a brief tutorial on the GNN fundamentals, an overview of the evolution of the field in the last decade, and a summary of operations carried out in the multiple phases of different GNN algorithm variants. On the other hand, an in-depth analysis of current software and hardware acceleration schemes is provided, from which a hardware-software, graph-aware, and communication-centric vision for GNN accelerators is distilled.
New Series Representations for the Two-Loop Massive Sunset Diagram<|sep|>We derive new convergent series representations for the two-loop sunset diagram with three different propagator masses m1, m2 and m3 and external momentum p by techniques of analytic continuation on a well-known triple series that corresponds to the Lauricella Fc function. The convergence regions of the new series contain regions of interest to physical problems. These include some ranges of masses and squared external momentum values which make them useful from Chiral Perturbation Theory to some regions of the parameter space of the Minimal Supersymmetric Standard Model. The analytic continuation results presented on the Lauricella series could be used in other settings as well.
Mean-payoff Automaton Expressions<|sep|>Quantitative languages are an extension of boolean languages that assign to each word a real number. Mean-payoff automata are finite automata with numerical weights on transitions that assign to each infinite path the long-run average of the transition weights. When the mode of branching of the automaton is deterministic, nondeterministic, or alternating, the corresponding class of quantitative languages is not robust as it is not closed under the pointwise operations of max, min, sum, and numerical complement. Nondeterministic and alternating mean-payoff automata are not decidable either, as the quantitative generalization of the problems of universality and language inclusion is undecidable. We introduce a new class of quantitative languages, defined by mean-payoff automaton expressions, which is robust and decidable: it is closed under the four pointwise operations, and we show that all decision problems are decidable for this class. Mean-payoff automaton expressions subsume deterministic mean-payoff automata, and we show that they have expressive power incomparable to nondeterministic and alternating mean-payoff automata. We also present for the first time an algorithm to compute distance between two quantitative languages, and in our case the quantitative languages are given as mean-payoff automaton expressions.
Virtual Particle Interpretation of Quantum Mechanics - a non-dualistic model of QM with a natural probability interpretation<|sep|>An interpretation of non-relativistic quantum mechanics is presented in the spirit of Erwin Madelung's hydrodynamic formulation of QM and Louis de Broglie's and David Bohm's pilot wave models. The aims of the approach are as follows: 1) to have a clear ontology for QM, 2) to describe QM in a causal way, 3) to get rid of the wave-particle dualism in pilot wave theories, 4) to provide a theoretical framework for describing creation and annihilation of particles, and 5) to provide a possible connection between particle QM and virtual particles in QFT. These goals are achieved, if the wave function is replaced by a fluid of so called virtual particles. It is also assumed that in this fluid of virtual particles exist a few real particles and that only these real particles can be directly observed. This has relevance for the measurement problem in QM and it is found that quantum probabilities arise in a very natural way from the structure of the theory. The model presented here is very similar to a recent computational model of quantum physics and recent Bohmian models of QFT.
Magnetized Particle Motion Around Black Hole in Braneworld<|sep|>We investigate the motion of a magnetized particle orbiting around a black hole in braneworld placed in asymptotically uniform magnetic field. The influence of brane parameter on effective potential of the radial motion of magnetized spinning particle around the braneworld black hole using Hamilton-Jacobi formalism is studied. It is found that circular orbits for photons and slowly moving particles may become stable near $r = 3M$. It was argued that the radii of the innermost stable circular orbits are sensitive on the change of brane parameter. Similar discussion without Weil parameter has been considered by de Felice et all in Ref. \refcite{rs99,98}.
Integrating temporal and spatial scales: Human structural network motifs across age and region-of-interest size<|sep|>Human brain networks can be characterized at different temporal or spatial scales given by the age of the subject or the spatial resolution of the neuroimaging method. Integration of data across scales can only be successful if the combined networks show a similar architecture. One way to compare networks is to look at spatial features, based on fibre length, and topological features of individual nodes where outlier nodes form single node motifs whose frequency yields a fingerprint of the network. Here, we observe how characteristic single node motifs change over age (12-23 years) and network size (414, 813, and 1615 nodes) for diffusion tensor imaging (DTI) structural connectivity in healthy human subjects. First, we find the number and diversity of motifs in a network to be strongly correlated. Second, comparing different scales, the number and diversity of motifs varied across the temporal (subject age) and spatial (network resolution) scale: certain motifs might only occur at one spatial scale or for a certain age range. Third, regions of interest which show one motif at a lower resolution may show a range of motifs at a higher resolution which may or may not include the original motif at the lower resolution. Therefore, both the type and localisation of motifs differ for different spatial resolutions. Our results also indicate that spatial resolution has a higher effect on topological measures whereas spatial measures, based on fibre lengths, remain more comparable between resolutions. Therefore, spatial resolution is crucial when comparing characteristic node fingerprints given by topological and spatial network features. As node motifs are based on topological and spatial properties of brain connectivity networks, these conclusions are also relevant to other studies using connectome analysis.
Metastable Vacua in Brane Worlds<|sep|>We analyze vacuum decay in brane world setups, where a free scalar field in five dimensions has a localized potential admitting metastable vacua. We study in particular the bounce solution and its properties in flat and warped spaces. In the latter case, placing into a deeply warped region the term in the potential that lifts the vacuum degeneracy, can increase indefinitely the lifetime of the false vacuum. We discuss the application to metastable vacua in supersymmetric brane-world constructions.
Understanding the TeV emission from a distant blazar PKS 1424+240 in a lepto-hadronic jet model<|sep|>We investigate the formation of TeV spectrum of a distant blazar PKS 1424+240 residing at a redshift $z\geq0.6$ in two scenarios in the frame of a lepto-hadronic jet model taking both the uncertainties of the extragalactic background light (EBL) and its redshift into account. In the first scenario, TeV emission is attributed to the synchrotron emission of pair cascades resulting from $p\gamma$ interaction; in the second scenario, TeV emission is attributed to the proton-synchrotron emission, and an internal absorption due to interaction with the photons around the jet is included. The results show that in the first scenario the 68\% upper limit of its redshift within which this scenario can explain the VERITAS TeV spectrum in 2009 well is $\sim0.75$; in the second scenario, this upper limit of the redshift becomes $\sim1.03$. However, the second scenario can be excluded because it requires an unreasonable photon field around the jet with a luminosity of $\sim10^{43}\rm \ erg\ s^{-1}$. In conclusion, the jet model can explain its TeV spectrum with a low EBL density if $0.6<z<0.75$.
The Effectiveness of Memory Replay in Large Scale Continual Learning<|sep|>We study continual learning in the large scale setting where tasks in the input sequence are not limited to classification, and the outputs can be of high dimension. Among multiple state-of-the-art methods, we found vanilla experience replay (ER) still very competitive in terms of both performance and scalability, despite its simplicity. However, a degraded performance is observed for ER with small memory. A further visualization of the feature space reveals that the intermediate representation undergoes a distributional drift. While existing methods usually replay only the input-output pairs, we hypothesize that their regularization effect is inadequate for complex deep models and diverse tasks with small replay buffer size. Following this observation, we propose to replay the activation of the intermediate layers in addition to the input-output pairs. Considering that saving raw activation maps can dramatically increase memory and compute cost, we propose the Compressed Activation Replay technique, where compressed representations of layer activation are saved to the replay buffer. We show that this approach can achieve superior regularization effect while adding negligible memory overhead to replay method. Experiments on both the large-scale Taskonomy benchmark with a diverse set of tasks and standard common datasets (Split-CIFAR and Split-miniImageNet) demonstrate the effectiveness of the proposed method.
Smart Contracts Software Metrics: a First Study<|sep|>Smart contracts (SC) are software codes which reside and run over a blockchain. The code can be written in different languages with the common purpose of implementing various kinds of transactions onto the hosting blockchain, They are ruled by the blockchain infrastructure and work in order to satisfy conditions typical of traditional contracts. The software code must satisfy constrains strongly context dependent which are quite different from traditional software code. In particular, since the bytecode is uploaded in the hosting blockchain, size, computational resources, interaction between different parts of software are all limited and even if the specific software languages implement more or less the same constructs of traditional languages there is not the same freedom as in normal software development. SC software is expected to reflect these constrains on SC software metrics which should display metric values characteristic of the domain and different from more traditional software metrics. We tested this hypothesis on the code of more than twelve thousands SC written in Solidity and uploaded on the Ethereum blockchain. We downloaded the SC from a public repository and computed the statistics of a set of software metrics related to SC and compared them to the metrics extracted from more traditional software projects. Our results show that generally Smart Contracts metrics have ranges more restricted than the corresponding metrics in traditional software systems. Some of the stylized facts, like power law in the tail of the distribution of some metrics, are only approximate but the lines of code follow a log normal distribution which reminds of the same behavior already found in traditional software systems.
Preserving differential privacy under finite-precision semantics<|sep|>The approximation introduced by finite-precision representation of continuous data can induce arbitrarily large information leaks even when the computation using exact semantics is secure. Such leakage can thus undermine design efforts aimed at protecting sensitive information. We focus here on differential privacy, an approach to privacy that emerged from the area of statistical databases and is now widely applied also in other domains. In this approach, privacy is protected by the addition of noise to a true (private) value. To date, this approach to privacy has been proved correct only in the ideal case in which computations are made using an idealized, infinite-precision semantics. In this paper, we analyze the situation at the implementation level, where the semantics is necessarily finite-precision, i.e. the representation of real numbers and the operations on them, are rounded according to some level of precision. We show that in general there are violations of the differential privacy property, and we study the conditions under which we can still guarantee a limited (but, arguably, totally acceptable) variant of the property, under only a minor degradation of the privacy level. Finally, we illustrate our results on two cases of noise-generating distributions: the standard Laplacian mechanism commonly used in differential privacy, and a bivariate version of the Laplacian recently introduced in the setting of privacy-aware geolocation.
Optical design for CETUS: a wide-field 1.5m aperture UV payload being studied for a NASA probe class mission study<|sep|>As part of a study funded by NASA Headquarters, we are developing a Probe-class mission concept called the Cosmic Evolution Through UV Spectroscopy (CETUS). CETUS includes a 1.5-m aperture diameter telescope with a large field-of-view (FOV). CETUS includes three scientific instruments: a Far Ultraviolet (FUV) and Near Ultraviolet (NUV) imaging camera (CAM); a NUV Multi-Object Spectrograph (MOS); and a dual-channel Point Source Spectrograph (PSS) in the Lyman Ultraviolet (LUV), FUV, and NUV spectral regions. The large FOV Three Mirror Anastigmatic (TMA) Optical Telescope Assembly (OTA) simultaneously feeds the three separate scientific instruments. That is, the instruments view separate portions of the TMA image plane, enabling parallel operation of the three instruments. The field viewed by the MOS, whose design is based on an Offner-type spectrographic configuration to provide wide FOV correction, is actively configured to select and isolate numerous field sources using a next-generation Micro-Shutter Array (MSA). The two-channel camera design is also based on an Offner-like configuration. The Point Source Spectrograph (PSS) performs high spectral resolution spectroscopy on unresolved objects over the NUV region with spectral resolving power, R~ 40,000, in an echelle mode. The PSS also performs long-slit imaging spectroscopy at R~ 20,000 in the LUV and FUV spectral regions with two aberration-corrected, blazed, holographic gratings used in a Rowland-like configuration. The optical system also includes two Fine Guidance Sensors (FGS), and Wavefront Sensors (WFS) that sample numerous locations over the full OTA FOV. In-flight wavelength calibration is performed by a Wavelength Calibration System (WCS), and flat-fielding is also performed, both using in-flight calibration sources. This paper will describe the current optical design and the major trade studies leading to the design.
The environment of radio sources in the VLA-COSMOS Survey field<|sep|>This work studies the correlation among environmental density and radio AGN presence up to $z = 2$. Using data from the photometric COSMOS survey and its radio 1.4 GHz follow-up (VLA-COSMOS), a sample of radio AGNs has been defined. The environment was studied using the richness distributions inside a parallelepiped with base side of 1 Mpc and height proportional to the photometric redshift precision. Radio AGNs are found to be always located in environments significantly richer than those around galaxies with no radio emission. Moreover, a distinction based on radio AGN power shows that the significance of the environmental effect is only maintained for low-power radio sources. The results of this work show that denser environments play a significant role in enhancing the probability that a galaxy hosts a radio AGN and, in particular, low-power ones.
Modeling the total and polarized emission in evolving galaxies: "spotty" magnetic structures<|sep|>Future radio observations with the SKA and its precursors will be sensitive to trace spiral galaxies and their magnetic field configurations up to redshift $z\approx3$. We suggest an evolutionary model for the magnetic configuration in star-forming disk galaxies and simulate the magnetic field distribution, the total and polarized synchrotron emission, and the Faraday rotation measures for disk galaxies at $z\la 3$. Since details of dynamo action in young galaxies are quite uncertain, we model the dynamo action heuristically relying only on well-established ideas of the form and evolution of magnetic fields produced by the mean-field dynamo in a thin disk. We assume a small-scale seed field which is then amplified by the small-scale turbulent dynamo up to energy equipartition with kinetic energy of turbulence. The large-scale galactic dynamo starts from seed fields of 100 pc and an averaged regular field strength of 0.02\,$\mu$G, which then evolves to a "spotty" magnetic field configuration in about 0.8\,Gyr with scales of about one kpc and an averaged regular field strength of 0.6\,$\mu$G. The evolution of these magnetic spots is simulated under the influence of star formation, dynamo action, stretching by differential rotation of the disk, and turbulent diffusion. The evolution of the regular magnetic field in a disk of a spiral galaxy, as well as the expected total intensity, linear polarization and Faraday rotation are simulated in the rest frame of a galaxy at 5\,GHz and 150\,MHz and in the rest frame of the observer at 150\,MHz. We present the corresponding maps for several epochs after disk formation. (abridged)
DeepGalaxy: Testing Neural Network Verifiers via Two-Dimensional Input Space Exploration<|sep|>Deep neural networks (DNNs) are widely developed and applied in many areas, and the quality assurance of DNNs is critical. Neural network verification (NNV) aims to provide formal guarantees to DNN models. Similar to traditional software, neural network verifiers could also contain bugs, which would have a critical and serious impact, especially in safety-critical areas. However, little work exists on validating neural network verifiers. In this work, we propose DeepGalaxy, an automated approach based on differential testing to tackle this problem. Specifically, we (1) propose a line of mutation rules, including model level mutation and specification level mutation, to effectively explore the two-dimensional input space of neural network verifiers; and (2) propose heuristic strategies to select test cases. We leveraged our implementation of DeepGalaxy to test three state-of-the-art neural network verifies, Marabou, Eran, and Neurify. The experimental results support the efficiency and effectiveness of DeepGalaxy. Moreover, five unique unknown bugs were discovered
Time-delayed feedback in neurosystems<|sep|>The influence of time delay in systems of two coupled excitable neurons is studied in the framework of the FitzHugh-Nagumo model. Time-delay can occur in the coupling between neurons or in a self-feedback loop. The stochastic synchronization of instantaneously coupled neurons under the influence of white noise can be deliberately controlled by local time-delayed feedback. By appropriate choice of the delay time synchronization can be either enhanced or suppressed. In delay-coupled neurons, antiphase oscillations can be induced for sufficiently large delay and coupling strength. The additional application of time-delayed self-feedback leads to complex scenarios of synchronized in-phase or antiphase oscillations, bursting patterns, or amplitude death.
Using radiative energy losses to constrain the magnetisation and magnetic reconnection rate at the base of black hole jets<|sep|>We calculate the severe radiative energy losses which occur at the base of black hole jets using a relativistic fluid jet model, including in-situ acceleration of non-thermal leptons by magnetic reconnection. Our results demonstrate that including a self-consistent treatment of radiative energy losses is necessary to perform accurate MHD simulations of powerful jets and that jet spectra calculated via post-processing are liable to vastly overestimate the amount of non-thermal emission. If no more than 95% of the initial total jet power is radiated away by the plasma travels as it travels along the length of the jet, we can place a lower bound on the magnetisation of the jet plasma at the base of the jet. For typical powerful jets, we find that the plasma at the jet base is required to be highly magnetised, with at least 10,000 times more energy contained in magnetic fields than in non-thermal leptons. Using a simple power-law model of magnetic reconnection, motivated by simulations of collisionless reconnection, we determine the allowed range of the large-scale average reconnection rate along the jet, by restricting the total radiative energy losses incurred and the distance at which the jet first comes into equipartition. We calculate analytic expressions for the cumulative radiative energy losses due to synchrotron and inverse-Compton emission along jets, and derive analytic formulae for the constraint on the initial magnetisation.
Fully distributed and fault tolerant task management based on diffusions<|sep|>The task management is a critical component for the computational grids. The aim is to assign tasks on nodes according to a global scheduling policy and a view of local resources of nodes. A peer-to-peer approach for the task management involves a better scalability for the grid and a higher fault tolerance. But some mechanisms have to be proposed to avoid the computation of replicated tasks that can reduce the efficiency and increase the load of nodes. In the same way, these mechanisms have to limit the number of exchanged messages to avoid the overload of the network. In a previous paper, we have proposed two methods for the task management called active and passive. These methods are based on a random walk: they are fully distributed and fault tolerant. Each node owns a local tasks states set updated thanks to a random walk and each node is in charge of the local assignment. Here, we propose three methods to improve the efficiency of the active method. These new methods are based on a circulating word. The nodes local tasks states sets are updated thanks to periodical diffusions along trees built from the circulating word. Particularly, we show that these methods increase the efficiency of the active method: they produce less replicated tasks. These three methods are also fully distributed and fault tolerant. On the other way, the circulating word can be exploited for other applications like the resources management or the nodes synchronization.
A Simple Phenomenological Model for Grain Clustering in Turbulence<|sep|>We propose a simple model for density fluctuations of aerodynamic grains, embedded in a turbulent, gravitating gas disk. The model combines a calculation for the behavior of a group of grains encountering a single turbulent eddy, with a hierarchical approximation of the eddy statistics. This makes analytic predictions for a range of quantities including: distributions of grain densities, power spectra and correlation functions of fluctuations, and maximum grain densities reached. We predict how these scale as a function of grain drag time t_stop, spatial scale, grain-to-gas mass ratio, strength of turbulence (alpha), and detailed disk properties. We test these against numerical simulations with various turbulence-driving mechanisms. The simulations agree well with the predictions, spanning t_stop*Omega ~ 1e-4 - 10, alpha ~ 1e-10 - 1e-2, and grain-to-gas mass ratio ~0-3. Results from 'turbulent concentration' simulations and laboratory experiments are also predicted as a special case. Vortices on a wide range of scales disperse and concentrate grains hierarchically. For small grains this is most efficient in eddies with turnover time comparable to the stopping time, but fluctuations are also damped by local gas-grain drift. For large grains, shear and gravity lead to a much broader range of eddy scales driving fluctuations, with most power on the largest scales. The grain density distribution has a log-Poisson shape, with fluctuations for large grains up to factors >1000. We provide simple analytic expressions for the predictions, and discuss implications for planetesimal formation, grain growth, and the structure of turbulence.
Validation of Spherically Symmetric Inversion by Use of a Tomographic Reconstructed Three-Dimensional Electron Density of the Solar Corona<|sep|>Determination of the coronal electron density by the inversion of white-light polarized brightness (pB) measurements by coronagraphs is a classic problem in solar physics. An inversion technique based on the spherically symmetric geometry (Spherically Symmetric Inversion, SSI) was developed in the 1950s, and has been widely applied to interpret various observations. However, to date there is no study about uncertainty estimation of this method. In this study we present the detailed assessment of this method using a three-dimensional (3D) electron density in the corona from 1.5 to 4 Rsun as a model, which is reconstructed by tomography method from STEREO/COR1 observations during solar minimum in February 2008. We first show in theory and observation that the spherically symmetric polynomial approximation (SSPA) method and the Van de Hulst inversion technique are equivalent. Then we assess the SSPA method using synthesized pB images from the 3D density model, and find that the SSPA density values are close to the model inputs for the streamer core near the plane of the sky (POS) with differences generally less than a factor of two or so; the former has the lower peak but more spread in both longitudinal and latitudinal directions than the latter. We estimate that the SSPA method may resolve the coronal density structure near the POS with angular resolution in longitude of about 50 degrees. Our results confirm the suggestion that the SSI method is applicable to the solar minimum streamer (belt) as stated in some previous studies. In addition, we demonstrate that the SSPA method can be used to reconstruct the 3D coronal density, roughly in agreement with that by tomography for a period of low solar activity. We suggest that the SSI method is complementary to the 3D tomographic technique in some cases, given that the development of the latter is still an ongoing research effort.
The supporting halfspace- quadratic programming strategy for the dual of the best approximation problem<|sep|>We consider the best approximation problem (BAP) of projecting a point onto the intersection of a number of convex sets. It is known that Dykstra's algorithm is alternating minimization on the dual problem. We extend Dykstra's algorithm so that it can be enhanced by the SHQP strategy of using quadratic programming to project onto the intersection of supporting halfspaces generated by earlier projection operations. By looking at a structured alternating minimization problem, we show the convergence rate of Dykstra's algorithm when reasonable conditions are imposed to guarantee a dual minimizer. We also establish convergence of using a warmstart iterate for Dykstra's algorithm, show how all the results for the Dykstra's algorithm can be carried over to the simultaneous Dykstra's algorithm, and discuss a different way of incorporating the SHQP strategy. Lastly, we show that the dual of the best approximation problem can have an O(1/k^2) accelerated algorithm that also incorporates the SHQP strategy.
Galaxies that Shine: radiation-hydrodynamical simulations of disk galaxies<|sep|>Radiation feedback is typically implemented using subgrid recipes in hydrodynamical simulations of galaxies. Very little work has so far been performed using radiation-hydrodynamics (RHD), and there is no consensus on the importance of radiation feedback in galaxy evolution. We present RHD simulations of isolated galaxy disks of different masses with a resolution of 18 pc. Besides accounting for supernova feedback, our simulations are the first galaxy-scale simulations to include RHD treatments of photo-ionisation heating and radiation pressure, from both direct optical/UV radiation and multi-scattered, re-processed infrared (IR) radiation. Photo-heating smooths and thickens the disks and suppresses star formation about as much as the inclusion of ("thermal dump") supernova feedback does. These effects decrease with galaxy mass and are mainly due to the prevention of the formation of dense clouds, as opposed to their destruction. Radiation pressure, whether from direct or IR radiation, has little effect, but for the IR radiation we show that its impact is limited by our inability to resolve the high optical depths for which multi-scattering becomes important. While artificially boosting the IR optical depths does reduce the star formation, it does so by smoothing the gas rather than by generating stronger outflows. We conclude that although higher-resolution simulations, and potentially also different supernova implementations, are needed for confirmation, our findings suggest that radiation feedback is more gentle and less effective than is often assumed in subgrid prescriptions.
Bilinear-biquadratic spin-1 rings: an SU(2)-symmetric MPS algorithm for periodic boundary conditions<|sep|>An efficient algorithm for SU(2) symmetric matrix product states (MPS) with periodic boundary conditions (PBC) is proposed and implemented. It is applied to a study of the spectrum and correlation properties of the spin-1 bilinear-biquadratic Heisenberg model. We characterize the various phases of this model by the lowest states of the spectrum with angular momentum J = 0, 1, 2 for systems of up to 100 spins. Furthermore, we provide precision results for the dimerization correlator as well as the string correlator.
Dependency-based Text Graphs for Keyphrase and Summary Extraction with Applications to Interactive Content Retrieval<|sep|>We build a bridge between neural network-based machine learning and graph-based natural language processing and introduce a unified approach to keyphrase, summary and relation extraction by aggregating dependency graphs from links provided by a deep-learning based dependency parser. We reorganize dependency graphs to focus on the most relevant content elements of a sentence, integrate sentence identifiers as graph nodes and after ranking the graph, we extract our keyphrases and summaries from its largest strongly-connected component. We take advantage of the implicit structural information that dependency links bring to extract subject-verb-object, is-a and part-of relations. We put it all together into a proof-of-concept dialog engine that specializes the text graph with respect to a query and reveals interactively the document's most relevant content elements. The open-source code of the integrated system is available at https://github.com/ptarau/DeepRank . Keywords: graph-based natural language processing, dependency graphs, keyphrase, summary and relation extraction, query-driven salient sentence extraction, logic-based dialog engine, synergies between neural and symbolic processing.
On Solving Cubic-Quartic Nonlinear Schr\"odinger Equation in a Cnoidal Trap<|sep|>The recent observations of quantum droplet in ultra-cold atomic gases have opened up new avenues of fundamental research. The competition between mean-field and beyond mean-field interactions, in ultra-cold dilute alkali gases, are believed to be instrumental in stabilizing the droplets. These new understanding has motivated us to investigate the analytical solutions of a trapped cubic-quartic nonlinear Schr\"odinger equation (CQNLSE). The quartic contribution in the NLSE is derived from the beyond mean-field formalism of Bose-Einstein condensate (BEC). To the best of our knowledge, a comprehensive analytical description of CQNLSE is non-existent. Here, we study the existence of the analytical solutions which are of the cnoidal type for CQNLSE. The external trapping plays a significant role in the stabilization of the system. In the limiting case, the cnoidal wave solutions lead to the localized solution of bright solution and delocalized kink-antikink pair. The nonexistence of the sinusoidal mode in the current scheme is also revealed in our analysis.
An improved design method for conventional straight dipole magnets<|sep|>The standard design method for conventional straight dipole magnets is improved in this paper. The good field region is not symmetric with respect to the magnet mechanical center, and its width is not enlarged to include the beam sagitta. The integrated field quality is obtained by integrating the field along nominal beam paths. 2D and 3D design procedures of the improved design method are introduced, and two application examples of straight dipole magnets are presented. It is shown that the differences in integrated field quality between different field integration paths cannot be neglected. Compared with the traditional design method of straight dipole magnets, the advantage of the improved method is that the integrated field quality is accurate; the pole width, magnet dimension and weight of a straight dipole magnet can be reduced.
Sample Drop Detection for Distant-speech Recognition with Asynchronous Devices Distributed in Space<|sep|>In many applications of multi-microphone multi-device processing, the synchronization among different input channels can be affected by the lack of a common clock and isolated drops of samples. In this work, we address the issue of sample drop detection in the context of a conversational speech scenario, recorded by a set of microphones distributed in space. The goal is to design a neural-based model that given a short window in the time domain, detects whether one or more devices have been subjected to a sample drop event. The candidate time windows are selected from a set of large time intervals, possibly including a sample drop, and by using a preprocessing step. The latter is based on the application of normalized cross-correlation between signals acquired by different devices. The architecture of the neural network relies on a CNN-LSTM encoder, followed by multi-head attention. The experiments are conducted using both artificial and real data. Our proposed approach obtained F1 score of 88% on an evaluation set extracted from the CHiME-5 corpus. A comparable performance was found in a larger set of experiments conducted on a set of multi-channel artificial scenes.
Properties of Rotating Neutron Star in Density-dependent Relativistic Mean-field Models<|sep|>Equilibrium sequences were developed for rotating NS in the relativistic mean-field interaction framework using four density-dependent equations of state for the NS matter. These sequences were constructed for the observed rotation frequencies of 25, 317, 346, 716, and 1122 Hz. The bounds of sequences were calculated in each model to determine the stability region. The gravitational mass, quadrupole moment, polar, forward and backward redshifts, and Kerr parameter were calculated. DDF and DD-ME$\delta$ were unable to properly describe the low-frequency neutron stars, PSR J0348+432, PSR J1614-2230 , and PSR J0740+6620 rotate at a frequency of 25, 317, and 346 Hz, respectively. All the selected EOSs properly described the rotation of PSR J1748-244ad, and PSR J1739-285 at a frequency of 716 and 1122 Hz, respectively. The mass of these stars was in the range of [0.68, 2.14]M$_{\odot}$ and [1.67, 2.24]M$_{\odot}$, respectively. The polar, forward and backward redshifts, and the quadrupole moment were calculated in all selected rotating frequencies and the Keplerian sequence. The results were consistent with observations. Confirming the mass of $1.5^{+0.4}_{-1.0}M_{\odot}$ for EXO 0748-676, our result will be close to the observed value, and the EOSs used in this study properly describe this star. The extremum of Kerr parameter, polar, forward and backward redshifts in all models reached constant values of, a/M$\approx$ 0.7, Z$_{p}\approx$ 0.8, Z$_{eq}^{f}\approx$ -0.3 and Z$_{eq}^{b}\approx$ 2.2. These behaviors of redshifts and Kerr parameter are approximately independent of EOS. The observed behaviors must evaluate by other EOSs to find universal relations for these quantities. Also, a limit value was found for each of these parameters. In this case where these parameters are greater than the limit value, the star can rotate at a frequency equal to or greater than $\nu$= 1122 Hz.
Low-energy antiproton physics and the FLAIR facility<|sep|>FLAIR, the Facility for Low-energy Antiproton and Ion Research has been proposed in 2004 as an extension of the planned FAIR facility at Darmstadt, Germany. FLAIR was not included into the Modularized Start Version of FAIR, but the recent installation of the CRYRING storage ring at GSI Darmstadt has opened new perspectives for physics with low-energy antiprotons at FAIR.
Dirac spectrum and the BEC-BCS crossover in QCD at nonzero isospin asymmetry<|sep|>For large isospin asymmetries, perturbation theory predicts the QCD ground state to be a superfluid phase of $u$ and $\bar{d}$ Cooper pairs. This phase, which is denoted as the BCS phase, is expected to be smoothly connected to the standard phase with Bose-Einstein condensation (BEC) of charged pions at $\mu_I\ge m_\pi/2$ by an analytic crossover. A first hint for the existence of the BCS phase, which is likely characterised by the presence of both, deconfinement and charged pion condensation, is coming from the lattice observation that the deconfinement crossover smoothly penetrates into the BEC phase. To further scrutinize the existence of the BCS phase, in this proceedings article we investigate the complex spectrum of the massive Dirac operator in 2+1-flavor QCD at nonzero temperature and isospin chemical potential. The spectral density near the origin is related to the BCS gap via a generalization of the Banks-Casher relation to the case of complex Dirac eigenvalues (derived for the zero-temperature, high-density limits of QCD at nonzero isospin chemical potential).
Curriculum Learning with a Progression Function<|sep|>Curriculum Learning for Reinforcement Learning is an increasingly popular technique that involves training an agent on a sequence of intermediate tasks, called a Curriculum, to increase the agent's performance and learning speed. This paper introduces a novel paradigm for curriculum generation based on progression and mapping functions. While progression functions specify the complexity of the environment at any given time, mapping functions generate environments of a specific complexity. Different progression functions are introduced, including an autonomous online task progression based on the agent's performance. Our approach's benefits and wide applicability are shown by empirically comparing its performance to two state-of-the-art Curriculum Learning algorithms on six domains.
Duality between Feature Selection and Data Clustering<|sep|>The feature-selection problem is formulated from an information-theoretic perspective. We show that the problem can be efficiently solved by an extension of the recently proposed info-clustering paradigm. This reveals the fundamental duality between feature selection and data clustering,which is a consequence of the more general duality between the principal partition and the principal lattice of partitions in combinatorial optimization.
Broadband spectral modelling of bent jets of Active Galactic Nuclei<|sep|>In this thesis, models have been developed to study the radiation emission processes from the knots of AGN jets as well as for blazar jets. A continuous injection plasma model is developed to study the X-ray emission from the knots of sources 1136-135, 1150+497, 1354+195 and 3C 371. The knot dynamics is then studied within the framework of internal shock model. In such a scenario, knots are formed due to the collision of two successive matter blobs emitted sporadically from the central engine of AGN. Shocks, generated in such collisions, accelerate electrons to relativistic energies. These electrons subsequently emit radiation via synchrotron and/or inverse Compton processes in the radio-to-X-ray energy range. The study of M87 knots involves a two zone model where the electrons with a power-law distribution are further accelerated. The synchrotron emission from these energetic electrons is then used to explain the observed spectrum. The limb-brightening feature observed in the radio maps of the BL Lac object MKN501 is studied considering shear acceleration of electrons at the boundary of the jet. This interpretation does not require a large viewing angle of the jet as demanded by the earlier models and is consistent with the constraints obtained from very high energy studies. In case of MKN421, the dependence of temporal behaviour of radiation emission on the particle acceleration mechanism has been studied within the framework of two zone model.
Quantum Instability of the Cauchy Horizon in Reissner-Nordstr\"om-deSitter Spacetime<|sep|>In classical General Relativity, the values of fields on spacetime are uniquely determined by their values at an initial time within the domain of dependence of this initial data surface. However, it may occur that the spacetime under consideration extends beyond this domain of dependence, and fields, therefore, are not entirely determined by their initial data. This occurs, for example, in the well-known (maximally) extended Reissner-Nordstr\"om or Reissner-Nordstr\"om-deSitter (RNdS) spacetimes. The boundary of the region determined by the initial data is called the "Cauchy horizon." It is located inside the black hole in these spacetimes. The strong cosmic censorship conjecture asserts that the Cauchy horizon does not, in fact, exist in practice because the slightest perturbation (of the metric itself or the matter fields) will become singular there in a sufficiently catastrophic way that solutions cannot be extended beyond the Cauchy horizon. Thus, if strong cosmic censorship holds, the Cauchy horizon will be converted into a "final singularity," and determinism will hold. Recently, however, it has been found that, classically this is not the case in RNdS spacetimes in a certain range of mass, charge, and cosmological constant. In this paper, we consider a quantum scalar field in RNdS spacetime and show that quantum theory comes to the rescue of strong cosmic censorship. We find that for any state that is nonsingular (i.e., Hadamard) within the domain of dependence, the expected stress-tensor blows up with affine parameter, $V$, along a radial null geodesic transverse to the Cauchy horizon as $T_{VV} \sim C/V^2$ with $C$ independent of the state and $C \neq 0$ generically in RNdS spacetimes. This divergence is stronger than in the classical theory and should be sufficient to convert the Cauchy horizon into a strong curvature singularity.
Space-time qubits<|sep|>We construct a qubit algebra from field creation and annihilation operators acting on a global vacuum state. Particles to be used as qubits are created from the vacuum by a near-deterministic single particle source. Our formulation makes the space-time dependence of the qubits explicit, preparing the way for quantum computation within a field framework. The method can be generalized to deal with interacting qubits whose wavepackets are not perfectly matched to each other. We give an example of how to calculate the Heisenberg evolution of a simple two-qubit circuit, taking expectation values in the field vacuum state.
Study of $\psi(2S)$ decays into $\gamma K^+K^-$ and $\gamma \pi^+\pi^-$<|sep|>Radiative charmonium decays from the BESII sample of 14$\times10^{6}$ $\psi(2S)$-events into two different final states, $\gamma K^+K^-$ and $\gamma\pi^+\pi^-$, are analyzed. Product branching fractions for $\psi(2S)\to\gamma X\to \gamma\pi^+\pi^-$, $\gamma K^+K^-$ are given, where $X=f_2(1270)$, $f_0(1500)$, and $f_0(1710)$ in $\pi^+\pi^-$ and $f_2(1270)$, $f_2'(1525)$, and $f_0(1700)$ in $K^+K^-$. An angular analysis gives the ratios of the helicity projections for the $f_2(1270)$ in $\psi(2S)\to\gamma f_2(1270)\to\gamma\pi^+\pi^-$.
Missing measurements on RIPE Atlas<|sep|>We show that it is common to lose some datapoints for mea-surements scheduled at regular interval on RIPE Atlas. Thetemporal correlation between missing measurements and con-nection events are analyzed, in the pursuit of understandingreasons behind such missings. To our surprise, a big part of measurements are lost while probes are connected.
Further improving security of Vector Stream Cipher<|sep|>Vector Stream Cipher (VSC) is a stream cipher which consists of permutation polynomial over a ring of modulo $2^w$. The algorithm for generating key stream is very simple and the encryption is very fast. Some theoretical attacks for VSC have been reported so far since the invention of VSC in 2004. Then, the authors proposed some improvements and developed "Vector Stream Cipher 2.0 (VSC 2.0)" to be immune against the theoretical attacks. In this paper, we propose further improvement of VSC 2.0 to publish as a new chaos cipher "Vector Stream Cipher 2.1 (VSC2.1)". VSC 2.1 is faster and more secure than VSC 2.0. Our result suggests that permutation polynomials over a ring of modulo $2^w$ are useful for cryptography.
Boundary conditions for the quantum Hall effect<|sep|>We formulate a self-consistent model of the integer quantum Hall effect on an infinite strip, using boundary conditions to investigate the influence of finite-size effects on the Hall conductivity. By exploiting the translation symmetry along the strip, we determine both the general spectral properties of the system for a large class of boundary conditions respecting such symmetry, and the full spectrum for (fibered) Robin boundary conditions. In particular, we find that the latter introduce a new kind of states with no classical analogues, and add a finer structure to the quantization pattern of the Hall conductivity. Moreover, our model also predicts the breakdown of the quantum Hall effect at high values of the applied electric field.
A Markov Random Field and Active Contour Image Segmentation Model for Animal Spots Patterns<|sep|>Non-intrusive biometrics of animals using images allows to analyze phenotypic populations and individuals with patterns like stripes and spots without affecting the studied subjects. However, non-intrusive biometrics demand a well trained subject or the development of computer vision algorithms that ease the identification task. In this work, an analysis of classic segmentation approaches that require a supervised tuning of their parameters such as threshold, adaptive threshold, histogram equalization, and saturation correction is presented. In contrast, a general unsupervised algorithm using Markov Random Fields (MRF) for segmentation of spots patterns is proposed. Active contours are used to boost results using MRF output as seeds. As study subject the Diploglossus millepunctatus lizard is used. The proposed method achieved a maximum efficiency of $91.11\%$.
Refined toric branes, surface operators and factorization of generalized Macdonald polynomials<|sep|>We find new universal factorization identities for generalized Macdonald polynomials on the topological locus. We prove the identities (which include all previously known forumlas of this kind) using factorization identities for matrix model averages, which are themselves consequences of Ding-Iohara-Miki constraints. Factorized expressions for generalized Macdonald polynomials are identified with refined topological string amplitudes containing a toric brane on an intermediate preferred leg, surface operators in gauge theory and certain degenerate CFT vertex operators.
On the Existence of Semi-Regular Sequences<|sep|>Semi-regular sequences over $\mathbb{F}_2$ are sequences of homogeneous elements of the algebra $ B^{(n)}=\mathbb{F}_2[X_1,...,X_n]/(X_1^2,...,X_n^2) $, which have as few relations between them as possible. They were introduced in order to assess the complexity of Gr\"obner basis algorithms such as ${\bf F}_4, {\bf F}_5$ for the solution of polynomial equations. Despite the experimental evidence that semi-regular sequences are common, it was unknown whether there existed semi-regular sequences for all $n$, except in extremely trivial situations. We prove some results on the existence and non-existence of semi-regular sequences. In particular, we show that if an element of degree $d$ in $B^{(n)}$ is semi-regular, then we must have $n\leq 3d$. Also, we show that if $d=2^t$ and $n=3d$ there exits a semi-regular element of degree $d$ establishing that the bound is sharp for infinitely many $n$. Finally, we generalize the result of non-existence of semi-regular elements to the case of sequences of a fixed length $m$.
Synthesis of radio signals from extensive air showers using previously computed microscopic simulations<|sep|>The detection of extensive air showers (EAS) through their radio signal is becoming one of the most promising techniques for the study of Neutrinos and Cosmic rays at the highest energies. For the design, optimization and characterization of radio arrays, and of their associated reconstruction algorithms, tens of thousands of Monte Carlo simulations are needed. Current available simulation codes can take several days to compute the signals produced by a single shower, making it impossible to produce the required simulations in a reasonable amount of time, in a cost-effective and environmental-conscious way. In this article we present a method to synthesize the expected signals (the full time trace, not just the peak amplitude) at any point around the shower core, given a set of signals simulated in a finite number of antennas strategically located in a pattern that exploits the signature features of the radio wavefront. The method can be applied indistinctly to the electric field or to the antenna response to the electric field, in the three polarization directions. The synthesized signal can be used to evaluate trigger conditions, compute the fluence or reconstruct the shower incoming direction, allowing for the production of one single library of simulations that can be used and re-used for the characterization and optimization of radio arrays and their associated reconstruction methods, for a thousandth part of the otherwise required CPU time.
A holistic look at requirements engineering practices in the gaming industry<|sep|>In this work we present an account of the status of requirements engineering in the gaming industry. Recent papers in the area were surveyed. Characterizations of the gaming industry were deliberated upon by portraying its relations with the market industry. Some research directions in the area of requirements engineering in the gaming industry were also mentioned.
Three years of Fermi GBM Earth Occultation Monitoring: Observations of Hard X-ray/Soft Gamma-Ray Sources<|sep|>The Gamma ray Burst Monitor (GBM) on board Fermi Gamma-ray Space Telescope has been providing continuous data to the astronomical community since 2008 August 12. We will present the results of the analysis of the first three years of these continuous data using the Earth occultation technique to monitor a catalog of 209 sources. Although the occultation technique is in principle quite simple, in practice there are many complications including the dynamic instrument response, source confusion, and scattering in the Earth's atmosphere, which will be described. We detect 99 sources, including 40 low-mass X-ray binary/neutron star systems, 31 high-mass X-ray binary/neutron star systems, 12 black hole binaries, 12 active galaxies, 2 other sources, plus the Crab Nebula and the Sun. Nine of these sources are detected in the 100-300 keV band, including seven black-hole binaries, the active galaxy Cen A, and the Crab. The Crab and Cyg X-1 are also detected in the 300-500 keV band. GBM provides complementary data to other sky monitors below 100 keV and is the only all-sky monitor above 100 keV. In our fourth year of monitoring, we have already increased the number of transient sources detected and expect several of the weaker persistent sources to cross the detection threshold. I will briefly discuss these new sources and what to expect from our five year occultation catalog.
A Windowed Green Function method for elastic scattering problems on a half-space<|sep|>This paper presents a windowed Green function (WGF) method for the numerical solution of problems of elastic scattering by "locally-rough surfaces" (i.e., local perturbations of a half space), under either Dirichlet or Neumann boundary conditions, and in both two and three spatial dimensions. The proposed WGF method relies on an integral-equation formulation based on the free-space Green function, together with smooth operator windowing (based on a "slow-rise" windowing function) and efficient high-order singular-integration methods. The approach avoids the evaluation of the expensive layer Green function for elastic problems on a half-space, and it yields uniformly fast convergence for all incident angles. Numerical experiments for both two and three dimensional problems are presented, demonstrating the accuracy and super-algebraically fast convergence of the proposed method as the window-size grows.
Stress-energy tensor of a radiating sphere inclosing black hole<|sep|>We consider a uniformly luminous radiating sphere and a static black hole located in the center of that sphere. We give analytic formulas for radiation stress-energy tensor components in such a configuration, for the observer located at an arbitrary distance from the static black hole horizon.
Transport properties of quantum dots in the Wigner molecule regime<|sep|>The transport properties of quantum dots with up to N=7 electrons ranging from the weak to the strong interacting regime are investigated via the projected Hartree-Fock technique. As interactions increase radial order develops in the dot, with the formation of ring and centered-ring structures. Subsequently, angular correlations appear, signalling the formation of a Wigner molecule state. We show striking signatures of the emergence of Wigner molecules, detected in transport. In the linear regime, conductance is exponentially suppressed as the interaction strength grows. A further suppression is observed when centered-ring structures develop, or peculiar spin textures appear. In the nonlinear regime, the formation of molecular states may even lead to a conductance enhancement.
A reduced unified continuum formulation for vascular fluid-structure interaction<|sep|>We recently derived the unified continuum and variational multiscale formulation for fluid-structure interaction (FSI) using the Gibbs free energy. Restricting our attention to vascular FSI, we now reduce this arbitrary Lagrangian-Eulerian (ALE) formulation by adopting three assumptions for the vascular wall. The resulting reduced unified continuum formulation achieves monolithic FSI coupling in the Eulerian frame through a simple modification of the fluid boundary integral. While ostensibly similar to the semi-discrete formulation of the coupled momentum method, its underlying derivation does not rely on an assumption of a fictitious body force in the elastodynamics sub-problem and therefore represents a direct simplification of the ALE method. Uniform temporal discretization is performed via the generalized-$\alpha$ scheme. In contrast to the predominant approach yielding only first-order accuracy for pressure, we collocate both pressure and velocity at the intermediate time step to achieve uniform second-order temporal accuracy. In conjunction with quadratic tetrahedral elements, our methodology offers higher-order temporal and spatial accuracy for quantities of clinical interest. Furthermore, without loss of consistency, a segregated predictor multi-corrector algorithm is developed to preserve the same block structure as for the incompressible Navier-Stokes equations in the implicit solver's associated linear system. Block preconditioning of a monolithically coupled FSI system is therefore made possible for the first time. Compared to alternative preconditioners, our three-level nested block preconditioner, which improves representation of the Schur complement, demonstrates robust performance over a wide range of physical parameters. We present verification against Womersley's deformable wall theory and additionally develop practical modeling techniques for clinical applications.
Semi-Supervised Learning for Neural Machine Translation<|sep|>While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.
Finsler geometrization of classic theory for fields on the interphase boundary including monomolecular 2D-system<|sep|>We prove that the Ricci scalar curvature and the Berwald scalar curvature of a two-dimensional Finsler space, considered over a vector field on the 3-dimensional flat space, are naturally related to 2-dimensional electro-capillary phenomena effects observed for a compressed monolayer. The Cartan tensor and the nonlinear Barthel connection of the Finsler model are determined, and the geometric objects which depend on compression speed and on the characteristics of the electrically charged double layer are used in order to reveal several classes of structure formation within the phase transition of the first order.
Learning Based Methods for Traffic Matrix Estimation from Link Measurements<|sep|>Network traffic demand matrix is a critical input for capacity planning, anomaly detection and many other network management related tasks. The demand matrix is often computed from link load measurements. The traffic matrix (TM) estimation problem is the determination of the traffic demand matrix from link load measurements. The relationship between the link loads and the traffic matrix that generated the link load can be modeled as an under-determined linear system and has multiple feasible solutions. Therefore, prior knowledge of the traffic demand pattern has to be used in order to find a potentially feasible demand matrix. In this paper, we consider the TM estimation problem where we have information about the distribution of the demand sizes. This information can be obtained from the analysis of a few traffic matrices measured in the past or from operator experience. We develop an iterative projection based algorithm for the solution of this problem. If large number of past traffic matrices are accessible, we propose a Generative Adversarial Network (GAN) based approach for solving the problem. We compare the strengths of the two approaches and evaluate their performance for several networks using varying amounts of past data.
Inflation, Leptogenesis, and Yukawa Quasi-Unification within a Supersymmetric Left-Right Model<|sep|>A simple extension of the minimal left-right symmetric supersymmetric grand unified theory model is constructed by adding two pairs of superfields. This naturally violates the partial Yukawa unification predicted by the minimal model. After including supergravity corrections, we find that this extended model naturally supports hilltop F-term hybrid inflation along its trivial inflationary path with only a very mild tuning of the initial conditions. With a convenient choice of signs of the terms in the Kahler potential, we can reconcile the inflationary scale with the supersymmetric grand unified theory scale. All the current data on the inflationary observables are readily reproduced. Inflation is followed by non-thermal leptogenesis via the decay of the right-handed neutrinos emerging from the decay of the inflaton and any possible washout of the lepton asymmetry is avoided thanks to the violation of partial Yukawa unification. The extra superfields also assist us in reducing the reheat temperature so as to satisfy the gravitino constraint. The observed baryon asymmetry of the universe is naturally reproduced consistently with the neutrino oscillation parameters.
Unsupervised edge map scoring: a statistical complexity approach<|sep|>We propose a new Statistical Complexity Measure (SCM) to qualify edge maps without Ground Truth (GT) knowledge. The measure is the product of two indices, an \emph{Equilibrium} index $\mathcal{E}$ obtained by projecting the edge map into a family of edge patterns, and an \emph{Entropy} index $\mathcal{H}$, defined as a function of the Kolmogorov Smirnov (KS) statistic. This new measure can be used for performance characterization which includes: (i)~the specific evaluation of an algorithm (intra-technique process) in order to identify its best parameters, and (ii)~the comparison of different algorithms (inter-technique process) in order to classify them according to their quality. Results made over images of the South Florida and Berkeley databases show that our approach significantly improves over Pratt's Figure of Merit (PFoM) which is the objective reference-based edge map evaluation standard, as it takes into account more features in its evaluation.
Towards a General Framework for Formal Reasoning about Java Bytecode Transformation<|sep|>Program transformation has gained a wide interest since it is used for several purposes: altering semantics of a program, adding features to a program or performing optimizations. In this paper we focus on program transformations at the bytecode level. Because these transformations may introduce errors, our goal is to provide a formal way to verify the update and establish its correctness. The formal framework presented includes a definition of a formal semantics of updates which is the base of a static verification and a scheme based on Hoare triples and weakest precondition calculus to reason about behavioral aspects in bytecode transformation
The Encyclopedic Reference of Critical Points for SO(8)-Gauged N=8 Supergravity Part 1: Cosmological Constants in the Range -\Lambda/g^2 \in [6;14.7)<|sep|>This article is part of a collection that strives to collect and provide in an unified form data about all the critical points on the scalar manifold of SO(8)-gauged N=8 supergravity in four dimensions known so far. The vast majority of these were obtained using the enhanced sensitivity backpropagation method introduced by the author in 2008. This part of the collection describes 41 critical points, 7 of which have been known for more than two decades, 8 of which were discovered recently, and 26 are novel. The residual gauge symmetries of these 41 critical points (likely) are SO(8) with N=8 SUSY (1x), SO(7) (2x), SU(4) (1x), G2 with N=1 SUSY (1x), SU(3)xU(1) with N=2 SUSY (1x), SO(3)xSO(3) (2x), SO(3)xU(1)xU(1) (1x), SO(3)xU(1) (3x), SO(3) (3x), U(1)xU(1) with N=1 SUSY (1x), U(1)xU(1) without SUSY (4x), U(1) (11x), and None (10x). Analytic conjectures (not yet proven but overwhelmingly likely correct) are given for the locations and cosmological constants of some critical points.
Nucleation of superconductivity and vortex matter in superconductor - ferromagnet hybrids<|sep|>The theoretical and experimental results concerning the thermodynamical and low-frequency transport properties of hybrid structures, consisting of spatially-separated conventional low-temperature superconductor (S) and ferromagnet (F), is reviewed. Since the superconducting and ferromagnetic parts are assumed to be electrically insulated, no proximity effect is present and thus the interaction between both subsystems is through their respective magnetic stray fields. Depending on the temperature range and the value of the external field H_{ext}, different behavior of such S/F hybrids is anticipated. Rather close to the superconducting phase transition line, when the superconducting state is only weakly developed, the magnetization of the ferromagnet is solely determined by the magnetic history of the system and it is not influenced by the field generated by the supercurrents. In contrast to that, the nonuniform magnetic field pattern, induced by the ferromagnet, strongly affect the nucleation of superconductivity leading to an exotic dependence of the critical temperature T_{c} on H_{ext}. Deeper in the superconducting state the effect of the screening currents cannot be neglected anymore. In this region of the phase diagram various aspects of the interaction between vortices and magnetic inhomogeneities are discussed. In the last section we briefly summarize the physics of S/F hybrids when the magnetization of the ferromagnet is no longer fixed but can change under the influence of the superconducting currents. As a consequence, the superconductor and ferromagnet become truly coupled and the equilibrium configuration of this "soft" S/F hybrids requires rearrangements of both, superconducting and ferromagnetic characteristics, as compared with "hard" S/F structures.
Group classification of steady two-dimensional boundary-layer stagnation-point flow equations<|sep|>Lie symmetry group method is applied to study the boundary-layer equations for two-dimensional steady flow of an incompressible, viscous fluid near a stagnation point at a heated stretching sheet placed in a porous medium equation. The symmetry group and its optimal system are given, and group invariant solutions associated to the symmetries are obtained. Finally the structure of the Lie algebra symmetries is determined.
Deterministic and Probabilistic Binary Search in Graphs<|sep|>We consider the following natural generalization of Binary Search: in a given undirected, positively weighted graph, one vertex is a target. The algorithm's task is to identify the target by adaptively querying vertices. In response to querying a node $q$, the algorithm learns either that $q$ is the target, or is given an edge out of $q$ that lies on a shortest path from $q$ to the target. We study this problem in a general noisy model in which each query independently receives a correct answer with probability $p > \frac{1}{2}$ (a known constant), and an (adversarial) incorrect one with probability $1-p$. Our main positive result is that when $p = 1$ (i.e., all answers are correct), $\log_2 n$ queries are always sufficient. For general $p$, we give an (almost information-theoretically optimal) algorithm that uses, in expectation, no more than $(1 - \delta)\frac{\log_2 n}{1 - H(p)} + o(\log n) + O(\log^2 (1/\delta))$ queries, and identifies the target correctly with probability at leas $1-\delta$. Here, $H(p) = -(p \log p + (1-p) \log(1-p))$ denotes the entropy. The first bound is achieved by the algorithm that iteratively queries a 1-median of the nodes not ruled out yet; the second bound by careful repeated invocations of a multiplicative weights algorithm. Even for $p = 1$, we show several hardness results for the problem of determining whether a target can be found using $K$ queries. Our upper bound of $\log_2 n$ implies a quasipolynomial-time algorithm for undirected connected graphs; we show that this is best-possible under the Strong Exponential Time Hypothesis (SETH). Furthermore, for directed graphs, or for undirected graphs with non-uniform node querying costs, the problem is PSPACE-complete. For a semi-adaptive version, in which one may query $r$ nodes each in $k$ rounds, we show membership in $\Sigma_{2k-1}$ in the polynomial hierarchy, and hardness for $\Sigma_{2k-5}$.
Personalization of Itineraries search using Ontology and Rules to Avoid Congestion in Urban Areas<|sep|>There is a relatively small amount of research covering urban freight movements. Most research dealing with the subject of urban mobility focuses on passenger vehicles, not commercial vehicles hauling freight. However, in many ways, urban freight transport contributes to congestion, air pollution, noise, accident and more fuel consumption which raises logistic costs, and hence the price of products. The main focus of this paper is to propose a new solution for congestion in order to improve the distribution process of goods in urban areas and optimize transportation cost, time of delivery, fuel consumption, and environmental impact, while guaranteeing the safety of goods and passengers. A novel technique for personalization in itinerary search based on city logistics ontology and rules is proposed to overcome this problem. The integration of personalization plays a key role in capturing or inferring the needs of each stakeholder (user), and then satisfying these needs in a given context. The proposed approach is implemented to an itinerary search problem for freight transportation in urban areas to demonstrate its ability in facilitating intelligent decision support by retrieving the best itinerary that satisfies the most users preferences (stakeholders).
Medium polarization and finite size effects on the superfluidity of the inner crust of neutron stars<|sep|>The 1S0 pairing gap associated with the inner crust of a neutron star is calculated, taking into account the coexistence of the nuclear lattice with the sea of free neutrons (finite size effects), as well as medium polarization effects associated with the exchange of density and spin fluctuations. Both effects are found to be important and to lead to an overall quenching of the pairing gap. This result, whose quantitative value is dependent on the effective interaction used to generate the single-particle levels, is a consequence of the balance between the attractive (repulsive) induced interaction arising from the exchange of density (spin) modes, balance which in turn is influenced by the presence of the protons and depends on the single-particle structure of the system.
A scaling relation between proton-nucleus and nucleus-nucleus collisions<|sep|>It is recently discovered that at high multiplicy, the proton-nucleus ($pA$) collisions give rise to two particle correlations that are strikingly similar to those of nucleus-nucleus ($AA$) collisions at the same multiplicity, although the system size is smaller in $pA$. Using an independent cluster model and a simple conformal scaling argument, where the ratio of the mean free path to the system size stays constant at fixed multiplicity, we argue that flow in $pA$ emerges as a collective response to the fluctuations in the position of clusters, just like in $AA$ collisions. With several physically motivated and parameter free rescalings of the recent LHC data, we show that this simple model captures the essential physics of elliptic and triangular flow in $pA$ collisions.
Partial Stability Concept in Extremum Seeking Problems<|sep|>The paper deals with the extremum seeking problem for a class of cost functions depending only on a part of state variables of a control system. This problem is related to the concept of partial asymptotic stability and analyzed by Lyapunov's direct method and averaging schemes. Sufficient conditions for the practical partial stability of a system with oscillating inputs are derived with the use of Lie bracket approximation techniques. These conditions are exploited to describe a broad class of extremum-seeking controllers ensuring the partial stability of the set of minima of a cost function. The obtained theoretical results are illustrated by the Brockett integrator and rotating rigid body.
Activation Functions: Dive into an optimal activation function<|sep|>Activation functions have come up as one of the essential components of neural networks. The choice of adequate activation function can impact the accuracy of these methods. In this study, we experiment for finding an optimal activation function by defining it as a weighted sum of existing activation functions and then further optimizing these weights while training the network. The study uses three activation functions, ReLU, tanh, and sin, over three popular image datasets, MNIST, FashionMNIST, and KMNIST. We observe that the ReLU activation function can easily overlook other activation functions. Also, we see that initial layers prefer to have ReLU or LeakyReLU type of activation functions, but deeper layers tend to prefer more convergent activation functions.
Deformable Video Transformer<|sep|>Video transformers have recently emerged as an effective alternative to convolutional networks for action classification. However, most prior video transformers adopt either global space-time attention or hand-defined strategies to compare patches within and across frames. These fixed attention schemes not only have high computational cost but, by comparing patches at predetermined locations, they neglect the motion dynamics in the video. In this paper, we introduce the Deformable Video Transformer (DVT), which dynamically predicts a small subset of video patches to attend for each query location based on motion information, thus allowing the model to decide where to look in the video based on correspondences across frames. Crucially, these motion-based correspondences are obtained at zero-cost from information stored in the compressed format of the video. Our deformable attention mechanism is optimised directly with respect to classification performance, thus eliminating the need for suboptimal hand-design of attention strategies. Experiments on four large-scale video benchmarks (Kinetics-400, Something-Something-V2, EPIC-KITCHENS and Diving-48) demonstrate that, compared to existing video transformers, our model achieves higher accuracy at the same or lower computational cost, and it attains state-of-the-art results on these four datasets.
Herschel reveals a T_dust-unbiased selection of z~2 ULIRGs<|sep|>Using Herschel PACS and SPIRE observations of Lockman Hole-North and GOODS-N as part of the HerMES project, we explore the far-IR properties of a sample of mid-IR selected starburst dominated ultra-luminous infrared galaxies (ULIRGs) at z ~ 2. The selection of the sample is based on the detection of the stellar bump that appears in the SED of star-forming galaxies at 1.6um. We derive robust estimates of infrared luminosities (L_IR) and dust temperatures (T_d) of the population and find that while the luminosities in our sample span less than an order of magnitude (12.24< log(L_IR/Lo) < 12.94), they cover a wide range of dust temperatures (25< T_d < 62 K). Galaxies in our sample range from those that are as cold as high-z sub-millimeter galaxies (SMGs) to those that are as warm as optically faint radio galaxies (OFRGs) and local ULIRGs. Nevertheless, our sample has median T_d=42.3 K, filling the gap between SMGs and OFRGs, bridging the two populations. We demonstrate that a significant fraction of our sample would be missed from ground based (sub)mm surveys (850-1200um) showing that the latter introduce a bias towards the detection of colder sources. We conclude that Herschel} observations, confirm the existence of high-z ULIRGs warmer than SMGs, show that the mid-IR selection of high-z ULIRGs is not T_d-dependent, reveal a large dispersion in T_d of high-z ULIRGs, and provide the means to characterize the bulk of the ULIRG population, free from selection biases introduced by ground based (sub)mm surveys.
Equivalent Model of Transient Gyrotron Cathode Response<|sep|>The gyrotron is the high power millimeter wave source in the electron cyclotron resonance heating system for the tokamak. The cathode power supply is one of the most important ancillary devices for gyrotron. Some interesting transient phenomena about the cathode voltage and the cathode current was found in the gyrotron operation in the electron cyclotron resonance heating system for the Experimental Advanced Superconducting Tokamak. The cathode voltage drops to about 10% of the original value by about 90 {\mu}s in the overcurrent case, which is much longer than the 25 {\mu}s in the normal case. In order to explain these phenomena, an equivalent circuit model of the magnetron injection gun was proposed. The equivalent circuit is composed of parallel resistors and capacitors, and it can explain the test results very well. Using the equivalent circuit model to analyze gyrotrons may become an effective means of developing and operating an electron cyclotron heating system.
Diffusion maps for changing data<|sep|>Graph Laplacians and related nonlinear mappings into low dimensional spaces have been shown to be powerful tools for organizing high dimensional data. Here we consider a data set X in which the graph associated with it changes depending on some set of parameters. We analyze this type of data in terms of the diffusion distance and the corresponding diffusion map. As the data changes over the parameter space, the low dimensional embedding changes as well. We give a way to go between these embeddings, and furthermore, map them all into a common space, allowing one to track the evolution of X in its intrinsic geometry. A global diffusion distance is also defined, which gives a measure of the global behavior of the data over the parameter space. Approximation theorems in terms of randomly sampled data are presented, as are potential applications.
Strong lensing with superfluid dark matter<|sep|>In superfluid dark matter the exchange of phonons can create an additional force that has an effect similar to Modified Newtonian Dynamics (MOND). To test whether this hypothesis is compatible with observation, we study a set of strong gravitational lenses from the SLACS survey and check whether the measurements can be explained by a superfluid in the central region of galaxies. Concretely, we try to simultaneously fit each lens's Einstein radius and velocity dispersion with a spherically symmetric density profile of a fluid that has both a normal and a superfluid component. We demonstrate that we can successfully fit all galaxies except one, and that the fits have reasonable stellar mass-to-light-ratios. We conclude that strong gravitational lensing does not pose a challenge for the idea that superfluid dark matter mimics modified gravity.
The splitting of electrons and violation of the Luttinger sum rule<|sep|>We obtain a controlled description of a strongly correlated regime of electronic behaviour. We begin by arguing that there are two ways to characterise the electronic degree of freedom, either by the canonical fermion algebra or the graded Lie algebra su(2|2). The first underlies the Fermi liquid description of correlated matter, and we identify a novel regime governed by the latter. We exploit an exceptional central extension of su(2|2) to employ a perturbative scheme recently developed by Shastry, and obtain a series of successive approximations for the electronic Green's function. We then focus on the leading approximation, which reveals a splitting in two of the electronic dispersion. The Luttinger sum rule is violated, and a Mott metal-insulator transition is exhibited. We offer a perspective.
Caching Placement in Stochastic Wireless Caching Helper Networks: Channel Selection Diversity via Caching<|sep|>Content delivery success in wireless caching helper networks depends mainly on cache-based channel selection diversity and network interference. For given channel fading and network geometry, both channel selection diversity and network interference dynamically vary according to what and how the caching helpers cache at their finite storage space. We study probabilistic content placement (or caching placement) to desirably control cache-based channel selection diversity and network interference in a stochastic wireless caching helper network, with sophisticated considerations of wireless fading channels, interactions among multiple users such as interference and loads at caching helpers, and arbitrary memory size. Using stochastic geometry, we derive optimal caching probabilities in closed form to maximize the average success probability of content delivery and propose an efficient algorithm to find the solution in a noise-limited network. In an interference-limited network, based on a lower bound of the average success probability of content delivery, we find near-optimal caching probabilities in closed form to control the channel selection diversity and the network interference. We numerically verify that the proposed content placement is superior to other comparable content placement strategies.
Halo Occupation Distributions of Moderate X-ray AGNs through Major and Minor Mergers in a $\Lambda$-CDM Cosmology<|sep|>Motivated by recent inferred form of the halo occupation distribution (HOD) of X-ray selected AGNs, in the COSMOS field by Allevato et al. (2012), we investigate the HOD properties of moderate X-ray luminosity Active Galactic Nuclei (mXAGNs) using a simple model based on merging activity between dark matter halos (DMHs) in a $\Lambda$-CDM cosmology. The HODs and number densities of the simulated mXAGNs at $z=0.5$, under the above scenarios to compare with Allevato et al. (2012) results. We find that the simulated HODs of major and minor mergers, and the observed for mXAGNs are consistent among them. Our main result is that minor mergers, contrary to what one might expect, can play an important role in activity mAGNs.
Optomechanical atom-cavity interaction in the sub-recoil regime<|sep|>We study the optomechanical interaction of a Bose-Einstein condensate with a single longitudinal mode of an ultra-high finesse standing wave optical resonator. As a unique feature the resonator combines three extreme regimes, previously not realized together, i.e., strong cooperative coupling, cavity dominated scattering with a Purcell factor far above unity, and sub-recoil resolution provided by a cavity damping rate smaller than four times the single photon recoil frequency. We present experimental observations in good agreement with a two-mode model predicting highly non-linear dynamics with signatures as bistability, hysteresis, persistent oscillations, and superradiant back-scattering instabilities.
Effect of magnetic anisotropy relaxation on laser-induced magnetization precession in thin galfenol films<|sep|>The rate and pathways of relaxation of a magnetic medium to its equilibrium following excitation with intense and short laser pulses are the key ingredients of ultrafast optical control of spins. Here we study experimentally the evolution of the magnetization and magnetic anisotropy of thin films of a ferromagnetic metal galfenol (Fe$_{0.81}$Ga$_{0.19}$) resulting from excitation with a femtosecond laser pulse. From the temporal evolution of the hysteresis loops we deduce that the magnetization $M_S$ and magnetic anisotropy parameters $K$ recover within a nanosecond, and the ratio between $K$ and $M_S$ satisfies the thermal equilibrium's power law in the whole time range spanning from a few picoseconds to 3 nanoseconds. We further use the experimentally obtained relaxation times of $M_S$ and $K$ to analyze the laser-induced precession and demonstrate how they contribute to its frequency evolution at the nanosecond timescale.
Detecting intermediate mass black holes in globular clusters with machine learning<|sep|>Mergers of stellar-mass black holes were recently observed in the gravitational wave window opened by LIGO. This puts the spotlight on dense stellar systems and their ability to create intermediate-mass black holes (IMBHs) through repeated merging. Unfortunately, attempts at direct and indirect IMBH detection in star clusters in the nearby universe have proven inconclusive as of now. Indirect detection methods attempt to constrain IMBHs through their effect on star cluster photometric and kinematic observables. They are usually based on looking for a specific, physically motivated signature. While this approach is justified, it may be suboptimal in its usage of the available data. Here I present a new indirect detection method, based on machine learning, that is unaffected by these restrictions. I reduce the scientific question whether a star cluster hosts an IMBH to a classification problem in the machine learning framework. I present preliminary results to illustrate how machine learning models are trained on simulated datasets and measure their performance on previously unseen, simulated data.
Invertible Program Restructurings for Continuing Modular Maintenance<|sep|>When one chooses a main axis of structural decompostion for a software, such as function- or data-oriented decompositions, the other axes become secondary, which can be harmful when one of these secondary axes becomes of main importance. This is called the tyranny of the dominant decomposition. In the context of modular extension, this problem is known as the Expression Problem and has found many solutions, but few solutions have been proposed in a larger context of modular maintenance. We solve the tyranny of the dominant decomposition in maintenance with invertible program transformations. We illustrate this on the typical Expression Problem example. We also report our experiments with Java and Haskell programs and discuss the open problems with our approach.
Global Reasoning over Database Structures for Text-to-SQL Parsing<|sep|>State-of-the-art semantic parsers rely on auto-regressive decoding, emitting one symbol at a time. When tested against complex databases that are unobserved at training time (zero-shot), the parser often struggles to select the correct set of database constants in the new database, due to the local nature of decoding. In this work, we propose a semantic parser that globally reasons about the structure of the output query to make a more contextually-informed selection of database constants. We use message-passing through a graph neural network to softly select a subset of database constants for the output query, conditioned on the question. Moreover, we train a model to rank queries based on the global alignment of database constants to question words. We apply our techniques to the current state-of-the-art model for Spider, a zero-shot semantic parsing dataset with complex databases, increasing accuracy from 39.4% to 47.4%.
Thresholding Bandit for Dose-ranging: The Impact of Monotonicity<|sep|>We analyze the sample complexity of the thresholding bandit problem, with and without the assumption that the mean values of the arms are increasing. In each case, we provide a lower bound valid for any risk $\delta$ and any $\delta$-correct algorithm; in addition, we propose an algorithm whose sample complexity is of the same order of magnitude for small risks. This work is motivated by phase 1 clinical trials, a practically important setting where the arm means are increasing by nature, and where no satisfactory solution is available so far.
Relative periodic orbits form the backbone of turbulent pipe flow<|sep|>Chaotic dynamics of low-dimensional systems, such as Lorenz or R\"ossler flows, is guided by the infinity of periodic orbits embedded in their strange attractors. Whether this also be the case for the infinite-dimensional dynamics of Navier--Stokes equations has long been speculated, and is a topic of ongoing study. Periodic and relative periodic solutions have been shown to be involved in transitions to turbulence. Their relevance to turbulent dynamics---specifically, whether periodic orbits play the same role in high-dimensional nonlinear systems like the Navier--Stokes equations as they do in lower-dimensional systems---is the focus of the present investigation. We perform here a detailed study of pipe flow relative periodic orbits with energies and mean dissipations close to turbulent values. We outline several approaches to reduction of the translational symmetry of the system. We study pipe flow in a minimal computational cell, and report a library of invariant solutions found with the aid of the method of slices. Detailed study of the unstable manifolds of a sample of these solutions is consistent with the picture that relative periodic orbits are embedded in the chaotic saddle and that they guide the turbulent dynamics.
Application of machine learning algorithms to the study of noise artifacts in gravitational-wave data<|sep|>The sensitivity of searches for astrophysical transients in data from the LIGO is generally limited by the presence of transient, non-Gaussian noise artifacts, which occur at a high-enough rate such that accidental coincidence across multiple detectors is non-negligible. Furthermore, non-Gaussian noise artifacts typically dominate over the background contributed from stationary noise. These "glitches" can easily be confused for transient gravitational-wave signals, and their robust identification and removal will help any search for astrophysical gravitational-waves. We apply Machine Learning Algorithms (MLAs) to the problem, using data from auxiliary channels within the LIGO detectors that monitor degrees of freedom unaffected by astrophysical signals. The number of auxiliary-channel parameters describing these disturbances may also be extremely large; an area where MLAs are particularly well-suited. We demonstrate the feasibility and applicability of three very different MLAs: Artificial Neural Networks, Support Vector Machines, and Random Forests. These classifiers identify and remove a substantial fraction of the glitches present in two very different data sets: four weeks of LIGO's fourth science run and one week of LIGO's sixth science run. We observe that all three algorithms agree on which events are glitches to within 10% for the sixth science run data, and support this by showing that the different optimization criteria used by each classifier generate the same decision surface, based on a likelihood-ratio statistic. Furthermore, we find that all classifiers obtain similar limiting performance, suggesting that most of the useful information currently contained in the auxiliary channel parameters we extract is already being used.
On the Complexity of Robust PCA and $\ell_1$-norm Low-Rank Matrix Approximation<|sep|>The low-rank matrix approximation problem with respect to the component-wise $\ell_1$-norm ($\ell_1$-LRA), which is closely related to robust principal component analysis (PCA), has become a very popular tool in data mining and machine learning. Robust PCA aims at recovering a low-rank matrix that was perturbed with sparse noise, with applications for example in foreground-background video separation. Although $\ell_1$-LRA is strongly believed to be NP-hard, there is, to the best of our knowledge, no formal proof of this fact. In this paper, we prove that $\ell_1$-LRA is NP-hard, already in the rank-one case, using a reduction from MAX CUT. Our derivations draw interesting connections between $\ell_1$-LRA and several other well-known problems, namely, robust PCA, $\ell_0$-LRA, binary matrix factorization, a particular densest bipartite subgraph problem, the computation of the cut norm of $\{-1,+1\}$ matrices, and the discrete basis problem, which we all prove to be NP-hard.
Multi-view Deep Features for Robust Facial Kinship Verification<|sep|>Automatic kinship verification from facial images is an emerging research topic in machine learning community. In this paper, we proposed an effective facial features extraction model based on multi-view deep features. Thus, we used four pre-trained deep learning models using eight features layers (FC6 and FC7 layers of each VGG-F, VGG-M, VGG-S and VGG-Face models) to train the proposed Multilinear Side-Information based Discriminant Analysis integrating Within Class Covariance Normalization (MSIDA+WCCN) method. Furthermore, we show that how can metric learning methods based on WCCN method integration improves the Simple Scoring Cosine similarity (SSC) method. We refer that we used the SSC method in RFIW'20 competition using the eight deep features concatenation. Thus, the integration of WCCN in the metric learning methods decreases the intra-class variations effect introduced by the deep features weights. We evaluate our proposed method on two kinship benchmarks namely KinFaceW-I and KinFaceW-II databases using four Parent-Child relations (Father-Son, Father-Daughter, Mother-Son and Mother-Daughter). Thus, the proposed MSIDA+WCCN method improves the SSC method with 12.80% and 14.65% on KinFaceW-I and KinFaceW-II databases, respectively. The results obtained are positively compared with some modern methods, including those that rely on deep learning.
A High Reliability Survey of Discrete Epoch of Reionization Foreground Sources in the MWA EoR0 Field<|sep|>Detection of the Epoch of Reionization HI signal requires a precise understanding of the intervening galaxies and AGN, both for instrumental calibration and foreground removal. We present a catalogue of 7394 extragalactic sources at 182 MHz detected in the RA=0 field of the Murchison Widefield Array Epoch of Reionization observation programme. Motivated by unprecedented requirements for precision and reliability we develop new methods for source finding and selection. We apply machine learning methods to self-consistently classify the relative reliability of 9490 source candidates. A subset of 7466 are selected based on reliability class and signal-to-noise ratio criteria. These are statistically cross-matched to four other radio surveys using both position and flux density information. We find 7369 sources to have confident matches, including 90 partially resolved sources that split into a total of 192 sub-components. An additional 25 unmatched sources are included as new radio detections. The catalogue sources have a median spectral index of -0.85. Spectral flattening is seen toward lower frequencies with a median of -0.71 predicted at 182 MHz. The astrometric error is 7 arcsec. compared to a 2.3 arcmin. beam FWHM. The resulting catalogue covers approximately 1400 sq. deg. and is complete to approximately 80 mJy within half beam power. This provides the most reliable discrete source sky model available to date in the MWA EoR0 field for precision foreground subtraction.
Negative Even Grade mKdV Hierarchy and its Soliton Solutions<|sep|>In this paper we provide an algebraic construction for the negative even mKdV hierarchy which gives rise to time evolutions associated to even graded Lie algebraic structure. We propose a modification of the dressing method, in order to incorporate a non-trivial vacuum configuration and construct a deformed vertex operator for $\hat{sl}(2)$, that enable us to obtain explicit and systematic solutions for the whole negative even grade equations.
Point Proposal Network for Reconstructing 3D Particle Endpoints with Sub-Pixel Precision in Liquid Argon Time Projection Chambers<|sep|>Liquid Argon Time Projection Chambers (LArTPC) are particle imaging detectors recording 2D or 3D images of trajectories of charged particles. Identifying points of interest in these images, namely the initial and terminal points of track-like particle trajectories such as muons and protons, and the initial points of electromagnetic shower-like particle trajectories such as electrons and gamma rays, is a crucial step of identifying and analyzing these particles and impacts the inference of physics signals such as neutrino interaction. The Point Proposal Network is designed to discover these specific points of interest. The algorithm predicts with a sub-voxel precision their spatial location, and also determines the category of the identified points of interest. Using as a benchmark the PILArNet public LArTPC data sample in which the voxel resolution is 3mm/voxel, our algorithm successfully predicted 96.8% and 97.8% of 3D points within a distance of 3 and 10~voxels from the provided true point locations respectively. For the predicted 3D points within 3 voxels of the closest true point locations, the median distance is found to be 0.25 voxels, achieving the sub-voxel level precision. In addition, we report our analysis of the mistakes where our algorithm prediction differs from the provided true point positions by more than 10~voxels. Among 50 mistakes visually scanned, 25 were due to the definition of true position location, 15 were legitimate mistakes where a physicist cannot visually disagree with the algorithm's prediction, and 10 were genuine mistakes that we wish to improve in the future. Further, using these predicted points, we demonstrate a simple algorithm to cluster 3D voxels into individual track-like particle trajectories with a clustering efficiency, purity, and Adjusted Rand Index of 96%, 93%, and 91% respectively.
Randomized Scheduling of Real-Time Traffic in Wireless Networks Over Fading Channels<|sep|>Despite the rich literature on scheduling algorithms for wireless networks, algorithms that can provide deadline guarantees on packet delivery for general traffic and interference models are very limited. In this paper, we study the problem of scheduling real-time traffic under a conflict-graph interference model with unreliable links due to channel fading. Packets that are not successfully delivered within their deadlines are of no value. We consider traffic (packet arrival and deadline) and fading (link reliability) processes that evolve as an unknown finite-state Markov chain. The performance metric is efficiency ratio which is the fraction of packets of each link which are delivered within their deadlines compared to that under the optimal (unknown) policy. We first show a conversion result that shows classical non-real-time scheduling algorithms can be ported to the real-time setting and yield a constant efficiency ratio, in particular, Max-Weight Scheduling (MWS) yields an efficiency ratio of 1/2. We then propose randomized algorithms that achieve efficiency ratios strictly higher than 1/2, by carefully randomizing over the maximal schedules. We further propose low-complexity and myopic distributed randomized algorithms, and characterize their efficiency ratio. Simulation results are presented that verify that randomized algorithms outperform classical algorithms such as MWS and GMS.
X-ray constraints on the spectral energy distribution of the $z=5.18$ blazar SDSS J013127.34-032100.1<|sep|>We report on X-ray measurements constraining the spectral energy distribution (SED) of the high-redshift $z=5.18$ blazar SDSS J013127.34$-$032100.1 with new XMM-Newton and NuSTAR exposures. The blazar's X-ray spectrum is well fit by a power law with $\Gamma=1.9$ and $N_{\rm H}=1.1\times10^{21}\rm \ cm^{-2}$, or a broken power law with $\Gamma_l=0.5$, $\Gamma_h=1.8$, and a break energy $E_b=0.7$ keV for an expected absorbing column density of $N_{\rm H}=3.6\times 10^{20}\rm \ cm^{-2}$, supported by spectral fitting of a nearby bright source. No additional spectral break is found at higher X-ray energies (1-30 keV). We supplement the X-ray data with lower-energy radio-to-optical measurements and Fermi-LAT gamma-ray upper limits, construct broadband SEDs of the source, and model the SEDs using a synchro-Compton scenario. This modeling constrains the bulk Doppler factor of the jets to $\ge$7 and $\ge$6 (90%) for the low- and high-$N_{\rm H}$ SEDs, respectively. The corresponding beaming implies $\ge$130 (low $N_{\rm H}$) or $\ge$100 (high $N_{\rm H}$) high-spin supermassive black holes similar to J0131 exist at similar redshifts.
Boundary layers and the vanishing viscosity limit for incompressible 2D flow<|sep|>This manuscript is a survey on results related to boundary layers and the vanishing viscosity limit for incompressible flow. It is the lecture notes for a 10 hour minicourse given at the Morningside Center, Academia Sinica, Beijing, PRC from 11/28 to 12/07, 2007. The main topics covered are: a derivation of Prandtl's boundary layer equation; an outline of the rigorous theory of Prandtl's equation, without proofs; Kato's criterion for the vanishing viscosity limit; the vanishing viscosity limit with Navier friction condition; rigorous boundary layer theory for the Navier friction condition and boundary layers for flows in a rotating cylinder.
Transverse Single Spin Asymmetries of Heavy Flavor Electrons and Charged Pions in 200 GeV $p+p^{\uparrow}$ Collisions at Midrapidity<|sep|>Transverse single spin asymmetries of particles produced in $p+p^{\uparrow}$ collisions provide insight on the partonic spin and momentum structure of hadrons; heavy flavor electrons provide access to initial state spin-momentum correlations of gluons in the proton, while charged pions provide access to initial and final state transverse spin effects. Charged particles are measured at midrapidity at PHENIX using the silicon vertex detector and central arm spectrometer, made of an electromagnetic calorimeter, a ring-imaging Cherenkov detector, and drift and pad chambers. Recent results for both electron and charged pion measurements from the 2015 running period will be presented.
A Computational Framework for Motor Skill Acquisition<|sep|>There have been numerous attempts in explaining the general learning behaviours by various cognitive models. Multiple hypotheses have been put further to qualitatively argue the best-fit model for motor skill acquisition task and its variations. In this context, for a discrete sequence production (DSP) task, one of the most insightful models is Verwey's Dual Processor Model (DPM). It largely explains the learning and behavioural phenomenon of skilled discrete key-press sequences without providing any concrete computational basis of reinforcement. Therefore, we propose a quantitative explanation for Verwey's DPM hypothesis by experimentally establishing a general computational framework for motor skill learning. We attempt combining the qualitative and quantitative theories based on a best-fit model of the experimental simulations of variations of dual processor models. The fundamental premise of sequential decision making for skill learning is based on interacting model-based (MB) and model-free (MF) reinforcement learning (RL) processes. Our unifying framework shows the proposed idea agrees well to Verwey's DPM and Fitts' three phases of skill learning. The accuracy of our model can further be validated by its statistical fit with the human-generated data on simple environment tasks like the grid-world.
Optimal multi-period dispatch of distributed energy resources in unbalanced distribution feeders<|sep|>This paper develops an efficient algorithm for the multi-period optimal dispatch of deterministic inverter-interfaced energy storage in an unbalanced distribution feeder with significant solar PV penetration. The three-phase, non-convex loss-minimization problem is formulated as a convex second order cone program (SOCP) for the dispatch of batteries in a receding-horizon fashion in order to counter against the variable renewable net-load generation. The solution of the SOCP is used to initialize a nonlinear program (NLP) in order to ensure a physically realizable solution. The phenomenon of simultaneous charging and discharging of batteries is rigorously analyzed and conditions are derived that guarantee it is avoided. Simulation scenarios are implemented with GridLab-D for the IEEE-13 and IEEE-123node test feeders and illustrate not only AC feasibility of the solution, but also near-optimal performance and solve-times within a minute.
Clustering in $^{18}$O -- absolute determination of branching ratios via high-resolution particle spectroscopy<|sep|>The determination of absolute branching ratios for high-energy states in light nuclei is an important and useful tool for probing the underlying nuclear structure of individual resonances: for example, in establishing the tendency of an excited state towards $\alpha$-cluster structure. Difficulty arises in measuring these branching ratios due to similarities in available decay channels, such as ($\mathbf{^{18}}$O,$\mathbf{n}$) and ($\mathbf{^{18}}$O,$\mathbf{2n}$), as well as differences in geometric efficiencies due to population of bound excited levels in daughter nuclei. Methods are presented using Monte Carlo techniques to overcome these issues.
The ACS Survey of Galactic Globular Clusters. VII. Relative Ages<|sep|>The ACS Survey of Galactic Globular Clusters is a Hubble Space Telescope (HST) Treasury program designed to provide a new large, deep and homogeneous photometric database. Based on observations from this program, we have measured precise relative ages for a sample of 64 Galactic globular clusters by comparing the relative position of the clusters' main sequence turn offs, using main-sequence fitting to cross-compare clusters within the sample. This method provides relative ages to a formal precision of 2-7%. We demonstrate that the calculated relative ages are independent of the choice of theoretical model. We find that the Galactic globular cluster sample can be divided into two groups -- a population of old clusters with an age dispersion of ~5% and no age-metallicity relation, and a group of younger clusters with an age-metallicity relation similar to that of the globular clusters associated with the Sagittarius dwarf galaxy. These results are consistent with the Milky Way halo having formed in two phases or processes. The first one would be compatible with a rapid (<0.8 Gyr) assembling process of the halo, in which the clusters in the old group were formed. Also these clusters could have been formed before reionization in dwarf galaxies that would later merge to build the Milky Way halo as predicted by Lambda-CDM cosmology. However, the galactocentric metallicity gradient shown by these clusters seems difficult to reconcile with the latter. As for the younger clusters, it is very tempting to argue that their origin is related to their formation within Milky Way satellite galaxies that were later accreted, but the origin of the age-metallicity relation remains unclear.
Automatic semantic role labeling on non-revised syntactic trees of journalistic texts<|sep|>Semantic Role Labeling (SRL) is a Natural Language Processing task that enables the detection of events described in sentences and the participants of these events. For Brazilian Portuguese (BP), there are two studies recently concluded that perform SRL in journalistic texts. [1] obtained F1-measure scores of 79.6, using the PropBank.Br corpus, which has syntactic trees manually revised, [8], without using a treebank for training, obtained F1-measure scores of 68.0 for the same corpus. However, the use of manually revised syntactic trees for this task does not represent a real scenario of application. The goal of this paper is to evaluate the performance of SRL on revised and non-revised syntactic trees using a larger and balanced corpus of BP journalistic texts. First, we have shown that [1]'s system also performs better than [8]'s system on the larger corpus. Second, the SRL system trained on non-revised syntactic trees performs better over non-revised trees than a system trained on gold-standard data.
Self-propelled Vicsek particles at low speed and low density<|sep|>We study through numerical simulation the Vicsek model for very low speeds and densities. We consider scalar noise in 2-d and 3-d, and vector noise in 3-d. We focus on the behavior of the critical noise with density and speed, trying to clarify seemingly contradictory earlier results. We find that, for scalar noise, the critical noise is a power law both in density and speed, but although we confirm the density exponent in 2-d, we find a speed exponent different from earlier reports (we consider lower speeds than previous studies). On the other hand, for the vector noise case we find that the dependence of the critical noise cannot be separated as a product of power laws in speed and density. Finally, we study the dependence of the relaxation time with speed and find the same power law in 2-d and 3-d, with and exponent that depends on whether the noise is above or below the critical value.
Facial Soft Biometrics for Recognition in the Wild: Recent Works, Annotation, and COTS Evaluation<|sep|>The role of soft biometrics to enhance person recognition systems in unconstrained scenarios has not been extensively studied. Here, we explore the utility of the following modalities: gender, ethnicity, age, glasses, beard, and moustache. We consider two assumptions: 1) manual estimation of soft biometrics and 2) automatic estimation from two commercial off-the-shelf systems (COTS). All experiments are reported using the labeled faces in the wild (LFW) database. First, we study the discrimination capabilities of soft biometrics standalone. Then, experiments are carried out fusing soft biometrics with two state-of-the-art face recognition systems based on deep learning. We observe that soft biometrics is a valuable complement to the face modality in unconstrained scenarios, with relative improvements up to 40%/15% in the verification performance when using manual/automatic soft biometrics estimation. Results are reproducible as we make public our manual annotations and COTS outputs of soft biometrics over LFW, as well as the face recognition scores.
End-to-End Video Text Spotting with Transformer<|sep|>Recent video text spotting methods usually require the three-staged pipeline, i.e., detecting text in individual images, recognizing localized text, tracking text streams with post-processing to generate final results. These methods typically follow the tracking-by-match paradigm and develop sophisticated pipelines. In this paper, rooted in Transformer sequence modeling, we propose a simple, but effective end-to-end video text DEtection, Tracking, and Recognition framework (TransDETR). TransDETR mainly includes two advantages: 1) Different from the explicit match paradigm in the adjacent frame, TransDETR tracks and recognizes each text implicitly by the different query termed text query over long-range temporal sequence (more than 7 frames). 2) TransDETR is the first end-to-end trainable video text spotting framework, which simultaneously addresses the three sub-tasks (e.g., text detection, tracking, recognition). Extensive experiments in four video text datasets (i.e.,ICDAR2013 Video, ICDAR2015 Video, Minetto, and YouTube Video Text) are conducted to demonstrate that TransDETR achieves state-of-the-art performance with up to around 8.0% improvements on video text spotting tasks. The code of TransDETR can be found at https://github.com/weijiawu/TransDETR.
Adversarial Robustness of Deep Sensor Fusion Models<|sep|>We experimentally study the robustness of deep camera-LiDAR fusion architectures for 2D object detection in autonomous driving. First, we find that the fusion model is usually both more accurate, and more robust against single-source attacks than single-sensor deep neural networks. Furthermore, we show that without adversarial training, early fusion is more robust than late fusion, whereas the two perform similarly after adversarial training. However, we note that single-channel adversarial training of deep fusion is often detrimental even to robustness. Moreover, we observe cross-channel externalities, where single-channel adversarial training reduces robustness to attacks on the other channel. Additionally, we observe that the choice of adversarial model in adversarial training is critical: using attacks restricted to cars' bounding boxes is more effective in adversarial training and exhibits less significant cross-channel externalities. Finally, we find that joint-channel adversarial training helps mitigate many of the issues above, but does not significantly boost adversarial robustness.
Hubble Frontier Field Free-Form Mass Mapping of the Massive Multiple-Merging Cluster MACSJ0717.5+3745<|sep|>We examine the latest data on the cluster MACSJ0717.5+3745 from the Hubble Frontier Fields campaign. The critically lensed area is the largest known of any lens and very irregular making it a challenge for parametric modelling. Using our Free-Form method we obtain an accurate solution, identify here many new sets of multiple images, doubling the number of constraints and improving the reconstruction of the dark matter distribution. Our reconstructed mass map shows several distinct central substructures with shallow density profiles, clarifying earlier work and defining well the relation between the dark matter distribution and the luminous and X-ray peaks within the critically lensed region. Using our free-form method, we are able to meaningfully subtract the mass contribution from cluster members to the deflection field to trace the smoothly distributed cluster dark matter distribution. We find 4 distinct concentrations, 3 of which are coincident with the luminous matter. The fourth peak has a significant offset from both the closest luminous and X-ray peaks. These findings, together with dynamical data from the motions of galaxies and gas will be important for uncovering the potentially important implications of this extremely massive and intriguing system.
Learning shape distributions from large databases of healthy organs: applications to zero-shot and few-shot abnormal pancreas detection<|sep|>We propose a scalable and data-driven approach to learn shape distributions from large databases of healthy organs. To do so, volumetric segmentation masks are embedded into a common probabilistic shape space that is learned with a variational auto-encoding network. The resulting latent shape representations are leveraged to derive zeroshot and few-shot methods for abnormal shape detection. The proposed distribution learning approach is illustrated on a large database of 1200 healthy pancreas shapes. Downstream qualitative and quantitative experiments are conducted on a separate test set of 224 pancreas from patients with mixed conditions. The abnormal pancreas detection AUC reached up to 65.41% in the zero-shot configuration, and 78.97% in the few-shot configuration with as few as 15 abnormal examples, outperforming a baseline approach based on the sole volume.
The future of gravitational theories in the era of the gravitational wave astronomy<|sep|>We discuss the future of gravitational theories in the framework of gravitational wave (GW) astronomy after the recent GW detections (the events GW150914, GW151226, GW170104, GW170814, GW170817 and GW170608). In particular, a calculation of the frequency and angular dependent response function that a GW detector would see if massive modes from f(R) theories or scalar tensor gravity (STG) were present, allowing for sources incident from any direction on the sky, is shown. In addition, through separate theoretical results which do not involve the recent GW detections, we show that f(R) theories of gravity having a third massless mode are ultimately ruled out while there is still room for STG having a third (massive or massless) mode and for f(R) theories of gravity having a third massive mode.
Pointer Graph Networks<|sep|>Graph neural networks (GNNs) are typically applied to static graphs that are assumed to be known upfront. This static input structure is often informed purely by insight of the machine learning practitioner, and might not be optimal for the actual task the GNN is solving. In absence of reliable domain expertise, one might resort to inferring the latent graph structure, which is often difficult due to the vast search space of possible graphs. Here we introduce Pointer Graph Networks (PGNs) which augment sets or graphs with additional inferred edges for improved model generalisation ability. PGNs allow each node to dynamically point to another node, followed by message passing over these pointers. The sparsity of this adaptable graph structure makes learning tractable while still being sufficiently expressive to simulate complex algorithms. Critically, the pointing mechanism is directly supervised to model long-term sequences of operations on classical data structures, incorporating useful structural inductive biases from theoretical computer science. Qualitatively, we demonstrate that PGNs can learn parallelisable variants of pointer-based data structures, namely disjoint set unions and link/cut trees. PGNs generalise out-of-distribution to 5x larger test inputs on dynamic graph connectivity tasks, outperforming unrestricted GNNs and Deep Sets.
An unlikely route to low lattice thermal conductivity: small atoms in a simple layered structure<|sep|>In the design of materials with low lattice thermal conductivity, compounds with high density, low speed of sound, and complexity at either the atomic, nano- or microstructural level are preferred. The layered compound Mg$_3$Sb$_2$ defies these prevailing paradigms, exhibiting lattice thermal conductivity comparable to PbTe and Bi$_2$Te$_3$, despite its low density and simple structure. The excellent thermoelectric performance ($zT$ $\sim$ 1.5) in $n$-type Mg$_3$Sb$_2$ has thus far been attributed to its multi-valley conduction band, while its anomalous thermal properties have been largely overlooked. To explain the origin of the low lattice thermal conductivity of Mg$_3$Sb$_2$, we have used both experimental methods and ab initio phonon calculations to investigate trends in the elasticity, thermal expansion and anharmonicity of $A$Mg$_2Pn_2$ Zintl compounds with $A$ = Mg, Ca, Yb, and $Pn$ = Sb and Bi. Phonon calculations within the quasi-harmonic approximation reveal large mode Gr\"uneisen parameters in Mg$_3$Sb$_2$ compared with isostructural compounds, in particular in transverse acoustic modes involving shearing of adjacent anionic layers. Measurements of the elastic moduli and sound velocity as a function of temperature using resonant ultrasound spectroscopy provide a window into the softening of the acoustic branches at high temperature, confirming their exceptionally high anharmonicity. We attribute the anomalous thermal behavior of Mg$_3$Sb$_2$ to the diminutive size of Mg, which may be too small for the octahedrally-coordinated site, leading to weak, unstable interlayer Mg-Sb bonding. This suggests more broadly that soft shear modes resulting from undersized cations provide a potential route to achieving low lattice thermal conductivity low-density, earth-abundant materials.
Can in-home laboratories foster learning, self-efficacy, and motivation during the COVID-19 pandemic? -- A case study in two engineering programs<|sep|>The COVID-19 pandemic has represented a challenge for higher education in terms to provide quality education despite the lockdown periods, the transformation of the in-person classes to virtual classes, and the demotivation and anxiety that are experimented by the students. Because the basis of engineering is the experimentation through hands-on activities and learning by doing, the lockdown periods and the temporary suspension of the in-person classes and laboratories have meant a problem for educators that try to teach and motivate the students despite the situation. In this context, this study presents an educational methodology based on Problem-Based Learning (PBL) and in-home laboratories in engineering. The methodology was carried out in two phases during 2020, in the academic programs of Industrial Engineering and Technology in Electronics with (n=44) students. The in-home laboratories were sent to the students as part of "kits" with the devices needed in each subject. Besides, due to the difficulties in monitoring the learning process, the students made videos and blogs as a strategy to reinforce their learning and evidence the progress in the courses. The outcomes of the methodology show mainly the following points: (1) An improvement of the academic performance and learning of the students in the courses. (2) A positive influence of the usage of in-home laboratories in motivation, self-efficacy, and reduction of anxiety. (3) Positive correlations between the usage of in-home laboratories, the blogs and videos, and the teacher's feedback for learning, motivation, and self-efficacy. Thus, these results evidence that other alternatives that gather the cognitive and affective learning domains can emerge from engineering to deal with the educational problems produced by the crisis periods.
Non-detection of Contamination by Stellar Activity in the Spitzer Transit Light Curves of TRAPPIST-1<|sep|>We apply the transit light curve self-contamination technique of Morris et al. (2018) to search for the effect of stellar activity on the transits of the ultracool dwarf TRAPPIST-1 with 2018 Spitzer photometry. The self-contamination method fits the transit light curves of planets orbiting spotted stars, allowing the host star to be a source of contaminating positive or negative flux which influences the transit depths but not the ingress/egress durations. We find that none of the planets show statistically significant evidence for self-contamination by bright or dark regions of the stellar photosphere. However, we show that small-scale magnetic activity, analogous in size to the smallest sunspots, could still be lurking in the transit photometry undetected.
A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking<|sep|>Person re identification is a challenging retrieval task that requires matching a person's acquired image across non overlapping camera views. In this paper we propose an effective approach that incorporates both the fine and coarse pose information of the person to learn a discriminative embedding. In contrast to the recent direction of explicitly modeling body parts or correcting for misalignment based on these, we show that a rather straightforward inclusion of acquired camera view and/or the detected joint locations into a convolutional neural network helps to learn a very effective representation. To increase retrieval performance, re-ranking techniques based on computed distances have recently gained much attention. We propose a new unsupervised and automatic re-ranking framework that achieves state-of-the-art re-ranking performance. We show that in contrast to the current state-of-the-art re-ranking methods our approach does not require to compute new rank lists for each image pair (e.g., based on reciprocal neighbors) and performs well by using simple direct rank list based comparison or even by just using the already computed euclidean distances between the images. We show that both our learned representation and our re-ranking method achieve state-of-the-art performance on a number of challenging surveillance image and video datasets. The code is available online at: https://github.com/pse-ecn/pose-sensitive-embedding
An isolated mass gap black hole or neutron star detected with astrometric microlensing<|sep|>We present the analysis of five black hole candidates identified from gravitational microlensing surveys. Hubble Space Telescope astrometric data and densely sampled lightcurves from ground-based microlensing surveys are fit with a single-source, single-lens microlensing model in order to measure the mass and luminosity of each lens and determine if it is a black hole. One of the five targets (OGLE-2011-BLG-0462/MOA-2011-BLG-191 or OB110462 for short) shows a significant $>1$ mas coherent astrometric shift, little to no lens flux, and has an inferred lens mass of 1.6 - 4.4 $M_\odot$. This makes OB110462 the first definitive discovery of a compact object through astrometric microlensing and it is most likely either a neutron star or a low-mass black hole. This compact object lens is relatively nearby (0.70-1.92 kpc) and has a slow transverse motion of $<$30 km/s. OB110462 shows significant tension between models well-fit to photometry vs. astrometry, making it currently difficult to distinguish between a neutron star and a black hole. Additional observations and modeling with more complex system geometries, such as binary sources are needed to resolve the puzzling nature of this object. For the remaining four candidates, the lens masses are $<2 M_\odot$ and they are unlikely to be black holes; two of the four are likely white dwarfs or neutron stars. We compare the full sample of five candidates to theoretical expectations on the number of black holes in the Milky Way ($\sim 10^8$) and find reasonable agreement given the small sample size.
Wormholes in viable $f(R)$ modified theories of gravity and Weak Energy Condition<|sep|>In this work wormholes in viable $f(R)$ gravity models are analysed. We are interested in exact solutions for stress-energy tensor components depending on different shape and redshift functions. Several solutions of gravitational equations for different $f(R)$ models are examined. Found solutions imply no need for exotic material, while this need is implied in the standard general theory of relativity. Simple expression for WEC violation near the throat is derived and analysed. High curvature regime is also discussed, as well as the question of the highest possible values of Ricci scalar for which WEC is not violated near the throat, and corresponding functions are calculated for the several models. The approach here differs from the one that has been common since no additional assumptions to simplify the equations are made, and functions in $f(R)$ models are not taken to be arbitrary functions, but rather a feature of the theory that has to be evaluated on the basis of consistency with observations for the Solar System and cosmological evolution.
Evaluation of nano-frictional and mechanical properties of a novel Langmuir-Blodgett monolayer/self-assembly monolayer composite structure<|sep|>A novel stearic acid (SA)/3-aminopropyltrethoxysilane (APS) composite structure was fabricated using the combined method of the Langmuir-Blodgett technique and self-assembly monolayer (SAM) technique. Its frictional, adhesive properties and interface contact types between the atomic force microscope tip and the samples were evaluated based on Amonton's laws and the general Carpick's transition equation, respectively. The results showed that the tip-sample contacts corresponded to the Johnson-Kendall-Robert/Derjaguin-Muller-Toporov (DMT) transition model for SiO2, APS-SAMs, and the unheated SA-APS composite structure, and for the heated SA-APS bilayer to the DMT model. Frictional forces for the four samples were linearly dependent on external loads at higher loads, and at lower loads they were significantly affected by adhesive forces. Frictional and scratching tests showed that the heated SA-APS composite structure exhibited the best lubricating properties and adhesion resistance ability, and its wear resistance capacity was greatly improved due to the binding-mode conversion from hydrogen bonds to covalent bonds. Thus, this kind of composite bilayer might be promising for applications in the lubrication of nano/microelectromechanical systems. I.
Bayesian timing analysis of giant flare of SGR 1806-20 by RXTE PCA<|sep|>By detecting high frequency quasi-periodic oscillations (QPOs) and estimating frequencies of them during the decaying tail of giant flares from Soft Gamma-ray Repeaters (SGRs) useful constraints for the equation of state (EoS) of superdense matter may be obtained via comparison with theoretical predictions of eigenfrequencies. We used the data collected by the Rossi X-Ray Timing Explorer (RXTE/XTE) Proportional Counter Array (PCA) of a giant flare of SGR 1806-20 on 2004 Dec 27 and applied a Bayesian periodicity detection method (Gregory & Loredo, 1992) for the search of oscillations of transient nature. In addition to the already detected frequencies, we found a few new frequencies (f_{QPOs} ~ 16.9, 21.4, 36.4, 59.0, 116.3 Hz) of oscillations predicted by Colaiuda et al. (2009) based on the APR_{14} EoS (Akmal et al., 1998) for SGR 1806-20.
Testing data types implementations from algebraic specifications<|sep|>Algebraic specifications of data types provide a natural basis for testing data types implementations. In this framework, the conformance relation is based on the satisfaction of axioms. This makes it possible to formally state the fundamental concepts of testing: exhaustive test set, testability hypotheses, oracle. Various criteria for selecting finite test sets have been proposed. They depend on the form of the axioms, and on the possibilities of observation of the implementation under test. This last point is related to the well-known oracle problem. As the main interest of algebraic specifications is data type abstraction, testing a concrete implementation raises the issue of the gap between the abstract description and the concrete representation. The observational semantics of algebraic specifications bring solutions on the basis of the so-called observable contexts. After a description of testing methods based on algebraic specifications, the chapter gives a brief presentation of some tools and case studies, and presents some applications to other formal methods involving datatypes.
Spatial Knowledge Distillation to aid Visual Reasoning<|sep|>For tasks involving language and vision, the current state-of-the-art methods tend not to leverage any additional information that might be present to gather relevant (commonsense) knowledge. A representative task is Visual Question Answering where large diagnostic datasets have been proposed to test a system's capability of answering questions about images. The training data is often accompanied by annotations of individual object properties and spatial locations. In this work, we take a step towards integrating this additional privileged information in the form of spatial knowledge to aid in visual reasoning. We propose a framework that combines recent advances in knowledge distillation (teacher-student framework), relational reasoning and probabilistic logical languages to incorporate such knowledge in existing neural networks for the task of Visual Question Answering. Specifically, for a question posed against an image, we use a probabilistic logical language to encode the spatial knowledge and the spatial understanding about the question in the form of a mask that is directly provided to the teacher network. The student network learns from the ground-truth information as well as the teachers prediction via distillation. We also demonstrate the impact of predicting such a mask inside the teachers network using attention. Empirically, we show that both the methods improve the test accuracy over a state-of-the-art approach on a publicly available dataset.
A Low Complexity Detection Algorithm for SCMA<|sep|>Sparse code multiple access (SCMA) is a new multiple access technique which supports massive connectivity. Compared with the current Long Term Evolution (LTE) system, it enables the overloading of active users on limited orthogonal resources and thus meets the requirement of the fifth generation (5G) wireless networks. However, the computation complexity of existing detection algorithms increases exponentially with $d_f$ (the degree of the resource nodes). Although the codebooks are designed to have low density, the detection still takes considerable time. The parameter $d_f$ must be designed to be very small, which largely limits the choice of codebooks. In this paper, a new detection algorithm is proposed by discretizing the probability distribution functions (PDFs) in the layer nodes (variable nodes). Given $M$ as the size of one codebook, the detection complexity of each resource node (function node) is reduced from $O(d_f M^{d_f})$ to $O(d_f^3 \ln (d_f))$. Its detection accuracy can quickly approach that of the previous detection algorithms with the decrease of sampling interval in discretization.
A Knowledge-Enhanced Recommendation Model with Attribute-Level Co-Attention<|sep|>Deep neural networks (DNNs) have been widely employed in recommender systems including incorporating attention mechanism for performance improvement. However, most of existing attention-based models only apply item-level attention on user side, restricting the further enhancement of recommendation performance. In this paper, we propose a knowledge-enhanced recommendation model ACAM, which incorporates item attributes distilled from knowledge graphs (KGs) as side information, and is built with a co-attention mechanism on attribute-level to achieve performance gains. Specifically, each user and item in ACAM are represented by a set of attribute embeddings at first. Then, user representations and item representations are augmented simultaneously through capturing the correlations between different attributes by a co-attention module. Our extensive experiments over two realistic datasets show that the user representations and item representations augmented by attribute-level co-attention gain ACAM's superiority over the state-of-the-art deep models.
Triple differntial cross section for electron impact ionization of hydrogen atom<|sep|>The electron impact ionization of atomic hydrogen is calculated for incident elrctron energy 76.46 eV. The Hartree-Fock approximation is used to calculate the initial state which includes both bound and continum wave functions. The final state continuum electron wave functions are obtained in the potential of hydrogen ion. The interaction between the two final state continuum electrons is approximated with the screening potential determined variationally.
Discrete Wavelet Transform Based Algorithm for Recognition of QRS Complexes<|sep|>This paper proposes the application of Discrete Wavelet Transform (DWT) to detect the QRS (ECG is characterized by a recurrent wave sequence of P, QRS and T-wave) of an electrocardiogram (ECG) signal. Wavelet Transform provides localization in both time and frequency. In preprocessing stage, DWT is used to remove the baseline wander in the ECG signal. The performance of the algorithm of QRS detection is evaluated against the standard MIT BIH (Massachusetts Institute of Technology, Beth Israel Hospital) Arrhythmia database. The average QRS complexes detection rate of 98.1 % is achieved.
A study of VLF signals variations associated with the changes of ionization level in the D-region in consequence of solar conditions<|sep|>In this paper we confine our attention to the analysis of amplitude and phase data acquired by monitoring VLF/LF radio signals emitted by four European transmitters during a seven-year period (2008-2014). All the data were recorded at a Belgrade site (44.85$^{0}$ N, 20.38$^{0}$ E) by the Stanford University ELF/VLF receiver AWESOME. Propagation of VLF/LF radio signal takes place in the Earth-ionosphere waveguide and strongly depends on ionization level of the D-region, which means that it is mainly controlled by solar conditions. Some results of amplitude and phase variations on GQD/22.10 kHz, DHO/23.40 kHz, ICV/20.27 kHz and NSC/45.90 kHz radio signals measurements at short distances ($D < 2$ Mm) over Central Europe and their interpretation are summarized in this paper. Attention is restricted to regular diurnal, seasonal and solar variations including sunrise and sunset effects on propagation characteristics of four VLF/LF radio signals. We study VLF/LF propagation over short path as a superposition of different number of discrete modes which depends on the variations of the path parameters. Although the solar X-ray flare effects on propagation of VLF/LF radio signals are well recognized on all paths, similarities and differences between them are defined under existing conditions over the paths. Statistical results show that the size of amplitude and phase perturbations on VLF/LF radio signal is in correlation with the intensity of X-ray flux. We present the calculations of electron density enhancements in the D-region caused by different classes of solar X-ray flares during the period of ascending phase and maximum of the solar cycle 24.
Heavy neutral fermions at the high-luminosity LHC<|sep|>Long-lived light particles (LLLPs) appear in many extensions of the standard model. LLLPs are usually motivated by the observed small neutrino masses, by dark matter or both. Typical examples for fermionic LLLPs (a.k.a. heavy neutral fermions, HNFs) are sterile neutrinos or the lightest neutralino in R-parity violating supersymmetry. The high luminosity LHC is expected to deliver up to 3/ab of data. Searches for LLLPs in dedicated experiments at the LHC could then probe the parameter space of LLLP models with unprecedented sensitivity. Here, we compare the prospects of several recent experimental proposals, FASER, CODEX-b and MATHUSLA, to search for HNFs and discuss their relative merits.
Entanglement and interaction in a topological quantum walk<|sep|>We study the quantum walk of two interacting particles on a line with an interface separating two topologically distinct regions. The interaction induces a localization-delocalization transition of the edge state at the interface. We characterize the transition through the entanglement between the two particles.
Towards Multi-agent Reinforcement Learning for Wireless Network Protocol Synthesis<|sep|>This paper proposes a multi-agent reinforcement learning based medium access framework for wireless networks. The access problem is formulated as a Markov Decision Process (MDP), and solved using reinforcement learning with every network node acting as a distributed learning agent. The solution components are developed step by step, starting from a single-node access scenario in which a node agent incrementally learns to control MAC layer packet loads for reining in self-collisions. The strategy is then scaled up for multi-node fully-connected scenarios by using more elaborate reward structures. It also demonstrates preliminary feasibility for more general partially connected topologies. It is shown that by learning to adjust MAC layer transmission probabilities, the protocol is not only able to attain theoretical maximum throughput at an optimal load, but unlike classical approaches, it can also retain that maximum throughput at higher loading conditions. Additionally, the mechanism is agnostic to heterogeneous loading while preserving that feature. It is also shown that access priorities of the protocol across nodes can be parametrically adjusted. Finally, it is also shown that the online learning feature of reinforcement learning is able to make the protocol adapt to time-varying loading conditions.
Hierarchy of space-time structures, Boltzmann equation, and functional mechanics<|sep|>In this report we discuss the organization of different levels of nature and the corresponding space-time structures by the consideration of a particular problem of time irreversibility. The fundamental time irreversibility problem consists in the following: how to reconcile the time-reversible microscopic dynamics and the irreversible macroscopic one. The recently proposed functional formulation of mechanics is aimed to solve this problem. The basic concept of this formulation is not a material point and a trajectory, like in the traditional formulation of mechanics, but a probability density function. Even if we deal with a single particle (not with an ensemble of particles), we describe its state as a probability density function. We justify this approach using measurement theory. A particular problem in the framework of the irreversibility problem is the derivation of the Boltzmann kinetic equation from the equations of microscopic dynamics. We propose a procedure for obtaining the Boltzmann equation from the Liouville equation based on the BBGKY hierarchy, the recently proposed functional formulation of classical mechanics, and the distinguishing between two scales of space-time, i.e., macro- and microscale. The notion of a space-time structure is introduced. It takes into account not only the space-time itself (i.e., a pseudo-Riemannian manifold), but also a characteristic length and time. The space-time structures form a hierarchy in sense that the initial values for the processes on the microscopic space-time structure (interactions of the particles) are assigned from the processes on the macroscopic one (kinetic phenomena).
Characterizing Diseases from Unstructured Text: A Vocabulary Driven Word2vec Approach<|sep|>Traditional disease surveillance can be augmented with a wide variety of real-time sources such as, news and social media. However, these sources are in general unstructured and, construction of surveillance tools such as taxonomical correlations and trace mapping involves considerable human supervision. In this paper, we motivate a disease vocabulary driven word2vec model (Dis2Vec) to model diseases and constituent attributes as word embeddings from the HealthMap news corpus. We use these word embeddings to automatically create disease taxonomies and evaluate our model against corresponding human annotated taxonomies. We compare our model accuracies against several state-of-the art word2vec methods. Our results demonstrate that Dis2Vec outperforms traditional distributed vector representations in its ability to faithfully capture taxonomical attributes across different class of diseases such as endemic, emerging and rare.
BDGS: A Scalable Big Data Generator Suite in Big Data Benchmarking<|sep|>Data generation is a key issue in big data benchmarking that aims to generate application-specific data sets to meet the 4V requirements of big data. Specifically, big data generators need to generate scalable data (Volume) of different types (Variety) under controllable generation rates (Velocity) while keeping the important characteristics of raw data (Veracity). This gives rise to various new challenges about how we design generators efficiently and successfully. To date, most existing techniques can only generate limited types of data and support specific big data systems such as Hadoop. Hence we develop a tool, called Big Data Generator Suite (BDGS), to efficiently generate scalable big data while employing data models derived from real data to preserve data veracity. The effectiveness of BDGS is demonstrated by developing six data generators covering three representative data types (structured, semi-structured and unstructured) and three data sources (text, graph, and table data).
Do Images really do the Talking? Analysing the significance of Images in Tamil Troll meme classification<|sep|>A meme is an part of media created to share an opinion or emotion across the internet. Due to its popularity, memes have become the new forms of communication on social media. However, due to its nature, they are being used in harmful ways such as trolling and cyberbullying progressively. Various data modelling methods create different possibilities in feature extraction and turning them into beneficial information. The variety of modalities included in data plays a significant part in predicting the results. We try to explore the significance of visual features of images in classifying memes. Memes are a blend of both image and text, where the text is embedded into the image. We try to incorporate the memes as troll and non-trolling memes based on the images and the text on them. However, the images are to be analysed and combined with the text to increase performance. Our work illustrates different textual analysis methods and contrasting multimodal methods ranging from simple merging to cross attention to utilising both worlds' - best visual and textual features. The fine-tuned cross-lingual language model, XLM, performed the best in textual analysis, and the multimodal transformer performs the best in multimodal analysis.
Reconstructing the Thermal Sunyaev-Zel'dovich Effect in 3D<|sep|>The thermal Sunyaev-Zel'dovich (tSZ) effect measures the line-of-sight projection of the thermal pressure of free electrons and lacks any redshift information. By cross correlating the tSZ effect with an external cosmological tracer we can recover a good fraction of this lost information. Weak lensing (WL) is thought to provide an unbiased probe of the dark Universe, with many WL surveys having sky coverage that overlaps with tSZ surveys. Generalising the tomographic approach, we advocate the use of the spherical Fourier-Bessel (sFB) expansion to perform an analysis of the cross-correlation between the projected (2D) tSZ Compton $y$-parameter maps and 3D weak lensing convergence maps. We use redshift dependent linear biasing and the halo model as a tool to investigate the tSZ-WL cross-correlations in 3D. We use the Press-Schechter (PS) and the Sheth-Tormen (ST) mass-functions in our calculations, finding that the results are quite sensitive to detailed modelling. We provide detailed analysis of surveys with photometric and spectroscopic redshifts. The signal-to-noise (S/N) of the cross-spectra $\mathcal{C}_{\ell} (k)$ for individual 3D modes, defined by the radial and tangential wave numbers $(k;\ell)$, remains comparable to, but below, unity though optimal binning is expected to improve this. The results presented can be generalised to analyse other CMB secondaries, such as the kinetic Sunyaev-Zel'dovich (kSZ) effect.
Characterization of Low Surface Brightness structures in annotated deep images<|sep|>The characterization of Low Surface Brightness (LSB) stellar structures around galaxies such as tidal debris of on-going or past collisions is essential to constrain models of galactic evolution. Our goal is to obtain quantitative measurements of LSB structures identified in deep images of samples consisting of hundreds of galaxies. We developed an online annotation tool that enables contributors to delineate the shapes of diffuse extended stellar structures, as well as artefacts or foreground structures. All parameters are automatically stored in a database which may be queried to retrieve quantitative measurements. We annotated LSB structures around 352 nearby massive galaxies with deep images obtained with the CFHT as part of two large programs: MATLAS and UNIONS/CFIS. Each LSB structure was delineated and labeled according to its likely nature: stellar shells, streams associated to a disrupted satellite, tails formed in major mergers, ghost reflections or cirrus. From our database containing 8441 annotations, the area, size, median surface brightness and distance to the host of 228 structures were computed. The results confirm the fact that tidal structures defined as streams are thinner than tails, as expected by numerical simulations. In addition, tidal tails appear to exhibit a higher surface brightness than streams (by about 1 mag), which may be related to different survival times for the two types of collisional debris. We did not detect any tidal feature fainter than 27.5 mag.arcsec$^{-2}$, while the nominal surface brightness limits of our surveys range between 28.3 and 29 mag.arcsec$^{-2}$, a difference that needs to be taken into account when estimating the sensitivity of future surveys to identify LSB structures. Our annotation database of observed LSB structures may be used for quantitative analysis and as a training set for machine learning algorithms (abbreviated).
Testing the Running of non-Gaussianity through the CMB mu-distortion and the Halo Bias<|sep|>The primordial non-Gaussianity parameters fNL and tauNL may be scale-dependent. We investigate the capability of future measurements of the CMB mu-distortion, which is very sensitive to small scales, and of the large-scale halo bias to test the running of local non-Gaussianity. We show that, for an experiment such as PIXIE, a measurement of the mu-temperature correlation can pin down the spectral indices n_fNL and n_tauNL to values of the order of 0.3 if fNL = 20 and tauNL = 5000. A similar value can be achieved with an all-sky survey extending to redshift z ~ 1. In the particular case in which the two spectral indices are equal, as predicted in models where the cosmological perturbations are generated by a single-field other than the inflaton, then the 1-sigma error on the scale-dependence of the non-linearity parameters goes down to 0.2.
Testing the nonlinearity of the BVIcJHKs period-luminosity relations for the Large Magellanic Cloud Cepheids<|sep|>A number of recent works have suggested that the period-luminosity (PL) relation for the Large Magellanic Cloud (LMC) Cepheids exhibits a controversial nonlinear feature with a break period at 10 days. Therefore, the aim of this Research Note is to test the linearity/nonlinearity of the PL relations for the LMC Cepheids in BVIcJHKs band, as well as in the Wesenheit functions. We show that simply comparing the long and short period slopes, together with their associate d standard deviations, leads to a strictly larger error rate than applying rigorous statistical tests such as the F-test. We applied various statistical tests to the current published LMC Cepheid data. These statistical tests include the F-test, the testimator test, and the Schwarz information criterion (SIC) method. The results from these statistical tests strongly suggest that the LMC PL relation is nonlinear in BVIcJH band but linear in the Ks band and in the Wesenheit functions. Using the properties of period-color relations at maximum light and multi-phase relations, we believe that the nonlinear PL relation is not caused by extinction errors.
Nuclear matter EOS with light clusters within the mean-field approximation<|sep|>The crust of a neutron star is essentially determined by the low-density region ($\rho<\rho_0\approx0.15-0.16\unit{fm}^{-3}$) of the equation of state. At the bottom of the inner crust, where the density is $\rho\lesssim0.1\rho_0$, the formation of light clusters in nuclear matter will be energetically favorable at finite temperature. At very low densities and moderate temperatures, the few body correlations are expected to become important and light nuclei like deuterons, tritons, helions and $\alpha$-particles will form. Due to Pauli blocking, these clusters will dissolve at higher densities $\rho\gtrsim 0.1\rho_0$. The presence of these clusters influences the cooling process and quantities, such as the neutrino emissivity and gravitational waves emission. The dissolution density of these light clusters, treated as point-like particles, will be studied within the Relativistic Mean Field approximation. In particular, the dependence of the dissolution density on the clusters-meson couplings is studied.
Guided magnon transport in spin chains: transport speed and correcting for disorder<|sep|>High fidelity quantum information transport is necessary for most practical models of quantum computation. By analogy with optical wave guides, a spatio-temporally varying magnetic potential on a one dimensional spin chain can achieve high fidelity transport of spin excitations. By comparing different potential shapes, we establish the effects of potential shape on the fidelity and transport speed. We incorporate disorder into our model and show methods to minimise its effect on transport. Finally, we discuss implementations of our scheme in several accessible systems based on hydrogenic approximations.
High-resolution Transmission Spectroscopy of MASCARA-2 b with EXPRES<|sep|>We report detections of atomic species in the atmosphere of MASCARA-2 b, using the first transit observations obtained with the newly commissioned EXPRES spectrograph. EXPRES is a highly stabilised optical echelle spectrograph, designed to detect stellar reflex motions with amplitudes down to 30 cm/s, and was recently deployed at the Lowell Discovery Telescope. By analysing the transmission spectrum of the ultra-hot Jupiter MASCARA-2 b using the cross-correlation method, we confirm previous detections of Fe I, Fe II and Na I, which likely originate in the upper regions of the inflated atmosphere. In addition, we report significant detections of Mg I and Cr II. The absorption strengths change slightly with time, possibly indicating different temperatures and chemistry in the day-side and night-side terminators. Using the effective stellar line-shape variation induced by the transiting planet, we constrain the projected spin-orbit misalignment of the system to $1.6\pm3.1$ degrees, consistent with an aligned orbit. We demonstrate that EXPRES joins a suite of instruments capable of phase-resolved spectroscopy of exoplanet atmospheres.
Implications of HERA measurements for LHC<|sep|>I discuss the theoretical understanding of key measurements at HERA and their relevance for physics at LHC, focusing on recent developments for structure functions and for diffraction.
A comparison of gyrochronological and isochronal age estimates for transiting exoplanet host stars<|sep|>Previous studies suggest that tidal interactions may be responsible for discrepancies between the ages of exoplanet host stars estimated using stellar models (isochronal ages) and age estimates based on the stars' rotation periods (gyrochronological ages). We have compiled a sample of 28 transiting exoplanet host stars with measured rotation periods. We use a Bayesian Markov chain Monte Carlo method to determine the joint posterior distribution for the mass and age of each star in the sample, and extend this method to include a calculation of the posterior distribution of the gyrochronological age. The gyrochronological age ($\tau_{\rm gyro}$) is significantly less than the isochronal age for about half of the stars in our sample. Tidal interactions between the star and planet are a reasonable explanation for this discrepancy in some cases, but not all. The distribution of $\tau_{\rm gyro}$ values is evenly spread from very young ages up to a maximum value of a few Gyr. There is no clear correlation between $\tau_{\rm gyro}$ and the strength of the tidal force on the star due to the innermost planet. There is clear evidence that the isochronal ages for some K-type stars are too large, and this may also be the case for some G-type stars. This may be the result of magnetic inhibition of convection. There is currently no satisfactory explanation for the discrepancy between the young age for CoRoT-2 estimated from either gyrochronology or its high lithium abundance, and the extremely old age for its K-type stellar companion inferred from its very low X-ray flux. There is now strong evidence that the gyrochronological ages of some transiting exoplanet host stars are significantly less than their isochronal ages, but it is not always clear that this is good evidence for tidal interactions between the star and the planet.
Ranking vertices for active module recovery problem<|sep|>Selecting a connected subnetwork enriched in individually important vertices is an approach commonly used in many areas of bioinformatics, including analysis of gene expression data, mutations, metabolomic profiles and others. It can be formulated as a recovery of an active module from which an experimental signal is generated. Commonly, methods for solving this problem result in a single subnetwork that is considered to be a good candidate. However, it is usually useful to consider not one but multiple candidate modules at different significance threshold levels. Therefore, in this paper we suggest to consider a problem of finding a vertex ranking instead of finding a single module. We also propose two algorithms for solving this problem: one that we consider to be optimal but computationally expensive for real-world networks and one that works close to the optimal in practice and is also able to work with big networks.
An Algorithm for Computing $m$-Tight Error Linear Complexity of Sequences over $GF(p^{m})$ with Period $p^{m}$<|sep|>The linear complexity (LC) of a sequence has been used as a convenient measure of the randomness of a sequence. Based on the theories of linear complexity, $k$-error linear complexity, the minimum error and the $k$-error linear complexity profile, the notion of $m$-tight error linear complexity is presented. An efficient algorithm for computing $m$-tight error linear complexity is derived from the algorithm for computing $k$-error linear complexity of sequences over GF($p^{m}$) with period $p^n$, where $p$ is a prime. The validity of the algorithm is shown. The algorithm is also realized with C language, and an example is presented to illustrate the algorithm.
Variations of the initial mass function in semi-analytical models: implications for the mass assembly and the chemical enrichment of galaxies in the GAEA model<|sep|>In this work, we investigate the implications of the Integrated Galaxy-wide stellar Initial Mass Function (IGIMF) approach in the framework of the semi-analytic model GAEA (GAlaxy Evolution and Assembly), which features a detailed treatment of chemical enrichment and stellar feedback. The IGIMF provides an analytic description of the dependence of the stellar IMF shape on the rate of star formation in galaxies. We find that our model with a universal IMF predicts a rather flat [$\alpha$/Fe]-stellar mass relation. The model assuming the IGIMF, instead, is able to reproduce the observed increase of $\alpha$-enhancement with stellar mass, in agreement with previous studies. This is mainly due to the fact that massive galaxies are characterized by larger star formation rates at high-redshift, leading to stronger $\alpha$-enhancement with respect to low-mass galaxies. At the same time, the IGIMF hypothesis does not affect significantly the trend for shorter star formation timescales for more massive galaxies. We argue that in the IGIMF scenario the [$\alpha$/Fe] ratios are good tracers of the highest star formation events. The final stellar masses and mass-to-light-ratio of our model massive galaxies are larger than those estimated from the synthetic photometry assuming a universal IMF, providing a self-consistent interpretation of similar recent results, based on dynamical analysis of local early type galaxies.
Inferring physical parameters in solar prominence threads<|sep|>High resolution observations have permitted to resolve the solar prominences/filaments as sets of threads/fibrils. However, the values of the physical parameters of these threads and their structuring remain poorly constrained. We use prominence seismology techniques to analyse transverse oscillations in threads through the comparison between magnetohydrodynamic (MHD) models and observations. We apply Bayesian methods to obtain two different types of information. We first infer the marginal posterior distribution of physical parameters, such as the magnetic field strength or the length of the thread, when a totally filled tube, a partially filled tube, and three damping models (resonant absorption in the Alfv\'en continuum, resonant absorption in the slow continuum, and Cowling's diffusion) are considered as certain. Then, we compare the relative plausibility between alternative MHD models by computing the Bayes factors. Well constrained probability density distributions can be obtained for the magnetic field strength, the length of the thread, the density contrast, and parameters associated to damping models. When comparing the damping models of resonant absorption in the Alfv\'en continuum, resonant absorption in the slow continuum and Cowling's diffusion due to partial ionisation of prominence plasma, the resonant absorption in the Alfv\'en continuum is the most plausible mechanism in explaining the existing observations. Relations between periods of fundamental and first overtone kink modes with values around 1 are better explained by expressions of the period ratio in the long thread approximation, while the rest of the values are more probable in the short thread limit for the period ratio. Our results show that Bayesian analysis offers valuable methods for performing parameter inference and model comparison in the context of prominence seismology.
A mid-IR study of Hickson Compact Groups I : Probing the Effects of Environment in Galaxy Interactions<|sep|>Hickson Compact Groups (HCGs) are among the densest galaxy environments of the local universe. To examine the effects of the environment on the infrared properties of these systems, we present an analysis of Spitzer and ISO mid-infrared imaging as well as deep ground based near-infrared imaging of 14 HCGs containing a total of 69 galaxies. Based on mid-infrared color diagnostics we identify the galaxies which appear to host an active nucleus, while using a suite of templates, we fit the complete infrared spectral energy distribution for each group member. We compare our estimates of galaxy mass, star formation rate, total infrared luminosities, and specific star formation rates (sSFR) for our HCG sample, to samples of isolated galaxies and interacting pairs and find that overall there is no discernible difference among them. However, HCGs which can be considered as dynamically "old", host late-type galaxies with a slightly lower sSFR than the one found in dynamically "young" groups. This could be attributed to multiple past interactions among the galaxies in old groups, that have led to the build up of their stellar mass. It is also consistent with our prediction for the presence of diffuse cold dust in the intergalactic medium of 9 of the dynamically "old" groups.
Analysis of opinionated text for opinion mining<|sep|>In sentiment analysis, the polarities of the opinions expressed on an object/feature are determined to assess the sentiment of a sentence or document whether it is positive/negative/neutral. Naturally, the object/feature is a noun representation which refers to a product or a component of a product, let us say, the "lens" in a camera and opinions emanating on it are captured in adjectives, verbs, adverbs and noun words themselves. Apart from such words, other meta-information and diverse effective features are also going to play an important role in influencing the sentiment polarity and contribute significantly to the performance of the system. In this paper, some of the associated information/meta-data are explored and investigated in the sentiment text. Based on the analysis results presented here, there is scope for further assessment and utilization of the meta-information as features in text categorization, ranking text document, identification of spam documents and polarity classification problems.
Post Common Envelope Binaries from SDSS. XV: Accurate stellar parameters for a cool 0.4-solar mass white dwarf and a 0.16-solar mass M-dwarf in a 3 hour eclipsing binary<|sep|>We identify SDSSJ121010.1+334722.9 as an eclipsing post-common-envelope binary, with an orbital period of P ~ 3 hrs, containing a very cool, low-mass, DAZ white dwarf and a low-mass main-sequence star of spectral type M5. A model atmosphere analysis of the metal absorption lines detected in the blue part of the optical spectrum, along with the GALEX near-ultraviolet flux, yields a white dwarf temperature of 6000 +/- 200 K and a metallicity value of log(Z/H)= -2.0 +/- 0.3. The sodium absorption doublet is used to measure the radial velocity of the secondary star, K2 ~ 252 km/s and iron absorption lines in the blue part of the spectrum provide the radial velocity of the white dwarf, K1 ~ 95 km/s, yielding a mass ratio of q ~ 0.38. Light curve model fitting, using the Markov Chain Monte Carlo (MCMC) method, gives the inclination angle as i = (79.05 - 79.36) +/- 0.15 degrees, and the stellar masses as M1 = 0.415 +/- 0.010 solar-masses and M2 = 0.158 +/- 0.006 solar-masses. Systematic uncertainties in the absolute calibration of the photometric data influence the determination of the stellar radii. The radius of the white dwarf is found to be R1 = (0.0157 - 0.0161) +/- 0.0003 solar-radii and the volume-averaged radius of the tidally distorted secondary is R2 = (0.197 - 0.203) +/- 0.003 solar-radii. The white dwarf in J1210+3347 is a very strong He-core candidate.
Decentralized Signal Temporal Logic Control for Perturbed Interconnected Systems via Assume-Guarantee Contract Optimization<|sep|>We develop a novel decentralized control method for a network of perturbed linear systems with dynamical couplings subject to Signal Temporal Logic (STL) specifications. We first transform the STL requirements into set containment problems and then we develop controllers to solve these problems. Our approach is based on treating the couplings between subsystems as disturbances, which are bounded sets that the subsystems negotiate in the form of parametric assume-guarantee contracts. The set containment requirements and parameterized contracts are added to the subsystems' constraints. We introduce a centralized optimization problem to derive the contracts, reachability tubes, and decentralized closed-loop control laws. We show that, when the STL formula is separable with respect to the subsystems, the centralized optimization problem can be solved in a distributed way, which scales to large systems. We present formal theoretical guarantees on robustness of STL satisfaction. The effectiveness of the proposed method is demonstrated via a power network case study.
The Pristine survey XVIII: C-19: Tidal debris of a dark matter-dominated globular cluster?<|sep|>The recently discovered C-19 stellar stream is a collection of kinematically associated metal-poor stars in the halo of the Milky Way lacking an obvious progenitor. The stream spans an arc of ~15 degrees in the sky, and orbit-fitting suggests an apocentric distance of ~20 kpc and a pericentre of ~10 kpc. The narrow metallicity dispersion of stars with available spectra, together with light element abundance variations, suggests a globular cluster (GC) origin. The observed metallicity ([Fe/H] ~ -3.4), however, is much lower than that of any known GC. In addition, the width and velocity dispersion of the stream are similar to those expected from disrupting dwarf galaxies, and substantially larger than the tidal debris of GCs able to disrupt on C-19's orbit. We propose here an unconventional model where the C-19 progenitor is a dark matter-dominated stellar system with GC-like abundance patterns. We use N-body simulations to show that the tidal disruption of a ~100 pc King-model stellar component embedded in a ~20 km/s cuspy cold dark matter halo yields debris consistent with C-19's observed width and velocity dispersion. The stellar component of the progenitor is fully disrupted, and is spread over two distinct streams; one corresponding to C-19 and another possibly hiding behind the Galactic plane. If such companion stream were found, it would suggest that dark matter-dominated dwarfs may also develop GC-like enrichment patterns, a finding that would inform our theoretical understanding of the formation of multiple populations in GCs and dwarf galaxies alike.
Toward a better understanding of supernova environments: a study of SNe 2004dg and 2012P in NGC 5806 with HST and MUSE<|sep|>Core-collapse supernovae (SNe) are the inevitable fate of most massive stars. Since most stars form in groups, SN progenitors can be constrained with information of their environments. It remains challenging to accurately analyse the various components in the environment and to correctly identify their relationships with the SN progenitors. Using a combined dataset of VLT/MUSE spatially-resolved integral-field-unit (IFU) spectroscopy and HST/ACS+WFC3 high-spatial resolution imaging, we present a detailed investigation of the environment of the Type II-P SN 2004dg and Type IIb SN 2012P. The two SNe occurred in a spiral arm of NGC 5806, where a star-forming complex is apparent with a giant H II region. By modelling the ionised gas, a compact star cluster and the resolved stars, we derive the ages and extinctions of stellar populations in the vicinity of the SNe. The various components are consistent with a sequence of triggered star formation as the spiral density wave swept through their positions. For SNe 2004dg and 2012P, we identify their host stellar populations and derive initial masses of $10.0^{+0.3}_{-0.2}~M_\odot$ and $15.2^{+2.0}_{-1.0}~M_\odot$ for their progenitors, respectively. Both results are consistent with those from pre-explosion images or nebular-phase spectroscopy. SN 2012P is spatially coincident but less likely to be coeval with the star-forming complex. As in this case, star formation bursts on small scales may appear correlated if they are controlled by any physical processes on larger scales; this may lead to a high probability of chance alignment between older SN progenitors and younger stellar populations.
Modeling Price Elasticity for Occupancy Prediction in Hotel Dynamic Pricing<|sep|>Demand estimation plays an important role in dynamic pricing where the optimal price can be obtained via maximizing the revenue based on the demand curve. In online hotel booking platform, the demand or occupancy of rooms varies across room-types and changes over time, and thus it is challenging to get an accurate occupancy estimate. In this paper, we propose a novel hotel demand function that explicitly models the price elasticity of demand for occupancy prediction, and design a price elasticity prediction model to learn the dynamic price elasticity coefficient from a variety of affecting factors. Our model is composed of carefully designed elasticity learning modules to alleviate the endogeneity problem, and trained in a multi-task framework to tackle the data sparseness. We conduct comprehensive experiments on real-world datasets and validate the superiority of our method over the state-of-the-art baselines for both occupancy prediction and dynamic pricing.
EEG source localization analysis in epileptic children during a visual working-memory task<|sep|>We localize the sources of brain activity of children with epilepsy based on EEG recordings acquired during a visual discrimination working memory task. For the numerical solution of the inverse problem, with the aid of age-specific MRI scans processed from a publicly available database, we use and compare three regularization numerical methods, namely the standarized Low Resolution Electromagnetic Tomography (sLORETA), the weighted Minimum Norm Estimation (wMNE) and the dynamic Statistical Parametric Mapping (dSPM). We show that all three methods provide the same spatio-temporal patterns of differences between epileptic and control children. In particular, our analysis reveals statistically significant differences between the two groups in regions of the Parietal Cortex indicating that these may serve as "biomarkers" for diagnostic purposes and ultimately localized treatment.
AirHopper: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies<|sep|>Information is the most critical asset of modern organizations, and accordingly coveted by adversaries. When highly sensitive data is involved, an organization may resort to air-gap isolation, in which there is no networking connection between the inner network and the external world. While infiltrating an air-gapped network has been proven feasible in recent years (e.g., Stuxnet), data exfiltration from an air-gapped network is still considered to be one of the most challenging phases of an advanced cyber-attack. In this paper we present "AirHopper", a bifurcated malware that bridges the air-gap between an isolated network and nearby infected mobile phones using FM signals. While it is known that software can intentionally create radio emissions from a video display unit, this is the first time that mobile phones are considered in an attack model as the intended receivers of maliciously crafted radio signals. We examine the attack model and its limitations, and discuss implementation considerations such as stealth and modulation methods. Finally, we evaluate AirHopper and demonstrate how textual and binary data can be exfiltrated from physically isolated computer to mobile phones at a distance of 1-7 meters, with effective bandwidth of 13-60 Bps (Bytes per second).
Accelerating 3D MULTIPLEX MRI Reconstruction with Deep Learning<|sep|>Multi-contrast MRI images provide complementary contrast information about the characteristics of anatomical structures and are commonly used in clinical practice. Recently, a multi-flip-angle (FA) and multi-echo GRE method (MULTIPLEX MRI) has been developed to simultaneously acquire multiple parametric images with just one single scan. However, it poses two challenges for MULTIPLEX to be used in the 3D high-resolution setting: a relatively long scan time and the huge amount of 3D multi-contrast data for reconstruction. Currently, no DL based method has been proposed for 3D MULTIPLEX data reconstruction. We propose a deep learning framework for undersampled 3D MRI data reconstruction and apply it to MULTIPLEX MRI. The proposed deep learning method shows good performance in image quality and reconstruction time.
Theory on the Structure and Coloring of Maximal Planar Graphs (1)Recursion Formulae of Chromatic Polynomial and Four-Color Conjecture<|sep|>In this paper, two recursion formulae of chromatic polynomial of a maximal planar graph G are obtained. Moreover, the application of these formulaes to the proof of Four-Color Conjecture is investigated. By using these formulae, the proof of Four-Color Conjecture boils down to the study on a special class of graphs, viz., 4-chromatic-funnel pseudo uniquely-4-colorable maximal planar graphs.
Counterexample-guided computation of polyhedral Lyapunov functions for hybrid systems<|sep|>This paper presents a counterexample-guided iterative algorithm to compute convex, piecewise linear (polyhedral) Lyapunov functions for uncertain continuous-time linear hybrid systems. Polyhedral Lyapunov functions provide an alternative to commonly used polynomial Lyapunov functions. Our approach first characterizes intrinsic properties of a polyhedral Lyapunov function including its "eccentricity" and "robustness" to perturbations. We then derive an algorithm that either computes a polyhedral Lyapunov function proving that the system is stable, or concludes that no polyhedral Lyapunov function exists whose eccentricity and robustness parameters satisfy some user-provided limits. Significantly, our approach places no a priori bounds on the number of linear pieces that make up the desired polyhedral Lyapunov function. The algorithm alternates between a learning step and a verification step, always maintaining a finite set of witness states. The learning step solves a linear program to compute a candidate Lyapunov function compatible with a finite set of witness states. In the verification step, our approach verifies whether the candidate Lyapunov function is a valid Lyapunov function for the system. If verification fails, we obtain a new witness. We prove a theoretical bound on the maximum number of iterations needed by our algorithm. We demonstrate the applicability of the algorithm on numerical examples.
The statistical properties of protein folding in the {\phi}^4 theory<|sep|>The statistical properties of protein folding within the {\phi}^4 model are investigated. The calculation is performed using statistical mechanics and path integral method. In particular, the evolution of heat capacity in term of temperature is given for various levels of the nonlinearity of source and the strength of interaction between protein backbone and nonlinear source. It is found that the nonlinear source contributes constructively to the specific heat especially at higher temperature when it is weakly interacting with the protein backbone. This indicates increasing energy absorption as the intensity of nonlinear sources are getting greater. The simulation of protein folding dynamics within the model is also refined.
The Nature of Length, Area, and Volume in Taxicab Geometry<|sep|>While the concept of straight-line length is well understood in taxicab geometry, little research has been done into the length of curves or the nature of area and volume in this geometry. This paper sets forth a comprehensive view of the basic dimensional measures in taxicab geometry.
Revisiting Bimaximal Neutrino Mixing in a Model with S4 Discrete Symmetry<|sep|>In view of the fact that the data on neutrino mixing are still compatible with a situation where Bimaximal mixing is valid in first approximation and it is then corrected by terms of order of the Cabibbo angle, arising from the diagonalization of the charged lepton masses, we construct a model based on the discrete group S4 where those properties are naturally realized. The model is supersymmetric in 4-dimensions and the complete flavour group is S4 x Z4 x U(1)_FN, which also allows to reproduce the hierarchy of the charged lepton spectrum. The only fine tuning needed in the model is to reproduce the small observed value of r, the ratio between the neutrino mass squared differences. Once the relevant parameters are set to accommodate r then the spectrum of light neutrinos shows a moderate normal hierarchy and is compatible, within large ambiguities, with the constraints from leptogenesis as an explanation of the baryon asymmetry in the Universe.
Equilibrium and dynamics of a three-state opinion model<|sep|>We introduce a three-state model to study the effects of a neutral party on opinion spreading, in which the tendency of agents to agree with their neighbors can be tuned to favor either the neutral party or two oppositely polarized parties, and can be disrupted by social agitation mimicked as temperature. We study the equilibrium phase diagram and the non-equilibrium stochastic dynamics of the model with various analytical approaches and with Monte Carlo simulations on different substrates: the fully-connected (FC) graph, the one-dimensional (1D) chain, and Erd\"os-R\'enyi (ER) random graphs. We show that, in the mean-field approximation, the phase boundary between the disordered and polarized phases is characterized by a tricritical point. On the FC graph, in the absence of social agitation, kinetic barriers prevent the system from reaching optimal consensus. On the 1D chain, the main result is that the dynamics is governed by the growth of opinion clusters. Finally, for the ER ensemble a phase transition analogous to that of the FC graph takes place, but now the system is able to reach optimal consensus at low temperatures, except when the average connectivity is low, in which case dynamical traps arise from local frozen configurations.
Deuterium fractionation of nitrogen hydrides: detections of NHD and ND$_2$<|sep|>Although ammonia is an abundant molecule commonly observed towards the dense interstellar medium, it has not yet been established whether its main formation route is from gas-phase ion-molecule reactions or grain-surface hydrogen additions on adsorbed nitrogen atoms. Deuterium fractionation can be used as a tool to constrain formation mechanisms. High abundances of deuterated molecules are routinely observed in the dense interstellar medium, with the ratio between deuterated molecules and the main isotopologue enhanced by several orders of magnitude with respect to the elemental D/H ratio. In the case of ammonia, the detection of its triply deuterated isotopologue hints at high abundances of the deuterated intermediate nitrogen radicals, ND, NHD and ND$_2$. So far however, only ND has been detected in the interstellar medium. In this paper, to constrain the formation of ammonia, we aim at determining the NHD/NH$_2$ and ND$_2$/NHD abundance ratios, and compare them with the predictions of both pure gas-phase and grain-surface chemical models. We searched for the fundamental rotational transitions of NHD and ND$_2$ towards the class 0 protostar IRAS16293-2422, towards which NH, NH$_2$ and ND had been previously detected. Both NHD and ND$_2$ are detected in absorption towards the source. The relative abundance ratios NH$_2$ : NHD : ND$_2$ are close to 8 : 4 : 1. These ratios can be reproduced by our gas-phase chemical model within a factor of two-three. Statistical ratios as expected from grain-surface chemistry are also consistent with our data. Further investigations of the ortho-to-para ratio in ND$_2$ , both theoretical and observational, could bring new constraints to better understand nitrogen hydride chemistry.
Three Formulations of the Kuramoto Model as a System of Polynomial Equations<|sep|>We compare three formulations of stationary equations of the Kuramoto model as systems of polynomial equations. In the comparison, we present bounds on the numbers of real equilibria based on the work of Bernstein, Kushnirenko, and Khovanskii, and performance of methods for the optimisation over the set of equilibria based on the work of Lasserre, both of which could be of independent interest.
Finite Size Scaling for Criticality of the Schr\"odinger Equation<|sep|>By solving the Schr\"odinger equation one obtains the whole energy spectrum, both the bound and the continuum states. If the Hamiltonian depends on a set of parameters, these could be tuned to a transition from bound to continuum states. The behavior of systems near the threshold, which separates bound-states from continuum states, is important in the study of such phenomenon as: ionization of atoms and molecules, molecule dissociation, scattering collisions and stability of matter. In general, the energy is non-analytic as a function of the Hamiltonian parameters or a bound-state does not exist at the threshold energy. The overall goal of this chapter is to show how one can predict, generate and identify new class of stable quantum systems using large-dimensional models and the finite size scaling approach. Within this approach, the finite size corresponds not to the spatial dimension but to the number of elements in a complete basis set used to expand the exact eigenfunction of a given Hamiltonian. This method is efficient and very accurate for estimating the critical parameters, $\{\lambda_i\}$, for stability of a given Hamiltonian, $H(\lambda_i)$. We present two methods of obtaining critical parameters using finite size scaling for a given quantum Hamiltonian: The finite element method and the basis set expansion method.
Quantifying ionospheric effects on time-domain astrophysics with the Murchison Widefield Array<|sep|>Refraction and diffraction of incoming radio waves by the ionosphere induce time variability in the angular positions, peak amplitudes and shapes of radio sources, potentially complicating the automated cross-matching and identification of transient and variable radio sources. In this work, we empirically assess the effects of the ionosphere on data taken by the Murchison Widefield Array (MWA) radio telescope. We directly examine 51 hours of data observed over 10 nights under quiet geomagnetic conditions (global storm index Kp < 2), analysing the behaviour of short-timescale angular position and peak flux density variations of around ten thousand unresolved sources. We find that while much of the variation in angular position can be attributed to ionospheric refraction, the characteristic displacements (10-20 arcsec) at 154 MHz are small enough that search radii of 1-2 arcmin should be sufficient for cross-matching under typical conditions. By examining bulk trends in amplitude variability, we place upper limits on the modulation index associated with ionospheric scintillation of 1-3% for the various nights. For sources fainter than ~1 Jy, this variation is below the image noise at typical MWA sensitivities. Our results demonstrate that the ionosphere is not a significant impediment to the goals of time-domain science with the MWA at 154 MHz.
The place of the Sun among the Sun-like stars<|sep|>Context. Monitoring of the photometric and chromospheric HK emission data series of stars similar to the Sun in age and average activity level showed that there is an empirical correlation between the average stellar chromospheric activity level and the photometric variability. In general, more active stars show larger photometric variability. Interestingly, the measurements and reconstructions of the solar irradiance show that the Sun is significantly less variable than indicated by the empirical relationship. Aims. We aim to identify possible reasons for the Sun to be currently outside of this relationship. Methods. We employed different scenarios of solar HK emission and irradiance variability and compared them with available time series of Sun-like stars. Results. We show that the position of the Sun on the diagram of photometric variability versus chromospheric activity changes with time. The present solar position is different from its temporal mean position as the satellite era of continuous solar irradiance measurements has accidentally coincided with a period of unusually high and stable solar activity. Our analysis suggests that although present solar variability is significantly smaller than indicated by the stellar data, the temporal mean solar variability might be in agreement with the stellar data. We propose that the continuation of the photometric program and its expansion to a larger stellar sample will ultimately allow us to constrain the historical solar variability.
Mean Field Stochastic Games with Binary Action Spaces and Monotone Costs<|sep|>This paper considers mean field games in a multi-agent Markov decision process (MDP) framework. Each player has a continuum state and binary action. By active control, a player can bring its state to a resetting point. All players are coupled through their cost functions. The structural property of the individual strategies is characterized in terms of threshold policies when the mean field game admits a solution. We further introduce a stationary equation system of the mean field game and analyze uniqueness of its solution under positive externalities.
On Implementing Real-time Specification Patterns Using Observers<|sep|>English language requirements are often used to specify the behavior of complex cyber-physical systems. The process of transforming these requirements to a formal specification language is often challenging, especially if the specification language does not contain constructs analogous to those used in the original requirements. For example, requirements often contain real-time constraints, but many specification languages for model checkers have discrete time semantics. Work in specification patterns helps to bridge these gaps, allowing straightforward expression of common requirements patterns in formal languages. In this work we demonstrate how we support real-time specification patterns in the Assume Guarantee Reasoning Environment (AGREE) using observers. We demonstrate that there are subtle challenges, not mentioned in previous literature, to express real-time patterns accurately using observers. We then demonstrate that these patterns are sufficient to model real-time requirements for a real-world avionics system.
TypeShift: A User Interface for Visualizing the Typing Production Process<|sep|>TypeShift is a tool for visualizing linguistic patterns in the timing of typing production. Language production is a complex process which draws on linguistic, cognitive and motor skills. By visualizing holistic trends in the typing process, TypeShift aims to elucidate the often noisy information signals that are used to represent typing patterns, both at the word-level and character-level. It accomplishes this by enabling a researcher to compare and contrast specific linguistic phenomena, and compare an individual typing session to multiple group averages. Finally, although TypeShift was originally designed for typing data, it can easy be adapted to accommodate speech data, as well. A web demo is available at https://angoodkind.shinyapps.io/TypeShift/. The source code can be accessed at https://github.com/angoodkind/TypeShift.
Polarimetric Monocular Dense Mapping Using Relative Deep Depth Prior<|sep|>This paper is concerned with polarimetric dense map reconstruction based on a polarization camera with the help of relative depth information as a prior. In general, polarization imaging is able to reveal information about surface normal such as azimuth and zenith angles, which can support the development of solutions to the problem of dense reconstruction, especially in texture-poor regions. However, polarimetric shape cues are ambiguous due to two types of polarized reflection (specular/diffuse). Although methods have been proposed to address this issue, they either are offline and therefore not practical in robotics applications, or use incomplete polarimetric cues, leading to sub-optimal performance. In this paper, we propose an online reconstruction method that uses full polarimetric cues available from the polarization camera. With our online method, we can propagate sparse depth values both along and perpendicular to iso-depth contours. Through comprehensive experiments on challenging image sequences, we demonstrate that our method is able to significantly improve the accuracy of the depthmap as well as increase its density, specially in regions of poor texture.
The Bedrock of Byzantine Fault Tolerance: A Unified Platform for BFT Protocol Design and Implementation<|sep|>Byzantine Fault-Tolerant (BFT) protocols have recently been extensively used by decentralized data management systems with non-trustworthy infrastructures, e.g., permissioned blockchains. BFT protocols cover a broad spectrum of design dimensions from infrastructure settings such as the communication topology, to more technical features such as commitment strategy and even fundamental social choice properties like order-fairness. The proliferation of different BFT protocols has rendered it difficult to navigate the BFT landscape, let alone determine the protocol that best meets application needs. This paper presents Bedrock, a unified platform for BFT protocols design, analysis, implementation, and experiments. Bedrock proposes a design space consisting of a set of design choices capturing the trade-offs between different design space dimensions and providing fundamentally new insights into the strengths and weaknesses of BFT protocols. Bedrock enables users to analyze and experiment with BFT protocols within the space of plausible choices, evolve current protocols to design new ones, and even uncover previously unknown protocols. Our experimental results demonstrate the capability of Bedrock to uniformly evaluate BFT protocols in new ways that were not possible before due to the diverse assumptions made by these protocols. The results validate Bedrock's ability to analyze and derive BFT protocols.
CFHTLenS: Testing the Laws of Gravity with Tomographic Weak Lensing and Redshift Space Distortions<|sep|>Dark energy may be the first sign of new fundamental physics in the Universe, taking either a physical form or revealing a correction to Einsteinian gravity. Weak gravitational lensing and galaxy peculiar velocities provide complementary probes of General Relativity, and in combination allow us to test modified theories of gravity in a unique way. We perform such an analysis by combining measurements of cosmic shear tomography from the Canada-France Hawaii Telescope Lensing Survey (CFHTLenS) with the growth of structure from the WiggleZ Dark Energy Survey and the Six-degree-Field Galaxy Survey (6dFGS), producing the strongest existing joint constraints on the metric potentials that describe general theories of gravity. For scale-independent modifications to the metric potentials which evolve linearly with the effective dark energy density, we find present-day cosmological deviations in the Newtonian potential and curvature potential from the prediction of General Relativity to be (Delta Psi)/Psi = 0.05 \pm 0.25 and (Delta Phi)/Phi = -0.05 \pm 0.3 respectively (68 per cent CL).
Black Hole Spin Signature in the Black Hole Shadow of M87 in the Flaring State<|sep|>Imaging the immediate vicinity of supermassive black holes (SMBH) and extracting a BH-spin signature is one of the grand challenges in astrophysics. M87 is known as one of the best targets for imaging the BH shadow and it can be partially thick against synchrotron self-absorption (SSA), particularly in a flaring state with high mass accretion rate. However, little is known about influences of the SSA-thick region on BH shadow images. Here we investigate BH shadow images of M87 at 230 GHz properly taking into account the SSA-thick region. When the BH has a high spin value, the corresponding BH shadow image shows the positional offset between the center of the photon ring and that of the SSA-thick ring at the innermost stable circular orbit (ISCO) due to the frame-dragging effect in the Kerr spacetime. As a result, we find that a dark-crescent structure is generally produced between the photon ring and the SSA-thick ISCO ring in the BH shadow image. The scale size of the dark-crescent increase with BH spin: its width reaches up to $\sim 2$ gravitational radius when the BH spin is 99.8% of its maximum value. The dark crescent is regarded as a new signature of a highly spinning BH. This feature is expected to appear in flaring states with relatively high mass accretion rate rather than the quiescent states. We have simulated the image reconstruction of our theoretical image by assuming the current and future Event Horizon Telescope (EHT) array, and have found that the future EHT including space-very long baseline interferometry in 2020s can detect the dark crescent.
Which is faster: Bowtie2GP > Bowtie > Bowtie2 > BWA<|sep|>We have recently used genetic programming to automatically generate an improved version of Langmead's DNA read alignment tool Bowtie2 Sect.5.3 RN/12/09. We find it runs more than four times faster than the Bioinformatics sequencing tool (BWA) currently used with short next generation paired end DNA sequences by the Cancer Institute, takes less memory and yet finds similar matches in the human genome.
An Approach to Autonomous Science by Modeling Geological Knowledge in a Bayesian Framework<|sep|>Autonomous Science is a field of study which aims to extend the autonomy of exploration robots from low level functionality, such as on-board perception and obstacle avoidance, to science autonomy, which allows scientists to specify missions at task level. This will enable more remote and extreme environments such as deep ocean and other planets to be studied, leading to significant science discoveries. This paper presents an approach to extend the high level autonomy of robots by enabling them to model and reason about scientific knowledge on-board. We achieve this by using Bayesian networks to encode scientific knowledge and adapting Monte Carlo Tree Search techniques to reason about the network and plan informative sensing actions. The resulting knowledge representation and reasoning framework is anytime, handles large state spaces and robust to uncertainty making it highly applicable to field robotics. We apply the approach to a Mars exploration mission in which the robot is required to plan paths and decide when to use its sensing modalities to study a scientific latent variable of interest. Extensive simulation results show that our approach has significant performance benefits over alternative methods. We also demonstrate the practicality of our approach in an analog Martian environment where our experimental rover, Continuum, plans and executes a science mission autonomously.
Reflective Arrayed Waveguide Grating with Sagnac Loop Reflectors in Silicon-on-Insulator with Gaussian Pass-band<|sep|>In this paper the experimental demonstration of a Silicon-on-Insulator Reflective Arrayed Waveguide Grating (R-AWG) is reported. The device employs one Sagnac loop mirror per arm in the array, built with a 1x2 input/outputs, 50:50 splitting ratio, Multimode Interference coupler, for total reflection. The spectral responses obtained are compared to those of regular AWGs fabricated in the same die.
Optimal Vertex Cover for the Small-World Hanoi Networks<|sep|>The vertex-cover problem on the Hanoi networks HN3 and HN5 is analyzed with an exact renormalization group and parallel-tempering Monte Carlo simulations. The grand canonical partition function of the equivalent hard-core repulsive lattice-gas problem is recast first as an Ising-like canonical partition function, which allows for a closed set of renormalization group equations. The flow of these equations is analyzed for the limit of infinite chemical potential, at which the vertex-cover problem is attained. The relevant fixed point and its neighborhood are analyzed, and non-trivial results are obtained both, for the coverage as well as for the ground state entropy density, which indicates the complex structure of the solution space. Using special hierarchy-dependent operators in the renormalization group and Monte-Carlo simulations, structural details of optimal configurations are revealed. These studies indicate that the optimal coverages (or packings) are not related by a simple symmetry. Using a clustering analysis of the solutions obtained in the Monte Carlo simulations, a complex solution space structure is revealed for each system size. Nevertheless, in the thermodynamic limit, the solution landscape is dominated by one huge set of very similar solutions.
Bayesian Hierarchical Model of Total Variation Regularisation for Image Deblurring<|sep|>A Bayesian hierarchical model for total variation regularisation is presented in this paper. All the parameters of an inverse problem, including the "regularisation parameter", are estimated simultaneously from the data in the model. The model is based on the characterisation of the Laplace density prior as a scale mixture of Gaussians. With different priors on the mixture variable, other total variation like regularisations e.g. a prior that is related to t-distribution, are also obtained. An approximation of the resulting posterior mean is found using a variational Bayes method. In addition, an iterative alternating sequential algorithm for computing the maximum a posteriori estimate is presented. The methods are illustrated with examples of image deblurring. Results show that the proposed model can be used for automatic edge-preserving inversion in the case of image deblurring. Despite promising results, some difficulties with the model were encountered and are subject to future work.
Travelling waves for diffusive and strongly competitive systems: relative motility and invasion speed<|sep|>Our interest here is to find the invader in a two species, diffusive and competitive Lotka-Volterra system in the particular case of travelling wave solutions. We investigate the role of diffusion in homogeneous domains. We might expect a priori two different cases: strong interspecific competition and weak interspecific competition. In this paper, we study the first one and obtain a clear conclusion: the invading species is, up to a fixed multiplicative constant, the more diffusive one.
Thermodynamics of de Sitter black hole in massive gravity<|sep|>In this paper, by taking de Sitter space-time as a thermodynamic system, we study the equivalent thermodynamic quantities of de Sitter black hole in massive gravity, and furthermore obtain the equivalent thermodynamic quantities of the space-time. Our results show that the entropy of this type of space-time takes the same form as that in Reissner-Nordstrom-de Sitter space-time, which lays a solid foundation for deeply understanding the universal thermodynamic characteristics of de Sitter space-time in the future. Moreover, our analysis indicates that the equivalent thermodynamic quantities and relevant parameters play a very important role, especially in the investigation of the stability and evolution of de Sitter space-time.
Distributed averaging integral Nash equilibrium seeking on networks<|sep|>Continuous-time gradient-based Nash equilibrium seeking algorithms enjoy a passivity property under a suitable monotonicity assumption. This feature has been exploited to design distributed algorithms that converge to Nash equilibria and use local information only. We further exploit the passivity property to interconnect the algorithms with distributed averaging integral controllers that tune on-line the weights of the communication graph. The main advantage is to guarantee convergence to a Nash equilibrium without requiring a strong coupling condition on the algebraic connectivity of the communication graph over which the players exchange information, nor a global high-gain.
SC^2-PCR: A Second Order Spatial Compatibility for Efficient and Robust Point Cloud Registration<|sep|>In this paper, we present a second order spatial compatibility (SC^2) measure based method for efficient and robust point cloud registration (PCR), called SC^2-PCR. Firstly, we propose a second order spatial compatibility (SC^2) measure to compute the similarity between correspondences. It considers the global compatibility instead of local consistency, allowing for more distinctive clustering between inliers and outliers at early stage. Based on this measure, our registration pipeline employs a global spectral technique to find some reliable seeds from the initial correspondences. Then we design a two-stage strategy to expand each seed to a consensus set based on the SC^2 measure matrix. Finally, we feed each consensus set to a weighted SVD algorithm to generate a candidate rigid transformation and select the best model as the final result. Our method can guarantee to find a certain number of outlier-free consensus sets using fewer samplings, making the model estimation more efficient and robust. In addition, the proposed SC^2 measure is general and can be easily plugged into deep learning based frameworks. Extensive experiments are carried out to investigate the performance of our method. Code will be available at \url{https://github.com/ZhiChen902/SC2-PCR}.
Atmospheric PSF Interpolation for Weak Lensing in Short Exposure Imaging Data<|sep|>A main science goal for the Large Synoptic Survey Telescope (LSST) is to measure the cosmic shear signal from weak lensing to extreme accuracy. One difficulty, however, is that with the short exposure time ($\simeq$15 seconds) proposed, the spatial variation of the Point Spread Function (PSF) shapes may be dominated by the atmosphere, in addition to optics errors. While optics errors mainly cause the PSF to vary on angular scales similar or larger than a single CCD sensor, the atmosphere generates stochastic structures on a wide range of angular scales. It thus becomes a challenge to infer the multi-scale, complex atmospheric PSF patterns by interpolating the sparsely sampled stars in the field. In this paper we present a new method, PSFent, for interpolating the PSF shape parameters, based on reconstructing underlying shape parameter maps with a multi-scale maximum entropy algorithm. We demonstrate, using images from the LSST Photon Simulator, the performance of our approach relative to a 5th-order polynomial fit (representing the current standard) and a simple boxcar smoothing technique. Quantitatively, PSFent predicts more accurate PSF models in all scenarios and the residual PSF errors are spatially less correlated. This improvement in PSF interpolation leads to a factor of 3.5 lower systematic errors in the shear power spectrum on scales smaller than $\sim13'$, compared to polynomial fitting. We estimate that with PSFent and for stellar densities greater than $\simeq1/{\rm arcmin}^{2}$, the spurious shear correlation from PSF interpolation, after combining a complete 10-year dataset from LSST, is lower than the corresponding statistical uncertainties on the cosmic shear power spectrum, even under a conservative scenario.
VEGAS: A VST Early-type GAlaxy Survey. II. Photometric study of giant ellipticals and their stellar halos<|sep|>Observations of diffuse starlight in the outskirts of galaxies are thought to be a fundamental source of constraints on the cosmological context of galaxy assembly in the $\Lambda$CDM model. Such observations are not trivial because of the extreme faintness of such regions. In this work, we investigate the photometric properties of six massive early type galaxies (ETGs) in the VEGAS sample (NGC 1399, NGC 3923, NGC 4365, NGC 4472, NGC 5044, and NGC 5846) out to extremely low surface brightness levels, with the goal of characterizing the global structure of their light profiles for comparison to state-of-the-art galaxy formation models. We carry out deep and detailed photometric mapping of our ETG sample taking advantage of deep imaging with VST/OmegaCAM in the g and i bands. By fitting the light profiles, and comparing the results to simulations of elliptical galaxy assembly, we identify signatures of a transition between "relaxed" and "unrelaxed" accreted components and can constrain the balance between in situ and accreted stars. The very good agreement of our results with predictions from theoretical simulations demonstrates that the full VEGAS sample of $\sim 100$ ETGs will allow us to use the distribution of diffuse light as a robust statistical probe of the hierarchical assembly of massive galaxies.
VLA/JVLA Monitoring of Bright Northern Radio Sources<|sep|>We report multiple epoch VLA/JVLA observations of 89 northern hemisphere sources, most with 37\,GHz flux density > 1 Jy, observed at 4.8, 8.5, 33.5, and 43.3 GHz. The high frequency selection leads to a predominantly flat spectrum sample, with 85% of our sources being in the Planck Early Release Compact Source Catalog (ERCSC). These observations allow us to: 1) validate Planck's 30 and 44 GHz flux density scale, 2) extend the radio SEDs of Planck sources to lower frequencies allowing for the full 5-857GHz regime to be studied, and 3) characterize the variability of these sources. At 30 GHz and 44 GHz, the JVLA and Planck flux densities agree to within 3%. On timescales of less than two months the median variability of our sources is 2%. On timescales of about a year the median variability increases to 14%. Using the WMAP 7-year data, the 30 GHz median variability on a 1-6 years timescale is 16%.
Instance-Dependent Generalization Bounds via Optimal Transport<|sep|>Existing generalization bounds fail to explain crucial factors that drive generalization of modern neural networks. Since such bounds often hold uniformly over all parameters, they suffer from over-parametrization, and fail to account for the strong inductive bias of initialization and stochastic gradient descent. As an alternative, we propose a novel optimal transport interpretation of the generalization problem. This allows us to derive instance-dependent generalization bounds that depend on the local Lipschitz regularity of the earned prediction function in the data space. Therefore, our bounds are agnostic to the parametrization of the model and work well when the number of training samples is much smaller than the number of parameters. With small modifications, our approach yields accelerated rates for data on low-dimensional manifolds, and guarantees under distribution shifts. We empirically analyze our generalization bounds for neural networks, showing that the bound values are meaningful and capture the effect of popular regularization methods during training.
Exploring Opportunistic Meta-knowledge to Reduce Search Spaces for Automated Machine Learning<|sep|>Machine learning (ML) pipeline composition and optimisation have been studied to seek multi-stage ML models, i.e. preprocessor-inclusive, that are both valid and well-performing. These processes typically require the design and traversal of complex configuration spaces consisting of not just individual ML components and their hyperparameters, but also higher-level pipeline structures that link these components together. Optimisation efficiency and resulting ML-model accuracy both suffer if this pipeline search space is unwieldy and excessively large; it becomes an appealing notion to avoid costly evaluations of poorly performing ML components ahead of time. Accordingly, this paper investigates whether, based on previous experience, a pool of available classifiers/regressors can be preemptively culled ahead of initiating a pipeline composition/optimisation process for a new ML problem, i.e. dataset. The previous experience comes in the form of classifier/regressor accuracy rankings derived, with loose assumptions, from a substantial but non-exhaustive number of pipeline evaluations; this meta-knowledge is considered 'opportunistic'. Numerous experiments with the AutoWeka4MCPS package, including ones leveraging similarities between datasets via the relative landmarking method, show that, despite its seeming unreliability, opportunistic meta-knowledge can improve ML outcomes. However, results also indicate that the culling of classifiers/regressors should not be too severe either. In effect, it is better to search through a 'top tier' of recommended predictors than to pin hopes onto one previously supreme performer.
Supervised Neural Discrete Universal Denoiser for Adaptive Denoising<|sep|>We improve the recently developed Neural DUDE, a neural network-based adaptive discrete denoiser, by combining it with the supervised learning framework. Namely, we make the supervised pre-training of Neural DUDE compatible with the adaptive fine-tuning of the parameters based on the given noisy data subject to denoising. As a result, we achieve a significant denoising performance boost compared to the vanilla Neural DUDE, which only carries out the adaptive fine-tuning step with randomly initialized parameters. Moreover, we show the adaptive fine-tuning makes the algorithm robust such that a noise-mismatched or blindly trained supervised model can still achieve the performance of that of the matched model. Furthermore, we make a few algorithmic advancements to make Neural DUDE more scalable and deal with multi-dimensional data or data with larger alphabet size. We systematically show our improvements on two very diverse datasets, binary images and DNA sequences.
Probabilistically Robust Optimization of IRS-aided SWIPT Under Coordinated Spectrum Underlay<|sep|>This study considers the Joint Transmit/Reflect Beamforming and Power Splitting (JTRBPS) optimization problem in a spectrum underlay setting, such that the transmit sum-energy of the intelligent reflecting surface (IRS)-aided secondary transmitter (ST) is minimized subject to the quality-of-service requirements of the PS-simultaneous wireless information and power transfer (SWIPT) secondary receivers and the interference constraints of the primary receivers (PR). The interference at the PRs caused by the reception of IRS-reflected signals sent by the primary transmitter is taken into account. A coordinated channel state information (CSI) acquisition protocol is proposed. Next, assuming availability at the ST of perfect CSI for all direct and IRS-cascaded transmitter--receiver channels, two penalty-based iterative algorithms are developed: an alternating minimization algorithm that involves semi-definite relaxation in JTBPS design and successive convex approximation in RB optimization, and a block coordinate descent algorithm that employs the Riemannian conjugate gradient algorithm in RB updates. Finally, an outage-constrained robust design under imperfect CSI is devised. Numerical simulations highlight the performance gains of the proposed strategies over benchmarks, corroborate the benefits of using an IRS, and provide valuable insights.
Constraining scalar-tensor quintessence by cosmic clocks<|sep|>Scalar-tensor quintessence models can be constrained by identifying suitable cosmic clocks which allow to select confidence regions for cosmological parameters. In particular, we constrain the characterizing parameters of non-minimally coupled scalar-tensor cosmological models which admit exact solutions of the Einstein field equations. Lookback time to galaxy clusters at low intermediate, and high redshifts is considered. The high redshift time-scale problem is also discussed in order to select other cosmic clocks such as quasars.
CMB Lensing Beyond the Power Spectrum: Cosmological Constraints from the One-Point PDF and Peak Counts<|sep|>Unprecedentedly precise cosmic microwave background (CMB) data are expected from ongoing and near-future CMB Stage-III and IV surveys, which will yield reconstructed CMB lensing maps with effective resolution approaching several arcminutes. The small-scale CMB lensing fluctuations receive non-negligible contributions from nonlinear structure in the late-time density field. These fluctuations are not fully characterized by traditional two-point statistics, such as the power spectrum. Here, we use $N$-body ray-tracing simulations of CMB lensing maps to examine two higher-order statistics: the lensing convergence one-point probability distribution function (PDF) and peak counts. We show that these statistics contain significant information not captured by the two-point function, and provide specific forecasts for the ongoing Stage-III Advanced Atacama Cosmology Telescope (AdvACT) experiment. Considering only the temperature-based reconstruction estimator, we forecast 9$\sigma$ (PDF) and 6$\sigma$ (peaks) detections of these statistics with AdvACT. Our simulation pipeline fully accounts for the non-Gaussianity of the lensing reconstruction noise, which is significant and cannot be neglected. Combining the power spectrum, PDF, and peak counts for AdvACT will tighten cosmological constraints in the $\Omega_m$-$\sigma_8$ plane by $\approx 30\%$, compared to using the power spectrum alone.
Compositional Reasoning for Explicit Resource Management in Channel-Based Concurrency<|sep|>We define a pi-calculus variant with a costed semantics where channels are treated as resources that must explicitly be allocated before they are used and can be deallocated when no longer required. We use a substructural type system tracking permission transfer to construct coinductive proof techniques for comparing behaviour and resource usage efficiency of concurrent processes. We establish full abstraction results between our coinductive definitions and a contextual behavioural preorder describing a notion of process efficiency w.r.t. its management of resources. We also justify these definitions and respective proof techniques through numerous examples and a case study comparing two concurrent implementations of an extensible buffer.
Threshold effects on prediction for proton decay in non-supersymmetric $E_6$ GUT with intermediate trinification symmetry<|sep|>We consider a non-supersymmetric $E_6$ Grand Unified Theory (GUT) with intermediate trinification symmetry $SU(3)_C \times SU(3)_L \times SU(3)_R \times D$ (D denoted as D-parity for discrete left-right symmetry) and study the effect of one-loop threshold corrections arising due to every class of superheavy particles (scalars, fermions and vectors). It is observed that, the intermediate mass scale $M_I$ and $\sin^2\theta_W$ remain unaffected by GUT threshold contributions. The threshold modified unification mass scale $M_U$ is in excellent agreement with the present experimental proton decay constraint. The novel feature of the model is that GUT threshold uncertainty of $M_U$ is found to be controlled by superheavy scalars only, leading to a very predictive scenario for proton decay, which can be verifiable within the foreseeable experiments.
Superfield approach to higher derivative N=1 superconformal mechanics<|sep|>We formulate the equations which determine a potential function in an $\mathcal{N}=1$ higher derivative supersymmetric mechanics compatible with the $osp(2|1)\oplus so(d)$ symmetry and provide a few explicit examples.
Extraction of Hierarchical Functional Connectivity Components in human brain using Adversarial Learning<|sep|>The estimation of sparse hierarchical components reflecting patterns of the brain's functional connectivity from rsfMRI data can contribute to our understanding of the brain's functional organization, and can lead to biomarkers of diseases. However, inter-scanner variations and other confounding factors pose a challenge to the robust and reproducible estimation of functionally-interpretable brain networks, and especially to reproducible biomarkers. Moreover, the brain is believed to be organized hierarchically, and hence single-scale decompositions miss this hierarchy. The paper aims to use current advancements in adversarial learning to estimate interpretable hierarchical patterns in the human brain using rsfMRI data, which are robust to "adversarial effects" such as inter-scanner variations. We write the estimation problem as a minimization problem and solve it using alternating updates. Extensive experiments on simulation and a real-world dataset show high reproducibility of the components compared to other well-known methods.
Trans-Planckian relics in the scalar to tensor ratio<|sep|>The physical properties of our universe at energy scales above the expansion rate during inflation can affect predictions for the ratio between the amplitudes of the primordial scalar and tensor fluctuations. In particular, we study here the effects of a breakdown of a locally Lorentz invariant description of nature at tiny space-time intervals. In some instances, these effects shift the amplitudes by a constant amount, altering the standard relation between this ratio and the slow-roll parameters. More generally, "trans-Planckian" effects introduce a modulation in the primordial power spectra which grows at shorter scales, making the value of the ratio sensitive to the scale at which it is defined. We also present a model where symmetries are broken at horizon scales during inflation. In this case, the power at large scales today could then be suppressed, relative to that at smaller scales.
Model order reduction for Linear Noise Approximation using time-scale separation (Extended Version)<|sep|>In this paper, we focus on model reduction of biomolecular systems with multiple time-scales, modeled using the Linear Noise Approximation. Considering systems where the Linear Noise Approximation can be written in singular perturbation form, with $\epsilon$ as the singular perturbation parameter, we obtain a reduced order model that approximates the slow variable dynamics of the original system. In particular, we show that, on a finite time-interval, the first and second moments of the reduced system are within an $O(\epsilon)$-neighborhood of the first and second moments of the slow variable dynamics of the original system. The approach is illustrated on an example of a biomolecular system that exhibits time-scale separation.
OSPF Weight Setting Optimization for Single Link Failures<|sep|>In operational networks, nodes are connected via multiple links for load sharing and redundancy. This is done to make sure that a failure of a link does not disconnect or isolate some parts of the network. However, link failures have an effect on routing, as the routers find alternate paths for the traffic originally flowing through the link which has failed. This effect is severe in case of failure of a critical link in the network, such as backbone links or the links carrying higher traffic loads. When routing is done using the Open Shortest Path First (OSPF) routing protocol, the original weight selection for the normal state topology may not be as efficient for the failure state. In this paper, we investigate the single link failure issue with an objective to find a weight setting which results in efficient routing in normal and failure states. We engineer Tabu Search Iterative heuristic using two different implementation strategies to solve the OSPF weight setting problem for link failure scenarios. We evaluate these heuristics and show through experimental results that both heuristics efficiently handle weight setting for the failure state. A comparison of both strategies is also presented.
ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking<|sep|>We introduce ReFinED, an efficient end-to-end entity linking model which uses fine-grained entity types and entity descriptions to perform linking. The model performs mention detection, fine-grained entity typing, and entity disambiguation for all mentions within a document in a single forward pass, making it more than 60 times faster than competitive existing approaches. ReFinED also surpasses state-of-the-art performance on standard entity linking datasets by an average of 3.7 F1. The model is capable of generalising to large-scale knowledge bases such as Wikidata (which has 15 times more entities than Wikipedia) and of zero-shot entity linking. The combination of speed, accuracy and scale makes ReFinED an effective and cost-efficient system for extracting entities from web-scale datasets, for which the model has been successfully deployed. Our code and pre-trained models are available at https://github.com/alexa/ReFinED
Malware Detection Using Dynamic Birthmarks<|sep|>In this paper, we explore the effectiveness of dynamic analysis techniques for identifying malware, using Hidden Markov Models (HMMs) and Profile Hidden Markov Models (PHMMs), both trained on sequences of API calls. We contrast our results to static analysis using HMMs trained on sequences of opcodes, and show that dynamic analysis achieves significantly stronger results in many cases. Furthermore, in contrasting our two dynamic analysis techniques, we find that using PHMMs consistently outperforms our analysis based on HMMs.
Prior-Knowledge and Attention-based Meta-Learning for Few-Shot Learning<|sep|>Recently, meta-learning has been shown as a promising way to solve few-shot learning. In this paper, inspired by the human cognition process which utilizes both prior-knowledge and vision attention in learning new knowledge, we present a novel paradigm of meta-learning approach with three developments to introduce attention mechanism and prior-knowledge for meta-learning. In our approach, prior-knowledge is responsible for helping meta-learner expressing the input data into high-level representation space, and attention mechanism enables meta-learner focusing on key features of the data in the representation space. Compared with existing meta-learning approaches that pay little attention to prior-knowledge and vision attention, our approach alleviates the meta-learner's few-shot cognition burden. Furthermore, a Task-Over-Fitting (TOF) problem, which indicates that the meta-learner has poor generalization on different K-shot learning tasks, is discovered and we propose a Cross-Entropy across Tasks (CET) metric to model and solve the TOF problem. Extensive experiments demonstrate that we improve the meta-learner with state-of-the-art performance on several few-shot learning benchmarks, and at the same time the TOF problem can also be released greatly.
Local SUSY-breaking minima in N_f=N_c SQCD?<|sep|>We study non-supersymmetric minima in N_f=N_c SQCD conjectured by Intriligator, Seiberg and Shih (ISS). We show that the existence of such minima depends on the signs of three non-calculable parameters and that no evidence can be inferred by deforming the theory. We illustrate this by demonstrating that the conjectured minimum is destabilized in a different deformation of N_f=N_c SQCD. We also comment briefly on the phenomenological consequences of this instability.
Segregation of polymers under cylindrical confinement: Effects of polymer topology and crowding<|sep|>Monte Carlo computer simulations are used to study the segregation behaviour of two polymers under cylindrical confinement. Using a multiple-histogram method, the conformational free energy, F, of the polymers was measured as a function of the centre-of-mass separation distance, \lambda. We examined the scaling of the free energy functions with the polymer length, the length and diameter of the confining cylinder, the polymer topology (i.e. linear vs ring polymers), and the packing fraction and size of mobile crowding agents. In the absence of crowders, the observed scaling of F(\lambda) is similar to that predicted using a simple model employing the de~Gennes blob model and the approximation that the free energy of overlapping chains in a tube is equal to that of two isolated chains each in a tube of half the cross-sectional area. Simulations were used to test the latter approximation and reveal that it yields poor quantitative predictions. This, along with generic finite-size effects, likely gives rise to the discrepancies between the predicted and measured values of scaling exponents for F(\lambda). For segregation in the presence of crowding agents, the free energy barrier generally decreases with increasing crowder packing fraction, thus reducing the entropic forces driving segregation. However, for fixed packing fraction, the barrier increases as the crowder/monomer size ratio decreases.
Data Augmentation for Skin Lesion Analysis<|sep|>Deep learning models show remarkable results in automated skin lesion analysis. However, these models demand considerable amounts of data, while the availability of annotated skin lesion images is often limited. Data augmentation can expand the training dataset by transforming input images. In this work, we investigate the impact of 13 data augmentation scenarios for melanoma classification trained on three CNNs (Inception-v4, ResNet, and DenseNet). Scenarios include traditional color and geometric transforms, and more unusual augmentations such as elastic transforms, random erasing and a novel augmentation that mixes different lesions. We also explore the use of data augmentation at test-time and the impact of data augmentation on various dataset sizes. Our results confirm the importance of data augmentation in both training and testing and show that it can lead to more performance gains than obtaining new images. The best scenario results in an AUC of 0.882 for melanoma classification without using external data, outperforming the top-ranked submission (0.874) for the ISIC Challenge 2017, which was trained with additional data.
A Synoptic Map of Halo Substructures from the Pan-STARRS1 3\pi\ Survey<|sep|>We present a panoramic map of the entire Milky Way halo north of dec~-30 degrees (~30,000 deg^2), constructed by applying the matched-filter technique to the Pan-STARRS1 3Pi Survey dataset. Using single-epoch photometry reaching to g~22, we are sensitive to stellar substructures with heliocentric distances between 3.5 and ~35 kpc. We recover almost all previously-reported streams in this volume and demonstrate that several of these are significantly more extended than earlier datasets have indicated. In addition, we also report five new candidate stellar streams. One of these features appears significantly broader and more luminous than the others and is likely the remnant of a dwarf galaxy. The other four streams are consistent with a globular cluster origin, and three of these are rather short in projection (<10 degrees), suggesting that streams like Ophiuchus may not be that rare. Finally, a significant number of more marginal substructures are also revealed by our analysis; many of these features can also be discerned in matched-filter maps produced by other authors from SDSS data, and hence they are very likely to be genuine. However, the extant 3Pi data is currently too shallow to determine their properties or produce convincing CMDs. The global view of the Milky Way provided by Pan-STARRS1 provides further evidence for the important role of both globular cluster disruption and dwarf galaxy accretion in building the Milky Way's stellar halo.
RR Lyrae in the LMC: Insights Into the Oosterhoff Phenomenon<|sep|>Although more than eight decades have passed since P. Th. Oosterhoff drew attention to differences in the properties of RR Lyrae variables in globular clusters, the origin and significance of the Oosterhoff groups remain unclear. Nonetheless, the accumulation of extensive new observations of RR Lyrae stars in globular clusters of the Milky Way and Local Group galaxies allows a fresh look at the phenomenon. Insights come not only from surveys of variables within the original Oosterhoff groups I and II but also from recent observations of the Oosterhoff-intermediate systems found especially in smaller Local Group galaxies. We will compare properties of RR Lyrae in several systems to investigate what they reveal about system-to-system differences of transition temperature between fundamental-mode and first overtone pulsators and of horizontal branch luminosity. Both transition temperature and horizontal branch luminosity have at various times been credited as playing roles in the creation of the Oosterhoff dichotomy.
A Study of the Radiative Ke3 Decay and Search for Direct Photon Emission with the KLOE Detector<|sep|>We present a measurement of the ratio R = \Gamma(\keg;\Estar>30\mev,\qstar>20^\circ)$/$\Gamma(\kegf)$ and a first measurement of the direct emission contribution in KL semileptonic decays. The measurement is done at the DAFNE phi-factory selecting phi->KL KS decays with the KLOE detector. We use 328 pb^{-1}$ of data corresponding to about 3.5 million Ke3(g) events and about 9000 radiative events. Our result is R=(924 +/- 23(stat) +/-16(syst)10^{-5} for the branching ratio and X=-2.3 +/- 1.3(stat) +/- 1.4(syst) for the parameter describing direct emission.
Dynamics of Modified Chaplygin Gas Inflation on the Brane with Bulk Viscous Pressure<|sep|>We investigate the role of bulk viscous pressure on the warm inflationary modified Chaplygin gas in brane-world framework in the presence of standard scalar field. We assume the intermediate inflationary scenario in strong dissipative regime and constructed the inflaton, potential, entropy density, slow-roll parameters, scalar and tensor power spectra, scalar spectral index and tensor-to-scalar ratio. We develop various trajectories such as $n_s - N$, $n_s - r$ and $n_s - \alpha_s$ (where $n_s$ is the spectral index, $\alpha_s$ is the running of spectral index, $N$ is the number of e-folds and $r$ is tensor-to-scalar ratio) for variable as well as constant dissipation and bulk viscous coefficients at high dissipative regime. It is interesting to remark here that our results of these parameters are compatible with recent observational data such as WMAP $7+9$, BICEP$2$ and Planck data.
Innovative observing strategy and orbit determination for Low Earth Orbit Space Debris<|sep|>We present the results of a large scale simulation, reproducing the behavior of a data center for the build-up and maintenance of a complete catalog of space debris in the upper part of the low Earth orbits region (LEO). The purpose is to determine the performances of a network of advanced optical sensors, through the use of the newest orbit determination algorithms developed by the Department of Mathematics of Pisa (DM). Such a network has been proposed to ESA in the Space Situational Awareness (SSA) framework by Carlo Gavazzi Space SpA (CGS), Istituto Nazionale di Astrofisica (INAF), DM, and Istituto di Scienza e Tecnologie dell'Informazione (ISTI-CNR). The conclusion is that it is possible to use a network of optical sensors to build up a catalog containing more than 98% of the objects with perigee height between 1100 and 2000 km, which would be observable by a reference radar system selected as comparison. It is also possible to maintain such a catalog within the accuracy requirements motivated by collision avoidance, and to detect catastrophic fragmentation events. However, such results depend upon specific assumptions on the sensor and on the software technologies.
Mass loss rate of accretion disk in FRADO<|sep|>We have developed the 2.5D version of the basic physically motivated 1D model of Czerny & Hryniewicz (2011), i.e. Failed Radiatively Accelerated Dusty Outflow (FRADO) model. This model is based on the idea that radiation pressure acting on dust is responsible for the formation of the low ionized part of the Broad Line Region (BLR). Such radiation pressure is strong enough to form a fast outflow from the disk surface in the inner part of low ionized BLR. The outflow properties depend on the basic physical parameters, like black hole mass, Eddington ratio and gas metallicity. We here aim at estimating the disk mass loss rate due to this process, and comparing the results with outflows detected in Broad Absorption Line (BAL) quasars.
Museum Automation with RFID<|sep|>By increase of culture and knowledge of the people, request for visiting museums has increased and made the management of these places more complex. Valuable things in a museum or ancient place must be maintained well and also it need to managing visitors. To maintain things we should prevent them from theft, as well as environmental factors such as temperature, humidity, PH, chemical factors and mechanical events should be monitored. And if the conditions are damaging, appropriate alerts or reports to managers and experts should be announced. Visitors should also be monitored, as well as visitors need to be guided and getting information in the environment. By utilizing RFID technology and short-distance network tools, technical solutions for more efficient management and more effective retention in museums can be implemented.
A Generic Storage API<|sep|>We present a generic API suitable for provision of highly generic storage facilities that can be tailored to produce various individually customised storage infrastructures. The paper identifies a candidate set of minimal storage system building blocks, which are sufficiently simple to avoid encapsulating policy where it cannot be customised by applications, and composable to build highly flexible storage architectures. Four main generic components are defined: the store, the namer, the caster and the interpreter. It is hypothesised that these are sufficiently general that they could act as building blocks for any information storage and retrieval system. The essential characteristics of each are defined by an interface, which may be implemented by multiple implementing classes.
Gas inflows towards the nucleus of the active galaxy NGC7213<|sep|>We present two-dimensional stellar and gaseous kinematics of the inner 0.8x1.1kpc^2 of the LINER/Seyfert 1 galaxy NGC7213, from optical spectra obtained with the GMOS integral field spectrograph on the Gemini South telescope at a spatial resolution of 60pc. The stellar kinematics shows an average velocity dispersion of 177km/s, circular rotation with a projected velocity amplitude of 50km/s and a kinematic major axis at a position angle of -4degrees (west of north). From the average velocity dispersion we estimate a black hole mass of M_BH=8_{-6}^{+16}x10^7 M_sun. The gas kinematics is dominated by non-circular motions, mainly along two spiral arms extending from the nucleus out to 4arcsec (280pc) to the NW and SE, that are cospatial with a nuclear dusty spiral seen in a structure map of the nuclear region of the galaxy. The projected gas velocities along the spiral arms show blueshifts in the far side and redshifts in the near side, with values of up to 200km/s. This kinematics can be interpreted as gas inflows towards the nucleus along the spiral arms if the gas is in the plane of the galaxy. We estimate the mass inflow rate using two different methods. The first is based of the observed velocities and geometry of the flow, and gives a mass inflow rate in the ionised gas of 7x10^-2 M_sun/yr. In the second method, we calculate the net ionised gas mass flow rate through concentric circles of decreasing radii around the nucleus resulting in mass inflow rates ranging from 0.4 M_sun/yr at 300pc down to 0.2 M_sun/yr at 100pc from the nucleus. These rates are larger than necessary to power the active nucleus.
Multi-Resolution Multi-Modal Sensor Fusion For Remote Sensing Data With Label Uncertainty<|sep|>In remote sensing, each sensor can provide complementary or reinforcing information. It is valuable to fuse outputs from multiple sensors to boost overall performance. Previous supervised fusion methods often require accurate labels for each pixel in the training data. However, in many remote sensing applications, pixel-level labels are difficult or infeasible to obtain. In addition, outputs from multiple sensors often have different resolution or modalities. For example, rasterized hyperspectral imagery presents data in a pixel grid while airborne Light Detection and Ranging (LiDAR) generates dense three-dimensional (3D) point clouds. It is often difficult to directly fuse such multi-modal, multi-resolution data. To address these challenges, we present a novel Multiple Instance Multi-Resolution Fusion (MIMRF) framework that can fuse multi-resolution and multi-modal sensor outputs while learning from automatically-generated, imprecisely-labeled data. Experiments were conducted on the MUUFL Gulfport hyperspectral and LiDAR data set and a remotely-sensed soybean and weed data set. Results show improved, consistent performance on scene understanding and agricultural applications when compared to traditional fusion methods.
An iterative hard thresholding estimator for low rank matrix recovery with explicit limiting distribution<|sep|>We consider the problem of low rank matrix recovery in a stochastically noisy high dimensional setting. We propose a new estimator for the low rank matrix, based on the iterative hard thresholding method, and that is computationally efficient and simple. We prove that our estimator is efficient both in terms of the Frobenius risk, and in terms of the entry-wise risk uniformly over any change of orthonormal basis. This result allows us, in the case where the design is Gaussian, to provide the limiting distribution of the estimator, which is of great interest for constructing tests and confidence sets for low dimensional subsets of entries of the low rank matrix.
Quantum memories based on engineered dissipation<|sep|>Storing quantum information for long times without disruptions is a major requirement for most quantum information technologies. A very appealing approach is to use self-correcting Hamiltonians, i.e. tailoring local interactions among the qubits such that when the system is weakly coupled to a cold bath the thermalization process takes a long time. Here we propose an alternative but more powerful approach in which the coupling to a bath is engineered, so that dissipation protects the encoded qubit against more general kinds of errors. We show that the method can be implemented locally in four dimensional lattice geometries by means of a toric code, and propose a simple 2D set-up for proof of principle experiments.
Optoelectronic properties of defective MoS$_2$ and WS$_2$ monolayers<|sep|>We theoretically explore the effect of metal and disulphur vacancies on electronic and optical properties of MoS$_2$ and WS$_2$ monolayers based on a Slater-Koster tight-binding model and including the spin-orbit coupling. We show that the vacancy defects create electronic flat bands by shifting the Fermi level towards the valence band, indicating that both types of vacancies may act as acceptor sites. The optical spectra of the pristine monolayers show step-like features corresponding to the transition from spin split valence band to the conduction band minimum, whereas the defective monolayers exhibit additional peaks in their spectra arising from induced midgap states in their band structures. We find that Mo and W vacancies contribute mostly in the low-energy optical spectrum, while the S$_2$ vacancies enhance the optical conductivity mainly in the visible range of the spectrum. This suggests that depending on the type of vacancy, the atomic defects in MoS$_2$ and WS$_2$ monolayers may increase the efficiency of solar cells used in photovoltaic systems.
A Spectroscopic Analysis of White Dwarfs in the Kiso Survey<|sep|>We present a spectroscopic analysis of white dwarfs found in the Kiso survey. Spectroscopic observations at high signal-to-noise ratio have been obtained for all DA and DB stars in the Kiso Schmidt ultraviolet excess survey (KUV stars). These observations led to the reclassification of several KUV objects, including the discovery of three unresolved DA+DB double degenerate binaries. The atmospheric parameters (Teff and log g) are obtained from detailed model atmosphere fits to optical spectroscopic data. The mass distribution of our sample is characterized by a mean value of 0.606 Msun and a dispersion of 0.135 Msun for DA stars, and 0.758 Msun and a dispersion of 0.192 Msun for DB stars. Absolute visual magnitudes obtained from our spectroscopic fits allow us to derive an improved luminosity function for the DA and DB stars identified in the Kiso survey. Our luminosity function is found to be significantly different from earlier estimates based on empirical photometric calibrations of Mv for the same sample. The results for the DA stars now appear entirely consistent with those obtained for the PG survey using the same spectroscopic approach. The space density for DA stars with Mv<12.75 is 2.80X10^-4 pc^-3 in the Kiso survey, which is 9.6% smaller than the value found in the PG survey. The completeness of both surveys is briefly discussed.
Collusion-Resistant Worker Set Selection for Transparent and Verifiable Voting<|sep|>Collusion occurs when multiple malicious participants of a distributed protocol work together to sabotage or spy on honest participants. Decentralized protocols often rely on a subset of participants called workers for critical operations. Collusion between workers can be particularly harmful to the security of the protocol. We propose two protocols that select a subset of workers from the set of participants such that the probability of the workers colluding together is minimized. Our first solution is a decentralized protocol that randomly selects workers in a verifiable manner without any trusted entities. The second solution is an algorithm that uses a social graph of participants and community detection to select workers that are socially distant in order to reduce the probability of collusion. We present our solutions in the context of a decentralized voting protocol proposed by Schiedermeier et al. [24] that guarantees transparency and verifiability. Enabling collusion-resistance in order to ensure democratic voting is clearly of paramount importance thus the voting protocol provides a suitable use case for our solutions.
Dark Matter scenarios at IceCube<|sep|>The recent study on the the 6-year up-going muon neutrinos by the IceCube Collaboration and the multi-messenger analyses support the hypothesis of a two-component scenario explaining the diffuse TeV-PeV neutrino flux. Depending on the steepness of the astrophysical power-law, an excess in the IceCube data is shown in the energy range 10-100 TeV (low-energy excess) or at PeV (high-energy excess). In both cases, we characterize a two-component neutrino flux where decaying Dark Matter particles provide a contribution to the IceCube observations.
Few-shot Action Recognition with Permutation-invariant Attention<|sep|>Many few-shot learning models focus on recognising images. In contrast, we tackle a challenging task of few-shot action recognition from videos. We build on a C3D encoder for spatio-temporal video blocks to capture short-range action patterns. Such encoded blocks are aggregated by permutation-invariant pooling to make our approach robust to varying action lengths and long-range temporal dependencies whose patterns are unlikely to repeat even in clips of the same class. Subsequently, the pooled representations are combined into simple relation descriptors which encode so-called query and support clips. Finally, relation descriptors are fed to the comparator with the goal of similarity learning between query and support clips. Importantly, to re-weight block contributions during pooling, we exploit spatial and temporal attention modules and self-supervision. In naturalistic clips (of the same class) there exists a temporal distribution shift--the locations of discriminative temporal action hotspots vary. Thus, we permute blocks of a clip and align the resulting attention regions with similarly permuted attention regions of non-permuted clip to train the attention mechanism invariant to block (and thus long-term hotspot) permutations. Our method outperforms the state of the art on the HMDB51, UCF101, miniMIT datasets.
ConStance: Modeling Annotation Contexts to Improve Stance Classification<|sep|>Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis, sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on Twitter, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the model's interpretable parameters shed light on the effects of each context.
Active Ornstein-Uhlenbeck particles<|sep|>Active Ornstein-Uhlenbeck particles (AOUPs) are overdamped particles in an interaction potential subject to external Ornstein-Uhlenbeck noises. They can be transformed into a system of underdamped particles under additional velocity dependent forces and subject to white noise forces. There has been some discussion in the literature on whether AOUPs can be in equilibrium for particular interaction potentials and how far from equilibrium they are in the limit of small persistence time. By using a theorem on the time reversed form of the AOUP Langevin-Ito equations, I prove that they have an equilibrium probability density invariant under time reversal if and only if their smooth interaction potential has zero third derivatives. In the limit of small persistence Ornstein-Uhlenbeck time $\tau$, a Chapman-Enskog expansion of the Fokker-Planck equation shows that the probability density has a local equilibrium solution in the particle momenta modulated by a reduced probability density that varies slowly with the position. The reduced probability density satisfies a continuity equation in which the probability current has an asymptotic expansion in powers of $\tau$. Keeping up to $O(\tau)$ terms, this equation is a diffusion equation, which has an equilibrium stationary solution with zero current. However, $O(\tau^2)$ terms contain fifth and sixth order spatial derivatives and the continuity equation no longer has a zero current stationary solution. The expansion of the overall stationary solution now contains odd terms in the momenta, which clearly shows that it is not an equilibrium.
Fully Proprioceptive Slip-Velocity-Aware State Estimation for Mobile Robots via Invariant Kalman Filtering and Disturbance Observer<|sep|>This paper develops a novel slip estimator using the invariant observer design theory and Disturbance Observer (DOB). The proposed state estimator for mobile robots is fully proprioceptive and combines data from an inertial measurement unit and body velocity within a Right Invariant Extended Kalman Filter (RI-EKF). By embedding the slip velocity into $\mathrm{SE}_3(3)$ Lie group, the developed DOB-based RI-EKF provides real-time accurate velocity and slip velocity estimates on different terrains. Experimental results using a Husky wheeled robot confirm the mathematical derivations and show better performance than a standard RI-EKF baseline. Open source software is available for download and reproducing the presented results.
Structure Learning in Bayesian Networks of Moderate Size by Efficient Sampling<|sep|>We study the Bayesian model averaging approach to learning Bayesian network structures (DAGs) from data. We develop new algorithms including the first algorithm that is able to efficiently sample DAGs according to the exact structure posterior. The DAG samples can then be used to construct estimators for the posterior of any feature. We theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state-of-the-art methods.
Misassembly Detection using Paired-End Sequence Reads and Optical Mapping Data<|sep|>A crucial problem in genome assembly is the discovery and correction of misassembly errors in draft genomes. We develop a method that will enhance the quality of draft genomes by identifying and removing misassembly errors using paired short read sequence data and optical mapping data. We apply our method to various assemblies of the loblolly pine and Francisella tularensis genomes. Our results demonstrate that we detect more than 54% of extensively misassembled contigs and more than 60% of locally misassembed contigs in an assembly of Francisella tularensis, and between 31% and 100% of extensively misassembled contigs and between 57% and 73% of locally misassembed contigs in the assemblies of loblolly pine. MISSEQUEL can be downloaded at http://www.cs.colostate.edu/seq/.
Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning, Extended version<|sep|>This work tackles the problem of robust zero-shot planning in non-stationary stochastic environments. We study Markov Decision Processes (MDPs) evolving over time and consider Model-Based Reinforcement Learning algorithms in this setting. We make two hypotheses: 1) the environment evolves continuously with a bounded evolution rate; 2) a current model is known at each decision epoch but not its evolution. Our contribution can be presented in four points. 1) we define a specific class of MDPs that we call Non-Stationary MDPs (NSMDPs). We introduce the notion of regular evolution by making an hypothesis of Lipschitz-Continuity on the transition and reward functions w.r.t. time; 2) we consider a planning agent using the current model of the environment but unaware of its future evolution. This leads us to consider a worst-case method where the environment is seen as an adversarial agent; 3) following this approach, we propose the Risk-Averse Tree-Search (RATS) algorithm, a zero-shot Model-Based method similar to Minimax search; 4) we illustrate the benefits brought by RATS empirically and compare its performance with reference Model-Based algorithms.
Planck intermediate results. XVI. Profile likelihoods for cosmological parameters<|sep|>We explore the 2013 Planck likelihood function with a high-precision multi-dimensional minimizer (Minuit). This allows a refinement of the Lambda-cdm best-fit solution with respect to previously-released results, and the construction of frequentist confidence intervals using profile likelihoods. The agreement with the cosmological results from the Bayesian framework is excellent, demonstrating the robustness of the Planck results to the statistical methodology. We investigate the inclusion of neutrino masses, where more significant differences may appear due to the non-Gaussian nature of the posterior mass distribution. By applying the Feldman--Cousins prescription, we again obtain results very similar to those of the Bayesian methodology. However, the profile-likelihood analysis of the CMB combination (Planck+WP+highL) reveals a minimum well within the unphysical negative-mass region. We show that inclusion of the Planck CMB-lensing information regularizes this issue, and provide a robust frequentist upper limit $M_\nu < 0.26 eV$ ($95%$ confidence) from the CMB+lensing+BAO data combination.
Hysteresis and synchronization processes of Kuramoto oscillators on high-dimensional simplicial complexes with the competing simplex-encoded couplings<|sep|>Recent studies of dynamic properties in complex systems point out the profound impact of hidden geometry features known as simplicial complexes, which enable geometrically conditioned many-body interactions. Studies of collective behaviours on the controlled-structure complexes can reveal the subtle interplay of geometry and dynamics. Here, we investigate the phase synchronisation dynamics under the competing interactions embedded on 1-simplex (edges) and 2-simplex (triangles) faces of a homogeneous 4-dimensional simplicial complex. Its underlying network is a 1-hyperbolic graph with the assortative correlations among the node's degrees and the spectral dimension that exceeds $d_s=4$. We determine the time-averaged system's order parameter to characterise the synchronisation level. In the absence of higher interactions, the complete synchrony is continuously reached with the increasing positive pairwise interactions ($K_1>0$), and a partial synchronisation for the negative couplings ($K_1<0$) with no apparent hysteresis. Similar behaviour occurs in the degree-preserved randomised network. In contrast, the synchronisation is absent for the negative pairwise coupling in the entirely random graph and simple scale-free networks. Increasing the strength $K_2\neq 0$ of the triangle-based interactions gradually hinders the synchronisation promoted by pairwise couplings, and the non-symmetric hysteresis loop opens with an abrupt desynchronisation transition towards the $K_1<0$ branch. However, for substantial triangle-based interactions, the frustration effects prevail, preventing the complete synchronisation, and the abrupt transition disappears. These findings shed new light on the mechanisms by which the high-dimensional simplicial complexes in natural systems, such as human connectomes, can modulate their native synchronisation processes.
On Event-Based Sampling for LQG-Optimal Control<|sep|>We consider the problem of finding an event-based sampling scheme that optimizes the trade-off between average sampling rate and control performance in a linear-quadratic-Gaussian (LQG) control problem setting with output feedback. Our analysis is based on a recently presented sampled-data controller structure, which remains LQG-optimal for any choice of sampling scheme. We show that optimization of the sampling scheme is related to an elliptic convection-diffusion type partial differential equation over a domain with free boundary, a so called Stefan problem. A numerical method is presented to solve this problem for second order systems, and thus obtain an optimal sampling scheme. The method also directly generalizes to higher order systems, although with a higher computational cost. For the special case of multidimensional integrator systems, we present the optimal sampling scheme on closed form, and prove that it will always outperform its periodic counterpart. Tight bounds on the improvement are presented. The improved performance is also demonstrated in numerical examples, both for an integrator system and a more general case.
Sudden switch of generalized Lieb-Robinson velocity in a transverse field Ising spin chain<|sep|>The Lieb-Robinson theorem states that the speed at which the correlations between two distant nodes in a spin network can be built through local interactions has an upper bound, which is called the Lieb-Robinson velocity. Our central aim is to demonstrate how to observe the Lieb-Robinson velocity in an Ising spin chain with a strong transverse field. We adopt and compare four correlation measures for characterizing different types of correlations, which include correlation function, mutual information, quantum discord, and entanglement of formation. We prove that one of correlation functions shows a special behavior depending on the parity of the spin number. All the information-theoretical correlation measures demonstrate the existence of the Lieb-Robinson velocity. In particular, we find that there is a sudden switch of the Lieb-Robinson speed with the increasing of the number of spin.
LSTM-based Deep Learning Models for Non-factoid Answer Selection<|sep|>In this paper, we apply a general deep learning (DL) framework for the answer selection task, which does not depend on manually defined features or linguistic tools. The basic framework is to build the embeddings of questions and answers based on bidirectional long short-term memory (biLSTM) models, and measure their closeness by cosine similarity. We further extend this basic model in two directions. One direction is to define a more composite representation for questions and answers by combining convolutional neural network with the basic framework. The other direction is to utilize a simple but efficient attention mechanism in order to generate the answer representation according to the question context. Several variations of models are provided. The models are examined by two datasets, including TREC-QA and InsuranceQA. Experimental results demonstrate that the proposed models substantially outperform several strong baselines.
The SAMI Galaxy Survey: Gas velocity dispersions in low-$z$ star-forming galaxies and the drivers of turbulence<|sep|>We infer the intrinsic ionised gas kinematics for 383 star-forming galaxies across a range of integrated star-formation rates (SFR $\in [10^{-3}, 10^2]$ M$_\odot$ yr$^{-1}$) at $z \lesssim 0.1$ using a consistent 3D forward-modelling technique. The total sample is a combination of galaxies from the SAMI Galaxy Survey and DYNAMO survey. For typical low-$z$ galaxies taken from the SAMI Galaxy Survey, we find the vertical velocity dispersion ($\sigma_{v, z}$) to be positively correlated with measures of star-formation rate, stellar mass, HI gas mass, and rotational velocity. The greatest correlation is with star-formation rate surface density ($\Sigma_\text{SFR}$). Using the total sample, we find $\sigma_{v, z}$ increases slowly as a function of integrated star-formation rate in the range SFR $\in$ [$10^{-3}$, 1] M$_\odot$ yr$^{-1}$ from $17\pm3$ km s$^{-1}$ to $24\pm5$ km s$^{-1}$ followed by a steeper increase up to $\sigma_{v, z}$ $\sim 80$ km s$^{-1}$ for SFR $\gtrsim 1$ M$_\odot$ yr$^{-1}$. This is consistent with recent theoretical models that suggest a $\sigma_{v, z}$ floor driven by star-formation feedback processes with an upturn in $\sigma_{v, z}$ at higher SFR driven by gravitational transport of gas through the disc.
TANAMI: Tracking Active Galactic Nuclei with Austral Milliarcsecond Interferometry I. First-Epoch 8.4 GHz Images<|sep|>We introduce the TANAMI program (Tracking Active Galactic Nuclei with Austral Milliarcsecond Interferometry) which is monitoring an initial sample of 43 extragalactic jets located south of -30 degrees declination at 8.4 GHz and 22 GHz since 2007. All aspects of the program are discussed. First epoch results at 8.4 GHz are presented along with physical parameters derived therefrom. We present first epoch images for 43 sources, some observed for the first time at milliarcsecond resolution. Parameters of these images as well as physical parameters derived from them are also presented and discussed. These and subsequent images from the TANAMI survey are available at http://pulsar.sternwarte.uni-erlangen.de/tanami/ We obtain reliable, high dynamic range images of the southern hemisphere AGN. All the quasars and BL Lac objects in the sample have a single-sided radio morphology. Galaxies are either double-sided, single-sided or irregular. About 28% of the TANAMI sample has been detected by LAT during its first three months of operations. Initial analysis suggests that when galaxies are excluded, sources detected by LAT have larger opening angles than those not detected by LAT. Brightness temperatures of LAT detections and non-detections seem to have similar distributions. The redshift distributions of the TANAMI sample and sub-samples are similar to those seen for the bright gamma-ray AGN seen by LAT and EGRET but none of the sources with a redshift above 1.8 have been detected by LAT.
Similarity-Based Classification in Partially Labeled Networks<|sep|>We propose a similarity-based method, using the similarity between nodes, to address the problem of classification in partially labeled networks. The basic assumption is that two nodes are more likely to be categorized into the same class if they are more similar. In this paper, we introduce ten similarity indices, including five local ones and five global ones. Empirical results on the co-purchase network of political books show that the similarity-based method can give high accurate classification even when the labeled nodes are sparse which is one of the difficulties in classification. Furthermore, we find that when the target network has many labeled nodes, the local indices can perform as good as those global indices do, while when the data is sparce the global indices perform better. Besides, the similarity-based method can to some extent overcome the unconsistency problem which is another difficulty in classification.
Towards a Gravity Dual for the Large Scale Structure of the Universe<|sep|>The dynamics of the large-scale structure of the universe enjoys at all scales, even in the highly non-linear regime, a Lifshitz symmetry during the matter-dominated period. In this paper we propose a general class of six-dimensional spacetimes which could be a gravity dual to the four-dimensional large-scale structure of the universe. In this set-up, the Lifshitz symmetry manifests itself as an isometry in the bulk and our universe is a four-dimensional brane moving in such six-dimensional bulk. After finding the correspondence between the bulk and the brane dynamical Lifshitz exponents, we find the intriguing result that the preferred value of the dynamical Lifshitz exponent of our observed universe, at both linear and non-linear scales, corresponds to a fixed point of the RGE flow of the dynamical Lifshitz exponent in the dual system where the symmetry is enhanced to the Schrodinger group containing a non-relativistic conformal symmetry. We also investigate the RGE flow between fixed points of the Lifshitz dynamical exponent in the bulk and observe that this flow is reflected in a growth rate of the large-scale structure, which seems to be in qualitative agreement with what is observed in current data. Our set-up might provide an interesting new arena for testing the ideas of holography and gravitational duals.
Fast Online Deconvolution of Calcium Imaging Data<|sep|>Fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations, but extracting the activity of each neuron from raw fluorescence calcium imaging data is a nontrivial problem. We present a fast online active set method to solve this sparse non-negative deconvolution problem. Importantly, the algorithm progresses through each time series sequentially from beginning to end, thus enabling real-time online estimation of neural activity during the imaging session. Our algorithm is a generalization of the pool adjacent violators algorithm (PAVA) for isotonic regression and inherits its linear-time computational complexity. We gain remarkable increases in processing speed: more than one order of magnitude compared to currently employed state of the art convex solvers relying on interior point methods. Unlike these approaches, our method can exploit warm starts; therefore optimizing model hyperparameters only requires a handful of passes through the data. A minor modification can further improve the quality of activity inference by imposing a constraint on the minimum spike size. The algorithm enables real-time simultaneous deconvolution of $O(10^5)$ traces of whole-brain larval zebrafish imaging data on a laptop.
On the Parameterized Complexity of Default Logic and Autoepistemic Logic<|sep|>We investigate the application of Courcelle's Theorem and the logspace version of Elberfeld etal. in the context of the implication problem for propositional sets of formulae, the extension existence problem for default logic, as well as the expansion existence problem for autoepistemic logic and obtain fixed-parameter time and space efficient algorithms for these problems. On the other hand, we exhibit, for each of the above problems, families of instances of a very simple structure that, for a wide range of different parameterizations, do not have efficient fixed-parameter algorithms (even in the sense of the large class XPnu), unless P=NP.
Rapid Application of the Spherical Harmonic Transform via Interpolative Decomposition Butterfly Factorization<|sep|>We describe an algorithm for the application of the forward and inverse spherical harmonic transforms. It is based on a new method for rapidly computing the forward and inverse associated Legendre transforms by hierarchically applying the interpolative decomposition butterfly factorization (IDBF). Experimental evidence suggests that the total running time of our method -- including all necessary precomputations -- is $\mathcal{O}(N^2 \log^3(N))$, where $N$ is the order of the transform. This is nearly asymptotically optimal. Moreover, unlike existing algorithms which are asymptotically optimal or nearly so, the constant in the running time of our algorithm is small enough to make it competitive with state-of-the-art $\mathcal{O}\left(N^3\right)$ methods at relatively small values of $N$. Numerical results are provided to demonstrate the effectiveness and numerical stability of the new framework.
Locally Repairable Codes and Matroid Theory<|sep|>Locally repairable codes (LRCs) are error correcting codes used in distributed data storage. A traditional approach is to look for codes which simultaneously maximize error tolerance and minimize storage space consumption. However, this tends to yield codes for which error correction requires an unrealistic amount of communication between storage nodes. LRCs solve this problem by allowing errors to be corrected locally. This thesis reviews previous results on the subject presented in [1]. These include that every almost affine LRC induces a matroid such that the essential properties of the code are determined by the matroid. Also, the generalized Singleton bound for LRCs can be extended to matroids as well. Then, matroid theory can be used to find classes of matroids that either achieve the bound, meaning they are optimal in a certain sense, or at least come close to the bound. This thesis presents an improvement to the results of [1] in both of these cases. [1] T. Westerb\"ack, R. Freij, T. Ernvall and C. Hollanti, "On the Combinatorics of Locally Repairable Codes via Matroid Theory", arXiv:1501.00153 [cs.IT], 2014.
Majorana Higgses at colliders<|sep|>Collider signals of heavy Majorana neutrino mass origin are studied in the minimal Left-Right symmetric model, where their mass is generated spontaneously together with the breaking of lepton number. The right-handed triplet Higgs boson $\Delta$, responsible for such breaking, can be copiously produced at the LHC through the Higgs portal in the gluon fusion and less so in gauge mediated channels. At $\Delta$ masses below the opening of the $VV$ decay channel, the two observable modes are pair-production of heavy neutrinos via the triplet gluon fusion $gg \to \Delta \to NN$ and pair production of triplets from the Higgs $h \to \Delta \Delta \to 4N$ decay. The latter features tri- and quad same-sign lepton final states that break lepton number by four units and have no significant background. In both cases up to four displaced vertices may be present and their displacement may serve as a discriminating variable. The backgrounds at the LHC, including the jet fake rate, are estimated and the resulting sensitivity to the Left-Right breaking scale extends well beyond 10 TeV. In addition, sub-dominant radiative modes are surveyed: the $\gamma \gamma$, $Z \gamma$ and lepton flavour violating ones. Finally, prospects for $\Delta$ signals at future $e^+ e^-$ colliders are presented.
Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis<|sep|>This paper studies a combination of generative Markov random field (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controling the image layout at an abstract level. We apply the method to both photographic and non-photo-realistic (artwork) synthesis tasks. The MRF regularizer prevents over-excitation artifacts and reduces implausible feature mixtures common to previous dCNN inversion approaches, permitting synthezing photographic content with increased visual plausibility. Unlike standard MRF-based texture synthesis, the combined system can both match and adapt local features with considerable variability, yielding results far out of reach of classic generative MRF methods.
Disentangling the Black Hole Vacuum<|sep|>We study the question whether disentanglement of Hawking radiation can be achieved with any local operation. We assume that the operation we look for is unitary and can be described by a Bogoliubov transformation. This allows to formulate requirements on the operation of disentanglement. We then show that these requirements can be fulfilled by a timelike boundary condition in the near-horizon area and that the local observer does not notice the presence of the boundary and does not encounter a firewall.
Measurement simulability and incompatibility in quantum theory and other operational theories<|sep|>In this thesis, we consider the properties of measurements in quantum theory and other operational theories. After having introduced the framework of operational theories, we consider a communication scheme based on an experimental prepare-and-measure scenario and demonstrate this with different communication tasks. This gives us context for how the different communication tasks can be implemented in different theories, in doing so establishing quantum theory intuitively as an operational theory among other theories. The main property of measurements we focus on in this work is the simulation of measurements, which consists of manipulating the inputs and outputs of the measurement devices. We study how using this process on existing measurement devices can be used to operationally imitate new devices, and what kind of structure the simulation process induces on measurements. We also consider applications of simulability. Firstly, we consider operational restrictions imposed upon measurements. We argue that the restricted set of physical measurements must be closed with respect to the simulation process since the simulation of physical devices must lead to other physically feasible devices. We demonstrate different types of restrictions by classifying them. As a second application we see how the simulation of measurements relates to compatibility of measurements and how it can be viewed as a generalisation of it. This allows us to present an operational principle previously known to quantum theory, the no-free-information principle, according to which any measurement that is compatible with all other measurement must not provide any useful, and therefore free, information about the system. Whilst this principle holds in quantum theory, there are non-classical theories for which it is violated, and so enforcing this principle may be considered a way to exclude some unphysical theories.
Frustration of signed networks: How does it affect the thermodynamic properties of a system?<|sep|>Signed networks with positive and negative interaction are widely observed in the real systems. The negative links would induce frustration, then affect global properties of the system. Based on previous studies, frustration of signed networks is investigated and quantified. Frustrations of $\pm J$ (Edwards-Anderson) Ising model with a concentration $p$ of negative bonds, constructed on different networks, such as triangular lattice, square lattice and random regular networks (RRN) with connectivity $k=6$ are estimated by theoretical and numerical approaches. Based on the quantitative measurement of frustration, its effects on phase transitions characterized by order parameter $q_{EA}$ are studied. The relationship of critical temperature $T_c$ with the quantified frustration $\mu$ is given by mean-field theory. It shows that $T_c$ decreases linearly with frustration $\mu$ . The theory is checked by numerical estimations, such as the Metropolis algorithm and Replica Symmetric Population Dynamics Algorithm. The numerical estimates are consistent well with the mean-field prediction.
Resonant state selection in synthetic ferrimagnets<|sep|>Resonant activation of a synthetic antiferromagnet (SAF) is known to result in a dynamic running state, where the SAF's symmetric spin-flop pair continuously rotates between the two antiparallel ground states of the system, with the two magnetic moments in-phase in the so-called acoustical spin-resonance mode. The symmetry of an ideal SAF does not allow, however, to deterministically select a particular ground state using a resonant excitation. In this work, we study asymmetric SAF's, or synthetic ferrimagnets (SFi), in which the two magnetic particles are different in thickness or are biased asymmetrically with an external field. We show how the magnetic phase space of the system can be reversibly tuned, post-fabrication, between the antiferro- and ferri-magnetic behavior by exploiting these two asymmetry parameters and applying a uniform external field. We observe a splitting of the optical spin-resonance for the two ground states of the SFi system, with a frequency spacing that can be controlled by a quasistatic uniform external field. We demonstrate how the tunable magnetic asymmetry in SFi allows to deterministically select a particular ground state using the splitting of the optical spin-resonance. These results offer a new way of controlling the magnetic state of a spin-flop bilayer, currently used in such large scale applications as magnetic memory.
Electroweak & QCD corrections to Drell Yan processes<|sep|>The relevance of single-W and single-Z production processes at hadron colliders is well known: in the present paper the status of theoretical calculations of Drell-Yan processes is summarized and some results on the combination of electroweak and QCD corrections to a sample of observables of the process $p p \to W^\pm \to \mu^\pm + X$ at the LHC are discussed. The phenomenological analysis shows that a high-precision knowledge of QCD and a careful combination of electroweak and strong contributions is mandatory in view of the anticipated LHC experimental accuracy. One of the authors (O.N.) dedicates these notes to Prof. S. Jadach, in honour of his 60th birthday and grateful for all that Prof. Jadach taught him during their fruitful collaboration.
OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings<|sep|>Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this challenge, we propose OSCaR (Orthogonal Subspace Correction and Rectification), a bias-mitigating method that focuses on disentangling biased associations between concepts instead of removing concepts wholesale. Our experiments on gender biases show that OSCaR is a well-balanced approach that ensures that semantic information is retained in the embeddings and bias is also effectively mitigated.
Sparse Inverse Covariance Estimation via an Adaptive Gradient-Based Method<|sep|>We study the problem of estimating from data, a sparse approximation to the inverse covariance matrix. Estimating a sparsity constrained inverse covariance matrix is a key component in Gaussian graphical model learning, but one that is numerically very challenging. We address this challenge by developing a new adaptive gradient-based method that carefully combines gradient information with an adaptive step-scaling strategy, which results in a scalable, highly competitive method. Our algorithm, like its predecessors, maximizes an $\ell_1$-norm penalized log-likelihood and has the same per iteration arithmetic complexity as the best methods in its class. Our experiments reveal that our approach outperforms state-of-the-art competitors, often significantly so, for large problems.
High-Scalability CMOS Quantum Magnetometer with Spin-State Excitation and Detection of Diamond Color Centers<|sep|>Magnetometers based on quantum mechanical processes enable high sensitivity and long-term stability without the need for re-calibration, but their integration into fieldable devices remains challenging. This paper presents a CMOS quantum vector-field magnetometer that miniaturizes the conventional quantum sensing platforms using nitrogen-vacancy (NV) centers in diamond. By integrating key components for spin control and readout, the chip performs magnetometry through optically detected magnetic resonance (ODMR) through a diamond slab attached to a custom CMOS chip. The ODMR control is highly uniform across the NV centers in the diamond, which is enabled by a CMOS-generated $\sim$2.87 GHz magnetic field with <5% inhomogeneity across a large-area current-driven wire array. The magnetometer chip is 1.5 mm$^2$ in size, prototyped in 65-nm bulk CMOS technology, and attached to a 300$\times$80 $\mu$m2 diamond slab. NV fluorescence is measured by CMOS-integrated photodetectors. This on-chip measurement is enabled by efficient rejection of the green pump light from the red fluorescence through a CMOS-integrated spectral filter based on a combination of spectrally dependent plasmonic losses and diffractive filtering in the CMOS back-end-of-line (BEOL). This filter achieves $\sim$25 dB of green light rejection. We measure a sensitivity of 245 nT/Hz$^{1/2}$, marking a 130$\times$ improvement over a previous CMOS-NV sensor prototype, largely thanks to the better spectral filtering and homogeneous microwave generation over larger area.
Ensemble-filtered vortex modeling of strongly disturbed aerodynamic flows<|sep|>The task of dynamic flow estimation is to construct an approximation of an evolving flow---and particularly, its response to disturbances---using measurements from available sensors. Building from previous work by Darakananda et al.~(Phys Rev Fluids 2018), we further develop an ensemble Kalman filter (EnKF) framework for aerodynamic flows based on an ensemble of randomly-perturbed inviscid vortex models of flow about an infinitely-thin plate. In the forecast step, vortex elements in each ensemble member are advected by the flow and new elements are released from each edge of the plate; the elements are aggregated to maintain an efficient representation. The vortex elements and leading edge constraint are corrected in the analysis step by assimilating the surface pressure differences across the plate measured from the truth system. We show that the overall framework can be physically interpreted as a series of adjustments to the position and shape of an elliptical region of uncertainty associated with each vortex element. In this work, we compare the previously-used stochastic EnKF with the ensemble transform Kalman filter (ETKF), which uses a deterministic analysis step. We examine the response of the flat plate at $20^\circ$ in two perturbed flows, with truth data obtained from high-fidelity simulation at Reynolds number 500. In the first case, we apply a sequence of large-amplitude pulses near the leading edge of the plate to mimic flow actuation. In the second, we place the plate in a vortex street wake behind a cylinder. In both cases, we show that the vortex-based framework accurately estimates the pressure distribution and normal force, with no {\em a priori} knowledge of the perturbations. We show that the ETKF is consistently more robust than the stochastic EnKF. Finally, we examine the mapping from measurements to state update in the analysis step through SVD of the Kalman gain.
On the existence, uniqueness and regularity of solutions of a viscoelastic Stokes problem modelling salt rocks<|sep|>A Stokes-type problem for a viscoelastic model of salt rocks is considered, and existence, uniqueness and regularity are investigated in the scale of $L^2$-based Sobolev spaces. The system is transformed into a generalized Stokes problem, and the proper conditions on the parameters of the model that guarantee that the system is uniformly elliptic are given. Under those conditions, existence, uniqueness and low-order regularity are obtained under classical regularity conditions on the data, while higher-order regularity is proved under less stringent conditions than classical ones. Explicit estimates for the solution in terms of the data are given accordingly.
Particle Models and the Small-Scale Structure of Dark Matter<|sep|>The kinetic decoupling of weakly interacting massive particles (WIMPs) in the early universe sets a scale that can directly be translated into a small-scale cutoff in the spectrum of matter density fluctuations. The formalism presented here allows a precise description of the decoupling process and thus the determination of this scale to a high accuracy from the details of the underlying WIMP microphysics. With decoupling temperatures of several MeV to a few GeV, the smallest protohalos to be formed range between 10^{-11} and almost 10^{-3} solar masses -- a somewhat smaller range than what was found earlier using order-of-magnitude estimates for the decoupling temperature; for a given WIMP model, the actual cutoff mass is typically about a factor of 10 greater than derived in that way, though in some cases the difference may be as large as a factor of several 100. Observational consequences and prospects to probe this small-scale cutoff, which would provide a fascinating new window into the particle nature of dark matter, are discussed
Fully refractive adaptive optics fluorescence microscope using an optofluidic wavefront modulator<|sep|>Adaptive optics (AO) is a powerful image correction technique with proven benefits for many life-science microscopy methods. However, the complexity of adding a reflective wavefront modulator and a wavefront sensor into already complicated microscope has made AO prohibitive for its widespread adaptation in microscopy systems. We present here the design and performance of a compact fluorescence microscope using a fully refractive optofluidic wavefront modulator yielding imaging performance on par with that of conventional deformable mirrors, both in correction fidelity and articulation. We combine this device with a modal sensorless wavefront estimation algorithm that uses spatial frequency content of acquired images as a quality metric and thereby demonstrate a completely in-line adaptive optics microscope which can perform aberration correction up to 4$^{th}$ radial order of Zernike modes. This entirely new concept for adaptive optics microscopy may prove to extend the performance limits and widespread applicability of AO in life science imaging.
Formation of Transient Coronal Holes during Eruption of a Quiescent Filament and its Overlying Sigmoid<|sep|>By using H$\alpha$, He I 10830, EUV and soft X-ray (SXR) data, we examined a filament eruption that occurred on a quiet-sun region near the center of the solar disk on 2006 January 12, which disturbed a sigmoid overlying the filament channel observed by the $\emph{GOES-12}$ SXR Imager (SXI), and led to the eruption of the sigmoid. The event was associated with a partial halo coronal mass ejection (CME) observed by the Large Angle and Spectrometric Coronagraphs (LASCO) on board the Solar and Heliospheric Observatory ($\emph{SOHO}$), and resulted in the formation of two flare-like ribbons, post-eruption coronal loops, and two transient coronal holes (TCHs), but there were no significantly recorded $\emph{GOES}$ or H$\alpha$ flares corresponding to the eruption. The two TCHs were dominated by opposite magnetic polarities and were located on the two ends of the eruptive sigmoid. They showed similar locations and shapes in He I 10830, EUV and SXR observations. During the early eruption phase, brightenings first appeared on the locations of the two subsequent TCHs, which could be clearly identified on He I 10830, EUV and SXR images. This eruption event could be explained by the magnetic flux rope model, and the two TCHs were likely to be the feet of the flux rope.
Spectral shifting strongly constrains molecular cloud disruption by radiation pressure on dust<|sep|>${\bf Aim:}$ To test the hypothesis that radiation pressure from star clusters acting on dust is the dominant feedback agent disrupting the largest star-forming molecular clouds and thus regulating the star-formation process. ${\bf Methods:}$ We perform multi-frequency, 3D, RT calculations including scattering, absorption, and re-emission to longer wavelengths for clouds with masses of $10^4$-$10^7\,$M$_{\odot}$, with embedded clusters and a star formation efficiencies of 0.009%-91%, and varying maximum grain sizes up to 200$\,\mu$m. We calculate the ratio between radiative force and gravity to determine whether radiation pressure can disrupt clouds. ${\bf Results:}$ We find that radiation acting on dust almost never disrupts star-forming clouds. UV and optical photons to which the cloud is optically thick do not scatter much. Instead, they quickly get absorbed and re-emitted by at thermal wavelengths. As the cloud is typically optically thin to far-IR radiation, it promptly escapes, depositing little momentum. The resulting spectrum is more narrowly peaked than the corresponding Planck function with an extended tail at longer wavelengths. As the opacity drops significantly across the sub-mm and mm, the resulting radiative force is even smaller than for the corresponding single-temperature black body. The force from radiation pressure falls below the strength of gravitational attraction by an order of magnitude or more for either Milky Way or starbust conditions. For unrealistically large maximum grain sizes, and star formation efficiencies far exceeding 50% do we find that the strength of radiation pressure can exceed gravity. ${\bf Conclusions:}$ We conclude that radiation pressure acting on dust does not disrupt star-forming molecular clouds in any Local Group galaxies. Radiation pressure thus appears unlikely to regulate the star-formation process on either local or global scales.
Reinforcement Learning Guided by Provable Normative Compliance<|sep|>Reinforcement learning (RL) has shown promise as a tool for engineering safe, ethical, or legal behaviour in autonomous agents. Its use typically relies on assigning punishments to state-action pairs that constitute unsafe or unethical choices. Despite this assignment being a crucial step in this approach, however, there has been limited discussion on generalizing the process of selecting punishments and deciding where to apply them. In this paper, we adopt an approach that leverages an existing framework -- the normative supervisor of (Neufeld et al., 2021) -- during training. This normative supervisor is used to dynamically translate states and the applicable normative system into defeasible deontic logic theories, feed these theories to a theorem prover, and use the conclusions derived to decide whether or not to assign a punishment to the agent. We use multi-objective RL (MORL) to balance the ethical objective of avoiding violations with a non-ethical objective; we will demonstrate that our approach works for a multiplicity of MORL techniques, and show that it is effective regardless of the magnitude of the punishment we assign.
Improving Generalization by Controlling Label-Noise Information in Neural Network Weights<|sep|>In the presence of noisy or incorrect labels, neural networks have the undesirable tendency to memorize information about the noise. Standard regularization techniques such as dropout, weight decay or data augmentation sometimes help, but do not prevent this behavior. If one considers neural network weights as random variables that depend on the data and stochasticity of training, the amount of memorized information can be quantified with the Shannon mutual information between weights and the vector of all training labels given inputs, $I(w ; \mathbf{y} \mid \mathbf{x})$. We show that for any training algorithm, low values of this term correspond to reduction in memorization of label-noise and better generalization bounds. To obtain these low values, we propose training algorithms that employ an auxiliary network that predicts gradients in the final layers of a classifier without accessing labels. We illustrate the effectiveness of our approach on versions of MNIST, CIFAR-10, and CIFAR-100 corrupted with various noise models, and on a large-scale dataset Clothing1M that has noisy labels.
A distributed Integrity Catalog for digital repositories<|sep|>Digital repositories, either digital preservation systems or archival systems, periodically check the integrity of stored objects to assure users of their correctness. To do so, prior solutions calculate integrity metadata and require the repository to store it alongside the actual data objects. This integrity metadata is essential for regularly verifying the correctness of the stored data objects. To safeguard and detect damage to this metadata, prior solutions rely on widely visible media, that is unaffiliated third parties, to store and provide back digests of the metadata to verify it is intact. However, they do not address recovery of the integrity metadata in case of damage or attack by an adversary. In essence, they do not preserve this metadata. We introduce IntegrityCatalog, a system that collects all integrity related metadata in a single component, and treats them as first class objects, managing both their integrity and their preservation. We introduce a treap-based persistent authenticated dictionary managing arbitrary length key/value pairs, which we use to store all integrity metadata, accessible simply by object name. Additionally, IntegrityCatalog is a distributed system that includes a network protocol that manages both corruption detection and preservation of this metadata, using administrator-selected network peers with two possible roles. Verifiers store and offer attestations on digests and have minimal storage requirements, while preservers efficiently synchronize a complete copy of the catalog to assist in recovery in case of a detected catalog compromise on the local system. We describe our prototype implementation of IntegrityCatalog, measure its performance empirically, and demonstrate its effectiveness in real-world situations, with worst measured throughput of approximately 1K insertions per second, and 2K verified search operations per second.
The Central 3 kpc of NGC 5850<|sep|>NGC 5850 is a nearby (z=0.0085) early type spiral galaxy classified as LINER. It is considered as a prototype double-barred system. Our optical Integral Field Spectroscopic (IFS) data of the central 21x19 arcsec^2 of NGC 5850 show extended LINER-like emission which we ascribe to the presence of a hot and evolved stellar population, possibly together with a faint AGN. Additionally NGC 5850 shows extended `composite' ionization patterns, likely to stem from a mixture of LINER-like ionization and photoionization by star formation. The kinematics of the gas deviates strongly from a simple rotational structure.
The Neutron Star Outer Crust Equation of State: A Machine Learning approach<|sep|>Constructing the outer crust of the neutron stars requires the knowledge of the Binding Energy (BE) of the atomic nuclei. Although the BE of a lot of the nuclei is experimentally determined and can be obtained from the AME data table, for the others we need to depend on theoretical models. There exist a lot of physical theories to predict the BE, each with its own strengths and weaknesses. In this paper, we apply Machine Learning (ML) algorithms on AME2016 data set to predict the Binding Energy {of atomic nuclei}. The novel feature of our work is that it is model independent. We do not assume or use any nuclear physics model but use only ML algorithms directly on the AME2016 data set. Our results are further refined by using another ML algorithm to train the errors of the first algorithm, and repeating this process iteratively. Our best algorithm gives $\sigma_{\rm rms} \approx 0.58$ MeV for Binding Energy on randomized testing sets. This is comparable to all physics models or ML improved physics models studied in literature till date. Using the predictions of our Machine Learning algorithm, we construct the outer crust equation of state (EoS) of a neutron star and show that our model is comparable to existing models. This work also demonstrates the use of various ML algorithms and a detailed analysis on how we arrived at our best algorithm. It will help the physics community in understanding how to choose an ML algorithm which would be suited for their data set. Our algorithms and best fit model is also made publicly available for the use of the community.
Towards Extraction of $\pi^+ p$ and $\pi^+\pi^+$ cross-sections from Charge Exchange Processes at the LHC<|sep|>We study the possibilities to analyse the data on leading neutrons production at first LHC runs. These data could be used to extract from it $\pi^+ p$ and $\pi^+\pi^+$ cross-sections. In this note we estimate relative contributions of $\pi$, $\rho$ and $a_2$ reggeons to charge exchanges and discuss related problems of measurements.
Practical and efficient experimental characterization of multiqubit stabilizer states<|sep|>Vast developments in quantum technology have enabled the preparation of quantum states with more than a dozen entangled qubits. The full characterization of such systems demands distinct constructions depending on their specific type and the purpose of their use. Here we present a method that scales linearly with the number of qubits for characterizing stabilizer states. Our approach allows simultaneous extraction of information about the fidelity, the entanglement, and the nonlocality of the state and thus is of high practical relevance. We demonstrate the efficient applicability of our method by performing an experimental characterization of a photonic four-qubit cluster state and three- and four-qubit Greenberger-Horne-Zeilinger states. Our scheme can be directly extended to larger-scale quantum information tasks.
Polylidar -- Polygons from Triangular Meshes<|sep|>This paper presents Polylidar, an efficient algorithm to extract non-convex polygons from 2D point sets, including interior holes. Plane segmented point clouds can be input into Polylidar to extract their polygonal counterpart, thereby reducing map size and improving visualization. The algorithm begins by triangulating the point set and filtering triangles by user configurable parameters such as triangle edge length. Next, connected triangles are extracted into triangular mesh regions representing the shape of the point set. Finally each region is converted to a polygon through a novel boundary following method which accounts for holes. Real-world and synthetic benchmarks are presented to comparatively evaluate Polylidar speed and accuracy. Results show comparable accuracy and more than four times speedup compared to other concave polygon extraction methods.
Dynamical Models and Tracking Regret in Online Convex Programming<|sep|>This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with the comparator's deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed Dynamic Mirror Descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network.
Dynamics of the Disks of Nearby Galaxies<|sep|>I describe how the dynamics of galactic disks can be inferred by imaging and spectroscopy. Next I demonstrate that the decomposition of the rotation curves of spiral galaxies into the contributions by the various components of the galaxies is highly degenerate. Constraints on the decomposition can be found by considering implications for the dynamics of the galactic disks. An important diagnostic is the Toomre Q stability parameter which controls the stability of a galactic disk against local Jeans collapse. I also show how the density wave theory of galactic spiral arms can be employed to constrain the mass of a galactic disk. Applying both diagnostics to the example of NGC 2985 and discussing also the implied mass-to-light ratio I demonstrate that the inner parts of the galaxy, where the optical disk resides, are dominated by baryons. When I apply this method to the disks of low surface brightness galaxies, I find unexpectedly high mass-to light ratios. These could be explained by population synthesis models which assume a bottom heavy initial mass function similar to the recently proposed `integrated galactic initial mass function'.
Non-Equilibrium Chemistry of Dynamically Evolving Prestellar Cores: II. Ionization and Magnetic Field<|sep|>We study the effect that non-equilibrium chemistry in dynamical models of collapsing molecular cloud cores has on measurements of the magnetic field in these cores, the degree of ionization, and the mean molecular weight of ions. We find that OH and CN, usually used in Zeeman observations of the line-of-sight magnetic field, have an abundance that decreases toward the center of the core much faster than the density increases. As a result, Zeeman observations tend to sample the outer layers of the core and consistently underestimate the core magnetic field. The degree of ionization follows a complicated dependence on the number density at central densities up to 10^5 cm^{-3} for magnetic models and 10^6 cm^{-3} in non-magnetic models. At higher central densities the scaling approaches a power-law with a slope of -0.6 and a normalization which depends on the cosmic-ray ionization rate {\zeta} and the temperature T as ({\zeta}T)^1/2. The mean molecular weight of ions is systematically lower than the usually assumed value of 20 - 30, and, at high densities, approaches a value of 3 due to the asymptotic dominance of the H3+ ion. This significantly lower value implies that ambipolar diffusion operates faster.
Threshold Resummation for Polarized High-$p_T$ Hadron Production at COMPASS<|sep|>We study the cross section for the photoproduction process $\gamma N\rightarrow h X$ where the incident photon and nucleon are longitudinally polarized and a hadron $h$ is observed at high transverse momentum. Specifically, we address the "direct" part of the cross section, for which the photon interacts in a pointlike way. For this contribution we perform an all-order resummation of logarithmic threshold corrections generated by soft or collinear gluon emission to next-to-leading logarithmic accuracy. We present phenomenological results relevant for the COMPASS experiment and compare to recent COMPASS data.
Performance of XFaster likelihood in real CMB experiments<|sep|>We assess the strengths and weaknesses of several likelihood formalisms, including the XFaster likelihood. We compare the performance of the XFaster likelihood to that of the Offset Lognormal Bandpower likelihood on simulated data for the Planck satellite. Parameters estimated with these two likelihoods are in good agreement. The advantages of the XFaster likelihood can therefore be realized without compromising performance.
A new Determination of the Extragalactic Background of Diffuse Gamma Rays taking into account Dark Matter Annihilation<|sep|>The extragalactic background (EGB) of diffuse gamma rays can be determined by subtracting the Galactic contribution from the data. This requires a Galactic model (GM) and we include for the first time the contribution of dark matter annihilation (DMA), which was previously proposed as an explanation for the EGRET excess of diffuse Galactic gamma rays above 1 GeV. In this paper it is shown that the newly determined EGB shows a characteristic high energy bump on top of a steeply falling soft contribution. The bump is shown to be compatible with a contribution from an extragalactic DMA signal from weakly interacting massive particles (WIMPs) with a mass between 50 and 100 GeV in agreement with the EGRET excess of the Galactic diffuse gamma rays and in disagreement with earlier analysis. The remaining soft contribution of the EGB is shown to resemble the spectra of the observed point sources in our Galaxy.
Low-Velocity Halo Clouds<|sep|>Models that reproduce the observed high-velocity clouds (HVCs) also predict clouds at lower radial velocities that may easily be confused with Galactic disk (|z| < 1 kpc) gas. We describe the first search for these low-velocity halo clouds (LVHCs) using IRAS data and the initial data from the Galactic Arecibo L-band Feed Array survey in HI (GALFA-HI). The technique is based upon the expectation that such clouds should, like HVCs, have very limited infrared thermal dust emission as compared to their HI column density. We describe our 'displacement-map' technique for robustly determining the dust-to-gas ratio of clouds and the associated errors that takes into account the significant scatter in the infrared flux from the Galactic disk gas. We find that there exist lower-velocity clouds that have extremely low dust-to-gas ratios, consistent with being in the Galactic halo - candidate LVHCs. We also confirm the lack of dust in many HVCs with the notable exception of complex M, which we consider to be the first detection of warm dust in HVCs. We do not confirm the previously reported detection of dust in complex C. In addition, we find that most Intermediate- and Low-Velocity clouds that are part of the Galactic disk have a higher 60 micron/100 micron flux ratio than is typically seen in Galactic HI, which is consistent with a previously proposed picture in which fast-moving Galactic clouds have smaller, hotter dust grains.
The SUrvey for Pulsars and Extragalactic Radio Bursts IV: Discovery and polarimetry of a 12.1-second radio pulsar<|sep|>We report the discovery of PSR~J2251$-$3711, a radio pulsar with a spin period of 12.1 seconds, the second longest currently known. Its timing parameters imply a characteristic age of 15 Myr, a surface magnetic field of $1.3 \times 10^{13}$~G and a spin-down luminosity of $2.9 \times 10^{29}~\mathrm{erg~s}^{-1}$. Its dispersion measure of 12.12(1)~$\mathrm{pc}~\mathrm{cm}^{-3}$ leads to distance estimates of 0.5 and 1.3 kpc according to the NE2001 and YMW16 Galactic free electron density models, respectively. Some of its single pulses show an uninterrupted 180 degree sweep of the phase-resolved polarization position angle, with an S-shape reminiscent of the rotating vector model prediction. However, the fact that this sweep occurs at different phases from one pulse to another is remarkable and without straightforward explanation. Although PSR~J2251$-$3711 lies in the region of the $P-\dot{P}$ parameter space occupied by the X-ray Isolated Neutron Stars (XINS), there is no evidence for an X-ray counterpart in our Swift XRT observation; this places a 99\%-confidence upper bound on its unabsorbed bolometric thermal luminosity of $1.1 \times 10^{31}~(d / 1~\mathrm{kpc})^2~\mathrm{erg/s}$ for an assumed temperature of 85 eV, where $d$ is the distance to the pulsar. Further observations are needed to determine whether it is a rotation-powered pulsar with a true age of at least several Myr, or a much younger object such as an XINS or a recently cooled magnetar. Extreme specimens like PSR J2251$-$3711 help bridge populations in the so-called neutron star zoo in an attempt to understand their origins and evolution.
Semi-Supervised Deep Learning for Multi-Tissue Segmentation from Multi-Contrast MRI<|sep|>Segmentation of thigh tissues (muscle, fat, inter-muscular adipose tissue (IMAT), bone, and bone marrow) from magnetic resonance imaging (MRI) scans is useful for clinical and research investigations in various conditions such as aging, diabetes mellitus, obesity, metabolic syndrome, and their associated comorbidities. Towards a fully automated, robust, and precise quantification of thigh tissues, herein we designed a novel semi-supervised segmentation algorithm based on deep network architectures. Built upon Tiramisu segmentation engine, our proposed deep networks use variational and specially designed targeted dropouts for faster and robust convergence, and utilize multi-contrast MRI scans as input data. In our experiments, we have used 150 scans from 50 distinct subjects from the Baltimore Longitudinal Study of Aging (BLSA). The proposed system made use of both labeled and unlabeled data with high efficacy for training, and outperformed the current state-of-the-art methods with dice scores of 97.52%, 94.61%, 80.14%, 95.93%, and 96.83% for muscle, fat, IMAT, bone, and bone marrow tissues, respectively. Our results indicate that the proposed system can be useful for clinical research studies where volumetric and distributional tissue quantification is pivotal and labeling is a significant issue. To the best of our knowledge, the proposed system is the first attempt at multi-tissue segmentation using a single end-to-end semi-supervised deep learning framework for multi-contrast thigh MRI scans.
Numerical estimation of reachable and controllability sets for a two-level open quantum system driven by coherent and incoherent controls<|sep|>The article considers a two-level open quantum system, whose evolution is governed by the Gorini--Kossakowski--Lindblad--Sudarshan master equation with Hamiltonian and dissipation superoperator depending, correspondingly, on piecewise constant coherent and incoherent controls with constrained magnitudes. Additional constraints on controls' variations are also considered. The system is analyzed using Bloch parametrization of the system's density matrix. We adapt the section method for obtaining outer parallelepipedal and pointwise estimations of reachable and controllability sets in the Bloch ball via solving a number of problems for optimizing coherent and incoherent controls with respect to some objective criteria. The differential evolution and dual annealing optimization methods are used. The numerical results show how the reachable sets' estimations depend on distances between the system's initial states and the Bloch ball's center point, final times, constraints on controls' magnitudes and variations.
Self-dual String and Higher Instanton Solutions<|sep|>We present and discuss explicit solutions to the non-abelian self-dual string equation as well as to the non-abelian self-duality equation in six dimensions. These solutions are generalizations of the 't Hooft-Polyakov monopole and the BPST instanton to higher gauge theory. We expect that these solutions are relevant to the effective description of M2- and M5-branes.
SCA: Streaming Cross-attention Alignment for Echo Cancellation<|sep|>End-to-End deep learning has shown promising results for speech enhancement tasks, such as noise suppression, dereverberation, and speech separation. However, most state-of-the-art methods for echo cancellation are either classical DSP-based or hybrid DSP-ML algorithms. Components such as the delay estimator and adaptive linear filter are based on traditional signal processing concepts, and deep learning algorithms typically only serve to replace the non-linear residual echo suppressor. This paper introduces an end-to-end echo cancellation network with a streaming cross-attention alignment (SCA). Our proposed method can handle unaligned inputs without requiring external alignment and generate high-quality speech without echoes. At the same time, the end-to-end algorithm simplifies the current echo cancellation pipeline for time-variant echo path cases. We test our proposed method on the ICASSP2022 and Interspeech2021 Microsoft deep echo cancellation challenge evaluation dataset, where our method outperforms some of the other hybrid and end-to-end methods.
Consistency Checking and Querying in Probabilistic Databases under Integrity Constraints<|sep|>We address the issue of incorporating a particular yet expressive form of integrity constraints (namely, denial constraints) into probabilistic databases. To this aim, we move away from the common way of giving semantics to probabilistic databases, which relies on considering a unique interpretation of the data, and address two fundamental problems: consistency checking and query evaluation. The former consists in verifying whether there is an interpretation which conforms to both the marginal probabilities of the tuples and the integrity constraints. The latter is the problem of answering queries under a "cautious" paradigm, taking into account all interpretations of the data in accordance with the constraints. In this setting, we investigate the complexity of the above-mentioned problems, and identify several tractable cases of practical relevance.
Educating Text Autoencoders: Latent Representation Guidance via Denoising<|sep|>Generative autoencoders offer a promising approach for controllable text generation by leveraging their latent sentence representations. However, current models struggle to maintain coherent latent spaces required to perform meaningful text manipulations via latent vector operations. Specifically, we demonstrate by example that neural encoders do not necessarily map similar sentences to nearby latent vectors. A theoretical explanation for this phenomenon establishes that high capacity autoencoders can learn an arbitrary mapping between sequences and associated latent representations. To remedy this issue, we augment adversarial autoencoders with a denoising objective where original sentences are reconstructed from perturbed versions (referred to as DAAE). We prove that this simple modification guides the latent space geometry of the resulting model by encouraging the encoder to map similar texts to similar latent representations. In empirical comparisons with various types of autoencoders, our model provides the best trade-off between generation quality and reconstruction capacity. Moreover, the improved geometry of the DAAE latent space enables zero-shot text style transfer via simple latent vector arithmetic.
Quantum Theory and The Symbolic Dynamics of Invariant Sets: Towards a Gravitational Theory of the Quantum<|sep|>A realistic measurement-free theory for the quantum physics of multiple qubits is proposed. This theory is based on a symbolic representation of a fractal state-space geometry which is invariant under the action of deterministic and locally causal dynamics. This symbolic representation is constructed from self-similar families of quaternionic operators. Using number-theoretic properties of the cosine function, the statistical properties of the symbolic representation of the invariant set are shown to be consistent with the contextual requirements of the Kochen-Specker theorem, are not constrained by Bell inequalities, and mirror the statistics of entangled qubits. These number-theoretic properties in turn reflect the sparseness of the invariant set in state space, and relate to the metaphysical notion of counterfactual incompleteness. Using the concept of probability, the complex Hilbert Space can be considered the completion of this symbolic representation into the state space continuum. As a result, it is proposed that the complex Hilbert Space should merely be considered a computational convenience in the light of the algorithmic intractability of the invariant set geometry, and consequently the superposed state should not be considered a fundamental aspect of physical theory. The physical basis for the proposed theory is relativistic gravity; for example the symbols used to describe the invariant set themselves label gravitationally distinct cosmological space-times. This implies that the very notion of a `quantum theory of gravity' may be profoundly misguided - erroneously putting the quantum cart before the gravitational horse. Here some elements of an alternative `gravitational theory of the quantum' are proposed, based on a deterministic and locally causal theory of gravity which extends general relativity by being geometric in both space-time and state space.
Facial Expression Recognition Using Sparse Gaussian Conditional Random Field<|sep|>The analysis of expression and facial Action Units (AUs) detection are very important tasks in fields of computer vision and Human Computer Interaction (HCI) due to the wide range of applications in human life. Many works has been done during the past few years which has their own advantages and disadvantages. In this work we present a new model based on Gaussian Conditional Random Field. We solve our objective problem using ADMM and we show how well the proposed model works. We train and test our work on two facial expression datasets, CK+ and RU-FACS. Experimental evaluation shows that our proposed approach outperform state of the art expression recognition.
On two-dimensional Hamiltonian systems with sixth-order integrals of motion<|sep|>We obtain 21 two-dimensional natural Hamiltonian systems with sextic invariants, which are polynomial of the sixth order in momenta. Following to Bertrand, Darboux, and Drach these results of the standard brute force experiments can be applied to construct a new mathematical theory.
Resummed inclusive cross-section in ADD model at N$^3$LL+NNLO<|sep|>We present three loop soft-plus-virtual (SV) corrections to the spin-2 production at the Large Hadron Collider (LHC). For this calculation, we make use of the recently computed quark and gluon three loop form factors for the spin-2 production, the universal soft-collinear coefficients as well as the mass factorization kernels. The SV coefficients are presented up to next-to-next-to-next-to leading order (N$^3$LO). We also use these coefficients at three loops to compute the resummed prediction for inclusive cross-section to next-to-next-to-next-to leading logarithmic accuracy (N$^3$LL) matched to next-to-next-to leading order (NNLO). We use the standard technique to derive the Mellin N-dependent coefficients and also the N-independent coefficients to achieve the resummation using the minimal prescription matching procedure. Considering the spin-2 propagator in the large extra dimensional (ADD) model, we also study the numerical impact of these three-loop SV corrections as well as the resummed predictions on the di-lepton invariant mass distribution at the 13 TeV LHC. We find that the conventional scale uncertainties in the NNLO+N$^3$LL resummed results substantially get reduced to as low as 2\% in the high invariant mass region. We also estimate the PDF uncertainties in our predictions that will be useful in the experimental searches for large extra dimensions.
Integrals and Valuations<|sep|>We construct a homeomorphism between the compact regular locale of integrals on a Riesz space and the locale of (valuations) on its spectrum. In fact, we construct two geometric theories and show that they are biinterpretable. The constructions are elementary and tightly connected to the Riesz space structure.
A Bayesian and Machine Learning approach to estimating Influence Model parameters for IM-RO<|sep|>The rise of Online Social Networks (OSNs) has caused an insurmountable amount of interest from advertisers and researchers seeking to monopolize on its features. Researchers aim to develop strategies for determining how information is propagated among users within an OSN that is captured by diffusion or influence models. We consider the influence models for the IM-RO problem, a novel formulation to the Influence Maximization (IM) problem based on implementing Stochastic Dynamic Programming (SDP). In contrast to existing approaches involving influence spread and the theory of submodular functions, the SDP method focuses on optimizing clicks and ultimately revenue to advertisers in OSNs. Existing approaches to influence maximization have been actively researched over the past decade, with applications to multiple fields, however, our approach is a more practical variant to the original IM problem. In this paper, we provide an analysis on the influence models of the IM-RO problem by conducting experiments on synthetic and real-world datasets. We propose a Bayesian and Machine Learning approach for estimating the parameters of the influence models for the (Influence Maximization- Revenue Optimization) IM-RO problem. We present a Bayesian hierarchical model and implement the well-known Naive Bayes classifier (NBC), Decision Trees classifier (DTC) and Random Forest classifier (RFC) on three real-world datasets. Compared to previous approaches to estimating influence model parameters, our strategy has the great advantage of being directly implementable in standard software packages such as WinBUGS/OpenBUGS/JAGS and Apache Spark. We demonstrate the efficiency and usability of our methods in terms of spreading information and generating revenue for advertisers in the context of OSNs.
On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing<|sep|>Different languages might have different word orders. In this paper, we investigate cross-lingual transfer and posit that an order-agnostic model will perform better when transferring to distant foreign languages. To test our hypothesis, we train dependency parsers on an English corpus and evaluate their transfer performance on 30 other languages. Specifically, we compare encoders and decoders based on Recurrent Neural Networks (RNNs) and modified self-attentive architectures. The former relies on sequential information while the latter is more flexible at modeling word order. Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to English, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages.
Network tomography based on 1-D projections<|sep|>Network tomography has been regarded as one of the most promising methodologies for performance evaluation and diagnosis of the massive and decentralized Internet. This paper proposes a new estimation approach for solving a class of inverse problems in network tomography, based on marginal distributions of a sequence of one-dimensional linear projections of the observed data. We give a general identifiability result for the proposed method and study the design issue of these one dimensional projections in terms of statistical efficiency. We show that for a simple Gaussian tomography model, there is an optimal set of one-dimensional projections such that the estimator obtained from these projections is asymptotically as efficient as the maximum likelihood estimator based on the joint distribution of the observed data. For practical applications, we carry out simulation studies of the proposed method for two instances of network tomography. The first is for traffic demand tomography using a Gaussian Origin-Destination traffic model with a power relation between its mean and variance, and the second is for network delay tomography where the link delays are to be estimated from the end-to-end path delays. We compare estimators obtained from our method and that obtained from using the joint distribution and other lower dimensional projections, and show that in both cases, the proposed method yields satisfactory results.
Spontaneous decay rate of an excited molecule placed near a circular aperture in a metal film<|sep|>We have investigated the spontaneous decay rate of an excited molecule placed near a circular aperture in a metal film of finite thickness and finite conductivity. We have considered the metal film both suspended freely in vacuum and lying on a substrate. A significant effect of molecule the position and the presence of the substrate on the rate of spontaneous emission of the molecule is shown. The asymptotes which can be used to describe this process are found. Total, radiative, and non-radiative spontaneous decay rates of the excited molecule are extracted and compared. The results may be useful in the development and interpretation of experiments investigating a single molecule with a scanning optical microscope and in the design of optical nanodevices based on the control of elementary quantum systems emission with a nanohole.
Thermodynamics and Cardy-like formula for nonminimally dressed, charged Lifshitz black holes in New Massive Gravity<|sep|>In three dimensions, we consider the Einstein-Maxwell Lagrangian dressed by a nonminimally coupled scalar field in New Massive Gravity. For this theory, we provide two families of electrically charged Lifshitz black holes where their metric functions depend only on an integration constant. We calculate their masses using the quasilocal approach, as well as their entropy and electric charge. These charged configurations are interpreted as extremal in the sense that the mass vanishes identically while the entropy and electric charge are non zero thermodynamic quantities. Using these examples, we corroborate that the semiclassical entropy can be recovered through a charged Cardy-like formula, involving the corresponding magnetically charged solitons obtained by a double Wick rotation. Finally, the first law of thermodynamics, as well as the Smarr formula are also verified
Meta Navigation Functions: Adaptive Associations for Coordination of Multi-Agent Systems<|sep|>In this paper, we introduce a new class of potential fields, i.e., meta navigation functions (MNFs) to coordinate multi-agent systems. Thanks to the MNF formulation, agents can contribute to each other's coordination via partial and/or total associations, contrary to traditional decentralized navigation functions (DNFs). In particular, agents may stimulate each other via their MNFs. Moreover, MNFs need to be confined which is a weaker condition compared to the Morse condition of DNFs. An MNF is composed of a confined function and an attraction kernel. The critical points of the former can be confined in a safe region around a target critical point. The collision-free trajectory of an agent and its associations to its peers are governed by a confined function before reaching its safe region. Then, the attraction kernel drives the agent to its target in the safe region. MNFs provide faster coordination compared to DNFs. We illustrate how MNFs may exhibit some social behaviors in the course of partial and total associations among agents. Our simulations verify the efficiency of MNFs to coordinate complex swarms of agents.
Dark Matter-Induced Multi-Phase Dynamical Symmetry Breaking<|sep|>We consider the classically scale invariant Higgs-dilaton model of dynamical symmetry breaking extended with an extra scalar field that plays the role of dark matter. The Higgs boson is light near a critical boundary between different symmetry breaking phases, where quantum corrections beyond the usual Gildener-Weinberg approximation become relevant. This implies a tighter connection between dark matter and Higgs phenomenology. The model has only three free parameters, yet it allows for the observed relic abundance of dark matter while respecting all constraints. The direct detection cross section mediated by the Higgs boson is determined by the dark matter mass alone and is testable at future experiments.
Small-Signal Stability Analysis of a DC Shipboard Microgrid With Droop-Controlled Batteries and Constant Power Resources<|sep|>The presence of constant power loads (CPLs) in dc shipboard microgrids may lead to unstable conditions. The present work investigates the stability properties of dc microgrids where CPLs are fed by fuel cells (FCs), and energy storage systems (ESSs) equipped with voltage droop control. With respect to the previous literature, the dynamics of the duty cycles of the dc-dc converters implementing the droop regulation are considered. A mathematical model has been derived, and tuned to best mimic the behavior of the electrical representation implemented in DIgSILENT. Then the model is used to find the sufficient conditions for stability with respect to the droop coefficient, the dc-bus capacitor, and the inductances of the dc-dc converters.
Hydrodynamical models of cometary HII regions<|sep|>We have modelled the evolution of cometary HII regions produced by zero-age main-sequence stars of O and B spectral types, which are driving strong winds and are born off-centre from spherically symmetric cores with power-law ($\alpha = 2$) density slopes. A model parameter grid was produced that spans stellar mass, age and core density. Exploring this parameter space we investigated limb-brightening, a feature commonly seen in cometary HII regions. We found that stars with mass $M_\star \geq 12\, \mathrm{M}_\odot$ produce this feature. Our models have a cavity bounded by a contact discontinuity separating hot shocked wind and ionised ambient gas that is similar in size to the surrounding HII region. Due to early pressure confinement we did not see shocks outside of the contact discontinuity for stars with $M_\star \leq 40\, \mathrm{M}_\odot$, but the cavities were found to continue to grow. The cavity size in each model plateaus as the HII region stagnates. The spectral energy distributions of our models are similar to those from identical stars evolving in uniform density fields. The turn-over frequency is slightly lower in our power-law models due to a higher proportion of low density gas covered by the HII regions.
Pyramid Region-based Slot Attention Network for Temporal Action Proposal Generation<|sep|>It has been found that temporal action proposal generation, which aims to discover the temporal action instances within the range of the start and end frames in the untrimmed videos, can largely benefit from proper temporal and semantic context exploitation. The latest efforts were dedicated to considering the temporal context and similarity-based semantic contexts through self-attention modules. However, they still suffer from cluttered background information and limited contextual feature learning. In this paper, we propose a novel Pyramid Region-based Slot Attention (PRSlot) module to address these issues. Instead of using the similarity computation, our PRSlot module directly learns the local relations in an encoder-decoder manner and generates the representation of a local region enhanced based on the attention over input features called \textit{slot}. Specifically, upon the input snippet-level features, PRSlot module takes the target snippet as \textit{query}, its surrounding region as \textit{key} and then generates slot representations for each \textit{query-key} slot by aggregating the local snippet context with a parallel pyramid strategy. Based on PRSlot modules, we present a novel Pyramid Region-based Slot Attention Network termed PRSA-Net to learn a unified visual representation with rich temporal and semantic context for better proposal generation. Extensive experiments are conducted on two widely adopted THUMOS14 and ActivityNet-1.3 benchmarks. Our PRSA-Net outperforms other state-of-the-art methods. In particular, we improve the AR@100 from the previous best 50.67% to 56.12% for proposal generation and raise the mAP under 0.5 tIoU from 51.9\% to 58.7\% for action detection on THUMOS14. \textit{Code is available at} \url{https://github.com/handhand123/PRSA-Net}
The challenge of engaging all students via self-paced interactive e-learning tutorials for introductory physics<|sep|>As research-based self-paced e-learning tools become increasingly available, a critical issue educators encounter is implementing strategies to ensure that all students engage with them as intended. Here, we discuss the effectiveness of research-based e-learning tutorials as self-paced learning tools in large enrollment brick and mortar introductory physics courses. These interactive tutorials were developed via research in physics education and were found to be effective for a diverse group of introductory physics students in one-on-one implementation. Instructors encouraged the use of these self-paced tools in a self-paced learning environment by telling students that they would be helpful for solving the assigned homework problems and that the underlying physics principles in the tutorial problems would be similar to those in the in-class quizzes (which we call paired problems). We find that many students, who struggled in the courses in which these adaptive e-learning tutorials were assigned as a self-study tool, performed poorly on the paired problems. In contrast, a majority of student volunteers in one-on-one implementation greatly benefited from the tutorials and performed well on the paired problems. This suggests that many students enrolled in introductory physics courses did not effectively engage with the self-paced tutorials outside of class and may have only used them superficially. The findings suggest that many students in need of out-of-class remediation via self-paced learning tools may have difficulty motivating themselves and may lack the self-regulation and time-management skills to engage effectively with tools specially designed to help them learn at their own pace. We conclude by proposing a theoretical framework to help students with diverse prior preparations engage effectively with self-study tools.
Solving infinite-horizon Dec-POMDPs using Finite State Controllers within JESP<|sep|>This paper looks at solving collaborative planning problems formalized as Decentralized POMDPs (Dec-POMDPs) by searching for Nash equilibria, i.e., situations where each agent's policy is a best response to the other agents' (fixed) policies. While the Joint Equilibrium-based Search for Policies (JESP) algorithm does this in the finite-horizon setting relying on policy trees, we propose here to adapt it to infinite-horizon Dec-POMDPs by using finite state controller (FSC) policy representations. In this article, we (1) explain how to turn a Dec-POMDP with $N-1$ fixed FSCs into an infinite-horizon POMDP whose solution is an $N^\text{th}$ agent best response; (2) propose a JESP variant, called \infJESP, using this to solve infinite-horizon Dec-POMDPs; (3) introduce heuristic initializations for JESP aiming at leading to good solutions; and (4) conduct experiments on state-of-the-art benchmark problems to evaluate our approach.
Thermal photon $v_3$ at LHC from fluctuating initial conditions<|sep|>We calculate the triangular flow parameter $v_3$ of thermal photons for 0--40\% central collisions of Pb nuclei at LHC using an event-by-event hydrodynamic model with fluctuating initial conditions. Thermal photon $v_3$ with respect to the the participant plane angle is found to be positive and significant compared to the elliptic flow parameter $v_2$ of thermal photons. In addition, photon $v_3$ as a function of $p_T$ shows similar qualitative nature to photon $v_2$ in the region $1< p_T <6$ GeV/$c$. We argue that while $v_3$ originates from $\epsilon_3$ deformations of the initial state density distribution, fast buildup of radial flow due to fluctuations is the main driving mechanism for the observed large value.
The Effect of Distortions on the Prediction of Visual Attention<|sep|>Existing saliency models have been designed and evaluated for predicting the saliency in distortion-free images. However, in practice, the image quality is affected by a host of factors at several stages of the image processing pipeline such as acquisition, compression and transmission. Several studies have explored the effect of distortion on human visual attention; however, none of them have considered the performance of visual saliency models in the presence of distortion. Furthermore, given that one potential application of visual saliency prediction is to aid pooling of objective visual quality metrics, it is important to compare the performance of existing saliency models on distorted images. In this paper, we evaluate several state-of-the-art visual attention models over different databases consisting of distorted images with various types of distortions such as blur, noise and compression with varying levels of distortion severity. This paper also introduces new improved performance evaluation metrics that are shown to overcome shortcomings in existing performance metrics. We find that the performance of most models improves with moderate and high levels of distortions as compared to the near distortion-free case. In addition, model performance is also found to decrease with an increase in image complexity.
VieSum: How Robust Are Transformer-based Models on Vietnamese Summarization?<|sep|>Text summarization is a challenging task within natural language processing that involves text generation from lengthy input sequences. While this task has been widely studied in English, there is very limited research on summarization for Vietnamese text. In this paper, we investigate the robustness of transformer-based encoder-decoder architectures for Vietnamese abstractive summarization. Leveraging transfer learning and self-supervised learning, we validate the performance of the methods on two Vietnamese datasets.
Vanishing largest Lyapunov exponent and Tsallis entropy<|sep|>We present a geometric argument that explains why some systems having vanishing largest Lyapunov exponent have underlying dynamics aspects of which can be effectively described by the Tsallis entropy. We rely on a comparison of the generalised additivity of the Tsallis entropy versus the ordinary additivity of the BGS entropy. We translate this comparison in metric terms by using an effective hyperbolic metric on the configuration/phase space for the Tsallis entropy versus the Euclidean one in the case of the BGS entropy. Solving the Jacobi equation for such hyperbolic metrics effectively sets the largest Lyapunov exponent computed with respect to the corresponding Euclidean metric to zero. This conclusion is in agreement with all currently known results about systems that have a simple asymptotic behaviour and are described by the Tsallis entropy.
Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks<|sep|>Deep neural networks are powerful machines for visual pattern recognition, but reasoning tasks that are easy for humans may still be difficult for neural models. Humans possess the ability to extrapolate reasoning strategies learned on simple problems to solve harder examples, often by thinking for longer. For example, a person who has learned to solve small mazes can easily extend the very same search techniques to solve much larger mazes by spending more time. In computers, this behavior is often achieved through the use of algorithms, which scale to arbitrarily hard problem instances at the cost of more computation. In contrast, the sequential computing budget of feed-forward neural networks is limited by their depth, and networks trained on simple problems have no way of extending their reasoning to accommodate harder problems. In this work, we show that recurrent networks trained to solve simple problems with few recurrent steps can indeed solve much more complex problems simply by performing additional recurrences during inference. We demonstrate this algorithmic behavior of recurrent networks on prefix sum computation, mazes, and chess. In all three domains, networks trained on simple problem instances are able to extend their reasoning abilities at test time simply by "thinking for longer."
Different Prospects of the Universe Depending on Dark Energy and/or a New Kind of Extra Dimension<|sep|>Generally making the cosmological scale factor $R$ be a function of the coordinate of the extra dimension $\sigma $ that is also a function of time $t$, we achieve a new kind of cosmic acceleration mechanism depending on extra dimension. We give the constraints on $\sigma $ under four different prospects of the universe, and indicate that dark energy is not required for both the small extra dimension and the accelerating expansion of our universe. This results in this paper show that the accelerating expansion of our universe may come from both dark energy and the achieved new mechanism here, or the latter alone.
Theory of current-induced spin polarizations in an electron gas<|sep|>We derive the Bloch equations for the spin dynamics of a two-dimensional electron gas in the presence of spin-orbit coupling. For the latter we consider both the intrinsic mechanisms of structure inversion asymmetry (Rashba) and bulk inversion asymmetry (Dresselhaus), and the extrinsic ones arising from the scattering from impurities. The derivation is based on the SU(2) gauge-field formulation of the Rashba-Dresselhaus spin-orbit coupling. Our main result is the identification of a new spin-generation torque arising from the Elliot-Yafet process, which opposes a similar term arising from the Dyakonov-Perel process. The new spin-generation torque contributes to the current-induced spin polarization (CISP) -- also known as the Edelstein or inverse spin-galvanic effect. As a result, the behavior of the CISP turns out to be more complex than one would surmise from consideration of the internal Rashba-Dresselhaus fields alone. In particular, the symmetry of the current-induced spin polarization does not necessarily coincide with that of the internal Rashba-Dresselhaus field, and an out-of-plane component of the CISP is generally predicted, as observed in recent experiments. We also discuss the extension to the three-dimensional electron gas, which may be relevant for the interpretation of experiments in thin films.
Completing the physical representation of quantum algorithms provides a quantitative explanation of their computational speedup<|sep|>The usual representation of quantum algorithms, limited to the process of solving the problem, is physically incomplete. We complete it in three steps: (i) extending the representation to the process of setting the problem, (ii) relativizing the extended representation to the problem solver to whom the problem setting must be concealed, and (iii) symmetrizing the relativized representation for time reversal to represent the reversibility of the underlying physical process. The third steps projects the input state of the relativized representation, where the problem solver is completely ignorant of the setting and thus the solution of the problem, on one where she knows half solution (half of the information specifying it when the solution is an unstructured bit string). Completing the physical representation shows that the number of computation steps (oracle queries) required to solve any oracle problem in an optimal quantum way should be that of a classical algorithm endowed with the advanced knowledge of half solution. This fits the major quantum algorithms known today and would solve the quantum query complexity problem.
An Interpretable Graph-based Mapping of Trustworthy Machine Learning Research<|sep|>There is an increasing interest in ensuring machine learning (ML) frameworks behave in a socially responsible manner and are deemed trustworthy. Although considerable progress has been made in the field of Trustworthy ML (TwML) in the recent past, much of the current characterization of this progress is qualitative. Consequently, decisions about how to address issues of trustworthiness and future research goals are often left to the interested researcher. In this paper, we present the first quantitative approach to characterize the comprehension of TwML research. We build a co-occurrence network of words using a web-scraped corpus of more than 7,000 peer-reviewed recent ML papers -- consisting of papers both related and unrelated to TwML. We use community detection to obtain semantic clusters of words in this network that can infer relative positions of TwML topics. We propose an innovative fingerprinting algorithm to obtain probabilistic similarity scores for individual words, then combine them to give a paper-level relevance score. The outcomes of our analysis inform a number of interesting insights on advancing the field of TwML research.
Collective excitation frequencies and stationary states of trapped dipolar Bose-Einstein condensates in the Thomas-Fermi regime<|sep|>We present a general method for obtaining the exact static solutions and collective excitation frequencies of a trapped Bose-Einstein condensate (BEC) with dipolar atomic interactions in the Thomas-Fermi regime. The method incorporates analytic expressions for the dipolar potential of an arbitrary polynomial density profile, thereby reducing the problem of handling non-local dipolar interactions to the solution of algebraic equations. We comprehensively map out the static solutions and excitation modes, including non-cylindrically symmetric traps, and also the case of negative scattering length where dipolar interactions stabilize an otherwise unstable condensate. The dynamical stability of the excitation modes gives insight into the onset of collapse of a dipolar BEC. We find that global collapse is consistently mediated by an anisotropic quadrupolar collective mode, although there are two trapping regimes in which the BEC is stable against quadrupole fluctuations even as the ratio of the dipolar to s-wave interactions becomes infinite. Motivated by the possibility of fragmented BEC in a dipolar Bose gas due to the partially attractive interactions, we pay special attention to the scissors modes, which can provide a signature of superfluidity, and identify a long-range restoring force which is peculiar to dipolar systems. As part of the supporting material for this paper we provide the computer program used to make the calculations, including a graphical user interface.
The causal effect of environment on halo mass and concentration<|sep|>Understanding the impact of environment on the formation and evolution of dark matter halos and galaxies is a crucial open problem. Studying statistical correlations in large simulated populations sheds some light on these impacts, but the causal effect of an environment on individual objects is harder to pinpoint. Addressing this, we present a new method for resimulating a single dark matter halo in multiple large-scale environments. In the initial conditions, we 'splice' (i.e. insert) the Lagrangian region of a halo into different Gaussian random fields, while enforcing consistency with the statistical properties of $\Lambda$CDM. Applying this technique, we demonstrate that the mass of halos is primarily determined by the density structure inside their Lagrangian patches, while the halos' concentration is more strongly affected by environment. The splicing approach will also allow us to study, for example, the impact of the cosmic web on accretion processes and galaxy quenching.
Streaming Models for Joint Speech Recognition and Translation<|sep|>Using end-to-end models for speech translation (ST) has increasingly been the focus of the ST community. These models condense the previously cascaded systems by directly converting sound waves into translated text. However, cascaded models have the advantage of including automatic speech recognition output, useful for a variety of practical ST systems that often display transcripts to the user alongside the translations. To bridge this gap, recent work has shown initial progress into the feasibility for end-to-end models to produce both of these outputs. However, all previous work has only looked at this problem from the consecutive perspective, leaving uncertainty on whether these approaches are effective in the more challenging streaming setting. We develop an end-to-end streaming ST model based on a re-translation approach and compare against standard cascading approaches. We also introduce a novel inference method for the joint case, interleaving both transcript and translation in generation and removing the need to use separate decoders. Our evaluation across a range of metrics capturing accuracy, latency, and consistency shows that our end-to-end models are statistically similar to cascading models, while having half the number of parameters. We also find that both systems provide strong translation quality at low latency, keeping 99% of consecutive quality at a lag of just under a second.
On the gamma-ray emission from 3C 120<|sep|>We report the analysis of Fermi Large Area Telescope data from five years of observations of the broad line radio galaxy 3C 120. The accumulation of larger data set results in the detection of high-energy $\gamma$-rays up to 10 GeV, with a detection significance of about $8.7\sigma$. A power-law spectrum with a photon index of $2.72\pm0.1$ and integrated flux of $F_{\gamma}=(2.35\pm0.5)\times10^{-8}\:\mathrm{photon\:cm}^{-2}s^{-1}$ above 100 MeV well describe the data averaged over five year observations. The variability analysis of the light curve with 180-, and 365- day bins reveals flux increase (nearly twice from its average level) during the last year of observation. This variability on month timescales indicates the compactness of the emitting region. The $\gamma$-ray spectrum can be described as synchrotron self-Compton (SSC) emission from the electron population producing the radio-to-X-ray emission in the jet. The required electron energy density exceeds the one of magnetic field only by a factor of 2 meaning no significant deviation from equipartition.
A class of loops categorically isomorphic to Bruck loops of odd order<|sep|>We define a new variety of loops we call $\Gamma$-loops. After showing $\Gamma$-loops are power associative, our main goal will be showing a categorical isomorphism between Bruck loops of odd order and $\Gamma$-loops of odd order. Once this has been established, we can use the well known structure of Bruck loops of odd order to derive the Odd Order, Lagrange and Cauchy Theorems for $\Gamma$-loops of odd order, as well as the nontriviality of the center of finite $\Gamma$-$p$-loops ($p$ odd). Finally, we answer a question posed by Jedli\v{c}ka, Kinyon and Vojt\v{e}chovsk\'{y} about the existence of Hall $\pi$-subloops and Sylow $p$-subloops in commutative automorphic loops. By showing commutative automorphic loops are $\Gamma$-loops and using the categorical isomorphism, we answer in the affirmative.
Outflow forces in intermediate mass star formation<|sep|>Intermediate mass protostarsprovide a bridge between theories of low- and high-mass star formation. Emerging molecular outflows can be used to determine the influence of fragmentation and multiplicity on protostellar evolution through the correlation of outflow forces of intermediate mass protostars with the luminosity. The aim of this paper is to derive outflow forces from outflows of six intermediate mass protostellar regions and validate the apparent correlation between total luminosity and outflow force seen in earlier work, as well as remove uncertainties caused by different methodology. By comparing CO 6--5 observations obtained with APEX with non-LTE radiative transfer model predictions, optical depths, temperatures, densities of the gas of the molecular outflows are derived. Outflow forces, dynamical timescales and kinetic luminosities are subsequently calculated. Outflow parameters, including the forces, were derived for all sources. Temperatures in excess of 50 K were found for all flows, in line with recent low-mass results. However, comparison with other studies could not corroborate conclusions from earlier work on intermediate mass protostars which hypothesized that fragmentation enhances outflow forces in clustered intermediate mass star formation. Any enhancement in comparison with the classical relation between outflow force and luminosity can be attributed the use of a higher excitation line and improvement in methods; They are in line with results from low-mass protostars using similar techniques. The role of fragmentation on outflows is an important ingredient to understand clustered star formation and the link between low and high-mass star formation. However, detailed information on spatial scales of a few 100 AU, covering all individual members is needed to make the necessary progress.
Galaxy Formation with BECDM -- II. Cosmic Filaments and First Galaxies<|sep|>Bose-Einstein Condensate Dark Matter (BECDM; also known as Fuzzy Dark Matter) is motivated by fundamental physics and has recently received significant attention as a serious alternative to the established Cold Dark Matter (CDM) model. We perform cosmological simulations of BECDM gravitationally coupled to baryons and investigate structure formation at high redshifts ($z \gtrsim 5$) for a boson mass $m=2.5\cdot 10^{-22}~{\rm eV}$, exploring the dynamical effects of its wavelike nature on the cosmic web and the formation of first galaxies. Our BECDM simulations are directly compared to CDM as well as to simulations where the dynamical quantum potential is ignored and only the initial suppression of the power spectrum is considered -- a Warm Dark Matter-like ("WDM") model often used as a proxy for BECDM. Our simulations confirm that "WDM" is a good approximation to BECDM on large cosmological scales even in the presence of the baryonic feedback. Similarities also exist on small scales, with primordial star formation happening both in isolated haloes and continuously along cosmic filaments; the latter effect is not present in CDM. Global star formation and metal enrichment in these first galaxies are delayed in BECDM/"WDM" compared to the CDM case: in BECDM/"WDM" first stars form at $z\sim 13$/$13.5$ while in CDM star formation starts at $z\sim 35$. The signature of BECDM interference, not present in "WDM", is seen in the evolved dark matter power spectrum: although the small scale structure is initially suppressed, power on kpc scales is added at lower redshifts. Our simulations lay the groundwork for realistic simulations of galaxy formation in BECDM.
Distributed Quantum Computing with QMPI<|sep|>Practical applications of quantum computers require millions of physical qubits and it will be challenging for individual quantum processors to reach such qubit numbers. It is therefore timely to investigate the resource requirements of quantum algorithms in a distributed setting, where multiple quantum processors are interconnected by a coherent network. We introduce an extension of the Message Passing Interface (MPI) to enable high-performance implementations of distributed quantum algorithms. In turn, these implementations can be used for testing, debugging, and resource estimation. In addition to a prototype implementation of quantum MPI, we present a performance model for distributed quantum computing, SENDQ. The model is inspired by the classical LogP model, making it useful to inform algorithmic decisions when programming distributed quantum computers. Specifically, we consider several optimizations of two quantum algorithms for problems in physics and chemistry, and we detail their effects on performance in the SENDQ model.
Testing nonlinear-QED at the future linear collider with an intense laser<|sep|>The future linear collider will collide dense $e^+e^-$ bunches at high energies up to 1 TeV, generating very intense electromagnetic fields at the interaction point (IP). These fields are strong enough to lead to nonlinear effects which affect all IP processes and which are described by strong field physics theory. In order to test this theory, we propose an experiment that will focus an intense laser on the LC electron beam post-IP. Similar experiments at SLAC E144 have investigated nonlinear Compton scattering, Breit-Wheeler pair production using an electron beam of 46.6 GeV. The higher beam energies available at the future LC would allow more precise studies of these phenomena. Mass-shift and spin-dependent effects could also be investigated.
Time-varying formation tracking of multiple manipulators via distributed finite-time control<|sep|>Comparing with traditional fixed formation for a group of dynamical systems, time-varying formation can produce the following benefits: i) covering the greater part of complex environments; ii) collision avoidance. This paper studies the time-varying formation tracking for multiple manipulator systems (MMSs) under fixed and switching directed graphs with a dynamic leader, whose acceleration cannot change too fast. An explicit mathematical formulation of time-varying formation is developed based on the related practical applications. A class of extended inverse dynamics control algorithms combining with distributed sliding-mode estimators are developed to address the aforementioned problem. By invoking finite-time stability arguments, several novel criteria (including sufficient criteria, necessary and sufficient criteria) for global finite-time stability of MMSs are established. Finally, numerical experiments are presented to verify the effectiveness of the theoretical results.
Diffractive Bremsstrahlung at High-$\beta^\star$ LHC Case Study<|sep|>Feasibility studies of the measurement of the exclusive diffractive bremsstrahlung cross-section in proton-proton scattering at the centre of mass energy of 13 TeV at the LHC are reported. Present studies were performed for the low luminosity LHC running with the betatron function value of 90~m using the ATLAS associated forward detectors ALFA and ZDC. A simplified approach to the event simulation and reconstruction is used. The background influence is also discussed.
An MCMC-free approach to post-selective inference<|sep|>We develop a Monte Carlo-free approach to inference post output from randomized algorithms with a convex loss and a convex penalty. The pivotal statistic based on a truncated law, called the selective pivot, usually lacks closed form expressions. Inference in these settings relies upon standard Monte Carlo sampling techniques at a reference parameter followed by an exponential tilting at the reference. Tilting can however be unstable for parameters that are far off from the reference parameter. We offer in this paper an alternative approach to construction of intervals and point estimates by proposing an approximation to the intractable selective pivot. Such an approximation solves a convex optimization problem in |E| dimensions, where |E| is the size of the active set observed from selection. We empirically show that the confidence intervals obtained by inverting the approximate pivot have valid coverage.
Periodic travelling waves of the modified KdV equation and rogue waves on the periodic background<|sep|>We address the most general periodic travelling wave of the modified Korteweg-de Vries (mKdV) equation written as a rational function of Jacobian elliptic functions. By applying an algebraic method which relates the periodic travelling waves and the squared periodic eigenfunctions of the Lax operators, we characterize explicitly the location of eigenvalues in the periodic spectral problem away from the imaginary axis. We show that Darboux transformations with the periodic eigenfunctions remain in the class of the same periodic travelling waves of the mKdV equation. In a general setting, there are exactly three symmetric pairs of eigenvalues away from the imaginary axis, and we give a new representation of the second non-periodic solution to the Lax equations for the same eigenvalues. We show that Darboux transformations with the non-periodic solutions to the Lax equations produce rogue waves on the periodic background, which are either brought from infinity by propagating algebraic solitons or formed in a finite region of the time-space plane.
WSEBP: A Novel Width-depth Synchronous Extension-based Basis Pursuit Algorithm for Multi-Layer Convolutional Sparse Coding<|sep|>The pursuit algorithms integrated in multi-layer convolutional sparse coding (ML-CSC) can interpret the convolutional neural networks (CNNs). However, many current state-of-art (SOTA) pursuit algorithms require multiple iterations to optimize the solution of ML-CSC, which limits their applications to deeper CNNs due to high computational cost and large number of resources for getting very tiny gain of performance. In this study, we focus on the 0th iteration in pursuit algorithm by introducing an effective initialization strategy for each layer, by which the solution for ML-CSC can be improved. Specifically, we first propose a novel width-depth synchronous extension-based basis pursuit (WSEBP) algorithm which solves the ML-CSC problem without the limitation of the number of iterations compared to the SOTA algorithms and maximizes the performance by an effective initialization in each layer. Then, we propose a simple and unified ML-CSC-based classification network (ML-CSC-Net) which consists of an ML-CSC-based feature encoder and a fully-connected layer to validate the performance of WSEBP on image classification task. The experimental results show that our proposed WSEBP outperforms SOTA algorithms in terms of accuracy and consumption resources. In addition, the WSEBP integrated in CNNs can improve the performance of deeper CNNs and make them interpretable. Finally, taking VGG as an example, we propose WSEBP-VGG13 to enhance the performance of VGG13, which achieves competitive results on four public datasets, i.e., 87.79% vs. 86.83% on Cifar-10 dataset, 58.01% vs. 54.60% on Cifar-100 dataset, 91.52% vs. 89.58% on COVID-19 dataset, and 99.88% vs. 99.78% on Crack dataset, respectively. The results show the effectiveness of the proposed WSEBP, the improved performance of ML-CSC with WSEBP, and interpretation of the CNNs or deeper CNNs.
On the equivalence between Starobinsky and Higgs inflationary models in gravity and supergravity<|sep|>Starobinsky inflation and Higgs inflation in gravity, and their equivalence based on the common inflationary potential are extended to supergravity in the proper framework, where the Starobinsky and Higgs descriptions of inflation arise in two different gauges of the same supergravity model.
Deep Model Reference Adaptive Control<|sep|>We present a new neuroadaptive architecture: Deep Neural Network based Model Reference Adaptive Control (DMRAC). Our architecture utilizes the power of deep neural network representations for modeling significant nonlinearities while marrying it with the boundedness guarantees that characterize MRAC based controllers. We demonstrate through simulations and analysis that DMRAC can subsume previously studied learning based MRAC methods, such as concurrent learning and GP-MRAC. This makes DMRAC a highly powerful architecture for high-performance control of nonlinear systems with long-term learning properties.
Free Boundary Problems in Shock Reflection/Diffraction and Related Transonic Flow Problems<|sep|>Shock waves are steep wave fronts that are fundamental in nature, especially in high-speed fluid flows. When a shock hits an obstacle, or a flying body meets a shock, shock reflection/diffraction phenomena occur. In this paper, we show how several longstanding shock reflection/diffraction problems can be formulated as free boundary problems, discuss some recent progress in developing mathematical ideas, approaches, and techniques for solving these problems, and present some further open problems in this direction. In particular, these shock problems include von Neumann's problem for shock reflection-diffraction by two-dimensional wedges with concave corner, Lighthill's problem for shock diffraction by two-dimensional wedges with convex corner, and Prandtl-Meyer's problem for supersonic flow impinging onto solid wedges, which are also fundamental in the mathematical theory of multidimensional conservation laws.
Testing gravitational theories using Eccentric Eclipsing Detached Binaries<|sep|>In this paper we compare the effects of different theories of gravitation on the apsidal motion of a sample of Eccentric Eclipsing Detached Binary stars. The comparison is performed by using the formalism of the Post-Newtonian parametrization to calculate the theoretical advance at periastron and compare it to the observed one, after having considered the effects of the structure and rotation of the involved stars. A variance analysis on the results of this comparison, shows that no significant difference can be found due to the effect of the different theories under test with respect to the standard General Relativity. It will be possible to observe differences, as we would expect, by checking the observed period variation on a much larger lapse of time. It can also be noticed from our results, that f(R) theory is the nearest to GR with respect to the other tested theories.
Gravitational excitation of high frequency QPOs<|sep|>We discuss the possibility that high-frequency QPOs in neutron-star binary systems may result from forced resonant oscillations of matter in the innermost parts of the accretion disc, excited by gravitational perturbations coming from asymmetries of the neutron star or from the companion star. We find that neutron-star asymmetries could, in principle, be effective for inducing both radial and vertical oscillations of relevant amplitude while the binary companion might possibly produce significant radial oscillations but not vertical ones. Misaligned neutron-star quadrupole moments of a size advocated elsewhere for explaining limiting neutron star periods could be large enough also for the present purpose.
Implicit Large Eddy Simulation of Cavitation in Micro Channel Flows<|sep|>We present a numerical method for Large Eddy Simulations (LES) of compressible two-phase flows. The method is validated for the flow in a micro channel with a step-like restriction. This setup is representative for typical cavitating multi-phase flows in fuel injectors and follows an experimental study of Iben et al., 2010. While a diesel-like test fuel was used in the experiment, we solve the compressible Navier-Stokes equations with a barotropic equation of state for water and vapor and a simple phase-change model based on equilibrium assumptions. Our LES resolve all wave dynamics in the compressible fluid and the turbulence production in shear layers.
Learning on Hypergraphs with Sparsity<|sep|>Hypergraph is a general way of representing high-order relations on a set of objects. It is a generalization of graph, in which only pairwise relations can be represented. It finds applications in various domains where relationships of more than two objects are observed. On a hypergraph, as a generalization of graph, one wishes to learn a smooth function with respect to its topology. A fundamental issue is to find suitable smoothness measures of functions on the nodes of a graph/hypergraph. We show a general framework that generalizes previously proposed smoothness measures and also gives rise to new ones. To address the problem of irrelevant or noisy data, we wish to incorporate sparse learning framework into learning on hypergraphs. We propose sparsely smooth formulations that learn smooth functions and induce sparsity on hypergraphs at both hyperedge and node levels. We show their properties and sparse support recovery results. We conduct experiments to show that our sparsely smooth models have benefits to irrelevant and noisy data, and usually give similar or improved performances compared to dense models.
UAV-based Autonomous Image Acquisition with Multi-View Stereo Quality Assurance by Confidence Prediction<|sep|>In this paper we present an autonomous system for acquiring close-range high-resolution images that maximize the quality of a later-on 3D reconstruction with respect to coverage, ground resolution and 3D uncertainty. In contrast to previous work, our system uses the already acquired images to predict the confidence in the output of a dense multi-view stereo approach without executing it. This confidence encodes the likelihood of a successful reconstruction with respect to the observed scene and potential camera constellations. Our prediction module runs in real-time and can be trained without any externally recorded ground truth. We use the confidence prediction for on-site quality assurance and for planning further views that are tailored for a specific multi-view stereo approach with respect to the given scene. We demonstrate the capabilities of our approach with an autonomous Unmanned Aerial Vehicle (UAV) in a challenging outdoor scenario.
Elliptic solutions in the H\'{e}non - Heiles model<|sep|>Equations of motion corresponding to the H\'{e}non - Heiles system are considered. A method enabling one to find all elliptic solutions of an autonomous ordinary differential equation or a system of autonomous ordinary differential equations is described. New families of elliptic solutions of a fourth--order equation related to the H\'{e}non - Heiles system are obtained. A classification of elliptic solutions up to the sixth order inclusively is presented.
A Framework for the Verification of Certifying Computations<|sep|>Formal verification of complex algorithms is challenging. Verifying their implementations goes beyond the state of the art of current automatic verification tools and usually involves intricate mathematical theorems. Certifying algorithms compute in addition to each output a witness certifying that the output is correct. A checker for such a witness is usually much simpler than the original algorithm - yet it is all the user has to trust. The verification of checkers is feasible with current tools and leads to computations that can be completely trusted. We describe a framework to seamlessly verify certifying computations. We use the automatic verifier VCC for establishing the correctness of the checker and the interactive theorem prover Isabelle/HOL for high-level mathematical properties of algorithms. We demonstrate the effectiveness of our approach by presenting the verification of typical examples of the industrial-level and widespread algorithmic library LEDA.
Order-chaos-order and invariant manifolds in the bounded planar Earth-Moon system<|sep|>In this work, we investigate the Earth-Moon system, as modeled by the planar circular restricted three-body problem, and relate its dynamical properties to the underlying structure associated with specific invariant manifolds. We consider a range of Jacobi constant values for which the neck around the Lagrangian point $L_1$ is always open but the orbits are bounded due to Hill stability. First, we show that the system displays three different dynamical scenarios in the neighborhood of the Moon: two mixed ones, with regular and chaotic orbits, and an almost entirely chaotic one in between. We then analyze the transitions between these scenarios using the Monodromy matrix theory and determine that they are given by two specific types of bifurcations. After that, we illustrate how the phase space configurations, particularly the shapes of stability regions and stickiness, are intrinsically related to the hyperbolic invariant manifolds of the Lyapunov orbits around $L_1$ and also to the ones of some particular unstable periodic orbits. Lastly, we define transit time in a manner that is useful to depict dynamical trapping and show that the traced geometrical structures are also connected to the transport properties of the system.
Social network dynamics of face-to-face interactions<|sep|>The recent availability of data describing social networks is changing our understanding of the "microscopic structure" of a social tie. A social tie indeed is an aggregated outcome of many social interactions such as face-to-face conversations or phone-calls. Analysis of data on face-to-face interactions shows that such events, as many other human activities, are bursty, with very heterogeneous durations. In this paper we present a model for social interactions at short time scales, aimed at describing contexts such as conference venues in which individuals interact in small groups. We present a detailed anayltical and numerical study of the model's dynamical properties, and show that it reproduces important features of empirical data. The model allows for many generalizations toward an increasingly realistic description of social interactions. In particular in this paper we investigate the case where the agents have intrinsic heterogeneities in their social behavior, or where dynamic variations of the local number of individuals are included. Finally we propose this model as a very flexible framework to investigate how dynamical processes unfold in social networks.
Impact parameter dependent colour glass condensate dipole model<|sep|>We show that the colour glass condensate dipole model of Iancu, Itakura and Munier, improved to include the impact parameter dependence, gives a good fit to the total gamma* p cross section measured at HERA if the anomalous dimension at the saturation scale, gamma_s, is treated as a free parameter. We find that the optimum value of gamma_s = 0.46 is close to the value determined from numerical solution of the Balitsky-Kovchegov equation. The impact parameter dependent saturation scale is generally less than 0.5 GeV^2 in the HERA kinematic regime for the most relevant impact parameters b ~ 2-3 GeV^{-1}. We compare predictions of the model to data on the longitudinal and heavy flavour structure functions, exclusive diffractive vector meson production and deeply virtual Compton scattering at HERA. The model is found to be deficient for observables sensitive to moderately small dipole sizes, where an alternative model with explicit DGLAP evolution performs better. The energy dependence of exclusive diffractive processes is shown to provide an important discriminator between different dipole model cross sections.
Exact solutions of nonlinear delay reaction-diffusion equations with variable coefficients<|sep|>A modified method of functional constraints is used to construct the exact solutions of nonlinear equations of reaction-diffusion type with delay and which are associated with variable coefficients. This study considers a most generalized form of nonlinear equations of reaction-diffusion type with delay and which are nonlinear and associated with variable coefficients. A novel technique is used in this study to obtain the exact solutions which are new and are of the form of traveling-wave solutions. Arbitrary functions are present in the solutions and they also contain free parameters, which make them suitable for usage in solving certain modeling problems, testing numerical and approximate analytical methods. The results of this study also find applications in obtaining the exact solutions of other forms of partial differential equations which are more complex. Specific examples of nonlinear equations of reaction-diffusion type with delay are given and their exact solutions are presented. Solutions of certain reaction-diffusion equations are also displayed graphically.
Expeditious Generation of Knowledge Graph Embeddings<|sep|>Knowledge Graph Embedding methods aim at representing entities and relations in a knowledge base as points or vectors in a continuous vector space. Several approaches using embeddings have shown promising results on tasks such as link prediction, entity recommendation, question answering, and triplet classification. However, only a few methods can compute low-dimensional embeddings of very large knowledge bases without needing state-of-the-art computational resources. In this paper, we propose KG2Vec, a simple and fast approach to Knowledge Graph Embedding based on the skip-gram model. Instead of using a predefined scoring function, we learn it relying on Long Short-Term Memories. We show that our embeddings achieve results comparable with the most scalable approaches on knowledge graph completion as well as on a new metric. Yet, KG2Vec can embed large graphs in lesser time by processing more than 250 million triples in less than 7 hours on common hardware.
Adaptation of the visibility graph algorithm to find the time lag between hydrogeological time series<|sep|>Estimating the time lag between two hydrogeologic time series (e.g. precipitation and water levels in an aquifer) is of significance for a hydrogeologist-modeler. In this paper, we present a method to quantify such lags by adapting the visibility graph algorithm, which converts time series into a mathematical graph. We present simulation results to assess the performance of the method. We also illustrate the utility of our approach using a real world hydrogeologic dataset.
Controlled irradiation hardening of tungsten by cyclic recrystallization<|sep|>The economical lifetime of the divertor is a key concern for realizing nuclear fusion reactors that may solve the world's energy problem. A main risk is thermo-mechanical failure of the plasma-facing tungsten monoblocks, as a consequence of irradiation hardening induced by neutron displacement cascades. Lifetime extensions that could be carried out without prolonged maintenance periods are desired. In this work, the effects of potential treatments for extending the lifetime of an operational reactor are explored. The proposed treatments make use of cyclic recrystallization processes that can occur in neutron-irradiated tungsten. Evolution of the microstructure under non-isothermal conditions is investigated, employing a multi-scale model that includes a physically-based mean-field recrystallization model and a cluster dynamics model for neutron irradiation effects. The model takes into account microstructural properties such as grain size and displacement-induced defect concentrations. The evolution of a hardness indicator under neutron irradiation was studied. The results reveal that, for the given microstructure and under the assumed model behaviour, periodical extra heating can have a significant positive influence on controlling the irradiation hardening. For example, at 800 C, if extra annealing at 1200 C was applied after every 100 hrs for the duration of 1 hr, then the hardness indicator reduces from maximum 140 to below 70.
Approximation of the truncated Zeta distribution and Zipf's law<|sep|>Zipf's law appears in many application areas but does not have a closed form expression, which may make its use cumbersome. Since it coincides with the truncated version of the Zeta distribution, in this paper we propose three approximate closed form expressions for the truncated Zeta distribution, which may be employed for Zipf's law as well. The three approximations are based on the replacement of the sum occurring in Zipf's law with an integral, and are named respectively the integral approximation, the average integral approximation, and the trapezoidal approximation. While the first one is shown to be of little use, the trapezoidal approximation exhibits an error which is typically lower than 1\%, but is as low as 0.1\% for the range of values of the Zipf parameter below 1.
Dynamics of isolated magnetic bright points derived from Hinode/SOT G-band observations<|sep|>Small-scale magnetic fields in the solar photosphere can be identified in high-resolution magnetograms or in the G-band as magnetic bright points (MBPs). Rapid motions of these fields can cause magneto-hydrodynamical waves and can also lead to nanoflares by magnetic field braiding and twisting. The MBP velocity distribution is a crucial parameter for estimating the amplitudes of those waves and the amount of energy they can contribute to coronal heating. The velocity and lifetime distributions of MBPs are derived from solar G-band images of a quiet sun region acquired by the Hinode/SOT instrument with different temporal and spatial sampling rates. We developed an automatic segmentation, identification and tracking algorithm to analyse G-Band image sequences to obtain the lifetime and velocity distributions of MBPs. The influence of temporal/spatial sampling rates on these distributions is studied and used to correct the obtained lifetimes and velocity distributions for these digitalisation effects. After the correction of algorithm effects, we obtained a mean MBP lifetime of (2.50 +- 0.05) min and mean MBP velocities, depending on smoothing processes, in the range of (1 - 2) km/s. Corrected for temporal sampling effects, we obtained for the effective velocity distribution a Rayleigh function with a coefficient of (1.62 +- 0.05) km/s. The x- and y- components of the velocity distributions are Gaussians. The lifetime distribution can be fitted by an exponential function.
QuECT: A New Quantum Programming Paradigm<|sep|>Quantum computation constitutes a rapidly expanding subfield of computer science. Development quantum algorithms is facilitated by the availability of efficient quantum programming languages, and a plethora of approaches has been already suggested in the literature, ranging from GUI-based simple tools to elaborate standalone programming languages. In this paper we propose a novel paradigm called Quantum Embeddable Circuit Technique (QuECT) that allows a programmer to embed a circuit diagram in a classical "host" language. The paradigm can be implemented in any modern classical language. A prototype has been developed by the author using Java.
Origin of the pseudogap and its influence on superconducting state<|sep|>When holes move in the background of strong antiferromagnetic correlation, two effects with different spatial scale emerge, leading to a much reduced hopping integral with an additional phase factor. An effective Hamiltonian is then proposed to investigate the underdoped cuprates. We argue that the pseudogap is the consequence of dressed hole moving in the antiferromagnetic background and has nothing to do with the superconductivity. The momentum distributions of the gap are qualitatively consistent with the recent ARPES measurements both in the pseudogap and superconducting state. Two thermal qualities are further calculated to justify our model. A two-gap scenario is concluded to describe the relation between the two gaps.
Branching ratio measurements and isospin violation in B-meson decays<|sep|>The approximate symmetry of the strong interactions under isospin transformations is among the most precise tools available to control hadronic matrix elements. It is crucial in extracting fundamental parameters, but also provides avenues for the search of phenomena beyond the Standard Model. The precision of the resulting predictions requires special care when determining the quantities they are to be tested with. Specifically, in the extraction of branching ratios often isospin symmetry is assumed at one point or another implicitly, implying a significant bias for precision analyses. We extract a bias-free value for the production asymmetry between charged and neutral $B$ meson pairs at $B$ factories and discuss its consequences for the determination of branching fractions generally, and isospin-violating observables like the rate asymmetries in B -> J/psi K or B -> K* gamma decays specifically.
Secure Routing in Wireless Mesh Networks<|sep|>Wireless mesh networks (WMNs) have emerged as a promising concept to meet the challenges in next-generation networks such as providing flexible, adaptive, and reconfigurable architecture while offering cost-effective solutions to the service providers. Unlike traditional Wi-Fi networks, with each access point (AP) connected to the wired network, in WMNs only a subset of the APs are required to be connected to the wired network. The APs that are connected to the wired network are called the Internet gateways (IGWs), while the APs that do not have wired connections are called the mesh routers (MRs). The MRs are connected to the IGWs using multi-hop communication. The IGWs provide access to conventional clients and interconnect ad hoc, sensor, cellular, and other networks to the Internet. However, most of the existing routing protocols for WMNs are extensions of protocols originally designed for mobile ad hoc networks (MANETs) and thus they perform sub-optimally. Moreover, most routing protocols for WMNs are designed without security issues in mind, where the nodes are all assumed to be honest. In practical deployment scenarios, this assumption does not hold. This chapter provides a comprehensive overview of security issues in WMNs and then particularly focuses on secure routing in these networks. First, it identifies security vulnerabilities in the medium access control (MAC) and the network layers. Various possibilities of compromising data confidentiality, data integrity, replay attacks and offline cryptanalysis are also discussed. Then various types of attacks in the MAC and the network layers are discussed. After enumerating the various types of attacks on the MAC and the network layer, the chapter briefly discusses on some of the preventive mechanisms for these attacks.
Parameterizing the interstellar dust temperature<|sep|>The temperature of interstellar dust particles is of great importance to astronomers. It plays a crucial role in the thermodynamics of interstellar clouds, because of the gas-dust collisional coupling. It is also a key parameter in astrochemical studies that governs the rate at which molecules form on dust. In 3D (magneto)hydrodynamic simulations often a simple expression for the dust temperature is adopted, because of computational constraints, while astrochemical modelers tend to keep the dust temperature constant over a large range of parameter space. Our aim is to provide an easy-to-use parametric expression for the dust temperature as a function of visual extinction ($A_{\rm V}$) and to shed light on the critical dependencies of the dust temperature on the grain composition. We obtain an expression for the dust temperature by semi-analytically solving the dust thermal balance for different types of grains and compare to a collection of recent observational measurements. We also explore the effect of ices on the dust temperature. Our results show that a mixed carbonaceous-silicate type dust with a high carbon volume fraction matches the observations best. We find that ice formation allows the dust to be warmer by up to 15% at high optical depths ($A_{\rm V}> 20$ mag) in the interstellar medium. Our parametric expression for the dust temperature is presented as $T_{\rm d} = \left[ 11 + 5.7\times \tanh\bigl( 0.61 - \log_{10}(A_{\rm V})\bigr) \right] \, \chi_{\rm uv}^{1/5.9}$, where $\chi_{\rm uv}$ is in units of the Draine (1978) UV field
Norm-Scaling for Out-of-Distribution Detection<|sep|>Out-of-Distribution (OoD) inputs are examples that do not belong to the true underlying distribution of the dataset. Research has shown that deep neural nets make confident mispredictions on OoD inputs. Therefore, it is critical to identify OoD inputs for safe and reliable deployment of deep neural nets. Often a threshold is applied on a similarity score to detect OoD inputs. One such similarity is angular similarity which is the dot product of latent representation with the mean class representation. Angular similarity encodes uncertainty, for example, if the angular similarity is less, it is less certain that the input belongs to that class. However, we observe that, different classes have different distributions of angular similarity. Therefore, applying a single threshold for all classes is not ideal since the same similarity score represents different uncertainties for different classes. In this paper, we propose norm-scaling which normalizes the logits separately for each class. This ensures that a single value consistently represents similar uncertainty for various classes. We show that norm-scaling, when used with maximum softmax probability detector, achieves 9.78% improvement in AUROC, 5.99% improvement in AUPR and 33.19% reduction in FPR95 metrics over previous state-of-the-art methods.
MVIN: Learning Multiview Items for Recommendation<|sep|>Researchers have begun to utilize heterogeneous knowledge graphs (KGs) as auxiliary information in recommendation systems to mitigate the cold start and sparsity issues. However, utilizing a graph neural network (GNN) to capture information in KG and further apply in RS is still problematic as it is unable to see each item's properties from multiple perspectives. To address these issues, we propose the multi-view item network (MVIN), a GNN-based recommendation model which provides superior recommendations by describing items from a unique mixed view from user and entity angles. MVIN learns item representations from both the user view and the entity view. From the user view, user-oriented modules score and aggregate features to make recommendations from a personalized perspective constructed according to KG entities which incorporates user click information. From the entity view, the mixing layer contrasts layer-wise GCN information to further obtain comprehensive features from internal entity-entity interactions in the KG. We evaluate MVIN on three real-world datasets: MovieLens-1M (ML-1M), LFM-1b 2015 (LFM-1b), and Amazon-Book (AZ-book). Results show that MVIN significantly outperforms state-of-the-art methods on these three datasets. In addition, from user-view cases, we find that MVIN indeed captures entities that attract users. Figures further illustrate that mixing layers in a heterogeneous KG plays a vital role in neighborhood information aggregation.
The radial distributions of the two main-sequence components in the young massive star cluster NGC 1856<|sep|>The recent discovery of double main sequences in the young, massive star cluster NGC 1856 has caught significant attention. The observations can be explained by invoking two stellar generations with different ages and metallicities or by a single generation of stars composed of two populations characterized by different rotation rates. We analyzed the number ratios of stars belonging to both main-sequence components in NGC 1856 as a function of radius. We found that their number ratios remain approximately unchanged from the cluster's central region to its periphery, indicating that both components are homogeneously distributed in space. Through a comparison of the loci of the best-fitting isochrones with the ridge lines of both stellar components, we found that both multiple stellar populations and rapid stellar rotation can potentially explain the observed main-sequence bifurcation in NGC 1856. However, if NGC1856 were a young representative of the old globular clusters, then the multiple stellar populations model would not be able to explain the observed homogeneity in the spatial distributions of these two components, since all relevant scenarios would predict that the second stellar generation should be formed in a more compact configuration than that of the first stellar generation, while NGC 1856 is too young for both stellar generations to have been fully mixed dynamically. We speculate that the rapid stellar rotation scenario would be the favored explanation of the observed multiple stellar sequences in NGC 1856.
Global Patterns of Synchronization in Human Communications<|sep|>Social media are transforming global communication and coordination. The data derived from social media can reveal patterns of human behavior at all levels and scales of society. Using geolocated Twitter data, we have quantified collective behaviors across multiple scales, ranging from the commutes of individuals, to the daily pulse of 50 major urban areas and global patterns of human coordination. Human activity and mobility patterns manifest the synchrony required for contingency of actions between individuals. Urban areas show regular cycles of contraction and expansion that resembles heartbeats linked primarily to social rather than natural cycles. Business hours and circadian rhythms influence daily cycles of work, recreation, and sleep. Different urban areas have characteristic signatures of daily collective activities. The differences are consistent with a new emergent global synchrony that couples behavior in distant regions across the world. A globally synchronized peak that includes exchange of ideas and information across Europe, Africa, Asia and Australasia. We propose a dynamical model to explain the emergence of global synchrony in the context of increasing global communication and reproduce the observed behavior. The collective patterns we observe show how social interactions lead to interdependence of behavior manifest in the synchronization of communication. The creation and maintenance of temporally sensitive social relationships results in the emergence of complexity of the larger scale behavior of the social system.
$B_K$ with improved staggered fermions: analysis using SU(3) staggered chiral perturbation theory<|sep|>We report updated results for $B_K$ using HYP-smeared staggered valence quarks on MILC asqtad lattices based on an analysis using SU(3) staggered chiral perturbation theory. The most important new feature of our data sample is the inclusion of a fourth ("ultrafine") lattice spacing. This improves the control over the continuum extrapolation and errors due our use of one-loop perturbative matching. We present a complete updated error budget, which leads to $B_K(\text{NDR}, \mu = 2 \text{GeV}) = 0.5309 \pm 0.0051 \pm 0.0424$ and $\hat{B}_K = B_K(\text{RGI}) = 0.727 \pm 0.07 \pm 0.058$. The results of the SU(3) analysis are inferior to those based on SU(2) staggered chiral perturbation theory, primarily because of the dependence on the Bayesian priors we use in the SU(3) fits.
The rest-frame optical sizes of massive galaxies with suppressed star formation at $z\sim4$<|sep|>We present the rest-frame optical sizes of massive quiescent galaxies (QGs) at $z\sim4$ measured at $K'$-band with the Infrared Camera and Spectrograph (IRCS) and AO188 on the Subaru telescope. Based on a deep multi-wavelength catalog in the Subaru XMM-Newton Deep Survey Field (SXDS), covering a wide wavelength range from the $u$-band to the IRAC $8.0\mu m$ over 0.7 deg$^2$, we evaluate photometric redshift to identify massive ($M_{\star}\sim10^{11}\ M_\odot$) galaxies with suppressed star formation. These galaxies show a prominent 4000$\rm \AA$ break feature at $z\sim4$, suggestive of an evolved stellar population. We then conduct follow-up $K'$-band imaging with adaptive optics for the five brightest galaxies ($K_{AB,total}=22.5\sim23.4$). Compared to lower redshift ones, QGs at $z\sim4$ have smaller physical sizes of effective radii $r_{eff}=0.2$ to $1.8$ kpc. The mean size measured by stacking the four brightest objects is $r_{eff}=0.7\rm\ kpc$. This is the first measurement of the rest-frame optical sizes of QGs at $z\sim4$. We evaluate the robustness of our size measurements using simulations and find that our size estimates are reasonably accurate with an expected systematic bias of $\sim0.2$ kpc. If we account for the stellar mass evolution, massive QGs at $z\sim4$ are likely to evolve into the most massive galaxies today. We find their size evolution with cosmic time in a form of $\log(r_e/{\rm kpc})= -0.44+1.77 \log(t/\rm Gyr)$. Their size growth is proportional to the square of stellar mass, indicating the size-stellar mass growth driven by minor dry mergers.
Code Farming: A Process for Creating Generic Computational Building Blocks<|sep|>Motivated by a desire to improve on the current state of the art in genetic programming, and aided by recent progress in understanding the computational aspects of evolutionary systems, we describe a process that creates a set of generic computational building blocks for the purpose of seeding initial populations of programs in any genetic programming system. This provides an advantage over the standard approach of initializing the population purely randomly in that it avoids the need to constantly rediscover such building blocks. It is also better than seeding the initial population with hand-coded building blocks, since it lessens the amount of human intervention required by the system.
Multi-Wavelength Observations of the HBL Object 1ES 1011+496 in Spring 2008<|sep|>In the spring of 2008 MAGIC organised multi-wavelength (MWL) observations of the blazar 1ES 1011+496. 1ES 1011+496 is a high-frequency peaked BL Lac object discovered at VHE gamma-rays by MAGIC in spring 2007 during an optical outburst reported by the Tuorla Blazar Monitoring Programme. MAGIC re-observed the source during the 2008 MWL campaign which also included the Mets\"ahovi, KVA, Swift and AGILE telescopes. This was the first MWL campaign on this source that also included VHE coverage. MAGIC observed 1ES 1011+496 from March 4th to May 24th 2008 for a total of 27.9 hours, of which 20 h remained after quality cuts. The observations resulted in a detection of the source a ~7 sigma significance level with a mean flux and spectral index similar to those during the discovery. Here we will present the results of the MAGIC observations of the source in combination with contemporaneous observations at other wavelengths (radio, optical, X-rays, high energy gamma-rays) and discuss their implications on the modelling of the spectral energy distribution.
A fast and robust solver for the scattering from a layered periodic structure containing multi-particle inclusions<|sep|>We present a solver for plane wave scattering from a periodic dielectric grating with a large number $M$ of inclusions lying in each period of its middle layer.Such composite material geometries have a growing role in modern photonic devices and solar cells. The high-order scheme is based on boundary integral equations, and achieves many digits of accuracy with ease. The usual way to periodize the integral equation---via the quasi-periodic Green's function---fails at Wood's anomalies. We instead use the free-space Green's kernel for the near field, add auxiliary basis functions for the far field, and enforce periodicity in an expanded linear system; this is robust for all parameters. Inverting the periodic and layer unknowns, we are left with a square linear system involving only the inclusion scattering coefficients. Preconditioning by the single-inclusion scattering matrix, this is solved iteratively in $O(M)$ time using a fast matrix-vector product. Numerical experiments show that a diffraction grating containing $M=1000$ inclusions per period can be solved to 9-digit accuracy in under 5 minutes on a laptop.
Fermions in a Walecka-type cosmology<|sep|>A simplified Walecka-type model is investigated in a cosmological scenario. The model includes fermionic, scalar and vector fields as sources. It is shown that their interactions, taking place in a Robertson-Walker metric, could be responsible for the transition of accelerated-decelerated periods in the early universe and a current accelerated regime. It is also discussed the role of the fermionic field as the promoter of the accelerated regimes in the early and the late stages of the universe.
Non-standard neutrino oscillations: perspective from unitarity triangles<|sep|>We formulate an alternative approach based on unitarity triangles to describe neutrino oscillations in presence of non-standard interactions (NSI). Using perturbation theory, we derive the expression for the oscillation probability in case of NSI and cast it in terms of the three independent parameters of the leptonic unitarity triangle (LUT). The form invariance of the probability expression (even in presence of new physics scenario as long as the mixing matrix is unitary) facilitates a neat geometric view of neutrino oscillations in terms of LUT. We examine the regime of validity of perturbative expansions in the NSI case and make comparisons with approximate expressions existing in literature. We uncover some interesting dependencies on NSI terms while studying the evolution of LUT parameters and the Jarlskog invariant. Interestingly, the geometric approach based on LUT allows us to express the oscillation probabilities for a given pair of neutrino flavours in terms of only three (and not four) degrees of freedom which are related to the geometric properties (sides and angles) of the triangle. Moreover, the LUT parameters are invariant under rephasing transformations and independent of the parameterization adopted.
Lack of Transit Timing Variations of OGLE-TR-111b: A re-analysis with six new epochs<|sep|>We present six new transits of the exoplanet OGLE-TR-111b observed with the Magellan Telescopes in Chile between April 2008 and March 2009. We combine these new transits with five previously published transit epochs for this planet between 2005 and 2006 to extend the analysis of transit timing variations reported for this system. We derive a new planetary radius value of 1.019 +/- 0.026 R_J, which is intermediate to the previously reported radii of 1.067 +/- 0.054 R_J (Winn et al. 2007) and 0.922 +/- 0.057 R_J (Diaz et al. 2008). We also examine the transit timing variation and duration change claims of Diaz et al. (2008). Our analysis of all eleven transit epochs does not reveal any points with deviations larger than 2 sigma, and most points are well within 1 sigma. Although the transit duration nominally decreases over the four year span of the data, systematic errors in the photometry can account for this result. Therefore, there is no compelling evidence for either a timing or a duration variation in this system. Numerical integrations place an upper limit of about 1 M_E on the mass of a potential second planet in a 2:1 mean-motion resonance with OGLE-TR-111b.
Estimating Appearance Models for Image Segmentation via Tensor Factorization<|sep|>Image Segmentation is one of the core tasks in Computer Vision and solving it often depends on modeling the image appearance data via the color distributions of each it its constituent regions. Whereas many segmentation algorithms handle the appearance models dependence using alternation or implicit methods, we propose here a new approach to directly estimate them from the image without prior information on the underlying segmentation. Our method uses local high order color statistics from the image as an input to tensor factorization-based estimator for latent variable models. This approach is able to estimate models in multiregion images and automatically output the regions proportions without prior user interaction, overcoming the drawbacks from a prior attempt to this problem. We also demonstrate the performance of our proposed method in many challenging synthetic and real imaging scenarios and show that it leads to an efficient segmentation algorithm.
Satisfiability of General Intruder Constraints with and without a Set Constructor<|sep|>Many decision problems on security protocols can be reduced to solving so-called intruder constraints in Dolev Yao model. Most constraint solving procedures for protocol security rely on two properties of constraint systems called monotonicity and variable origination. In this work we relax these restrictions by giving a decision procedure for solving general intruder constraints (that do not have these properties) that stays in NP. Our result extends a first work by L. Mazar\'e in several directions: we allow non-atomic keys, and an associative, commutative and idempotent symbol (for modeling sets). We also discuss several new applications of the results.
Towards Large-scale Inconsistency Measurement<|sep|>We investigate the problem of inconsistency measurement on large knowledge bases by considering stream-based inconsistency measurement, i.e., we investigate inconsistency measures that cannot consider a knowledge base as a whole but process it within a stream. For that, we present, first, a novel inconsistency measure that is apt to be applied to the streaming case and, second, stream-based approximations for the new and some existing inconsistency measures. We conduct an extensive empirical analysis on the behavior of these inconsistency measures on large knowledge bases, in terms of runtime, accuracy, and scalability. We conclude that for two of these measures, the approximation of the new inconsistency measure and an approximation of the contension inconsistency measure, large-scale inconsistency measurement is feasible.
A numerical adaptation of SAW identities from the honeycomb to other 2D lattices<|sep|>Recently, Duminil-Copin and Smirnov proved a long-standing conjecture by Nienhuis that the connective constant of self-avoiding walks on the honeycomb lattice is $\sqrt{2+\sqrt{2}}.$ A key identity used in that proof depends on the existence of a parafermionic observable for self-avoiding walks on the honeycomb lattice. Despite the absence of a corresponding observable for SAW on the square and triangular lattices, we show that in the limit of large lattices, some of the consequences observed on the honeycomb lattice persist on other lattices. This permits the accurate estimation, though not an exact evaluation, of certain critical amplitudes, as well as critical points, for these lattices. For the honeycomb lattice an exact amplitude for loops is proved.
Readout method based on PCIe over optical fiber for CBM-TOF super module quality evaluation<|sep|>The Compressed Baryonic Matter (CBM) experiment will investigate the quantum chromodynamics (QCD) phase diagram at high net baryon densities and moderate temperatures. CBM Time of Flight (TOF) system is composed of super modules containing high performance Multi-gap Resistive Plate Chambers (MRPCs). During the mass production, each super module assembled with MRPCs needs quality evaluation, which includes time measurement and data readout. Read out electronics encounter the challenge of reading data from a super module at a speed of about 6 Gbps. In this paper, a read out method based on Peripheral Component Interconnect Express (PCIe) over optical fiber is proposed for CBM-TOF super module quality evaluation. The digitized data from super module will be concentrated at the front-end electronics, and then be transmitted to a PCIe switch module (PSM) over optical fiber using PCIe protocol. The PSM is directly plugged into the motherboard via gold fingers at the backend data acquisition server. With this readout method, a high-speed transmission rate can be reached. Furthermore, a PSM can receives data from several super modules simultaneously, which is important to improve the evaluation efficiency. This readout method simplifies the architecture of readout electronics and supports long distance transmission between frontend and backend.
Low-Energy Truly Random Number Generation with Superparamagnetic Tunnel Junctions for Unconventional Computing<|sep|>Low-energy random number generation is critical for many emerging computing schemes proposed to complement or replace von Neumann architectures. However, current random number generators are always associated with an energy cost that is prohibitive for these computing schemes. In this paper, we introduce random number bit generation based on specific nanodevices: superparamagnetic tunnel junctions. We experimentally demonstrate high quality random bit generation that represents orders-of-magnitude improvements in energy efficiency compared to current solutions. We show that the random generation speed improves with nanodevice scaling, and investigate the impact of temperature, magnetic field and crosstalk. Finally, we show how alternative computing schemes can be implemented using superparamagentic tunnel junctions as random number generators. These results open the way for fabricating efficient hardware computing devices leveraging stochasticity, and highlight a novel use for emerging nanodevices.
Effect of the interactions and environment on nuclear activity<|sep|>We present a study of the prevalence of optical and radio nuclear activity with respect to the environment and interactions in a sample of SDSS galaxies. We defined a local density parameter and a tidal forces estimator and used a cluster richness estimator from the literature. The possible correlations between these parameters were removed using a principal component analysis. We applied a stratified statistical method that takes into account the effect of possible confounding factors like the galaxy mass. We found that the prevalence of optical AGN is a factor 2-3 lower in the densest environments, but increases by a factor of ~2 in the presence of strong one-on-one interactions. The importance of galaxy interactions decreases from star-forming nuclei (SFN) to Seyferts to LINERs to passive galaxies, in accordance with previous suggestions of an evolutionary time-sequence. The fraction of radio AGN increases strongly towards denser environments, and is enhanced by galaxy interactions. Overall, the results agree with a scenario in which the mechanisms of accretion into the black hole are determined by the presence and nature of a supply of gas, which in turn is controlled by the local density of galaxies and their interactions. A plentiful cold gas supply is required to trigger SFN, optical AGN and radiatively-efficient radio AGN. This is less common in the cold-gas-poor environments of groups and clusters, but is enhanced by one-on-one interactions which result in the flow of gas into nuclear regions; these two factors compete against each other. In the denser environments where cold gas is rare, cooling hot gas can supply the nucleus at a sufficient rate to fuel low-luminosity radiatively-inefficient radio AGN. However, the increased prevalence of these AGN in interacting galaxies suggests that this is not the only mechanism by which radiatively-inefficient AGN can be triggered.
Applications of Multivariate Statistical Methods and Simulation Libraries to Analysis of Electron Backscatter Diffraction and Transmission Kikuchi Diffraction Datasets<|sep|>Multivariate statistical methods are widely used throughout the sciences, including microscopy, however, their utilisation for analysis of electron backscatter diffraction (EBSD) data has not been adequately explored. The basic aim of most EBSD analysis is to segment the spatial domain to reveal and quantify the microstructure, and links this to knowledge of the crystallography (eg crystal phase, orientation) within each segmented region. Two analysis strategies have been explored; principal component analysis (PCA) and k-means clustering. The intensity at individual (binned) pixels on the detector were used as the variables defining the multidimensional space in which each pattern in the map generates a single discrete point. PCA analysis alone did not work well but rotating factors to the VARIMAX solution did. K-means clustering also successfully segmented the data but was computational more expensive. The characteristic patterns produced by either VARIMAX or k-means clustering enhance weak patterns, remove pattern overlap, and allow subtle effects from polarity to be distinguished. Combining multivariate statistical analysis (MSA) approaches with template matching to simulation libraries can significantly reduce computational demand as the number of patterns to be matched is drastically reduced. Both template matching and MSA approaches may augment existing analysis methods but will not replace them in the majority of applications.
Decidable Inductive Invariants for Verification of Cryptographic Protocols with Unbounded Sessions<|sep|>We develop a theory of decidable inductive invariants for an infinite-state variant of the Applied pi-calculus, with applications to automatic verification of stateful cryptographic protocols with unbounded sessions/nonces. Since the problem is undecidable in general, we introduce depth-bounded protocols, a strict generalisation of a class from the literature, for which our decidable analysis is sound and complete. Our core contribution is a procedure to check that an invariant is inductive, which implies that every reachable configuration satisfies it. Our invariants can capture security properties like secrecy, can be inferred automatically, and represent an independently checkable certificate of correctness. We provide a prototype implementation and we report on its performance on some textbook examples.
Integer Quantum Hall Effect of Interacting Electrons in Graphene<|sep|>By taking into account the charge and spin orderings and the exchange interactions between all the Landau levels, we investigate the integer quantum Hall effect of electrons in graphene using the mean-field theory. At the fillings $\nu = 4n+2$ with $n = 0, 1, \cdots$, the system is in the high-symmetry state with the Landau levels four-fold degenerated. We show that with doping the degenerated lowest empty levels can be sequentially filled one level by one level, the filled level is lower than the empty ones because of the symmetry breaking. This result explains the step $\Delta\nu$ = 1 in the integer quantized Hall conductivity of the experimental observations. We also present in the supplemental material a high efficient method for dealing with huge number of the Coulomb couplings between all the levels.
The Effect of Sensor Fusion on Data-Driven Learning of Koopman Operators<|sep|>Dictionary methods for system identification typically rely on one set of measurements to learn governing dynamics of a system. In this paper, we investigate how fusion of output measurements with state measurements affects the dictionary selection process in Koopman operator learning problems. While prior methods use dynamical conjugacy to show a direct link between Koopman eigenfunctions in two distinct data spaces (measurement channels), we explore the specific case where output measurements are nonlinear, non-invertible functions of the system state. This setup reflects the measurement constraints of many classes of physical systems, e.g., biological measurement data, where one type of measurement does not directly transform to another. We propose output constrained Koopman operators (OC-KOs) as a new framework to fuse two measurement sets. We show that OC-KOs are effective for sensor fusion by proving that when learning a Koopman operator, output measurement functions serve to constrain the space of potential Koopman observables and their eigenfunctions. Further, low-dimensional output measurements can be embedded to inform selection of Koopman dictionary functions for high-dimensional models. We propose two algorithms to identify OC-KO representations directly from data: a direct optimization method that uses state and output data simultaneously and a sequential optimization method. We prove a theorem to show that the solution spaces of the two optimization problems are equivalent. We illustrate these findings with a theoretical example and two numerical simulations.
Effects of Coupling in Human-Virtual Agent Body Interaction<|sep|>This paper presents a study of the dynamic coupling between a user and a virtual character during body interaction. Coupling is directly linked with other dimensions, such as co-presence, engagement, and believability, and was measured in an experiment that allowed users to describe their subjective feelings about those dimensions of interest. The experiment was based on a theatrical game involving the imitation of slow upper-body movements and the proposal of new movements by the user and virtual agent. The agent's behaviour varied in autonomy: the agent could limit itself to imitating the user's movements only, initiate new movements, or combine both behaviours. After the game, each participant completed a questionnaire regarding their engagement in the interaction, their subjective feeling about the co-presence of the agent, etc. Based on four main dimensions of interest, we tested several hypotheses against our experimental results, which are discussed here.
Background-independent measurement of $\theta_{13}$ in Double Chooz<|sep|>The oscillation results published by the Double Chooz collaboration in 2011 and 2012 rely on background models substantiated by reactor-on data. In this analysis, we present a background-model-independent measurement of the mixing angle $\theta_{13}$ by including 7.53 days of reactor-off data. A global fit of the observed neutrino rates for different reactor power conditions is performed, yielding a measurement of both $\theta_{13}$ and the total background rate. The results on the mixing angle are improved significantly by including the reactor-off data in the fit, as it provides a direct measurement of the total background rate. This reactor rate modulation analysis considers antineutrino candidates with neutron captures on both Gd and H, whose combination yields $\sin^2(2\theta_{13})=$ 0.102 $\pm$ 0.028(stat.) $\pm$ 0.033(syst.). The results presented in this study are fully consistent with the ones already published by Double Chooz, achieving a competitive precision. They provide, for the first time, a determination of $\theta_{13}$ that does not depend on a background model.
Charge and spin transport through a ferromagnet/insulator/unconventional superconductor junction<|sep|>We analyze the charge and spin transport through a ballistic ferromagnet/insulator/superconductor junction by means of the Bogoliubov-de Gennes equations. For the ferromagnetic side we assume that ferromagnetism may be driven by an unequal mass renormalization of oppositely polarized carriers, i.e. a spin bandwidth asymmetry, and/or by a rigid splitting of up-and down-spin electron bands, as in a standard Stoner ferromagnet, whereas the superconducting side is assumed to exhibit a d-wave symmetry of the order parameter, which can be pure or accompanied by a minority component breaking time-reversal symmetry. Several remarkable features in the charge conductance arise in this kind of junction, providing useful information about the mechanism of ferromagnetism in the ferromagnetic electrode, as well as of the order parameter symmetry in the superconducting one. In particular, we show that when a time-reversal symmetry breaking superconductor is considered, the use of the two kinds of ferromagnet mentioned above represents a valuable tool to discriminate between the different superconducting mixed states. We also explain how this junction may mimic a switch able to turn on and off a spin current, leaving the charge conductance unchanged, and we show that for a wide range of insulating barrier strengths, a spin bandwidth asymmetry ferromagnet may support a spin current larger than a standard Stoner one.
Polarized proton spin filter for epithermal neutron based on dynamic nuclear polarization using photo-excited triplet electron spins<|sep|>For the polarization of neutrons with an energy level of \textcolor{black}{$>0.1$ eV}, we developed a novel polarized proton spin filter based on dynamic nuclear polarization using photo-excited triplet electron spins. The spin filter consists of a single crystal of naphthalene doped with deuterated pentacene and has a size of $\phi15\times4$ ${\rm mm}^3$, allowing it to cover a wide beam diameter. It was operated in 0.35 T and at 90 K. We succeeded in polarizing neutrons in the energy range $0.1-10$ eV using a RIKEN accelerator-driven compact neutron source. The averaged values of the proton and neutron polarization were $0.250\pm0.050$ and $0.076\pm0.015$, respectively.
Robust Discriminative Clustering with Sparse Regularizers<|sep|>Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from "noise-looking" variables. We cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem, (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions, (c) we propose a natural extension for the multi-label scenario, (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form $d=O(\sqrt{n})$ for the affine invariant case and $d=O(n)$ for the sparse case, where $n$ is the number of examples and $d$ the ambient dimension, and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to $O(nd^2)$, improving on earlier algorithms which had quadratic complexity in the number of examples.
Dynamic interventions with limited knowledge in network games<|sep|>This paper studies the problem of intervention design for steering the actions of noncooperative players in quadratic network games to the social optimum. The players choose their actions with the aim of maximizing their individual payoff functions, while a central regulator uses interventions to modify their marginal returns and maximize the social welfare function. This work builds on the key observation that the solution to the steering problem depends on the knowledge of the regulator on the players' parameters and the underlying network. We, therefore, consider different scenarios based on limited knowledge and propose suitable static, dynamic and adaptive intervention protocols. We formally prove convergence to the social optimum under the proposed mechanisms. We demonstrate our theoretical findings on a case study of Cournot competition with differentiated goods.
SO(10) inspired gauge-mediated supersymmetry breaking<|sep|>We consider a supersymmetric model motivated by a SO(10) grand unified theory: the gauge sector near the supersymmetry scale consists of SU(3)_c x SU(2)_L x U(1)_R x U(1)_{B-L}. We embed this model in minimal gauge mediation and incorporate neutrino data via an inverse seesaw mechanism. Also in this restricted model, the additional D terms can rise the light Higgs mass in a sizable way. Therefore, it is much easier to obtain m_h \simeq 125 GeV without the need to push the supersymmetry spectrum to extremely large values as it happens in models with minimal supersymmetric standard model particle content only. We show that this model predicts a diphoton rate of the Higgs equal to or smaller than the standard model expectation. We discuss briefly the collider phenomenology with a particular focus on the next to lightest supersymmetric particle in which this model offers the sneutrino as an additional possiblity. Moreover, we point out that, also in this model variant, supersymmetry can be discovered in Z' decays even in scenarios in which the strongly interacting particles are too heavy to be produced at a sizable rate at the LHC with 14 TeV. In addition, we show that lepton flavor violating observables constrain the size of the neutrino Yukawa couplings for which, in particular, muon decays and \mu-e conversion in heavy atoms are of particular importance. Once these constraints are fulfilled the rates for \tau decays are predicted to be below the reach of near-future experiments.
Bouncing and cyclic string gas cosmologies<|sep|>We show that, in the presence of a string gas, simple higher-derivative modifications to the effective action for gravity can lead to bouncing and cyclic cosmological models. The modifications bound the expansion rate and avoid singularities at finite times. In these models the scale factors can have long loitering phases that solve the horizon problem. Adding a potential for the dilaton gives a simple realization of the pre-big bang scenario. Entropy production in the cyclic phase drives an eventual transition to a radiation-dominated universe. As a test of the Brandenberger-Vafa scenario, we comment on the probability of decompactifying three spatial dimensions in this class of models.
Catalog of Gamma-ray Glows during Four Winter Seasons in Japan<|sep|>In 2015 the Gamma-Ray Observation of Winter Thunderstorms (GROWTH) collaboration launched a mapping observation campaign for high-energy atmospheric phenomena related to thunderstorms and lightning discharges. This campaign has developed a detection network of gamma rays with up to 10 radiation monitors installed in Kanazawa and Komatsu cities, Ishikawa Prefecture, Japan, where low-charge-center winter thunderstorms frequently occur. During four winter seasons from October 2016 to April 2020, in total 70 gamma-ray glows, minute-lasting bursts of gamma rays originating from thunderclouds, were detected. Their average duration is 58.9 sec. Among the detected events, 77% were observed in nighttime. The gamma-ray glows can be classified into temporally-symmetric, temporally-asymmetric, and lightning-terminated types based on their count-rate histories. An averaged energy spectrum of the gamma-ray glows is well fitted with a power-law function with an exponential cutoff, whose photon index, cutoff energy, and flux are $0.613\pm0.009$, $4.68\pm0.04$ MeV, and $(1.013\pm0.003)\times10^{-5}$ erg cm$^{-2}$ s$^{-1}$ (0.2-20.0MeV), respectively. The present paper provides the first catalog of gamma-ray glows and their statistical analysis detected during winter thunderstorms in the Kanazawa and Komatsu areas.
Shear-free Anisotropic Cosmological Models in f(R) Gravity<|sep|>We study a class of shear-free, homogeneous but anisotropic cosmological models with imperfect matter sources in the context of f(R) gravity. We show that the anisotropic stresses are related to the electric part of the Weyl tensor in such a way that they balance each other. We also show that within the class of orthogonal f(R) models, small perturbations of shear are damped, and that the electric part of the Weyl tensor and the anisotropic stress tensor decay with the expansion as well as the heat flux of the curvature fluid. Specializing in locally rotationally symmetric spacetimes in orthonormal frames, we examine the late-time behaviour of the de Sitter universe in $f(R)$ gravity. For the Starobinsky model of f(R), we study the evolutionary behavior of the Universe by numerically integrating the Friedmann equation, where the initial conditions for the expansion, acceleration and jerk parameters are taken from observational data.
A Comparison of LSTMs and Attention Mechanisms for Forecasting Financial Time Series<|sep|>While LSTMs show increasingly promising results for forecasting Financial Time Series (FTS), this paper seeks to assess if attention mechanisms can further improve performance. The hypothesis is that attention can help prevent long-term dependencies experienced by LSTM models. To test this hypothesis, the main contribution of this paper is the implementation of an LSTM with attention. Both the benchmark LSTM and the LSTM with attention were compared and both achieved reasonable performances of up to 60% on five stocks from Kaggle's Two Sigma dataset. This comparative analysis demonstrates that an LSTM with attention can indeed outperform standalone LSTMs but further investigation is required as issues do arise with such model architectures.
Localized linear polynomial operators and quadrature formulas on the sphere<|sep|>The purpose of this paper is to construct universal, auto--adaptive, localized, linear, polynomial (-valued) operators based on scattered data on the (hyper--)sphere $\SS^q$ ($q\ge 2$). The approximation and localization properties of our operators are studied theoretically in deterministic as well as probabilistic settings. Numerical experiments are presented to demonstrate their superiority over traditional least squares and discrete Fourier projection polynomial approximations. An essential ingredient in our construction is the construction of quadrature formulas based on scattered data, exact for integrating spherical polynomials of (moderately) high degree. Our formulas are based on scattered sites; i.e., in contrast to such well known formulas as Driscoll--Healy formulas, we need not choose the location of the sites in any particular manner. While the previous attempts to construct such formulas have yielded formulas exact for spherical polynomials of degree at most 18, we are able to construct formulas exact for spherical polynomials of degree 178.
Robust design of Si/Si3N4 high contrast grating mirror for mid-infrared VCSEL application<|sep|>A Si/Si3N4 high contrast grating mirror has been designed for a VCSEL integration in mid-infrared ({\lambda} = 2.65 $\mu$m). The use of an optimization algorithm which maximizes a VCSEL mirror quality factor allowed the adjustment of the grating parameters while keeping large and shallow grating pattern. The robustness with respect to fabrication error has been enhanced thanks to a precise study of the grating dimension tolerances. The final mirror exhibits large high reflectivity bandwidth with a polarization selectivity and several percent of tolerance on the grating dimensions.
BUZz: BUffer Zones for defending adversarial examples in image classification<|sep|>We propose a novel defense against all existing gradient based adversarial attacks on deep neural networks for image classification problems. Our defense is based on a combination of deep neural networks and simple image transformations. While straightforward in implementation, this defense yields a unique security property which we term buffer zones. We argue that our defense based on buffer zones offers significant improvements over state-of-the-art defenses. We are able to achieve this improvement even when the adversary has access to the {\em entire} original training data set and unlimited query access to the defense. We verify our claim through experimentation using Fashion-MNIST and CIFAR-10: We demonstrate $<11\%$ attack success rate -- significantly lower than what other well-known state-of-the-art defenses offer -- at only a price of a $11-18\%$ drop in clean accuracy. By using a new intuitive metric, we explain why this trade-off offers a significant improvement over prior work.
The International-Migration Network<|sep|>This paper studies international migration from a complex-network perspective. We define the international-migration network (IMN) as the weighted-directed graph where nodes are world countries and links account for the stock of migrants originated in a given country and living in another country at a given point in time. We characterize the binary and weighted architecture of the network and its evolution over time in the period 1960-2000. We find that the IMN is organized around a modular structure characterized by a small-world pattern displaying disassortativity and high clustering, with power-law distributed weighted-network statistics. We also show that a parsimonious gravity model of migration can account for most of observed IMN topological structure. Overall, our results suggest that socio-economic, geographical and political factors are more important than local-network properties in shaping the structure of the IMN.
Unidentified Features in the Ultraviolet Spectrum of X Per<|sep|>High-resolution ultraviolet spectra from the Space Telescope Imaging Spectrograph (STIS) were used to search for unidentified interstellar absorption features in the well studied sightline towards X Per (HD 24534). The significance of features detected was determined from Gaussian fits to the data, as well as the features' persistence in multiple observations. Fixed pattern noise characteristics were studied in STIS echelle data to distinguish between interstellar and instrumental features. We report the detection of two unidentified features that stand out from the more common fixed pattern noise features. Both features have depths of > 3% of the continuum level making them very likely of interstellar origin. Lastly, we comment on possible carriers, and discuss future prospects for studying these and perhaps other unidentified lines in larger samples of sightlines.
$\bar D^*D^*_0$ and $\bar B^*B^*_0 (1^--)$ molecules at N2LO from QSSR<|sep|>We estimate the $\bar D^*D^*_0$ and $\bar B^*B^*_0(1^--)$ molecules masses and couplings using QCD spectral sum rules (QSSR)known perturbatively to N2LO of PT series and including the contributions of non-perturabtive condensates up to the dimension-eight. Our results improve earlier LO results obtained from QSSR in the current literature. We obtain $M_{D^*D^*_0} = 5244(228)$ MeV which is heavier than the experimental candidates Y(4260); Y(4360); Y(4660) suggesting that they cannot be pure molecule states. We predict $M_{B^*B^*_0}= 11920(159)$ MeV to be tested in B-factory experiments.
Constraining the radio jet proper motion of the high-redshift quasar J2134-0419 at z=4.3<|sep|>To date, PMN J2134-0419 (at a redshift z=4.33) is the second most distant quasar known with a milliarcsecond-scale morphology permitting direct estimates of the jet proper motion. Based on two-epoch observations, we constrained its radio jet proper motion using the very long baseline interferometry (VLBI) technique. The observations were conducted with the European VLBI Network (EVN) at 5 GHz on 1999 November 26 and 2015 October 6. We imaged the central 10-pc scale radio jet emission and modeled its brightness distribution. By identifying a jet component at both epochs separated by 15.86 yr, a proper motion of mu=0.035 +- 0.023 mas/yr is found. It corresponds to an apparent superluminal speed of beta_a=4.1 +- 2.7 c . Relativistic beaming at both epochs suggests that the jet viewing angle with respect to the line of sight is smaller than 20 deg, with a minimum bulk Lorentz factor Gamma=4.3. The small value of the proper motion is in good agreement with the expectations from the cosmological interpretation of the redshift and the current cosmological model. Additionally we analyzed archival Very Large Array observations of J2143-0419 and found indication of a bent jet extending to ~30 kpc.
Seeing Behind Things: Extending Semantic Segmentation to Occluded Regions<|sep|>Semantic segmentation and instance level segmentation made substantial progress in recent years due to the emergence of deep neural networks (DNNs). A number of deep architectures with Convolution Neural Networks (CNNs) were proposed that surpass the traditional machine learning approaches for segmentation by a large margin. These architectures predict the directly observable semantic category of each pixel by usually optimizing a cross entropy loss. In this work we push the limit of semantic segmentation towards predicting semantic labels of directly visible as well as occluded objects or objects parts, where the network's input is a single depth image. We group the semantic categories into one background and multiple foreground object groups, and we propose a modification of the standard cross-entropy loss to cope with the settings. In our experiments we demonstrate that a CNN trained by minimizing the proposed loss is able to predict semantic categories for visible and occluded object parts without requiring to increase the network size (compared to a standard segmentation task). The results are validated on a newly generated dataset (augmented from SUNCG) dataset.
Electron Localization and Energy Levels' Oscillations Induced by Controlled Deformation<|sep|>Manipulating energy levels while controlling the electron localization is an essential step for many applications of confined systems. In this paper we demonstrate how to achieve electron localization and induce energy level oscillation in one-dimensional quantum systems by externally controlling the deformation of the system. From a practical point of view, the one-dimensional potentials can be realized using layered structures. In the analysis, we considered three different examples. The first one is a graded quantum well between confining infinite walls where the deformation is modeled by varying slightly the graded well. The second systems is a symmetric multiple quantum well between infinite walls under the effect of biasing voltage. The third system is a layered 2D hybrid perovskites where pressure is used to induce deformation. The calculations are conducted both numerically and analytically using the perturbation theory. It is shown that the obtained oscillations are associated with level avoided crossings and that the deformation results in changing the spatial localization of the electrons.
RealSmileNet: A Deep End-To-End Network for Spontaneous and Posed Smile Recognition<|sep|>Smiles play a vital role in the understanding of social interactions within different communities, and reveal the physical state of mind of people in both real and deceptive ways. Several methods have been proposed to recognize spontaneous and posed smiles. All follow a feature-engineering based pipeline requiring costly pre-processing steps such as manual annotation of face landmarks, tracking, segmentation of smile phases, and hand-crafted features. The resulting computation is expensive, and strongly dependent on pre-processing steps. We investigate an end-to-end deep learning model to address these problems, the first end-to-end model for spontaneous and posed smile recognition. Our fully automated model is fast and learns the feature extraction processes by training a series of convolution and ConvLSTM layer from scratch. Our experiments on four datasets demonstrate the robustness and generalization of the proposed model by achieving state-of-the-art performances.
NF is Consistent<|sep|>In this paper we will present a proof of the consistency of Quine's set theory "New Foundations" (hereinafter NF), so-called after the title of the 1937 paper in which it was introduced. This version takes the approach of building a model of tangled type theory rather than a model of the usual set theory without choice with a tangled web of cardinals.
Skyrmions as Compact, Robust and Energy-Efficient Interconnects for Domain Wall (DW)-based Systems<|sep|>Magnetic domain-wall (DW) has been widely investigated for future memory and computing systems. However, energy efficiency and stability become two major challenges of DW-based systems. In this letter, we first propose exploiting skyrmions as on-chip and inter-chip interconnects for DW-based systems, owing to the topological stability, small size and ultra-low depinning current density. In the proposed technique, data are processed in the form of DWs but are transmitted instead in the form of skyrmions. The reversible conversion between a skyrmion and a DW pair can be physically achieved by connecting a wide and a narrow magnetic nanowire. Our proposed technique can realize highly compact, robust and energy-efficient on-chip and inter-chip interconnects for DW-based systems, enabling the system to take advantages of both the DW and skyrmion.
On Measuring Social Biases in Prompt-Based Multi-Task Learning<|sep|>Large language models trained on a mixture of NLP tasks that are converted into a text-to-text format using prompts, can generalize into novel forms of language and handle novel tasks. A large body of work within prompt engineering attempts to understand the effects of input forms and prompts in achieving superior performance. We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs. In this paper, we study T0, a large-scale multi-task text-to-text language model trained using prompt-based learning. We consider two different forms of semantically equivalent inputs: question-answer format and premise-hypothesis format. We use an existing bias benchmark for the former BBQ and create the first bias benchmark in natural language inference BBNLI with hand-written hypotheses while also converting each benchmark into the other form. The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training, compared to premise-hypothesis form which is unlike its training examples. Code and data are released under https://github.com/feyzaakyurek/bbnli.
Achieving Shrinkage in a Time-Varying Parameter Model Framework<|sep|>Shrinkage for time-varying parameter (TVP) models is investigated within a Bayesian framework, with the aim to automatically reduce time-varying parameters to static ones, if the model is overfitting. This is achieved through placing the double gamma shrinkage prior on the process variances. An efficient Markov chain Monte Carlo scheme is developed, exploiting boosting based on the ancillarity-sufficiency interweaving strategy. The method is applicable both to TVP models for univariate as well as multivariate time series. Applications include a TVP generalized Phillips curve for EU area inflation modelling and a multivariate TVP Cholesky stochastic volatility model for joint modelling of the returns from the DAX-30 index.
Searching for Ultra-fast Outflows in AGN using Variability Spectra<|sep|>We present a qualitative search for ultra-fast outflows (UFOs) in excess variance spectra of radio-quiet active galactic nuclei (AGN). We analyse 42 sources from the Tombesi et al. (2010) spectroscopic UFO detection sample, and an additional 22 different sources from the Kara et al. (2016) variability sample. A total of 58 sources have sufficient observational data from XMM-Newton EPIC-pn and variability for an excess variance spectrum to be calculated. We examine these spectra for peaks corresponding to variable blue-shifted H- and He-like ion absorption lines from UFOs. We find good evidence for such outflows in 28% of the AGN sample and weak evidence in a further 31%, meaning that $\sim$ 30-60% of the AGN sample hosts such UFOs. The mean and median blue-shifted velocity is found to be $\sim$ 0.14c and 0.12c, respectively. Current variability methods allow for a fast, model-independent determination of UFOs, however, further work needs to be undertaken to better characterize the statistical significance of the peaks in these spectra by more rigorous modelling. Detecting good evidence for variable UFO lines in a large number of sources also lays the groundwork for detailed analysis of the variability timescales of the absorbers. This will allow us to probe their densities and hence distances from the central super-massive black hole.
Screening of a Luttinger liquid wire by a scanning tunneling microscope tip: II. Transport properties<|sep|>We study the effect of an electrostatic coupling between a scanning tunneling microscope tip and a Luttinger liquid wire on the tunneling current and noise between the two. Solving the Dyson equations non perturbatively for a local interaction potential, we derive the Green's functions associated to the wire and to the tip. Interestingly, the electrostatic coupling leads to the existence of new correlators, which we call mixed Green's functions, which are correlators between the bosonic fields of the wire and the tip. Next, we calculate the transport properties up to second order with the amplitude of the tunnel transfer: the tunnel current is strongly reduced by the presence of screening. The zero-frequency noise is modified in a similar way, but the Fano factor remains unchanged. We also consider the effect of the screening on the asymmetry of the finite-frequency non-symmetrized noise and on the conductance.
Algebraic study of receptor-ligand systems: a dose-response analysis<|sep|>The study of a receptor-ligand system generally relies on the analysis of its dose-response (or concentration-effect) curve, which quantifies the relation between ligand concentration and the biological effect (or cellular response) induced when binding its specific cell surface receptor. Mathematical models of receptor-ligand systems have been developed to compute a dose-response curve under the assumption that the biological effect is proportional to the number of ligand-bound receptors. Given a dose-response curve, two quantities (or metrics) have been defined to characterise the properties of the ligand-receptor system under consideration: amplitude and potency (or half-maximal effective concentration, and denoted by EC$_{50}$). Both the amplitude and the EC$_{50}$ are key quantities commonly used in pharmaco-dynamic modelling, yet a comprehensive mathematical investigation of the behaviour of these two metrics is still outstanding; for a large (and important) family of receptors, called cytokine receptors, we still do not know how amplitude and EC$_{50}$ depend on receptor copy numbers. Here we make use of algebraic approaches (Gr\"obner basis) to study these metrics for a large class of receptor-ligand models, with a focus on cytokine receptors. In particular, we introduce a method, making use of two motivating examples based on the interleukin-7 (IL-7) receptor, to compute analytic expressions for the amplitude and the EC$_{50}$. We then extend the method to a wider class of receptor-ligand systems, sequential receptor-ligand systems with extrinsic kinase, and provide some examples.
Variational Gaussian Process Auto-Encoder for Ordinal Prediction of Facial Action Units<|sep|>We address the task of simultaneous feature fusion and modeling of discrete ordinal outputs. We propose a novel Gaussian process(GP) auto-encoder modeling approach. In particular, we introduce GP encoders to project multiple observed features onto a latent space, while GP decoders are responsible for reconstructing the original features. Inference is performed in a novel variational framework, where the recovered latent representations are further constrained by the ordinal output labels. In this way, we seamlessly integrate the ordinal structure in the learned manifold, while attaining robust fusion of the input features. We demonstrate the representation abilities of our model on benchmark datasets from machine learning and affect analysis. We further evaluate the model on the tasks of feature fusion and joint ordinal prediction of facial action units. Our experiments demonstrate the benefits of the proposed approach compared to the state of the art.
The role of low-mass star clusters in forming the massive stars in DR 21<|sep|>We have studied the young low-mass pre-main sequence (PMS) stellar population associated with the massive star-forming region DR 21 by using archival X-ray Chandra observations and by complementing them with existing optical and IR surveys. The Chandra observations have revealed for the first time a new highly extincted population of PMS low-mass stars previously missed in observations at other wavelengths. The X-ray population exhibits three main stellar density peaks, coincident with the massive star-forming regions, being the DR 21 core the main peak. The cross-correlated X-ray/IR sample exhibits a radial "Spokes-like" stellar filamentary structure that extends from the DR 21 core towards the northeast. The near IR data reveal a centrally peaked structure for the extinction, which exhibits its maximum in the DR 21 core and gradually decreases with the distance to the N-S cloud axis and to the cluster center. We find evidence of a global mass segregation in the full low-mass stellar cluster, and of an stellar age segregation, with the youngest stars still embedded in the N-S cloud, and more evolved stars more spatially distributed. The results are consistent with the scenario where an elongated overall potential well created by the full low-mass stellar cluster funnels gas through filaments feeding stellar formation. Besides the full gravitational well, smaller-scale local potential wells created by dense stellar sub-clusters of low-mass stars are privileged in the competition for the gas of the common reservoir, allowing the formation of massive stars. We also discuss the possibility that a stellar collision in the very dense stellar cluster revealed by Chandra in the DR 21 core is the origin of the large-scale and highly-energetic outflow arising from this region.
Quantum Monte Carlo calculations of magnetic moments and M1 transitions in A<=7 nuclei including meson-exchange currents<|sep|>Green's function Monte Carlo calculations of magnetic moments and M1 transitions including two-body meson-exchange current (MEC) contributions are reported for A<=7 nuclei. The realistic Argonne v18 two-nucleon and Illinois-2 three-nucleon potentials are used to generate the nuclear wave functions. The two-body meson-exchange operators are constructed to satisfy the continuity equation with the Argonne v18 potential. The MEC contributions increase the A=3,7 isovector magnetic moments by 16% and the A=6,7 M1 transition rates by 17--34%, bringing them into very good agreement with the experimental data.
Interactive Search and Exploration in Online Discussion Forums Using Multimodal Embeddings<|sep|>In this paper we present a novel interactive multimodal learning system, which facilitates search and exploration in large networks of social multimedia users. It allows the analyst to identify and select users of interest, and to find similar users in an interactive learning setting. Our approach is based on novel multimodal representations of users, words and concepts, which we simultaneously learn by deploying a general-purpose neural embedding model. We show these representations to be useful not only for categorizing users, but also for automatically generating user and community profiles. Inspired by traditional summarization approaches, we create the profiles by selecting diverse and representative content from all available modalities, i.e. the text, image and user modality. The usefulness of the approach is evaluated using artificial actors, which simulate user behavior in a relevance feedback scenario. Multiple experiments were conducted in order to evaluate the quality of our multimodal representations, to compare different embedding strategies, and to determine the importance of different modalities. We demonstrate the capabilities of the proposed approach on two different multimedia collections originating from the violent online extremism forum Stormfront and the microblogging platform Twitter, which are particularly interesting due to the high semantic level of the discussions they feature.
Combining Similarity and Adversarial Learning to Generate Visual Explanation: Application to Medical Image Classification<|sep|>Explaining decisions of black-box classifiers is paramount in sensitive domains such as medical imaging since clinicians confidence is necessary for adoption. Various explanation approaches have been proposed, among which perturbation based approaches are very promising. Within this class of methods, we leverage a learning framework to produce our visual explanations method. From a given classifier, we train two generators to produce from an input image the so called similar and adversarial images. The similar image shall be classified as the input image whereas the adversarial shall not. Visual explanation is built as the difference between these two generated images. Using metrics from the literature, our method outperforms state-of-the-art approaches. The proposed approach is model-agnostic and has a low computation burden at prediction time. Thus, it is adapted for real-time systems. Finally, we show that random geometric augmentations applied to the original image play a regularization role that improves several previously proposed explanation methods. We validate our approach on a large chest X-ray database.
Angles on CP-violation in Higgs boson interactions<|sep|>CP-violation in the Higgs sector remains a possible source of the baryon asymmetry of the universe. Recent differential measurements of signed angular distributions in Higgs boson production provide a general experimental probe of the CP structure of Higgs boson interactions. We interpret these measurements using the Standard Model Effective Field Theory and show that they do not distinguish the various CP-violating operators that couple the Higgs and gauge fields. However, the constraints can be sharpened by measuring additional CP-sensitive observables and exploiting phase-space-dependent effects. Using these observables, we demonstrate that perturbatively meaningful constraints on CP-violating operators can be obtained at the LHC with luminosities of ${\cal{O}}$(100/fb). Our results provide a roadmap to a global Higgs boson coupling analysis that includes CP-violating effects.
Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation<|sep|>We propose a two-stage training approach for developing a single NMT model to translate unseen languages both to and from English. For the first stage, we initialize an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform multilingual fine-tuning on parallel data in 25 languages to English. We find this model can generalize to zero-shot translations on unseen languages. For the second stage, we leverage this generalization ability to generate synthetic parallel data from monolingual datasets, then train with successive rounds of back-translation. The final model extends to the English-to-Many direction, while retaining Many-to-English performance. We term our approach EcXTra (English-centric Crosslingual (X) Transfer). Our approach sequentially leverages auxiliary parallel data and monolingual data, and is conceptually simple, only using a standard cross-entropy objective in both stages. The final EcXTra model is evaluated on unsupervised NMT on 8 low-resource languages achieving a new state-of-the-art for English-to-Kazakh (22.3 > 10.4 BLEU), and competitive performance for the other 15 translation directions.
The gauge transformation of the constrained semi-discrete KP hierarchy<|sep|>In this paper, the gauge transformation of the constrained semi-discrete KP(cdKP) hierarchy is constructed explicitly by the suitable choice of the generating functions. Under the $m$-step successive gauge transformation $T_m$, we give the transformed (adjoint) eigenfunctions and the $\tau$-function of the transformed Lax operator of the cdKP hierarchy.
Algebraic Multilevel Preconditioning in Isogeometric Analysis: Construction and Numerical Studies<|sep|>We present algebraic multilevel iteration (AMLI) methods for isogeometric discretization of scalar second order elliptic problems. The construction of coarse grid operators and hierarchical complementary operators are given. Moreover, for a uniform mesh on a unit interval, the explicit representation of B-spline basis functions for a fixed mesh size $h$ is given for $p=2,3,4$ and for $C^{0}$- and $C^{p-1}$-continuity. The presented methods show $h$- and (almost) $p$-independent convergence rates. Supporting numerical results for convergence factor and iterations count for AMLI cycles ($V$-, linear $W$-, nonlinear $W$-) are provided. Numerical tests are performed, in two-dimensions on square domain and quarter annulus, and in three-dimensions on quarter thick ring.
Dimensional analysis in relativity and in differential geometry<|sep|>This note provides a short guide to dimensional analysis in Lorentzian and general relativity and in differential geometry. It tries to revive Dorgelo and Schouten's notion of 'intrinsic' or 'absolute' dimension of a tensorial quantity. The intrinsic dimension is independent of the dimensions of the coordinates and expresses the physical and operational meaning of a tensor. The dimensional analysis of several important tensors and tensor operations is summarized. In particular it is shown that the components of a tensor need not have all the same dimension, and that the Riemann (once contravariant and thrice covariant), Ricci (twice covariant), and Einstein (twice covariant) curvature tensors are dimensionless. The relation between dimension and operational meaning for the metric and stress-energy-momentum tensors is discussed; and the possible conventions for the dimensions of these two tensors and of Einstein's constant $\kappa$, including the curious possibility $\kappa = 8\pi G$ without $c$ factors, are reviewed.
Modeling HI distribution and kinematics in the edge-on dwarf irregular galaxy KK250<|sep|>We model the observed vertical distribution of the neutral hydrogen (HI) in the faint (M_B ~ -13.7 mag) edge-on dwarf irregular galaxy KK250. Our model assumes that the galaxy consists of axi-symmetric, co-planar gas and stellar discs in the external force-field of a spherical dark matter halo, and in vertical hydrostatic equilibrium. The velocity dispersion of the gas is left as a free parameter in the model. Our best fit model is able to reproduce the observed vertical distribution of the HI gas, as well as the observed velocity profiles. The best fit model has a large velocity dispersion (~ 22 km/s) at the centre of the galaxy, which falls to a value of ~ 8 km/s by a galacto-centric radius of ~ 1 kpc, which is similar to both the scale-length of the stellar disc, as well as the angular resolution of the data along the radial direction. Similarly we find that the thickness of the HI disc is also minimum at ~ 1 kpc, and increases by about a factor of ~ 2 as one goes to the centre of the galaxy or out to ~ 3 kpc. The minimum intrinsic HWHM of the HI vertical distribution in KK250 is ~ 350 pc. For comparison the HWHM of the vertical distribution of the HI in the solar neighbourhood is ~ 70-140 pc. Our results are hence consistent with other observations which indicate that dwarf galaxies have significantly puffier gas discs than spirals.
An XMM-Newton View of the Radio Galaxy 3C 411<|sep|>We present the first high signal-to-noise XMM-Newton observations of the broad-line radio galaxy 3C 411. After fitting various spectral models, an absorbed double power-law continuum and a blurred relativistic disk reflection model (kdblur) are found to be equally plausible descriptions of the data. While the softer power-law component ($\Gamma$=2.11) of the double power-law model is entirely consistent with that found in Seyfert galaxies (and hence likely originates from a disk corona), the additional power law component is very hard ($\Gamma$=1.05); amongst the AGN zoo, only flat-spectrum radio quasars have such hard spectra. Together with the very flat radio-spectrum displayed by this source, we suggest that it should instead be classified as a FSRQ. This leads to potential discrepancies regarding the jet inclination angle, with the radio morphology suggesting a large jet inclination but the FSRQ classification suggesting small inclinations. The kdblur model predicts an inner disk radius of at most 20 r$_g$ and relativistic reflection.
Search for neutral Higgs bosons decaying into four taus at LEP2<|sep|>A search for the production and non-standard decay of a Higgs boson, h, into four taus through intermediate pseudoscalars, a, is conducted on 683 pb-1 of data collected by the ALEPH experiment at centre-of-mass energies from 183 to 209 GeV. No excess of events above background is observed, and exclusion limits are placed on the combined production cross section times branching ratio, \xi^2 = \sigma(e+e- --> Zh)/\sigma_{SM}(e+e- --> Zh) x B(h --> aa)x B(a --> \tau^+\tau^-)^2. For mh < 107 GeV/c2 and 4 < ma < 10 GeV/c2, \xi^2 > 1 is excluded at the 95% confidence level.
Leveraging percolation theory to single out influential spreaders in networks<|sep|>Among the consequences of the disordered interaction topology underlying many social, techno- logical and biological systems, a particularly important one is that some nodes, just because of their position in the network, may have a disproportionate effect on dynamical processes mediated by the complex interaction pattern. For example, the early adoption by an opinion leader in a social network may change the fate of a commercial product, or just a few super-spreaders may determine the virality of a meme in social media. Despite many recent efforts, the formulation of an accurate method to optimally identify influential nodes in complex network topologies remains an unsolved challenge. Here, we present the exact solution of the problem for the specific, but highly relevant, case of the Susceptible-Infected-Removed (SIR) model for epidemic spreading at criticality. By exploiting the mapping between bond percolation and the static properties of SIR, we prove that the recently introduced Non-Backtracking centrality is the optimal criterion for the identification of influential spreaders in locally tree-like networks at criticality. By means of simulations on synthetic networks and on a very extensive set of real-world networks, we show that the Non-Backtracking centrality is a highly reliable metric to identify top influential spreaders also in generic graphs not embedded in space, and for noncritical spreading.
The complete census of optically selected AGNs in the Coma Supercluster: the dependence of AGN activity on the local environment<|sep|>To investigate the dependence of the occurrence of active galactic nuclei (AGNs) on local galaxy density, we study the nuclear properties of ~5000 galaxies in the Coma Supercluster whose density spans 2 orders of magnitude from the sparse filaments to the cores of the rich clusters. We obtained optical spectra of the nuclei of 177 galaxies using the 1.5m Cassini telescope, which are added to the 4785 spectra available from SDSS (DR7) to fill-in the incomplete coverage by SDSS of luminous galaxies. We perform a spectral classification of the nuclei of galaxies (with a completeness of 98% at r<17.77), classifying the nuclear spectra in six classes: three of them (SEY, sAGN, LIN) refer to AGNs and the remaining three (HII, RET, PAS) refer to different stages of starburst activity. To perform such classification, we use the WHAN diagnostic, after correcting Halpha by 1.3 A for underlying absorption. We find that 482 (10%) of 5027 galaxies host an AGN: their frequency strongly increases with increasing luminosity of the parent galaxies, such that 32% of galaxies with Log(i-Lum)<10.2 (Solar) harbor an AGN at their interior. In addition to their presence in luminous galaxies, AGNs are also found in red galaxies with <g-i> < 1.15 \pm 0.15 mag. The majority of SEY and sAGN (strong AGNs) are associated with luminous late-type (or S0a) galaxies, while LIN (weak AGNs) and RET ("retired"), are mostly found among E/S0as. The number density of AGNs, HII region-like, and retired galaxies is found to anti-correlate with the local density of galaxies, such that their frequency drops by a factor of two near the cluster cores, while the frequency of galaxies containing passive nuclei increases by the same amount towards the center of rich clusters. The dependence of AGN number density on the local galaxy density is greater than the one implied by morphology segregation alone.
Anomaly Detection for High-Dimensional Data Using Large Deviations Principle<|sep|>Most current anomaly detection methods suffer from the curse of dimensionality when dealing with high-dimensional data. We propose an anomaly detection algorithm that can scale to high-dimensional data using concepts from the theory of large deviations. The proposed Large Deviations Anomaly Detection (LAD) algorithm is shown to outperform state of art anomaly detection methods on a variety of large and high-dimensional benchmark data sets. Exploiting the ability of the algorithm to scale to high-dimensional data, we propose an online anomaly detection method to identify anomalies in a collection of multivariate time series. We demonstrate the applicability of the online algorithm in identifying counties in the United States with anomalous trends in terms of COVID-19 related cases and deaths. Several of the identified anomalous counties correlate with counties with documented poor response to the COVID pandemic.
Planetesimal-driven planet migration in the presence of a gas disk<|sep|>We report here on an extension of a previous study by Kirsh et al. (2009) of planetesimal-driven migration using our N-body code SyMBA (Duncan et al., 1998). The previous work focused on the case of a single planet of mass Mem, immersed in a planetesimal disk with a power-law surface density distribution and Rayleigh distributed eccentricities and inclinations. Typically 10^4-10^5 equal-mass planetesimals were used, where the gravitational force (and the back-reaction) on each planetesimal by the Sun and planetwere included, while planetesimal-planetesimal interactions were neglected. The runs reported on here incorporate the dynamical effects of a gas disk, where the Adachi et al. (1976) prescription of aerodynamic gas drag is implemented for all bodies. In some cases the Papaloizou and Larwood (2000) prescription of Type-I migration for the planet are implemented, as well as a mass distribution. In the gas-free cases, rapid planet migration was observed - at a rate independent of the planet's mass - provided the planet's mass was not large compared to the mass in planetesimals capable of entering its Hill sphere. In such cases, both inward and outward migrations can be self-sustaining, but there is a strong propensity for inward migration. When a gas disk is present, aerodynamic drag can substantially modify the dynamics of scattered planetesimals. For sufficiently large or small mono-dispersed planetesimals, the planet typically migrates inward. However, for a range of plausible planetesimal sizes (i.e. 0.5-5.0 km at 5.0 AU in a minimum mass Hayashi disk) outward migration is usually triggered, often accompanied by substantial planetary mass accretion. The origins of this behaviour are explained in terms of a toy model. The effects of including a size distribution and torques associated with Type-I migration are also discussed.
Turbulent magnetic fields in the quiet Sun: implications of Hinode observations and small-scale dynamo simulations<|sep|>Using turbulent MHD simulations (magnetic Reynolds numbers up to 8000) and Hinode observations, we study effects of turbulence on measuring the solar magnetic field outside active regions. Firstly, from synthetic Stokes V profiles for the FeI lines at 630.1 and 630.2 nm, we show that a peaked probability distribution function (PDF) for observationally-derived field estimates is consistent with a monotonic PDF for actual vertical field strengths. Hence, the prevalence of weak fields is greater than would be naively inferred from observations. Secondly, we employ the fractal self-similar geometry of the turbulent solar magnetic field to derive two estimates (numerical and observational) of the true mean vertical unsigned flux density. We also find observational evidence that the scales of magnetic structuring in the photosphere extend at least down to an order of magnitude smaller than 200 km: the self-similar power-law scaling in the signed measure from a Hinode magnetogram ranges (over two decades in length scales and including the granulation scale) down to the 200 km resolution limit. From the self-similar scaling, we determine a lower bound for the true quiet-Sun mean vertical unsigned flux density of ~50 G. This is consistent with our numerically-based estimates that 80% or more of the vertical unsigned flux should be invisible to Stokes-V observations at a resolution of 200 km owing to the cancellation of signal from opposite magnetic polarities. Our estimates significantly reduce the order-of-magnitude discrepancy between Zeeman- and Hanle-based estimates.
Brightest Cluster Galaxies in Cosmological Simulations with Adaptive Mesh Refinement: Successes and Failures<|sep|>A large sample of cosmological hydrodynamical zoom-in simulations with Adaptive Mesh Refinement (AMR) is analysed to study the properties of simulated Brightest Cluster Galaxies (BCGs). Following the formation and evolution of BCGs requires modeling an entire galaxy cluster, because the BCG properties are largely influenced by the state of the gas in the cluster and by interactions and mergers with satellites. BCG evolution is also deeply influenced by the presence of gas heating sources such as Active Galactic Nuclei (AGNs) that prevent catastrophic cooling of large amounts of gas. We show that AGN feedback is one of the most important mechanisms in shaping the properties of BCGs at low redshift by analysing our statistical sample of simulations with and without AGN feedback. When AGN feedback is included BCG masses, sizes, star formation rates and kinematic properties are closer to those of the observed systems. Some small discrepancies are observed only for the most massive BCGs and in the fraction of star-forming BCGs, effects that might be due to physical processes that are not included in our model.
NanoNet: Real-Time Polyp Segmentation in Video Capsule Endoscopy and Colonoscopy<|sep|>Deep learning in gastrointestinal endoscopy can assist to improve clinical performance and be helpful to assess lesions more accurately. To this extent, semantic segmentation methods that can perform automated real-time delineation of a region-of-interest, e.g., boundary identification of cancer or precancerous lesions, can benefit both diagnosis and interventions. However, accurate and real-time segmentation of endoscopic images is extremely challenging due to its high operator dependence and high-definition image quality. To utilize automated methods in clinical settings, it is crucial to design lightweight models with low latency such that they can be integrated with low-end endoscope hardware devices. In this work, we propose NanoNet, a novel architecture for the segmentation of video capsule endoscopy and colonoscopy images. Our proposed architecture allows real-time performance and has higher segmentation accuracy compared to other more complex ones. We use video capsule endoscopy and standard colonoscopy datasets with polyps, and a dataset consisting of endoscopy biopsies and surgical instruments, to evaluate the effectiveness of our approach. Our experiments demonstrate the increased performance of our architecture in terms of a trade-off between model complexity, speed, model parameters, and metric performances. Moreover, the resulting model size is relatively tiny, with only nearly 36,000 parameters compared to traditional deep learning approaches having millions of parameters.
Probing quantum scars and weak ergodicity-breaking through quantum complexity<|sep|>Scar states are special many-body eigenstates that weakly violate the eigenstate thermalization hypothesis (ETH). Using the explicit formalism of the Lanczos algorithm, usually known as the forward scattering approximation in this context, we compute the Krylov state (spread) complexity of typical states generated by the time evolution of the PXP Hamiltonian, hosting such states. We show that the complexity for the Neel state revives in an approximate sense, while complexity for the generic ETH-obeying state always increases. This can be attributed to the approximate SU(2) structure of the corresponding generators of the Hamiltonian. We quantify such ''closeness'' by the q-deformed SU(2) algebra and provide an analytic expression of Lanczos coefficients for the Neel state within the approximate Krylov subspace. We intuitively explain the results in terms of a tight-binding model. We further consider a deformation of the PXP Hamiltonian and compute the corresponding Lanczos coefficients and the complexity. We find that complexity for the Neel state shows nearly perfect revival while the same does not hold for a generic ETH-obeying state.
The viscosities of partially molten materials undergoing diffusion creep<|sep|>Partially molten materials resist shearing and compaction. This resistance is described by a fourth-rank effective viscosity tensor. When the tensor is isotropic, two scalars determine the resistance: an effective shear and an effective bulk viscosity. Here, calculations are presented of the effective viscosity tensor during diffusion creep for a 2D tiling of hexagonal unit cells and a 3D tessellation of tetrakaidecahedrons (truncated octahedrons). The geometry of the melt is determined by assuming textural equilibrium. The viscosity tensor for the 2D tiling is isotropic, but that for the 3D tessellation is anisotropic. Two parameters control the effect of melt on the viscosity tensor: the porosity and the dihedral angle. Calculations for both Nabarro-Herring (volume diffusion) and Coble (surface diffusion) creep are presented. For Nabarro-Herring creep the bulk viscosity becomes singular as the porosity vanishes. This singularity is logarithmic, a weaker singularity than typically assumed in geodynamic models. The presence of a small amount of melt (0.1% porosity) causes the effective shear viscosity to approximately halve. For Coble creep, previous modelling work has argued that a very small amount of melt may lead to a substantial, factor of 5, drop in the shear viscosity. Here, a much smaller, factor of 1.4, drop is obtained for tetrakaidecahedrons. Owing to a Cauchy relation symmetry, the Coble creep bulk viscosity is a constant multiple of the shear viscosity when melt is present.
Learning to Infer Entities, Properties and their Relations from Clinical Conversations<|sep|>Recently we proposed the Span Attribute Tagging (SAT) Model (Du et al., 2019) to infer clinical entities (e.g., symptoms) and their properties (e.g., duration). It tackles the challenge of large label space and limited training data using a hierarchical two-stage approach that identifies the span of interest in a tagging step and assigns labels to the span in a classification step. We extend the SAT model to jointly infer not only entities and their properties but also relations between them. Most relation extraction models restrict inferring relations between tokens within a few neighboring sentences, mainly to avoid high computational complexity. In contrast, our proposed Relation-SAT (R-SAT) model is computationally efficient and can infer relations over the entire conversation, spanning an average duration of 10 minutes. We evaluate our model on a corpus of clinical conversations. When the entities are given, the R-SAT outperforms baselines in identifying relations between symptoms and their properties by about 32% (0.82 vs 0.62 F-score) and by about 50% (0.60 vs 0.41 F-score) on medications and their properties. On the more difficult task of jointly inferring entities and relations, the R-SAT model achieves a performance of 0.34 and 0.45 for symptoms and medications respectively, which is significantly better than 0.18 and 0.35 for the baseline model. The contributions of different components of the model are quantified using ablation analysis.
Mathematical aspects of intertwining operators: the role of Riesz bases<|sep|>In this paper we continue our analysis of intertwining relations for both self-adjoint and not self-adjoint operators. In particular, in this last situation, we discuss the connection with pseudo-hermitian quantum mechanics and the role of Riesz bases.
The Dynamics of Subhalos in Warm Dark Matter Models<|sep|>We present a comparison of the properties of substructure halos (subhalos) orbiting within host halos that form in Cold Dark Matter (CDM) and Warm Dark Matter (WDM) cosmologies. Our study focuses on selected properties of these subhalos, namely their anisotropic spatial distribution within the hosts; the existence of a "backsplash'' population; the age-distance relation; the degree to which they suffer mass loss; and the distribution of relative (infall) velocities with respect to the hosts. We find that the number density of subhalos in our WDM model is suppressed relative to that in the CDM model, as we would expect. Interestingly, our analysis reveals that backsplash subhalos exist in both the WDM and CDM models. Indeed, there are no statistically significant differences between the spatial distributions of subhalos in the CDM and WDM models. There is evidence that subhalos in the WDM model suffer enhanced mass loss relative to their counterparts in the CDM model, reflecting their lower central densities. We note also a tendency for the (infall) velocities of subhalos in the WDM model to be higher than in the CDM model. Nevertheless, we conclude that observational tests based on either the spatial distribution or the kinematics of the subhalo population are unlikely to help us to differentiate between the CDM model and our adopted WDM model.
A Ly{\alpha} blob and zabs {\approx} zem damped Ly{\alpha} absorber in the dark matter halo of the binary quasar Q 0151+048<|sep|>Q0151+048 is a physical QSO pair at z ~ 1.929 with a separation of 3.3 arcsec on the sky. In the spectrum of Q0151+048A (qA), a DLA is observed at a higher redshift. We have previously detected the host galaxies of both QSOs, as well as a Lya blob. We performed low-resolution spectroscopy with the slit aligned with the extended emission. We also observed the system using the medium-resolution VLT/X-shooter spectrograph and the slit aligned with the two QSOs. We measure systemic redshifts of zem(A)=1.92924{\pm}0.00036 and zem(B)=1.92863{\pm}0.00042 from the H{\beta} and H{\alpha} emission lines, respectively. We estimate the masses of the black holes of the two QSOs to be 10^9.33 M{\odot} and 10^8.38 M{\odot} for qA and qB, respectively. From this we infered the mass of the dark matter halos hosting the two QSOs: 10^13.74 M{\odot} and 10^13.13 M{\odot} for qA and qB, respectively. We observe a velocity gradient along the major axis of the Lya blob consistent with the rotation curve of a large disk galaxy, but it may also be caused by gas inflow or outflow. We detect residual continuum in the DLA trough which we interpret as emission from the host galaxy of qA. The derived H0 column density of the DLA is log NH0 = 20.34 {\pm} 0.02. Metal column densities results in an overall metallicity of 0.01 Z{\odot}. We detect CII* which allows us to make a physical model of the DLA cloud. From the systemic redshifts of the QSOs, we conclude that the Lya blob is associated with qA rather than with the DLA. The DLA must be located in front of both the Lya blob and qA at a distance larger than 30 kpc. The two QSOs accrete at normal eddington ratios. The DM halo of this double quasar will grow to the mass of our local super-cluster at z=0. We point out that those objects therefore form an ideal laboratory to study the physical interactions in a z=2 pre-cursor of our local super-cluster.
Adversarial Attacks and Detection on Reinforcement Learning-Based Interactive Recommender Systems<|sep|>Adversarial attacks pose significant challenges for detecting adversarial attacks at an early stage. We propose attack-agnostic detection on reinforcement learning-based interactive recommendation systems. We first craft adversarial examples to show their diverse distributions and then augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our black-box detector trained with one crafting method has the generalization ability over several crafting methods.
A discrete version of CMA-ES<|sep|>Modern machine learning uses more and more advanced optimization techniques to find optimal hyper parameters. Whenever the objective function is non-convex, non continuous and with potentially multiple local minima, standard gradient descent optimization methods fail. A last resource and very different method is to assume that the optimum(s), not necessarily unique, is/are distributed according to a distribution and iteratively to adapt the distribution according to tested points. These strategies originated in the early 1960s, named Evolution Strategy (ES) have culminated with the CMA-ES (Covariance Matrix Adaptation) ES. It relies on a multi variate normal distribution and is supposed to be state of the art for general optimization program. However, it is far from being optimal for discrete variables. In this paper, we extend the method to multivariate binomial correlated distributions. For such a distribution, we show that it shares similar features to the multi variate normal: independence and correlation is equivalent and correlation is efficiently modeled by interaction between different variables. We discuss this distribution in the framework of the exponential family. We prove that the model can estimate not only pairwise interactions among the two variables but also is capable of modeling higher order interactions. This allows creating a version of CMA ES that can accommodate efficiently discrete variables. We provide the corresponding algorithm and conclude.
Thermodynamic geometry of the Gaussian core model fluid<|sep|>The three-dimensional Gaussian core model (GCM) for soft-matter systems has repulsive interparticle interaction potential $\phi (r) = \varepsilon\, {\rm exp}\left[ -(r/\sigma)^{2} \right]$, with $r$ the distance between a pair of atoms, and the positive constants $\varepsilon$ and $\sigma$ setting the energy and length scales, respectively. $\phi (r)$ is mostly soft in character, without the typical hard core present in fluid models. We work out the thermodynamic Ricci curvature scalar $R$ for the GCM, with particular attention to the sign of $R$, which, based on previous results, is expected to be positive/negative for microscopic interactions repulsive/attractive. Over most of the thermodynamic phase space, $R$ is found to be positive, with values of the order of $\sigma^3$. However, for low densities and temperatures, the GCM potential takes on the character of a hard-sphere repulsive system, and $R$ is found to have an anomalous negative sign. Such a sign was also found earlier in inverse power law potentials in the hard-sphere limit, and seems to be a persistent feature of hard-sphere models.
Summing over trajectories of stochastic dynamics with multiplicative noise<|sep|>We demonstrate that the conventional path integral formulations generate inconsistent results exemplified by the geometric Brownian motion under the general stochastic interpretation. We thus develop a novel path integral formulation for the overdamped Langevin equation with the multiplicative noise. The present path integral leads to the corresponding Fokker-Planck equation, and naturally gives a normalized transition probability consistently in examples for general stochastic interpretations. Our result can be applied to study the fluctuation theorems and numerical calculations based on the path integral framework.
Entangling Power in the Deterministic Quantum Computation with One Qubit<|sep|>The deterministic quantum computing with one qubit (DQC1) is a mixed-state quantum computation algorithm that evaluates the normalized trace of a unitary matrix and is more powerful than the classical counterpart. We find that the normalized trace of the unitary matrix can be directly described by the entangling power of the quantum circuit of the DQC1, so the nontrivial DQC1 is always accompanied with the non-vanishing entangling power. In addition, it is shown that the entangling power also determines the intrinsic complexity of this quantum computation algorithm, i.e., the larger entangling power corresponds to higher complexity. Besides, it is also shown that the non-vanishing entangling power does always exist in other similar tasks of DQC1.
Dev-for-Operations and Multi-sided Platform for Next Generation Platform as a Service<|sep|>This paper presents two new challenges for the Telco ecosystem transformation in the era of cloud-native microservice-based architectures. (1) Development-for-Operations (Dev-for-Operations) impacts not only the overall workflow for deploying a Platform as a Service (PaaS) in an open foundry environment, but also the Telco business as well as operational models to achieve an economy of scope and an economy of scale. (2) For that purpose, we construct an integrative platform business model in the form of a Multi-Sided Platform (MSP) for building Telco PaaSes. The proposed MSP based architecture enables a multi-organizational ecosystem with increased automation possibilities for Telco-grade service creation and operation. The paper describes how the Dev-for-Operations and MSP lift constraints and offers an effective way for next-generation PaaS building, while mutually reinforcing each other in the Next Generation Platform as a Service (NGPaaS) framework.
Achievable Sum Rates of Half- and Full-Duplex Bidirectional OFDM Communication Links<|sep|>While full-duplex (FD) transmission has the potential to double the system capacity, its substantial benefit can be offset by the self-interference (SI) and non-ideality of practical transceivers. In this paper, we investigate the achievable sum rates (ASRs) of half-duplex (HD) and FD transmissions with orthogonal frequency division multiplexing (OFDM), where the non-ideality is taken into consideration. Four transmission strategies are considered, namely HD with uniform power allocation (UPA), HD with non-UPA (NUPA), FD with UPA, and FD with NUPA. For each of the four transmission strategies, an optimization problem is formulated to maximize its ASR, and a (suboptimal/optimal) solution with low complexity is accordingly derived. Performance evaluations and comparisons are conducted for three typical channels, namely symmetric frequency-flat/selective and asymmetric frequency-selective channels. Results show that the proposed solutions for both HD and FD transmissions can achieve near optimal performances. For FD transmissions, the optimal solution can be obtained under typical conditions. In addition, several observations are made on the ASR performances of HD and FD transmissions.
Equilibria of a charged artificial satellite subject to gravitational and Lorentz torques<|sep|>Attitude Dynamics of a rigid artificial satellite subject to gravity gradient and Lorentz torques in a circular orbit is considered. Lorentz torque is developed on the basis of the electrodynamic effects of the Lorentz force acting on the charged satellite's surface. We assume that the satellite is moving in Low Earth Orbit (LEO) in the geomagnetic field which is considered as a dipole model. Our model of the torque due to the Lorentz force is developed for a general shape of artificial satellite, and the nonlinear differential equations of Euler are used to describe its attitude orientation. All equilibrium positions are determined and {their} existence conditions are obtained. The numerical results show that the charge $q$ and radius $\rho_0$ of the charged center of satellite provide a certain type of semi passive control for the attitude of satellite. The technique for such kind of control would be to increase or decrease the electrostatic radiation screening of the satellite. The results {obtained} confirm that the change in charge can effect the magnitude of the Lorentz torque, which may affect the satellite's control. Moreover, the relation between the magnitude of the Lorentz torque and inclination of the orbits is investigated.
Build Electronic Arabic Lexicon<|sep|>There are many known Arabic lexicons organized on different ways, each of them has a different number of Arabic words according to its organization way. This paper has used mathematical relations to count a number of Arabic words, which proofs the number of Arabic words presented by Al Farahidy. The paper also presents new way to build an electronic Arabic lexicon by using a hash function that converts each word (as input) to correspond a unique integer number (as output), these integer numbers will be used as an index to a lexicon entry.
Diffuse gamma-ray emission in the vicinity of young star cluster Westerlund 2<|sep|>We report the results of our analysis of the publicly available data obtained by the Large Area Telescope (LAT) on board of the Fermi satellite towards the direction of the young massive star cluster Westerlund 2. We found significant extended gamma-ray emission in the vicinity of Westerlund 2 with a hard power-law energy spectrum extending from 1 GeV to 250 GeV with a photon index of 2.0 +/- 0.1. We argue that amongst several alternatives, the luminous stars in Westerlund 2 are likely sites of acceleration of particles responsible for the diffuse gamma-ray emission of the surrounding interstellar medium. In particular, the young star cluster Westerlund 2 can provide sufficient non-thermal energy to account for the gamma-ray emission. In this scenario, since the gamma-ray production region is significantly larger than the area occupied by the star cluster, we conclude that the gamma-ray production is caused by hadronic interactions of the accelerated protons and nuclei with the ambient gas. In that case, the total energy budget in relativistic particles is estimated of the order of 1e50 erg.
Nano granular metallic Fe - oxygen deficient TiO$_{2-\delta}$ composite films: A room temperature, highly carrier polarized magnetic semiconductor<|sep|>Nano granular metallic iron (Fe) and titanium dioxide (TiO$_{2-\delta}$) were co-deposited on (100) lanthanum aluminate (LaAlO$_3$) substrates in a low oxygen chamber pressure using a pulsed laser ablation deposition (PLD) technique. The co-deposition of Fe and TiO$_2$ resulted in $\approx$ 10 nm metallic Fe spherical grains suspended within a TiO$_{2-\delta}$ matrix. The films show ferromagnetic behavior with a saturation magnetization of 3100 Gauss at room temperature. Our estimate of the saturation magnetization based on the size and distribution of the Fe spheres agreed well with the measured value. The film composite structure was characterized as p-type magnetic semiconductor at 300 K with a carrier density of the order of $ 10^{22} /{\rm cm^3}$. The hole carriers were excited at the interface between the nano granular Fe and TiO$_{2-\delta}$ matrix similar to holes excited in the metal/n-type semiconductor interface commonly observed in Metal-Oxide-Semiconductor (MOS) devices. From the large anomalous Hall effect directly observed in these films it follows that the holes at the interface were strongly spin polarized. Structure and magneto transport properties suggested that these PLD films have potential nano spintronics applications.
Computing mixed strategies equilibria in presence of switching costs by the solution of nonconvex QP problems<|sep|>In this paper we address game theory problems arising in the context of network security. In traditional game theory problems, given a defender and an attacker, one searches for mixed strategies which minimize a linear payoff functional. In the problems addressed in this paper an additional quadratic term is added to the minimization problem. Such term represents switching costs, i.e., the costs for the defender of switching from a given strategy to another one at successive rounds of a Nash game. The resulting problems are nonconvex QP ones with linear constraints and turn out to be very challenging. We will show that the most recent approaches for the minimization of nonconvex QP functions over polytopes, including commercial solvers such as CPLEX and GUROBI, are unable to solve to optimality even test instances with n = 50 variables. For this reason, we propose to extend with them the current benchmark set of test instances for QP problems. We also present a spatial branch-and-bound approach for the solution of these problems, where a predominant role is played by an optimality-based domain reduction, with multiple solutions of LP problems at each node of the branch-and-bound tree. Of course, domain reductions are standard tools in spatial branch-and-bound approaches. However, our contribution lies in the observation that, from the computational point of view, a rather aggressive application of these tools appears to be the best way to tackle the proposed instances. Indeed, according to our experiments, while they make the computational cost per node high, this is largely compensated by the rather slow growth of the number of nodes in the branch-and-bound tree, so that the proposed approach strongly outperforms the existing solvers for QP problems.
The little-studied cluster Berkeley 90. I. LS III +46 11: a very massive O3.5 If* + O3.5 If* binary<|sep|>Context: It appears that most (if not all) massive stars are born in multiple systems. At the same time, the most massive binaries are hard to find due to their low numbers throughout the Galaxy and the implied large distances and extinctions. AIMS: We want to study: [a] LS III +46 11, identified in this paper as a very massive binary; [b] another nearby massive system, LS III +46 12; and [c] the surrounding stellar cluster, Berkeley 90. Methods: Most of the data used in this paper are multi-epoch high-S/N optical spectra though we also use Lucky Imaging and archival photometry. The spectra are reduced with devoted pipelines and processed with our own software, such as a spectroscopic-orbit code, CHORIZOS, and MGB. Results: LS III +46 11 is identified as a new very-early-O-type spectroscopic binary [O3.5 If* + O3.5 If*] and LS III +46 12 as another early O-type system [O4.5 V((f))]. We measure a 97.2-day period for LS III +46 12 and derive minimum masses of 38.80$\pm$0.83 M_Sol and 35.60$\pm$0.77 M_Sol for its two stars. We measure the extinction to both stars, estimate the distance, search for optical companions, and study the surrounding cluster. In doing so, a variable extinction is found as well as discrepant results for the distance. We discuss possible explanations and suggest that LS III +46 12 may be a hidden binary system, where the companion is currently undetected.
Engagement Detection with Multi-Task Training in E-Learning Environments<|sep|>Recognition of user interaction, in particular engagement detection, became highly crucial for online working and learning environments, especially during the COVID-19 outbreak. Such recognition and detection systems significantly improve the user experience and efficiency by providing valuable feedback. In this paper, we propose a novel Engagement Detection with Multi-Task Training (ED-MTT) system which minimizes mean squared error and triplet loss together to determine the engagement level of students in an e-learning environment. The performance of this system is evaluated and compared against the state-of-the-art on a publicly available dataset as well as videos collected from real-life scenarios. The results show that ED-MTT achieves 6% lower MSE than the best state-of-the-art performance with highly acceptable training time and lightweight feature extraction.
Distinguishability of Quantum States by Positive Operator-Valued Measures with Positive Partial Transpose<|sep|>We study the distinguishability of bipartite quantum states by Positive Operator-Valued Measures with positive partial transpose (PPT POVMs). The contributions of this paper include: (1). We give a negative answer to an open problem of [M. Horodecki $et. al$, Phys. Rev. Lett. 90, 047902(2003)] showing a limitation of their method for detecting nondistinguishability. (2). We show that a maximally entangled state and its orthogonal complement, no matter how many copies are supplied, can not be distinguished by PPT POVMs, even unambiguously. This result is much stronger than the previous known ones \cite{DUAN06,BAN11}. (3). We study the entanglement cost of distinguishing quantum states. It is proved that $\sqrt{2/3}\ket{00}+\sqrt{1/3}\ket{11}$ is sufficient and necessary for distinguishing three Bell states by PPT POVMs. An upper bound of entanglement cost of distinguishing a $d\otimes d$ pure state and its orthogonal complement is obtained for separable operations. Based on this bound, we are able to construct two orthogonal quantum states which cannot be distinguished unambiguously by separable POVMs, but finite copies would make them perfectly distinguishable by LOCC. We further observe that a two-qubit maximally entangled state is always enough for distinguishing a $d\otimes d$ pure state and its orthogonal complement by PPT POVMs, no matter the value of $d$. In sharp contrast, an entangled state with Schmidt number at least $d$ is always needed for distinguishing such two states by separable POVMs. As an application, we show that the entanglement cost of distinguishing a $d\otimes d$ maximally entangled state and its orthogonal complement must be a maximally entangled state for $d=2$,which implies that teleportation is optimal; and in general, it could be chosen as $\mathcal{O}(\frac{\log d}{d})$.
Dynamic effective mass of granular media and the attenuation of structure-borne sound<|sep|>We report an experimental and theoretical investigation of the frequency-dependent effective mass, $\tilde{M}(\omega)$, of loose granular particles which occupy a rigid cavity to a given filling fraction, the remaining volume being air of differing humidities. This allow us to study the mechanisms of elastic response and attenuation of acoustic modes in granular media. We demonstrate that this is a sensitive and direct way to measure those properties of the granular medium that are the cause of the changes in acoustic properties of structures containing grain-filled cavities. Specifically, we apply this understanding to the case of the flexural resonances of a rectangular bar with a grain-filled cavity within it. The dominant features of $\tilde{M}(\omega)$ are a sharp resonance and a broad background, which we analyze within the context of simple models. We find that: a) These systems may be understood in terms of a height-dependent and diameter-dependent effective sound speed ($\sim 100-300$ m/s) and an effective viscosity ($\sim 5\times 10^4$ Poise). b) There is a dynamic Janssen effect in the sense that, at any frequency, and depending on the method of sample preparation, approximately one-half of the effective mass is borne by the side walls of the cavity and one-half by the bottom. c) By performing experiments under varying humidity conditions we conclude that, on a fundamental level, damping of acoustic modes is dominated by adsorbed films of water at grain-grain contacts in our experiments, not by global viscous dampening. d) There is a monotonically increasing effect of humidity on the dampening of the fundamental resonance within the granular medium which translates to a non-monotonic, but predictable, variation of dampening within the grain-loaded bar.
Screening like-charges in one-dimensional Coulomb systems: Exact results<|sep|>The possibility that like-charges can attract each other under the mediation of mobile counterions is by now well documented experimentally, numerically, and analytically. Yet, obtaining exact results is in general impossible, or restricted to some limiting cases. We work out here in detail a one dimensional model that retains the essence of the phenomena present in higher dimensional systems. The partition function is obtained explicitly, from which a wealth of relevant quantities follow, such as the effective force between the charges or the counterion profile in their vicinity. Isobaric and canonical ensembles are distinguished. The case of two equal charges screened by an arbitrary number $N$ of counterions is first studied, before the more general asymmetric situation is addressed. It is shown that the parity of $N$ plays a key role in the long range physics.
Weak Lensing Magnification Reconstruction with the Modified Internal Linear Combination Method<|sep|>Measuring weak lensing cosmic magnification signal is very challenging due to the overwhelming intrinsic clustering in the observed galaxy distribution. In this paper, we modify the Internal Linear Combination (ILC) method to reconstruct the lensing signal with an extra constraint to suppress the intrinsic clustering. To quantify the performance, we construct a realistic galaxy catalogue for the LSST-like photometric survey, covering $20\,000\ \deg^2$ with mean source redshift at $z_s\sim 1$. We find that the reconstruction performance depends on the width of the photo-z bin we choose. Due to the correlation between the lensing signal and the source galaxy distribution, the derived signal has smaller systematic bias but larger statistical uncertainty for a narrower photo-z bin. We conclude that the lensing signal reconstruction with the Modified ILC method is unbiased with a statistical uncertainty $<5\%$ for bin width $\Delta z^P = 0.2$.
Azimuthal asymmetries of charged hadrons produced by high-energy muons scattered off longitudinally polarised deuterons<|sep|>Azimuthal asymmetries in semi-inclusive production of positive (h^+) and negative hadrons (h^-) have been measured by scattering 160 GeV muons off longitudinally polarised deuterons at CERN. The asymmetries were decomposed in several terms according to their expected modulation in the azimuthal angle phi of the outgoing hadron. Each term receives contributions from one or several spin and transverse-momentum-dependent parton distribution and fragmentation functions. The amplitudes of all phi-modulation terms of the hadron asymmetries integrated over the kinematic variables are found to be consistent with zero within statistical errors, while the constant terms are nonzero and equal for h^+ and h^- within the statistical errors. The dependencies of the phi-modulated terms versus the Bjorken momentum fraction x, the hadron fractional momentum z, and the hadron transverse momentum p_h^T were studied. The x dependence of the constant terms for both positive and negative hadrons is in agreement with the longitudinal double-spin hadron asymmetries, measured in semi-inclusive deep-inelastic scattering. The x dependence of the sin phi-modulation term is less pronounced than that in the corresponding HERMES data. All other dependencies of the phi-modulation amplitudes are consistent with zero within the statistical errors.
Temporal Registration in In-Utero Volumetric MRI Time Series<|sep|>We present a robust method to correct for motion and deformations for in-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI requires robust alignment across time in the presence of substantial and unpredictable motion. We make a Markov assumption on the nature of deformations to take advantage of the temporal structure in the image data. Forward message passing in the corresponding hidden Markov model (HMM) yields an estimation algorithm that only has to account for relatively small motion between consecutive frames. We demonstrate the utility of the temporal model by showing that its use improves the accuracy of the segmentation propagation through temporal registration. Our results suggest that the proposed model captures accurately the temporal dynamics of deformations in in-utero MRI time series.
A new seeding technique for the reliable fabrication of large, SmBCO single grains containing silver using top seeded melt growth<|sep|>Silver (Ag) is an established additive for improving the mechanical properties of single grain, (RE)BCO bulk superconductors (where RE = Sm, Gd and Y). The presence of Ag in the (RE)BCO bulk composition, however, typically reduces the melting temperature of the single crystal seed in the top seeded melt growth (TSMG) process, which complicates significantly the controlled nucleation and subsequent epitaxial growth of a single grain, which is essential for high field engineering applications. The reduced reliability of the seeding process in the presence of Ag is particularly acute for the SmBCO system, since the melting temperature of SmBCO is very close to that of the generic NdBCO(MgO) seed. SmBCO has the highest superconducting transition temperature, Tc, and exhibits the most pronounced "peak" effect at higher magnetic field of all materials in the family of (RE)BCO bulk superconductors and, therefore, has the greatest potential for use in practical applications (compared to GdBCO and YBCO, in particular). Development of an effective seeding process, therefore, is one of the major challenges of the TSMG process for the growth of large, high quantity single grain superconductors. In this paper, we report a novel technique that involves introducing a buffer layer between the seed crystal and the precursor pellet, primarily to inhibit the diffusion of Ag from the green body to the seed during melt processing in order to prevent the melting of the seed. The success rate of the seeding process using this technique is 100% for relatively small batch samples. The superconducting properties, Tc, Jc and trapped fields, of the single grains fabricated using the buffers are reported and the micro-structures in the vicinity of the buffer of single grains fabricated by the modified technique are analysed.
The role of flavon cross couplings in leptonic flavour mixing<|sep|>In models with discrete flavour symmetries, flavons are critical to realise specific flavour structures. Leptonic flavour mixing originates from the misalignment of flavon vacuum expectation values which respect different residual symmetries in the charged lepton and neutrino sectors. Flavon cross couplings are usually forbidden, in order to protect these symmetries. Contrary to this approach, we show that cross couplings can play a key role and give raise to necessary corrections to flavour-mixing patterns, including a non-zero value for the reactor angle and CP violation. For definiteness, we present two models based on $A_4$. In the first model, all flavons are assumed to be real or pseudo-real, with 7 real degrees of freedom in the flavon sector in total. A sizable reactor angle associated with nearly maximal CP violation is achieved, and, as both originate from the same cross coupling, a sum rule results with a precise prediction for the value of the Dirac CP-violating phase. In the second model, the flavons are taken to be complex scalars, which can be connected with supersymmetric models and multi-Higgs models. The complexity properties of flavons provide new sources for generating the reactor angle. Models in this new approach introduce very few degrees of freedom beyond the Standard Model and can be more economical than those in the framework of extra dimension or supersymmetry.
Investigating Societal Biases in a Poetry Composition System<|sep|>There is a growing collection of work analyzing and mitigating societal biases in language understanding, generation, and retrieval tasks, though examining biases in creative tasks remains underexplored. Creative language applications are meant for direct interaction with users, so it is important to quantify and mitigate societal biases in these applications. We introduce a novel study on a pipeline to mitigate societal biases when retrieving next verse suggestions in a poetry composition system. Our results suggest that data augmentation through sentiment style transfer has potential for mitigating societal biases.
Groupes quantiques associes aux courbes rationnelles et elliptiques et leurs applications<|sep|>The thesis was defended by the author in University of Angers (France). It consists of four parts. The fist part (in French) is introductory and is devoted to relation between quantum groups, integrable systems and statistical models. In the second part (in English) the transition function of the periodic Toda chain is interpreted in terms of the formalism of rational Lax operators. In the third part (in French) one compares two elliptic quantum groups and one conclude that they belong to two different bialgebra categories. The fourth part (in English) contains a construction of the partition function of the SOS model in terms of the projections of an elliptic quantum group.
Dilepton and Four-Lepton Signals at the LHC in the Littlest Higgs Model with T-parity Violation<|sep|>In the presence of the T-parity violating Wess-Zumino-Witten (WZW) anomaly term, the otherwise stable heavy photon A_H in the Littlest Higgs model with T-parity (LHT) decays to either Standard Model (SM) gauge boson pairs, or to SM fermions via loop diagrams. We make a detailed study of the collider signatures where the A_H can be reconstructed from invariant mass peaks in the opposite sign same flavor dilepton or the four-lepton channels. This enables us to obtain information about the fundamental symmetry breaking scale f in the LHT and thereby the low-lying mass spectrum of the theory. In addition, indication of the presence of the WZW term gives us hints of the possible UV completion of the LHT via strong dynamics. The crucial observation is that the sum of all production processes of heavy T-odd quark pairs has a sizeable cross-section at the LHC and these T-odd particles eventually all cascade decay down to the heavy photon A_H. We show that for certain regions of the parameter space with either a small f of around 500 GeV or relatively light T-odd quarks with a mass of around 400 GeV, one can reconstruct the A_H even at the early LHC run with \sqrt{s}=10 TeV and a modest integrated luminosity of 200 pb^{-1}. At \sqrt{s} = 14 TeV and with an integrated luminosity of 30 fb^{-1}, it is possible to cover a large part of the typical parameter space of the LHT, with the scale f up to 1.5 TeV and with T-odd quark masses almost up to 1 TeV. In this region of the parameter space, the mass of the reconstructed A_H ranges from 66 GeV to 230 GeV.
A control theoretic approach to achieve proportional fairness in 802.11e EDCA WLANs<|sep|>This paper considers proportional fairness amongst ACs in an EDCA WLAN for provision of distinct QoS requirements and priority parameters. A detailed theoretical analysis is provided to derive the optimal station attempt probability which leads to a proportional fair allocation of station throughputs. The desirable fairness can be achieved using a centralised adaptive control approach. This approach is based on multivariable statespace control theory and uses the Linear Quadratic Integral (LQI) controller to periodically update CWmin till the optimal fair point of operation. Performance evaluation demonstrates that the control approach has high accuracy performance and fast convergence speed for general network scenarios. To our knowledge this might be the first time that a closed-loop control system is designed for EDCA WLANs to achieve proportional fairness.
End-to-end Global to Local CNN Learning for Hand Pose Recovery in Depth Data<|sep|>Despite recent advances in 3D pose estimation of human hands, especially thanks to the advent of CNNs and depth cameras, this task is still far from being solved. This is mainly due to the highly non-linear dynamics of fingers, which make hand model training a challenging task. In this paper, we exploit a novel hierarchical tree-like structured CNN, in which branches are trained to become specialized in predefined subsets of hand joints, called local poses. We further fuse local pose features, extracted from hierarchical CNN branches, to learn higher order dependencies among joints in the final pose by end-to-end training. Lastly, the loss function used is also defined to incorporate appearance and physical constraints about doable hand motion and deformation. Finally, we introduce a non-rigid data augmentation approach to increase the amount of training depth data. Experimental results suggest that feeding a tree-shaped CNN, specialized in local poses, into a fusion network for modeling joints correlations and dependencies, helps to increase the precision of final estimations, outperforming state-of-the-art results on NYU and SyntheticHand datasets.
Evaluating Maintainability Prejudices with a Large-Scale Study of Open-Source Projects<|sep|>Exaggeration or context changes can render maintainability experience into prejudice. For example, JavaScript is often seen as least elegant language and hence of lowest maintainability. Such prejudice should not guide decisions without prior empirical validation. We formulated 10 hypotheses about maintainability based on prejudices and test them in a large set of open-source projects (6,897 GitHub repositories, 402 million lines, 5 programming languages). We operationalize maintainability with five static analysis metrics. We found that JavaScript code is not worse than other code, Java code shows higher maintainability than C# code and C code has longer methods than other code. The quality of interface documentation is better in Java code than in other code. Code developed by teams is not of higher and large code bases not of lower maintainability. Projects with high maintainability are not more popular or more often forked. Overall, most hypotheses are not supported by open-source data.
Dynamics of hot random hyperbolic graphs<|sep|>We derive the most basic dynamical properties of random hyperbolic graphs (the distributions of contact and intercontact durations) in the hot regime (network temperature $T > 1$). We show that for sufficiently large networks the contact distribution decays as a power law with exponent $2+T > 3$ for durations $t > T$, while for $t < T$ it exhibits exponential-like decays. This result holds irrespective of the expected degree distribution, as long as it has a finite $T^{\text{th}}$ moment. Otherwise, the contact distribution depends on the expected degree distribution and we show that if the latter is a power law with exponent $\gamma \in (2, T+1]$, then the former decays as a power law with exponent $\gamma+1 > 3$. On the other hand, the intercontact distribution exhibits power-law decays with exponent $2-T \in (0, 1)$ for $T \in (1,2)$, while for $T > 2$ it displays linear decays with a slope that depends on the observation interval. This result holds irrespective of the expected degree distribution as long as it has a finite $T^{\text{th}}$ moment if $T \in (1,2)$, or a finite second moment if $T > 2$. Otherwise, the intercontact distribution depends on the expected degree distribution and if the latter is a power law with exponent $\gamma \in (2, 3)$, then the former decays as a power law with exponent $3-\gamma \in (0,1)$. Thus, hot random hyperbolic graphs can give rise to contact and intercontact distributions that both decay as power laws. These power laws however are unrealistic for the case of the intercontact distribution, as their exponent is always less than one. These results mean that hot random hyperbolic graphs are not adequate for modeling real temporal networks, in stark contrast to cold random hyperbolic graphs ($T < 1$). Since the configuration model emerges at $T \to \infty$, these results also suggest that this is not an adequate null temporal network model.
Signatures of Rashba spin-orbit interaction in the superconducting proximity effect in helical Luttinger liquids<|sep|>We consider the superconducting proximity effect in a helical Luttinger liquid at the edge of a 2D topological insulator, and derive the low-energy Hamiltonian for an edge state tunnel-coupled to a s-wave superconductor. In addition to correlations between the left and right moving modes, the coupling can induce them inside a single mode, as the spin axis of the edge modes is not necessarily constant. This can be induced controllably in HgTe/CdTe quantum wells via the Rashba spin-orbit coupling, and is a consequence of the 2D nature of the edge state wave function. The distinction of these two features in the proximity effect is also vital for the use of such helical modes in order to split Cooper-pairs. We discuss the consequent transport signatures, and point out a long-ranged feature in a dc conductance measurement that can be used to distinguish the two types of correlations present and to determine the magnitude of the Rashba interaction.
Top Quark Pair Production beyond NNLO<|sep|>We construct an approximate expression for the total cross section for the production of a heavy quark-antiquark pair in hadronic collisions at next-to-next-to-next-to-leading order (N$^3$LO) in $\alpha_s$. We use a technique which exploits the analyticity of the Mellin space cross section, and the information on its singularity structure coming from large N (soft gluon, Sudakov) and small N (high energy, BFKL) all order resummations, previously introduced and used in the case of Higgs production. We validate our method by comparing to available exact results up to NNLO. We find that N$^3$LO corrections increase the predicted top pair cross section at the LHC by about 4% over the NNLO.
Model-Based Opponent Modeling<|sep|>When one agent interacts with a multi-agent environment, it is challenging to deal with various opponents unseen before. Modeling the behaviors, goals, or beliefs of opponents could help the agent adjust its policy to adapt to different opponents. In addition, it is also important to consider opponents who are learning simultaneously or capable of reasoning. However, existing work usually tackles only one of the aforementioned types of opponents. In this paper, we propose model-based opponent modeling (MBOM), which employs the environment model to adapt to all kinds of opponents. MBOM simulates the recursive reasoning process in the environment model and imagines a set of improving opponent policies. To effectively and accurately represent the opponent policy, MBOM further mixes the imagined opponent policies according to the similarity with the real behaviors of opponents. Empirically, we show that MBOM achieves more effective adaptation than existing methods in a variety of tasks, respectively with different types of opponents, i.e., fixed policy, na\"ive learner, and reasoning learner.
Local Deep Implicit Functions for 3D Shape<|sep|>The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.
DFE/THP duality for FBMC with highly frequency selective channels<|sep|>Filter bank based multicarrier with Offset-QAM systems (FBMC/OQAM) are strong candidates for the waveform of future 5-th generation (5G) wireless standards. These systems can achieve maximum spectral efficiency compared to other multicarrier schemes, particularly in highly frequency selective propagation conditions. In this case a multi-tap, fractionally spaced equalizer or precoder needs to be inserted in each subcarrier at the receiver or transmitter side to compensate inter-symbol interference (ISI) and inter-carrier interference (ICI). In this paper we propose a new Tomlinson-Harashima precoder (THP) design for FBMC/OQAM based on the mean squared error (MSE) duality from a minimum MSE (MMSE) designed decision feedback equalizer (DFE).
MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models<|sep|>This paper presents MOAT, a family of neural networks that build on top of MObile convolution (i.e., inverted residual blocks) and ATtention. Unlike the current works that stack separate mobile convolution and transformer blocks, we effectively merge them into a MOAT block. Starting with a standard Transformer block, we replace its multi-layer perceptron with a mobile convolution block, and further reorder it before the self-attention operation. The mobile convolution block not only enhances the network representation capacity, but also produces better downsampled features. Our conceptually simple MOAT networks are surprisingly effective, achieving 89.1% top-1 accuracy on ImageNet-1K with ImageNet-22K pretraining. Additionally, MOAT can be seamlessly applied to downstream tasks that require large resolution inputs by simply converting the global attention to window attention. Thanks to the mobile convolution that effectively exchanges local information between pixels (and thus cross-windows), MOAT does not need the extra window-shifting mechanism. As a result, on COCO object detection, MOAT achieves 59.2% box AP with 227M model parameters (single-scale inference, and hard NMS), and on ADE20K semantic segmentation, MOAT attains 57.6% mIoU with 496M model parameters (single-scale inference). Finally, the tiny-MOAT family, obtained by simply reducing the channel sizes, also surprisingly outperforms several mobile-specific transformer-based models on ImageNet. We hope our simple yet effective MOAT will inspire more seamless integration of convolution and self-attention. Code is made publicly available.
Simulation with Fluctuating and Singular Rates<|sep|>In this paper we present a method to generate independent samples for a general random variable, either continuous or discrete. The algorithm is an extension of the acceptance-rejection method, and it is particularly useful for kinetic simulation in which the rates are fluctuating in time and have singular limits, as occurs for example in simulation of recombination interactions in a plasma. Although it depends on some additional requirements, the new method is easy to implement and rejects less samples than the acceptance-rejection method.
Recognition and classification of the cosmic-ray events in images captured by CMOS/CCD cameras<|sep|>Muons and other ionizing radiation produced by cosmic rays and radiative decays affect CMOS/CCD sensor. When particles colliding with sensors atoms cause specific kind of noise on images recorded by cameras. We present a concept and preliminary implementation of method for recognizing those events and algorithms for image processing and their classification by machine learning. Our method consists of analyzing the shape of traces present in images recorded by a camera sensor and metadata related to an image like camera model, GPS location of camera, vertical and horizontal orientation of a camera sensor, timestamp of image acquisition, and other events recognized near-by sensors. The so created feature vectors are classified as either a muon-like event, an electron-like event or the other event, possibly noise. For muon-like events our method estimates azimuth of a muon track. Source of the data is database of CREDO (Cosmic-Ray Extremely Distributed Observatory) project and ESO (European Southern Observatory) archives. The telescope dark frames from ESO are analysed. CREDO project collected so far over 2 millions images of events from many kinds of cameralike: smartphones camera, laptop webcams and Internet of Things cameras localised around the globe.
Data Modeling with Large Random Matrices in a Cognitive Radio Network Testbed: Initial Experimental Demonstrations with 70 Nodes<|sep|>This short paper reports some initial experimental demonstrations of the theoretical framework: the massive amount of data in the large-scale cognitive radio network can be naturally modeled as (large) random matrices. In particular, using experimental data we will demonstrate that the empirical spectral distribution of the large sample covariance matrix---a Hermitian random matrix---agree with its theoretical distribution (Marchenko-Pastur law). On the other hand, the eigenvalues of the large data matrix ---a non-Hermitian random matrix---are experimentally found to follow the single ring law, a theoretical result that has been discovered relatively recently. To our best knowledge, our paper is the first such attempt, in the context of large-scale wireless network, to compare theoretical predictions with experimental findings.
Performance Analysis of Uplink & Downlink Transmission in CDMA System<|sep|>CDMA is a multiple access method in which the user's uses spread spectrum techniques and occupy the entire spectrum whenever they transmit. In wireless communication signal-to-noise ratio (SNR) is the very important parameter that influences the system performance. Any mode of mobile transmission is not free from channel impairment such as noise, interference and fading. This channel impairment caused signal distortion and degradation in SNR.Also there are differences between uplink (forward channel) and downlink (reverse channel).Along with these differences, both the links use different codes for chanellizing the individual users. This paper simulates the expressions for the pdfs of the SNR for both uplink and downlink transmission assuming that the system is operating at an average signal-to-noise ratio is 6dB per information bit.
Semi-discrete finite element approximation applied to Maxwell's equations in nonlinear media<|sep|>In this paper the semi-discrete finite element approximation of initial boundary value problems for Maxwell's equations in nonliear media of Kerr-type is investigated. For the case of N\'ed\'elec elements from the first family, a priori error estimates are established for the approximation.
On the Robustness of Cooperative Multi-Agent Reinforcement Learning<|sep|>In cooperative multi-agent reinforcement learning (c-MARL), agents learn to cooperatively take actions as a team to maximize a total team reward. We analyze the robustness of c-MARL to adversaries capable of attacking one of the agents on a team. Through the ability to manipulate this agent's observations, the adversary seeks to decrease the total team reward. Attacking c-MARL is challenging for three reasons: first, it is difficult to estimate team rewards or how they are impacted by an agent mispredicting; second, models are non-differentiable; and third, the feature space is low-dimensional. Thus, we introduce a novel attack. The attacker first trains a policy network with reinforcement learning to find a wrong action it should encourage the victim agent to take. Then, the adversary uses targeted adversarial examples to force the victim to take this action. Our results on the StartCraft II multi-agent benchmark demonstrate that c-MARL teams are highly vulnerable to perturbations applied to one of their agent's observations. By attacking a single agent, our attack method has highly negative impact on the overall team reward, reducing it from 20 to 9.4. This results in the team's winning rate to go down from 98.9% to 0%.
Probing the $\mu\nu$SSM with light scalars, pseudoscalars and neutralinos from the decay of a SM-like Higgs boson at the LHC<|sep|>The "$\mu$ from $\nu$" supersymmetric standard model ($\mu\nu$SSM) can accommodate the newly discovered Higgs-like scalar boson with a mass around 125 GeV. This model provides a solution to the $\mu$-problem and simultaneously reproduces correct neutrino physics by the simple use of right-handed neutrino superfields. These new superfields together with the introduced $R$-parity violation can produce novel and characteristic signatures of the $\mu\nu$SSM at the LHC. We explore the signatures produced through two-body Higgs decays into the new states, provided that these states lie below in the mass spectrum. For example, a pair produced light neutralinos depending on the associated decay length can give rise to displaced multi-leptons/taus/jets/photons with small/moderate missing transverse energy. In the same spirit, a Higgs-like scalar decaying to a pair of scalars/pseudoscalars can produce final states with prompt multi-leptons/taus/jets/photons.
The nature of the ferromagnetic ground state in the Mn4 molecular magnet<|sep|>Using ab initio band structure and model calculations we studied magnetic properties of one of the Mn$_4$ molecular magnets (Mn4(hmp)6), where two types of the Mn ions exist: Mn3+ and Mn2+. The direct calculation of the exchange constants in the GGA+U approximation shows that in contrast to a common belief the strongest exchange coupling is not between two Mn3+ ions (J_{bb}), but along two out of four exchange paths connecting Mn3+ and Mn2+ ions (J_{wb}). The microscopic analysis performed within the perturbation theory allowed to establish the mechanism for this largest ferromagnetic exchange constant. The charge ordering of the Mn ions results in the situation when the energy of the excited state in the exchange process is defined not by the large on-site Coulomb repulsion U, but by much smaller energy V, which stabilizes the charge ordered state. Together with strong Hund's rule coupling and specific orbital order this leads to a large ferromagnetic exchange interaction for two out of four Mn2+ --Mn3+ pairs.
Green communication via Type-I ARQ: Finite block-length analysis<|sep|>This paper studies the effect of optimal power allocation on the performance of communication systems utilizing automatic repeat request (ARQ). Considering Type-I ARQ, the problem is cast as the minimization of the outage probability subject to an average power constraint. The analysis is based on some recent results on the achievable rates of finite-length codes and we investigate the effect of codewords length on the performance of ARQ-based systems. We show that the performance of ARQ protocols is (almost) insensitive to the length of the codewords, for codewords of length $\ge 50$ channel uses. Also, optimal power allocation improves the power efficiency of the ARQ-based systems substantially. For instance, consider a Rayleigh fading channel, codewords of rate 1 nats-per-channel-use and outage probability $10^{-3}.$ Then, with a maximum of 2 and 3 transmissions, the implementation of power-adaptive ARQ reduces the average power, compared to the open-loop communication setup, by 17 and 23 dB, respectively, a result which is (almost) independent of the codewords length. Also, optimal power allocation increases the diversity gain of the ARQ protocols considerably.
Critical Behaviour Of Directed Percolation In The Presence Of Synthetic Velocity Field<|sep|>Using perturbative renormalization group we study the influence of random velocity field on the critical behaviour of directed bond percolation process near its second-order phase transition between absorbing and active phase. We consider Kraichnan model with finite correlation time for modelling advecting velocity field. Using functional integral representation we are able to apply field-theoretic renormalization group to determine possible universality classes. The model is analyzed near its critical dimension by means of three-parameter expansion in $\epsilon,\delta,\eta$, where $\epsilon$ is the deviation from the Kolmogorov scaling, $\delta$ the deviation from the critical space dimension $d_c$ and $\eta$ is the deviation from the parabolic dispersion law for the velocity correlator. Fixed points with corresponding regions of stability are evaluated to the leading order in the perturbation scheme.
Infrared Search for Young Brown Dwarf Companions around Young Stellar Objects in the rho Ophiucus Molecular Cloud and the Serpens Molecular Cloud<|sep|>We conducted an infrared search for faint companions around 351 young stellar objects in the rho Ophiucus molecular cloud and the Serpens molecular cloud. Nine objects in the Spitzer/IRAC archival images were identified as young stellar companion candidates. They showed an intrinsic infrared excess; one object was extremely red both in the [3.6] - [4.5] color and in the [4.5] - [5.8] color, and two objects were red in the [4.5] - [5.8] color. They were as faint as 15 mag in the [3.6] band. Follow-up K-band spectroscopy revealed that three objects had deep water absorption bands, indicative of low effective temperatures. By comparing the spectra and infrared spectral energy distributions with synthesized spectra of low-temperature objects, we derived the effective temperatures and continuum excess for these objects. It seems highly likely that one of the three objects is a low-mass stellar companion and two objects are young brown dwarf companions associated with the young stellar objects.
A Novel Strategy Selection Method for Multi-Objective Clustering Algorithms Using Game Theory<|sep|>The most important factors which contribute to the efficiency of game-theoretical algorithms are time and game complexity. In this study, we have offered an elegant method to deal with high complexity of game theoretic multi-objective clustering methods in large-sized data sets. Here, we have developed a method which selects a subset of strategies from strategies profile for each player. In this case, the size of payoff matrices reduces significantly which has a remarkable impact on time complexity. Therefore, practical problems with more data are tractable with less computational complexity. Although strategies set may grow with increasing the number of data points, the presented model of strategy selection reduces the strategy space, considerably, where clusters are subdivided into several sub-clusters in each local game. The remarkable results demonstrate the efficiency of the presented approach in reducing computational complexity of the problem of concern.
Accurate evolutions of inspiralling neutron-star binaries: assessment of the truncation error<|sep|>We have recently presented an investigation in full general relativity of the dynamics and gravitational-wave emission from binary neutron stars which inspiral and merge, producing a black hole surrounded by a torus (see arXiv:0804.0594). We here discuss in more detail the convergence properties of the results presented in arXiv:0804.0594 and, in particular, the deterioration of the convergence rate at the merger and during the survival of the merged object, when strong shocks are formed and turbulence develops. We also show that physically reasonable and numerically convergent results obtained at low-resolution suffer however from large truncation errors and hence are of little physical use. We summarize our findings in an "error budget", which includes the different sources of possible inaccuracies we have investigated and provides a first quantitative assessment of the precision in the modelling of compact fluid binaries.
Ontology-based Representation and Reasoning on Process Models: A Logic Programming Approach<|sep|>We propose a framework grounded in Logic Programming for representing and reasoning about business processes from both the procedural and ontological point of views. In particular, our goal is threefold: (1) define a logical language and a formal semantics for process models enriched with ontology-based annotations; (2) provide an effective inference mechanism that supports the combination of reasoning services dealing with the structural definition of a process model, its behavior, and the domain knowledge related to the participating business entities; (3) implement such a theoretical framework into a process modeling and reasoning platform. To this end we define a process ontology coping with a relevant fragment of the popular BPMN modeling notation. The behavioral semantics of a process is defined as a state transition system by following an approach similar to the Fluent Calculus, and allows us to specify state change in terms of preconditions and effects of the enactment of activities. Then we show how the procedural process knowledge can be seamlessly integrated with the domain knowledge specified by using the OWL 2 RL rule-based ontology language. Our framework provides a wide range of reasoning services, including CTL model checking, which can be performed by using standard Logic Programming inference engines through a goal-oriented, efficient, sound and complete evaluation procedure. We also present a software environment implementing the proposed framework, and we report on an experimental evaluation of the system, whose results are encouraging and show the viability of the approach.
Nuclear recoil spectroscopy of levitated particles<|sep|>We propose a new method for the detection and characterization of nuclear decay processes. Specifically, we describe how nuclear decay recoil can be observed within small particles levitated in an optical trap with high positional resolution. Precise measurements of the magnitude of each recoil as well as their rate of occurrence can provide accurate information about the isotopic composition of a radioactive sample. We expect that this new technique for nuclear material characterization will be especially useful in the area of nuclear forensic analysis.
Discriminating cosmic muons and radioactivity using a liquid scintillation fiber detector<|sep|>In the case of underground experiments for neutrino physics or rare event searches, the background caused by cosmic muons contributes significantly and therefore must be identified and rejected. We proposed and optimized a new detector using liquid scintillator with wavelenghth-shifting fibers which can be employed as a veto detector for cosmic muons background rejection. From the prototype study, it has been found that the detector has good performances and is capable of discriminating between muons induced signals and environmental radiation background. Its muons detection efficiency is greater than 98$\%$, and on average, 58 photo-electrons (p.e.) are collected when a muon passes through the detector. To optimize the design and enhance the collection of light, the reflectivity of the coating materials has been studied in detail. A Monte Carlo simulation of the detector has been developed and compared to the performed measurements showing a good agreement between data and simulation results.
Survey of Security and Privacy Issues of Internet of Things<|sep|>This paper is a general survey of all the security issues existing in the Internet of Things (IoT) along with an analysis of the privacy issues that an end-user may face as a consequence of the spread of IoT. The majority of the survey is focused on the security loopholes arising out of the information exchange technologies used in Internet of Things. No countermeasure to the security drawbacks has been analyzed in the paper.
CRT-6D: Fast 6D Object Pose Estimation with Cascaded Refinement Transformers<|sep|>Learning based 6D object pose estimation methods rely on computing large intermediate pose representations and/or iteratively refining an initial estimation with a slow render-compare pipeline. This paper introduces a novel method we call Cascaded Pose Refinement Transformers, or CRT-6D. We replace the commonly used dense intermediate representation with a sparse set of features sampled from the feature pyramid we call OSKFs(Object Surface Keypoint Features) where each element corresponds to an object keypoint. We employ lightweight deformable transformers and chain them together to iteratively refine proposed poses over the sampled OSKFs. We achieve inference runtimes 2x faster than the closest real-time state of the art methods while supporting up to 21 objects on a single model. We demonstrate the effectiveness of CRT-6D by performing extensive experiments on the LM-O and YCBV datasets. Compared to real-time methods, we achieve state of the art on LM-O and YCB-V, falling slightly behind methods with inference runtimes one order of magnitude higher. The source code is available at: https://github.com/PedroCastro/CRT-6D
Monotone cubic spline interpolation for functions with a strong gradient<|sep|>Spline interpolation has been used in several applications due to its favorable properties regarding smoothness and accuracy of the interpolant. However, when there exists a discontinuity or a steep gradient in the data, some artifacts can appear due to the Gibbs phenomenon. Also, preservation of data monotonicity is a requirement in some applications, and that property is not automatically verified by the interpolator. In this paper, we study sufficient conditions to obtain monotone cubic splines based on Hermite cubic interpolators and propose different ways to construct them using non-linear formulas. The order of approximation, in each case, is calculated and several numerical experiments are performed to contrast the theoretical results.
Optimal Portfolio Choice for a Behavioural Investor in Continuous-Time Markets<|sep|>The aim of this work consists in the study of the optimal investment strategy for a behavioural investor, whose preference towards risk is described by both a probability distortion and an S-shaped utility function. Within a continuous-time financial market framework and assuming that asset prices are modelled by semimartingales, we derive sufficient and necessary conditions for the well-posedness of the optimisation problem in the case of piecewise-power probability distortion and utility functions. Finally, under straightforwardly verifiable conditions, we further demonstrate the existence of an optimal strategy.
Planck/SDSS Cluster Mass and Gas Scaling Relations for a Volume-Complete redMaPPer Sample<|sep|>Using Planck satellite data, we construct SZ gas pressure profiles for a large, volume-complete sample of optically selected clusters. We have defined a sample of over 8,000 redMaPPer clusters from the Sloan Digital Sky Survey (SDSS), within the volume-complete redshift region 0.100 < z < 0.325, for which we construct Sunyaev-Zel'dovich (SZ) effect maps by stacking Planck data over the full range of richness. Dividing the sample into richness bins we simultaneously solve for the mean cluster mass in each bin together with the corresponding radial pressure profile parameters, employing an MCMC analysis. These profiles are well detected over a much wider range of cluster mass and radius than previous work, showing a clear trend towards larger break radius with increasing cluster mass. Our SZ-based masses fall ~24% below the mass-richness relations from weak lensing, in a similar fashion as the "hydrostatic bias" related with X-ray derived masses. We correct for this bias to derive an optimal mass-richness relation finding a slope 1.22 +/- 0.04 and a pivot mass log(M_500/M_0)= 14.432 +/- 0.041, evaluated at a richness lambda=60. Finally, we derive a tight Y_500-M_500 relation over a wide range of cluster mass, with a power law slope equal to 1.72 +/- 0.07, that agrees well with the independent slope obtained by the Planck team with an SZ-selected cluster sample, but extends to lower masses with higher precision.
Greedy-layer Pruning: Speeding up Transformer Models for Natural Language Processing<|sep|>Fine-tuning transformer models after unsupervised pre-training reaches a very high performance on many different natural language processing tasks. Unfortunately, transformers suffer from long inference times which greatly increases costs in production. One possible solution is to use knowledge distillation, which solves this problem by transferring information from large teacher models to smaller student models. Knowledge distillation maintains high performance and reaches high compression rates, nevertheless, the size of the student model is fixed after pre-training and can not be changed individually for a given downstream task and use-case to reach a desired performance/speedup ratio. Another solution to reduce the size of models in a much more fine-grained and computationally cheaper fashion is to prune layers after the pre-training. The price to pay is that the performance of layer-wise pruning algorithms is not on par with state-of-the-art knowledge distillation methods. In this paper, Greedy-layer pruning is introduced to (1) outperform current state-of-the-art for layer-wise pruning, (2) close the performance gap when compared to knowledge distillation, while (3) providing a method to adapt the model size dynamically to reach a desired performance/speedup tradeoff without the need of additional pre-training phases. Our source code is available on https://github.com/deepopinion/greedy-layer-pruning.
The Reproducibility of Programming-Related Issues in Stack Overflow Questions<|sep|>Software developers often look for solutions to their code-level problems using the Stack Overflow Q&A website. To receive help, developers frequently submit questions containing sample code segments and the description of the programming issue. Unfortunately, it is not always possible to reproduce the issues from the code segments that may impede questions from receiving prompt and appropriate solutions. We conducted an exploratory study on the reproducibility of issues discussed in 400 Java and 400 Python questions. We parsed, compiled, executed, and carefully examined the code segments from these questions to reproduce the reported programming issues. The outcomes of our study are three-fold. First, we found that we can reproduce approximately 68% of Java and 71% of Python issues, whereas we were unable to reproduce approximately 22% of Java and 19% of Python issues using the code segments. Of the issues that were reproducible, approximately 67% of the Java code segments and 20% of the Python code segments required minor or major modifications to reproduce the issues. Second, we carefully investigated why programming issues could not be reproduced and provided evidence-based guidelines for writing effective code examples for Stack Overflow questions. Third, we investigated the correlation between the issue reproducibility status of questions and the corresponding answer meta-data, such as the presence of an accepted answer. According to our analysis, a reproducible question has at least two times higher chance of receiving an accepted answer than an irreproducible question. Besides, the median time delay in receiving accepted answers is double if the issues reported in questions could not be reproduced. We also investigate the confounding factors (e.g., reputation) and find that confounding factors do not hurt the correlation between reproducibility status and answer meta-data.
Intracluster stars in simulations with AGN feedback<|sep|>We use a set of high-resolution hydrodynamical simulations of clusters of galaxies to study the build-up of the intracluster light (ICL), an interesting and likely significant component of their total stellar mass. Our sample of groups and clusters includes AGN feedback and is of high enough resolution to accurately resolve galaxy populations down to the smallest galaxies that are expected to significantly contribute to the stellar mass budget. We describe and test four different methods to identify the ICL in simulations, thereby allowing us to assess the reliability of the measurements. For all of the methods, we consistently find a very significant ICL stellar fraction (~45%) which exceeds the values typically inferred from observations. However, we show that this result is robust with respect to numerical resolution and integration accuracy, remarkably insensitive to changes in the star formation model, and almost independent of halo mass. It is also almost invariant when black hole growth is included, even though AGN feedback successfully prevents excessive overcooling in clusters and leads to a drastically improved agreement of the simulated cluster galaxy population with observations. In particular, the luminosities of central galaxies and the ages of their stellar populations are much more realistic when including AGN. In the light of these findings, it appears challenging to construct a simulation model that simultaneously matches the cluster galaxy population and at the same time produces a low ICL component. We find that intracluster stars are preferentially stripped in a cluster's densest region from massive galaxies that fall into the cluster at z>1. Surprisingly, some of the intracluster stars also form in the intracluster medium inside cold gas clouds that are stripped out of infalling galaxies.
Typical representatives of free homotopy classes in a multi-punctured plane<|sep|>We show that a uniform probability measure supported on a specific set of piecewise linear loops in a non-trivial free homotopy class in a multi-punctured plane is overwhelmingly concentrated around loops of minimal lengths. Our approach is based on extending Mogulskii's theorem to closed paths, which is a useful result of independent interest. In addition, we show that the above measure can be sampled using standard Markov Chain Monte Carlo techniques, thus providing a simple methods for approximating shortest loops.
Efficient Estimation of Compressible State-Space Models with Application to Calcium Signal Deconvolution<|sep|>In this paper, we consider linear state-space models with compressible innovations and convergent transition matrices in order to model spatiotemporally sparse transient events. We perform parameter and state estimation using a dynamic compressed sensing framework and develop an efficient solution consisting of two nested Expectation-Maximization (EM) algorithms. Under suitable sparsity assumptions on the innovations, we prove recovery guarantees and derive confidence bounds for the state estimates. We provide simulation studies as well as application to spike deconvolution from calcium imaging data which verify our theoretical results and show significant improvement over existing algorithms.
Robustness of Structurally Equivalent Concurrent Parity Games<|sep|>We consider two-player stochastic games played on a finite state space for an infinite number of rounds. The games are concurrent: in each round, the two players (player 1 and player 2) choose their moves independently and simultaneously; the current state and the two moves determine a probability distribution over the successor states. We also consider the important special case of turn-based stochastic games where players make moves in turns, rather than concurrently. We study concurrent games with \omega-regular winning conditions specified as parity objectives. The value for player 1 for a parity objective is the maximal probability with which the player can guarantee the satisfaction of the objective against all strategies of the opponent. We study the problem of continuity and robustness of the value function in concurrent and turn-based stochastic parity gameswith respect to imprecision in the transition probabilities. We present quantitative bounds on the difference of the value function (in terms of the imprecision of the transition probabilities) and show the value continuity for structurally equivalent concurrent games (two games are structurally equivalent if the support of the transition function is same and the probabilities differ). We also show robustness of optimal strategies for structurally equivalent turn-based stochastic parity games. Finally we show that the value continuity property breaks without the structurally equivalent assumption (even for Markov chains) and show that our quantitative bound is asymptotically optimal. Hence our results are tight (the assumption is both necessary and sufficient) and optimal (our quantitative bound is asymptotically optimal).
Space-Time Models based on Random Fields with Local Interactions<|sep|>The analysis of space-time data from complex, real-life phenomena requires the use of flexible and physically motivated covariance functions. In most cases, it is not possible to explicitly solve the equations of motion for the fields or the respective covariance functions. In the statistical literature, covariance functions are often based on mathematical constructions. We propose deriving space-time covariance functions by solving "effective equations of motion", which can be used as statistical representations of systems with diffusive behavior. In particular, we propose using the linear response theory to formulate space-time covariance functions based on an equilibrium effective Hamiltonian. The effective space-time dynamics are then generated by a stochastic perturbation around the equilibrium point of the classical field Hamiltonian leading to an associated Langevin equation. We employ a Hamiltonian which extends the classical Gaussian field theory by including a curvature term and leads to a diffusive Langevin equation. Finally, we derive new forms of space-time covariance functions.
Entanglement and Extreme Spin Squeezing for a Fluctuating Number of Indistinguishable Particles<|sep|>We extend the criteria for $k$-particle entanglement from the spin squeezing parameter presented in [A.S. S{\o}rensen and K. M{\o}lmer, Phys. Rev. Lett. {\bf 86}, 4431 (2001)] to systems with a fluctating number of particles. We also discuss how other spin squeezing inequalities can be generalized to this situation. Further, we give an operational meaning to the bounds for cases where the individual particles cannot be addressed. As a by-product, this allows us to show that in spin squeezing experiments with cold gases the particles are typically distinguishable in practise. Our results justify the application of the S{\o}rensen-M{\o}lmer bounds in recent experiments on spin squeezing in Bose-Einstein condensates.
ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation<|sep|>Video frame interpolation (VFI) is currently a very active research topic, with applications spanning computer vision, post production and video encoding. VFI can be extremely challenging, particularly in sequences containing large motions, occlusions or dynamic textures, where existing approaches fail to offer perceptually robust interpolation performance. In this context, we present a novel deep learning based VFI method, ST-MFNet, based on a Spatio-Temporal Multi-Flow architecture. ST-MFNet employs a new multi-scale multi-flow predictor to estimate many-to-one intermediate flows, which are combined with conventional one-to-one optical flows to capture both large and complex motions. In order to enhance interpolation performance for various textures, a 3D CNN is also employed to model the content dynamics over an extended temporal window. Moreover, ST-MFNet has been trained within an ST-GAN framework, which was originally developed for texture synthesis, with the aim of further improving perceptual interpolation quality. Our approach has been comprehensively evaluated -- compared with fourteen state-of-the-art VFI algorithms -- clearly demonstrating that ST-MFNet consistently outperforms these benchmarks on varied and representative test datasets, with significant gains up to 1.09dB in PSNR for cases including large motions and dynamic textures. Project page: https://danielism97.github.io/ST-MFNet.
Diagrams of States in Quantum Information: an Illustrative Tutorial<|sep|>We present "Diagrams of States", a way to graphically represent and analyze how quantum information is elaborated during the execution of quantum circuits. This introductory tutorial illustrates the basics, providing useful examples of quantum computations: elementary operations in single-qubit, two-qubit and three-qubit systems, immersions of gates on higher dimensional spaces, generation of single and multi-qubit states, procedures to synthesize unitary, controlled and diagonal matrices. To perform the analysis of quantum processes, we directly derive diagrams of states from physical implementations of quantum circuits associated to the processes. Complete diagrams are then rearranged into simplified diagrams, to visualize the overall effects of computations. Conversely, diagrams of states help to conceive new quantum algorithms, by schematically describing desired manipulations of quantum information with intuitive diagrams and then by guessing the equivalent complete diagrams, from which the corresponding quantum circuit is obtained effortlessly. Related examples and analysis of complex algorithms will be provided in future works, for whose comprehension this first tutorial offers the necessary introduction.
3D Gaze Estimation from 2D Pupil Positions on Monocular Head-Mounted Eye Trackers<|sep|>3D gaze information is important for scene-centric attention analysis but accurate estimation and analysis of 3D gaze in real-world environments remains challenging. We present a novel 3D gaze estimation method for monocular head-mounted eye trackers. In contrast to previous work, our method does not aim to infer 3D eyeball poses but directly maps 2D pupil positions to 3D gaze directions in scene camera coordinate space. We first provide a detailed discussion of the 3D gaze estimation task and summarize different methods, including our own. We then evaluate the performance of different 3D gaze estimation approaches using both simulated and real data. Through experimental validation, we demonstrate the effectiveness of our method in reducing parallax error, and we identify research challenges for the design of 3D calibration procedures.
On the Complexity of Learning from Label Proportions<|sep|>In the problem of learning with label proportions, which we call LLP learning, the training data is unlabeled, and only the proportions of examples receiving each label are given. The goal is to learn a hypothesis that predicts the proportions of labels on the distribution underlying the sample. This model of learning is applicable to a wide variety of settings, including predicting the number of votes for candidates in political elections from polls. In this paper, we formally define this class and resolve foundational questions regarding the computational complexity of LLP and characterize its relationship to PAC learning. Among our results, we show, perhaps surprisingly, that for finite VC classes what can be efficiently LLP learned is a strict subset of what can be leaned efficiently in PAC, under standard complexity assumptions. We also show that there exist classes of functions whose learnability in LLP is independent of ZFC, the standard set theoretic axioms. This implies that LLP learning cannot be easily characterized (like PAC by VC dimension).
Time correlations and persistence probability of a Brownian particle in a shear flow<|sep|>In this article, results have been presented for the two-time correlation functions for a free and a harmonically confined Brownian particle in a simple shear flow. For a free Brownian particle, the motion along the direction of shear exhibit two distinct dynamics, with the mean-square-displacement being diffusive at short times while at late times scales as $t^3$. In contrast the cross-correlation $\la x(t) y(t) \ra $ scales quadratically for all times. In the case of a harmonically trapped Brownian particle, the mean-square-displacement exhibits a plateau determined by the strength of the confinement and the shear. Further, the analysis is extended to a chain of Brownian particles interacting via a harmonic and a bending potential. Finally, the persistence probability is constructed from the two-time correlation functions.
Generalised diffeomorphisms for E$_9$<|sep|>We construct generalised diffeomorphisms for E$_9$ exceptional field theory. The transformations, which like in the E$_8$ case contain constrained local transformations, close when acting on fields. This is the first example of a generalised diffeomorphism algebra based on an infinite-dimensional Lie algebra and an infinite-dimensional coordinate module. As a byproduct, we give a simple generic expression for the invariant tensors used in any extended geometry. We perform a generalised Scherk--Schwarz reduction and verify that our transformations reproduce the structure of gauged supergravity in two dimensions. The results are valid also for other affine algebras.
Definite Non-Ancestral Relations and Structure Learning<|sep|>In causal graphical models based on directed acyclic graphs (DAGs), directed paths represent causal pathways between the corresponding variables. The variable at the beginning of such a path is referred to as an ancestor of the variable at the end of the path. Ancestral relations between variables play an important role in causal modeling. In existing literature on structure learning, these relations are usually deduced from learned structures and used for orienting edges or formulating constraints of the space of possible DAGs. However, they are usually not posed as immediate target of inference. In this work we investigate the graphical characterization of ancestral relations via CPDAGs and d-separation relations. We propose a framework that can learn definite non-ancestral relations without first learning the skeleton. This frame-work yields structural information that can be used in both score- and constraint-based algorithms to learn causal DAGs more efficiently.
Mechanical coupling effects of 2D lattices uncovered by decoupled micropolar elasticity tensor and symmetry operation<|sep|>Mechanical couplings such as axial-shear and axial-bending have great potential in the design of active mechanical metamaterials with directional control of input and output loads in sensors and actuators. However, the current ad hoc design of mechanical coupling without theoretical support of elasticity cannot provide design guidelines for mechanical coupling with lattice geometries. Moreover, the correlation between mechanical coupling effects and geometric symmetry is not yet clearly understood. In this work, we systematically search for all possible mechanical couplings in 2D lattice structures by determining the non-zero diagonal terms in the decomposed micropolar elasticity tensor. We also correlate the mechanical couplings with the point-group symmetry of 2D lattices by applying the symmetry operation to the decomposed micropolar elasticity tensor. The decoupled micropolar constitutive equation uncovers eight coupling effects for 2D lattice structures. The symmetry operation of the decoupled micropolar elasticity tensor reveals the correlation of the mechanical coupling with the point groups. Our findings can strengthen the design of mechanical metamaterials with potential applications in areas including sensors, actuators, soft robots, and active metamaterials for elastic/acoustic wave guidance and thermal management.
Quaternionic Representation of Snub 24-Cell and its Dual Polytope Derived From E_8 Root System<|sep|>Vertices of the 4-dimensional semi-regular polytope, \textit{snub 24-cell} and its symmetry group $W(D_{4}):C_{3} $ of order 576 are represented in terms of quaternions with unit norm. It follows from the icosian representation of \textbf{$E_{8} $} root system. The quaternionic root system of $H_{4} $ splits as the vertices of 24-cell and the \textit{snub 24-cell} under the symmetry group of the \textit{snub 24-cell} which is one of the maximal subgroups of the group \textbf{$W(H_{4})$} as well as $W(F_{4})$. It is noted that the group is isomorphic to the\textbf{}semi-direct product of the Weyl group of $D_{4}$ with the cyclic group of order 3 denoted by $W(D_{4}):C_{3} $, the Coxeter notation for which is $[3,4,3^{+}]$. We analyze the vertex structure of the \textit{snub 24-cell} and decompose the orbits of \textbf{$W(H_{4})$} under the orbits of $W(D_{4}):C_{3} $. The cell structure of the snub 24-cell has been explicitly analyzed with quaternions by using the subgroups of the group $W(D_{4}):C_{3} $. In particular, it has been shown that the dual polytopes 600-cell with 120 vertices and 120-cell with 600 vertices decompose as 120=24+96 and 600=24+96+192+288 respectively under the group $W(D_{4}):C_{3} $. The dual polytope of the \textit{snub 24-cell} is explicitly constructed. Decompositions of the Archimedean $W(H_{4})$ polytopes under the symmetry of the group $W(D_{4}):C_{3} $ are given in the appendix.
Avalanche frontiers in dissipative abelian sandpile model as off-critical SLE(2)<|sep|>Avalanche frontiers in Abelian Sandpile Model (ASM) are random simple curves whose continuum limit is known to be a Schramm-Loewner Evolution (SLE) with diffusivity parameter $\kappa = 2$. In this paper we consider the dissipative ASM and study the statistics of the avalanche and wave frontiers for various rates of dissipation. We examine the scaling behavior of a number of functions such as the correlation length, the exponent of distribution function of loop lengths and gyration radius defined for waves and avalanches. We find that they do scale with the rate of dissipation. Two significant length scales are observed. For length scales much smaller than the correlation length, these curves show properties close to the critical curves and the corresponding diffusivity parameter is nearly the same as the critical limit. We interpret this as the ultra violet (UV) limit where $\kappa = 2$ corresponding to $c=-2$. For length scales much larger than the correlation length we find that the avalanche frontiers tend to Self-Avoiding Walk, the corresponding driving function is proportional to the Brownian motion with the diffusion parameter $\kappa =8/3$ corresponding to a field theory with $c = 0$. This is the infra red (IR) limit. Correspondingly the central charge decreases from the IR to the UV point.
Scaling Multi-Domain Dialogue State Tracking via Query Reformulation<|sep|>We present a novel approach to dialogue state tracking and referring expression resolution tasks. Successful contextual understanding of multi-turn spoken dialogues requires resolving referring expressions across turns and tracking the entities relevant to the conversation across turns. Tracking conversational state is particularly challenging in a multi-domain scenario when there exist multiple spoken language understanding (SLU) sub-systems, and each SLU sub-system operates on its domain-specific meaning representation. While previous approaches have addressed the disparate schema issue by learning candidate transformations of the meaning representation, in this paper, we instead model the reference resolution as a dialogue context-aware user query reformulation task -- the dialog state is serialized to a sequence of natural language tokens representing the conversation. We develop our model for query reformulation using a pointer-generator network and a novel multi-task learning setup. In our experiments, we show a significant improvement in absolute F1 on an internal as well as a, soon to be released, public benchmark respectively.
Manifold Alignment Determination: finding correspondences across different data views<|sep|>We present Manifold Alignment Determination (MAD), an algorithm for learning alignments between data points from multiple views or modalities. The approach is capable of learning correspondences between views as well as correspondences between individual data-points. The proposed method requires only a few aligned examples from which it is capable to recover a global alignment through a probabilistic model. The strong, yet flexible regularization provided by the generative model is sufficient to align the views. We provide experiments on both synthetic and real data to highlight the benefit of the proposed approach.
A Surprising Reversal of Temperatures in the Brown-Dwarf Eclipsing Binary 2MASS J05352184-0546085<|sep|>The newly discovered brown-dwarf eclipsing binary 2MASS J05352184-0546085 provides a unique laboratory for testing the predictions of theoretical models of brown-dwarf formation and evolution. The finding that the lower-mass brown dwarf in this system is hotter than its higher-mass companion represents a challenge to brown-dwarf evolutionary models, none of which predict this behavior. Here we present updated determinations of the basic physical properties of 2M0535-05, bolstering the surprising reversal of temperatures with mass in this system. We compare these measurements with widely used brown-dwarf evolutionary tracks, and find that the temperature reversal can be explained by some models if the components of 2M0535-05 are mildly non-coeval, possibly consistent with dynamical simulations of brown-dwarf formation. Alternatively, a strong magnetic field on the higher-mass brown dwarf might explain its anomalously low surface temperature, consistent with emerging evidence that convection is suppressed in magnetically active, low-mass stars. Finally, we discuss future observational and theoretical work needed to further characterize and understand this benchmark system.
Non-resonant n = 1 helical core induced by m/n = 2/1 tearing mode in JT-60U<|sep|>In JT-60U, simultaneous excitation of n = 1 helical cores (HCs) and m/n = 2/1 Tearing Modes (TMs) was observed [T. Bando et al., Plasma Phys. Control. Fusion 61 115014 (2019)]. In this paper, we have investigated the excitation mechanism of n = 1 HCs with m/n = 2/1 TMs based on the experimental observations and a simple quasi-linear MHD model. In the previous study, it was reported that a "coupling" on the phase of the MHD mode is observed between n = 1 HCs and m/n = 2/1 TMs. In this study, it is found that the coupling is observed with the mode frequency from several Hz to 6 kHz. This indicates that the resistive wall and the plasma control system do not induce the coupling because the both time scales are different from the mode frequency. In addition, n = 1 HCs appear to be the non-resonant mode from the two observations: n = 1 HCs do not rotate with the plasma around the q = 1 surface in the core and the coupling is also observed even when qmin > 1. It is also observed that the electron fluctuation due to an n = 1 HC in the core region disappears with the stabilization of an m/n = 2/1 neoclassical tearing mode by electron cyclotron current drive, implying that n = 1 HCs are driven by m/n = 2/1 TMs. This perspective, n = 1 HCs are driven by m/n = 2/1 TMs, is supported by the observation that the saturated amplitude of the m/n = 1/1 component of the radial displacement in the core is smaller than that of the m/n = 2/1 component. Finally, we revisit a quasi-linear MHD model where the m/n = 1/1 HC is induced directly by the sideband of the current for the m/n = 2/1 TM, which allows to excite the non-resonant m/n = 1/1 mode. The model also describes the characteristic of the coupling, fm/n=1/1(HC) = 2fm/n=2/1(TM).
A practical method for computing with piecewise Chebyshevian splines<|sep|>A piecewise Chebyshevian spline space is good for design when it possesses a B-spline basis and this property is preserved under knot insertion. The interest in such kind of spaces is justified by the fact that, similarly as for polynomial splines, the related parametric curves exhibit the desired properties of convex hull inclusion, variation diminution and intuitive relation between the curve shape and the location of the control points. For a good-for-design space, in this paper we construct a set of functions, called transition functions, which allow for efficient computation of the B-spline basis, even in the case of nonuniform and multiple knots. Moreover, we show how the spline coefficients of the representations associated with a refined knot partition and with a raised order can conveniently be expressed by means of transition functions. This result allows us to provide effective procedures that generalize the classical knot insertion and degree raising algorithms for polynomial splines. We further discuss how the approach can straightforwardly be generalized to deal with geometrically continuous piecewise Chebyshevian splines as well as with splines having section spaces of different dimensions. From a numerical point of view, we show that the proposed evaluation method is easier to implement and has higher accuracy than other existing algorithms.
Robust Wasserstein Profile Inference and Applications to Machine Learning<|sep|>We show that several machine learning estimators, including square-root LASSO (Least Absolute Shrinkage and Selection) and regularized logistic regression can be represented as solutions to distributionally robust optimization (DRO) problems. The associated uncertainty regions are based on suitably defined Wasserstein distances. Hence, our representations allow us to view regularization as a result of introducing an artificial adversary that perturbs the empirical distribution to account for out-of-sample effects in loss estimation. In addition, we introduce RWPI (Robust Wasserstein Profile Inference), a novel inference methodology which extends the use of methods inspired by Empirical Likelihood to the setting of optimal transport costs (of which Wasserstein distances are a particular case). We use RWPI to show how to optimally select the size of uncertainty regions, and as a consequence, we are able to choose regularization parameters for these machine learning estimators without the use of cross validation. Numerical experiments are also given to validate our theoretical findings.
Relative Entropy for Fermionic Quantum Field Theory<|sep|>We study the relative entropy, in the sense of Araki, for the representation of a self-dual CAR algebra $\mathfrak{A}_{SDC}(\mathcal{H},\Gamma)$. We notice, for a specific choice of $f \in \mathcal{H}$, that the associated element in $\mathfrak{A}_{SDC}(\mathcal{H},\Gamma)$ is unitary. As a consequence, we explicitly compute the relative entropy between a quasifree state over $\mathfrak{A}_{SDC}(\mathcal{H},\Gamma)$ and an excitation of it with respect to the abovely mentioned unitary element. The generality of the approach, allows us to consider $\mathcal{H}$ as the Hilbert space of solutions of the classical Dirac equation over globally hyperbolic spacetimes, making our result, a computation of relative entropy for a Fermionic Quantum Field Theory. Our result extends those of Longo and Casini et al. for the relative entropy between a quasifree state and a coherent excitation for a free Scalar Quantum Field Theory, to the case of fermions. As a first application, we computed such a relative entropy for a Majorana field on an ultrastatic spacetime.
On Supervised Selection of Bayesian Networks<|sep|>Given a set of possible models (e.g., Bayesian network structures) and a data sample, in the unsupervised model selection problem the task is to choose the most accurate model with respect to the domain joint probability distribution. In contrast to this, in supervised model selection it is a priori known that the chosen model will be used in the future for prediction tasks involving more ``focused' predictive distributions. Although focused predictive distributions can be produced from the joint probability distribution by marginalization, in practice the best model in the unsupervised sense does not necessarily perform well in supervised domains. In particular, the standard marginal likelihood score is a criterion for the unsupervised task, and, although frequently used for supervised model selection also, does not perform well in such tasks. In this paper we study the performance of the marginal likelihood score empirically in supervised Bayesian network selection tasks by using a large number of publicly available classification data sets, and compare the results to those obtained by alternative model selection criteria, including empirical crossvalidation methods, an approximation of a supervised marginal likelihood measure, and a supervised version of Dawids prequential(predictive sequential) principle.The results demonstrate that the marginal likelihood score does NOT perform well FOR supervised model selection, WHILE the best results are obtained BY using Dawids prequential r napproach.
RelWalk A Latent Variable Model Approach to Knowledge Graph Embedding<|sep|>Embedding entities and relations of a knowledge graph in a low-dimensional space has shown impressive performance in predicting missing links between entities. Although progresses have been achieved, existing methods are heuristically motivated and theoretical understanding of such embeddings is comparatively underdeveloped. This paper extends the random walk model (Arora et al., 2016a) of word embeddings to Knowledge Graph Embeddings (KGEs) to derive a scoring function that evaluates the strength of a relation R between two entities h (head) and t (tail). Moreover, we show that marginal loss minimisation, a popular objective used in much prior work in KGE, follows naturally from the log-likelihood ratio maximisation under the probabilities estimated from the KGEs according to our theoretical relationship. We propose a learning objective motivated by the theoretical analysis to learn KGEs from a given knowledge graph. Using the derived objective, accurate KGEs are learnt from FB15K237 and WN18RR benchmark datasets, providing empirical evidence in support of the theory.
Additive Approximations in High Dimensional Nonparametric Regression via the SALSA<|sep|>High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of \emph{first order}, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose SALSA, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. SALSA minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on $15$ real datasets, we show that our method is competitive against $21$ other alternatives.
On the uniqueness of invariant states<|sep|>Given an abelian group G endowed with a T-pre-symplectic form, we assign to it a symplectic twisted group *-algebra W_G and then we provide criteria for the uniqueness of states invariant under the ergodic action of the symplectic group of automorphism. As an application, we discuss the notion of natural states in quantum abelian Chern-Simons theory.
Modeling an accretion disc stochastical variability<|sep|>Hot spots residing on the surface of an accretion disc have been considered as a model of short-term variability of active galactic nuclei. In this paper we apply the theory of random point processes to model the observed signal from an ensemble of randomly generated spots. The influence of general relativistic effects near a black hole is taken into account and it is shown that typical features of power spectral density can be reproduced. Connection among spots is also discussed in terms of Hawkes' process, which produces more power at low frequencies. We derive a semi-analytical way to approximate the resulting power-spectral density.
Normal state electronic properties of LaO$_{1-x}$F$_{x}$BiS$_{2}$ superconductors<|sep|>A good description of the electronic structure of BiS$_{2}$-based superconductors is essential to understand their phase diagram, normal state and superconducting properties. To describe the first reports of normal state electronic structure features from angle resolved photoemission spectroscopy (ARPES) in LaO$_{1-x}$F$_{x}$BiS$_{2}$, we used a minimal microscopic model to study their low energy properties. It includes the two effective tight-binding bands proposed by Usui et al [Phys.Rev.B 86, 220501(R)(2012)], and we added moderate intra- and inter-orbital electron correlations related to Bi-($p_{Y}$, $p_{X}$) and S-($p_{Y}$, $p_{X}$) orbitals. We calculated the electron Green's functions using their equations of motion, which we decoupled in second-order of perturbations on the correlations. We determined the normal state spectral density function and total density of states for LaO$_{1-x}$F$_{x}$BiS$_{2}$, focusing on the description of the k-dependence, effect of doping, and the prediction of the temperature dependence of spectral properties. Including moderate electron correlations, improves the description of the few experimental ARPES and soft X-ray photoemission data available for LaO$_{1-x}$F$_{x}$BiS$_{2}$. Our analytical approximation enabled us to calculate the spectral density around the conduction band minimum at $\vec{k}_{0}=(0.45\pi,0.45\pi)$, and to predict the temperature dependence of the spectral properties at different BZ points, which might be verified by temperature dependent ARPES.
Multiple Partitioning of Multiplex Signed Networks: Application to European Parliament Votes<|sep|>For more than a decade, graphs have been used to model the voting behavior taking place in parliaments. However, the methods described in the literature suffer from several limitations. The two main ones are that 1) they rely on some temporal integration of the raw data, which causes some information loss, and/or 2) they identify groups of antagonistic voters, but not the context associated to their occurrence. In this article, we propose a novel method taking advantage of multiplex signed graphs to solve both these issues. It consists in first partitioning separately each layer, before grouping these partitions by similarity. We show the interest of our approach by applying it to a European Parliament dataset.
Weakly supervised spoken term discovery using cross-lingual side information<|sep|>Recent work on unsupervised term discovery (UTD) aims to identify and cluster repeated word-like units from audio alone. These systems are promising for some very low-resource languages where transcribed audio is unavailable, or where no written form of the language exists. However, in some cases it may still be feasible (e.g., through crowdsourcing) to obtain (possibly noisy) text translations of the audio. If so, this information could be used as a source of side information to improve UTD. Here, we present a simple method for rescoring the output of a UTD system using text translations, and test it on a corpus of Spanish audio with English translations. We show that it greatly improves the average precision of the results over a wide range of system configurations and data preprocessing methods.
A stochastic root finding approach: The Homotopy Analysis Method applied to Dyson-Schwinger Equations<|sep|>We present the construction and stochastic summation of rooted-tree diagrams, based on the expansion of a root finding algorithm applied to the Dyson-Schwinger equations (DSEs). The mathematical formulation shows superior convergence properties compared to the bold diagrammatic Monte Carlo approach and the developed algorithm allows one to tackle generic high-dimensional integral equations, to avoid the curse of dealing explicitly with high-dimensional objects and to access non-perturbative regimes. The sign problem remains the limiting factor, but it is not found to be worse than in other approaches. We illustrate the method for $\phi^4$ theory but note that it applies in principle to any model.
Quasi-probabilities of work and heat in an open quantum system<|sep|>We discuss an approach to determine averages of the work, dissipated heat and variation of internal energy of an open quantum system driven by an external classical field. These quantities are measured by coupling the quantum system to a quantum detector at different times. This approach allows us to preserve the full quantum features of the evolution. From the measured phase, we are able to obtain a quasi-characteristic function and a quasi-probability density function for the corresponding observables. Despite the fact that these quasi-probability density functions are not the results of direct measurements, they reproduce the expected value of the physical quantities. Analogously to the Wigner function, the negative regions of these quasi-probability density functions are directly related to pure quantum processes which are not interpretable in classical terms. We use this feature to show that in the limit of strong dissipation, the quantum features vanish and interpret this as the emergence of the classical limit of the energy exchange process. Our analysis explains and confirms the behavior observed in recent experiments performed on IBMQ devices [1]. The possibility to discriminate between classical and quantum features makes the proposed approach an excellent tool to determine if, and in which conditions, quantum effects can be exploited to increase the efficiency in an energy exchange process at the quantum level.
Neutrino diffusive transport in hot quark matter: a detailed analysis<|sep|>We perform an extensive analysis of neutrino diffusion in quark matter within the MIT bag model at arbitrary temperature and degeneracy. We examine in detail the contribution of each of the relevant weak interaction processes to the total neutrino opacity and evaluate the effect of the strange quark mass, the bag constant, and the QCD perturbative corrections to the MIT bag model. We also investigate the anisotropic contribution to the neutrino distribution function in scatterings, the mean energy transfer and the mean scattering angle. The density and temperature dependence of the diffusion coefficients $D_n$ that govern the cooling and deleptonization of a compact star is shown in detail. Finally, our numerical results for the neutrino mean free paths are compared against known analytic approximations. We conclude that neutrino scattering constitutes a significant portion of the total neutrino opacity in leptonized quark matter and neutrino-quark scattering is, in general, very similar to neutrino-electron scattering with respect to both mean energy transfer per scattering and mean scattering angle.
Transmission and conversion of magnetoacoustic waves on the magnetic canopy in a quiet Sun region<|sep|>We present evidence for the conversion and transmission of wave modes on the magnetic flux tubes that constitute mottles and form the magnetic canopy in a quiet Sun region, highlighting the details and key parameters of the mechanism that produces power halos and magnetic shadows at the magnetic network observed in H{\alpha}. We use calculations of the magnetic field vector and the height of the magnetic canopy and simple assumptions to determine the turning height, i.e., the height at which the fast magneto-acoustic waves reflect. We compare the variation of acoustic power in the magnetic shadow and the power halo with the results of a two-dimensional model on mode conversion and transmission. The key parameter of the model is the attack angle, which is related to the inclination of the magnetic field vector at the canopy height. Our analysis takes also into account that 1) there are projection effects on the propagation of waves, 2) the magnetic canopy and the turning height are curved layers, 3) waves with periods longer than 3 min reach the chromosphere in the presence of inclined magnetic fields (ramp effect), 4) mottles are canopy structures, and 5) the wings of H{\alpha} contain mixed signal from low- and high-{\beta} plasma. The dependence of power on the attack angle follows the anticipated by the two-dimensional model very well. Long-period slow waves are channeled to the upper chromospheric layers following the magnetic field lines of mottles, while short-period fast waves penetrate the magnetic canopy and reflect at the turning height. Although both magnetoacoustic modes contribute to velocity signals, making the interpretation of observations a challenging task, we conclude that conversion and transmission of the acoustic waves into fast and slow magnetoacoustic waves are responsible for forming power halos and magnetic shadows in the quiet Sun region.
World-volume Effective Action of Exotic Five-brane in M-theory<|sep|>We study the world-volume effective action of an exotic five-brane, known as the M-theory 5${}^3$-brane (M5${}^3$-brane) in eleven dimensions. The supermultiplet of the world-volume theory is the $\mathcal{N} = (2, 0)$ tensor multiplet in six dimensions. The world-volume action contains three Killing vectors $\hat{k}_{\hat{I}} {}^M \ (\hat{I} =1,2,3)$ associated with the $U(1)^3$ isometry. We find the effective T-duality rule for the eleven-dimensional backgrounds that transforms the M5-brane effective action to that of the M5${}^3$-brane. We also show that our action provides the source term for the M5${}^3$-brane geometry in eleven-dimensional supergravity
Expectation on probing the origin of the cosmic ray knee with the LHAASO experiment<|sep|>The cosmic-ray (CR) knee and the compositions contain abundant information for probing the CR's origin, acceleration and propagation mechanisms, as well as the frontier of the fundamental physics. Realizing that major proposals toward the knee's shape can be divided into two categories: the rigidity-dependent (also Z-dependent) knee and the mass-dependent (also A-dependent) knee, where the former one relates to the acceleration or the propagation mechanisms, and the other one is often associated with the new physics, it is essential to precisely measure the individual compositions. Benefit from the high altitude and hybrid detection methods, the LHAASO experiment has the ability in determining the individual component and brings us an opportunity in discriminating these two models. We test this expected ability of LHAASO from 100 TeV to 10 PeV with 3-year observation. And find the dominant component at the knee is essential to this issue, while much heavier nuclei occupying the knee leads to higher significance. In the analysis, the He-dominant knee under the A-dependent case can be recognized at the significance about 6.6 $\sigma$, while the P-dominant knee under the Z-dependent case will be classified with 2 $\sigma$ significance.
X-Ray Emission from a Supermassive Black Hole Ejected from the Center of a Galaxy<|sep|>Recent studies have indicated that the emission of gravitational waves at the merger of two black holes gives a kick to the final black hole. If the supermassive black hole at the center of a disk galaxy is kicked but the velocity is not large enough to escape from the host galaxy, it will fall back onto the the disk and accrete the interstellar medium in the disk. We study the X-ray emission from the black holes with masses of ~10^7 M_sun recoiled from the galactic center with velocities of ~600 km s^-1. We find that their luminosities can reach ~>10^39 erg s^-1, when they pass the apastrons in the disk. While the X-ray luminosities are comparable to those of ultra-luminous X-ray sources (ULXs) observed in disk galaxies, ULXs observed so far do not seem to be such supermassive black holes. Statical studies could constrain the probability of merger and recoil of supermassive black holes.
Formulation of small-strain magneto-elastic problems<|sep|>Despite of the topical engineering need and all scientific investments, the mathematical formulation of modeling elastic deformations in magnetic systems is not yet fully established. Often, especially in electrical engineering applications, a model assuming small (infinitesimal) strains seems sufficient. To express such small-strain magneto-elastic problems in a suitable form for discretization methods, we present here a formulation in the framework of differential geometry. The given analysis shows algebraic similarity between small-strain elasticity and magnetism. This suggests that a class of magnetic, elastic, and magneto-elastic problems may be modeled in the same algebraic category, constituting suitable domain for discretizations.
Electric-Dipole Effect of Defects on Energy Band Alignment of Rutile and Anatase TiO2<|sep|>Titanium dioxide materials have been studied intensively and extensively due to photocatalytic applications. A long-standing open question is the energy band alignment of rutile and anatase TiO2 phases, which can affect the photocatalytic process in the composite system. There are basically two contradictory viewpoints about the alignment of these two TiO2 phases supported by respective experiments: 1) straddling type and 2) staggered type. In this work, our DFT plus U calculations find that the perfect rutile (110) and anatase (101) surfaces have the straddling type band alignment, whereas the surfaces with defects can turn the band alignment into the staggered type. The electric dipoles induced by defects are responsible for the reversal of band alignment. Thus the defects introduced during preparations and post-treatment processes of materials are probably the answer to above open question regarding the band alignment, which can be considered in real practice to tune the photocatalytic activity of materials.
Optimal Scheduling of Electric Vehicles Charging in low-Voltage Distribution Systems<|sep|>Uncoordinated charging of large-scale electric vehicles (EVs) will have a negative impact on the secure and economic operation of the power system, especially at the distribution level. Given that the charging load of EVs can be controlled to some extent, research on the optimal charging control of EVs has been extensively carried out. In this paper, two possible smart charging scenarios in China are studied: centralized optimal charging operated by an aggregator and decentralized optimal charging managed by individual users. Under the assumption that the aggregators and individual users only concern the economic benefits, new load peaks will arise under time of use (TOU) pricing which is extensively employed in China. To solve this problem, a simple incentive mechanism is proposed for centralized optimal charging while a rolling-update pricing scheme is devised for decentralized optimal charging. The original optimal charging models are modified to account for the developed schemes. Simulated tests corroborate the efficacy of optimal scheduling for charging EVs in various scenarios.
Plane Partition Realization of (Web of) W-algebra Minimal Models<|sep|>Recently, Gaiotto and Rapcak (GR) proposed a new family of the vertex operator algebra (VOA) as the symmetry appearing at an intersection of five-branes to which they refer as Y algebra. Prochazka and Rapcak, then proposed to interpret Y algebra as a truncation of affine Yangian whose module is directly connected to plane partitions (PP). They also developed GR's idea to generate a new VOA by connecting plane partitions through an infinite leg shared by them and referred it as the web of W-algebra (WoW). In this paper, we demonstrate that double truncation of PP gives the minimal models of such VOAs. For a single PP, it generates all the minimal model irreducible representations of W-algebra. We find that the rule connecting two PPs is more involved than those in the literature when the U(1) charge connecting two PPs is negative. For the simplest nontrivial WoW, N=2 superconformal algebra, we demonstrate that the improved rule precisely reproduces the known character of the minimal models.
Constructing Quantum Logic Gates Using q-Deformed Harmonic Oscillator Algebras<|sep|>We study two-level q-deformed angular momentum states and us- ing q-deformed harmonic oscillators, we provide a framework for con- structing qubits and quantum gates. We also present the construction of some basic quantum gates including CNOT, SWAP, Toffoli and Fredkin.
Which Pomeron survives at LHC energies?<|sep|>An eikonalized elastic proton-proton and proton-antiproton scattering amplitude, based on the suggestion of a finite sum of ladder diagrams, calculated from QCD and the number of $s-$channel gluon rungs and correspondingly the powers of logarithms in total cross section depends on available increasing energy. Explicit expressions for total cross section involving three and four rungs (four and five prongs), as highest terms, respectively) are fitted to the all available proton-proton and proton-antiproton total cross section data. Predictions for pp total cross section at LHC energy are given and compared with prediction of several Regge-models including possible hard Pomeron contribution.
Shortest polygonal chains covering each planar square grid<|sep|>Given any $n \in \mathbb{Z}^{+}$, we constructively prove the existence of covering paths and circuits in the plane which are characterized by the same link length of the minimum-link covering trails for the two-dimensional grid $G_n^2 := \{0,1, \dots ,n-1\} \times \{0,1, \dots ,n-1\}$. Furthermore, we introduce a general algorithm that returns a covering cycle of analogous link length for any even value of $n$. Finally, we provide the tight upper bound $n^2 - 3 + 5 \cdot \sqrt{2}$ units for the minimum total distance travelled to visit all the nodes of $G_n^2$ with a minimum-link trail (i.e., a trail with $2 \cdot n - 2$ edges if $n$ is above two).
The Internet as Quantitative Social Science Platform: Insights from a Trillion Observations<|sep|>With the large-scale penetration of the internet, for the first time, humanity has become linked by a single, open, communications platform. Harnessing this fact, we report insights arising from a unified internet activity and location dataset of an unparalleled scope and accuracy drawn from over a trillion (1.5$\times 10^{12}$) observations of end-user internet connections, with temporal resolution of just 15min over 2006-2012. We first apply this dataset to the expansion of the internet itself over 1,647 urban agglomerations globally. We find that unique IP per capita counts reach saturation at approximately one IP per three people, and take, on average, 16.1 years to achieve; eclipsing the estimated 100- and 60- year saturation times for steam-power and electrification respectively. Next, we use intra-diurnal internet activity features to up-scale traditional over-night sleep observations, producing the first global estimate of over-night sleep duration in 645 cities over 7 years. We find statistically significant variation between continental, national and regional sleep durations including some evidence of global sleep duration convergence. Finally, we estimate the relationship between internet concentration and economic outcomes in 411 OECD regions and find that the internet's expansion is associated with negative or positive productivity gains, depending strongly on sectoral considerations. To our knowledge, our study is the first of its kind to use online/offline activity of the entire internet to infer social science insights, demonstrating the unparalleled potential of the internet as a social data-science platform.
Measuring neutrino mass with radioactive ions in a storage ring<|sep|>We propose a method to measure the neutrino mass kinematically using beams of ions which undergo beta decay. The idea is to tune the ion beam momentum so that in most decays, the electron is forward moving with respect to the beam, and only in decays near the endpoint is the electron moving backwards. Then, by counting the backward moving electrons one can observe the effect of neutrino mass on the beta spectrum close to the endpoint. In order to reach sensitivities for $m_\nu < 0.2$ eV, it is necessary to control the ion momentum with a precision better than $\delta p/p < 10^{-5}$, identify suitable nuclei with low Q-values (in the few to ten keV range), and one must be able to observe at least O($10^{18}$) decays.
Observation of the period ratio P_1/P_2 of transversal oscillations in solar macro-spicules<|sep|>We analyze the time series of oxygen line profiles (O vi 1031.93 A and O vi 1037.61 A obtained from SUMER/SOHO on the solar south limb. We calculated Doppler shifts and consequently Doppler velocities in three heights 4", 14", and 24" from the limb on a coronal hole region. Then, we performed wavelet analysis with Morlet wavelet transform to determine the periods of fundamental mode and its first harmonic mode. The calculated period ratios have departures from its canonical value of $2$. The density stratification and magnetic twist are two main factors which may cause these departures.
The importance of being constrained: dealing with infeasible solutions in Differential Evolution and beyond<|sep|>We argue that results produced by a heuristic optimisation algorithm cannot be considered reproducible unless the algorithm fully specifies what should be done with solutions generated outside the domain, even in the case of simple box constraints. Currently, in the field of heuristic optimisation, such specification is rarely mentioned or investigated due to the assumed triviality or insignificance of this question. Here, we demonstrate that, at least in algorithms based on Differential Evolution, this choice induces notably different behaviours - in terms of performance, disruptiveness and population diversity. This is shown theoretically (where possible) for standard Differential Evolution in the absence of selection pressure and experimentally for the standard and state-of-the-art Differential Evolution variants on special test function $f_0$ and BBOB benchmarking suite, respectively. Moreover, we demonstrate that the importance of this choice quickly grows with problem's dimensionality. Different Evolution is not at all special in this regard - there is no reason to presume that other heuristic optimisers are not equally affected by the aforementioned algorithmic choice. Thus, we urge the field of heuristic optimisation to formalise and adopt the idea of a new algorithmic component in heuristic optimisers, which we call here a strategy of dealing with infeasible solutions. This component needs to be consistently (a) specified in algorithmic descriptions to guarantee reproducibility of results, (b) studied to better understand its impact on algorithm's performance in a wider sense and (c) included in the (automatic) algorithmic design. All of these should be done even for problems with box constraints.
Hyperparameter Transfer Learning through Surrogate Alignment for Efficient Deep Neural Network Training<|sep|>Recently, several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks (DNNs). The methods work by modeling the joint distribution of hyperparameter values and corresponding error. Those methods become less practical when applied to modern DNNs whose training may take a few days and thus one cannot collect sufficient observations to accurately model the distribution. To address this challenging issue, we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. As opposed to existing transfer learning methods, our proposed method does not use hand-designed features. Instead, it uses surrogates to model the hyperparameter-error distributions of the two datasets and trains a neural network to learn the transfer function. Extensive experiments on three CV benchmark datasets clearly demonstrate the efficiency of our method.
Branching diffusion representation of semi-linear elliptic PDEs and estimation using Monte Carlo method<|sep|>We study semi-linear elliptic PDEs with polynomial non-linearity and provide a probabilistic representation of their solution using branching diffusion processes. When the non-linearity involves the unknown function but not its derivatives, we extend previous results in the literature by showing that our probabilistic representation provides a solution to the PDE without assuming its existence. In the general case, we derive a new representation of the solution by using marked branching diffusion processes and automatic differentiation formulas to account for the non-linear gradient term. In both cases, we develop new theoretical tools to provide explicit sufficient conditions under which our probabilistic representations hold. As an application, we consider several examples including multi-dimensional semi-linear elliptic PDEs and estimate their solution by using the Monte Carlo method.
Hamiltonian Nature of Monopole Dynamics<|sep|>Classical electromagnetism with magnetic monopoles is not a Hamiltonian field theory because the Jacobi identity for the Poisson bracket fails. The Jacobi identity is recovered only if all of the species have the same ratio of electric to magnetic charge or if an electron and a monopole can never collide. Without the Jacobi identity, there are no local canonical coordinates or Lagrangian action principle. To build a quantum theory of magnetic monopoles, we either must explain why the positions of electrons and monopoles can never coincide or we must resort to new quantization techniques.
Distributed manipulation of two-qubit entanglement with coupled continuous variables<|sep|>We study the dynamics of two qubits separately sent through two coupled resonators, each initially containing a coherent state field. We present analytical arguments and numerical calculations for the qubit-field system under different two-qubit initial states, photon hopping strengths, and detunings. In far off-resonant regime, the maximal entanglement of two qubits can be generated with the initial qubit state in which one qubit is in the excited state and the other is in the ground state, and the initially maximal two-qubit entanglement can be frozen and fully revived even for large mean photon number. When the qubits are both initially in their excited states or ground states, the qubit-qubit entanglement birth and death apparently appear in the regime where the photon hopping strength is close to qubit-field detuning, and its peaks do not decrease monotonically as the interaction time increases. It is interesting to observe that when there is photon hopping strength between two fields, the field-field entanglement can be larger than one and increases as the initial amplitude of the coherent state grows. By postselecting the fields both in their coherent states, the entanglement of two initially unentangled qubits can be largely improved. Our present setup is fundamental for the distributed quantum information processing and applicable to different physical qubit-resonator systems.
Higgs Branching Ratios in Constrained Minimal and Next-to-Minimal Supersymmetry Scenarios Surveyed<|sep|>In the CMSSM the heaviest scalar and pseudo-scalar Higgs bosons decay largely into b-quarks and tau-leptons because of the large $\tan\beta$ values favored by the relic density. In the NMSSM the number of possible decay modes is much richer. In addition to the CMSSM-like scenarios, the decay of the heavy Higgs bosons is preferentially into top quark pairs (if kinematically allowed), lighter Higgs bosons or neutralinos, leading to invisible decays. We provide a scan over the NMSSM parameter space to project the 6D parameter space of the Higgs sector on the 3D space of the Higgs masses to determine the range of branching ratios as function of the Higgs boson mass for all Higgs bosons. Specific LHC benchmark points are proposed, which represent the salient NMSSM features.
Dispersive probing of driven pseudo-spin dynamics in a gradient field<|sep|>We have studied the coherent evolution of ultracold atomic rubidium clouds subjected to a microwave field driving Rabi oscillations between the stretched states of the F=1 and F=2 hyperfine levels. A phase winding of the two-level system pseudo-spin vector is encountered for elongated samples of atoms exposed to an axial magnetic field gradient and can be observed directly in state-selective absorption imaging. When dispersively recording the sample-integrated spin population during the Rabi drive, we observe a damped oscillation directly related to the magnetic field gradient, which we quantify using a simple dephasing model. By analyzing such dispersively acquired data from millimeter sized atomic samples, we demonstrate that field gradients can be determined with an accuracy of $\sim25$ nT/mm. The dispersive probing of inhomogeneously broadened Rabi oscillations in prolate samples opens up a path to gradiometry with bandwidths in the kilohertz domain.
Resource Optimization of Product Development Projects with Time-Varying Dependency Structure<|sep|>Project managers are continuously under pressure to shorten product development durations. One practical approach for reducing the project duration is lessening dependencies between different development components and teams. However, most of the resource allocation strategies for lessening dependencies place the implicit and simplistic assumption that the dependency structure between components is static (i.e., does not change over time). This assumption, however, does not necessarily hold true in all product development projects. In this paper, we present an analytical framework for optimally allocating resources to shorten the lead-time of product development projects having a time-varying dependency structure. We build our theoretical framework on a linear system model of product development processes, in which system integration and local development teams exchange information asynchronously and aperiodically. By utilizing a convexity result from the matrix theory, we show that the optimal resource allocation can be efficiently found by solving a convex optimization problem. We provide illustrative examples to demonstrate the proposed framework. We also present boundary analyses based on major graph models to provide managerial guidelines for improving empirical PD processes.
PEACE: Pulsar Evaluation Algorithm for Candidate Extraction -- A software package for post-analysis processing of pulsar survey candidates<|sep|>Modern radio pulsar surveys produce a large volume of prospective candidates, the majority of which are polluted by human-created radio frequency interference or other forms of noise. Typically, large numbers of candidates need to be visually inspected in order to determine if they are real pulsars. This process can be labor intensive. In this paper, we introduce an algorithm called PEACE (Pulsar Evaluation Algorithm for Candidate Extraction) which improves the efficiency of identifying pulsar signals. The algorithm ranks the candidates based on a score function. Unlike popular machine-learning based algorithms, no prior training data sets are required. This algorithm has been applied to data from several large-scale radio pulsar surveys. Using the human-based ranking results generated by students in the Arecibo Remote Command enter programme, the statistical performance of PEACE was evaluated. It was found that PEACE ranked 68% of the student-identified pulsars within the top 0.17% of sorted candidates, 95% within the top 0.34%, and 100% within the top 3.7%. This clearly demonstrates that PEACE significantly increases the pulsar identification rate by a factor of about 50 to 1000. To date, PEACE has been directly responsible for the discovery of 47 new pulsars, 5 of which are millisecond pulsars that may be useful for pulsar timing based gravitational-wave detection projects.
Dynamics of interacting bosons using the Herman-Kluk semiclassical initial value representation<|sep|>Recent experimental progress using ultracold gases in optical lattices necessitates a quantitative theoretical description for a significant number of bosons. In the present paper, we investigate if time-dependent semiclassical initial value methods, with propagators expressed as integrals over phase space using classical trajectories, is suitable to describe interacting bosons, concentrating on a single mode. Despite the nonlinear contribution from the self-interaction, the corresponding classical dynamics allows for a largely analytical treatment of the semiclassical propagator. We find that the Herman-Kluk (HK) propagator conserves unitarity in the semiclassical limit ($n\to\infty$), but a decay of the norm is seen for low $n$. The frozen Gaussian approximation (FGA, i.e. HK with unit prefactor) is explicitly shown to violate unitarity in the present system for non-vanishing interaction strength, even in the semiclassical limit. Furthermore, we show by evaluating the phase space integral in steepest descent approximation, that the HK propagator reproduces the exact spectrum correctly in the limit $n\to \infty$. An error is, however, incurred in next-to-next-to-leading order (small parameter $1/n$), as seen upon numerical evaluation of the integral or by considering finite $n$ corrections to steepest descent. The FGA, in contrast, is only accurate to lowest order, and an erroneous next-to-leading order term in the energy spectrum was found analytically. Finally, as an example application, we study the dynamics of wave packets by computing the time evolution of the Wigner function. While the often-used truncated Wigner approximation cannot capture any interferences present in the exact quantum mechanical solution (known analytically), we find that the HK approach, despite also using classical information only, reproduces the salient features of the exact solution correctly.
Understanding Boltzmann Machine and Deep Learning via A Confident Information First Principle<|sep|>Typical dimensionality reduction methods focus on directly reducing the number of random variables while retaining maximal variations in the data. In this paper, we consider the dimensionality reduction in parameter spaces of binary multivariate distributions. We propose a general Confident-Information-First (CIF) principle to maximally preserve parameters with confident estimates and rule out unreliable or noisy parameters. Formally, the confidence of a parameter can be assessed by its Fisher information, which establishes a connection with the inverse variance of any unbiased estimate for the parameter via the Cram\'{e}r-Rao bound. We then revisit Boltzmann machines (BM) and theoretically show that both single-layer BM without hidden units (SBM) and restricted BM (RBM) can be solidly derived using the CIF principle. This can not only help us uncover and formalize the essential parts of the target density that SBM and RBM capture, but also suggest that the deep neural network consisting of several layers of RBM can be seen as the layer-wise application of CIF. Guided by the theoretical analysis, we develop a sample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and a CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are studied in a series of density estimation experiments.
Solar off-limb line widths: Alfven waves, ion-cyclotron waves, and preferential heating<|sep|>Alfven waves and ion-cyclotron absorption of high-frequency waves are frequently brought into models devoted to coronal heating and fast solar-wind acceleration. Signatures of ion-cyclotron resonance have already been observed in situ in the solar wind (HELIOS spacecrafts) and, recently, in the upper corona (UVCS/SOHO remote-sensing results). We propose a method to constrain both the Alfven wave amplitude and the preferential heating induced by ion-cyclotron resonance, above a partially developed polar coronal hole observed with the SUMER/SOHO spectrometer. The instrumental stray light contribution is first substracted from the spectra. By supposing that the non-thermal velocity is related to the Alfven wave amplitude, it is constrained through a density diagnostic and the gradient of the width of the Mg X 625 A line. The temperatures of several coronal ions, as functions of the distance above the limb, are then determined by substracting the non-thermal component to the observed line widths. The effect of stray light explains the apparent decrease with height in the width of several spectral lines, this decrease usually starting about 0.1-0.2 Rs above the limb. This result rules out any direct evidence of damping of the Alfven waves, often suggested by other authors. We also find that the ions with the smallest charge-to-mass ratios are the hottest ones at a fixed altitude and that they are subject to a stronger heating, as compared to the others, between 57" and 102" above the limb. This constitutes a serious clue to ion-cyclotron preferential heating.
Applying Bayesian Hierarchical Probit Model to Interview Grade Evaluation<|sep|>Job interviews are a fundamental activity for most corporations to acquire potential candidates, and for job seekers to get well-rewarded and fulfilling career opportunities. In many cases, interviews are conducted in multiple processes such as telephone interviews and several face-to-face interviews. At each stage, candidates are evaluated in various aspects. Among them, grade evaluation, such as a rating on a 1-4 scale, might be used as a reasonable method to evaluate candidates. However, because each evaluation is based on a subjective judgment of interviewers, the aggregated evaluations can be biased because the difference in toughness of interviewers is not examined. Additionally, it is noteworthy that the toughness of interviewers might vary depending on the interview round. As described herein, we propose an analytical framework of simultaneous estimation for both the true potential of candidates and toughness of interviewers' judgment considering job interview rounds, with algorithms to extract unseen knowledge of the true potential of candidates and toughness of interviewers as latent variables through analyzing grade data of job interviews. We apply a Bayesian Hierarchical Ordered Probit Model to the grade data from HRMOS, a cloud-based Applicant Tracking System (ATS) operated by BizReach, Inc., an IT start-up particularly addressing human-resource needs in Japan. Our model successfully quantifies the candidate potential and the interviewers' toughness. An interpretation and applications of the model are given along with a discussion of its place within hiring processes in real-world settings. The parameters are estimated by Markov Chain Monte Carlo (MCMC). A discussion of uncertainty, which is given by the posterior distribution of the parameters, is also provided along with the analysis.
Beyond Beaconing: Emerging Applications and Challenges of BLE<|sep|>As an emerging technology with exceptional low energy consumption and low-latency data transmissions, Bluetooth Low Energy (BLE) has gained significant momentum in various application domains, such as Indoor Positioning, Home Automation, and Wireless Personal Area Network (WPAN) communications. With various novel protocol stack features, BLE is finding use on resource-constrained sensor nodes as well as more powerful gateway devices. Particularly proximity detection using BLE beacons has been a popular usage scenario ever since the release of Bluetooth 4.0, primarily due to the beacons' energy efficiency and ease of deployment. However, with the rapid rise of the Internet of Things (IoT), BLE is likely to be a significant component in many other applications with widely varying performance and Quality-of-Service (QoS) requirements and there is a need for a consolidated view of the role that BLE will play in applications beyond beaconing. This paper comprehensively surveys state-of-the-art applications built with BLE, obstacles to adoption of BLE in new application areas, and current solutions from academia and industry that further expand the capabilities of BLE.
A statistical approach to radio emission from shell-type SNRs. I. Basic ideas, techniques, and first results<|sep|>Shell-type supernova remnants (SNRs) exhibit correlations between radio surface brightness, SNR diameter, and ambient medium density. We investigate these correlations, to extract useful information about the typical evolutionary stage of radio SNRs, and to obtain insight into the origin of the relativistic electrons and magnetic fields responsible for the radio emission. We propose a scenario, according to which the observed correlations are the combined effect of SNRs evolving in a wide range of ambient conditions, rather than the evolutionary track of a "typical" SNR. We then develop a parametric approach to interpret the statistical data, and apply it to the data sample previously published by Berkhuijsen, as well as to a sample of SNRs in the galaxy M33. We find that SNRs cease to emit effectively in radio at a stage near the end of their Sedov evolution, and that models of synchrotron emission with constant efficiencies in particle acceleration and magnetic field amplification do not provide a close match to the data. We discuss the problem of the cumulative distribution in size, showing that the slope of this distribution does not relate to the expansion law of SNRs, as usually assumed, but only to the ambient density distribution. This solves a long-standing paradox: the almost linear cumulative distribution of SNRs led several authors to conclude that these SNRs are still in free expansion, which also implies very low ambient densities. Within this framework, we discuss the case of the starburst galaxy M82. Statistical properties of SNR samples may be used to shed light on both the physics of electron acceleration and the evolution of SNRs. More precise results could be obtained by combining data of several surveys of SNRs in nearby galaxies.
The Effects of Grain Size and Temperature Distributions on the Formation of Interstellar Ice Mantles<|sep|>Computational models of interstellar gas-grain chemistry have historically adopted a single dust-grain size of 0.1 micron, assumed to be representative of the size distribution present in the interstellar medium. Here, we investigate the effects of a broad grain-size distribution on the chemistry on dust-grain surfaces and the subsequent build-up of molecular ices on the grains, using a three-phase gas-grain chemical model of a quiescent dark cloud. We include an explicit treatment of the grain temperatures, governed both by the visual extinction of the cloud and the size of each individual grain-size population. We find that the temperature difference plays a significant role in determining the total bulk ice composition across the grain-size distribution, while the effects of geometrical differences between size populations appear marginal. We also consider collapse from a diffuse to a dark cloud, allowing dust temperatures to fall. Under the initial diffuse conditions, small grains are too warm to promote grain-mantle build-up, with most ices forming on the mid-sized grains. As collapse proceeds, the more abundant, smallest grains cool and become the dominant ice carriers; the large population of small grains means that this ice is distributed across many grains, with perhaps no more than 40 monolayers of ice each (versus several hundred assuming a single grain size). This effect may be important for the subsequent processing and desorption of the ice during the hot-core phase of star-formation, exposing a significant proportion of the ice to the gas phase, increasing the importance of ice-surface chemistry and surface-gas interactions.
The HIVE Tool for Informed Swarm State Space Exploration<|sep|>Swarm verification and parallel randomised depth-first search are very effective parallel techniques to hunt bugs in large state spaces. In case bugs are absent, however, scalability of the parallelisation is completely lost. In recent work, we proposed a mechanism to inform the workers which parts of the state space to explore. This mechanism is compatible with any action-based formalism, where a state space can be represented by a labelled transition system. With this extension, each worker can be strictly bounded to explore only a small fraction of the state space at a time. In this paper, we present the HIVE tool together with two search algorithms which were added to the LTSmin tool suite to both perform a preprocessing step, and execute a bounded worker search. The new tool is used to coordinate informed swarm explorations, and the two new LTSmin algorithms are employed for preprocessing a model and performing the individual searches.
Automatic Synthesis of Light Processing Functions for Programmable Photonics: Theory and Realization<|sep|>Linear light processing functions (e.g., routing, splitting, filtering) are key functions requiring configuration to implement on a programmable photonic integrated circuit (PPIC). In recirculating waveguide meshes (which include loop-backs), this is usually done manually. Some previous results describe explorations to perform this task automatically, but their efficiency or applicability is still limited. In this paper, we propose an efficient method that can automatically realize configurations for many light processing functions on a square-mesh PPIC. At its heart is an automatic differentiation subroutine built upon analytical expressions of scattering matrices, which enables gradient descent optimization for functional circuit synthesis. As demonstrated in numerical results, the advantages of our method include: (i) gradients are calculated analytically instead of using numerical differentiation, making the high-dimensional optimization solvable in time scales of minutes; and (ii) our method is suited to a wide range of light processing functions, and can synthesize multiple functions on the same PPIC simultaneously.
On the nodal structure of nonlinear stationary waves on star graphs<|sep|>We consider stationary waves on nonlinear quantum star graphs, i.e. solutions to the stationary (cubic) nonlinear Schr\"odinger equation on a metric star graph with Kirchhoff matching conditions at the centre. We prove the existence of solutions that vanish at the centre of the star and classify them according to the nodal structure on each edge (i.e. the number of nodal domains or nodal points that the solution has on each edge). We discuss the relevance of these solutions in more applied settings as starting points for numerical calculations of spectral curves and put our results into the wider context of nodal counting such as the classic Sturm oscillation theorem.
The effect of metallicity on the delay-time distribution of type Ia supernova<|sep|>Measuring the delay-time distribution (DTD) of type Ia supernova(SNe Ia) is an important way to constrain the progenitor nature of SNe Ia. Recently, Strolger et al. (2010) obtained a very delayed DTD, which is much different from other measurements. They suggested that metallicity could be the origin of their delayed DTD. In this paper, we show the effect of metallicity on the DTD of SNe Ia from single-degenerate models (including WD + MS and WD+RG channels). Via a binary population synthesis approach, we find that the DTD from a low metallicity population is significantly delayed compared with that from a high metallicity one. In addition, we also find that a substantial fraction of SNe Ia have a delay time shorter than 1 Gyr, and the fraction of SNe Ia with short delay times increases with metallicity, i.e. about 35% for Z=0.001, while more than 70% for Z=0.02. These results would help to qualitatively explain the result of Strolger et al. (2010). Furthermore, we noticed that the contribution of WD + RG channel from the low metallicity population is higher than that from the high metallicity one. However, we can not quantitatively obtain a DTD consistent with the results of Strolger et al. (2010) by changing metallicity. As a consequence, metallicity may partly contribute to the DTD of SNe Ia and should therefore be checked carefully when one derives the DTD of SNe Ia from observations.
Investigation of Power8 processors for astronomical adaptive optics real-time control<|sep|>The forthcoming Extremely Large Telescopes all require adaptive optics systems for their successful operation. The real-time control for these systems becomes computationally challenging, in part limited by the memory bandwidths required for wavefront reconstruction. We investigate new POWER8 processor technologies applied to the problem of real-time control for adaptive optics. These processors have a large memory bandwidth, and we show that they are suitable for operation of first-light ELT instrumentation, and propose some potential real-time control system designs. A CPU-based real-time control system significantly reduces complexity, improves maintainability, and leads to increased longevity for the real-time control system.
Tomography of cool giant and supergiant star atmospheres II. Signature of convection in the atmosphere of the red supergiant star $\mu$ Cep<|sep|>Red supergiants are cool massive stars and are the largest and the most luminous stars in the universe. They are characterized by irregular or semi-regular photometric variations, the physics of which is not clearly understood. The paper aims at deriving the velocity field in the red supergiant star $\mu$ Cep and relating it to the photometric variability with the help of the tomographic method. The tomographic method allows to recover the line-of-sight velocity distribution over the stellar disk and within different optical-depth slices. The method is applied to a series of high-resolution spectra of $\mu$ Cep, and these results are compared to those obtained from 3D radiative-hydrodynamics CO5BOLD simulations of red supergiants. Fluctuations in the velocity field are compared with photometric and spectroscopic variations, the latter being derived from the TiO band strength and serving (at least partly) a proxy of the variations in effective temperature. The tomographic method reveals a phase shift between the velocity and spectroscopic/photometric variations. This phase shift results in a hysteresis loop in the temperature - velocity plane, with a timescale of a few hundred days, similar to the photometric one. The similarity between the hysteresis loop timescale measured in $\mu$ Cep and the timescale of acoustic waves disturbing the convective pattern suggests that such waves play an important role in triggering the hysteresis loops.
Influence of macroclumping on type II supernova light curves<|sep|>Core-collapse supernova (SN) ejecta are probably structured on both small and large scales, with greater deviations from spherical symmetry nearer the explosion site. Here, we present 2D and 3D gray radiation-hydrodynamics simulations of type II SN light curves from red (RSG) and blue supergiant (BSG) star explosions to investigate the impact on SN observables of inhomogeneities in density or composition, with a characteristic scale set to a few percent of the local radius. Clumping is found to hasten the release of stored radiation, boosting the early time luminosity and shortening the photospheric phase. Around the photosphere, radiation leaks between the clumps where the photon mean free path is greater. Since radiation is stored uniformly in volume, a greater clumping can increase this leakage by storing more and more mass into smaller and denser clumps containing less and less radiation energy. An inhomogeneous medium in which different regions recombine at different temperatures can also impact the light curve. Clumping can thus be a source of diversity in SN brightness. Clumping may lead to a systematic underestimate of ejecta masses from light curve modeling, although a significant offset seems to require a large density contrast of a few tens between clumps and interclump medium.
A Tail Sensitive Test for Cumulative Distribution Functions<|sep|>We propose a simple way of testing whether a given set of observations can come from a given theoretical cumulative distribution. In the test more weight is attached to the tails of the distribution than in the usual Kolmogorov or Smirnov tests. The respective probability distribution is derived.
Searching for low-lying multi-particle thresholds in lattice spectroscopy<|sep|>We explore the Euclidean-time tails of odd-parity nucleon correlation functions in a search for the S-wave pion-nucleon scattering-state threshold contribution. The analysis is performed using 2+1 flavor 32^3 x 64 PACS-CS gauge configurations available via the ILDG. Correlation matrices composed with various levels of fermion source/sink smearing are used to project low-lying states. The consideration of 25,600 fermion propagators reveals the presence of more than one state in what would normally be regarded as an eigenstate-projected correlation function. This observation is in accord with the scenario where the eigenstates contain a strong mixing of single and multi-particle states but only the single particle component has a strong coupling to the interpolating field. Employing a two-exponential fit to the eigenvector-projected correlation function, we are able to confirm the presence of two eigenstates. The lower-lying eigenstate is consistent with a N-pi scattering threshold and has a relatively small coupling to the three-quark interpolating field. We discuss the impact of this small scattering-state contamination in the eigenvector projected correlation function on previous results presented in the literature.
Distributed Association and Relaying with Fairness in Millimeter Wave Networks<|sep|>Millimeter wave (mmWave) systems are emerging as an essential technology to enable extremely high data rate wireless communications. The main limiting factors of mmWave systems are blockage (high penetration loss) and deafness (misalignment between the beams of the transmitter and receiver). To alleviate these problems, it is imperative to incorporate efficient association and relaying between terminals and access points. Unfortunately, the existing association techniques are designed for the traditional interference-limited networks, and thus are highly suboptimal for mmWave communications due to narrow-beam operations and the resulting non-negligible interference-free behavior. This paper introduces a distributed approach that solves the joint association and relaying problem in mmWave networks considering the load balancing at access points. The problem is posed as a novel stochastic optimization problem, which is solved by distributed auction algorithms where the clients and relays act asynchronously to achieve optimal client-relay-access point association. It is shown that the algorithms provably converge to a solution that maximizes the aggregate logarithmic utility within a desired bound. Numerical results allow to quantify the performance enhancements introduced by the relays, and the substantial improvements of the network throughput and fairness among the clients by the proposed association method as compared to standard approaches. It is concluded that mmWave communications with proper association and relaying mechanisms can support extremely high data rates, connection reliability, and fairness among the clients.
Sequential Voting with Relational Box Fields for Active Object Detection<|sep|>A key component of understanding hand-object interactions is the ability to identify the active object -- the object that is being manipulated by the human hand. In order to accurately localize the active object, any method must reason using information encoded by each image pixel, such as whether it belongs to the hand, the object, or the background. To leverage each pixel as evidence to determine the bounding box of the active object, we propose a pixel-wise voting function. Our pixel-wise voting function takes an initial bounding box as input and produces an improved bounding box of the active object as output. The voting function is designed so that each pixel inside of the input bounding box votes for an improved bounding box, and the box with the majority vote is selected as the output. We call the collection of bounding boxes generated inside of the voting function, the Relational Box Field, as it characterizes a field of bounding boxes defined in relationship to the current bounding box. While our voting function is able to improve the bounding box of the active object, one round of voting is typically not enough to accurately localize the active object. Therefore, we repeatedly apply the voting function to sequentially improve the location of the bounding box. However, since it is known that repeatedly applying a one-step predictor (i.e., auto-regressive processing with our voting function) can cause a data distribution shift, we mitigate this issue using reinforcement learning (RL). We adopt standard RL to learn the voting function parameters and show that it provides a meaningful improvement over a standard supervised learning approach. We perform experiments on two large-scale datasets: 100DOH and MECCANO, improving AP50 performance by 8% and 30%, respectively, over the state of the art.
Probing Photon-ALP Oscillations from the Flat Spectrum Radio Quasar 4C+21.35<|sep|>Flat spectrum radio quasar (FSRQ) is the most luminous blazar at the GeV energies. In this paper, we probe the photon-axion-like particle (ALP) oscillation effect on the latest very-high-energy (VHE) $\gamma$-ray observations of the FSRQ 4C+21.35 (PKS 1222+216). The $\gamma$-ray spectra are measured by the collaborations Major Atmospheric Gamma Imaging Cherenkov Telescopes (MAGIC), Very Energetic Radiation Imaging Telescope Array System (VERITAS), and Fermi Large Area Telescope (Fermi-LAT), which cover two activity VHE flares of 4C+21.35 in 2010 and 2014. We show the spectral energy distributions (SEDs) of these two phases under the null and ALP hypotheses, and set the combined limit on the ALP parameter space. The 95% $\rm C.L.$ combined limit set by the FSRQ 4C+21.35 observations measured by MAGIC, VERITAS, and Fermi-LAT in the $m_a-g_{a\gamma}$ plane is roughly at the photon-ALP coupling $g_{a\gamma} \gtrsim 8\times 10^{-12} \rm \, GeV^{-1}$ for the ALP mass $[\,2\times 10^{-10}\, {\rm eV} \lesssim m_a \lesssim 2\times 10^{-8}\, \rm eV\,]$. Compared with the constraint of NGC 1275 set by Fermi-LAT, no stringent limit result is derived with the photon-ALP coupling $g_{a\gamma}$ from the FSRQ 4C+21.35, while this result could slightly broaden the ALP mass $m_a$ limit at the low-mass region.
Finslerian MOND versus the Strong Gravitational Lensing of the Early-type Galaxies<|sep|>The gravitational lensing of Bullet Clusters and early-type galaxies pose serious challenges on the validity of MOND. Recently, Finslerian MOND, a generalization of MOND in the framework of Finsler gravity, has been proposed to explain the mass discrepancy problem of Bullet Cluster 1E 0657\ 558. In this paper, we check the validity of the Finslerian MOND in describing the strong gravitational lensing of early-type galaxies. The investigation on ten strong lenses of the CASTLES samples shows that there is no strong evidence for the existence of dark matter.
Simulation of stochastic systems via polynomial chaos expansions and convex optimization<|sep|>Polynomial Chaos Expansions represent a powerful tool to simulate stochastic models of dynamical systems. Yet, deriving the expansion's coefficients for complex systems might require a significant and non-trivial manipulation of the model, or the computation of large numbers of simulation runs, rendering the approach too time consuming and impracticable for applications with more than a handful of random variables. We introduce a novel computationally tractable technique for computing the coefficients of polynomial chaos expansions. The approach exploits a regularization technique with a particular choice of weighting matrices, which allow to take into account the specific features of Polynomial Chaos expansions. The method, completely based on convex optimization, can be applied to problems with a large number of random variables and uses a modest number of Monte Carlo simulations, while avoiding model manipulations. Additional information on the stochastic process, when available, can be also incorporated in the approach by means of convex constraints. We show the effectiveness of the proposed technique in three applications in diverse fields, including the analysis of a nonlinear electric circuit, a chaotic model of organizational behavior, finally a chemical oscillator.
On the $d$-Claw Vertex Deletion Problem<|sep|>Let $d$-claw (or $d$-star) stand for $K_{1,d}$, the complete bipartite graph with 1 and $d\ge 1$ vertices on each part. The $d$-claw vertex deletion problem, $d$-CLAW-VD, asks for a given graph $G$ and an integer $k$ if one can delete at most $k$ vertices from $G$ such that the resulting graph has no $d$-claw as an induced subgraph. Thus, 1-CLAW-VD and 2-CLAW-VD are just the famous VERTEX COVER problem and the CLUSTER VERTEX DELETION problem, respectively. In this paper, we strengthen a hardness result in [M. Yannakakis, Node-Deletion Problems on Bipartite Graphs, SIAM J. Comput. (1981)], by showing that CLUSTER VERTEX DELETION remains NP-complete when restricted to bipartite graphs of maximum degree 3. Moreover, for every $d\ge 3$, we show that $d$-CLAW-VD is NP-complete even when restricted to bipartite graphs of maximum degree $d$. These hardness results are optimal with respect to degree constraint. By extending the hardness result in [F. Bonomo-Braberman et al., Linear-Time Algorithms for Eliminating Claws in Graphs, COCOON 2020], we show that, for every $d\ge 3$, $d$-CLAW-VD is NP-complete even when restricted to split graphs without $(d+1)$-claws, and split graphs of diameter 2. On the positive side, we prove that $d$-CLAW-VD is polynomially solvable on what we call $d$-block graphs, a class properly contains all block graphs. This result extends the polynomial-time algorithm in [Y. Cao et al., Vertex deletion problems on chordal graphs, Theor. Comput. Sci. (2018)] for 2-CLAW-VD on block graphs to $d$-CLAW-VD for all $d\ge 2$ and improves the polynomial-time algorithm proposed by F. Bonomo-Brabeman et al. for (unweighted) 3-CLAW-VD on block graphs to 3-block graphs.
Cosmology with powerful radio-loud AGNs<|sep|>Immensely bright quasars and radio-loud active galactic nuclei (AGNs) provide an enticing opportunity to construct standard candles detectable up to the very early universe. An analytic theory is proposed to measure the distance to powerful \citeauthor{FR+1974} type-II radio sources based on their integrated flux density across a broad range of radio frequencies, and the angular size and axis ratio of their synchrotron-emitting lobes. This technique can be used at low-redshift to construct absolute standard candles in conjunction with X-ray observations of the host cluster, or at high-redshift to measure the relative distances of objects and constrain the curvature of our universe. Distances calculated with this method are consistent for dissimilar objects at the same redshift; the two lobes of Cygnus A have flux densities, linear sizes and spectral break frequencies varying by between 15-35\% yet their fitted distances are the same to within 7\%. These distance estimates together yield a transverse comoving distance to Cygnus A of $261_{-55}^{+70}\rm\, Mpc$ corresponding to a Hubble constant of $H_0 = 64_{-13}^{+17}\rm\, km\, s^{-1}\, Mpc^{-1}$. Large samples of suitable FR-II sources could provide a measure of the Hubble constant independent of existing techniques such as the cosmic microwave background, baryon acoustic oscillations, and type 1a supernovae.
Distinguishing the effects of internal and forced atmospheric variability in climate networks<|sep|>The fact that the Earth climate is a highly complex dynamical system is well-known. In the last few decades a lot of effort has been focused on understanding how climate phenomena in one geographical region affects the climate of other regions. Complex networks are a powerful framework for identifying climate interdependencies. To further exploit the knowledge of the links uncovered via the network analysis (for, e.g., improvements in prediction), a good understanding of the physical mechanisms underlying these links is required. Here we focus in understanding the role of atmospheric variability, and construct climate networks representing internal and forced variability. In the connectivity of these networks we assess the influence of two main indices, NINO3.4 and the North Atlantic Oscillation (NAO), by calculating the networks from time-series where these indices were linearly removed. We find that the connectivity of the forced variability network is heavily affected by ``El Ni\~no'': removing the NINO3.4 index yields a general loss of connectivity; even teleconnections between regions far away from the equatorial Pacific ocean are lost, suggesting that these regions are not directly linked, but rather, are indirectly interconnected via ``El Ni\~no'', particularly on interannual time scales. On the contrary, in the internal variability network (independent of sea surface temperature forcing) we find that the links are significantly affected by NAO with a maximum in intra-annual time scales. While the strongest non-local links found are those forced by the ocean, we show that there are also strong teleconnections due to internal atmospheric variability.
4D Higher Spin Black Holes with Nonlinear Scalar Fluctuations<|sep|>We construct an infinite-dimensional space of solutions to Vasiliev's equations in four dimensions that are asymptotic to AdS spacetime and superpose massless scalar particle modes over static higher spin black holes. Each solution is obtained by a large gauge transformation of an all-order perturbatively defined particular solution given in a simple gauge, in which the spacetime connection vanishes, the twistor space connection is holomorphic, and all local degrees of freedom are encoded into the residual twistor space dependence of the spacetime zero-forms. The latter are expanded over two dual spaces of Fock space operators, corresponding to scalar particle and static black hole modes, equipped with positive definite sesquilinear and bilinear forms, respectively. Switching on an AdS vacuum gauge function, the twistor space connection becomes analytic at generic spacetime points, which makes it possible to reach Vasiliev's gauge, in which Fronsdal fields arise asymptotically, by another large transformation given here at first order. The particle and black hole modes are related by a twistor space Fourier transform, resulting in a black hole backreaction already at the second order of classical perturbation theory. We speculate on the existence of a fine-tuned branch of moduli space that is free from black hole modes and directly related to the quasi-local deformed Fronsdal theory. Finally, we comment on a possible interpretation of the higher spin black hole solutions as black-hole microstates.
High dimensional PCA: a new model selection criterion<|sep|>Given a random sample from a multivariate population, estimating the number of large eigenvalues of the population covariance matrix is an important problem in Statistics with wide applications in many areas. In the context of Principal Component Analysis (PCA), the linear combinations of the original variables having the largest amounts of variation are determined by this number. In this paper, we study the high dimensional asymptotic regime where the number of variables grows at the same rate as the number of observations, and use the spiked covariance model proposed in Johnstone (2001), under which the problem reduces to model selection. Our focus is on the Akaike Information Criterion (AIC) which is known to be strongly consistent from the work of Bai et al. (2018). However, Bai et al. (2018) requires a certain "gap condition" ensuring the dominant eigenvalues to be above a threshold strictly larger than the BBP threshold (Baik et al. (2005), both quantities depending on the limiting ratio of the number of variables and observations. It is well-known that, below the BBP threshold, a spiked covariance structure becomes indistinguishable from one with no spikes. Thus the strong consistency of AIC requires some extra signal strength. In this paper, we investigate whether consistency continues to hold even if the "gap" is made smaller. We show that strong consistency under arbitrarily small gap is achievable if we alter the penalty term of AIC suitably depending on the target gap. Furthermore, another intuitive alteration of the penalty can indeed make the gap exactly zero, although we can only achieve weak consistency in this case. We compare the two newly-proposed estimators with other existing estimators in the literature via extensive simulation studies, and show, by suitably calibrating our proposals, that a significant improvement in terms of mean-squared error is achievable.
Zero energy bound states in tunneling conductance spectra at the interface of an s-wave superconductor and a topological insulator in NbN-$Bi_2Se_3$-Au thin film junctions<|sep|>Measurements of conductance spectra in a superconductor - topological insulator - normal metal thin film junctions of NbN-$\rm Bi_2Se_3$-Au are reported. Junctions with ex-situ and in-situ prepared $\rm NbN-Bi_2Se_3$ interfaces were studied. At low temperatures, all the ex-situ junctions showed coherence peaks in their conductance spectra, but imbedded robust zero bias conductance peaks were observed only in junctions with a metallic or a metal to insulator transition below $\rm T_c$ of the NbN electrode. The in-situ junctions which had about two orders of magnitude lower resistance at low temperatures, generally showed flat conductance spectra at low bias, with no coherence or broad Andreev peaks, since the critical current of the NbN electrode was reached first, at voltage bias below the energy gap of the superconductor. A weak zero bias conductance peak however, was observed in one of these junctions. We conclude that significant tunneling barriers, as in the ex-situ prepared junctions, are essential for the observation of coherence peaks and the zero energy bound states. The later seem to originate in the $\rm Bi_2Se_3$-NbN interface, as they are absent in reference Au-NbN junctions without the topological layer sandwiched in between.
Deterministic Bayesian Information Fusion and the Analysis of its Performance<|sep|>This paper develops a mathematical and computational framework for analyzing the expected performance of Bayesian data fusion, or joint statistical inference, within a sensor network. We use variational techniques to obtain the posterior expectation as the optimal fusion rule under a deterministic constraint and a quadratic cost, and study the smoothness and other properties of its classification performance. For a certain class of fusion problems, we prove that this fusion rule is also optimal in a much wider sense and satisfies strong asymptotic convergence results. We show how these results apply to a variety of examples with Gaussian, exponential and other statistics, and discuss computational methods for determining the fusion system's performance in more general, large-scale problems. These results are motivated by studying the performance of fusing multi-modal radar and acoustic sensors for detecting explosive substances, but have broad applicability to other Bayesian decision problems.
Inference in conditioned dynamics through causality restoration<|sep|>Computing observables from conditioned dynamics is typically computationally hard, because, although obtaining independent samples efficiently from the unconditioned dynamics is usually feasible, generally most of the samples must be discarded (in a form of importance sampling) because they do not satisfy the imposed conditions. Sampling directly from the conditioned distribution is non-trivial, as conditioning breaks the causal properties of the dynamics which ultimately renders the sampling procedure efficient. One standard way of achieving it is through a Metropolis Monte-Carlo procedure, but this procedure is normally slow and a very large number of Monte-Carlo steps is needed to obtain a small number of statistically independent samples. In this work, we propose an alternative method to produce independent samples from a conditioned distribution. The method learns the parameters of a generalized dynamical model that optimally describe the conditioned distribution in a variational sense. The outcome is an effective, unconditioned, dynamical model, from which one can trivially obtain independent samples, effectively restoring causality of the conditioned distribution. The consequences are twofold: on the one hand, it allows us to efficiently compute observables from the conditioned dynamics by simply averaging over independent samples. On the other hand, the method gives an effective unconditioned distribution which is easier to interpret. The method is flexible and can be applied virtually to any dynamics. We discuss an important application of the method, namely the problem of epidemic risk assessment from (imperfect) clinical tests, for a large family of time-continuous epidemic models endowed with a Gillespie-like sampler. We show that the method compares favorably against the state of the art, including the soft-margin approach and mean-field methods.
On the origin of non-monotonic doping dependence of the in-plane resistivity anisotropy in Ba(Fe$_{1-x}T_x$)$_2$As$_2$, $T$ = Co, Ni and Cu<|sep|>The in-plane resistivity anisotropy has been measured for detwinned single crystals of Ba(Fe$_{1-x}$Ni$_x$)$_2$As$_2$ and Ba(Fe$_{1-x}$Cu$_x$)$_2$As$_2$. The data reveal a non-monotonic doping dependence, similar to previous observations for Ba(Fe$_{1-x}$Co$_x$)$_2$As$_2$. Magnetotransport measurements of the parent compound reveal a non-linear Hall coefficient and a strong linear term in the transverse magnetoresistance. Both effects are rapidly suppressed with chemical substitution over a similar compositional range as the onset of the large in-plane resistivity anisotropy. It is suggested that the relatively small in-plane anisotropy of the parent compound in the spin density wave state is due to the presence of an isotropic, high mobility pocket of reconstructed Fermi surface. Progressive suppression of the contribution to the conductivity arising from this isotropic pocket with chemical substitution eventually reveals the underlying in-plane anisotropy associated with the remaining FS pockets.
Verification of the Quantum Nonequilibrium Work Relation in the Presence of Decoherence<|sep|>Although nonequilibrium work and fluctuation relations have been studied in detail within classical statistical physics, extending these results to open quantum systems has proven to be conceptually difficult. For systems that undergo decoherence but not dissipation, we argue that it is natural to define quantum work exactly as for isolated quantum systems, using the two-point measurement protocol. Complementing previous theoretical analysis using quantum channels, we show that the nonequilibrium work relation remains valid in this situation, and we test this assertion experimentally using a system engineered from an optically trapped ion. Our experimental results reveal the work relation's validity over a variety of driving speeds, decoherence rates, and effective temperatures and represent the first confirmation of the work relation for non-unitary dynamics.
Thin Client Web-Based Campus Information Systems for Fiji National University<|sep|>Fiji National University is encountering many difficulties with its current administrative systems. These difficulties include accessibility, scalability, performance, flexibility and integration. We propose a new campus information system, FNU-CIS to addresses these difficulties. FNU-CIS has the potential to provide wide range of the services for students and staffs at the university. In order to assist in the design and implementation of proposed FNU-CIS, we present an overview, software architecture and prototype implementation of our proposed system. We discuss the key properties of our system, compare it with other similar systems available and outline our future plans for research in FNU-CIS implementation.
A note on confidence intervals for parameter estimates of a spatio-temporal Ornstein-Uhlenbeck process<|sep|>We compare two ways of constructing confidence intervals for the moments-matching parameter estimates of a Gaussian spatio-temporal Ornstein-Uhlenbeck process. It was found that those obtained via pairwise likelihood approximations had lower coverages and were more prone to the curse of dimensionality as opposed to those from a parametric bootstrap procedure.
The high-temperature expansion of the classical Ising model with S_z^2 term<|sep|>We derive the high-temperature expansion of the Helmholtz free energy up to the order \beta^{17} of the one-dimensional spin-S Ising model, with single-ion anisotropy term, in the presence of a longitudinal magnetic field. We show that the values of some thermodynamical functions for the ferromagnetic models, in the presence of a weak magnetic field, are not small corrections to their values with h=0. This model with S=3 was applied by Kishine et al. [J.-i. Kishine et al., Phys. Rev. B, 2006, 74, 224419] to analyze experimental data of the single-chain magnet [Mn (saltmen)]_2 [Ni(pac)_2 (py)_2] (PF_6)_2 for T<40 K. We show that for T<35 K the thermodynamic functions of the large-spin limit model are poor approximations to their analogous spin-3 functions.
Minimal Supersymmetric SU(5) and Gauge Coupling Unification at Three Loops<|sep|>We consider the relations between the gauge couplings at the electroweak scale and the high scale where unification of the three gauge couplings is expected. Threshold corrections are incorporated both at the supersymmetric and at the grand unified scale and, where available three-loop running and two-loop decoupling are employed. We study the impact of the current experimental uncertainties of the coupling constants and the supersymmetric mass spectrum on the prediction of the super-heavy masses within the so-called minimal supersymmetric SU(5). As a main result of the three-loop analysis we confirm that minimal supersymmetric SU(5) cannot be ruled out by the current experimental data on proton decay rates.
Multiwavelength campaign on the HBL PKS 2155-304 : A new insight on its spectral energy distribution<|sep|>The blazar PKS~2155-304 was the target of a multiwavelength campaign from June to October 2013 which widely improves our knowledge of its spectral energy distribution. This campaign involved the NuSTAR satellite (3-79 keV), the Fermi Large Area Telescope (LAT, 100~MeV-300~GeV) and the High Energy Stereoscopic System (H.E.S.S.) array phase II (with an energy threshold of few tens of GeV). While the observations with NuSTAR extend the X-ray spectrum to higher energies than before, H.E.S.S. phase II, together with the use of the LAT PASS 8, enhance the coverage of the $\gamma$-ray regime with an unprecedented precision. In this work, preliminary results from the multi-wavelength analysis are presented.
Memory-assisted measurement-device-independent quantum key distribution<|sep|>A protocol with the potential of beating the existing distance records for conventional quantum key distribution (QKD) systems is proposed. It borrows ideas from quantum repeaters by using memories in the middle of the link, and that of measurement-device-independent QKD, which only requires optical source equipment at the user's end. For certain fast memories, our scheme allows a higher repetition rate than that of quantum repeaters, thereby requiring lower coherence times. By accounting for various sources of nonideality, such as memory decoherence, dark counts, misalignment errors, and background noise, as well as timing issues with memories, we develop a mathematical framework within which we can compare QKD systems with and without memories. In particular, we show that with the state-of-the-art technology for quantum memories, it is possible to devise memory-assisted QKD systems that, at certain distances of practical interest, outperform current QKD implementations.
Balancing expression dags for more efficient lazy adaptive evaluation<|sep|>Arithmetic expression dags are widely applied in robust geometric computing. In this paper we restructure expression dags by balancing consecutive additions or multiplications. We predict an asymptotic improvement in running time and experimentally confirm the theoretical results. Finally, we discuss some pitfalls of the approach resulting from changes in evaluation order.
Measurement of the B^0_s - \bar{B}^0_s oscillation frequency Delta m_s in B^0_s -> D_s^-(3) pi decays<|sep|>The B^0_s-\bar{B}^0_s oscillation frequency Delta m_s is measured with 36 pb^{-1} of data collected in pp collisions at \sqrt{s} = 7 TeV by the LHCb experiment at the Large Hadron Collider. A total of 1381 B^0_s -> D_s^- \pi^+ and \B^0_s -> D_s^- pi^+ pi^- pi^+ signal decays are reconstructed, with average decay time resolutions of 44 fs and 36 fs, respectively. An oscillation signal with a statistical significance of 4.6 sigma is observed. The measured oscillation frequency is Delta m_s = 17.63 \pm 0.11 (stat) \pm 0.02 (syst) ps^{-1}.
Accelerating AGN jets to parsec scales using general relativistic MHD simulations<|sep|>Accreting black holes produce collimated outflows, or jets, that traverse many orders of magnitude in distance, accelerate to relativistic velocities, and collimate into tight opening angles. Of these, perhaps the least understood is jet collimation due to the interaction with the ambient medium. In order to investigate this interaction, we carried out axisymmetric general relativistic magnetohydrodynamic simulations of jets produced by a large accretion disc, spanning over 5 orders of magnitude in time and distance, at an unprecedented resolution. Supported by such a disc, the jet attains a parabolic shape, similar to the M87 galaxy jet, and the product of the Lorentz factor and the jet half-opening angle, $\gamma\theta\ll 1$, similar to values found from very long baseline interferometry (VLBI) observations of active galactic nuclei (AGN) jets; this suggests extended discs in AGN. We find that the interaction between the jet and the ambient medium leads to the development of pinch instabilities, which produce significant radial and lateral variability across the jet by converting magnetic and kinetic energy into heat. Thus pinched regions in the jet can be detectable as radiating hotspots and may provide an ideal site for particle acceleration. Pinching also causes gas from the ambient medium to become squeezed between magnetic field lines in the jet, leading to enhanced mass-loading of the jet and potentially contributing to the spine-sheath structure observed in AGN outflows.
Estimating the angular power spectrum of the gravitational-wave background in the presence of shot noise<|sep|>There has been much recent interest in studying anisotropies in the astrophysical gravitational-wave (GW) background, as these could provide us with interesting new information about galaxy clustering and large-scale structure. However, this information is obscured by shot noise, caused by the finite number of GW sources that contribute to the background at any given time. We develop a new method for estimating the angular spectrum of anisotropies, based on the principle of combining statistically-independent data segments. We show that this gives an unbiased estimate of the true, astrophysical spectrum, removing the offset due to shot noise power, and that in the limit of many data segments, it is the most efficient (i.e. lowest-variance) estimator possible.
Trigger efficiencies at BES III<|sep|>Trigger efficiencies at BES III were determined for both the J/psi and psi' data taking of 2009. Both dedicated runs and physics datasets are used; efficiencies are presented for Bhabha-scattering events, generic hadronic decay events involving charged tracks, dimuon events and psi' -> pi+pi-J/psi, J/psi -> l+l- events (l an electron or muon). The efficiencies are found to lie well above 99% for all relevant physics cases, thus fulfilling the BES III design specifications.
Black-box Attacks on Automatic Speaker Verification using Feedback-controlled Voice Conversion<|sep|>Automatic speaker verification (ASV) systems in practice are greatly vulnerable to spoofing attacks. The latest voice conversion technologies are able to produce perceptually natural sounding speech that mimics any target speakers. However, the perceptual closeness to a speaker's identity may not be enough to deceive an ASV system. In this work, we propose a framework that uses the output scores of an ASV system as the feedback to a voice conversion system. The attacker framework is a black-box adversary that steals one's voice identity, because it does not require any knowledge about the ASV system but the system outputs. Experimental results conducted on ASVspoof 2019 database confirm that the proposed feedback-controlled voice conversion framework produces adversarial samples that are more deceptive than the straightforward voice conversion, thereby boosting the impostor ASV scores. Further, the perceptual evaluation studies reveal that converted speech does not adversely affect the voice quality from the baseline system.
MMSys'21 Grand Challenge on Detecting Cheapfakes<|sep|>Cheapfake is a recently coined term that encompasses non-AI ("cheap") manipulations of multimedia content. Cheapfakes are known to be more prevalent than deepfakes. Cheapfake media can be created using editing software for image/video manipulations, or even without using any software, by simply altering the context of an image/video by sharing the media alongside misleading claims. This alteration of context is referred to as out-of-context (OOC) misuse} of media. OOC media is much harder to detect than fake media, since the images and videos are not tampered. In this challenge, we focus on detecting OOC images, and more specifically the misuse of real photographs with conflicting image captions in news items. The aim of this challenge is to develop and benchmark models that can be used to detect whether given samples (news image and associated captions) are OOC, based on the recently compiled COSMOS dataset.
Optimality Implies Kernel Sum Classifiers are Statistically Efficient<|sep|>We propose a novel combination of optimization tools with learning theory bounds in order to analyze the sample complexity of optimal kernel sum classifiers. This contrasts the typical learning theoretic results which hold for all (potentially suboptimal) classifiers. Our work also justifies assumptions made in prior work on multiple kernel learning. As a byproduct of our analysis, we also provide a new form of Rademacher complexity for hypothesis classes containing only optimal classifiers.
Testing predictors of eruptivity using parametric flux emergence simulations<|sep|>Solar flares and coronal mass ejections (CMEs) are among the most energetic events in the solar system, impacting the near-Earth environment. Flare productivity is empirically known to be correlated with the size and complexity of active regions. Several indicators, based on magnetic-field data from active regions, have been tested for flare forecasting in recent years. None of these indicators, or combinations thereof, have yet demonstrated an unambiguous eruption or flare criterion. Furthermore, numerical simulations have been only barely used to test the predictability of these parameters. In this context, we used the 3D parametric MHD numerical simulations of the self-consistent formation of the flux emergence of a twisted flux tube, inducing the formation of stable and unstable magnetic flux ropes of Leake (2013, 2014). We use these numerical simulations to investigate the eruptive signatures observable in various magnetic scalar parameters and provide highlights on data analysis processing. Time series of 2D photospheric-like magnetograms are used from parametric simulations of stable and unstable flux emergence, to compute a list of about 100 different indicators. This list includes parameters previously used for operational forecasting, physical parameters used for the first time, as well as new quantities specifically developed for this purpose. Our results indicate that only parameters measuring the total non-potentiality of active regions associated with magnetic inversion line properties, such as the Falconer parameters $L_{ss}$, $WL_{ss}$, $L_{sg}$ and $WL_{sg}$, as well as the new current integral $WL_{sc}$ and length $L_{sc}$ parameters, present a significant ability to distinguish the eruptive cases of the model from the non-eruptive cases, possibly indicating that they are promising flare and eruption predictors.
Universality in all-order $\alpha'$ corrections to BPS/non-BPS brane world volume theories<|sep|>Knowledge of all-$\alpha'$ higher derivative corrections to leading order BPS and non-BPS brane actions would serve in future endeavor of determining the complete form of the non-abelian BPS and tachyonic effective actions. In this paper, we note that there is a universality in the all-$\alpha'$ order corrections to BPS and non-BPS branes. We compute amplitudes between one Ramond-Ramond $C$-field vertex operator and several SYM gauge/scalar vertex operators. Specifically, we evaluate in closed form string correlators of two-point amplitudes $\cal A^{C\phi}$, $\cal A^{CA}$, a three-point amplitude $\cal A^{C\phi\phi}$ and a four-point amplitude $\cal A^{C\phi\phi\phi}$. We carry out pole and contact term analysis. In particular we reproduce some of the contact terms and the infinite massless poles of $\cal A^{C\phi\phi\phi}$ by SYM vertices obtained through the universality.
Statistics of correlation functions in the random Heisenberg chain<|sep|>Ergodic quantum many-body systems satisfy the eigenstate thermalization hypothesis (ETH). However, strong disorder can destroy ergodicity through many-body localization (MBL) -- at least in one dimensional systems -- leading to a clear signal of the MBL transition in the probability distributions of energy eigenstate expectation values of local operators. For a paradigmatic model of MBL, namely the random-field Heisenberg spin chain, we consider the full probability distribution of eigenstate correlation functions across the entire phase diagram. We find gaussian distributions at weak disorder, as predicted by pure ETH. At intermediate disorder -- in the thermal phase -- we find further evidence for anomalous thermalization in the form of heavy tails of the distributions. In the MBL phase, we observe peculiar features of the correlator distributions: a strong asymmetry in $S_i^z S_{i+r}^z$ correlators skewed towards negative values; and a multimodal distribution for spin-flip correlators. A quantitative quasi-degenerate perturbation theory calculation of these correlators yields a surprising agreement of the full distribution with the exact results, revealing, in particular, the origin of the multiple peaks in the spin-flip correlator distribution as arising from the resonant and off-resonant admixture of spin configurations. The distribution of the $S_i^zS_{i+r}^z$ correlator exhibits striking differences between the MBL and Anderson insulator cases.
A Multiple Dry Merger at z=0.18: Witnessing The Assembly of a Massive Elliptical Galaxy<|sep|>Mergers of gas-poor galaxies, so-called dry mergers, may play a fundamental role in the assembly of the most massive galaxies, and therefore, in galaxy formation theories. Using the SDSS, we have serendipitously discovered a rare system in the observational and theoretical context, possibly a quintuple dry merger at low redshift. As a follow-up, we have obtained NOT long-slit spectra of the group, in order to measure the individual redshifts and gain insight into its merger fate. Our results show an isolated, low-redshift galaxy group consisting of massive, quiescent, early-type galaxies, composed of two clumps (possibly themselves in the process of merging), which we estimate will hypothetically merge in roughly less than a Gyr. With the possible exception of the high line-of-sight velocity dispersion, the overall properties of the system may be comparable to a compact Shakhbazyan group. However, when the small projected separations and relative mass ratios of the galaxies are taken into account in cosmological simulations, we find that this system is rather unique. We hypothesize that this group is a dry merger, whose fate will result in the assembly of an isolated, massive elliptical galaxy at low redshift.
Novel View Synthesis from only a 6-DoF Camera Pose by Two-stage Networks<|sep|>Novel view synthesis is a challenging problem in computer vision and robotics. Different from the existing works, which need the reference images or 3D models of the scene to generate images under novel views, we propose a novel paradigm to this problem. That is, we synthesize the novel view from only a 6-DoF camera pose directly. Although this setting is the most straightforward way, there are few works addressing it. While, our experiments demonstrate that, with a concise CNN, we could get a meaningful parametric model that could reconstruct the correct scenery images only from the 6-DoF pose. To this end, we propose a two-stage learning strategy, which consists of two consecutive CNNs: GenNet and RefineNet. GenNet generates a coarse image from a camera pose. RefineNet is a generative adversarial network that refines the coarse image. In this way, we decouple the geometric relationship between mapping and texture detail rendering. Extensive experiments conducted on the public datasets prove the effectiveness of our method. We believe this paradigm is of high research and application value and could be an important direction in novel view synthesis.
Effects of electronic correlations and magnetic field on a molecular ring out of equilibrium<|sep|>We study effects of electron-electron interactions on the steady-state characteristics of a hexagonal molecular ring in a magnetic field, as a model for a benzene molecular junction. The system is driven out of equilibrium by applying a bias voltage across two metallic leads. We employ a model Hamiltonian approach to evaluate the effects of on-site as well as nearest-neighbour density-density type interactions in a physically relevant parameter regime. Results for the steady-state current, charge density and magnetization in three different junction setups (para, meta and ortho) are presented. Our findings indicate that interactions beyond the mean-field level renormalize voltage thresholds as well as current plateaus. Electron-electron interactions lead to substantial charge redistribution as compared to the mean-field results. We identify a strong response of the circular current on the electronic structure of the metallic leads. Our results are obtained by steady-state Cluster Perturbation Theory, a systematically improvable approximation to study interacting molecular junctions out of equilibrium, even in magnetic fields. Within this framework general expressions for the current, charge density and magnetization in the steady-state are derived. The method is flexible and fast and can straight-forwardly be applied to effective models as obtained from ab-initio calculations.
Conformal Mapping Approach to Dipole Shim Design<|sep|>Passive shims are often used to reduce the size and cost of room-temperature magnetic dipoles. In this paper we revisit an analytic approach to the problem of optimum shim design, and we extend it by taking into consideration the effect of magnetic saturation. We derive an abacus curve to determine optimum shim dimensions as a function of the desired dipole nominal field. We show that, for nominal fields below 1.2 T, a pole with such shims can be made at least one half gap height narrower than a pole without. We discuss the range of validity of this approach and verify its predictions using 2 and 3-dimensional finite-element calculations.
The Single Robot Line Coverage Problem: Theory, Algorithms and Experiments<|sep|>Line coverage is the task of servicing a given set of one-dimensional features in an environment. It is important for the inspection of linear infrastructure such as road networks, power lines, and oil and gas pipelines. This paper addresses the single robot line coverage problem for aerial and ground robots by modeling it as an optimization problem on a graph. The problem belongs to the broad class of arc routing problems and is closely related to the asymmetric rural postman problem (RPP). The paper presents an integer linear programming formulation with proof of correctness. Using the minimum cost flow problem, we develop approximation algorithms with guarantees on the solution quality. These guarantees also improve the existing results for the asymmetric RPP. The main algorithm partitions the problem into three cases based on the structure of the required graph, i.e., the graph induced by the features that require servicing. We evaluate our algorithms on road networks from the 50 most populous cities in the world. The algorithms, augmented with improvement heuristics, run within 3s and generate solutions that are within 10% of the optimum. We experimentally demonstrate our algorithms with commercial UAVs on the UNC Charlotte campus road network.
Energy Efficient Processing Allocation in Opportunistic Cloud-Fog-Vehicular Edge Cloud Architectures<|sep|>This paper investigates distributed processing in Vehicular Edge Cloud (VECs), where a group of vehicles in a car park, at a charging station or at a road traffic intersection, cluster and form a temporary vehicular cloud by combining their computational resources in the cluster. We investigated the problem of energy efficient processing task allocation in VEC by developing a Mixed Integer Linear Programming (MILP) model to minimize power consumption by optimizing the allocation of different processing tasks to the available network resources, cloud resources, fog resources and vehicular processing nodes resources. Three dimensions of processing allocation were investigated. The first dimension compared centralized processing (in the central cloud) to distributed processing (in the multi-layer fog nodes). The second dimension introduced opportunistic processing in the vehicular nodes with low and high vehicular node density. The third dimension considered non-splittable tasks (single allocation) versus splittable tasks (distributed allocation), representing real-time versus non real-time applications respectively. The results revealed that a power savings up to 70% can be achieved by allocating processing to the vehicles. However, many factors have an impact on the power saving such the vehicle processing capacities, vehicles density, workload size, and the number of generated tasks. It was observed that the power saving is improved by exploiting the flexibility offered by task splitting among the available vehicles.
Photospheric response to EB-like event<|sep|>Ellerman Bombs are signatures of magnetic reconnection, which is an important physical process in the solar atmosphere. How and where they occur is a subject of debate. In this paper we analyse Sunrise/IMaX data together with 3D MHD simulations that aim to reproduce the exact scenario proposed for the formation of these features. Although the observed event seems to be more dynamic and violent than the simulated one, simulations clearly confirm the basic scenario for the production of EBs. The simulations also reveal the full complexity of the underlying process. The simulated observations show that the Fe I 525.02 nm line gives no information on the height where reconnection takes place. It can only give clues about the heating in the aftermath of the reconnection. The information on the magnetic field vector and velocity at this spatial resolution is, however, extremely valuable because it shows what numerical models miss and how they can be improved.
Neutrino masses and oscillations<|sep|>A report on neutrino masses, mixing and oscillations, made in Dubna at the symposium dedicated to 100 years of the Rutherford's discovery of atomic nucleus, is presented. We start with the hypothesis of neutrino which was proposed by W. Pauli in December 1930 in order to solve some problems of nuclei (the problem of spin of $^{7}N_{14}$ and other nuclei and the problem of continuous $\beta$-spectra). After that we consider the theory of massless two-component neutrino and first ideas of neutrino oscillations which were put forward by B. Pontecorvo in Dubna in 1957-58. The present status of neutrino mixing and oscillations is briefly reviewed. The seesaw mechanism of the generation of small Majorana neutrino masses is discussed and neutrinoless double $\beta$-decay of nuclei is considered. A possibility to probe the Majorana mass mechanism of the $0\nu\beta\beta$-decay is discussed.
Scheme for fault-tolerant holonomic computation on stabilizer codes<|sep|>This paper generalizes and expands upon the work [Phys. Rev. Lett. 102, 070502 (2009)] where we introduced a scheme for fault-tolerant holonomic quantum computation (HQC) on stabilizer codes. HQC is an all-geometric strategy based on non-Abelian adiabatic holonomies, which is known to be robust against various types of errors in the control parameters. The scheme we present shows that HQC is a scalable method of computation, and opens the possibility for combining the benefits of error correction with the inherent resilience of the holonomic approach. We show that with the Bacon-Shor code the scheme can be implemented using Hamiltonian operators of weight 2 and 3.
Bidding in Multi-Unit Auctions under Limited Information<|sep|>We study multi-unit auctions in which bidders have limited knowledge of opponent strategies and values. We characterize optimal prior-free bids; these bids minimize the maximal loss in expected utility resulting from uncertainty surrounding opponent behavior. Optimal bids are simply computable despite bidders having multi-dimensional private information, and in certain cases admit closed-form solutions. In the pay-as-bid auction the minimax-loss bid is unique; in the uniform-price auction the minimax-loss bid is unique if the bidder is allowed to determine the quantities for which they bid, as in many practical applications. Payments to the seller may be higher in either auction format, but minimax-loss bids are never uniformly higher in the pay-as-bid auction.
Capture the Bot: Using Adversarial Examples to Improve CAPTCHA Robustness to Bot Attacks<|sep|>To this date, CAPTCHAs have served as the first line of defense preventing unauthorized access by (malicious) bots to web-based services, while at the same time maintaining a trouble-free experience for human visitors. However, recent work in the literature has provided evidence of sophisticated bots that make use of advancements in machine learning (ML) to easily bypass existing CAPTCHA-based defenses. In this work, we take the first step to address this problem. We introduce CAPTURE, a novel CAPTCHA scheme based on adversarial examples. While typically adversarial examples are used to lead an ML model astray, with CAPTURE, we attempt to make a "good use" of such mechanisms. Our empirical evaluations show that CAPTURE can produce CAPTCHAs that are easy to solve by humans while at the same time, effectively thwarting ML-based bot solvers.
On the nature of dark matter in the Coma Cluster<|sep|>Recent precise observations of the 2.7 K CMB by the Planck mission toward the Coma cluster are not in agreement with X-ray measurements. To reconcile both types of measuring techniques we suggest that unstable dark matter is the cause of this mismatch. Decaying dark matter, which gravitationally dominates the galaxy cluster, can affect the estimated hot plasma content, which is then missing in the measured SZ effect from exactly the same place in the sky. The model independent lifetime of dark matter decaying entirely to X-rays is estimated to be about 6x10^{24} sec; this lifetime scales down with the fraction of the radiatively decaying dark matter. In addition, it is shown that the potential of such dark matter investigations in space is superior to the largest volume Earth-bound dark matter decay searches. Other clusters might provide additional evidence for or against this suggestion.
Classical General Relativity Effects to Second Order in Mass, Spin, and Quadrupole Moment<|sep|>In this contribution, we calculate the light deflection, perihelium shift, time delay and gravitational redshift using an approximate metric that contains the Kerr metric and an approximaction of the Erez-Rosen spacetime. The results were obtained directly using Mathematica. The results agree with the ones presented in the literature, but they are extended until second order terms of mass, angular momentum and mass quadrupole.
Inverse Compton Scattering of the Central Source Photons as an X-ray Emission Mechanism on Kiloparsec Scales in PKS 1127-145<|sep|>The beamed inverse Compton/cosmic microwave background model has generally been used for the interpretation of X-ray radiation from kiloparsec-scale jets of the core-dominated quasars. Recent \textit{Fermi}-LAT and \textit{HST} observations have brought this model into question. We examine the assumption that X-rays from the kiloparsec-scale jet of the quasar PKS~1127$-$145 are produced by inverse Compton scattering of the central source emission. In this context, we show that both similarity and distinction between the observed radio and X-ray spectral indices for some of the jet knots can be explained under a single power-law electron energy distribution. We derive that the viewing angle of the kiloparsec-scale jet is about $35^\circ$ and the jet has a moderate relativistic speed of $\approx 0.8c$. The predicted gamma-ray flux of the jet is found to be a few orders of magnitude lower than the minimum flux level measured by \textit{Fermi}-LAT, further supporting our scenario.
A flexible algorithm for calculating pair interactions on SIMD architectures<|sep|>Calculating interactions or correlations between pairs of particles is typically the most time-consuming task in particle simulation or correlation analysis. Straightforward implementations using a double loop over particle pairs have traditionally worked well, especially since compilers usually do a good job of unrolling the inner loop. In order to reach high performance on modern CPU and accelerator architectures, single-instruction multiple-data (SIMD) parallelization has become essential. Avoiding memory bottlenecks is also increasingly important and requires reducing the ratio of memory to arithmetic operations. Moreover, when pairs only interact within a certain cut-off distance, good SIMD utilization can only be achieved by reordering input and output data, which quickly becomes a limiting factor. Here we present an algorithm for SIMD parallelization based on grouping a fixed number of particles, e.g. 2, 4, or 8, into spatial clusters. Calculating all interactions between particles in a pair of such clusters improves data reuse compared to the traditional scheme and results in a more efficient SIMD parallelization. Adjusting the cluster size allows the algorithm to map to SIMD units of various widths. This flexibility not only enables fast and efficient implementation on current CPUs and accelerator architectures like GPUs or Intel MIC, but it also makes the algorithm future-proof. We present the algorithm with an application to molecular dynamics simulations, where we can also make use of the effective buffering the method introduces.
Better Together: Joint Reasoning for Non-rigid 3D Reconstruction with Specularities and Shading<|sep|>We demonstrate the use of shape-from-shading (SfS) to improve both the quality and the robustness of 3D reconstruction of dynamic objects captured by a single camera. Unlike previous approaches that made use of SfS as a post-processing step, we offer a principled integrated approach that solves dynamic object tracking and reconstruction and SfS as a single unified cost function. Moving beyond Lambertian S f S , we propose a general approach that models both specularities and shading while simultaneously tracking and reconstructing general dynamic objects. Solving these problems jointly prevents the kinds of tracking failures which can not be recovered from by pipeline approaches. We show state-of-the-art results both qualitatively and quantitatively.
Confinement in Gapped Graphene with Magnetic Flux<|sep|>We study the propagation of electrons in a circular quantum dot of gapped graphene subject to the magnetic flux $\phi$. We present analytical expressions for the eigenstates, scattering coefficients, scattering efficiency and radial component of the reflected current. We identify different scattering regimes as a function of the physical parameters such as the incident electronic energy, potential barrier, radius of quantum dot, gap and $\phi$. We choose two values of the flux $\phi=1/2, 3/2$ and show that for low energy of the incident electron, the scattering resonances appear and the far-field scattered current presents distinct preferred scattering directions.
Efficient Subpixel Refinement with Symbolic Linear Predictors<|sep|>We present an efficient subpixel refinement method usinga learning-based approach called Linear Predictors. Two key ideas are shown in this paper. Firstly, we present a novel technique, called Symbolic Linear Predictors, which makes the learning step efficient for subpixel refinement. This makes our approach feasible for online applications without compromising accuracy, while taking advantage of the run-time efficiency of learning based approaches. Secondly, we show how Linear Predictors can be used to predict the expected alignment error, allowing us to use only the best keypoints in resource constrained applications. We show the efficiency and accuracy of our method through extensive experiments.
Controlling the coherence in a pure dephasing model for an arbitrarily prescribed time span<|sep|>We present an open-loop unitary strategy to control the coherence in a pure dephasing model (related to the phase-flip channel) that is able to recover, for whatever prescribed time span, the initial coherence at the end of the control process. The strategy's key idea is to steer the quantum state to the subset of invariant states and keep it there the necessary time, using a fine tuned control Hamiltonian.
A Brief Tutorial on Transformation Based Markov Chain Monte Carlo and Optimal Scaling of the Additive Transformation<|sep|>We consider the recently introduced Transformation-based Markov Chain Monte Carlo (TMCMC) (Dutta and Bhattacharya (2014)), a methodology that is designed to update all the parameters simultaneously using some simple deterministic transformation of a onedimensional random variable drawn from some arbitrary distribution on a relevant support. The additive transformation based TMCMC is similar in spirit to random walk Metropolis, except the fact that unlike the latter, additive TMCMC uses a single draw from a onedimensional proposal distribution to update the high-dimensional parameter. In this paper, we first provide a brief tutorial on TMCMC, exploring its connections and contrasts with various available MCMC methods. Then we study the diffusion limits of additive TMCMC under various set-ups ranging from the product structure of the target density to the case where the target is absolutely continuous with respect to a Gaussian measure; we also consider the additive TMCMC within Gibbs approach for all the above set-ups. These investigations lead to appropriate scaling of the one-dimensional proposal density. We also show that the optimal acceptance rate of additive TMCMC is 0.439 under all the aforementioned set-ups, in contrast with the well-established 0.234 acceptance rate associated with optimal random walk Metropolis algorithms under the same set-ups. We also elucidate the ramifications of our results and clear advantages of additive TMCMC over random walk Metropolis with ample simulation studies and Bayesian analysis of a real, spatial dataset with which 160 unknowns are associated.
Azimuthal decorrelation of Mueller-Navelet jets at the Tevatron and the LHC<|sep|>We study the production of Mueller-Navelet jets at hadron colliders in the Balitsky-Fadin-Kuraev-Lipatov (BFKL) framework. We show that a measurement of the relative azimuthal angle \Delta\Phi between the jets can provide a good testing ground for corrections due to next-leading logarithms (NLL). Besides the well-known azimuthal decorrelation with increasing rapidity interval \Delta\eta between the jets, we propose to also measure this effect as a function of R=k_2/k_1, the ratio between the jets transverse momenta. Using renormalisation-group improved NLL kernel, we obtain predictions for d\sigma/d\Delta\eta dR d\Delta\Phi. We analyse NLL-scheme and renormalisation-scale uncertainties, and energy-momentum conservation effects, in order to motivate a measurement at the Tevatron and the LHC.
Finding complex balanced and detailed balanced realizations of chemical reaction networks<|sep|>Reversibility, weak reversibility and deficiency, detailed and complex balancing are generally not "encoded" in the kinetic differential equations but they are realization properties that may imply local or even global asymptotic stability of the underlying reaction kinetic system when further conditions are also fulfilled. In this paper, efficient numerical procedures are given for finding complex balanced or detailed balanced realizations of mass action type chemical reaction networks or kinetic dynamical systems in the framework of linear programming. The procedures are illustrated on numerical examples.
The cosmological singularity problem<|sep|>Despite impressive phenomenological successes, cosmological models are incomplete without an understanding of what happened at the big bang singularity. Depending on the model, one would like to understand how appropriate initial conditions were selected at the big bang singularity, or how a pre-existing contracting universe underwent a big crunch/big bang transition, if such transitions are possible at all. In this talk, after an introduction to these questions, an attempt is described to study cosmological singularities using the AdS/CFT correspondence. A specific model in which asymptotically AdS initial data evolve into a big crunch singularity is discussed and a dual field theory description is provided.
Converse extensionality and apartness<|sep|>In this paper we try to find a computational interpretation for a strong form of extensionality, which we call "converse extensionality". Converse extensionality principles, which arise as the Dialectica interpretation of the axiom of extensionality, were first studied by Howard. In order to give a computational interpretation to these principles, we reconsider Brouwer's apartness relation, a strong constructive form of inequality. Formally, we provide a categorical construction to endow every typed combinatory algebra with an apartness relation. We then exploit that functions reflect apartness, in addition to preserving equality, to prove that the resulting categories of assemblies model a converse extensionality principle.
New non-local SUSY KdV conservation laws from a recursive gradient algorithm<|sep|>A complete proof of the recursive gradient approach is presented. It gives a construction of all the hierarchy structures of N=1 Super KdV, including the non-local one. A precise definition of the ring of superfields involved in the non-local construction is given. In particular, new non-local conserved quantities of N=1 Super KdV are found.
Parity violating effects in an exotic perturbation of the rigid rotator<|sep|>The perturbation of the free rigid rotator by the trigonometric Scarf potential is shown to conserve its energy excitation patterns and change only the wave functions towards spherical harmonics rescaled by a function of an unspecified parity, or mixtures of such rescaled harmonics of equal magnetic quantum numbers and different angular momenta. In effect, no parity can be assigned to the states of the rotational bands emerging in this exotic way, and the electric dipole operator is allowed to acquire non-vanishing expectation values.
Controversy on a dispersion relation for MHD waves<|sep|>Kumar et al. (2006) obtained a fifth order polynomial in $\omega$ for the dispersion relation and pointed out that the calculations preformed by Porter et al. (1994) and by Dwivedi & Pandey (2003) seem to be in error, as they obtained a sixth order polynomial. The energy equation of Dwivedi & Pandey (2003) was dimensionally wrong. Dwivedi & Pandey (2006) corrected the energy equation and still claimed that the dispersion relation must be a sixth order polynomial. The equations (11) $-$ (19) of Dwivedi & Pandey (2006) and the equations (24) $-$ (32) Kumar et al. (2006) are the same. This fact has been expressed by Kumar et al. (2006) themselves. Even then they tried to show this set of equations on one side gives the sixth order polynomial as they got; on the other side, the same set of equations gives the fifth order polynomial as Kumar et al. (2006) obtained. The situation appears to be non-scientific, as the system of equations is a linear one. These are simple algebraic equations where the variables are to be eliminated. However, it is a matter of surprise that by solving these equations, two scientific groups are getting polynomials of different degrees. In the present discussion, we have attempted to short out this discrepancy.
Constraining the duty cycle of transient low-mass X-ray binaries through simulations<|sep|>We performed simulations of a large number of so-called very faint X-ray transient sources from surveys obtained using the X-ray telescope aboard the Neil Gehrels \emph{Swift} Observatory on two Galactic globular clusters, and the Galactic Center. We calculated the ratio between the duty cycle we input in our simulations and the one we measure after the simulations. We found that fluctuations in outburst duration and recurrence times affect our estimation of the duty cycle more than non detected outbursts. This biases our measures to overestimate the simulated duty cycle of sources. Moreover, we determined that compact surveys are necessary to detect outbursts with short duration because they could fall in gaps between observations, if such gaps are longer than their duration. On the other hand, long surveys are necessary to detect sources with low duty cycle because the smallest duty cycle a survey can observe is given by the ratio between the shortest outburst duration and the total length of the survey. If one has a limited amount of observing time, these two effects are competing, and a compromise is required which is set by the goals of the proposed survey. We have also performed simulations with several artificial survey strategies in order to evaluate the optimal observing campaign aimed at detecting transients as well as at having the most accurate estimates of the duty cycle. As expected, the best campaign would be a regular and dense monitoring that extends for a very long period. The closest real example of such a dataset is the monitoring of the Galactic Centre.
Wet paper codes and the dual distance in steganography<|sep|>In 1998 Crandall introduced a method based on coding theory to secretly embed a message in a digital support such as an image. Later Fridrich et al. improved this method to minimize the distortion introduced by the embedding; a process called wet paper. However, as previously emphasized in the literature, this method can fail during the embedding step. Here we find sufficient and necessary conditions to guarantee a successful embedding by studying the dual distance of a linear code. Since these results are essentially of combinatorial nature, they can be generalized to systematic codes, a large family containing all linear codes. We also compute the exact number of solutions and point out the relationship between wet paper codes and orthogonal arrays.
Meson-exchange currents and quasielastic neutrino cross sections in the SuperScaling Approximation model<|sep|>We evaluate the quasielastic double differential neutrino cross sections obtained in a phenomenological model based on the superscaling behavior of electron scattering data. We compare our results with the recent experimental data for neutrinos of MiniBooNE and estimate the contribution of the vector meson-exchange currents in the 2p-2h sector.
Spontaneous breaking of permutation symmetry in pseudo-Hermitian quantum mechanics<|sep|>By adding an imaginary interacting term proportional to ip_1p_2 to the Hamiltonian of a free anisotropic planar oscillator, we construct a new model which is described by the PT-pseudo-Hermitian Hamiltonian with the permutation symmetry of two dimensions. We prove that our model is equivalent to the Pais-Uhlenbeck oscillator and thus establish a relationship between our PT-pseudo-Hermitian system and the fourth-order derivative oscillator model. We also point out the spontaneous breaking of permutation symmetry which plays a crucial role in giving a real spectrum free of interchange of positive and negative energy levels in our model. Moreover, we find that the permutation symmetry of two dimensions in our Hamiltonian corresponds to the identity (not in magnitude but in attribute) of two different frequencies in the Pais-Uhlenbeck oscillator, and reveal that the unequal-frequency condition imposed as a prerequisite upon the Pais-Uhlenbeck oscillator can reasonably be explained as the spontaneous breaking of this identity.
$p$-Laplacian Based Graph Neural Networks<|sep|>Graph neural networks (GNNs) have demonstrated superior performance for semi-supervised node classification on graphs, as a result of their ability to exploit node features and topological information simultaneously. However, most GNNs implicitly assume that the labels of nodes and their neighbors in a graph are the same or consistent, which does not hold in heterophilic graphs, where the labels of linked nodes are likely to differ. Hence, when the topology is non-informative for label prediction, ordinary GNNs may work significantly worse than simply applying multi-layer perceptrons (MLPs) on each node. To tackle the above problem, we propose a new $p$-Laplacian based GNN model, termed as $^p$GNN, whose message passing mechanism is derived from a discrete regularization framework and could be theoretically explained as an approximation of a polynomial graph filter defined on the spectral domain of $p$-Laplacians. The spectral analysis shows that the new message passing mechanism works simultaneously as low-pass and high-pass filters, thus making $^p$GNNs are effective on both homophilic and heterophilic graphs. Empirical studies on real-world and synthetic datasets validate our findings and demonstrate that $^p$GNNs significantly outperform several state-of-the-art GNN architectures on heterophilic benchmarks while achieving competitive performance on homophilic benchmarks. Moreover, $^p$GNNs can adaptively learn aggregation weights and are robust to noisy edges.
Decoding Methods for Neural Narrative Generation<|sep|>Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters -- specifically, maximum mutual information -- analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best with thresholds between 0.7 and 0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric.
Pareto Probing: Trading Off Accuracy for Complexity<|sep|>The question of how to probe contextual word representations for linguistic structure in a way that is both principled and useful has seen significant attention recently in the NLP literature. In our contribution to this discussion, we argue for a probe metric that reflects the fundamental trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments using Pareto hypervolume as an evaluation metric show that probes often do not conform to our expectations---e.g., why should the non-contextual fastText representations encode more morpho-syntactic information than the contextual BERT representations? These results suggest that common, simplistic probing tasks, such as part-of-speech labeling and dependency arc labeling, are inadequate to evaluate the linguistic structure encoded in contextual word representations. This leads us to propose full dependency parsing as a probing task. In support of our suggestion that harder probing tasks are necessary, our experiments with dependency parsing reveal a wide gap in syntactic knowledge between contextual and non-contextual representations.
$SU(5)$ GUTs with $A_4$ modular symmetry<|sep|>We combine $SU(5)$ Grand Unified Theories (GUTs) with $A_4$ modular symmetry and present a comprehensive analysis of the resulting quark and lepton mass matrices for all the simplest cases. Classifying the models according to the representation assignments of the matter fields under $A_4$, we find that there are seven types of $SU(5)$ models with $A_4$ modular symmetry. We present 53 benchmark models with the fewest free parameters. The parameter space of each model is scanned to optimize the agreement between predictions and experimental data, and predictions for the masses and mixing parameters of quarks and leptons are given at the best fitting points. The best fit predictions for the leptonic CP violating Dirac phase, the lightest neutrino mass and the neutrinoless double beta decay parameter when displayed graphically are observed to cover a wide range of possible values, but are clustered around particular regions, allowing future neutrino experiments to discriminate between the different types of models.
Observational signatures of strongly naked singularities: image of the thin accretion disk<|sep|>We study the optical appearance of a thin accretion disk around the strongly naked static Janis-Newman-Winicour singularity. The solution does not possess a photon sphere, which results in the formation of a complex structure of bright rings in the central region of the disk image. Such structure is absent in the case of the Schwarzschild black hole with a thin accretion disk, where instead of the image we observe the black hole shadow. Some of the rings emit with the maximal observable radiation flux from the accretion disk, and should be experimentally detectable. Thus, this qualitatively new feature can be used to distinguish observationally black holes from naked singularities. We elucidate the appearance of the ring structure by revealing the physical mechanism of its formation, and explaining the nature of each of the ring images. We make the conjecture that a similar structure would also appear for other solutions without a photon sphere and it can serve as a general observational signature for distinguishing compact objects possessing no photon sphere from black holes.
A Committee of Convolutional Neural Networks for Image Classication in the Concurrent Presence of Feature and Label Noise<|sep|>Image classification has become a ubiquitous task. Models trained on good quality data achieve accuracy which in some application domains is already above human-level performance. Unfortunately, real-world data are quite often degenerated by the noise existing in features and/or labels. There are quite many papers that handle the problem of either feature or label noise, separately. However, to the best of our knowledge, this piece of research is the first attempt to address the problem of concurrent occurrence of both types of noise. Basing on the MNIST, CIFAR-10 and CIFAR-100 datasets, we experimentally proved that the difference by which committees beat single models increases along with noise level, no matter it is an attribute or label disruption. Thus, it makes ensembles legitimate to be applied to noisy images with noisy labels. The aforementioned committees' advantage over single models is positively correlated with dataset difficulty level as well. We propose three committee selection algorithms that outperform a strong baseline algorithm which relies on an ensemble of individual (nonassociated) best models.
An Incentivized Approach for Fair Participation in Wireless Ad hoc Networks<|sep|>In Wireless Ad hoc networks (WANETs), nodes separated by considerable distance communicate with each other by relaying their messages through other nodes. However, it might not be in the best interests of a node to forward the message of another node due to power constraints. In addition, all nodes being rational, some nodes may be selfish, i.e. they might not relay data from other nodes so as to increase their lifetime. In this paper, we present a fair and incentivized approach for participation in Ad hoc networks. Given the power required for each transmission, we are able to determine the power saving contributed by each intermediate hop. We propose the FAir Share incenTivizEd Ad hoc paRticipation protocol (FASTER), which takes a selected route from a routing protocol as input, to calculate the worth of each node using the cooperative game theory concept of 'Shapley Value' applied on the power saved by each node. This value can be used for allocation of Virtual Currency to the nodes, which can be spent on subsequent message transmissions.
Efficient Query Processing for SPARQL Federations with Replicated Fragments<|sep|>Low reliability and availability of public SPARQL endpoints prevent real-world applications from exploiting all the potential of these querying infras-tructures. Fragmenting data on servers can improve data availability but degrades performance. Replicating fragments can offer new tradeoff between performance and availability. We propose FEDRA, a framework for querying Linked Data that takes advantage of client-side data replication, and performs a source selection algorithm that aims to reduce the number of selected public SPARQL endpoints, execution time, and intermediate results. FEDRA has been implemented on the state-of-the-art query engines ANAPSID and FedX, and empirically evaluated on a variety of real-world datasets.
A Pushing-Grasping Collaborative Method Based on Deep Q-Network Algorithm in Dual Perspectives<|sep|>Aiming at the traditional grasping method for manipulators based on 2D camera, when faced with the scene of gathering or covering, it can hardly perform well in unstructured scenes that appear as gathering and covering, for the reason that can't recognize objects accurately in cluster scenes from a single perspective and the manipulators can't make the environment better for grasping. In this case, a novel method of pushing-grasping collaborative based on the deep Q-network in dual perspectives is proposed in this paper. This method adopts an improved deep Q network algorithm, with an RGB-D camera to obtain the information of objects' RGB images and point clouds from two perspectives, and combines the pushing and grasping actions so that the trained manipulator can make the scenes better for grasping so that it can perform well in more complicated grasping scenes. What's more, we improved the reward function of the deep Q-network and propose the piecewise reward function to speed up the convergence of the deep Q-network. We trained different models and tried different methods in the V-REP simulation environment, and it concluded that the method proposed in this paper converges quickly and the success rate of grasping objects in unstructured scenes raises up to 83.5%. Besides, it shows the generalization ability and well performance when novel objects appear in the scenes that the manipulator has never grasped before.
The nature of the tensor order in Cd2Re2O7<|sep|>The pyrochlore metal Cd2Re2O7 has been recently investigated by second-harmonic generation (SHG) reflectivity. In this paper, we develop a general formalism that allows for the identification of the relevant tensor components of the SHG from azimuthal scans. We demonstrate that the secondary order parameter identified by SHG at the structural phase transition is the x2-y2 component of the axial toroidal quadrupole. This differs from the 3z2-r2 symmetry of the atomic displacements associated with the I-4m2 crystal structure that was previously thought to be its origin. Within the same formalism, we suggest that the primary order parameter detected in the SHG experiment is the 3z2-r2 component of the magnetic quadrupole. We discuss the general mechanism driving the phase transition in our proposed framework, and suggest experiments, particularly resonant X-ray scattering ones, that could clarify this issue.
The Hanle Effect of the Hydrogen Ly-alpha Line for Probing the Magnetism of the Solar Transition Region<|sep|>We present some theoretical predictions concerning the amplitude and magnetic sensitivity of the linear polarization signals produced by scattering processes in the hydrogen Ly-alpha line of the solar transition region. To this end, we have calculated the atomic level polarization (population imbalances and quantum coherences) induced by anisotropic radiation pumping in semi-empirical and hydrodynamical models of the solar atmosphere, taking into account radiative transfer and the Hanle effect caused by the presence of organized and random magnetic fields. The line-center amplitudes of the emergent linear polarization signals are found to vary typically between 0.1% and 1%, depending on the scattering geometry and the strength and orientation of the magnetic field. The results shown here encourage the development of UV polarimeters for sounding rockets and space telescopes with the aim of opening up a diagnostic window for magnetic field measurements in the upper chromosphere and transition region of the Sun.
Are news important to predict large losses?<|sep|>In this paper we investigate the impact of news to predict extreme financial returns using high frequency data. We consider several model specifications differing for the dynamic property of the underlying stochastic process as well as for the innovation process. Since news are essentially qualitative measures, they are firstly transformed into quantitative measures which are subsequently introduced as exogenous regressors into the conditional volatility dynamics. Three basic sentiment indexes are constructed starting from three list of words defined by historical market news response and by a discriminant analysis. Models are evaluated in terms of their predictive accuracy to forecast out-of-sample Value-at-Risk of the STOXX Europe 600 sectors at different confidence levels using several statistic tests and the Model Confidence Set procedure of Hansen et al. (2011). Since the Hansen's procedure usually delivers a set of models having the same VaR predictive ability, we propose a new forecasting combination technique that dynamically weights the VaR predictions obtained by the models belonging to the optimal final set. Our results confirms that the inclusion of exogenous information as well as the right specification of the returns' conditional distribution significantly decrease the number of actual versus expected VaR violations towards one, as this is especially true for higher confidence levels.
Measuring and Mitigating the Risk of IP Reuse on Public Clouds<|sep|>Public clouds provide scalable and cost-efficient computing through resource sharing. However, moving from traditional on-premises service management to clouds introduces new challenges; failure to correctly provision, maintain, or decommission elastic services can lead to functional failure and vulnerability to attack. In this paper, we explore a broad class of attacks on clouds which we refer to as cloud squatting. In a cloud squatting attack, an adversary allocates resources in the cloud (e.g., IP addresses) and thereafter leverages latent configuration to exploit prior tenants. To measure and categorize cloud squatting we deployed a custom Internet telescope within the Amazon Web Services us-east-1 region. Using this apparatus, we deployed over 3 million servers receiving 1.5 million unique IP addresses (56% of the available pool) over 101 days beginning in March of 2021. We identified 4 classes of cloud services, 7 classes of third-party services, and DNS as sources of exploitable latent configurations. We discovered that exploitable configurations were both common and in many cases extremely dangerous; we received over 5 million cloud messages, many containing sensitive data such as financial transactions, GPS location, and PII. Within the 7 classes of third-party services, we identified dozens of exploitable software systems spanning hundreds of servers (e.g., databases, caches, mobile applications, and web services). Lastly, we identified 5446 exploitable domains spanning 231 eTLDs-including 105 in the top 10,000 and 23 in the top 1000 popular domains. Through tenant disclosures we have identified several root causes, including (a) a lack of organizational controls, (b) poor service hygiene, and (c) failure to follow best practices. We conclude with a discussion of the space of possible mitigations and describe the mitigations to be deployed by Amazon in response to this study.
Heavy quark production in $k_t$ factorization approach at LHC energies<|sep|>A new version of the $k_T$ factorization approach is formulated for the high energy heavy quark production. The results are in reasonable agreement with the experimental data at LHC energies.
Computing Optimal Kernels in Two Dimensions<|sep|>Let $P$ be a set of $n$ points in $\mathbb{R}^2$. A subset $C\subseteq P$ is an $\varepsilon$-kernel of $P$ if the projection of the convex hull of $C$ approximates that of $P$ within $(1-\varepsilon)$-factor in every direction. The set $C$ is a weak $\varepsilon$-kernel if its directional width approximates that of $P$ in every direction. We present fast algorithms for computing a minimum-size $\varepsilon$-kernel as well as a weak $\varepsilon$-kernel. We also present a fast algorithm for the Hausdorff variant of this problem. In addition, we introduce the notion of $\varepsilon$-core, a convex polygon lying inside $CH(P)$, prove that it is a good approximation of the optimal $\varepsilon$-kernel, present an efficient algorithm for computing it, and use it to compute an $\varepsilon$-kernel of small size.
A nonparametric HMM for genetic imputation and coalescent inference<|sep|>Genetic sequence data are well described by hidden Markov models (HMMs) in which latent states correspond to clusters of similar mutation patterns. Theory from statistical genetics suggests that these HMMs are nonhomogeneous (their transition probabilities vary along the chromosome) and have large support for self transitions. We develop a new nonparametric model of genetic sequence data, based on the hierarchical Dirichlet process, which supports these self transitions and nonhomogeneity. Our model provides a parameterization of the genetic process that is more parsimonious than other more general nonparametric models which have previously been applied to population genetics. We provide truncation-free MCMC inference for our model using a new auxiliary sampling scheme for Bayesian nonparametric HMMs. In a series of experiments on male X chromosome data from the Thousand Genomes Project and also on data simulated from a population bottleneck we show the benefits of our model over the popular finite model fastPHASE, which can itself be seen as a parametric truncation of our model. We find that the number of HMM states found by our model is correlated with the time to the most recent common ancestor in population bottlenecks. This work demonstrates the flexibility of Bayesian nonparametrics applied to large and complex genetic data.
The Bullet cluster at its best: weighing stars, gas and dark matter<|sep|>We present a new strong lensing mass reconstruction of the Bullet cluster (1E 0657-56) at z=0.296, based on WFC3 and ACS HST imaging and VLT/FORS2 spectroscopy. The strong lensing constraints underwent substantial revision compared to previously published analysis, there are now 14 (six new and eight previously known) multiply-imaged systems, of which three have spectroscopically confirmed redshifts (including one newly measured from this work). The reconstructed mass distribution explicitly included the combination of three mass components: i) the intra-cluster gas mass derived from X-ray observation, ii) the cluster galaxies modeled by their fundamental plane scaling relations and iii) dark matter. The model that includes the intra-cluster gas is the one with the best Bayesian evidence. This model has a total RMS value of 0.158" between the predicted and measured image positions for the 14 multiple images considered. The proximity of the total RMS to resolution of HST/WFC3 and ACS (0.07-0.15" FWHM) demonstrates the excellent precision of our mass model. The derived mass model confirms the spatial offset between the X-ray gas and dark matter peaks. The fraction of the galaxy halos mass to total mass is found to be f_s=11+/-5% for a total mass of 2.5+/-0.1 x 10^14 solar mass within a 250 kpc radial aperture.
Confronting quasi-exponential inflation with WMAP seven<|sep|>We confront quasi-exponential models of inflation with WMAP seven years dataset using Hamilton Jacobi formalism. With a phenomenological Hubble parameter, representing quasi exponential inflation, we develop the formalism and subject the analysis to confrontation with WMAP seven using the publicly available code CAMB. The observable parameters are found to fair extremely well with WMAP seven. We also obtain a ratio of tensor to scalar amplitudes which may be detectable in PLANCK.
Deep Learning Techniques for Compressive Sensing-Based Reconstruction and Inference -- A Ubiquitous Systems Perspective<|sep|>Compressive sensing (CS) is a mathematically elegant tool for reducing the sampling rate, potentially bringing context-awareness to a wider range of devices. Nevertheless, practical issues with the sampling and reconstruction algorithms prevent further proliferation of CS in real world domains, especially among heterogeneous ubiquitous devices. Deep learning (DL) naturally complements CS for adapting the sampling matrix, reconstructing the signal, and learning form the compressed samples. While the CS-DL integration has received substantial research interest recently, it has not yet been thoroughly surveyed, nor has the light been shed on practical issues towards bringing the CS-DL to real world implementations in the ubicomp domain. In this paper we identify main possible ways in which CS and DL can interplay, extract key ideas for making CS-DL efficient, identify major trends in CS-DL research space, and derive guidelines for future evolution of CS-DL within the ubicomp domain.
Augmented Neural ODEs<|sep|>We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.
Mass Functions, Luminosity Functions, and Completeness Measurements from Clustering Redshifts<|sep|>This paper presents stellar mass functions and i-band luminosity functions for Sloan Digital Sky Survey (SDSS) galaxies at $i < 21$ using clustering redshifts, from which we also compute targeting completeness measurements for the Baryon Oscillation Spectroscopic Survey (BOSS). Clustering redshifts is a method of obtaining the redshift distribution of a sample of galaxies with only photometric information by measuring the angular crosscorrelation with a spectroscopic sample in different redshift bins. We construct a spectroscopic sample containing data from the BOSS + eBOSS surveys, allowing us to recover redshift distributions from photometric data out to $z\simeq 2.5$. We produce k-corrected i-band luminosity functions and stellar mass functions by applying clustering redshifts to SDSS DR8 galaxies in small bins of colour and magnitude. There is little evolution in the mass function between $0.2 < z < 0.8$, implying the most massive galaxies form most of their mass before $z = 0.8$. These mass functions are used to produce stellar mass completeness estimates for the Baryon Oscillation Spectroscopic Survey (BOSS), giving a stellar mass completeness of $80\%$ above $M_{\star} > 10^{11.4}$ between $0.2 < z < 0.7$, with completeness falling significantly at redshifts higher than 0.7, and at lower masses. Large photometric datasets will be available in the near future (DECaLS, DES, Euclid), so this, and similar techniques will become increasingly useful in order to fully utilise this data.
High-performance parallel classical scheme for simulating shallow quantum circuits<|sep|>Recently, constant-depth quantum circuits are proved more powerful than their classical counterparts at solving certain problems, e.g., the two-dimensional (2D) hidden linear function (HLF) problem regarding a symmetric binary matrix. To further investigate the boundary between classical and quantum computing models, in this work we propose a high-performance two-stage classical scheme to solve a full-sampling variant of the 2D HLF problem, which combines traditional classical parallel algorithms and a gate-based classical circuit model together for exactly simulating the target shallow quantum circuits. Under reasonable parameter assumptions, a theoretical analysis reveals our classical simulator consumes less runtime than that of near-term quantum processors for most problem instances. Furthermore, we demonstrate the typical all-connected 2D grid instances by moderate FPGA circuits, and show our designed parallel scheme is a practically scalable, high-efficient and operationally convenient tool for simulating and verifying graph-state circuits performed by current quantum hardware.
A Reconnecting Current Sheet Imaged in A Solar Flare<|sep|>Magnetic reconnection changes the magnetic field topology and powers explosive events in astrophysical, space and laboratory plasmas. For flares and coronal mass ejections (CMEs) in the solar atmosphere, the standard model predicts the presence of a reconnecting current sheet, which has been the subject of considerable theoretical and numerical modeling over the last fifty years, yet direct, unambiguous observational verification has been absent. In this Letter we show a bright sheet structure of global length (>0.25 Rsun) and macroscopic width ((5 - 10)x10^3 km) distinctly above the cusp-shaped flaring loop, imaged during the flare rising phase in EUV. The sheet formed due to the stretch of a transequatorial loop system, and was accompanied by various reconnection signatures that have been dispersed in the literature. This unique event provides a comprehensive view of the reconnection geometry and dynamics in the solar corona.
Fair Insurance Premium Level in Connected SIR Model under Epidemic Outbreak<|sep|>In this paper we aim to study an optimal insurance premium level for health-care in a deterministic and stochastic SIR models with migration fluxes and vaccination of population. The studied model considers two standard SIR centres connected via links and continuous migration fluxes. The premium is calculated using the basic equivalence principle. Even in this simple setup there are non-intuitive results that illustrate how the premium depends on migration rates, severeness of a disease and initial distribution of healthy and infected individuals through the centres. We investigate how the vaccination program effects the insurance costs by comparing the savings in benefits with the expenses for vaccination. We compare the results of deterministic and stochastic models.
Learning Non-Lambertian Object Intrinsics across ShapeNet Categories<|sep|>We consider the non-Lambertian object intrinsic problem of recovering diffuse albedo, shading, and specular highlights from a single image of an object. We build a large-scale object intrinsics database based on existing 3D models in the ShapeNet database. Rendered with realistic environment maps, millions of synthetic images of objects and their corresponding albedo, shading, and specular ground-truth images are used to train an encoder-decoder CNN. Once trained, the network can decompose an image into the product of albedo and shading components, along with an additive specular component. Our CNN delivers accurate and sharp results in this classical inverse problem of computer vision, sharp details attributed to skip layer connections at corresponding resolutions from the encoder to the decoder. Benchmarked on our ShapeNet and MIT intrinsics datasets, our model consistently outperforms the state-of-the-art by a large margin. We train and test our CNN on different object categories. Perhaps surprising especially from the CNN classification perspective, our intrinsics CNN generalizes very well across categories. Our analysis shows that feature learning at the encoder stage is more crucial for developing a universal representation across categories. We apply our synthetic data trained model to images and videos downloaded from the internet, and observe robust and realistic intrinsics results. Quality non-Lambertian intrinsics could open up many interesting applications such as image-based albedo and specular editing.
Modelling and Analysis of AODV in UPPAAL<|sep|>This paper describes work in progress towards an automated formal and rigorous analysis of the Ad hoc On-Demand Distance Vector (AODV) routing protocol, a popular protocol used in ad hoc wireless networks. We give a brief overview of a model of AODV implemented in the UPPAAL model checker, and describe experiments carried out to explore AODV's behaviour in two network topologies. We were able to locate automatically and confirm some known problematic and undesirable behaviours. We believe this use of model checking as a diagnostic tool complements other formal methods based protocol modelling and verification techniques, such as process algebras. Model checking is in particular useful for the discovery of protocol limitations and in the development of improved variations.
A BRITE view on delta Scuti and gamma Doradus stars<|sep|>BRITE-Constellation has obtained data for a few delta Scuti and gamma Doradus type stars. A short overview of the pulsational content found in five stars - beta Cassiopeiae, epsilon Cephei, M Velorum, beta Pictoris and QW Puppis - is given and the potential of BRITE-Constellation observations of delta Scuti and gamma Doradus pulsators is discussed.
3D simulations of the accretion process in Kerr space-time with arbitrary value of the spin parameter<|sep|>We present the results of three-dimensional general relativistic hydrodynamic simulations of adiabatic and spherically symmetric accretion in Kerr space-time. We consider compact objects with spin parameter $|a_*| \le 1$ (black holes) and with $|a_*| > 1$ (super-spinars). Our full three-dimensional simulations confirm the formation of equatorial outflows for high values of $|a_*|$, as found in our previous work in 2.5 dimensions. We show that the critical value of $|a_*|$ determining the onset of powerful outflows depends mainly on the radius of the compact object. The phenomenon of equatorial outflows can hardly occur around a black hole and may thus be used to test the bound $|a_*| \le 1$ for astrophysical black hole candidates.
Facke: a Survey on Generative Models for Face Swapping<|sep|>In this work, we investigate into the performance of mainstream neural generative models on the very task of swapping faces. We have experimented on CVAE, CGAN, CVAE-GAN, and conditioned diffusion models. Existing finely trained models have already managed to produce fake faces (Facke) indistinguishable to the naked eye as well as achieve high objective metrics. We perform a comparison among them and analyze their pros and cons. Furthermore, we proposed some promising tricks though they do not apply to this task.
Faster Computation of Path-Width<|sep|>Tree-width and path-width are widely successful concepts. Many NP-hard problems have efficient solutions when restricted to graphs of bounded tree-width. Many efficient algorithms are based on a tree decomposition. Sometimes the more restricted path decomposition is required. The bottleneck for such algorithms is often the computation of the width and a corresponding tree or path decomposition. For graphs with $n$ vertices and tree-width or path-width $k$, the standard linear time algorithm to compute these decompositions dates back to 1996. Its running time is linear in $n$ and exponential in $k^3$ and not usable in practice. Here we present a more efficient algorithm to compute the path-width and provide a path decomposition. Its running time is $2^{O(k^2)} n$. In the classical algorithm of Bodlaender and Kloks, the path decomposition is computed from a tree decomposition. Here, an optimal path decomposition is computed from a path decomposition of about twice the width. The latter is computed from a constant factor smaller graph.
Chirp Spread Spectrum Signaling for Future Air-Ground Communications<|sep|>In this paper, we investigate the use of chirp spread spectrum signaling over air-ground channels. This includes evaluation of not only the traditional linear chirp, but also of a new chirp signal format we have devised for multiple access applications. This new format is more practical than prior multi-user chirp systems in the literature, because we allow for imperfect synchronism. Specifically we evaluate multi-user chirp signaling over air-ground channels in a quasi-synchronous condition. The air-ground channels we employ are models based upon an extensive NASA measurement campaign. We show that our new signaling scheme outperforms the classic linear chirp in these air-ground settings.
D-NetPAD: An Explainable and Interpretable Iris Presentation Attack Detector<|sep|>An iris recognition system is vulnerable to presentation attacks, or PAs, where an adversary presents artifacts such as printed eyes, plastic eyes, or cosmetic contact lenses to circumvent the system. In this work, we propose an effective and robust iris PA detector called D-NetPAD based on the DenseNet convolutional neural network architecture. It demonstrates generalizability across PA artifacts, sensors and datasets. Experiments conducted on a proprietary dataset and a publicly available dataset (LivDet-2017) substantiate the effectiveness of the proposed method for iris PA detection. The proposed method results in a true detection rate of 98.58\% at a false detection rate of 0.2\% on the proprietary dataset and outperfoms state-of-the-art methods on the LivDet-2017 dataset. We visualize intermediate feature distributions and fixation heatmaps using t-SNE plots and Grad-CAM, respectively, in order to explain the performance of D-NetPAD. Further, we conduct a frequency analysis to explain the nature of features being extracted by the network. The source code and trained model are available at https://github.com/iPRoBe-lab/D-NetPAD.
Moir\'e-of-moir\'e low-energy effective theory of twisted trilayer graphene<|sep|>Stacking three monolayers of graphene with a twist generally produces two moir\'e patterns. A moir\'e of moir\'e structure then emerges at larger distance where the three layers periodically realign. We devise here an effective low-energy theory to describe the spectrum at distances larger than the moir\'e lengthscale. In each valley of the underlying graphene, the theory comprises one Dirac cone at the ${\bf \Gamma}_M$ point of the moir\'e Brillouin zone and two weakly gapped points at ${\bf K}_M$ and ${\bf K}'_M$. The velocities and small gaps exhibit a spatial dependence in the moir\'e-of-moir\'e unit cell, entailing a non-abelian connection potential which ensures gauge invariance. The resulting model is numerically solved and a fully connected spectrum is obtained, which is protected by the combination of time-reversal and twofold-rotation symmetries.
A simple model for explaining Galaxy Rotation Curves<|sep|>A new simple expression for the circular velocity of spiral galaxies is proposed and tested against HI Nearby Galaxy Survey (THINGS) data set. Its accuracy is compared with the one coming from MOND.
Mean-Reverting Portfolio Design via Majorization-Minimization Method<|sep|>This paper considers the mean-reverting portfolio design problem arising from statistical arbitrage in the financial markets. The problem is formulated by optimizing a criterion characterizing the mean-reversion strength of the portfolio and taking into consideration the variance of the portfolio and an investment budget constraint at the same time. An efficient algorithm based on the majorization-minimization (MM) method is proposed to solve the problem. Numerical results show that our proposed mean-reverting portfolio design method can significantly outperform every underlying single spread and the benchmark method in the literature.
Multiple Kernel Sparse Representations for Supervised and Unsupervised Learning<|sep|>In complex visual recognition tasks it is typical to adopt multiple descriptors, that describe different aspects of the images, for obtaining an improved recognition performance. Descriptors that have diverse forms can be fused into a unified feature space in a principled manner using kernel methods. Sparse models that generalize well to the test data can be learned in the unified kernel space, and appropriate constraints can be incorporated for application in supervised and unsupervised learning. In this paper, we propose to perform sparse coding and dictionary learning in the multiple kernel space, where the weights of the ensemble kernel are tuned based on graph-embedding principles such that class discrimination is maximized. In our proposed algorithm, dictionaries are inferred using multiple levels of 1-D subspace clustering in the kernel space, and the sparse codes are obtained using a simple levelwise pursuit scheme. Empirical results for object recognition and image clustering show that our algorithm outperforms existing sparse coding based approaches, and compares favorably to other state-of-the-art methods.
Nonparametric kernel estimation of the error density<|sep|>Consider the nonparametric regression model Y=m(X)+E, where the function m is smooth but unknown, and E is independent of X. An estimator of the density of the error term E is proposed and its weak consistency is obtained. The contribution of this paper is twofold. First, we evaluate the impact of the estimation of the regression function on the error density estimator. Secondly, the optimal choices of the first and second step bandwidths used for estimating the regression function and the error density are proposed. Further, we investigate the asymptotic normality of the error density estimator and evaluate its performances in simulated examples.
A practical approach to the sensitivity analysis for kinetic Monte Carlo simulation of heterogeneous catalysis<|sep|>Lattice kinetic Monte Carlo simulations have become a vital tool for predictive quality atomistic understanding of complex surface chemical reaction kinetics over a wide range of reaction conditions. In order to expand their practical value in terms of giving guidelines for atomic level design of catalytic systems, it is very desirable to readily evaluate a sensitivity analysis for a given model. The result of such a sensitivity analysis quantitatively expresses the dependency of the turnover frequency, being the main output variable, on the rate constants entering the model. In the past the application of sensitivity analysis, such as Degree of Rate Control, has been hampered by its exuberant computational effort required to accurately sample numerical derivatives of a property that is obtained from a stochastic simulation method. In this study we present an efficient and robust three stage approach that is capable of reliably evaluating the sensitivity measures for stiff microkinetic models as we demonstrate using CO oxidation on RuO2(110) as a prototypical reaction. In a first step, we utilize the Fisher Information Matrix for filtering out elementary processes which only yield negligible sensitivity. Then we employ an estimator based on linear response theory for calculating the sensitivity measure for non-critical conditions which covers the majority of cases. Finally we adopt a method for sampling coupled finite differences for evaluating the sensitivity measure of lattice based models. This allows efficient evaluation even in critical regions near a second order phase transition that are hitherto difficult to control. The combined approach leads to significant computational savings over straightforward numerical derivatives and should aid in accelerating the nano scale design of heterogeneous catalysts.
Category Theoretic Analysis of Photon-based Decision Making<|sep|>Decision making is a vital function in this age of machine learning and artificial intelligence, yet its physical realization and theoretical fundamentals are still not completely understood. In our former study, we demonstrated that single-photons can be used to make decisions in uncertain, dynamically changing environments. The two-armed bandit problem was successfully solved using the dual probabilistic and particle attributes of single photons. In this study, we present a category theoretic modeling and analysis of single-photon-based decision making, including a quantitative analysis that is in agreement with the experimental results. A category theoretic model reveals the complex interdependencies of subject matter entities in a simplified manner, even in dynamically changing environments. In particular, the octahedral and braid structures in triangulated categories provide a better understanding and quantitative metrics of the underlying mechanisms of a single-photon decision maker. This study provides both insight and a foundation for analyzing more complex and uncertain problems, to further machine learning and artificial intelligence.
Generalized Distributed Network Coding Based on Nonbinary Linear Block Codes for Multi-User Cooperative Communications<|sep|>In this work, we propose and analyze a generalized construction of distributed network codes for a network consisting of M users sending different information to a common base station through independent block fading channels. The aim is to increase the diversity order of the system without reducing its code rate. The proposed scheme, called generalized dynamic network codes (GDNC), is a generalization of the dynamic network codes (DNC) recently proposed by Xiao and Skoglund. The design of the network codes that maximizes the diversity order is recognized as equivalent to the design of linear block codes over a nonbinary finite field under the Hamming metric. The proposed scheme offers a much better tradeoff between rate and diversity order. An outage probability analysis showing the improved performance is carried out, and computer simulations results are shown to agree with the analytical results.
Comparison of nuclear data uncertainty propagation methodologies for PWR burn-up simulations<|sep|>Several methodologies using different levels of approximations have been developed for propagating nuclear data uncertainties in nuclear burn-up simulations. Most methods fall into the two broad classes of Monte Carlo approaches, which are exact apart from statistical uncertainties but require additional computation time, and first order perturbation theory approaches, which are efficient for not too large numbers of considered response functions but only applicable for sufficiently small nuclear data uncertainties. Some methods neglect isotopic composition uncertainties induced by the depletion steps of the simulations, others neglect neutron flux uncertainties, and the accuracy of a given approximation is often very hard to quantify. In order to get a better sense of the impact of different approximations, this work aims to compare results obtained based on different approximate methodologies with an exact method, namely the NUDUNA Monte Carlo based approach developed by AREVA GmbH. In addition, the impact of different covariance data is studied by comparing two of the presently most complete nuclear data covariance libraries (ENDF/B-VII.1 and SCALE 6.0), which reveals a high dependency of the uncertainty estimates on the source of covariance data. The burn-up benchmark Exercise I-1b proposed by the OECD expert group "Benchmarks for Uncertainty Analysis in Modeling (UAM) for the Design, Operation and Safety Analysis of LWRs" is studied as an example application. The burn-up simulations are performed with the SCALE 6.0 tool suite.
One Shot Joint Colocalization and Cosegmentation<|sep|>This paper presents a novel framework in which image cosegmentation and colocalization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. In contrast to multi-task learning paradigm that learns similar tasks using a shared representation, the proposed framework leverages two representations at different levels and simultaneously discriminates between foreground and background at the bounding box and superpixel level using discriminative clustering. We show empirically that constraining the two problems at different scales enables the transfer of semantic localization cues to improve cosegmentation output whereas local appearance based segmentation cues help colocalization. The unified framework outperforms strong baseline approaches, of learning the two problems separately, by a large margin on four benchmark datasets. Furthermore, it obtains competitive results compared to the state of the art for cosegmentation on two benchmark datasets and second best result for colocalization on Pascal VOC 2007.
On the Weights of General MDS Codes<|sep|>The weight spectra of MDS codes of length $ n $ and dimension $ k $ over the arbitrary alphabets are studied. For all $ q $-ary MDS codes of dimension $ k $ containing the zero codeword, it is shown that all $ k $ weights from $ n $ to $ n-k+1 $ are realized. The remaining case $ n=q+k-1 $ is also determined. Additionally, we prove that all binary MDS codes are equivalent to linear MDS codes. The proofs are combinatorial, and self contained.
An Analog Neural Network Computing Engine using CMOS-Compatible Charge-Trap-Transistor (CTT)<|sep|>An analog neural network computing engine based on CMOS-compatible charge-trap transistor (CTT) is proposed in this paper. CTT devices are used as analog multipliers. Compared to digital multipliers, CTT-based analog multiplier shows significant area and power reduction. The proposed computing engine is composed of a scalable CTT multiplier array and energy efficient analog-digital interfaces. Through implementing the sequential analog fabric (SAF), the engine mixed-signal interfaces are simplified and hardware overhead remains constant regardless of the size of the array. A proof-of-concept 784 by 784 CTT computing engine is implemented using TSMC 28nm CMOS technology and occupied 0.68mm2. The simulated performance achieves 76.8 TOPS (8-bit) with 500 MHz clock frequency and consumes 14.8 mW. As an example, we utilize this computing engine to address a classic pattern recognition problem -- classifying handwritten digits on MNIST database and obtained a performance comparable to state-of-the-art fully connected neural networks using 8-bit fixed-point resolution.
Source coding of audio signals with a generative model<|sep|>We consider source coding of audio signals with the help of a generative model. We use a construction where a waveform is first quantized, yielding a finite bitrate representation. The waveform is then reconstructed by random sampling from a model conditioned on the quantized waveform. The proposed coding scheme is theoretically analyzed. Using SampleRNN as the generative model, we demonstrate that the proposed coding structure provides performance competitive with state-of-the-art source coding tools for specific categories of audio signals.
Gamma-ray burst radio afterglows from Population III stars: Simulation methods and detection prospects with SKA precursors<|sep|>We investigate the prospects of detecting radio afterglows from long Gamma-Ray Bursts (GRBs) from Population III (Pop III) progenitors using the SKA precursor instruments WMA (Murchison Widefield Array) and ASKAP (Australian SKA Pathfinder). We derive a realistic model of GRB afterglows that encompasses the widest range of plausible physical parameters and observation angles. We define the best case scenario of Pop III GRB energy and redshift distributions. Using probability distribution functions fitted to the observed microphysical parameters of long GRBs, we simulate a large number of Pop III GRB afterglows to find the global probability of detection. We find that ASKAP may be able to detect 35% of Pop III GRB afterglows in the optimistic case, and 27% in the pessimistic case. A negligible number will be detectable by MWA in either case. Detections per image for ASKAP, found by incorporating intrinsic rates with detectable timescales, are as high as $\sim$ 6000 and as low as $\sim$ 11, which shows the optimistic case is unrealistic. We track how the afterglow flux density changes over various time intervals and find that, because of their very slow variability, the cadence for blind searches of these afterglows should be as long as possible. We also find Pop III GRBs at high redshift have radio afterglow lightcurves that are indistinguishable from those of regular long GRBs in the more local universe.
KaRMMa 2.0 -- Kappa Reconstruction for Mass Mapping<|sep|>We present KaRMMa 2.0, an updated version of the mass map reconstruction code introduced in Fiedorowicz et al. (2022). KaRMMa is a full-sky Bayesian algorithm for reconstructing weak lensing mass maps from shear data. It forward-models the convergence field as a realization of a lognormal field. The corresponding shear map is calculated using the standard Kaiser-Squires transformation, and compared to observations at the field level. The posterior distribution of maps given the shear data is sampled using Hamiltonian Monte Carlo chains. Our work improves on the original algorithm by making it numerically efficient, enabling full-sky reconstructions at $\approx$ 7 arcmin resolution with modest computational resources. These gains are made with no loss in accuracy or precision relative to KaRMMa 1.0. We compare the KaRMMa 2.0 posteriors against simulations across a variety of summary statistics (one-point function, two-point functions, and peak/void counts) to demonstrate our updated algorithm provides an accurate reconstruction of the convergence field at mildly non-linear scales. Unsurprisingly, the lognormal model fails as we approach non-linear scales ($\ell \gtrsim 200$), which in turn biases the map posteriors. These biases are at the 2% level in the recovered power spectrum, and at the 5% to 15% level for other statistics, depending on the resolution.
Read Operators and their Expressiveness in Process Algebras<|sep|>We study two different ways to enhance PAFAS, a process algebra for modelling asynchronous timed concurrent systems, with non-blocking reading actions. We first add reading in the form of a read-action prefix operator. This operator is very flexible, but its somewhat complex semantics requires two types of transition relations. We also present a read-set prefix operator with a simpler semantics, but with syntactic restrictions. We discuss the expressiveness of read prefixes; in particular, we compare them to read-arcs in Petri nets and justify the simple semantics of the second variant by showing that its processes can be translated into processes of the first with timed-bisimilar behaviour. It is still an open problem whether the first algebra is more expressive than the second; we give a number of laws that are interesting in their own right, and can help to find a backward translation.
Joint Entity Extraction and Assertion Detection for Clinical Text<|sep|>Negative medical findings are prevalent in clinical reports, yet discriminating them from positive findings remains a challenging task for information extraction. Most of the existing systems treat this task as a pipeline of two separate tasks, i.e., named entity recognition (NER) and rule-based negation detection. We consider this as a multi-task problem and present a novel end-to-end neural model to jointly extract entities and negations. We extend a standard hierarchical encoder-decoder NER model and first adopt a shared encoder followed by separate decoders for the two tasks. This architecture performs considerably better than the previous rule-based and machine learning-based systems. To overcome the problem of increased parameter size especially for low-resource settings, we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2b2/VA challenge dataset and a proprietary de-identified clinical dataset.
Numerical analysis of distributed optimal control problems governed by elliptic variational inequalities<|sep|>A continuous optimal control problem governed by an elliptic variational inequality was considered in Boukrouche-Tarzia, Comput. Optim. Appl., 53 (2012), 375-392 where the control variable is the internal energy $g$. It was proved the existence and uniqueness of the optimal control and its associated state system. The objective of this work is to make the numerical analysis of the above optimal control problem, through the finite element method with Lagrange's triangles of type 1. We discretize the elliptic variational inequality which define the state system and the corresponding cost functional, and we prove that there exists a discrete optimal control and its associated discrete state system for each positive $h$ (the parameter of the finite element method approximation). Finally, we show that the discrete optimal control and its associated state system converge to the continuous optimal control and its associated state system when the parameter $h$ goes to zero.
$\rho$-GNF : A Novel Sensitivity Analysis Approach Under Unobserved Confounders<|sep|>We propose a new sensitivity analysis model that combines copulas and normalizing flows for causal inference under unobserved confounding. We refer to the new model as $\rho$-GNF ($\rho$-Graphical Normalizing Flow), where $\rho{\in}[-1,+1]$ is a bounded sensitivity parameter representing the backdoor non-causal association due to unobserved confounding modeled using the most well studied and widely popular Gaussian copula. Specifically, $\rho$-GNF enables us to estimate and analyse the frontdoor causal effect or average causal effect (ACE) as a function of $\rho$. We call this the $\rho_{curve}$. The $\rho_{curve}$ enables us to specify the confounding strength required to nullify the ACE. We call this the $\rho_{value}$. Further, the $\rho_{curve}$ also enables us to provide bounds for the ACE given an interval of $\rho$ values. We illustrate the benefits of $\rho$-GNF with experiments on simulated and real-world data in terms of our empirical ACE bounds being narrower than other popular ACE bounds.
Forecasting Global Weather with Graph Neural Networks<|sep|>We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.
Geometric aspects of Extremal Kerr black hole entropy<|sep|>Extreme Black holes are an important theoretical laboratory for exploring the nature of entropy. We suggest that this unusual nature of the extremal limit could explain the entropy of extremal Kerr black holes. The time-independence of the extremal black hole, the zero surface gravity, the zero entropy and the absence of a bifurcate Killing horizon are all related properties that define and reduce to one single unique feature of the extremal Kerr spacetime. We suggest the presence of a true geometric discontinuity as the underlying cause of a vanishing entropy.
Sequeval: A Framework to Assess and Benchmark Sequence-based Recommender Systems<|sep|>In this paper, we present sequeval, a software tool capable of performing the offline evaluation of a recommender system designed to suggest a sequence of items. A sequence-based recommender is trained considering the sequences already available in the system and its purpose is to generate a personalized sequence starting from an initial seed. This tool automatically evaluates the sequence-based recommender considering a comprehensive set of eight different metrics adapted to the sequential scenario. sequeval has been developed following the best practices of software extensibility. For this reason, it is possible to easily integrate and evaluate novel recommendation techniques. sequeval is publicly available as an open source tool and it aims to become a focal point for the community to assess sequence-based recommender systems.
A perturbation approach to Translational Gravity<|sep|>Within a gauge formulation of 3+1 gravity relying on a nonlinear realization of the group of isometries of space-time, a natural expansion of the metric tensor arises and a simple choice of the gravity dynamical variables is possible. We show that the expansion parameter can be identified with the gravitational constant and that the first order depends only on a diagonal matrix in the ensuing perturbation approach. The explicit first order solution is calculated in the static isotropic case, and its general structure is worked out in the harmonic gauge.
On the Distributed Computation of Fractional Connected Dominating Set Packings<|sep|>One of the most fundamental problems in wireless networks is to achieve high throughput. Fractional Connected Dominating Set (FCDS) Packings can achieve a throughput of ${\Theta}(k/\log n)$ messages for networks with node connectivity $k$, which is optimal regarding routing-based message transmission. FCDS were proposed by Censor-Hillel \emph{et al.} [SODA'14,PODC'14] and are a natural generalization to Connected Dominating Sets (CDS), allowing each node to participate with a fraction of its weight in multiple FCDS. Thus, $\Omega(k)$ co-existing transmission backbones are established, taking full advantage of the networks connectivity. We propose a modified distributed algorithm that improves upon previous algorithms for $k\Delta \in o(\min\{\frac{n \log n}{k} ,D,\sqrt{n \log n} \log^* n\}\log n)$, where $\Delta$ is the maximum node degree, $D$ the diameter and $n$ the number of nodes in the network. We achieve this by explicitly computing connections between tentative dominating sets.
Evidence for weakly bound electrons in non-irradiated alkane crystals. The electrons as a probe of structural differences in crystals<|sep|>It is generally assumed that weakly bound (trapped) electrons in organic solids come only from radiolytical (or photochemical) processes like ionization caused by an excited positron entering the sample. This paper presents an evidence for the presence of these electrons in non-irradiated samples of docosane. We argue that these electrons can be located (trapped) either in interlamellar gaps or in spaces made by non-planar conformers. The electrons from the former ones are bound more weakly than those from the latter ones. The origin of Vis absorption for the samples is explained. These spectra can be used as a probe indicating differences in the solid structures of hydrocarbons.
Uneven horizon or several words about the superfluid He-4 theory<|sep|>The state of the superfluid He-II theory is briefly surveyed - some aspects of its history, achievements, and unsolved problems.
Theory of charge transport in ferromagnetic semiconductor/s-wave superconductor junction<|sep|>We study tunneling conductance in ferromagnetic semiconductor/insulator/s-wave superconductor junction where Rashba spin-orbit interaction (RSOI) and exchange field are taken into account in the ferromagnetic semiconductor. We show that normalized conductance at zero voltage has a maximum as a function of RSOI for high transparent interface and finite exchange field. This is because Andreev reflection probability shows a nonmonotonic dependence on RSOI in the presence of the exchange field. On the other hand, for intermediate transparent interface, normalized conductance at zero voltage has a reentrant shape at zero or small exchange field with increasing RSOI but is monotonically increasing by RSOI at large exchange field.
On kinematical constraints in fermion-antifermion systems<|sep|>We consider the scattering of fermions off antifermions with spin 1/2 and 3/2. Starting from helicity partial-wave scattering amplitudes we derive transformations that eliminate all kinematical constraints. Such amplitudes are expected to satisfy partial-wave dispersion relations and therefore provide a suitable basis for data analysis and the construction of effective field theories. Our derivation relies on a decomposition of the various scattering amplitudes into suitable sets of invariant functions.
Two incompatible types of invariants in the octonion spaces<|sep|>The paper aims to study some invariants and conservation laws relevant to electromagnetic and gravitational fields, by means of the rotational transformations of octonion coordinate systems. The scholars utilize the octonions to analyze the electromagnetic and gravitational fields simultaneously, including the octonion field potential, field strength, field source, linear momentum, angular momentum, torque and force. When the octonion coordinate system transforms rotationally, the vector part of one octonion may alter, while the scalar part of the octonion will remain unchanged. This property allows one to deduce a few invariants, such as the scalar part of octonion radius vector, speed of light, and norm of octonion radius vector. These invariants are the basic postulates for the Galilean transformation and Lorentz transformation. Further, from the rotation transform of octonion coordinate systems, it is capable of deducing several invariants, including the mass, energy and power relevant to gravitational fields, in one octonion space $\mathbb{O}$. And the term relevant to the electric charge will transform with the rotation of coordinate systems. In another octonion space $\mathbb{O}_u$, it is capable of inferring a few invariants, including the electric charge related to electromagnetic fields. And the terms relevant to the mass and energy will vary with the rotation of octonion coordinate systems. So the invariants are divided into two different groups. In particular, the mass conservation law and energy conservation law can be effective simultaneously. But the charge conservation law and mass conservation law are unable to be valid simultaneously, in the strict sense. It is beneficial to further understand the laws of conservation.
Emotion Detection from Text<|sep|>Emotion can be expressed in many ways that can be seen such as facial expression and gestures, speech and by written text. Emotion Detection in text documents is essentially a content - based classification problem involving concepts from the domains of Natural Language Processing as well as Machine Learning. In this paper emotion recognition based on textual data and the techniques used in emotion detection are discussed.
Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring<|sep|>This paper describes CAiRE's submission to the unsupervised machine translation track of the WMT'19 news shared task from German to Czech. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for German and Czech separately, and they are aligned using MUSE (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.
Inconsistency of Bayesian Inference for Misspecified Linear Models, and a Proposal for Repairing It<|sep|>We empirically show that Bayesian inference can be inconsistent under misspecification in simple linear regression problems, both in a model averaging/selection and in a Bayesian ridge regression setting. We use the standard linear model, which assumes homoskedasticity, whereas the data are heteroskedastic, and observe that the posterior puts its mass on ever more high-dimensional models as the sample size increases. To remedy the problem, we equip the likelihood in Bayes' theorem with an exponent called the learning rate, and we propose the Safe Bayesian method to learn the learning rate from the data. SafeBayes tends to select small learning rates as soon the standard posterior is not `cumulatively concentrated', and its results on our data are quite encouraging.
An Analysis of the Convergence of Graph Laplacians<|sep|>Existing approaches to analyzing the asymptotics of graph Laplacians typically assume a well-behaved kernel function with smoothness assumptions. We remove the smoothness assumption and generalize the analysis of graph Laplacians to include previously unstudied graphs including kNN graphs. We also introduce a kernel-free framework to analyze graph constructions with shrinking neighborhoods in general and apply it to analyze locally linear embedding (LLE). We also describe how for a given limiting Laplacian operator desirable properties such as a convergent spectrum and sparseness can be achieved choosing the appropriate graph construction.
Study of open cluster King 13 using CCD VI, 2MASS and Gaia DR2 Astrometry<|sep|>In this paper, we present astrophysical parameters of the open cluster King 13 based on the VI CCD and 2MASS JHKs photometric data. This is a poorly studied cluster, for which new results have been found in the present work. To identify probable members, we use proper motion data from Gaia DR2 catalogue. The mean proper motion of the cluster is determined as -2.8 \pm 0.2 and -0.88 \pm 0.14 mas yr{-1} and cluster extent is derived as 3'.2. Using color-magnitude diagrams, we estimate the age and distance of the cluster as 510 \pm 60 Myr and 3.84 \pm 0.15 kpc respectively. Interstellar reddening E(B-V) in the direction of the cluster is determined as 0.80 \pm 0.2 mag using color-color diagram. Mass function slope of the cluster is found to be comparable with the Salpeter value. The total mass of this cluster is derived as 270 M_{\odot}. The present analysis shows that King 13 is a dynamically relaxed cluster.
A non-distributive logic for semiconcepts of a context and its modal extension with semantics based on Kripke contexts<|sep|>A non-distributive two-sorted hypersequent calculus \textbf{PDBL} and its modal extension \textbf{MPDBL} are proposed for the classes of pure double Boolean algebras and pure double Boolean algebras with operators respectively. A relational semantics for \textbf{PDBL} is next proposed, where any formula is interpreted as a semiconcept of a context. For \textbf{MPDBL}, the relational semantics is based on Kripke contexts, and a formula is interpreted as a semiconcept of the underlying context. The systems are shown to be sound and complete with respect to the relational semantics. Adding appropriate sequents to \textbf{MPDBL} results in logics with semantics based on reflexive, symmetric or transitive Kripke contexts. One of these systems is a logic for topological pure double Boolean algebras. It is demonstrated that, using \textbf{PDBL}, the basic notions and relations of conceptual knowledge can be expressed and inferences involving negations can be obtained. Further, drawing a connection with rough set theory, lower and upper approximations of semiconcepts of a context are defined. It is then shown that, using the formulae and sequents involving modal operators in \textbf{MPDBL}, these approximation operators and their properties can be captured.
$TESS$ Phase Curve of the Hot Jupiter WASP-19b<|sep|>We analyze the phase curve of the short-period transiting hot Jupiter system WASP-19, which was observed by the Transiting Exoplanet Survey Satellite ($TESS$) in Sector 9. WASP-19 is one of only five transiting exoplanet systems with full-orbit phase curve measurements at both optical and infrared wavelengths. We measure a secondary eclipse depth of $470^{+130}_{-110}$ ppm and detect a strong atmospheric brightness modulation signal with a semi-amplitude of $319\pm51$ ppm. No significant offset is detected between the substellar point and the region of maximum brightness on the dayside. There is also no significant nightside flux detected, which is in agreement with the nightside effective blackbody temperature of $1090^{+190}_{-250}$ derived from the published $Spitzer$ phase curves for this planet. Placing the eclipse depth measured in the $TESS$ bandpass alongside the large body of previous values from the literature, we carry out the first atmospheric retrievals of WASP-19b's secondary eclipse spectrum using the SCARLET code. The retrieval analysis indicates that WASP-19b has a dayside atmosphere consistent with an isotherm at $T=2240\pm40$ K and a visible geometric albedo of $0.16\pm0.04$, indicating significant contribution from reflected starlight in the $TESS$ bandpass and moderately efficient day-night heat transport.
Direct determinations of the nucleon and pion $\sigma$ terms at nearly physical quark masses<|sep|>We present a high statistics study of the pion and nucleon light and strange quark sigma terms using $N_f=2$ dynamical non-perturbatively improved clover fermions with a range of pion masses down to $m_\pi\sim 150$ MeV and several volumes, $Lm_\pi=3.4$ up to $6.7$, and lattice spacings, $a=0.06-0.08$ fm, enabling a study of finite volume and discretisation effects for $m_\pi\gtrsim 260$ MeV. Systematics are found to be reasonably under control. For the nucleon we obtain $\sigma_{\pi N}=35(6)$ MeV and $\sigma_s=35(12)$ MeV, or equivalently in terms of the quark fractions, $f_{T_u}=0.021(4)$, $f_{T_d}=0.016(4)$ and $f_{T_s}=0.037(13)$, where the errors include estimates of both the systematic and statistical uncertainties. These values, together with perturbative matching in the heavy quark limit, lead to $f_{T_c}=0.075(4)$, $f_{T_b}=0.072(2)$ and $f_{T_t}=0.070(1)$. In addition, through the use of the (inverse) Feynman-Hellmann theorem our results for $\sigma_{\pi N}$ are shown to be consistent with the nucleon masses determined in the analysis. For the pion we implement a method which greatly reduces excited state contamination to the scalar matrix elements from states travelling across the temporal boundary. This enables us to demonstrate the Gell-Mann-Oakes-Renner expectation $\sigma_\pi=m_\pi/2$ over our range of pion masses.
A reassessment of the evidence of the Integrated Sachs-Wolfe effect through the WMAP-NVSS correlation<|sep|>We reassess the estimate of the cross-correlation of the spatial distribution of the NRAO VLA Sky Survey (NVSS) radio sources with that of Cosmic Microwave Background (CMB) anisotropies from the Wilkinson Microwave Anisotropy Probe (WMAP). This re-analysis is motivated by the fact that most previous studies adopted a redshift distribution of NVSS sources inconsistent with recent data. We find that the constraints on the bias-weighted redshift distribution, b(z)xN(z), of NVSS sources, set by the observed angular correlation function, w(theta), strongly mitigate the effect of the choice of N(z). If such constraints are met, even highly discrepant redshift distributions yield NVSS-WMAP cross-correlation functions consistent with each other within statistical errors. The models favoured by recent data imply a bias factor, b(z), decreasing with increasing z, rather than constant, as assumed by most previous analyses. As a consequence, the function b(z)xN(z) has more weight at z<1, i.e. in the redshift range yielding the maximum contribution to the ISW in a standard LambdaCDM cosmology. On the whole, the NVSS turns out to be better suited for ISW studies than generally believed, even in the absence of an observational determination of the redshift distribution. The NVSS-WMAP cross-correlation function is found to be fully consistent with the prediction of the standard LambdaCDM cosmology.
Progress on Ultraviolet Finiteness of Supergravity<|sep|>In this lecture we summarize recent calculations pointing to the possible ultraviolet finiteness of N = 8 supergravity in four dimensions. We outline the modern unitarity method, which enables multiloop calculations in this theory and allows us to exploit a remarkable relation between tree-level gravity and gauge-theory amplitudes. We also describe a link between observed cancellations at loop level and improved behavior of tree-level amplitudes under large complex deformations of momenta.
Standardized drought indices: A novel uni- and multivariate approach<|sep|>As drought is among the natural hazards which affects people and economies worldwide and often results in huge monetary losses sophisticated methods for drought monitoring and decision making are needed. Several different approaches to quantify drought have been developed during past decades. However, most of these drought indices suffer from different shortcomings and do not account for the multiple driving factors which promote drought conditions and their inter-dependencies. We provide a novel methodology for the calculation of (multivariate) drought indices, which combines the advantages of existing approaches and omits their disadvantages. Moreover, our approach benefits from the flexibility of vine copulas in modeling multivariate non-Gaussian inter-variable dependence structures. A three-variate data example is used in order to investigate drought conditions in Europe and to illustrate and reason the different modeling steps. The data analysis shows the appropriateness of the described methodology. Comparison to well-established drought indices shows the benefits of our multivariate approach. The validity of the new methodology is verified by comparing the spatial extent of historic drought events based on different drought indices. Further, we show that the assumption of non-Gaussian dependence structures is well-grounded in this real-world application.
Model Based Clustering of High-Dimensional Binary Data<|sep|>We propose a mixture of latent trait models with common slope parameters (MCLT) for model-based clustering of high-dimensional binary data, a data type for which few established methods exist. Recent work on clustering of binary data, based on a $d$-dimensional Gaussian latent variable, is extended by incorporating common factor analyzers. Accordingly, our approach facilitates a low-dimensional visual representation of the clusters. We extend the model further by the incorporation of random block effects. The dependencies in each block are taken into account through block-specific parameters that are considered to be random variables. A variational approximation to the likelihood is exploited to derive a fast algorithm for determining the model parameters. Our approach is demonstrated on real and simulated data.
Fast Exciton Annihilation by Capture of Electrons or Holes by Defects via Auger Scattering in Monolayer Metal Dichalcogenides<|sep|>The strong Coulomb interactions and the small exciton radii in two-dimensional metal dichalcogenides can result in very fast capture of electrons and holes of excitons by mid-gap defects from Auger processes. In the Auger processes considered here, an exciton is annihilated at a defect site with the capture of the electron (or the hole) by the defect and the hole (or the electron) is scattered to a high energy. In the case of excitons, the probability of finding an electron and a hole near each other is enhanced many folds compared to the case of free uncorrelated electrons and holes. Consequently, the rate of carrier capture by defects from Auger scattering for excitons in metal dichalcogenides can be 100-1000 times larger than for uncorrelated electrons and holes for carrier densities in the $10^{11}$-$10^{12}$ cm$^{-2}$ range. We calculate the capture times of electrons and holes by defects and show that the capture times can be in the sub-picosecond to a few picoseconds range. The capture rates exhibit linear as well as quadratic dependence on the exciton density. These fast time scales agree well with the recent experimental observations, and point to the importance of controlling defects in metal dichalcogenides for optoelectronic applications.
Efficient binary tomographic reconstruction<|sep|>Tomographic reconstruction of a binary image from few projections is considered. A novel {\em heuristic} algorithm is proposed, the central element of which is a nonlinear transformation $\psi(p)=\log(p/(1-p))$ of the probability $p$ that a pixel of the sought image be 1-valued. It consists of backprojections based on $\psi(p)$ and iterative corrections. Application of this algorithm to a series of artificial test cases leads to exact binary reconstructions, (i.e recovery of the binary image for each single pixel) from the knowledge of projection data over a few directions. Images up to $10^6$ pixels are reconstructed in a few seconds. A series of test cases is performed for comparison with previous methods, showing a better efficiency and reduced computation times.
A stable numerical strategy for Reynolds-Rayleigh-Plesset coupling<|sep|>The coupling of Reynolds and Rayleigh-Plesset equations has been used in several works to simulate lubricated devices considering cavitation. The numerical strategies proposed so far are variants of a staggered strategy where Reynolds equation is solved considering the bubble dynamics frozen, and then the Rayleigh-Plesset equation is solved to update the bubble radius with the pressure frozen. We show that this strategy has severe stability issues and a stable methodology is proposed. The proposed methodology performance is assessed on two physical settings. The first one concerns the propagation of a decompression wave along a fracture considering the presence of cavitation nuclei. The second one is a typical journal bearing, in which the coupled model is compared with the Elrod-Adams model.
Quantum revivals and magnetization tunneling in effective spin systems<|sep|>Quantum mechanical objects or nanoobjects have been proposed as bits for information storage. While time-averaged properties of magnetic, quantum-mechanical particles have been extensively studied experimentally and theoretically, experimental investigations of the real time evolution of magnetization in the quantum regime were not possible until recent developments in pump-probe techniques. Here we investigate the quantum dynamics of effective spin systems by means of analytical and numerical treatments. Particular attention is paid to the quantum revival time and its relation to the magnetization tunneling. The quantum revival time has been initially defined as the recurrence time of a total wave-function. Here we show that the quantum revivals of wave-functions and expectation values in spin systems may be quite different which gives rise to a more sophisticated definition of the quantum revival within the realm of experimental research. Particularly, the revival times for integer spins coincide which is not the case for half-integer spins. Furthermore, the quantum revival is found to be shortest for integer ratios between the on-site anisotropy and an external magnetic field paving the way to novel methods of anisotropy measurements. We show that the quantum tunneling of magnetization at avoided level crossing is coherent to the quantum revival time of expectation values, leading to a connection between these two fundamental properties of quantum mechanical spins.
Practical Conditions for Well-behaved-ness of Anisotropic Voronoi Diagrams<|sep|>Recently, simple conditions for well-behaved-ness of anisotropic Voronoi diagrams have been proposed. While these conditions ensure well-behaved-ness of two types of practical anisotropic Voronoi diagrams, as well as the geodesic-distance one, in any dimension, they are both prohibitively expensive to evaluate, and not well-suited for typical problems in approximation or optimization. We propose simple conditions that can be efficiently evaluated, and are better suited to practical problems of approximation and optimization. The practical utility of this analysis is enhanced by the fact that orphan-free anisotropic Voronoi diagrams have embedded triangulations as duals.
Evaluating COPY-BLEND Augmentation for Low Level Vision Tasks<|sep|>Region modification-based data augmentation techniques have shown to improve performance for high level vision tasks (object detection, semantic segmentation, image classification, etc.) by encouraging underlying algorithms to focus on multiple discriminative features. However, as these techniques destroy spatial relationship with neighboring regions, performance can be deteriorated when using them to train algorithms designed for low level vision tasks (low light image enhancement, image dehazing, deblurring, etc.) where textural consistency between recovered and its neighboring regions is important to ensure effective performance. In this paper, we examine the efficacy of a simple copy-blend data augmentation technique that copies patches from noisy images and blends onto a clean image and vice versa to ensure that an underlying algorithm localizes and recovers affected regions resulting in increased perceptual quality of a recovered image. To assess performance improvement, we perform extensive experiments alongside different region modification-based augmentation techniques and report observations such as improved performance, reduced requirement for training dataset, and early convergence across tasks such as low light image enhancement, image dehazing and image deblurring without any modification to baseline algorithm.
A biaxial apparatus for the study of heterogeneous and intermittent strains in granular materials<|sep|>We present an experimental apparatus specifically designed to investigate the precursors of failure in granular materials. A sample of granular material is placed between a latex membrane and a glass plate. A confining effective pressure is applied by applying vacuum to the sample. Displacement-controlled compression is applied in the vertical direction, while the specimen deforms in plane strain. A Diffusing Wave Spectroscopy visualization setup gives access to the measurement of deformations near the glass plate. After describing the different parts of this experimental setup, we present a demonstration experiment where extremely small (of order $10^{-5}$) heterogeneous strains are measured during the loading process.
Prospects for indirect dark matter searches with MeV photons<|sep|>Over the past decade, extensive studies have been undertaken to search for photon signals from dark matter annihilation or decay for dark matter particle masses above $\sim1$ GeV. However, due to the lacking sensitivity of current experiments at MeV-GeV energies, sometimes dubbed the 'MeV gap', dark matter models with MeV to sub-GeV particle masses have received little attention so far. Various proposed MeV missions (like, e.g., e-ASTROGAM or AMEGO) are aimed at closing this gap in the mid- or long-term future. This, and the absence of clear dark matter signals in the GeV-TeV range, makes it relevant to carefully reconsider the expected experimental instrumental sensitivities in this mass range. The most common two-body annihilation channels for sub-GeV dark matter are to neutrinos, electrons, pions or directly to photons. Among these, only the electron channel has been extensively studied, and almost exclusively in the context of the 511 keV line. In this work, we study the prospects for detecting MeV dark matter annihilation in general in future MeV missions, using e-ASTROGAM as reference, and focusing on dark matter masses in the range 1 MeV-3 GeV. In the case of leptonic annihilation, we emphasise the importance of the often overlooked bremsstrahlung and in-flight annihilation spectral features, which in many cases provide the dominant gamma-ray signal in this regime.
The quartic oscillator in an external field and the statistical physics of highly anisotropic solids<|sep|>The statistical mechanics of 1D and 2D Ginzburg-Landau systems is evaluated analytically, via the transfer matrix method, using an expression of the ground state energy of the quartic anharmonic oscillator in an external field. In the 2D case, the critical temperature of the order/disorder phase transition is expressed as a Lambert function of the inverse inter-chain coupling constant.
Preprocessing for Treewidth: A Combinatorial Analysis through Kernelization<|sep|>The notion of treewidth plays an important role in theoretical and practical studies of graph problems. It has been recognized that, especially in practical environments, when computing the treewidth of a graph it is invaluable to first apply an array of preprocessing rules that simplify and shrink it. This work seeks to prove rigorous performance guarantees for such preprocessing rules, both known and new ones, by studying them in the framework of kernelization from parameterized complexity. It is known that the NP-complete problem of determining whether a given graph G has treewidth at most k admits no polynomial-time preprocessing algorithm that reduces any input instance to size polynomial in k, unless NP is in coNP/poly and the polynomial hierarchy collapses to its third level. In this paper we therefore consider structural graph measures larger than treewidth, and determine whether efficient preprocessing can shrink the instance size to a polynomial in such a parameter value. We prove that given an instance (G,k) of treewidth we can efficiently reduce its size to O(fvs(G)^4) vertices, where fvs(G) is the size of a minimum feedback vertex set in G. We can also prove a size reduction to O(vc(G)^3) vertices, where vc(G) is the size of a minimum vertex cover. Phrased in the language of parameterized complexity, we show that Treewidth has a polynomial kernel when parameterized by the size of a given feedback vertex set, and also by the size of a vertex cover. In contrast we show that Treewidth parameterized by the vertex-deletion distance to a single clique, and Weighted Treewidth parameterized by the size of a vertex cover, do not admit polynomial kernelizations unless NP is in coNP/poly.
A well-balanced scheme for the simulation tool-kit A-MaZe: implementation, tests, and first applications to stellar structure<|sep|>Characterizing stellar convection in multiple dimensions is a topic at the forefront of stellar astrophysics. Numerical simulations are an essential tool for this task. We present an extension of the existing numerical tool-kit A-MaZe that enables such simulations of stratified flows in a gravitational field. The finite-volume based, cell-centered, and time-explicit hydrodynamics solver of A-MaZe was extended such that the scheme is now well-balanced in both momentum and energy. The algorithm maintains an initially static balance between gravity and pressure to machine precision. Quasi-stationary convection in slab-geometry preserves gas energy (internal plus kinetic) on average despite strong local up- and down-drafts. By contrast, a more standard numerical scheme is demonstrated to result in substantial gains of energy within a short time on purely numerical grounds. The test is further used to point out the role of dimensionality, viscosity, and Rayleigh number for compressible convection. Applications to a young sun in 2D and 3D, covering a part of the inner radiative zone as well as the outer convective zone, demonstrate that the scheme meets its initial design goal. Comparison with results obtained for a physically identical setup with a time-implicit code show qualitative agreement.
Joint Acoustic Echo Cancellation and Blind Source Extraction based on Independent Vector Extraction<|sep|>We describe a joint acoustic echo cancellation (AEC) and blind source extraction (BSE) approach for multi-microphone acoustic frontends. The proposed algorithm blindly estimates AEC and beamforming filters by maximizing the statistical independence of a non-Gaussian source of interest and a stationary Gaussian background modeling interfering signals and residual echo. Double talk-robust and fast-converging parameter updates are derived from a global maximum-likelihood objective function resulting in a computationally efficient Newton-type update rule. Evaluation with simulated acoustic data confirms the benefit of the proposed joint AEC and beamforming filter estimation in comparison to updating both filters individually.
Magnetohydrodynamics dynamical relaxation of coronal magnetic fields. I. Parallel untwisted magnetic fields in 2D<|sep|>Context. For the last thirty years, most of the studies on the relaxation of stressed magnetic fields in the solar environment have onlyconsidered the Lorentz force, neglecting plasma contributions, and therefore, limiting every equilibrium to that of a force-free field. Aims. Here we begin a study of the non-resistive evolution of finite beta plasmas and their relaxation to magnetohydrostatic states, where magnetic forces are balanced by plasma-pressure gradients, by using a simple 2D scenario involving a hydromagnetic disturbance to a uniform magnetic field. The final equilibrium state is predicted as a function of the initial disturbances, with aims to demonstrate what happens to the plasma during the relaxation process and to see what effects it has on the final equilibrium state. Methods. A set of numerical experiments are run using a full MHD code, with the relaxation driven by magnetoacoustic waves damped by viscous effects. The numerical results are compared with analytical calculations made within the linear regime, in which the whole process must remain adiabatic. Particular attention is paid to the thermodynamic behaviour of the plasma during the relaxation. Results. The analytical predictions for the final non force-free equilibrium depend only on the initial perturbations and the total pressure of the system. It is found that these predictions hold surprisingly well even for amplitudes of the perturbation far outside the linear regime. Conclusions. Including the effects of a finite plasma beta in relaxation experiments leads to significant differences from the force-free case.
Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding<|sep|>Dialogue systems powered by large pre-trained language models (LM) exhibit an innate ability to deliver fluent and natural-looking responses. Despite their impressive generation performance, these models can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving the faithfulness -- and thus reduce hallucination -- of Neural Dialogue Systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the k-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage consisting of a chain of two neural LM's that retrieves correct entities by crafting a query signal that is propagated over the k-hop subgraph. Our proposed model can easily be applied to any dialogue generated responses without retraining the model. We empirically validate our proposed approach on the OpenDialKG dataset against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020).
Spectrum of excited states using the stochastic LapH method<|sep|>Progress in computing the spectrum of excited baryons and mesons in lattice QCD is described. Our first results in the zero-momentum bosonic I=1, S=0, T1u+ symmetry sector of QCD using a correlation matrix of 56 operators are presented. In addition to a dozen spatially-extended meson operators, 44 two-meson operators are used, involving a wide variety of light isovector, isoscalar, and strange meson operators of varying relative momenta. All needed Wick contractions are efficiently evaluated using a stochastic method of treating the low-lying modes of quark propagation that exploits Laplacian Heaviside quark-field smearing. Level identification is discussed.
Identification and Properties of Isolated Field Elliptical Galaxies from CFHTLS-W1<|sep|>We present a catalogue of isolated field elliptical (IfE) galaxies drawn from the W1 field of the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS). 228 IfEs were identified from a flux-limited (r<21.8) galaxy catalogue which corresponds to a density of 3 IfE/sq.deg. For comparison we consider a sample of elliptical galaxies living in dense environments, based on identification of the brightest cluster galaxies (BGCs) in the same survey. Using the same dataset for the comparison sample ensures a uniform selection, including in the redshift range as IfEs (i.e. 0.1 < z < 0.9). A comparison of elliptical galaxies in different environments reveals that IfEs and BCGs have similar behaviours in their colours, star formation activities, and scaling relations of mass-size and size-luminosity. IfEs and BCGs have similar slopes in the scaling relations with respect to cluster ellipticals within the $-24 \leq M_{r} \leq -22$ magnitude and $10.2< \textrm{log}( \textrm M_{*}/ \textrm M_\odot)\leq12.0$ mass ranges. Three IfEs identified in this study can be associated with fossil groups found in the same survey area which gives clues for future studies.
StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery<|sep|>Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.
Abrupt Longitudinal Magnetic Field Changes and Ultraviolet Emissions Accompanying Solar Flares<|sep|>We have used Transition Region and Coronal Explorer (TRACE) 1600 \AA images and Global Oscillation Network Group (GONG) magnetograms to compare ultraviolet (UV) emissions from the chromosphere to longitudinal magnetic field changes in the photosphere during four X-class solar flares. An abrupt, significant, and persistent change in the magnetic field occurred across more than ten pixels in the GONG magnetograms for each flare. These magnetic changes lagged the GOES flare start times in all cases, showing that they were consequences and not causes of the flares. Ultraviolet emissions were spatially coincident with the field changes. The UV emissions tended to lag the GOES start times for the flares, and led the changes in the magnetic field in all pixels except one. The UV emissions led the photospheric field changes by 4 minutes on average with the longest lead being 9 minutes, however, the UV emissions continued for tens of minutes, and more than an hour in some cases, after the field changes were complete. The observations are consistent with the picture in which an Alfv\'{e}n wave from the field reconnection site in the corona propagates field changes outward in all directions near the onset of the impulsive phase, including downwards through the chromosphere and into the photosphere, causing the photospheric field changes, whereas the chromosphere emits in the UV in the form of flare kernels, ribbons and sequential chromospheric brightenings during all phases of the flare.
Quantum implications of a scale invariant regularisation<|sep|>We study scale invariance at the quantum level (three loops) in a perturbative approach. For a scale-invariant classical theory the scalar potential is computed at three-loop level while keeping manifest this symmetry. Spontaneous scale symmetry breaking is transmitted at quantum level to the visible sector (of $\phi$) by the associated Goldstone mode (dilaton $\sigma$) which enables a scale-invariant regularisation and whose vev $\langle\sigma\rangle$ generates the subtraction scale ($\mu$). While the hidden ($\sigma$) and visible sector ($\phi$) are classically decoupled in $d=4$ due to an enhanced Poincar\'e symmetry, they interact through (a series of) evanescent couplings $\propto\epsilon^k$, ($k\geq 1$), dictated by the scale invariance of the action in $d=4-2\epsilon$. At the quantum level these couplings generate new corrections to the potential, such as scale-invariant non-polynomial effective operators $\phi^{2n+4}/\sigma^{2n}$ and also log-like terms ($\propto \ln^k \sigma$) restoring the scale-invariance of known quantum corrections. The former are comparable in size to "standard" loop corrections and important for values of $\phi$ close to $\langle\sigma\rangle$. For $n=1,2$ the beta functions of their coefficient are computed at three-loops. In the infrared (IR) limit the dilaton fluctuations decouple, the effective operators are suppressed by large $\langle\sigma\rangle$ and the effective potential becomes that of a renormalizable theory with explicit scale symmetry breaking by the "usual" DR scheme (of $\mu=$constant).
Learning Molecular Dynamics with Simple Language Model built upon Long Short-Term Memory Neural Network<|sep|>Recurrent neural networks (RNNs) have led to breakthroughs in natural language processing and speech recognition, wherein hundreds of millions of people use such tools on a daily basis through smartphones, email servers and other avenues. In this work, we show such RNNs, specifically Long Short-Term Memory (LSTM) neural networks can also be applied to capturing the temporal evolution of typical trajectories arising in chemical and biological physics. Specifically, we use a character-level language model based on LSTM. This learns a probabilistic model from 1-dimensional stochastic trajectories generated from molecular dynamics simulations of a higher dimensional system. We show that the model can not only capture the Boltzmann statistics of the system but it also reproduce kinetics at a large spectrum of timescales. We demonstrate how the embedding layer, introduced originally for representing the contextual meaning of words or characters, exhibits here a nontrivial connectivity between different metastable states in the underlying physical system. We demonstrate the reliability of our model and interpretations through different benchmark systems and a single molecule force spectroscopy trajectory for multi-state riboswitch. We anticipate that our work represents a stepping stone in the understanding and use of RNNs for modeling and predicting dynamics of complex stochastic molecular systems.
On the effectiveness of threshold resummation away from hadronic endpoint<|sep|>We parameterize the enhancement of threshold effects away from hadronic endpoint that arise due to the steeply falling nature of parton distribution functions, within the context of soft-collinear effective theory. This is accomplished in a process-independent way by directly linking the charac- teristic scale of soft and collinear radiation, \lambda, to the shape of the pdfs. This allows us quantify the power corrections to partonic threshold resummation as a function of the invariant mass and rapidity of the final state. In the context of SCET, being able to compute \lambda in a process-independent manner allows us to determine the correct scale for threshold resummation after integration with the pdfs, without any additional procedure.
Regularized 3D spectroscopy with CubeFit: method and application to the Galactic Center Circumnuclear disk<|sep|>The Galactic Center black hole and the nuclear star cluster are surrounded by a clumpy ring of gas and dust (the circumnuclear disk, CND) that rotates about them at a standoff distance of ~1.5 pc. The mass and density of individual clumps in the CND are disputed. We seek to use H$_2$ to characterize the clump size distribution and to investigate the morphology and dynamics of the interface between the ionized interior layer of the CND and the molecular reservoir lying further out (corresponding to the inner rim of the CND, illuminated in ultraviolet light by the central star cluster). We have observed two fields of approximately 20"x20" in the CND at near-infrared wavelengths with the OSIRIS spectro-imager at the Keck Observatory. These two fields, located at the approaching and receding nodes of the CND, best display this interface. Our data cover two H$_2$ lines as well as the Br$\gamma$ line (tracing H ii). We have developed the tool CubeFit, an original method to extract maps of continuous physical parameters (such as velocity field and velocity dispersion) from integral-field spectroscopy data, using regularization to largely preserve spatial resolution in regions of low signal-to-noise ratio. This original method enables us to isolate compact, bright features in the interstellar medium of the CND. Several clumps in the southwestern field assume the appearance of filaments, many of which are parallel to each other. We conclude that these clumps cannot be self-gravitating.
Suppression of complete fusion due to breakup in the reactions $^{10,11}$B + $^{209}$Bi<|sep|>Above-barrier cross sections of $\alpha$-active heavy reaction products, as well as fission, were measured for the reactions of $^{10,11}$B with $^{209}$Bi. Detailed analysis showed that the heavy products include components from incomplete fusion as well as complete fusion (CF), but fission originates almost exclusively from CF. Compared with fusion calculations without breakup, the CF cross sections are suppressed by 15% for $^{10}$B and 7% for $^{11}$B. A consistent and systematic variation of the suppression of CF for reactions of the weakly bound nuclei $^{6,7}$Li, $^{9}$Be, $^{10,11}$B on targets of $^{208}$Pb and $^{209}$Bi is found as a function of the breakup threshold energy.
The interplay between structural, magnetic and electronic states in the pyrochlore iridate Eu2Ir2O7<|sep|>We address the concomitant metal-insulator transition (MIT) and antiferromagnetic ordering in the novel pyrochlore iridate Eu2Ir2O7 by combining x-ray absorption spectroscopy, x-ray and neutron diffractions and density functional theory (DFT) based calculations. The temperature dependent powder x-ray diffraction clearly rules out any change in the lattice symmetry below the MIT, nevertheless a clear anomaly in the Ir-O-Ir bond angle and Ir-O bond length is evident at the onset of MIT. From the x-ray absorption near edge structure (XANES) spectroscopic study of Ir-L3 and L2 edges, the effective spin-orbit coupling is found to be intermediate, at least quite far from the strong atomic spin-orbit coupling limit. Powder neutron diffraction measurement is in line with an all-in-all-out magnetic structure of the Ir-tetrahedra in this compound, which is quite common among rare-earth pyrochlore iridates. The sharp change in the Ir-O-Ir bond angle around the MIT possibly arises from the exchange striction mechanism, which favors an enhanced electron correlation via weakening of Ir-Ir orbital overlap and an insulating phase below TMI . The theoretical calculations indicate an insulating state for shorter bond angle validating the experimental observation. Our DFT calculations show a possibility of intriguing topological phase below a critical value of the Ir-O distance, which is shorter than the experimentally observed bond length. Therefore, a topological state may be realized in bulk Eu2Ir2O7 sample if the Ir-O bond length can be reduced by the application of sufficient external pressure.
Computing the eigenvalues of symmetric H2-matrices by slicing the spectrum<|sep|>The computation of eigenvalues of large-scale matrices arising from finite element discretizations has gained significant interest in the last decade. Here we present a new algorithm based on slicing the spectrum that takes advantage of the rank structure of resolvent matrices in order to compute m eigenvalues of the generalized symmetric eigenvalue problem in $\mathcal{O}(n m \log^\alpha n)$ operations, where $\alpha>0$ is a small constant.
O and Na abundance patterns in open clusters of the Galactic disk<|sep|>Aims. A global O-Na abundance anti-correlation is observed in globular clusters, which is not present in the Galactic field population. Open clusters are thought to be chemically homogeneous internally. We aim to explore the O and Na abundance pattern among the open cluster population of the Galactic disk. Methods. We combine open cluster abundance ratios of O and Na from high-resolution spectroscopic studies in the literature and normalize them to a common solar scale. We compare the open cluster abundances against the globular clusters and disk field. Results. We find that the different environments show different abundance patterns. The open clusters do not show the O-Na anti-correlation at the extreme O-depletion / Na-enhancement as observed in globular clusters. Furthermore, the high Na abundances in open clusters do not match the disk field stars. If real, it may be suggesting that the dissolution of present-day open clusters is not a significant contribution to building the Galactic disk. Large-scale homogeneous studies of clusters and field will further confirm the reality of the Na enhancement.
Entanglement of three-qubit random pure states<|sep|>We study non-local properties of generic three-qubit pure states. First, we obtain the distributions of both the coefficients and the only phase in the five-term decomposition of Ac\'in et al. for an ensemble of random pure states generated by the Haar measure on U(8). Furthermore, we analyze the probability distributions of two sets of polynomial invariants. One of these sets allows us to classify three-qubit pure states into four classes. Entanglement in each class is characterized using the minimal R\'enyi-Ingarden-Urbanik entropy. Besides, the fidelity of a three-qubit random state with the closest state in each entanglement class is investigated. We also present a characterization of these classes and the SLOCC classes in terms of the corresponding entanglement polytope.
Computable Random Variables and Conditioning<|sep|>The aim of this paper is to present an elementary computable theory of random variables, based on the approach to probability via valuations. The theory is based on a type of lower-measurable sets, which are controlled limits of open sets, and extends existing work in this area by providing a computable theory of conditional random variables. The theory is based within the framework of type-two effectivity, so has an explicit direct link with Turing computation, and is expressed in a system of computable types and operations, so has a clean mathematical description.
UNIONS: The impact of systematic errors on weak-lensing peak counts<|sep|>UNIONS is an ongoing deep photometric multi-band survey of the Northern sky. As part of UNIONS, CFIS provides r-band data which we use to study weak-lensing peak counts for cosmological inference. We assess systematic effects for weak-lensing peak counts and their impact on cosmological parameters for the UNIONS survey. In particular, we present results on local calibration, metacalibration shear bias, baryonic feedback, the source galaxy redshift estimate, intrinsic alignment, and the cluster member dilution. For each uncertainty and systematic effect, we describe our mitigation scheme and the impact on cosmological parameter constraints. We obtain constraints on cosmological parameters from MCMC using CFIS data and MassiveNuS N-body simulations as a model for peak counts statistics. Depending on the calibration (local versus global, and the inclusion of the residual multiplicative shear bias), the mean matter density parameter $\Omega_m$ can shift up to $-0.024$ ($-0.5\sigma$). We also see that including baryonic corrections can shift $\Omega_m$ by $+0.027$ ($+0.5 \sigma$) with respect to the DM-only simulations. Reducing the impact of the intrinsic alignment and cluster member dilution through signal-to-noise cuts can lead to a shift in $\Omega_m$ of $+0.027$ ($+0.5 \sigma$). Finally, with a mean redshift uncertainty of $\Delta \bar{z} = 0.03$, we see that the shift of $\Omega_m$ ($+0.001$ which corresponds to $+0.02 \sigma$) is not significant. This paper investigates for the first time with UNIONS weak-lensing data and peak counts the impact of systematic effects. The value of $\Omega_m$ is the most impacted and can shift up to $\sim 0.03$ which corresponds to $0.5\sigma$ depending on the choices for each systematics. We expect constraints to become more reliable with future (larger) data catalogues, for which the current pipeline will provide a starting point.
The Angular-Diameter-Distance-Maximum and Its Redshift as Constraints on $\Lambda \neq 0$ FLRW Models<|sep|>The plethora of recent cosmologically relevant data has indicated that our universe is very well fit by a standard Friedmann-Lema\^{i}tre-Robertson-Walker (FLRW) model, with $\Omega_{M} \approx 0.27$ and $\Omega_{\Lambda} \approx 0.73$ -- or, more generally, by nearly flat FLRW models with parameters close to these values. Additional independent cosmological information, particularly the maximum of the angular-diameter (observer-area) distance and the redshift at which it occurs, would improve and confirm these results, once sufficient precise Supernovae Ia data in the range $1.5 < z < 1.8$ become available. We obtain characteristic FLRW closed functional forms for $C = C(z)$ and $\hat{M}_0 = \hat{M}_0(z)$, the angular-diameter distance and the density per source counted, respectively, when $\Lambda \neq 0$, analogous to those we have for $\Lambda = 0$. More importantly, we verify that for flat FLRW models $z_{max}$ -- as is already known but rarely recognized -- the redshift of $C_{max}$, the maximum of the angular-diameter-distance, uniquely gives $\Omega_{\Lambda}$, the amount of vacuum energy in the universe, independently of $H_0$, the Hubble parameter. For non-flat models determination of both $z_{max}$ and $C_{max}$ gives both $\Omega_{\Lambda}$ and $\Omega_M$, the amount of matter in the universe, as long as we know $H_0$ independently. Finally, determination of $C_{max}$ automatically gives a very simple observational criterion for whether or not the universe is flat -- presuming that it is FLRW.
Symmetry-adapted perturbation theory based on multiconfigurational wave function description of monomers<|sep|>We present a formulation of the multiconfigurational (MC) wave function symmetry-adapted perturbation theory (SAPT). The method is applicable to noncovalent interactions between monomers which require a multiconfigurational description, in particular when the interacting system is strongly correlated or in an electronically excited state. SAPT(MC) is based on one- and two-particle reduced density matrices of the monomers and assumes the single-exchange approximation for the exchange energy contributions. Second-order terms are expressed through response properties from extended random phase approximation (ERPA) equations. SAPT(MC) is applied either with generalized valence bond perfect pairing (GVB) or with complete active space self consistent field (CASSCF) treatment of the monomers. We discuss two model multireference systems: the H2-H2 dimer in out-of-equilibrium geometries and interaction between the argon atom and excited state of ethylene. In both cases SAPT(MC) closely reproduces benchmark results. Using the C2H4-Ar complex as an example, we examine second-order terms arising from negative transitions in the linear response function of an excited monomer. We demonstrate that the negative-transition terms must be accounted for to ensure qualitative prediction of induction and dispersion energies and develop a procedure allowing for their computation. Factors limiting the accuracy of SAPT(MC) are discussed in comparison with other second-order SAPT schemes on a data set of small single-reference dimers.
An efficient method for removing point sources from full-sky radio interferometric maps<|sep|>A new generation of wide-field radio interferometers designed for 21-cm surveys is being built as drift scan instruments allowing them to observe large fractions of the sky. With large numbers of antennas and frequency channels the enormous instantaneous data rates of these telescopes require novel, efficient, data management and analysis techniques. The $m$-mode formalism exploits the periodicity of such data with the sidereal day, combined with the assumption of statistical isotropy of the sky, to achieve large computational savings and render optimal analysis methods computationally tractable. We present an extension to that work that allows us to adopt a more realistic sky model and treat objects such as bright point sources. We develop a linear procedure for deconvolving maps, using a Wiener filter reconstruction technique, which simultaneously allows filtering of these unwanted components. We construct an algorithm, based on the Sherman-Morrison-Woodbury formula, to efficiently invert the data covariance matrix, as required for any optimal signal-to-noise weighting. The performance of our algorithm is demonstrated using simulations of a cylindrical transit telescope.
Development of readout electronics a novel beam monitoring system for ion research facility accelerator<|sep|>This article presents the readout electronics of a novel beam monitoring system for ion research facility accelerator. The readout electronics are divided into Front-end Card (FEC) and Readout Control Unit (RCU). FEC uses Topmetal II minus to processes the energy of the hitting particles and convert it into a voltage signal. The main function of RCU is to digitize the analog output signal of FEC and format the raw data. On the other hand, the RCU also processes the control commands from the host and distributes the commands according to the mapping. The readout electronic has been characterized and calibrated in the laboratory, and have been installed with the detector. Implementation and testing of readout electronics have been discussed.
Random Quantum Circuits are Approximate 2-designs<|sep|>Given a universal gate set on two qubits, it is well known that applying random gates from the set to random pairs of qubits will eventually yield an approximately Haar-distributed unitary. However, this requires exponential time. We show that random circuits of only polynomial length will approximate the first and second moments of the Haar distribution, thus forming approximate 1- and 2-designs. Previous constructions required longer circuits and worked only for specific gate sets. As a corollary of our main result, we also improve previous bounds on the convergence rate of random walks on the Clifford group.
Mesoscale Modelling of the Tolman Length in Multi-component Systems<|sep|>In this paper we analyze the curvature corrections to the surface tension in the context of the Shan-Chen (SC) multi-component Lattice Boltzmann method (LBM). We demonstrate that the same techniques recently applied in the context of the Shan-Chen multi-phase model can be applied to multi-component mixtures. We implement, as a new application, the calculation of the surface of tension radius $R_s$ through the minimization of the generalized surface tension $\sigma[R]$. In turn we are able to estimate the Tolman length, i.e. the first order coefficient of the curvature expansion of the surface tension $\sigma(R)$, as well as the higher order corrections, i.e. the curvature- and the Gaussian-rigidity coefficients. The SC multi-component model allows to model both fully-symmetric as well as asymmetric interactions among the components. By performing an extensive set of simulations we present a first example of tunable Tolman length in the mesoscopic model, being zero for symmetric interactions and different from zero otherwise. This result paves the way for controlling such interface properties which are paramount in presence of thermal fluctuations. All reported results can be independently reproduced through the "idea.deploy" framework available at https://github.com/lullimat/idea.deploy.
Representation Matters: Improving Perception and Exploration for Robotics<|sep|>Projecting high-dimensional environment observations into lower-dimensional structured representations can considerably improve data-efficiency for reinforcement learning in domains with limited data such as robotics. Can a single generally useful representation be found? In order to answer this question, it is important to understand how the representation will be used by the agent and what properties such a 'good' representation should have. In this paper we systematically evaluate a number of common learnt and hand-engineered representations in the context of three robotics tasks: lifting, stacking and pushing of 3D blocks. The representations are evaluated in two use-cases: as input to the agent, or as a source of auxiliary tasks. Furthermore, the value of each representation is evaluated in terms of three properties: dimensionality, observability and disentanglement. We can significantly improve performance in both use-cases and demonstrate that some representations can perform commensurate to simulator states as agent inputs. Finally, our results challenge common intuitions by demonstrating that: 1) dimensionality strongly matters for task generation, but is negligible for inputs, 2) observability of task-relevant aspects mostly affects the input representation use-case, and 3) disentanglement leads to better auxiliary tasks, but has only limited benefits for input representations. This work serves as a step towards a more systematic understanding of what makes a 'good' representation for control in robotics, enabling practitioners to make more informed choices for developing new learned or hand-engineered representations.
Particle filter efficiency under limited communication<|sep|>Sequential Monte Carlo methods are typically not straightforward to implement on parallel architectures. This is because standard resampling schemes involve communication between all particles. The $\alpha$-sequential Monte Carlo method was proposed recently as a potential solution to this which limits communication between particles. This limited communication is controlled through a sequence of stochastic matrices known as $\alpha$-matrices. We study the influence of the communication structure on the convergence and stability properties of the resulting algorithms. In particular, we quantitatively show that the mixing properties of the $\alpha$-matrices play an important role in the stability properties of the algorithm. Moreover, we prove that one can ensure good mixing properties by using randomized communication structures where each particle only communicates with a few neighboring particles. The resulting algorithms converge at the usual Monte Carlo rate. This leads to efficient versions of distributed sequential Monte Carlo.
Demonstration of a dual-pass differential Fabry-Perot interferometer for future interferometric space gravitational wave antennas<|sep|>A dual-pass differential Fabry-Perot interferometer (DPDFPI) is one candidate of the interferometer configurations utilized in future Fabry-Perot type space gravitational wave antennas, such as Deci-hertz Interferometer Gravitational Wave Observatory. In this paper, the working principle of the DPDFPI has been investigated and necessity to adjust the absolute length of the cavity for the operation of the DPDFPI has been found. In addition, using the 55-cm-long prototype, the operation of the DPDFPI has been demonstrated for the first time and it has been confirmed that the adjustment of the absolute arm length reduces the cavity detuning as expected. This work provides the proof of concept of the DPDFPI for application to the future Fabry-Perot type space gravitational wave antennas.
Complexity of the robust weighted independent set problems on interval graphs<|sep|>This paper deals with the max-min and min-max regret versions of the maximum weighted independent set problem on interval graphswith uncertain vertex weights. Both problems have been recently investigated by Nobibon and Leus (2014), who showed that they are NP-hard for two scenarios and strongly NP-hard if the number of scenarios is a part of the input. In this paper, new complexity and approximation results on the problems under consideration are provided, which extend the ones previously obtained. Namely, for the discrete scenario uncertainty representation it is proven that if the number of scenarios $K$ is a part of the input, then the max-min version of the problem is not at all approximable. On the other hand, its min-max regret version is approximable within $K$ and not approximable within $O(\log^{1-\epsilon}K)$ for any $\epsilon>0$ unless the problems in NP have quasi polynomial algorithms. Furthermore, for the interval uncertainty representation it is shown that the min-max regret version is NP-hard and approximable within 2.
Towards the azimuthal characteristics of ionospheric and seismic effects of "Chelyabinsk" meteorite fall according to the data from coherent radar, GPS and seismic networks<|sep|>We present the results of a study of the azimuthal characteristics of ionospheric and seismic effects of the meteorite 'Chelyabinsk', based on the data from the network of GPS receivers, coherent decameter radar EKB SuperDARN and network of seismic stations. It is shown, that 6-14 minutes after the bolide explosion, GPS network observed the cone-shaped wavefront of TIDs that is interpreted as a ballistic acoustic wave. The typical TIDs propagation velocity were observed 661+/-256m/s, which corresponds to the expected acoustic wave speed for 240km height. 14 minutes after the bolide explosion, at distances of 200km we observed the emergence and propagation of a TID with spherical wavefront, that is interpreted as gravitational mode of internal acoustic waves. The propagation velocity of this TID was 337+/-89m/s which corresponds to the propagation velocity of these waves in similar situations. At EKB SuperDARN radar, we observed TIDs in the sector of azimuthal angles close to the perpendicular to the meteorite trajectory. The observed TID velocity (400 m/s) and azimuthal properties correlate well with the model of ballistic wave propagating at 120-140km altitude. It is shown, that the azimuthal distribution of the amplitude of vertical seismic oscillations can be described qualitatively by the model of vertical strike-slip rupture, propagating at 1km/s along the meteorite fall trajectory to distance of about 40km. These parameters correspond to the direction and velocity of propagation of the ballistic wave peak by the ground. It is shown, that the model of ballistic wave caused by supersonic motion and burning of the meteorite in the upper atmosphere can satisfactorily explain the various azimuthal ionospheric effects, observed by the coherent decameter radar EKB SuperDARN, GPS-receivers network, as well as the azimuthal characteristics of seismic waves at large distances.
Supersymmetry identifies molecular Stark states whose eigenproperties can be obtained analytically<|sep|>We made use of supersymmetric (SUSY) quantum mechanics to find a condition under which the Stark effect problem for a polar and polarizable closed-shell diatomic molecule subject to collinear electrostatic and nonresonant radiative fields becomes exactly solvable. The condition, $\Delta \omega = \frac{\omega^2}{4 (m+1)^2}$, connects values of the dimensionless parameters $\omega$ and $\Delta \omega$ that characterize the strengths of the permanent and induced dipole interactions of the molecule with the respective fields. The exact solutions are obtained for the $|\tilde{J}=m,m;\omega,\Delta \omega>$ family of "stretched" states. The field-free and strong-field limits of the combined-fields problem were found to exhibit supersymmetry and shape-invariance, which is indeed the reason why they are analytically solvable. By making use of the analytic form of the $|\tilde{J}=m,m;\omega,\Delta \omega>$ wavefunctions, we obtained simple formulae for the expectation values of the space-fixed electric dipole moment, the alignment cosine, the angular momentum squared, and derived a "sum rule" which combines the above expectation values into a formula for the eigenenergy. The analytic expressions for the characteristics of the strongly oriented and aligned states provide a direct access to the values of the interaction parameters required for creating such states in the laboratory.
Sequence Training of DNN Acoustic Models With Natural Gradient<|sep|>Deep Neural Network (DNN) acoustic models often use discriminative sequence training that optimises an objective function that better approximates the word error rate (WER) than frame-based training. Sequence training is normally implemented using Stochastic Gradient Descent (SGD) or Hessian Free (HF) training. This paper proposes an alternative batch style optimisation framework that employs a Natural Gradient (NG) approach to traverse through the parameter space. By correcting the gradient according to the local curvature of the KL-divergence, the NG optimisation process converges more quickly than HF. Furthermore, the proposed NG approach can be applied to any sequence discriminative training criterion. The efficacy of the NG method is shown using experiments on a Multi-Genre Broadcast (MGB) transcription task that demonstrates both the computational efficiency and the accuracy of the resulting DNN models.
Tropical cyclone size is strongly limited by the Rhines scale: experiments with a barotropic model<|sep|>Recent work found evidence using aquaplanet experiments that tropical cyclone size on Earth is limited by the Rhines scale, which depends on the planetary vorticity gradient, $\beta$. This study aims to examine how the Rhines scale limits the size of an individual tropical cyclone. The traditional Rhines scale is first re-expressed as a vortex Rhines scale and Rhines speed to characterize how wave effects vary with radius in a vortex whose wind profile is known. Experiments are performed using a simple barotropic model on a $\beta$-plane initialized with a TC-like axisymmetric vortex defined using a recently-developed theoretical model for the tropical cyclone wind profile. $\beta$ and initial vortex size are each systematically varied to investigate the detailed responses of the TC-like vortex to $\beta$. Results show that the vortex shrinks towards an equilibrium size that closely follows the vortex Rhines scale. Physically, this scale divides the vortex into a vortex-dominant region at small radii, where the axisymmetric circulation is steady, and a wave-dominant region at larger radii, where the circulation stimulates Rossby waves and dissipates. A larger initial vortex relative to its vortex Rhines scale will shrink faster, and the shrinking timescale is well described by the vortex Rhines timescale, which is defined as the overturning timescale of the circulation at the vortex Rhines scale and is shown to be directly related to the Rossby wave group velocity. The relationship between our idealized results and the real Earth is discussed.
A Multi-View Discriminant Learning Approach for Indoor Localization Using Bimodal Features of CSI<|sep|>With the growth of location-based services, indoor localization is attracting great interests as it facilitates further ubiquitous environments. Specifically, device free localization using wireless signals is getting increased attention as human location is estimated using its impact on the surrounding wireless signals without any active device tagged with subject. In this paper, we propose MuDLoc, the first multi-view discriminant learning approach for device free indoor localization using both amplitude and phase features of Channel State Information (CSI) from multiple APs. Multi-view learning is an emerging technique in machine learning which improve performance by utilizing diversity from different view data. In MuDLoc, the localization is modeled as a pattern matching problem, where the target location is predicted based on similarity measure of CSI features of an unknown location with those of the training locations. MuDLoc implements Generalized Inter-view and Intra-view Discriminant Correlation Analysis (GI$^{2}$DCA), a discriminative feature extraction approach using multi-view CSIs. It incorporates inter-view and intra-view class associations while maximizing pairwise correlations across multi-view data sets. A similarity measure is performed to find the best match to localize a subject. Experimental results from two cluttered environments show that MuDLoc can estimate location with high accuracy which outperforms other benchmark approaches.
A transient solution for vesicle electrodeformation and relaxation<|sep|>A transient analysis for vesicle deformation under DC electric fields is developed. The theory extends from a droplet model, with the additional consideration of a lipid membrane separating two fluids of arbitrary properties. For the latter, both a membrane-charging and a membrane-mechanical model are supplied. The vesicle is assumed to remain spheroidal in shape for all times. The main result is an ODE governing the evolution of the vesicle aspect ratio. The effects of initial membrane tension and pulse length are examined. The model prediction is extensively compared with experimental data, and is shown to accurately capture the system behavior in the regime of no or weak electroporation. More importantly, the comparison reveals that vesicle relaxation obeys a universal behavior regardless of the means of deformation. The process is governed by a single timescale that is a function of the vesicle initial radius, the fluid viscosity, and the initial membrane tension. This universal scaling law can be used to calculate membrane properties from experimental data.
Efficient tomography with unknown detectors<|sep|>We compare the two main techniques used for estimating the state of a physical system from unknown measurements: standard detector tomography and data-pattern tomography. Adopting linear inversion as a fair benchmark, we show that the difference between these two protocols can be traced back to the nonexistence of the reverse-order law for pseudoinverses. We capitalize on this fact to identify regimes where the data-pattern approach outperforms the standard one and vice versa. We corroborate these conclusions with numerical simulations of relevant examples of quantum state tomography.
Qubit exchange interactions from permutations of classical bits<|sep|>In order to prepare for the introduction of dynamical many-body and, eventually, field theoretical models, we show here that quantum mechanical exchange interactions in a three-spin chain can emerge from the deterministic dynamics of three classical Ising spins. States of the latter form an ontological basis, which will be discussed with reference to the ontology proposed in the Cellular Automaton Interpretation of Quantum Mechanics by 't Hooft. Our result illustrates a new Baker-Campbell-Hausdorff formula with terminating series expansion.
Cross-phase modulation mediated pulse control with Airy pulses in optical fibers<|sep|>We show that the velocity and thus the frequency of a signal pulse can be adjusted by the use of a control Airy pulse. In particular, we utilize a nonlinear Airy pulse which, via cross-phase modulation, creates an effective potential for the optical signal. Interestingly, during the interaction, the signal dispersion is suppressed. Importantly, the whole process is controllable and by using Airy pulses with different truncations leads to predetermined values of the frequency shifting. Such a functionality might be useful in wavelength division multiplexing networks.
The Radio Properties of Radio-Loud Narrow-Line Seyfert 1 Galaxies on Parsec Scales<|sep|>We present the detection of compact radio structures of fourteen radio-loud narrow line Seyfert 1 (NLS1) galaxies from Very Long Baseline Array observations at 5 GHz, which were performed in 2013. While 50\% of the sources of our sample show a compact core only, the remaining 50\% exhibit a core-jet structure. The measured brightness temperatures of the cores range from $10^{8.4}$ to $10^{11.4}$ K with a median value of $10^{10.1}$ K, indicating that the radio emission is from non-thermal jets, and that, likely, most sources are not strongly beamed, then implying a lower jet speed in these radio-loud NLS1 galaxies. In combination with archival data taken at multiple frequencies, we find that seven sources show flat or even inverted radio spectra, while steep spectra are revealed in the remaining seven objects. Although all these sources are very radio-loud with $R > 100$, their jet properties are diverse, in terms of their milli-arcsecond (mas) scale (pc scale) morphology and their overall radio spectral shape. The evidence for slow jet speeds (i.e., less relativistic jets), in combination with the low kinetic/radio power, may offer an explanation for the compact VLBA radio structure in most sources. The mildly relativistic jets in these high accretion rate systems are consistent with a scenario, where jets are accelerated from the hot corona above the disk by the magnetic field and the radiation force of the accretion disk. Alternatively, a low jet bulk velocity can be explained by low spin in the Blandford-Znajek mechanism.
Near-field thermal radiative transfer between two coated spheres<|sep|>In this work, we present an expression for the near-field thermal radiative transfer between two spheres with an arbitrary numbers of coatings. We numerically demonstrate that the spectrum of heat transfer between layered spheres exhibits novel features due to the newly introduced interfaces between coatings and cores. These features include broad super-Planckian peaks at non-resonant frequencies and near-field selective emission between metallic spheres with polar material coatings. Spheres with cores and coatings of two different polar materials are also shown to exceed the total conductance of homogeneous spheres in some cases.
Structural entanglements in protein complexes<|sep|>We consider multi-chain protein native structures and propose a criterion that determines whether two chains in the system are entangled or not. The criterion is based on the behavior observed by pulling at both temini of each chain simultaneously in the two chains. We have identified about 900 entangled systems in the Protein Data Bank and provided a more detailed analysis for several of them. We argue that entanglement enhances the thermodynamic stability of the system but it may have other functions: burying the hydrophobic residues at the interface, and increasing the DNA or RNA binding area. We also study the folding and stretching properties of the knotted dimeric proteins MJ0366, YibK and bacteriophytochrome. These proteins have been studied theoretically in their monomeric versions so far. The dimers are seen to separate on stretching through the tensile mechanism and the characteristic unraveling force depends on the pulling direction.
Automatic Data Expansion for Customer-care Spoken Language Understanding<|sep|>Spoken language understanding (SLU) systems are widely used in handling of customer-care calls.A traditional SLU system consists of an acoustic model (AM) and a language model (LM) that areused to decode the utterance and a natural language understanding (NLU) model that predicts theintent. While AM can be shared across different domains, LM and NLU models need to be trainedspecifically for every new task. However, preparing enough data to train these models is prohibitivelyexpensive. In this paper, we introduce an efficient method to expand the limited in-domain data. Theprocess starts with training a preliminary NLU model based on logistic regression on the in-domaindata. Since the features are based onn= 1,2-grams, we can detect the most informative n-gramsfor each intent class. Using these n-grams, we find the samples in the out-of-domain corpus that1) contain the desired n-gram and/or 2) have similar intent label. The ones which meet the firstconstraint are used to train a new LM model and the ones that meet both constraints are used to train anew NLU model. Our results on two divergent experimental setups show that the proposed approachreduces by 30% the absolute classification error rate (CER) comparing to the preliminary modelsand it significantly outperforms the traditional data expansion algorithms such as the ones based onsemi-supervised learning, TF-IDF and embedding vectors.
Exact recursive updating of uncertainty sets for discrete-time plants with a lag<|sep|>In a recent paper [arXiv:1612.04918] there are new results concerning the polytopic set of possible states of a linear discrete-time SISO system subject to bounded disturbances from measurements corrupted by bounded noise. Using these results we construct an algorithm which, for the special case of a plant with a lag, recursively updates these polytopic sets when new measurements arrive.
Reaching the boundary between stellar kinematic groups and very wide binaries. III. Sixteen new stars and eight new wide systems in the beta Pictoris moving group<|sep|>Aims. We look for common proper motion companions to stars of the nearby young beta Pictoris moving group. Methods. First, we compiled a list of 185 beta Pictoris members and candidate members from 35 representative works. Next, we used the Aladin and STILTS virtual observatory tools, and the PPMXL proper motion and Washington Double Star catalogues to look for companion candidates. The resulting potential companions were subjects of a dedicated astro-photometric follow-up using public data from all-sky surveys. After discarding 67 sources by proper motion and 31 by colour-magnitude diagrams, we obtained a final list of 36 common proper motion systems. The binding energy of two of them is perhaps too small to be considered physically bound. Results. Of the 36 pairs and multiple systems, eight are new, 16 have only one stellar component previously classified as a beta Pictoris member, and three have secondaries at or below the hydrogen-burning limit. Sixteen stars are reported here for the first time as moving group members. The unexpected large number of high-order multiple systems, 12 triples and two quadruples among 36 systems, may suggest a biased list of members towards close binaries or an increment of the high-order-multiple fraction for very wide systems.
An Effective Algorithmic Framework for Near Optimal Multi-Robot Path Planning<|sep|>We present a centralized algorithmic framework for solving multi-robot path planning problems in general, two-dimensional, continuous environments while minimizing globally the task completion time. The framework obtains high levels of effectiveness through the composition of an optimal discretization of the continuous environment and the subsequent fast, near-optimal resolution of the resulting discrete planning problem. This principled approach achieves orders of magnitudes better performance with respect to both speed and the supported robot density. For a wide variety of environments, our method is shown to compute globally near-optimal solutions for fifty robots in seconds with robots packed close to each other. In the extreme, the method can consistently solve problems with hundreds of robots that occupy over 30% of the free space.
Scalable and Flexible Classical Shadow Tomography with Tensor Networks<|sep|>Classical shadow tomography is a powerful randomized measurement protocol for predicting many properties of a quantum state with few measurements. Two classical shadow protocols have been extensively studied in the literature: the single-qubit (local) Pauli measurement, which is well suited for predicting local operators but inefficient for large operators; and the global Clifford measurement, which is efficient for low-rank operators but infeasible on near-term quantum devices due to the extensive gate overhead. In this work, we demonstrate a scalable classical shadow tomography approach for generic randomized measurements implemented with finite-depth local Clifford random unitary circuits, which interpolates between the limits of Pauli and Clifford measurements. The method combines the recently proposed locally-scrambled classical shadow tomography framework with tensor network techniques to achieve scalability for computing the classical shadow reconstruction map and evaluating various physical properties. The method enables classical shadow tomography to be performed on shallow quantum circuits with superior sample efficiency and minimal gate overhead and is friendly to noisy intermediate-scale quantum (NISQ) devices. We show that the shallow-circuit measurement protocol provides immediate, exponential advantages over the Pauli measurement protocol for predicting quasi-local operators. It also enables a more efficient fidelity estimation compared to the Pauli measurement.
IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation<|sep|>Text attribute transfer aims to automatically rewrite sentences such that they possess certain linguistic attributes, while simultaneously preserving their semantic content. This task remains challenging due to a lack of supervised parallel data. Existing approaches try to explicitly disentangle content and attribute information, but this is difficult and often results in poor content-preservation and ungrammaticality. In contrast, we propose a simpler approach, Iterative Matching and Translation (IMaT), which: (1) constructs a pseudo-parallel corpus by aligning a subset of semantically similar sentences from the source and the target corpora; (2) applies a standard sequence-to-sequence model to learn the attribute transfer; (3) iteratively improves the learned transfer function by refining imperfections in the alignment. In sentiment modification and formality transfer tasks, our method outperforms complex state-of-the-art systems by a large margin. As an auxiliary contribution, we produce a publicly-available test set with human-generated transfer references.
The jet kinetic power, distance and inclination of GRS 1915+105<|sep|>We apply a recently developed technique of calculating the minimum jet kinetic power to the major mass ejections of the black-hole binary GRS 1915+105 observed in radio wavelengths in 1994 and 1997. We derive for them the distance-dependent minimum power, and the corresponding mass flow rate and the total energy and mass content. We find that a fast increase of the jet power with the increasing distance combined with the jet power estimates based on the bolometric luminosity imply the source distance is <10 kpc. If the jet in GRS 1915 contains ions, their bulk motion dominates the jet power, which was either neglected or not properly taken into account earlier. We also reconsider the parameters of the binary, and derive the current best estimates of the distance-dependent black-hole mass and the inclination based on existing measurements combined with the kinematic constraints from the mass ejections. We also find the measurement of the donor radius of Steeghs et al. implies the distance to the system of <10 kpc, in agreement with the estimate from the jet power.
Retrieving and Ranking Similar Questions from Question-Answer Archives Using Topic Modelling and Topic Distribution Regression<|sep|>Presented herein is a novel model for similar question ranking within collaborative question answer platforms. The presented approach integrates a regression stage to relate topics derived from questions to those derived from question-answer pairs. This helps to avoid problems caused by the differences in vocabulary used within questions and answers, and the tendency for questions to be shorter than answers. The performance of the model is shown to outperform translation methods and topic modelling (without regression) on several real-world datasets.
Refinement of Predicted Missing Parts Enhance Point Cloud Completion<|sep|>Point cloud completion is the task of predicting complete geometry from partial observations using a point set representation for a 3D shape. Previous approaches propose neural networks to directly estimate the whole point cloud through encoder-decoder models fed by the incomplete point set. By predicting the complete model, the current methods compute redundant information because the output also contains the known incomplete input geometry. This paper proposes an end-to-end neural network architecture that focuses on computing the missing geometry and merging the known input and the predicted point cloud. Our method is composed of two neural networks: the missing part prediction network and the merging-refinement network. The first module focuses on extracting information from the incomplete input to infer the missing geometry. The second module merges both point clouds and improves the distribution of the points. Our experiments on ShapeNet dataset show that our method outperforms the state-of-the-art methods in point cloud completion. The code of our methods and experiments is available in \url{https://github.com/ivansipiran/Refinement-Point-Cloud-Completion}.
Anatomy of Fluorescence: Quantum trajectory statistics from continuously measuring spontaneous emission<|sep|>We investigate the continuous quantum measurement of a superconducting qubit undergoing fluorescence. The fluorescence of the qubit is detected via a phase-preserving heterodyne measurement, giving the fluorescence quadrature signals as two continuous qubit readout results. By using the stochastic path integral approach to the measurement physics, we derive most likely paths between boundary conditions on the state, and compute approximate time correlation functions between all stochastic variables via diagrammatic perturbation theory. We focus on paths that increase in energy during the continuous measurement. Our results are compared to Monte Carlo numerical simulation of the trajectories, and we find close agreement between direct simulation and theory. We generalize this analysis to arbitrary diffusive quantum systems that are continuously monitored.
Distributed Dynamic State Estimation for Microgrids<|sep|>Conventionally, the dynamic state estimation of variables in power networks is performed based on the forecasting-aided model of bus voltages. This approach is effective in the stiff grids at the transmission level, where the bus voltages are less sensitive to variations of the load. However, in microgrids, bus voltages can fluctuate significantly under load changes, the forecasting-aided model may not sufficiently accurate. To resolve this problem, this paper proposes a dynamic state estimation scheme for microgrids using the state-space model derived from differential equations of power networks. In the proposed scheme, the branch currents are the state variables, whereas the bus voltages become the inputs which can vary freely with loads. As a result, the entire microgrids system can be partitioned into local areas, where neighbor areas share the common inputs. The proposed estimation scheme then can be implemented in a distributed manner. A novel Kalman-based filtering method is derived to estimate both states and inputs simultaneously. Only information of common inputs is exchanged between neighboring estimators. Simulation results of the 13-bus Potsdam microgrid (New York State) are provided to prove the feasibility and performances of the proposed scheme.
Neural UpFlow: A Scene Flow Learning Approach to Increase the Apparent Resolution of Particle-Based Liquids<|sep|>We present a novel up-resing technique for generating high-resolution liquids based on scene flow estimation using deep neural networks. Our approach infers and synthesizes small- and large-scale details solely from a low-resolution particle-based liquid simulation. The proposed network leverages neighborhood contributions to encode inherent liquid properties throughout convolutions. We also propose a particle-based approach to interpolate between liquids generated from varying simulation discretizations using a state-of-the-art bidirectional optical flow solver method for fluids in addition to a novel key-event topological alignment constraint. In conjunction with the neighborhood contributions, our loss formulation allows the inference model throughout epochs to reward important differences in regard to significant gaps in simulation discretizations. Even when applied in an untested simulation setup, our approach is able to generate plausible high-resolution details. Using this interpolation approach and the predicted displacements, our approach combines the input liquid properties with the predicted motion to infer semi-Lagrangian advection. We furthermore showcase how the proposed interpolation approach can facilitate generating large simulation datasets with a subset of initial condition parameters.
Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model<|sep|>Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency graph or constituent trees. In this paper, we first propose to use the \textit{syntactic graph} to represent three types of syntactic information, i.e., word order, dependency and constituency features. We further employ a graph-to-sequence model to encode the syntactic graph and decode a logical form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.
Adsorption and ring-opening of lactide on a chiral metal surface studied by Density Functional Theory<|sep|>We study the adsorption and ring-opening of lactide on the naturally chiral metal surface Pt(321)$^S$. Lactide is a precursor for polylactic acid ring-opening polymerization and Pt is a well known catalyst surface. We study here the energetics of the ring-opening of lactide on a surface that has a high density of kink atoms. These sites are expected to be present on a realistic Pt surface and show enhanced catalytic activity. The use of a naturally chiral surface also enables us to study potential chiral selectivity effects of the reaction at the same time. Using Density Functional Theory (DFT) with a functional that includes the van der Waals forces in a first-principles manner, we find modest adsorption energies of around 1.4 eV for the pristine molecule and different ring-opened states. The energy barrier to be overcome in the ring-opening reaction is found to be very small at 0.32 eV and 0.30 eV for LL- and its chiral partner DD-lactide, respectively. These energies are much smaller than the activation energy for a dehydrogenation reaction of 0.78 eV. Our results thus indicate that (a) ring-opening reactions of lactide on Pt(321) can be expected already at very low temperatures and Pt might be a very effective catalyst for this reaction; (b) the ring-opening reaction rate shows noticeable enantioselectivity.
Non-integrability of geodesic flow on certain algebraic surfaces<|sep|>This paper addresses an open problem recently posed by V. Kozlov: a rigorous proof of the non-integrability of the geodesic flow on the cubic surface $x y z=1$. We prove this is the case using the Morales-Ramis theorem and Kovacic algorithm. We also consider some consequences and extensions of this result.
Holes within galaxies: the egg or the hen?<|sep|>Unsustained matter distributions unescapely collapse unless fragmentation and centrifugal or pressure support take place. Starting from the above evidence, supermassive compact objects at the centre of large-mass galaxies are conceived as the end-product of the gravitational collapse of local density maxima around which overdensities are located. At the beginning of evolution, local density maxima are idealized as homogeneous peaks, while the surrounding envelopes are described by a power-law density profile. The dependence of the density profile on a second parameter, chosen to be the ratio between peak and total mass, is analysed. Overdensity evolution is discussed in the context of quintessence cosmological models and further investigation is devoted to a special case with the aim to describe the central collapse. An empirical relation between hole and dark halo mass is translated into a dependence of the fractional hole mass on the overdensity mass. Computations are performed up to the end of central collapse, and density profiles of related configurations are determined together with additional parameters. The central collapse is completed in early times, no longer than a few hundredths of Gyr, which implies hole formation when proto-haloes, proto-bulges, and proto-disks are still expanding or contracting. No appreciable change in evolution is found with regard to different mean peak heights related to equal masses. On the other hand, it is recognized that homogeneous peaks collapse ``faster'' with respect to surroundings envelopes, in low-mass than in large-mass overdensities. In conclusion, it is inferred that gravitational collapse of homogeneous peaks within overdensities may be a viable mechanism for hole generation.
An Empirical Study on Activity Recognition in Long Surgical Videos<|sep|>Activity recognition in surgical videos is a key research area for developing next-generation devices and workflow monitoring systems. Since surgeries are long processes with highly-variable lengths, deep learning models used for surgical videos often consist of a two-stage setup using a backbone and temporal sequence model. In this paper, we investigate many state-of-the-art backbones and temporal models to find architectures that yield the strongest performance for surgical activity recognition. We first benchmark the models performance on a large-scale activity recognition dataset containing over 800 surgery videos captured in multiple clinical operating rooms. We further evaluate the models on the two smaller public datasets, the Cholec80 and Cataract-101 datasets, containing only 80 and 101 videos respectively. We empirically found that Swin-Transformer+BiGRU temporal model yielded strong performance on both datasets. Finally, we investigate the adaptability of the model to new domains by fine-tuning models to a new hospital and experimenting with a recent unsupervised domain adaptation approach.
Single Atom Detection With Optical Cavities<|sep|>We present a thorough analysis of single atom detection using optical cavities. The large set of parameters that influence the signal-to-noise ratio for cavity detection is considered, with an emphasis on detunings, probe power, cavity finesse and photon detection schemes. Real device operating restrictions for single photon counting modules and standard photodiodes are included in our discussion, with heterodyne detection emerging as the clearly favourable technique, particularly for detuned detection at high power.
String Art: Circle Drawing Using Straight Lines<|sep|>An algorithm to generate the locus of a circle using the intersection points of straight lines is proposed. The pixels on the circle are plotted independent of one another and the operations involved in finding the locus of the circle from the intersection of straight lines are parallelizable. Integer only arithmetic and algorithmic optimizations are used for speedup. The proposed algorithm makes use of an envelope to form a parabolic arc which is consequent transformed into a circle. The use of parabolic arcs for the transformation results in higher pixel errors as the radius of the circle to be drawn increases. At its current state, the algorithm presented may be suitable only for generating circles for string art.
Unitarity triangle angles from penguin-dominated B meson decays<|sep|>In this time of transition to a new generation of quark flavor experiments we review both the theoretical and the experimental progress on the determination of unitarity triangle angles from penguin-dominated B decays. This summarizes the activities of the Working Group VI at the CKM2010 workshop.
Unsupervised Learning of Compositional Energy Concepts<|sep|>Humans are able to rapidly understand scenes by utilizing concepts extracted from prior experience. Such concepts are diverse, and include global scene descriptors, such as the weather or lighting, as well as local scene descriptors, such as the color or size of a particular object. So far, unsupervised discovery of concepts has focused on either modeling the global scene-level or the local object-level factors of variation, but not both. In this work, we propose COMET, which discovers and represents concepts as separate energy functions, enabling us to represent both global concepts as well as objects under a unified framework. COMET discovers energy functions through recomposing the input image, which we find captures independent factors without additional supervision. Sample generation in COMET is formulated as an optimization process on underlying energy functions, enabling us to generate images with permuted and composed concepts. Finally, discovered visual concepts in COMET generalize well, enabling us to compose concepts between separate modalities of images as well as with other concepts discovered by a separate instance of COMET trained on a different dataset. Code and data available at https://energy-based-model.github.io/comet/.
GraphITE: Estimating Individual Effects of Graph-structured Treatments<|sep|>Outcome estimation of treatments for target individuals is an important foundation for decision making based on causal relations. Most existing outcome estimation methods deal with binary or multiple-choice treatments; however, in some applications, the number of treatments can be significantly large, while the treatments themselves have rich information. In this study, we considered one important instance of such cases: the outcome estimation problem of graph-structured treatments such as drugs. Owing to the large number of possible treatments, the counterfactual nature of observational data that appears in conventional treatment effect estimation becomes more of a concern for this problem. Our proposed method, GraphITE (pronounced "graphite") learns the representations of graph-structured treatments using graph neural networks while mitigating observation biases using Hilbert-Schmidt Independence Criterion regularization, which increases the independence of the representations of the targets and treatments. Experiments on two real-world datasets show that GraphITE outperforms baselines, especially in cases with a large number of treatments.
VLT/SPHERE deep insight of NGC 3603's core: Segregation or confusion?<|sep|>We present new near-infrared photometric measurements of the core of the young massive cluster NGC 3603 obtained with extreme adaptive optics. The data were obtained with the SPHERE instrument mounted on ESO Very Large Telescope, and cover three fields in the core of this cluster. We applied a correction for the effect of extinction to our data obtained in the J and K broadband filters and estimated the mass of detected sources inside the field of view of SPHERE/IRDIS, which is 13.5"x13.5". We derived the mass function (MF) slope for each spectral band and field. The MF slope in the core is unusual compared to previous results based on Hubble space telescope (HST) and very large telescope (VLT) observations. The average slope in the core is estimated as -1.06^{+0.26}_{-0.26} for the main sequence stars with 3.5 Msun < M < 120 Msun.Thanks to the SPHERE extreme adaptive optics, 814 low-mass stars were detected to estimate the MF slope for the pre-main sequence stars with 0.6 Msun< M < 3.5 Msun , Gamma = -0.54^{+0.11}_{-0.11} in the K-band images in two fields in the core of the cluster. For the first time, we derive the mass function of the very core of the NGC 3603 young cluster for masses in the range 0.6 - 120 Msun. Previous studies were either limited by crowding, lack of dynamic range, or a combination of both.
Dissipative instability in partially ionised prominence plasmas<|sep|>We investigate the nature of dissipative instability at the boundary (seen here as tangential discontinuity) between the viscous corona and the partially ionised prominence plasma in the incompressible limit. The importance of the partial ionisation is investigated in terms of the ionisation fraction. Matching the solutions for the transversal component of the velocity and total pressure at the interface between the prominence and coronal plasmas, we derive a dispersion relation whose imaginary part describes the evolution of the instability. Results are obtained in the limit of weak dissipation. Using simple analytical methods, we show that dissipative instabilities appear for flow speeds that are lower than the Kelvin-Helmholtz instability threshold. While viscosity tends to destabilise the plasma, the effect of partial ionisation (through the Cowling resistivity) will act towards stabilising the interface. For ionisation degrees closer to a neutral gas the interface will be unstable for larger values of equilibrium flow. The same principle is assumed when studying the appearance of instability at the interface between prominences and dark plumes. The unstable mode appearing in this case has a very small growth rate and dissipative instability cannot explain the appearance of flows in plumes. The present study improves our understanding of the complexity of dynamical processes at the interface of solar prominences and solar corona, and the role partial ionisation can have on the stability of the plasma. Our results clearly show that the problem of partial ionisation introduces new aspects of plasma stability with consequences on the evolution of solar prominences.
Probability models of chance fluctuations in spectra of astronomical sources with applications to X-ray absorption lines<|sep|>The search for faint emission or absorption lines in astronomical spectra has received considerable attention in recent years, especially in the X-ray wavelength range. These features usually appear as a deficit or excess of counts in a single resolution element of the detector, and as such they are referred to as unresolved fluctuations. The general problem under investigation is the probability of occurrence of chance fluctuations. A quantitative answer is necessary to determine whether detected fluctuations are a real (astronomical, in this case) signal, or if they can be attributed to chance. This application note provides a new comprehensive method to answer this question as function of the instrument's resolution, the wavelength coverage of the spectrum, the number of fluctuations of interest, and the confidence level chosen. The method is based on the binomial distribution, and addresses also the probability of multiple simultaneous fluctuations. A critical aspect of the model is the a priori knowledge of the location of possible fluctuations, which significantly affects the probability of detection. In fact, a wider wavelength range for the 'blind' search of possible fluctuations results in a larger number of 'tries' for the detection of a fluctuation, lowering the overall significance of a specific feature. The method is illustrated with a case study and examples using X-ray data.
Respondent-driven Sampling on Directed Networks<|sep|>Respondent-driven sampling (RDS) is a commonly used substitute for random sampling when studying hidden populations, such as injecting drug users or men who have sex with men, for which no sampling frame is known. The method is an extension of the snowball sample method and can, given that some assumptions are met, generate unbiased population estimates. One key assumption, not likely to be met, is that the acquaintance network in which the recruitment process takes place is undirected, meaning that all recruiters should have the potential to be recruited by the person they recruit. Here we investigate the potential bias of directedness by simulating RDS on real and artificial network structures. We show that directedness is likely to generate bias that cannot be compensated for unless the sampled individuals know how many that potentially may have recruited them (i.e. their indegree), which is unlikely in most situations. Based on one known parameter, we propose an estimator for RDS on directed networks when only outdegrees are observed. By comparison of current RDS estimators' performances on networks with varying structures, we find that our new estimator, together with a recent estimator, which requires the population size as a known quantity, have relatively low level of estimate error and bias. Based on our new estimator, sensitivity analysis can be made by varying values of the known parameter to take uncertainty of network directedness and error in reporting degrees into account. Finally, we have developed a bootstrap procedure for the new estimator to construct confidence intervals.
Ordering states with coherence measures<|sep|>The quantification of quantum coherence has attracted a growing attention, and based on various physical contexts, several coherence measures have been put forward. An interesting question is whether these coherence measures give the same ordering when they are used to quantify the coherence of quantum states. In this paper, we consider the two well-known coherence measures, the $l_1$ norm of coherence and the relative entropy of coherence, to show that there are the states for which the two measures give a different ordering. Our analysis can be extended to other coherence measures, and as an illustration of the extension we further consider the formation of coherence to show that the $l_1$ norm of coherence and the formation of coherence, as well as the relative entropy of coherence and the coherence of formation, do not give the same ordering too.
Stability of Brans-Dicke thin shell wormholes<|sep|>Recently, a class of spherically symmetric thin-shell wormholes in Brans-Dicke gravity have been introduced. Such wormholes can be supported by matter satisfying the weak energy condition (WEC). In this paper, we first obtain all the exact solutions satisfying the WEC. Then we show these solutions can be stable for certain parameters. A general requirement for stability is that $\beta^2>1$, which may imply that the speed of sound exceeds the speed of light.
Partial Differential Equations Meet Deep Neural Networks: A Survey<|sep|>Many problems in science and engineering can be represented by a set of partial differential equations (PDEs) through mathematical modeling. Mechanism-based computation following PDEs has long been an essential paradigm for studying topics such as computational fluid dynamics, multiphysics simulation, molecular dynamics, or even dynamical systems. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. At the same time, solving PDEs efficiently has been a long-standing challenge. Generally, except for a few differential equations for which analytical solutions are directly available, many more equations must rely on numerical approaches such as the finite difference method, finite element method, finite volume method, and boundary element method to be solved approximately. These numerical methods usually divide a continuous problem domain into discrete points and then concentrate on solving the system at each of those points. Though the effectiveness of these traditional numerical methods, the vast number of iterative operations accompanying each step forward significantly reduces the efficiency. Recently, another equally important paradigm, data-based computation represented by deep learning, has emerged as an effective means of solving PDEs. Surprisingly, a comprehensive review for this interesting subfield is still lacking. This survey aims to categorize and review the current progress on Deep Neural Networks (DNNs) for PDEs. We discuss the literature published in this subfield over the past decades and present them in a common taxonomy, followed by an overview and classification of applications of these related methods in scientific research and engineering scenarios. The origin, developing history, character, sort, as well as the future trends in each potential direction of this subfield are also introduced.
Determination of Higgs-boson couplings (SFitter)<|sep|>After the discovery of a Higgs boson, the next step is to measure its properties and test their accordance with the predictions of the Standard Model, in particular the couplings of the Higgs boson. In this talk we discuss what information the LHC will be able to give us over the coming years, and what remains as a task for a future Linear Collider. Using the well-established SFitter framework, we map measurements onto a weak-scale effective theory with general Higgs boson couplings. Our sophisticated error treatment allows us to take all theory and experimental errors, including arbitrary correlations, fully into account.
GRB 130603B: No Compelling Evidence For Neutron Star Merger<|sep|>Near infrared (NIR) flare/rebrightening in the afterglow of the short hard gamma ray burst (SHB) 130603B measured with the Hubble Space Telescope (HST) and an alleged late-time X-ray excess were interpreted as possible evidence of a neutron-star merger origin of this SHB. However, the X-ray afterglow that was measured with the Swift-XRT and Newton XMM have the canonical behaviour of a synchrotron afterglow produced by a highly relativistic jet. The H-band flux observed with HST 9.41 days after burst is that expected from the measured late-time X-ray afterglow. A late-time flare/re-brightening of a NIR-Optical afterglow of SHB can be produced by jet collision with an interstellar density bump, or by a kilonova, but jet plus kilonova can be produced also by the collapse of compact stars (neutron star, strange star, or quark star) to a more compact object due to cooling, loss of angular momentum, or mass accretion.
Deep Determinantal Point Processes<|sep|>Determinantal point processes (DPPs) have attracted significant attention as an elegant model that is able to capture the balance between quality and diversity within sets. DPPs are parameterized by a positive semi-definite kernel matrix. While DPPs have substantial expressive power, they are fundamentally limited by the parameterization of the kernel matrix and their inability to capture nonlinear interactions between items within sets. We present the deep DPP model as way to address these limitations, by using a deep feed-forward neural network to learn the kernel matrix. In addition to allowing us to capture nonlinear item interactions, the deep DPP also allows easy incorporation of item metadata into DPP learning. Since the learning target is the DPP kernel matrix, the deep DPP allows us to use existing DPP algorithms for efficient learning, sampling, and prediction. Through an evaluation on several real-world datasets, we show experimentally that the deep DPP can provide a considerable improvement in the predictive performance of DPPs, while also outperforming strong baseline models in many cases.
Applying the relativistic quantization condition to a three-particle bound state in a periodic box<|sep|>Using our recently developed relativistic three-particle quantization condition, we study the finite-volume energy shift of a spin-zero three-particle bound state. We reproduce the result obtained using non-relativistic quantum mechanics by Meissner, Rios and Rusetsky, and generalize the result to a moving frame.
Quantum Work Capacitances<|sep|>The possibility of using quantum effects to speed up the charging processes of batteries have been vastly investigated. In order to traslate these ideas into working devices it is however crucial to assess the stability of the storage phase in the quantum battery elements when they are in contact with environmental noise. In this work we formalize this problem introducing a series of operationally well defined figures of merit (the work capacitances and the Maximal Asymptotic Work/Energy Ratios) which gauge the highest efficiency one can attain in recovering useful energy from quantum battery models that are formed by large collections of identical and independent elements (quantum cells or q-cells). Explicit evaluations of such quantities are presented for the case where the energy storing system undergoes through dephasing and depolarizing noise.
Formation of cluster crystals in an ultra-soft potential model on a spherical surface<|sep|>We investigate the formation of cluster crystals with multiply occupied lattice sites on a spherical surface in systems of ultra-soft particles interacting via repulsive, bounded pair potentials. Not all interactions of this kind lead to clustering: we generalize the criterion devised in C.N. Likos et al., Phys. Rev. E, 2001, 63, 031206 to spherical systems in order to distinguish between cluster forming systems and fluids which display reentrant melting. We use both DFT and Monte Carlo simulations to characterize the behavior of the system, and obtain semi-quantitative agreement between the two. Furthermore, we study the effect of topological frustration on the system due to the sphere curvature by comparing the properties of disclinations, i.e., clusters with fewer than six neighbors, and non-defective clusters. Disclinations are shown to be less stable, contain fewer particles, and be closer to their neighbors than other lattice points: these properties are explained on the basis of geometric and energetic considerations.
Replacing Backpropagation with Biological Plausible Top-down Credit Assignment in Deep Neural Networks Training<|sep|>Top-down connections in the biological brain has been shown to be important in high cognitive functions. However, the function of this mechanism in machine learning has not been defined clearly. In this study, we propose to lay out a framework constituted by a bottom-up and a top-down network. Here, we use a Top-down Credit Assignment Network (TDCA-network) to replace the loss function and back propagation (BP) which serve as the feedback mechanism in traditional bottom-up network training paradigm. Our results show that the credit given by well-trained TDCA-network outperforms the gradient from backpropagation in classification task under different settings on multiple datasets. In addition, we successfully use a credit diffusing trick, which can keep training and testing performance remain unchanged, to reduce parameter complexity of the TDCA-network. More importantly, by comparing their trajectories in the parameter landscape, we find that TDCA-network directly achieved a global optimum, in contrast to that backpropagation only can gain a localized optimum. Thus, our results demonstrate that TDCA-network not only provide a biological plausible learning mechanism, but also has the potential to directly achieve global optimum, indicating that top-down credit assignment can substitute backpropagation, and provide a better learning framework for Deep Neural Networks.
Intrinsic Dimension Estimation via Nearest Constrained Subspace Classifier<|sep|>We consider the problems of classification and intrinsic dimension estimation on image data. A new subspace based classifier is proposed for supervised classification or intrinsic dimension estimation. The distribution of the data in each class is modeled by a union of of a finite number ofaffine subspaces of the feature space. The affine subspaces have a common dimension, which is assumed to be much less than the dimension of the feature space. The subspaces are found using regression based on the L0-norm. The proposed method is a generalisation of classical NN (Nearest Neighbor), NFL (Nearest Feature Line) classifiers and has a close relationship to NS (Nearest Subspace) classifier. The proposed classifier with an accurately estimated dimension parameter generally outperforms its competitors in terms of classification accuracy. We also propose a fast version of the classifier using a neighborhood representation to reduce its computational complexity. Experiments on publicly available datasets corroborate these claims.
Absorption lines from magnetically-driven winds in X-ray binaries<|sep|>High resolution X-ray spectra of black hole X-ray binaries (BHBs) show blueshifted absorption lines from disk winds which seem to be equatorial. Winds occur in the Softer (disk-dominated) states of the outburst and are less prominent or absent in the Harder (power-law dominated) states. We use self-similar magneto-hydrodynamic (MHD) accretion-ejection models to explain the disk winds in BHBs. In our models, the density at the base of the outflow from the accretion disk is not a free parameter, but is determined by solving the full set of dynamical MHD equations. Thus the physical properties of the outflow are controlled by the global structure of the disk. We studied different MHD solutions characterized by different values of (a) the disk aspect ratio ($\varepsilon$) and (b) the ejection efficiency ($p$). We use two kinds of MHD solutions depending on the absence (cold solution) or presence (warm solution) of heating at the disk surface. Such heating could be from e.g. dissipation of energy due to MHD turbulence in the disk or from illumination. We use each of these MHD solutions to predict the physical parameters of an outflow; put limits on the ionization parameter ($\xi$), column density and timescales, motivated by observational results; and thus select regions within the outflow which are consistent with the observed winds. The cold MHD solutions cannot account for winds due to their low ejection efficiency. But warm solutions can explain the observed physical quantities in the wind because they can have sufficiently high values of $p$ ($\gtrsim 0.1$, implying larger mass loading at the base of the outflow). Further from our thermodynamic equilibrium curve analysis for the outflowing gas, we found that in the Hard state a range of $\xi$ is thermodynamically unstable, and had to be excluded. This constrain made it impossible to have any wind at all, in the Hard state.
A large neighbourhood based heuristic for two-echelon routing problems<|sep|>In this paper, we address two optimisation problems arising in the context of city logistics and two-level transportation systems. The two-echelon vehicle routing problem and the two-echelon location routing problem seek to produce vehicle itineraries to deliver goods to customers, with transits through intermediate facilities. To efficiently solve these problems, we propose a hybrid metaheuristic which combines enumerative local searches with destroy-and-repair principles, as well as some tailored operators to optimise the selections of intermediate facilities. We conduct extensive computational experiments to investigate the contribution of these operators to the search performance, and measure the performance of the method on both problem classes. The proposed algorithm finds the current best known solutions, or better ones, for 95% of the two-echelon vehicle routing problem benchmark instances. Overall, for both problems, it achieves high-quality solutions within short computing times. Finally, for future reference, we resolve inconsistencies between different versions of benchmark instances, document their differences, and provide them all online in a unified format.
Nitsche-XFEM for optimal control problems governed by elliptic PDEs with interfaces<|sep|>For the optimal control problem governed by elliptic equations with interfaces, we present a numerical method based on the Hansbo's Nitsche-XFEM. We followed the Hinze's variational discretization concept to discretize the continuous problem on a uniform mesh. We derive optimal error estimates of the state, co-state and control both in mesh dependent norm and L2 norm. In addition, our method is suitable for the model with non-homogeneous interface condition. Numerical results confirmed our theoretical results, with the implementation details discussed.
GRI: focusing on the evolving violent Universe<|sep|>The Gamma-Ray Imager (GRI) is a novel mission concept that will provide an unprecedented sensitivity leap in the soft gamma-ray domain by using for the first time a focusing lens built of Laue diffracting crystals. The lens will cover an energy band from 200 - 1300 keV with an effective area reaching 600 cm2. It will be complemented by a single reflection multilayer coated mirror, extending the GRI energy band into the hard X-ray regime, down to ~10 keV. The concentrated photons will be collected by a position sensitive pixelised CZT stack detector. We estimate continuum sensitivities of better than 10^-7 ph/cm2/s/keV for a 100 ks exposure; the narrow line sensitivity will be better than 3 x 10^-6 ph/cm2/s for the same integration time. As focusing instrument, GRI will have an angular resolution of better than 30 arcsec within a field of view of roughly 5 arcmin - an unprecedented achievement in the gamma-ray domain. Owing to the large focal length of 100 m of the lens and the mirror, the optics and detector will be placed on two separate spacecrafts flying in formation in a high elliptical orbit. R&D work to enable the lens focusing technology and to develop the required focal plane detector is currently underway, financed by ASI, CNES, ESA, and the Spanish Ministery of Education and Science. The GRI mission is proposed as class M mission for ESA's Cosmic Vision 2015-2025 program. GRI will allow studies of particle acceleration processes and explosion physics in unprecedented detail, providing essential clues on the innermost nature of the most violent and most energetic processes in the Universe.
Do Encoder Representations of Generative Dialogue Models Encode Sufficient Information about the Task ?<|sep|>Predicting the next utterance in dialogue is contingent on encoding of users' input text to generate appropriate and relevant response in data-driven approaches. Although the semantic and syntactic quality of the language generated is evaluated, more often than not, the encoded representation of input is not evaluated. As the representation of the encoder is essential for predicting the appropriate response, evaluation of encoder representation is a challenging yet important problem. In this work, we showcase evaluating the text generated through human or automatic metrics is not sufficient to appropriately evaluate soundness of the language understanding of dialogue models and, to that end, propose a set of probe tasks to evaluate encoder representation of different language encoders commonly used in dialogue models. From experiments, we observe that some of the probe tasks are easier and some are harder for even sophisticated model architectures to learn. And, through experiments we observe that RNN based architectures have lower performance on automatic metrics on text generation than transformer model but perform better than the transformer model on the probe tasks indicating that RNNs might preserve task information better than the Transformers.
The Birth of a Galaxy. II. The Role of Radiation Pressure<|sep|>Massive stars provide feedback that shapes the interstellar medium of galaxies at all redshifts and their resulting stellar populations. Here we present three adaptive mesh refinement radiation hydrodynamics simulations that illustrate the impact of momentum transfer from ionising radiation to the absorbing gas on star formation in high-redshift dwarf galaxies. Momentum transfer is calculated by solving the radiative transfer equation with a ray tracing algorithm that is adaptive in spatial and angular coordinates. We find that momentum input partially affects star formation by increasing the turbulent support to a three-dimensional rms velocity equal to the circular velocity of early haloes. Compared to a calculation that neglects radiation pressure, the star formation rate is decreased by a factor of five to 1.8 x 10^{-2} Msun/yr in a dwarf galaxy with a dark matter and stellar mass of 2.0 x 10^8 and 4.5 x 10^5 solar masses, respectively, when radiation pressure is included. Its mean metallicity of 10^{-2.1} Z_sun is consistent with the observed dwarf galaxy luminosity-metallicity relation. However, what one may naively expect from the calculation without radiation pressure, the central region of the galaxy overcools and produces a compact, metal-rich stellar population with an average metallicity of 0.3 Z_sun, indicative of an incorrect physical recipe. In addition to photo-heating in HII regions, radiation pressure further drives dense gas from star forming regions, so supernovae feedback occurs in a warmer and more diffuse medium, launching metal-rich outflows. Capturing this aspect and a temporal separation between the start of radiative and supernova feedback are numerically important in the modeling of galaxies to avoid the "overcooling problem". We estimate that dust in early low-mass galaxies is unlikely to aid in momentum transfer from radiation to the gas.
Clustering of primordial black holes with non-Gaussian initial fluctuations<|sep|>We formulate the two-point correlation function of primordial black holes (PBHs) at their formation time, based on the functional integration approach which has often been used in the context of halo clustering. We find that PBH clustering on super-Hubble scales could never be induced in the case where the initial primordial fluctuations are Gaussian, while it can be enhanced by the so-called local-type trispectrum (four-point correlation function) of the primordial curvature perturbations.
Fitting strategies of accretion column models and application to the broadband spectrum of Cen X-3<|sep|>Due to the complexity of modeling the radiative transfer inside the accretion columns of neutron star binaries, their X-ray spectra are still commonly described with phenomenological models, for example, a cutoff power law. While the behavior of these models is well understood and they allow for a comparison of different sources and studying source behavior, the extent to which the underlying physics can be derived from the model parameters is very limited. During recent years, several physically motivated spectral models have been developed to overcome these limitations. Their application, however, is generally computationally much more expensive and they require a high number of parameters which are difficult to constrain. Previous works have presented an analytical solution to the radiative transfer equation inside the accretion column assuming a velocity profile that is linear in the optical depth. An implementation of this solution that is both fast and accurate enough to be fitted to observed spectra is available as a model in XSPEC. The main difficulty of this implementation is that some solutions violate energy conservation and therefore have to be rejected by the user. We propose a novel fitting strategy that ensures energy conservation during the $\chi^2$-minimization which simplifies the application of the model considerably. We demonstrate this approach as well a study of possible parameter degeneracies with a comprehensive Markov-chain Monte Carlo analysis of the complete parameter space for a combined NuSTAR and Swift/XRT dataset of Cen X-3. The derived accretion-flow structure features a small column radius of $\sim$63 m and a spectrum dominated by bulk-Comptonization of bremsstrahlung seed photons, in agreement with previous studies.
The Golden Age of Statistical Graphics<|sep|>Statistical graphics and data visualization have long histories, but their modern forms began only in the early 1800s. Between roughly 1850 and 1900 ($\pm10$), an explosive growth occurred in both the general use of graphic methods and the range of topics to which they were applied. Innovations were prodigious and some of the most exquisite graphics ever produced appeared, resulting in what may be called the ``Golden Age of Statistical Graphics.'' In this article I trace the origins of this period in terms of the infrastructure required to produce this explosive growth: recognition of the importance of systematic data collection by the state; the rise of statistical theory and statistical thinking; enabling developments of technology; and inventions of novel methods to portray statistical data. To illustrate, I describe some specific contributions that give rise to the appellation ``Golden Age.''
Subleading contributions to the chiral three-nucleon force I: long-range terms<|sep|>We derive the long-range contributions to the tree-nucleon force at next-to-next-to-next-to-leading order in the chiral expansion. We give both momentum and coordinate space representations.
Generalized Milankovitch Cycles and Longterm Climatic Habitability<|sep|>Although the Earth's orbit is never far from circular, terrestrial planets around other stars might experience substantial changes in eccentricity that could lead to climate changes, including possible "phase transitions" such as the snowball transition (or its opposite). There is evidence that Earth has gone through at least one globally frozen, "snowball" state in the last billion years, which it is thought to have exited after several million years because global ice-cover shut off the carbonate-silicate cycle, thereby allowing greenhouse gases to build up to sufficient concentration to melt the ice. Due to the positive feedback caused by the high albedo of snow and ice, susceptibility to falling into snowball states might be a generic feature of water-rich planets with the capacity to host life. This paper has two main thrusts. First, we revisit one-dimensional energy balance climate models as tools for probing possible climates of exoplanets, investigate the dimensional scaling of such models, and introduce a simple algorithm to treat the melting of the ice layer on a globally-frozen planet. We show that if a terrestrial planet undergoes Milankovitch-like oscillations of eccentricity that are of great enough magnitude, it could melt out of a snowball state. Second, we examine the kinds of variations of eccentricity that a terrestrial planet might experience due to the gravitational influence of a giant companion. We show that a giant planet on a sufficiently eccentric orbit can excite extreme eccentricity oscillations in the orbit of a habitable terrestrial planet. More generally, these two results demonstrate that the longterm habitability (and astronomical observables) of a terrestrial planet can depend on the detailed architecture of the planetary system in which it resides.
The dynamic nature of conflict in Wikipedia<|sep|>The voluntary process of Wikipedia edition provides an environment where the outcome is clearly a collective product of interactions involving a large number of people. We propose a simple agent-based model, developed from real data, to reproduce the collaborative process of Wikipedia edition. With a small number of simple ingredients, our model mimics several interesting features of real human behaviour, namely in the context of edit wars. We show that the level of conflict is determined by a tolerance parameter, which measures the editors' capability to accept different opinions and to change their own opinion. We propose to measure conflict with a parameter based on mutual reverts, which increases only in contentious situations. Using this parameter, we find a distribution for the inter-peace periods that is heavy-tailed. The effects of wiki-robots in the conflict levels and in the edition patterns are also studied. Our findings are compared with previous parameters used to measure conflicts in edit wars.
First demonstration of 200, 100, and 50 um pitch Resistive AC-Coupled Silicon Detectors (RSD) with 100% fill-factor for 4D particle tracking<|sep|>We designed, produced, and tested RSD (Resistive AC-Coupled Silicon Detectors) devices, an evolution of the standard LGAD (Low-Gain Avalanche Diode) technology where a resistive n-type implant and a coupling dielectric layer have been implemented. The first feature works as a resistive sheet, freezing the multiplied charges, while the second one acts as a capacitive coupling for readout pads. We succeeded in the challenging goal of obtaining very fine pitch (50, 100, and 200 um) while maintaining the signal waveforms suitable for high timing and 4D-tracking performances, as in the standard LGAD-based devices.
Quark nuggets search using 2350 Kg gravitational waves aluminum bar detectors<|sep|>The gravitational wave resonant detectors can be used as detectors of quark nuggets, like nuclearites (nuclear matter with a strange quark). This search has been carried out using data from two 2350 Kg, 2 K cooled, aluminum bar detectors: NAUTILUS, located in Frascati (Italy), and EXPLORER, that was located in CERN Geneva (CH). Both antennas are equipped with cosmic ray shower detectors: signals in the bar due to showers are continuously detected and used to characterize the antenna performances. The bar excitation mechanism is based on the so called thermo-acoustic effect, studied on dedicated experiments that use particle beams. This mechanism predicts that vibrations of bars are induced by the heat deposited in the bar from the particle. The geometrical acceptance of the bar detectors is 19.5 $\rm m^2$ sr, that is smaller than that of other detectors used for similar searches. However, the detection mechanism is completely different and is more straightforward than in other detectors. We will show the results of ten years of data from NAUTILUS (2003-2012) and 7 years from EXPLORER (2003-2009). The experimental limits we obtain are of interest because, for nuclearites of mass less than $10^{-4}$ grams, we find a flux smaller than that one predicted considering nuclearites as dark matter candidates.
Raptor Codes for Higher-Order Modulation Using a Multi-Edge Framework<|sep|>In this paper, we represent Raptor codes as multi-edge type low-density parity-check (MET-LDPC) codes, which gives a general framework to design them for higher-order modulation using MET density evolution. We then propose an efficient Raptor code design method for higher-order modulation, where we design distinct degree distributions for distinct bit levels. We consider a joint decoding scheme based on belief propagation for Raptor codes and also derive an exact expression for the stability condition. In several examples, we demonstrate that the higher-order modulated Raptor codes designed using the multi-edge framework outperform previously reported higher-order modulation codes in literature.
Event-Driven Network Model for Space Mission Optimization with High-Thrust and Low-Thrust Spacecraft<|sep|>Numerous high-thrust and low-thrust space propulsion technologies have been developed in the recent years with the goal of expanding space exploration capabilities; however, designing and optimizing a multi-mission campaign with both high-thrust and low-thrust propulsion options are challenging due to the coupling between logistics mission design and trajectory evaluation. Specifically, this computational burden arises because the deliverable mass fraction (i.e., final-to-initial mass ratio) and time of flight for low-thrust trajectories can can vary with the payload mass; thus, these trajectory metrics cannot be evaluated separately from the campaign-level mission design. To tackle this challenge, this paper develops a novel event-driven space logistics network optimization approach using mixed-integer linear programming for space campaign design. An example case of optimally designing a cislunar propellant supply chain to support multiple lunar surface access missions is used to demonstrate this new space logistics framework. The results are compared with an existing stochastic combinatorial formulation developed for incorporating low-thrust propulsion into space logistics design; our new approach provides superior results in terms of cost as well as utilization of the vehicle fleet. The event-driven space logistics network optimization method developed in this paper can trade off cost, time, and technology in an automated manner to optimally design space mission campaigns.
Prolongation of SMAP to Spatio-temporally Seamless Coverage of Continental US Using a Deep Learning Neural Network<|sep|>The Soil Moisture Active Passive (SMAP) mission has delivered valuable sensing of surface soil moisture since 2015. However, it has a short time span and irregular revisit schedule. Utilizing a state-of-the-art time-series deep learning neural network, Long Short-Term Memory (LSTM), we created a system that predicts SMAP level-3 soil moisture data with atmospheric forcing, model-simulated moisture, and static physiographic attributes as inputs. The system removes most of the bias with model simulations and improves predicted moisture climatology, achieving small test root-mean-squared error (<0.035) and high correlation coefficient >0.87 for over 75\% of Continental United States, including the forested Southeast. As the first application of LSTM in hydrology, we show the proposed network avoids overfitting and is robust for both temporal and spatial extrapolation tests. LSTM generalizes well across regions with distinct climates and physiography. With high fidelity to SMAP, LSTM shows great potential for hindcasting, data assimilation, and weather forecasting.
Importance Weighted Adversarial Variational Autoencoders for Spike Inference from Calcium Imaging Data<|sep|>The Importance Weighted Auto Encoder (IWAE) objective has been shown to improve the training of generative models over the standard Variational Auto Encoder (VAE) objective. Here, we derive importance weighted extensions to AVB and AAE. These latent variable models use implicitly defined inference networks whose approximate posterior density q_\phi(z|x) cannot be directly evaluated, an essential ingredient for importance weighting. We show improved training and inference in latent variable models with our adversarially trained importance weighting method, and derive new theoretical connections between adversarial generative model training criteria and marginal likelihood based methods. We apply these methods to the important problem of inferring spiking neural activity from calcium imaging data, a challenging posterior inference problem in neuroscience, and show that posterior samples from the adversarial methods outperform factorized posteriors used in VAEs.
Dynamo Saturation in Rapidly Rotating Solar-Type Stars<|sep|>The magnetic activity of solar-type stars generally increases with stellar rotation rate. The increase, however, saturates for fast rotation. The Babcock-Leighton mechanism of stellar dynamos saturates as well when the mean tilt-angle of active regions approaches ninety degrees. Saturation of magnetic activity may be a consequence of this property of the Babcock-Leighton mechanism. Stellar dynamo models with a tilt-angle proportional to the rotation rate are constructed to probe this idea. Two versions of the model - treating the tilt-angles globally and using Joy's law for its latitude dependence - are considered. Both models show a saturation of dynamo-generated magnetic flux at high rotation rates. The model with latitude-dependent tilt-angles also shows a change in dynamo regime in the saturation region. The new regime combines a cyclic dynamo at low latitudes with an (almost) steady polar dynamo.
Pull out all the stops: Textual analysis via punctuation sequences<|sep|>Whether enjoying the lucid prose of a favorite author or slogging through some other writer's cumbersome, heavy-set prattle (full of parentheses, em dashes, compound adjectives, and Oxford commas), readers will notice stylistic signatures not only in word choice and grammar, but also in punctuation itself. Indeed, visual sequences of punctuation from different authors produce marvelously different (and visually striking) sequences. Punctuation is a largely overlooked stylistic feature in "stylometry", the quantitative analysis of written text. In this paper, we examine punctuation sequences in a corpus of literary documents and ask the following questions: Are the properties of such sequences a distinctive feature of different authors? Is it possible to distinguish literary genres based on their punctuation sequences? Do the punctuation styles of authors evolve over time? Are we on to something interesting in trying to do stylometry without words, or are we full of sound and fury (signifying nothing)?
Symbolic models for nonlinear control systems affected by disturbances<|sep|>In the last few years there has been a growing interest in the use of symbolic models for the formal verification and control design of purely continuous or hybrid systems. Symbolic models are abstract descriptions of continuous systems where one symbol corresponds to an "aggregate" of continuous states. In this paper we face the problem of deriving symbolic models for nonlinear control systems affected by disturbances. The main contribution of this paper is in proposing symbolic models that can be effectively constructed and that approximate nonlinear control systems affected by disturbances in the sense of alternating approximate bisimulation.
Barcode Posets: Combinatorial Properties and Connections<|sep|>A barcode is a finite multiset of intervals on the real line, $B = \{ (b_i, d_i)\}_{i=1}^n$. Barcodes are important objects in topological data analysis, where they serve as summaries of the persistent homology groups of a filtration. The combinatorial properties of barcodes have also been studied, mainly in the context of interval orders and interval graphs. In this paper, we define a new family of maps from the space of barcodes with $n$ bars to the permutation sets of various multisets, known as multipermutations. These multipermutations provide new combinatorial invariants on the space of barcodes. We then define an order relation on these multipermutations, which we show can be interpreted as a crossing number for barcodes, reminiscent of T\'{u}ran's crossing number for graphs. Next, we show that the resulting posets are order-isomorphic to principal ideals of a well known poset known as the multinomial Newman lattice. Consequently, these posets form the graded face-lattices of polytopes, which we refer to as barcode lattices or barcode polytopes. Finally, we show that for a large class of barcodes, these invariants can provide bounds on the Wasserstein and bottleneck distances between a pair of barcodes, linking these discrete invariants to continuous metrics on barcodes.
The SILCC (SImulating the LifeCycle of molecular Clouds) project - II. Dynamical evolution of the supernova-driven ISM and the launching of outflows<|sep|>The SILCC project (SImulating the Life-Cycle of molecular Clouds) aims at a more self-consistent understanding of the interstellar medium (ISM) on small scales and its link to galaxy evolution. We present three-dimensional (magneto)hydrodynamic simulations of the ISM in a vertically stratified box including self-gravity, an external potential due to the stellar component of the galactic disc, and stellar feedback in the form of an interstellar radiation field and supernovae (SNe). The cooling of the gas is based on a chemical network that follows the abundances of H+, H, H2, C+, and CO and takes shielding into account consistently. We vary the SN feedback by comparing different SN rates, clustering and different positioning, in particular SNe in density peaks and at random positions, which has a major impact on the dynamics. Only for random SN positions the energy is injected in sufficiently low-density environments to reduce energy losses and enhance the effective kinetic coupling of the SNe with the gas. This leads to more realistic velocity dispersions (\sigma_HI ~ 0.8\sigma_(300-8000K) ~ 10-20km/s, \sigma_H\alpha ~ 0.6\sigma_(8000-3e5K) ~ 20-30km/s), and strong outflows with mass loading factors of up to 10 even for solar neighbourhood conditions. Clustered SNe abet the onset of outflows compared to individual SNe but do not influence the net outflow rate. The outflows do not contain any molecular gas and are mainly composed of atomic hydrogen. The bulk of the outflowing mass is dense (\rho ~ 1e-25-1e-24g/cc) and slow (v ~ 20-40km/s) but there is a high-velocity tail of up to v ~ 500km/s with \rho ~ 1e-28-1e-27g/cc.
Long tail distributions near the many body localization transition<|sep|>The random field S=1/2 Heisenberg chain exhibits a dynamical many body localization transition at a critical disorder strength, which depends on the energy density. At weak disorder, the eigenstate thermalization hypothesis (ETH) is fulfilled on average, making local observables smooth functions of energy, whose eigenstate-to-eigenstate fluctuations decrease exponentially with system size. We demonstrate the validity of ETH in the thermal phase as well as its breakdown in the localized phase and show that rare states exist which do not strictly follow ETH, becoming more frequent closer to the transition. Similarly, the probability distribution of the entanglement entropy at intermediate disorder develops long tails all the way down to zero entanglement. We propose that these low entanglement tails stem from localized regions at the subsystem boundaries which were recently discussed as a possible mechanism for subdiffusive transport in the ergodic phase.
Hierarchical Aggregation Approach for Distributed clustering of spatial datasets<|sep|>In this paper, we present a new approach of distributed clustering for spatial datasets, based on an innovative and efficient aggregation technique. This distributed approach consists of two phases: 1) local clustering phase, where each node performs a clustering on its local data, 2) aggregation phase, where the local clusters are aggregated to produce global clusters. This approach is characterised by the fact that the local clusters are represented in a simple and efficient way. And The aggregation phase is designed in such a way that the final clusters are compact and accurate while the overall process is efficient in both response time and memory allocation. We evaluated the approach with different datasets and compared it to well-known clustering techniques. The experimental results show that our approach is very promising and outperforms all those algorithms
Galactic model parameters of cataclysmic variables: Results from a new absolute magnitude calibration with 2MASS and WISE<|sep|>In order to determine the spatial distribution, Galactic model parameters and luminosity function of cataclysmic variables (CVs), a $J$-band magnitude limited sample of 263 CVs has been established using a newly constructed period-luminosity-colours (PLCs) relation which includes $J$, $K_{s}$ and $W1$-band magnitudes in 2MASS and WISE photometries, and the orbital periods of the systems. This CV sample is assumed to be homogeneous regarding to distances as the new PLCs relation is calibrated with new or re-measured trigonometric parallaxes. Our analysis shows that the scaleheight of CVs is increasing towards shorter periods, although selection effects for the periods shorter than 2.25 h dramatically decrease the scaleheight: the scaleheight of the systems increases from 192 pc to 326 pc as the orbital period decreases from 12 to 2.25h. The $z$-distribution of all CVs in the sample is well fitted by an exponential function with a scaleheight of 213$^{+11}_{-10}$ pc. However, we suggest that the scaleheight of CVs in the Solar vicinity should be $\sim$300 pc and that the scaleheights derived using the sech$^2$ function should be also considered in the population synthesis models. The space density of CVs in the Solar vicinity is found 5.58(1.35)$\times 10^{-6}$ pc$^{-3}$ which is in the range of previously derived space densities and not in agreement with the predictions of the population models. The analysis based on the comparisons of the luminosity function of white dwarfs with the luminosity function of CVs in this study show that the best fits are obtained by dividing the luminosity functions of white dwarfs by a factor of 350-450.
Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data<|sep|>What learning algorithms can be run directly on compressively-sensed data? In this work, we consider the question of accurately and efficiently computing low-rank matrix or tensor factorizations given data compressed via random projections. We examine the approach of first performing factorization in the compressed domain, and then reconstructing the original high-dimensional factors from the recovered (compressed) factors. In both the matrix and tensor settings, we establish conditions under which this natural approach will provably recover the original factors. While it is well-known that random projections preserve a number of geometric properties of a dataset, our work can be viewed as showing that they can also preserve certain solutions of non-convex, NP-Hard problems like non-negative matrix factorization. We support these theoretical results with experiments on synthetic data and demonstrate the practical applicability of compressed factorization on real-world gene expression and EEG time series datasets.
A systematic bias in fitting the surface-density profiles of interstellar filaments<|sep|>The surface-density profiles of dense filaments, in particular those traced by dust emission, appear to be well fit with Plummer profiles, i.e. Sigma(b)=Sigma_B+Sigma_O{1+[b/w_O]^2}^{[1-p]/2}. Here Sigma_B is the background surface-density; Sigma_B+Sigma_O is the surface-density on the filament spine; b is the impact parameter of the line-of-sight relative to the filament spine; w_O is the Plummer scale-length (which for fixed p is exactly proportional to the full-width at half-maximum, w_O=FWHM/2{2^{2/[p-1]}-1}^{1/2}); and p is the Plummer exponent (which reflects the slope of the surface-density profile away from the spine).} In order to improve signal-to-noise it is standard practice to average the observed surface-densities along a section of the filament, or even along its whole length, before fitting the profile. We show that, if filaments do indeed have intrinsic Plummer profiles with exponent p_INTRINSIC, but there is a range of w_O values along the length of the filament (and secondarily a range of Sigma_B values), the value of the Plummer exponent, p_FIT, estimated by fitting the averaged profile, may be significantly less than p_INTRINSIC. The decrease, Delta p = p_INTRINSIC - p_FIT, increases monotonically with increasing p_INTRINSIC; with increasing range of w_O values; and -- if, but only if, there is a finite range of w_O values -- with increasing range of Sigma_B values. For typical filament parameters the decrease is insignificant if p_INTRINSIC = 2 (0.05 <~ Delta p <~ 0.10), but for p_INTRINSIC =3 it is larger (0.18 <~ Delta p <~ 0.50), and for p_INTRINSIC =4 it is substantial (0.50 <~ Delta p <~ 1.15). On its own this effect is probably insufficient to support a value of p_INTRINSIC much greater than p_FIT ~ 2, but it could be important in combination with other effects.
Development of a hybrid method for stock trading based on TOPSIS, EMD and ELM<|sep|>Deciding when to buy or sell a stock is not an easy task because the market is hard to predict, being influenced by political and economic factors. Thus, methodologies based on computational intelligence have been applied to this challenging problem. In this work, every day the stocks are ranked by technique for order preference by similarity to ideal solution (TOPSIS) using technical analysis criteria, and the most suitable stock is selected for purchase. Even so, it may occur that the market is not favorable to purchase on certain days, or even, the TOPSIS make an incorrect selection. To improve the selection, another method should be used. So, a hybrid model composed of empirical mode decomposition (EMD) and extreme learning machine (ELM) is proposed. The EMD decomposes the series into several sub-series, and thus the main omponent (trend) is extracted. This component is processed by the ELM, which performs the prediction of the next element of component. If the value predicted by the ELM is greater than the last value, then the purchase of the stock is confirmed. The method was applied in a universe of 50 stocks in the Brazilian market. The selection made by TOPSIS showed promising results when compared to the random selection and the return generated by the Bovespa index. Confirmation with the EMD-ELM hybrid model was able to increase the percentage of profit tradings.
Dissipative spin chains: Implementation with cold atoms and steady-state properties<|sep|>We propose a quantum optical implementation of a class of dissipative spin systems, including the XXZ and Ising model, with ultra-cold atoms in optical lattices. Employing the motional degree of freedom of the atoms and detuned Raman transitions we show how to obtain engineerable dissipation and a tunable transversal magnetic field, enabling the study of the dynamics and steady-states of dissipative spin models. As an example of effects made accessible this way, we consider small spin chains and weak dissipation and show by numerical simulation that steady-state expectation values display pronounced peaks at certain critical system parameters. We show that this effect is related to degeneracies in the Hamiltonian and derive a sufficient condition for its occurrence.
Quantum transport through STM-lifted single PTCDA molecules<|sep|>Using a scanning tunneling microscope we have measured the quantum conductance through a PTCDA molecule for different configurations of the tip-molecule-surface junction. A peculiar conductance resonance arises at the Fermi level for certain tip to surface distances. We have relaxed the molecular junction coordinates and calculated transport by means of the Landauer/Keldysh approach. The zero bias transmission calculated for fixed tip positions in lateral dimensions but different tip substrate distances show a clear shift and sharpening of the molecular chemisorption level on increasing the STM-surface distance, in agreement with experiment.
Origin of Mott insulating behavior and superconductivity in twisted bilayer graphene<|sep|>A remarkable recent experiment has observed Mott insulator and proximate superconductor phases in twisted bilayer graphene when electrons partly fill a nearly flat mini-band that arises a `magic' twist angle. However, the nature of the Mott insulator, origin of superconductivity and an effective low energy model remain to be determined. We propose a Mott insulator with intervalley coherence that spontaneously breaks U(1) valley symmetry, and describe a mechanism that selects this order over the competing magnetically ordered states favored by the Hunds coupling. We also identify symmetry related features of the nearly flat band that are key to understanding the strong correlation physics and constrain any tight binding description. First, although the charge density is concentrated on the triangular lattice sites of the moir$\text{\'e }$ pattern, the Wannier states of the tight-binding model must be centered on different sites which form a honeycomb lattice. Next, spatially localizing electrons derived from the nearly flat band necessarily breaks valley and other symmetries within any mean-field treatment, which is suggestive of a valley-ordered Mott state, and also dictates that additional symmetry breaking is present to remove symmetry-enforced band contacts. Tight-binding models describing the nearly flat mini-band are derived, which highlight the importance of further neighbor hopping and interactions. We discuss consequences of this picture for superconducting states obtained on doping the valley ordered Mott insulator. We show how important features of the experimental phenomenology may be explained and suggest a number of further experiments for the future. We also describe a model for correlated states in trilayer graphene heterostructures and contrast it with the bilayer case.
Non-degenerate metrics, hypersurface deformation algebra, non-anomalous representations and density weights in quantum gravity<|sep|>A basic assumption in classical GR is that the metric field is nowhere degenerate in spacetime. In particular the induced metric on Cauchy surfaces must be nowhere degenerate. It is only under this assumption that one can derive the hypersurace deformation algebra between the initial value constraints which is absolutely transparent from the fact that the {\it inverse} of the induced metric is needed to close the algebra. This statement is independent of the density weight that one may want to equip the spatial metric with. Accordingly, the very definition of a non-anomalous representation of the hypersurface defomation algebra in quantum gravity has to address the issue of non-degenracy of the induced metric that is needed in the classical theory. In the Hilbert space representation employed in Loop Quantum Gravity (LQG) most emphasis has been layed to define an inverse metric operator on the dense domain of spin network states although they represent induced quantum geometries which are degenerate almost everywhere. It is no surprise that demonstration of closure of the constraint algebra on this domain meets difficulties because it is a sector of the quantum theory which is classically forbidden and which lies outside the domain of definition of the classical hypersurface deformation algebra. Various suggestions for addressing the issue such as non-standard operator topologies, dual spaces (habitats) and density weights have been propposed to address this issue with respect to the quantum dynamics of LQG. In this article we summarise these developments and argue that insisting on a dense domain of non-degenerate states within the LQG representation may provide a natural resolution of the issue thereby possibly avoiding the above mentioned non-standard constructions.
Detection of bistable structures via the Conley index and applications to biological systems<|sep|>Bistability is a ubiquitous phenomenon in life sciences. In this paper, two kinds of bistable structures in dynamical systems are studied: One is two one-point attractors, another is a one-point attractor accompanied by a cycle attractor. By the Conley index theory, we prove that there exist other isolated invariant sets besides the two attractors, and also obtain the possible components and their configuration. Moreover, we find that there is always a separatrix or cycle separatrix, which separates the two attractors. Finally, the biological meanings and implications of these structures are given and discussed.
An Experimental Analysis of Graph-Distance Algorithms for Comparing API Usages<|sep|>Modern software development heavily relies on the reuse of functionalities through Application Programming Interfaces (APIs). However, client developers can have issues identifying the correct usage of a certain API, causing misuses accompanied by software crashes or usability bugs. Therefore, researchers have aimed at identifying API misuses automatically by comparing client code usages to correct API usages. Some techniques rely on certain API-specific graph-based data structures to improve the abstract representation of API usages. Such techniques need to compare graphs, for instance, by computing distance metrics based on the minimal graph edit distance or the largest common subgraphs, whose computations are known to be NP-hard problems. Fortunately, there exist many abstractions for simplifying graph distance computation. However, their applicability for comparing graph representations of API usages has not been analyzed. In this paper, we provide a comparison of different distance algorithms of API-usage graphs regarding correctness and runtime. Particularly, correctness relates to the algorithms' ability to identify similar correct API usages, but also to discriminate similar correct and false usages as well as non-similar usages. For this purpose, we systematically identified a set of eight graph-based distance algorithms and applied them on two datasets of real-world API usages and misuses. Interestingly, our results suggest that existing distance algorithms are not reliable for comparing API usage graphs. To improve on this situation, we identified and discuss the algorithms' issues, based on which we formulate hypotheses to initiate research on overcoming them.
A Deep Generative Model for Graph Layout<|sep|>Different layouts can characterize different aspects of the same graph. Finding a "good" layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.
High resolution digitization system for the CROSS experiment<|sep|>The signal digitization for CROSS, a bolometric experiment for the search of neutrinoless double beta decay at LSC - Canfranc Underground Laboratory, will be based on a custom solution comprised of an analog-to-digital board interfaced to an Altera Cyclone V FPGA module. Each analog-to-digital board hosts 12 channels that allow data digitization up to 25 ksps per channel and an effective resolution of 21 bits at the typical sample rate required by the experiment (5 ksps). The board also allows to digitally select the cut-off frequency of the anti-aliasing filter with 10 bit resolution from 24 Hz up to 2.5 kHz, as required by fast scintillating bolometers. The FPGA is responsible for the synchronization of the analog-to-digital boards and for the data transfer to the storage, using UDP protocol on a standard Ethernet interface. Each FPGA can manage the data coming from 8 boards (96 channels), allowing an excellent scalability. In this contribution we will present a complete overview of the system, and a detailed characterization of the system performance.
Translating Hierarchical Block Diagrams into Composite Predicate Transformers<|sep|>Simulink is the de facto industrial standard for designing embedded control systems. When dealing with the formal verification of Simulink models, we face the problem of translating the graphical language of Simulink, namely, hierarchical block diagrams (HBDs), into a formalism suitable for verification. In this paper, we study the translation of HBDs into the compositional refinement calculus framework for reactive systems. Specifically, we consider as target language an algebra of atomic predicate transformers to capture basic Simulink blocks (both stateless and stateful), composed in series, in parallel, and in feedback. For a given HBD, there are many possible ways to translate it into a term in this algebra, with different tradeoffs. We explore these tradeoffs, and present three translation algorithms. We report on a prototype implementation of these algorithms in a tool that translates Simulink models into algebra terms implemented in the Isabelle theorem prover. We test our tool on several case studies including a benchmark Simulink model by Toyota. We compare the three translation algorithms, with respect to size and readability of generated terms, simplifiability of the corresponding formulas, and other metrics.
On computations with Double Schubert Automaton and stable maps of Multivariate Cryptography<|sep|>The families of bijective transformations $G_n$ of affine space $K^n$ over general commutative ring $K$ of increasing order with the property of stability will be constructed. Stability means that maximal degree of elements of cyclic subgroup generated by the transformation of degree $d$ is bounded by $d$. In the case $K=F_q$ these transformations of $K^n$ can be of an exponential order. We introduce large groups formed by quadratic transformations and numerical encryption algorithm protected by secure protocol of Noncommutative Cryptography. The construction of transformations is presented in terms of walks on Double Schubert Graphs.
CITlab ARGUS for Arabic Handwriting<|sep|>In the recent years it turned out that multidimensional recurrent neural networks (MDRNN) perform very well for offline handwriting recognition tasks like the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and dictionary lookup, our ARGUS software completed this task with an error rate of 26.27% in its primary setup.
Optimistic and Topological Value Iteration for Simple Stochastic Games<|sep|>While value iteration (VI) is a standard solution approach to simple stochastic games (SSGs), it suffered from the lack of a stopping criterion. Recently, several solutions have appeared, among them also "optimistic" VI (OVI). However, OVI is applicable only to one-player SSGs with no end components. We lift these two assumptions, making it available to general SSGs. Further, we utilize the idea in the context of topological VI, where we provide an efficient precise solution. In order to compare the new algorithms with the state of the art, we use not only the standard benchmarks, but we also design a random generator of SSGs, which can be biased towards various types of models, aiding in understanding the advantages of different algorithms on SSGs.
Banded operational matrices for Bernstein polynomials and application to the fractional advection-dispersion equation<|sep|>In the papers dealing with derivation and applications of operational matrices of Bernstein polynomials, a basis transformation, commonly a transformation to power basis, is used. The main disadvantage of this method is that the transformation may be ill-conditioned. Moreover, when applied to the numerical simulation of a functional differential equation, it leads to dense operational matrices and so a dense coefficient matrix is obtained. In this paper, we present a new property for Bernstein polynomials. Using this property, we build exact banded operational matrices for derivatives of Bernstein polynomials. Next, as an application, we propose a new numerical method based on a Petrov-Galerkin variational formulation and the new operational matrices utilizing the dual Bernstein basis for the time-fractional advection-dispersion equation. Finally, we show that the proposed method leads to a narrow-banded linear system and so less computational effort is required to obtain the desired accuracy for the approximate solution. Some numerical examples are provided to demonstrate the efficiency of the method.
Josephson junctions in double nanowires bridged by in-situ deposited superconductors<|sep|>We characterize parallel double quantum dot Josephson junctions based on closely-grown double nanowires bridged by in-situ deposited superconductors. The parallel double dot behavior occurs despite the closeness of the nanowires and the potential risk of nanowire clamping during growth. By tuning the charge filling and lead couplings, we map out the simplest parallel double quantum dot Yu-Shiba-Rusinov phase diagram. Our quasi-independent two-wire hybrids show promise for the realization of exotic topological phases.
Modeling the Saturation of the Bell Instability Using Hybrid Simulations<|sep|>The nonresonant cosmic ray instability, predicted by Bell (2004), is thought to play an important role in the acceleration and confinement of cosmic rays (CRs) close to supernova remnants. Despite its importance, the exact mechanism responsible for the saturation of the instability has not been determined, and there is no first-principle prediction for the amplitude of the saturated magnetic field. Using a survey of self-consistent kinetic hybrid simulations (with kinetic ions and fluid electrons), we study the saturation of the non-resonant streaming instability as a function of the parameters of both the thermal background plasma and the CR population. The strength of the saturated magnetic field has important implications for both CR acceleration in supernova remnants and CR diffusion in the Galaxy.
Informative Causality Extraction from Medical Literature via Dependency-tree based Patterns<|sep|>Extracting cause-effect entities from medical literature is an important task in medical information retrieval. A solution for solving this task can be used for compilation of various causality relations, such as, causality between disease and symptoms, between medications and side effects, between genes and diseases, etc. Existing solutions for extracting cause-effect entities work well for sentences where the cause and the effect phrases are name entities, single-word nouns, or noun phrases consisting of two to three words. Unfortunately, in medical literature, cause and effect phrases in a sentence are not simply nouns or noun phrases, rather they are complex phrases consisting of several words, and existing methods fail to correctly extract the cause and effect entities in such sentences. Partial extraction of cause and effect entities conveys poor quality, non informative, and often, contradictory facts, comparing to the one intended in the given sentence. In this work, we solve this problem by designing an unsupervised method for cause and effect phrase extraction, PatternCausality, which is specifically suitable for the medical literature. Our proposed approach first uses a collection of cause-effect dependency patterns as template to extract head words of cause and effect phrases and then it uses a novel phrase extraction method to obtain complete and meaningful cause and effect phrases from a sentence. Experiments on a cause-effect dataset built from sentences from PubMed articles show that for extracting cause and effect entities, PatternCausality is substantially better than the existing methods with an order of magnitude improvement in the F-score metric over the best of the existing methods.
Golden interpolation<|sep|>For the classic aesthetic interpolation problem, we propose an entirely new thought: apply the golden section. For how to apply the golden section to interpolation methods, we present three examples: the golden step interpolation, the golden piecewise linear interpolation and the golden curve interpolation, which respectively deal with the applications of golden section in the interpolation of degree 0, 1, and 2 in the plane. In each example, we present our basic ideas, the specific methods, comparative examples and applications, and relevant criteria. And it is worth mentioning that for aesthetics, we propose two novel concepts: the golden cuspidal hill and the golden domed hill. This paper aims to provide the reference for the combination of golden section and interpolation, and stimulate more and better related researches.
Understanding Incentivized Mobile App Installs on Google Play Store<|sep|>"Incentivized" advertising platforms allow mobile app developers to acquire new users by directly paying users to install and engage with mobile apps (e.g., create an account, make in-app purchases). Incentivized installs are banned by the Apple App Store and discouraged by the Google Play Store because they can manipulate app store metrics (e.g., install counts, appearance in top charts). Yet, many organizations still offer incentivized install services for Android apps. In this paper, we present the first study to understand the ecosystem of incentivized mobile app install campaigns in Android and its broader ramifications through a series of measurements. We identify incentivized install campaigns that require users to install an app and perform in-app tasks targeting manipulation of a wide variety of user engagement metrics (e.g., daily active users, user session lengths) and revenue. Our results suggest that these artificially inflated metrics can be effective in improving app store metrics as well as helping mobile app developers to attract funding from venture capitalists. Our study also indicates lax enforcement of the Google Play Store's existing policies to prevent these behaviors. It further motivates the need for stricter policing of incentivized install campaigns. Our proposed measurements can also be leveraged by the Google Play Store to identify potential policy violations.
Nonlinear Analysis of an Improved Swing Equation<|sep|>In this paper, we investigate the properties of an improved swing equation model for synchronous generators. This model is derived by omitting the main simplifying assumption of the conventional swing equation, and requires a novel analysis for the stability and frequency regulation. We consider two scenarios. First we study the case that a synchronous generator is connected to a constant load. Second, we inspect the case of the single machine connected to an infinite bus. Simulations verify the results.
Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting<|sep|>In this paper, we explore and evaluate the use of ranking-based objective functions for learning simultaneously a word string and a word image encoder. We consider retrieval frameworks in which the user expects a retrieval list ranked according to a defined relevance score. In the context of a word spotting problem, the relevance score has been set according to the string edit distance from the query string. We experimentally demonstrate the competitive performance of the proposed model on query-by-string word spotting for both, handwritten and real scene word images. We also provide the results for query-by-example word spotting, although it is not the main focus of this work.
Correlations and structure of interfaces in the Ising model. Theory and numerics<|sep|>We consider phase separation on the strip for the two-dimensional Ising model in the near-critical region. Within the framework of field theory, we find exact analytic results for certain two- and three-point correlation functions of the order parameter field. The analytic results for order parameter correlations, energy density profile, subleading corrections and passage probability density of the interface are confirmed by accurate Monte Carlo simulations we performed.
Network Optimization for Unified Packet and Circuit Switched Networks<|sep|>Internet traffic continues to grow relentlessly, driven largely by increasingly high resolution video content. Although studies have shown that the majority of packets processed by Internet routers are pass-through traffic, they nonetheless have to be queued and routed at every hop in current networks, which unnecessarily adds substantial delays and processing costs. Such pass-through traffic can be better circuit-switched through the underlying optical transport network by means of pre-established circuits, which is possible in a unified packet and circuit switched network. In this paper, we propose a novel convex optimization framework based on a new destination-based multicommodity flow formulation for the allocation of circuits in such unified networks. In particular, we consider two deployment settings, one based on real-time traffic monitoring, and the other relying upon history-based traffic predictions. In both cases, we formulate global network optimization objectives as concave functions that capture the fair sharing of network capacity among competing traffic flows. The convexity of our problem formulations ensures globally optimal solutions.
Dialogue manager domain adaptation using Gaussian process reinforcement learning<|sep|>Spoken dialogue systems allow humans to interact with machines using natural speech. As such, they have many benefits. By using speech as the primary communication medium, a computer interface can facilitate swift, human-like acquisition of information. In recent years, speech interfaces have become ever more popular, as is evident from the rise of personal assistants such as Siri, Google Now, Cortana and Amazon Alexa. Recently, data-driven machine learning methods have been applied to dialogue modelling and the results achieved for limited-domain applications are comparable to or outperform traditional approaches. Methods based on Gaussian processes are particularly effective as they enable good models to be estimated from limited training data. Furthermore, they provide an explicit estimate of the uncertainty which is particularly useful for reinforcement learning. This article explores the additional steps that are necessary to extend these methods to model multiple dialogue domains. We show that Gaussian process reinforcement learning is an elegant framework that naturally supports a range of methods, including prior knowledge, Bayesian committee machines and multi-agent learning, for facilitating extensible and adaptable dialogue systems.
Magnetic excitations in the normal and nematic phases of iron pnictides<|sep|>In this paper, we study theoretically the behaviors of the magnetic excitations(MEs) in the normal and nematic phases of iron pnictides. The normal state MEs exhibit commensurability to diamond and to square-like structure transition with the increase of energy. This structure transition persists in the spin and orbital scenarios of nematic phases, although the MEs show anisotropic behaviors due to the C4 symmetry breaking induced by the nematic orders. The MEs exhibit distinct energy evolution behaviors between the spin and orbital scenarios of nematicity. For the spinnematic scenario, the anisotropy of the MEs persists up to the high energy region. In contrast, for the orbital-nematic scenario, it reduces dramatically in the low energy region and is negligible in the high energy region. These distinct behaviors of the MEs are attributed to the different origins between the spin and orbital scenarios of nematic orders.
Thermal Conductivity of 1,2-Ethanediol and 1,2-Propanediol Binary Aqueous Solutions at Temperature from 253 K to 373 K<|sep|>1,2-Ethanediol, 1,2-propanediol and their aqueous solutions are widely used as heat transfer fluids. Their thermal conductivity is a vital physical property, yet there are only few reports in literature. In this paper, thermal conductivity of binary aqueous solutions of the two glycols was measured using the transient hot wire method at temperature from 253.15 K to 373.15 K at atmospheric pressure. Measurement was made for six compositions over the entire concentration range from 0 to 1 mole fraction of glycol, namely, 0.0, 0.2, 0.4, 0.6, 0.8, and 1.0 mole fraction of glycol. The uncertainties of temperature and concentration measurement were estimated to be 0.01 K and 0.1 %, respectively. The combined expanded uncertainty of thermal conductivity with a level of confidence of 0.95 (k = 2) was 2 %. The second-order Scheff\'e polynomial was used to correlate the temperature and composition dependence of the experimental thermal conductivity, which was found to be in good agreement with the experiment data from the present work and other reports.
Stable Magnetic Fields in Static Stars<|sep|>We prove that static fluid stars can stably support magnetic fields (within the ideal MHD approximation).
Arabic Language Learning Assisted by Computer, based on Automatic Speech Recognition<|sep|>This work consists of creating a system of the Computer Assisted Language Learning (CALL) based on a system of Automatic Speech Recognition (ASR) for the Arabic language using the tool CMU Sphinx3 [1], based on the approach of HMM. To this work, we have constructed a corpus of six hours of speech recordings with a number of nine speakers. we find in the robustness to noise a grounds for the choice of the HMM approach [2]. the results achieved are encouraging since our corpus is made by only nine speakers, but they are always reasons that open the door for other improvement works.
Residual nuclide formation in 206,207,208,nat-Pb and 209-Bi induced by 0.04-2.6 GeV Protons as well as in 56-Fe induced by 0.3-2.6 GeV Protons<|sep|>5972 independent and cumulative yields of radioactive residuals nuclei have been measured in 55 thin 206,207,208,nat-Pb and 209-Bi targets irradiated by 0.04, 0.07, 0.10, 0.15, 0.25, 0.6, 0.8, 1.2, 1.4, 1.6, and 2.6 GeV protons. Besides, 219 yields have been measured in 0.3, 0.5, 0.75, 1.0, 1.5, and 2.6 GeV proton-irradiated 56-Fe target. The protons were extracted from the ITEP U-10 synchrotron. The measured data are compared with experimental results obtained elsewhere and with theoretical calculations by LAHET, MCNPX, CEM03, LAQGSM03, CASCADE, CASCADO, and LAHETO codes. The predictive power was found to be different for each of the codes tested, but was satisfactory on the whole in the case of spallation products. At the same time, none of the codes can de-scribe well the product yields throughout the whole product mass range, and all codes must be further improved.
Massive and modified gravity as self-gravitating media<|sep|>We study the effective field theory that describes the low-energy physics of self-gravitating media. The field content consists of four derivatively coupled scalar fields that can be identified with the internal comoving coordinates of the medium. Imposing SO(3) internal spatial invariance, the theory describes supersolids. Stronger symmetry requirements lead to superfluids, solids and perfect fluids, at lowest order in derivatives. In the unitary gauge, massive gravity emerges, being thus the result of a continuous medium propagating in spacetime. Our results can be used to explore systematically the effects and signatures of modifying gravity consistently at large distances. The dark sector is then described as a self-gravitating medium with dynamical and thermodynamic properties dictated by internal symmetries. These results indicate that the divide between dark energy and modified gravity, at large distance scales, is simply a gauge choice.
Vortical versus skyrmionic states in mesoscopic \emph{p}-wave superconductors<|sep|>We investigate the superconducting states that arise as a consequence of mesoscopic confinement and a multi-component order parameter in the Ginzburg-Landau model for $p\,$-wave superconductivity. Conventional vortices, but also half-quantum vortices and skyrmions are found as the applied magnetic field and the anisotropy parameters of the Fermi surface are varied. The solutions are well differentiated by a topological charge that for skyrmions is given by the Hopf invariant and for vortices by the circulation of the superconducting velocity. We revealed several unique states combining vortices and skyrmions, their possible reconfiguration with varied magnetic field, as well as the novel temporal and field-induced transitions between vortical and skyrmionic states.
Cross-view Action Modeling, Learning and Recognition<|sep|>Existing methods on video-based action recognition are generally view-dependent, i.e., performing recognition from the same views seen in the training data. We present a novel multiview spatio-temporal AND-OR graph (MST-AOG) representation for cross-view action recognition, i.e., the recognition is performed on the video from an unknown and unseen view. As a compositional model, MST-AOG compactly represents the hierarchical combinatorial structures of cross-view actions by explicitly modeling the geometry, appearance and motion variations. This paper proposes effective methods to learn the structure and parameters of MST-AOG. The inference based on MST-AOG enables action recognition from novel views. The training of MST-AOG takes advantage of the 3D human skeleton data obtained from Kinect cameras to avoid annotating enormous multi-view video frames, which is error-prone and time-consuming, but the recognition does not need 3D information and is based on 2D video input. A new Multiview Action3D dataset has been created and will be released. Extensive experiments have demonstrated that this new action representation significantly improves the accuracy and robustness for cross-view action recognition on 2D videos.
Bond chaos in the Sherrington-Kirkpatrick model<|sep|>We calculate the probability distribution of the overlap between a spin glass and a copy of itself in which the bonds are randomly perturbed in varying degrees. The overlap distribution is shown to go to a delta distribution in the thermodynamic limit for arbitrarily small perturbations (bond chaos) and we obtain the scaling behaviour of the distribution with system size N in the high and low temperature phases and exactly at the critical temperature. The results are relevant for the free energy fluctuations in the Sherrington-Kirkpatrick model.
Reply to Comment arXiv:0704.3529v1<|sep|>In this reply, we show that the author of the Comment arXiv:0704.3529v1 inadvertently provides additional arguments against the use of Hardy functions as test functions for the Gamow states.
The cosmological information of shear peaks: beyond the abundance<|sep|>We study the cosmological information of weak lensing (WL) peaks, focusing on two other statistics besides their abundance: the stacked tangential-shear profiles and the peak-peak correlation function. We use a large ensemble of simulated WL maps with survey specifications relevant to future missions like Euclid and LSST, to explore the three peak probes. We find that the correlation function of peaks with high signal-to-noise (S/N) measured from fields of size 144 sq. deg. has a maximum of ~0.3 at an angular scale ~10 arcmin. For peaks with smaller S/N, the amplitude of the correlation function decreases, and its maximum occurs on smaller angular scales. We compare the peak observables measured with and without shape noise and find that for S/N~3 only ~5% of the peaks are due to large-scale structures, the rest being generated by shape noise. The covariance matrix of the probes is examined: the correlation function is only weakly covariant on scales < 30 arcmin, and slightly more on larger scales; the shear profiles are very correlated for theta > 2 arcmin, with a correlation coefficient as high as 0.7. Using the Fisher-matrix formalism, we compute the cosmological constraints for {Om_m, sig_8, w, n_s} considering each probe separately, as well as in combination. We find that the correlation function of peaks and shear profiles yield marginalized errors which are larger by a factor of 2-4 for {Om_m, sig_8} than the errors yielded by the peak abundance alone, while the errors for {w, n_s} are similar. By combining the three probes, the marginalized constraints are tightened by a factor of ~2 compared to the peak abundance alone, the least contributor to the error reduction being the correlation function. This work therefore recommends that future WL surveys use shear peaks beyond their abundance in order to constrain the cosmological model.
On possible existence of HOMFLY polynomials for virtual knots<|sep|>Virtual knots are associated with knot diagrams, which are not obligatory planar. The recently suggested generalization from N=2 to arbitrary N of the Kauffman-Khovanov calculus of cycles in resolved diagrams can be straightforwardly applied to non-planar case. In simple examples we demonstrate that this construction preserves topological invariance -- thus implying the existence of HOMFLY extension of cabled Jones polynomials for virtual knots and links.
Direct evidence of the gradient drift instability being the origin of a rotating spoke in a crossed field plasma<|sep|>A plasma rotating spoke in a crossed field discharge is studied using 2D radial-azimuthal fully kinetic Particle-In-Cell Monte Carlo Collision (PIC/MCC) simulations. The kinetic model reveals the whole perturbation spectrum of the gradient drift instability in the linear stage: Simon-Hoh, lower-hybrid and ion sound modes, providing direct evidence of the spoke of the gradient drift instability nature. The two-fluid dispersion relation of the gradient drift instability was utilized to analyze the linear development of instabilities in the simulations. The charge separation effect was incorporated in the fluid linear theory and a super-resolution signal processing method (multiple signal classification) was applied to obtain the numerical frequency spectrum. The simulated spectrum and growth rate show excellent agreement with the theoretical dispersion relation (real frequency and imaginary frequency) over investigated cases. The most linearly unstable mode was found to be the lower hybrid instability and the mode transition into the m=1 macroscopic rotating structure after saturation of the linear phase is accompanied by an inverse energy cascade. In the nonlinear stage, the pronounced spoke phenomena can occur when the heating of $\mathbf{E_{\theta}\times B}$ electron flow channeled in the spoke front passage suffices to provide the enhanced ionization.
A Trichotomy in the Complexity of Counting Answers to Conjunctive Queries<|sep|>Conjunctive queries are basic and heavily studied database queries; in relational algebra, they are the select-project-join queries. In this article, we study the fundamental problem of counting, given a conjunctive query and a relational database, the number of answers to the query on the database. In particular, we study the complexity of this problem relative to sets of conjunctive queries. We present a trichotomy theorem, which shows essentially that this problem on a set of conjunctive queries is either tractable, equivalent to the parameterized CLIQUE problem, or as hard as the parameterized counting CLIQUE problem; the criteria describing which of these situations occurs is simply stated, in terms of graph-theoretic conditions.
On the Global Regularity of a Helical-decimated Version of the 3D Navier-Stokes Equations<|sep|>We study the global regularity, for all time and all initial data in $H^{1/2}$, of a recently introduced decimated version of the incompressible 3D Navier-Stokes (dNS) equations. The model is based on a projection of the dynamical evolution of Navier-Stokes (NS) equations into the subspace where helicity (the $L^2-$scalar product of velocity and vorticity) is sign-definite. The presence of a second (beside energy) sign-definite inviscid conserved quadratic quantity, which is equivalent to the $H^{1/2}-$Sobolev norm, allows us to demonstrate global existence and uniqueness, of space-periodic solutions, together with continuity with respect to the initial conditions, for this decimated 3D model. This is achieved thanks to the establishment of two new estimates, for this 3D model, which show that the $H^{1/2}$ and the time average of the square of the $H^{3/2}$ norms of the velocity field remain finite. Such two additional bounds are known, in the spirit of the work of H. Fujita and T. Kato \cite{kato1,kato2}, to be sufficient for showing well-posedness for the 3D NS equations. Furthermore, they are directly linked to the helicity evolution for the dNS model, and therefore with a clear physical meaning and consequences.
Efficient Classification with Counterfactual Reasoning and Active Learning<|sep|>Data augmentation is one of the most successful techniques to improve the classification accuracy of machine learning models in computer vision. However, applying data augmentation to tabular data is a challenging problem since it is hard to generate synthetic samples with labels. In this paper, we propose an efficient classifier with a novel data augmentation technique for tabular data. Our method called CCRAL combines causal reasoning to learn counterfactual samples for the original training samples and active learning to select useful counterfactual samples based on a region of uncertainty. By doing this, our method can maximize our model's generalization on the unseen testing data. We validate our method analytically, and compare with the standard baselines. Our experimental results highlight that CCRAL achieves significantly better performance than those of the baselines across several real-world tabular datasets in terms of accuracy and AUC. Data and source code are available at: https://github.com/nphdang/CCRAL.
Investigation of tracer diffusion in crowded cylindrical channel<|sep|>Based on a coarse-grained model, we carry out molecular dynamics simulations to analyze the diffusion of a small tracer particle inside a cylindrical channel whose inner wall is covered with randomly grafted short polymeric chains. We observe an interesting transient subdiffusive behavior along the cylindrical axis at high attraction between the tracer and the chains, however, the long time diffusion is always normal. This process is found to be enhanced for the case that we immobilize the grafted chains, i.e. the sub-diffusive behavior sets in at an earlier time and spans over a longer time period before becoming diffusive. Even if the grafted chains are replaced with a frozen sea of repulsive, non-connected particles in the background, the transient subdiffusion is observed. The intermediate subdiffusive behavior only disappears when the grafted chains are replaced with a mobile background sea of mutually repulsive particles. Overall, the long time diffusion coefficient of the tracer along the cylinder axis decreases with the increase in system volume fraction, strength of attraction between the tracer and the background and also on freezing the background. We believe that the simple model presented here could be useful for a qualitative understanding of the process of macromolecular diffusion inside the nuclear pore complex.
Layer-Wise Data-Free CNN Compression<|sep|>We present a computationally efficient method for compressing a trained neural network without using real data. We break the problem of data-free network compression into independent layer-wise compressions. We show how to efficiently generate layer-wise training data using only a pretrained network. We use this data to perform independent layer-wise compressions on the pretrained network. We also show how to precondition the network to improve the accuracy of our layer-wise compression method. We present results for layer-wise compression using quantization and pruning. When quantizing, we compress with higher accuracy than related works while using orders of magnitude less compute. When compressing MobileNetV2 and evaluating on ImageNet, our method outperforms existing methods for quantization at all bit-widths, achieving a $+0.34\%$ improvement in $8$-bit quantization, and a stronger improvement at lower bit-widths (up to a $+28.50\%$ improvement at $5$ bits). When pruning, we outperform baselines of a similar compute envelope, achieving $1.5$ times the sparsity rate at the same accuracy. We also show how to combine our efficient method with high-compute generative methods to improve upon their results.
$f(\bar{R}, L(X))$-gravity in the context of dark energy with power law expansion and energy conditions<|sep|>The motto of this work is to generate a general formalism of $f(\bar{R}, L(X))-$gravity in the context of dark energy under the framework of the {\bf K-}essence emergent geometry with the Dirac-Born-Infeld (DBI) variety of action, where $\bar{R}$ is the familiar Ricci scalar, $L(X)$ is the DBI type non-canonical Lagrangian with $X={1\over 2}g^{\mu\nu}\nabla_{\mu}\phi\nabla_{\nu}\phi$ and $\phi$ is the scalar field of the {\bf K-}essence geometry. The emergent gravity metric $\bar{G}_{\mu\nu}$ and the well-known gravitational metric $g_{\mu\nu}$ are not conformally equivalent. We have constructed a modified field equation using the metric formalism in $f(\bar{R}, L(X))$-gravity incorporating the corresponding Friedmann equations in the framework of the background gravitational metric which is of Friedmann-Lema{\^i}tre-Robertson-Walker (FLRW) type. The solution of modified Friedmann equations have been deduced for the specific choice of $f(\bar{R}, L(X))$, which is of Starobinsky type, using the power law expansion method. The consistency of the model with the accelerating phase of the Universe has been shown, when we restrict ourselves to consider the value of the dark energy density, as $\dot\phi^{2}=\frac{8}{9}=0.888 <1$, which indicates that the present Universe is dark energy dominated. Graphical plots for the energy density ($\rho$), pressure ($p$) and equation of state parameter ($\omega$) w.r.t. time ($t$) based on parametric values are interestingly consistent with the dark energy domination and hence accelerating features. We also put some light on the corresponding energy conditions and constraints of the $f(\bar{R}, L(X))$ theory with one basic example.
TeV Scale Leptogenesis<|sep|>This is a mini-review on the mechanism of leptogenesis, with a special emphasis on low-scale leptogenesis models which are testable in foreseeable laboratory experiments at Energy and Intensity frontiers. We also stress the importance of flavor effects in the calculation of the lepton asymmetry and the necessity of a flavor-covariant framework to consistently capture these effects.
World Literature According to Wikipedia: Introduction to a DBpedia-Based Framework<|sep|>Among the manifold takes on world literature, it is our goal to contribute to the discussion from a digital point of view by analyzing the representation of world literature in Wikipedia with its millions of articles in hundreds of languages. As a preliminary, we introduce and compare three different approaches to identify writers on Wikipedia using data from DBpedia, a community project with the goal of extracting and providing structured information from Wikipedia. Equipped with our basic set of writers, we analyze how they are represented throughout the 15 biggest Wikipedia language versions. We combine intrinsic measures (mostly examining the connectedness of articles) with extrinsic ones (analyzing how often articles are frequented by readers) and develop methods to evaluate our results. The better part of our findings seems to convey a rather conservative, old-fashioned version of world literature, but a version derived from reproducible facts revealing an implicit literary canon based on the editing and reading behavior of millions of people. While still having to solve some known issues, the introduced methods will help us build an observatory of world literature to further investigate its representativeness and biases.
Semantic Matching by Weakly Supervised 2D Point Set Registration<|sep|>In this paper we address the problem of establishing correspondences between different instances of the same object. The problem is posed as finding the geometric transformation that aligns a given image pair. We use a convolutional neural network (CNN) to directly regress the parameters of the transformation model. The alignment problem is defined in the setting where an unordered set of semantic key-points per image are available, but, without the correspondence information. To this end we propose a novel loss function based on cyclic consistency that solves this 2D point set registration problem by inferring the optimal geometric transformation model parameters. We train and test our approach on a standard benchmark dataset Proposal-Flow (PF-PASCAL)\cite{proposal_flow}. The proposed approach achieves state-of-the-art results demonstrating the effectiveness of the method. In addition, we show our approach further benefits from additional training samples in PF-PASCAL generated by using category level information.
A Game-Theoretic Account of Responsibility Allocation<|sep|>When designing or analyzing multi-agent systems, a fundamental problem is responsibility ascription: to specify which agents are responsible for the joint outcome of their behaviors and to which extent. We model strategic multi-agent interaction as an extensive form game of imperfect information and define notions of forward (prospective) and backward (retrospective) responsibility. Forward responsibility identifies the responsibility of a group of agents for an outcome along all possible plays, whereas backward responsibility identifies the responsibility along a given play. We further distinguish between strategic and causal backward responsibility, where the former captures the epistemic knowledge of players along a play, while the latter formalizes which players -- possibly unknowingly -- caused the outcome. A formal connection between forward and backward notions is established in the case of perfect recall. We further ascribe quantitative responsibility through cooperative game theory. We show through a number of examples that our approach encompasses several prior formal accounts of responsibility attribution.
R-CNN minus R<|sep|>Deep convolutional neural networks (CNNs) have had a major impact in most areas of image understanding, including object category detection. In object detection, methods such as R-CNN have obtained excellent results by integrating CNNs with region proposal generation algorithms such as selective search. In this paper, we investigate the role of proposal generation in CNN-based detectors in order to determine whether it is a necessary modelling component, carrying essential geometric information not contained in the CNN, or whether it is merely a way of accelerating detection. We do so by designing and evaluating a detector that uses a trivial region generation scheme, constant for each image. Combined with SPP, this results in an excellent and fast detector that does not require to process an image with algorithms other than the CNN itself. We also streamline and simplify the training of CNN-based detectors by integrating several learning steps in a single algorithm, as well as by proposing a number of improvements that accelerate detection.
Statistical Aspects of the Quantum Supremacy Demonstration<|sep|>The notable claim of quantum supremacy presented by Google's team in 2019 consists of demonstrating the ability of a quantum circuit to generate, albeit with considerable noise, bitstrings from a distribution that is considered hard to simulate on classical computers. Verifying that the generated data is indeed from the claimed distribution and assessing the circuit's noise level and its fidelity is a purely statistical undertaking. The objective of this paper is to explain the relations between quantum computing and some of the statistical aspects involved in demonstrating quantum supremacy in terms that are accessible to statisticians, computer scientists, and mathematicians. Starting with the statistical analysis in Google's demonstration, which we explain, we study various estimators of the fidelity, and different approaches to testing the distributions generated by the quantum computer. We propose different noise models, and discuss their implications. A preliminary study of the Google data, focusing mostly on circuits of 12 and 14 qubits is discussed throughout the paper.
Mott insulator phases and first-order melting in BSCCO crystals with periodic surface holes<|sep|>We measured the effects of periodic surface holes, created using a focused ion beam, on the phase diagram of the vortex matter in high-T_c Bi_2Sr_2CaCu_2O_{8+\delta} crystals. Differential magneto-optical measurements show that the irreversibility line is shifted to higher fields and temperatures, with respect to the pristine melting line. The irreversibility line displays weak field dependence between integer matching fields indicating multiple-flux-quanta pinning at holes. We find reduced equilibrium compressibility of the vortex matter at integer matching fields, which is strong evidence for the existence of thermodynamic Mott insulator phases. Shaking with a transverse ac field surprisingly reveals first-order melting that is not shifted with respect to the pristine melting line and that seems to occur within the Mott insulator regions. This melting is understood to be the first-order transition in the bulk of the crystal beneath the surface holes. The transition is visible at the surface, despite the reduced vortex compressibility in the top layer.
Intermediate models for longitudinal profiles of cosmic showers<|sep|>Cosmic rays impacting on the atmosphere cause particle-showers. Several descriptions exist for the evolution of the shower size along the atmospheric depth. The well known functions for shower profiles, Greisen, Gaisser-Hillas and `Gaussian in Age', are intimately connected in that they all are approximate solutions of versions of the Rossi and Greisen diffusion equations. The mathematical connection will be demonstrated by means of two simple models for the longitudinal electromagnetic shower profile. Both models can be regarded either as a generalization of the Heitler model or as a simplification of the diffusion model of Rossi and Greisen. These models are far closer to reality than the Heitler model, while they are not as close to reality as the model of Rossi and Greisen. Therefore, they will be referred to as intermediate models. For each intermediate model the evolution of the shower is governed by either a single differential equation or a single integro-differential equation. The approximate solution of the differential equation is a Gaisser-Hillas function and can be adjusted such that it almost matches the Greisen profile. The approximate solution of the integro-differential equation is a `Gaussian in Age' function. The corresponding profile is, after suitable adjustment, in excellent agreement with the Greisen profile. The analysis also leads to an alternative functional form for the age parameter.
Quantum chicken-egg dilemmas: Delayed-choice causal order and the reality of causal non-separability<|sep|>Recent frameworks describing quantum mechanics in the absence of a global causal order admit the existence of causally indefinite processes, where it is impossible to ascribe causal order for events A and B. These frameworks even allow for processes that violate the so-called causal inequalities, which are analogous to Bell's inequalities. However, the physicality of these exotic processes is, in the general case, still under debate, bringing into question their foundational relevance. While it is known that causally indefinite processes can be probabilistically realised by means of a quantum circuit, along with an additional conditioning event C, concrete insights into the ontological meaning of such implementation schemes have heretofore been limited. Here, we show that causally indefinite processes can be realised with schemes where C serves only as a classical flag heralding which causally indefinite process was realised. We then show that there are processes where any pure conditioning measurement of C leads to a causally indefinite process for A and B, thus establishing causal indefiniteness as a basis-independent quantity. Finally, we demonstrate that quantum mechanics allows for phenomena where C can deterministically decide whether A comes before B or vice versa, without signalling to either. This is akin to Wheeler's famous delayed-choice experiment establishing definite causal order in quantum mechanics as instrument-\textit{dependent} property.
Exploring Practitioner Perspectives of Sourcing Risks: Towards the Development of an Integrated Risk and Control Framework<|sep|>Outsourcing of information and communication technologies (ICT) and related services is an established and growing industry. Recent trends, such as the move toward multi-sourcing have increased the complexity and risk of these outsourcing arrangements. There is a critical research need to identify the risks faced by both the organisations that outsource ICT and the vendors that provide it in this changing landscape. To address growing concerns regarding the best way to deal with risk and control in this environment, our research focuses on establishing a Sourcing Risk and Control Framework to assist organisations identify these risks and develop effective mitigation strategies. In this paper we report on the first stage of our research that sought to document how sourcing risk is represented and considered in practice. To date, limited empirical research has been conducted in an Australian context. Using a series of workshops involving client and vendor representatives, we identified a broad range of risks and developed a cohesive categorisation scheme that incorporates functional and multi-stakeholder perspectives.
Towards a New Interpretation of Separable Convolutions<|sep|>In recent times, the use of separable convolutions in deep convolutional neural network architectures has been explored. Several researchers, most notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in their deep architectures and have demonstrated state of the art or close to state of the art performance. However, the underlying mechanism of action of separable convolutions are still not fully understood. Although their mathematical definition is well understood as a depthwise convolution followed by a pointwise convolution, deeper interpretations such as the extreme Inception hypothesis (Chollet, 2016) have failed to provide a thorough explanation of their efficacy. In this paper, we propose a hybrid interpretation that we believe is a better model for explaining the efficacy of separable convolutions.
Sparse Signal Estimation by Maximally Sparse Convex Optimization<|sep|>This paper addresses the problem of sparsity penalized least squares for applications in sparse signal processing, e.g. sparse deconvolution. This paper aims to induce sparsity more strongly than L1 norm regularization, while avoiding non-convex optimization. For this purpose, this paper describes the design and use of non-convex penalty functions (regularizers) constrained so as to ensure the convexity of the total cost function, F, to be minimized. The method is based on parametric penalty functions, the parameters of which are constrained to ensure convexity of F. It is shown that optimal parameters can be obtained by semidefinite programming (SDP). This maximally sparse convex (MSC) approach yields maximally non-convex sparsity-inducing penalty functions constrained such that the total cost function, F, is convex. It is demonstrated that iterative MSC (IMSC) can yield solutions substantially more sparse than the standard convex sparsity-inducing approach, i.e., L1 norm minimization.
Proton elastic scattering on calcium isotopes from chiral nuclear optical potentials<|sep|>We formulate microscopic optical potentials for nucleon-nucleus scattering from chiral two- and three-nucleon forces. The real and imaginary central terms of the optical potentials are obtained from the nucleon self energy in infinite nuclear matter at a given density and isospin asymmetry, calculated self-consistently to second order in many-body perturbation theory. The real spin-orbit term is extracted from the same chiral potential using an improved density matrix expansion. The density-dependent optical potential is then folded with the nuclear density distributions of 40Ca, 42Ca, 44Ca, and 48Ca from which we study proton-nucleus elastic scattering and total reaction cross sections using the reaction code TALYS. We compare the results of the microscopic calculations to those of phenomenological models and experimental data up to projectile energies of E = 180 MeV. While overall satisfactory agreement with the available experimental data is obtained, we find that the elastic scattering and total reaction cross sections can be significantly improved with a weaker imaginary optical potential, particularly for larger projectile energies.
Viscous {\Lambda}CDM Universe Models<|sep|>We explore flat {\Lambda}CDM models with bulk viscosity, and study the role of the bulk viscosity in the evolution of these universe models. The dynamical equations for these models are obtained and solved for some cases of bulk viscosity. We obtain differential equations for the Hubble parameter H and the energy density of dark matter {\rho}, for which we give analytical solutions for some cases and for the general case we give a numerical solution. Also we calculate the statefinder parameters for this model and display them in the s-r-plane.
Sequence Discriminative Training for Deep Learning based Acoustic Keyword Spotting<|sep|>Speech recognition is a sequence prediction problem. Besides employing various deep learning approaches for framelevel classification, sequence-level discriminative training has been proved to be indispensable to achieve the state-of-the-art performance in large vocabulary continuous speech recognition (LVCSR). However, keyword spotting (KWS), as one of the most common speech recognition tasks, almost only benefits from frame-level deep learning due to the difficulty of getting competing sequence hypotheses. The few studies on sequence discriminative training for KWS are limited for fixed vocabulary or LVCSR based methods and have not been compared to the state-of-the-art deep learning based KWS approaches. In this paper, a sequence discriminative training framework is proposed for both fixed vocabulary and unrestricted acoustic KWS. Sequence discriminative training for both sequence-level generative and discriminative models are systematically investigated. By introducing word-independent phone lattices or non-keyword blank symbols to construct competing hypotheses, feasible and efficient sequence discriminative training approaches are proposed for acoustic KWS. Experiments showed that the proposed approaches obtained consistent and significant improvement in both fixed vocabulary and unrestricted KWS tasks, compared to previous frame-level deep learning based acoustic KWS methods.
Charged-particle production as a function of multiplicity and transverse spherocity in pp collisions at $\sqrt{s}$ = 5.02 and 13 TeV<|sep|>We present a study of the inclusive charged-particle transverse momentum ($p_{\rm T}$) spectra as a function of charged-particle multiplicity density at mid-pseudorapidity, ${\rm d}N_{\rm ch}/{\rm d}\eta$, in pp collisions at $\sqrt{s}$ = 5.02 and 13 TeV covering the kinematic range $|\eta|<0.8$ and $0.15<p_{\rm{T}}<20$ GeV/$c$. The results are presented for events with at least one charged particle in $|\eta|<1$ (INEL$ >0$). The $p_{\rm T}$ spectra are reported for two multiplicity estimators covering different pseudorapidity regions. The $p_{\rm T}$ spectra normalized to that for INEL $>0$ show little energy dependence. Moreover, the high-$p_{\rm T}$ yields of charged particles increase faster than the charged-particle multiplicity density. The average $\it{p}_{\rm T}$ as a function of multiplicity and transverse spherocity is reported for pp collisions at $\sqrt{s}=13$ TeV. For low- (high-) spherocity events, corresponding to jet-like (isotropic) events, the average $p_{\rm T}$ is higher (smaller) than that measured in INEL $>0$ pp collisions. Within uncertainties, the functional form of $\langle p_{\rm T} \rangle(N_{\rm ch})$ is not affected by the spherocity selection. While EPOS LHC gives a good description of many features of data, PYTHIA overestimates the average $p_{\rm T}$ in jet-like events.
Trajectory Tracking Control with Flat Inputs and a Dynamic Compensator<|sep|>This paper proposes a tracking controller based on the concept of flat inputs and a dynamic compensator. Flat inputs represent a dual approach to flat outputs. In contrast to conventional flatness-based control design, the regulated output may be a non-flat output, or the system may be non-flat. The method is applicable to observable systems with stable internal dynamics. The performance of the new design is demonstrated on the variable-length pendulum, a non-flat nonlinear system with a singularity in the relative degree.
Training Neural Networks Using the Property of Negative Feedback to Inverse a Function<|sep|>With high forward gain, a negative feedback system has the ability to perform the inverse of a linear or non linear function that is in the feedback path. This property of negative feedback systems has been widely used in analog circuits to construct precise closed-loop functions. This paper describes how the property of a negative feedback system to perform inverse of a function can be used for training neural networks. This method does not require that the cost or activation functions be differentiable. Hence, it is able to learn a class of non-differentiable functions as well where a gradient descent-based method fails. We also show that gradient descent emerges as a special case of the proposed method. We have applied this method to the MNIST dataset and obtained results that shows the method is viable for neural network training. This method, to the best of our knowledge, is novel in machine learning.
Photometric and spectroscopic variability of the FUor star V582 Aurigae<|sep|>We carried out BVRI CCD photometric observations in the field of V582 Aur from 2009 August to 2013 February. We acquired high-, medium-, and low-resolution spectroscopy of V582 Aur during this period. To study the pre-outburst variability of the target and construct its historical light curve, we searched for archival observations in photographic plate collections. Both CCD and photographic observations were analyzed using a sequence of 14 stars in the field of V582 Aur calibrated in BVRI. The pre-outburst photographic observations of V582 Aur show low-amplitude light variations typical of T Tauri stars. Archival photographic observations indicate that the increase in brightness began in late 1984 or early 1985 and the star reached the maximum level of brightness at 1986 January. The spectral type of V582 Aur can be defined as G0I with strong P Cyg profiles of H alpha and Na I D lines, which are typical of FU Orionis objects. Our BVRI photometric observations show large amplitude variations V~2.8 mag. during the 3.5 year period of observations. Most of the time, however, the star remains in a state close to the maximum brightness. The deepest drop in brightness was observed in the spring of 2012, when the brightness of the star fell to a level close to the pre-outburst. The multicolor photometric data show a color reversal during the minimum in brightness, which is typical of UX Ori variables. The corresponding spectral observations show strong variability in the profiles and intensities of the spectral lines (especially H alpha), which indicate significant changes in the accretion rate. On the basis of photometric monitoring performed over the past three years, the spectral properties of the maximal light, and the shape of the long-term light curve, we confirm the affiliation of V582 Aur to the group of FU Orionis objects.
The LHC Confronts the pMSSM<|sep|>We explore the impact of current (7+8 TeV) and future (14 TeV) LHC searches on the range of viable sparticle spectra within the 19/20 - dimensional phenomenological MSSM (pMSSM). Considering both neutralino and gravitino LSPs, we compare our results with simplified model exclusion limits and describe important cases where the pMSSM results differ significantly from the simplified model descriptions. We also consider models that are poorly constrained by LHC data because of unusual decay topologies and/or displaced decays, and discuss ways to improve the LHC sensitivity in these scenarios. Finally, motivated by naturalness, we examine the sensitivity of current searches to models with light stops and to a specialized set of models with fine-tuning better than 1\%. We show that the 14 TeV LHC will be a very powerful probe of natural pMSSM models.
What's in a Question: Using Visual Questions as a Form of Supervision<|sep|>Collecting fully annotated image datasets is challenging and expensive. Many types of weak supervision have been explored: weak manual annotations, web search results, temporal continuity, ambient sound and others. We focus on one particular unexplored mode: visual questions that are asked about images. The key observation that inspires our work is that the question itself provides useful information about the image (even without the answer being available). For instance, the question "what is the breed of the dog?" informs the AI that the animal in the scene is a dog and that there is only one dog present. We make three contributions: (1) providing an extensive qualitative and quantitative analysis of the information contained in human visual questions, (2) proposing two simple but surprisingly effective modifications to the standard visual question answering models that allow them to make use of weak supervision in the form of unanswered questions associated with images and (3) demonstrating that a simple data augmentation strategy inspired by our insights results in a 7.1% improvement on the standard VQA benchmark.
A Novel Method of Solving Linear Programs with an Analog Circuit<|sep|>We present the design of an analog circuit which solves linear programming (LP) problems. In particular, the steady-state circuit voltages are the components of the LP optimal solution. The paper shows how to construct the circuit and provides a proof of equivalence between the circuit and the LP problem. The proposed method is used to implement a LP-based Model Predictive Controller by using an analog circuit. Simulative and experimental results show the effectiveness of the proposed approach.
Points of constancy of the periodic linearized Korteweg--deVries equation<|sep|>We investigate the points of constancy in the piecewise constant solution profiles of the periodic linearized Korteweg--deVries equation with step function initial data at rational times. The solution formulas are given by certain Weyl sums, and we employ number theoretic techniques, including Kummer sums, in our analysis. These results constitute an initial attempt to understand the phenomenon of "fractalization" observed at irrational times.
Pop-up SLAM: Semantic Monocular Plane SLAM for Low-texture Environments<|sep|>Existing simultaneous localization and mapping (SLAM) algorithms are not robust in challenging low-texture environments because there are only few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from a pop-up 3D plane model applied to each single image. We also combine planes with point based SLAM to improve robustness. On a public TUM dataset, our algorithm generates a dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67%.
Thermodynamically stable equal-module exchange magnetic classes<|sep|>It is shown that within the framework of application of the general scheme of the Landau theory of phase transitions to the magnetic crystals the equilibrium and stable equal-module exchange structures automatically appears in addition to the other magnetic states. No additional conditions such as the existence of the Andreev-Marchenko spin scalar are needed. Furthermore, they are not constrained by the dimensionality of irreducible representation as it typically takes place for the Andreev-Marchenko-type structures.
Covariant Noether charges for type IIB and 11-dimensional supergravities<|sep|>The covariant Noether charge formalism (also known as the covariant phase method) of Wald and collaborators, including its cohomological extension, is a manifestly covariant Hamiltonian formalism that, in principle, allows one to define and compute the energy, angular momenta, and chemical potentials of generic solutions of gravitational theories. However, it has been observed that for some supergravity solutions the variation of the Noether charge is not (at least manifestably) integrable, and as a result it is unclear whether there are well-defined thermodynamic charges for these solutions. In this work, we derive an expression for the variation of the covariant Noether charges for any solution of Type IIB 10-dimensional supergravity or 11-dimensional supergravity. Although this Noether quantity is not integrable in general, we show that for asymptotically scale-invariant solutions, it is. In particular, the asymptotic scale-invariance allows one to define an energy density and conjugate chemical potentials which obey a first law of thermodynamics and a Smarr relation. These two thermodynamic relations can then be shown to imply that the variation of the Noether charge is integrable, and accordingly the energy and other thermodynamic charges may be defined unambiguously. We explicitly demonstrate and illustrate our claim by computing the thermodynamic charges of two non-trivial supergravity solutions that were recently constructed: 1) the Polchinski-Strassler black brane that is dual to the deconfined phase of $\mathcal{N}=1^*$ theory, and 2) the CGLP black brane that asymptotes to the mass deformed Cveti\v{c}-Gibbons-L\"u-Pope (CGLP) solution.
Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification<|sep|>Recent work on scene classification still makes use of generic CNN features in a rudimentary manner. In this ICCV 2015 paper, we present a novel pipeline built upon deep CNN features to harvest discriminative visual objects and parts for scene classification. We first use a region proposal technique to generate a set of high-quality patches potentially containing objects, and apply a pre-trained CNN to extract generic deep features from these patches. Then we perform both unsupervised and weakly supervised learning to screen these patches and discover discriminative ones representing category-specific objects and parts. We further apply discriminative clustering enhanced with local CNN fine-tuning to aggregate similar objects and parts into groups, called meta objects. A scene image representation is constructed by pooling the feature response maps of all the learned meta objects at multiple spatial scales. We have confirmed that the scene image representation obtained using this new pipeline is capable of delivering state-of-the-art performance on two popular scene benchmark datasets, MIT Indoor 67~\cite{MITIndoor67} and Sun397~\cite{Sun397}
Improving qubit coherence using closed-loop feedback<|sep|>Superconducting qubits are a promising platform for building a larger-scale quantum processor capable of solving otherwise intractable problems. In order for the processor to reach practical viability, the gate errors need to be further suppressed and remain stable for extended periods of time. With recent advances in qubit control, both single- and two-qubit gate fidelities are now in many cases limited by the coherence times of the qubits. Here we experimentally employ closed-loop feedback to stabilize the frequency fluctuations of a superconducting transmon qubit, thereby increasing its coherence time by 26\% and reducing the single-qubit error rate from $(8.5 \pm 2.1)\times 10^{-4}$ to $(5.9 \pm 0.7)\times 10^{-4}$. Importantly, the resulting high-fidelity operation remains effective even away from the qubit flux-noise insensitive point, significantly increasing the frequency bandwidth over which the qubit can be operated with high fidelity. This approach is helpful in large qubit grids, where frequency crowding and parasitic interactions between the qubits limit their performance.
How informative is the Order Book Beyond the Best Levels? Machine Learning Perspective<|sep|>Research on limit order book markets has been rapidly growing and nowadays high-frequency full order book data is widely available for researchers and practitioners. However, it is common that research papers use the best level data only, which motivates us to ask whether the exclusion of the quotes deeper in the book over multiple price levels causes performance degradation. In this paper, we address this question by using modern Machine Learning (ML) techniques to predict mid-price movements without assuming that limit order book markets represent a linear system. We provide a number of results that are robust across ML prediction models, feature selection algorithms, data sets, and prediction horizons. We find that the best bid and ask levels are systematically identified not only as the most informative levels in the order books, but also to carry most of the information needed for good prediction performance. On the other hand, even if the top-of-the-book levels contain most of the relevant information, to maximize models' performance one should use all data across all the levels. Additionally, the informativeness of the order book levels clearly decreases from the first to the fourth level while the rest of the levels are approximately equally important.
Influence of the biquadratic exchange interaction in the classical ground state magnetic response of the antiferromagnetic icosahedron<|sep|>The icosahedron has a ground state magnetization discontinuity in an external magnetic field when classical spins mounted on its vertices are coupled according to the antiferromagnetic Heisenberg model. This is so even if there is no magnetic anisotropy in the Hamiltonian. The discontinuity is a consequence of the frustrated nature of the interactions, which originates in the topology of the cluster. Here it is found that the addition of the next order isotropic spin exchange interaction term in the Hamiltonian, the biquadratic exchange interaction, significantly enriches the classical ground state magnetic response. For relatively weak biquadratic interaction new discontinuities emerge, while for even stronger the number of discontinuities for this small molecule can go up to seven, accompanied by a susceptibility discontinuity. These results demonstrate the possibility of using a small entity like the icosahedron as a magnetic unit whose ground state spin configuration and magnetization can be tuned between many different non-overlapping regimes with the application of an external field.
Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation<|sep|>Features selection is an essential step for successful data classification, since it reduces the data dimensionality by removing redundant features. Consequently, that minimizes the classification complexity and time in addition to maximizing its accuracy. In this article, a comparative study considering six features selection heuristics is conducted in order to select the best relevant features subset. The tested features vector consists of fourteen features that are computed for each pixel in the field of view of retinal images in the DRIVE database. The comparison is assessed in terms of sensitivity, specificity, and accuracy measurements of the recommended features subset resulted by each heuristic when applied with the ant colony system. Experimental results indicated that the features subset recommended by the relief heuristic outperformed the subsets recommended by the other experienced heuristics.
A Complete Sample of Long Bright Swift GRBs<|sep|>Starting from the Swift sample we define a complete sub-sample of 58 bright long Gamma Ray Bursts (GRB), 55 of them (95%) with a redshift determination, in order to characterize their properties. Our sample (BAT6) allows us to study the properties of the long GRB population and their evolution with cosmic time. We focus in particular on the GRB luminosity function, on the spectral-energy correlations of their prompt emission, on the nature of dark bursts, on possible correlations between the prompt and the X-ray afterglow properties, and on the dust extinction.
A critical analysis on the sensitivity enhancement of surface plasmon resonance sensors with graphene<|sep|>The use of graphene in surface plasmon resonance sensors, covering a metallic (plasmonic) film, has a number of demonstrated advantages, such protecting the film against corrosion/oxidation and facilitating the introduction of functional groups for selective sensing. Recently, a number of works have claimed that few-layer graphene can also increase the sensitivity of the sensor. However, graphene was treated as an isotropic thin film, with an out-of-plane refractive index that is identical to the in-plane index. Here, we critically examine the role of single and few layers of graphene in the sensitivity enhancement of surface plasmon resonance sensors. Graphene is introduced over the metallic film via three different descriptions: as an atomic-thick two-dimensional sheet, as a thin effective isotropic material (same conductivity in the three coordinate directions), and as an non-isotropic layer (different conductivity in the perpendicular direction to the two-dimensional plane). We find that only the isotropic layer model, which is known to be incorrect for the optically modelling of graphene, provides sizeable sensitivity increases, while the other, more accurate, models lead to negligible contribution to the sensitivity.
Evidence for Partial Taylor Relaxation from Changes in Magnetic Geometry and Energy during a Solar Flare<|sep|>Solar flares are powered by energy stored in the coronal magnetic field, a portion of which is released when the field reconfigures into a lower energy state. Investigation of sunspot magnetic field topology during flare activity is useful to improve our understanding of flaring processes. Here we investigate the deviation of the non-linear field configuration from that of the linear and potential configurations, and study the free energy available leading up to and after a flare. The evolution of the magnetic field in NOAA region 10953 was examined using data from Hinode/SOT-SP, over a period of 12 hours leading up to and after a GOES B1.0 flare. Previous work on this region found pre- and post-flare changes in photospheric vector magnetic field parameters of flux elements outside the primary sunspot. 3D geometry was thus investigated using potential, linear force-free, and non-linear force-free field extrapolations in order to fully understand the evolution of the field lines. Traced field line geometrical and footpoint orientation differences show that the field does not completely relax to a fully potential or linear force-free state after the flare. Magnetic and free magnetic energies increase significantly ~ 6.5-2.5 hours before the flare by ~ 10^31 erg. After the flare, the non-linear force-free magnetic energy and free magnetic energies decrease but do not return to pre-flare 'quiet' values. The post-flare non-linear force-free field configuration is closer (but not equal) to that of the linear force-free field configuration than a potential one. However, the small degree of similarity suggests that partial Taylor relaxation has occurred over a time scale of ~ 3-4 hours.
A joint routing and speed optimization problem<|sep|>Fuel cost contributes to a significant portion of operating cost in cargo transportation. Though classic routing models usually treat fuel cost as input data, fuel consumption heavily depends on the travel speed, which has led to the study of optimizing speeds over a given fixed route. In this paper, we propose a joint routing and speed optimization problem to minimize the total cost, which includes the fuel consumption cost. The only assumption made on the dependence between the fuel cost and travel speed is that it is a strictly convex differentiable function. This problem is very challenging, with medium-sized instances already difficult for a general mixed-integer convex optimization solver. We propose a novel set partitioning formulation and a branch-cut-and-price algorithm to solve this problem. Our algorithm clearly outperforms the off-the-shelf optimization solver, and is able to solve some benchmark instances to optimality for the first time.
Assessment of effective parameters on dilution using approximate reasoning methods in longwall mining method, Iran coal mines<|sep|>Approximately more than 90% of all coal production in Iranian underground mines is derived directly longwall mining method. Out of seam dilution is one of the essential problems in these mines. Therefore the dilution can impose the additional cost of mining and milling. As a result, recognition of the effective parameters on the dilution has a remarkable role in industry. In this way, this paper has analyzed the influence of 13 parameters (attributed variables) versus the decision attribute (dilution value), so that using two approximate reasoning methods, namely Rough Set Theory (RST) and Self Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our collected data sets has been extracted. The other benefit of later methods is to predict new unknown cases. So, the reduced sets (reducts) by RST have been obtained. Therefore the emerged results by utilizing mentioned methods shows that the high sensitive variables are thickness of layer, length of stope, rate of advance, number of miners, type of advancing.
The variability of the quasar 3C 273: a radio to gamma-ray view<|sep|>We have analysed the first 15 months of Fermi/LAT data of the radio loud quasar 3C 273. Intense gamma-ray activity has been detected, showing an average flux of F(> 100 MeV) = 1.4e-6 ph/cm^2/s, with a peak at F(> 100 MeV) = 5.6e-6 ph/cm^2/s detected during a flare in September 2009. Together with the brightening of the source, a possible hardening of the gamma-ray spectrum is observed, pointing to a shift of the inverse Compton peak toward higher energies than the 1-10 MeV range in which 3C 273 inverse Compton emission is typically observed to peak. During the 15 months of observations the photon index is measured to vary between 2.4 and 3.3, with an average value of 2.78 +/- 0.03. When compared to the observations at other wavelengths, the gamma-rays show the largest flux variations and we discuss the possibility that two different components are responsible for the inverse Compton hump emission below and above the MeV peak.
A Unified Framework for Adjustable Robust Optimization with Endogenous Uncertainty<|sep|>This work proposes a framework for multistage adjustable robust optimization that unifies the treatment of three different types of endogenous uncertainty, where decisions, respectively, (i) alter the uncertainty set, (ii) affect the materialization of uncertain parameters, and (iii) determine the time when the true values of uncertain parameters are observed. We provide a systematic analysis of the different types of endogenous uncertainty and highlight the connection between optimization under endogenous uncertainty and active learning. We consider decision-dependent polyhedral uncertainty sets and propose a decision rule approach that incorporates both continuous and binary recourse, including recourse decisions that affect the uncertainty set. The proposed method enables the modeling of decision-dependent nonanticipativity and results in a tractable reformulation of the problem. We demonstrate the effectiveness of the approach in computational experiments that cover a range of applications, including plant redesign, maintenance planning with inspections, optimizing revision points in capacity planning, and production scheduling with active parameter estimation. The results show significant benefits from the proper modeling of endogenous uncertainty and active learning.
On the validity of Strong Cosmic Censorship Conjecture in presence of Dirac fields<|sep|>A well posed theory of nature is expected to determine the future of an observer uniquely from a given set of appropriate initial data. In the context of general relativity, this is ensured by Penrose's strong cosmic censorship conjecture. But in recent years, several examples are found which suggest breakdown of the deterministic nature of the theory in Reissner-Nordstrom-de Sitter black holes under the influence of different fundamental fields. Nevertheless, the situation has been reassuring for the case of astrophysically meaningful Kerr-de Sitter black hole solutions which seems to respect the conjecture. However, the previous analyses were done considering only the effect of scalar fields. In this paper, we extend the study by considering Dirac fields in Kerr-de Sitter background and show that there exist a parameter space which does not respect the conjecture.
Exactly Solving the Maximum Weight Independent Set Problem on Large Real-World Graphs<|sep|>One powerful technique to solve NP-hard optimization problems in practice is branch-and-reduce search---which is branch-and-bound that intermixes branching with reductions to decrease the input size. While this technique is known to be very effective in practice for unweighted problems, very little is known for weighted problems, in part due to a lack of known effective reductions. In this work, we develop a full suite of new reductions for the maximum weight independent set problem and provide extensive experiments to show their effectiveness in practice on real-world graphs of up to millions of vertices and edges. Our experiments indicate that our approach is able to outperform existing state-of-the-art algorithms, solving many instances that were previously infeasible. In particular, we show that branch-and-reduce is able to solve a large number of instances up to two orders of magnitude faster than existing (inexact) local search algorithms---and is able to solve the majority of instances within 15 minutes. For those instances remaining infeasible, we show that combining kernelization with local search produces higher-quality solutions than local search alone.
Approximate Kalman-Bucy filter for continuous-time semi-Markov jump linear systems<|sep|>The aim of this paper is to propose a new numerical approximation of the Kalman-Bucy filter for semi-Markov jump linear systems. This approximation is based on the selection of typical trajectories of the driving semi-Markov chain of the process by using an optimal quantization technique. The main advantage of this approach is that it makes pre-computations possible. We derive a Lipschitz property for the solution of the Riccati equation and a general result on the convergence of perturbed solutions of semi-Markov switching Riccati equations when the perturbation comes from the driving semi-Markov chain. Based on these results, we prove the convergence of our approximation scheme in a general infinite countable state space framework and derive an error bound in terms of the quantization error and time discretization step. We employ the proposed filter in a magnetic levitation example with markovian failures and compare its performance with both the Kalman-Bucy filter and the Markovian linear minimum mean squares estimator.
Magnetic ripple domain structure in FeGa/MgO thin films<|sep|>The magnetic domain structure is studied in epitaxial Fe$_{100-x}$Ga$_x$/MgO(001) films with 0 $<$ x $<$ 30 and thicknesses below 60 nm by magnetic force microscopy. For low gallium content, domains with the magnetization lying in the film plane and domain walls separating micrometric areas are observed. Above x $\approx$ 20, the magnetic contrast shows a fine corrugation, ranging from 300 to 900 nm, suggesting a ripple substructure with a periodic oscillation of the magnetization. We discuss the presence of a random magnetic anisotropy contribution, that superimposed to the cubic coherent anisotropy, is able to break the uniform orientation of the magnetization. The origin of that random anisotropy is attributed to several factors: coexistence of crystal phases in the films, inhomogeneous distribution of both internal strain and Ga-Ga next nearest neighbor pairs and interface magnetic anisotropy due to the Fe-O bond.
Spectral analysis of LMC X-2 with XMM/Newton: unveiling the emission process in the extragalactic Z-source<|sep|>We present the results of the analysis of an archival observation of LMC X-2 performed with XMM/Newton. The spectra taken by high-precision instruments have never been analyzed before. We find an X-ray position for the source that is inconsistent with the one obtained by ROSAT, but in agreement with the Einstein position and that of the optical counterpart. The correlated spectral and timing behaviour of the source suggests that the source is probably in the normal branch of its X-ray color-color diagram. The spectrum of the source can be fitted with a blackbody with a temperature 1.5 keV plus a disk blackbody at 0.8 keV. Photoelectric absorption from neutral matter has an equivalent hydrogen column of 4 x 10^{20} cm^{-2}. An emission line, which we identify as the O VIII Lyman alpha line, is detected, while no feature due to iron is detected in the spectrum. We argue that the emission of this source can be straightforwardly interpreted as a sum of the emission from a boundary layer between the NS and the disc and a blackbody component coming from the disc itself. Other canonical models that are used to fit Z-sources do not give a satisfactory fit to the data. The detection of the O VIII emission line (and the lack of detection of lines in the iron region) can be due to the fact that the source lies in the Large Magellanic Cloud.
Characteristics and benchmarks of entanglement of mixed states -- the two qubit case<|sep|>We propose that the entanglement of mixed states is characterised properly in terms of a probability density function $\mathcal{P}_{\rho}(\mathcal{E})$. There is a need for such a measure since the prevalent measures (such as \textit{concurrence} and \textit{negativity}) for two qubit systems are rough benchmarks, and not monotones of each other. Focussing on the two qubit states, we provide an explicit construction of $\mathcal{P}_{\rho}(\mathcal{E})$ and show that it is characterised by a set of parameters, of which concurrence is but one particular combination. $\mathcal{P}_{\rho}(\mathcal{E})$ is manifestly invariant under $SU(2) \times SU(2)$ transformations. It can, in fact, reconstruct the state up to local operations - with the specification of at most four additional parameters. Finally the new measure resolves the controversy regarding the role of entanglement in quantum computation in NMR systems.
Marginal operators in quantum field theory with extra dimensions<|sep|>The classification of relevant, marginal and irrelevant operators is studied in the Randall-Sundrum spacetime. We find that there exist marginal and interacting operators in the Randall-Sundrum spacetime unlike a higher-dimensional effective theory near the free-field fixed point. This gives a direction to treat quantum corrections in the field-theoretical framework with extra dimensions by constructing models out of relevant and marginal operators.
On the classicality of quantum dephasing processes<|sep|>We analyze the multitime statistics associated with pure dephasing systems repeatedly probed with sharp measurements, and search for measurement protocols whose statistics satisfies the Kolmogorov consistency conditions possibly up to a finite order. We find a rich phenomenology of quantum dephasing processes which can be interpreted in classical terms. In particular, if the underlying dephasing process is Markovian, we find sufficient conditions under which classicality at every order can be found: this can be reached by choosing the dephasing and measurement basis to be fully compatible or fully incompatible, that is, mutually unbiased bases (MUBs). For non-Markovian processes, classicality can only be proven in the fully compatible case, thus revealing a key difference between Markovian and non-Markovian pure dephasing processes.
Robust Decoding from 1-Bit Compressive Sampling with Least Squares<|sep|>In 1-bit compressive sensing (1-bit CS) where target signal is coded into a binary measurement, one goal is to recover the signal from noisy and quantized samples. Mathematically, the 1-bit CS model reads: $y = \eta \odot\textrm{sign} (\Psi x^* + \epsilon)$, where $x^{*}\in \mathcal{R}^{n}, y\in \mathcal{R}^{m}$, $\Psi \in \mathcal{R}^{m\times n}$, and $\epsilon$ is the random error before quantization and $\eta\in \mathcal{R}^{n}$ is a random vector modeling the sign flips. Due to the presence of nonlinearity, noise and sign flips, it is quite challenging to decode from the 1-bit CS. In this paper, we consider least squares approach under the over-determined and under-determined settings. For $m>n$, we show that, up to a constant $c$, with high probability, the least squares solution $x_{\textrm{ls}}$ approximates $ x^*$ with precision $\delta$ as long as $m \geq\widetilde{\mathcal{O}}(\frac{n}{\delta^2})$. For $m< n$, we prove that, up to a constant $c$, with high probability, the $\ell_1$-regularized least-squares solution $x_{\ell_1}$ lies in the ball with center $x^*$ and radius $\delta$ provided that $m \geq \mathcal{O}( \frac{s\log n}{\delta^2})$ and $\|x^*\|_0 := s < m$. We introduce a Newton type method, the so-called primal and dual active set (PDAS) algorithm, to solve the nonsmooth optimization problem. The PDAS possesses the property of one-step convergence. It only requires to solve a small least squares problem on the active set. Therefore, the PDAS is extremely efficient for recovering sparse signals through continuation. We propose a novel regularization parameter selection rule which does not introduce any extra computational overhead. Extensive numerical experiments are presented to illustrate the robustness of our proposed model and the efficiency of our algorithm.
Reducing biases on $H_0$ measurements using strong lensing and galaxy dynamics: results from the EAGLE simulation<|sep|>Cosmological parameter constraints from observations of time-delay lenses are becoming increasingly precise. However, there may be significant bias and scatter in these measurements due to, among other things, the so-called mass-sheet degeneracy. To estimate these uncertainties, we analyze strong lenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet transformation to the radial density profiles of lenses, and by selecting lenses near isothermality, we find that the bias on H0 can be reduced to 5% with an intrinsic scatter of 10%, confirming previous results performed on a different simulation data set. We further investigate whether combining lensing observables with kinematic constraints helps to minimize this bias. We do not detect any significant dependence of the bias on lens model parameters or observational properties of the galaxy, but depending on the source--lens configuration, a bias may still exist. Cross lenses provide an accurate estimate of the Hubble constant, while fold (double) lenses tend to be biased low (high). With kinematic constraints, double lenses show bias and intrinsic scatter of 6% and 10%, respectively, while quad lenses show bias and intrinsic scatter of 0.5% and 10%, respectively. For lenses with a reduced $\chi^2 > 1$, a power-law dependence of the $\chi^2$ on the lens environment (number of nearby galaxies) is seen. Lastly, we model, in greater detail, the cases of two double lenses that are significantly biased. We are able to remove the bias, suggesting that the remaining biases could also be reduced by carefully taking into account additional sources of systematic uncertainty.
Enhanced IoV Security Network by Using Blockchain Governance Game<|sep|>This paper deals with the design of the secure network in an Enhanced Internet of Vehicles by using the Blockchain Governance Game (BGG). The BGG is a system model of a stochastic game to find best strategies towards preparation of preventing a network malfunction by an attacker and the paper applies this game model into the connected vehicle security. Analytically tractable results for decision-making parameters enable to predict the moment for safety operations and to deliver the optimal combination of the number of reserved nodes with the acceptance probability of backup nodes to protect a connected car. This research helps for whom considers the enhanced secure IoV architecture with the BGG within a decentralized network.
Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization<|sep|>This paper tackles the problem of large-scale image-based localization (IBL) where the spatial location of a query image is determined by finding out the most similar reference images in a large database. For solving this problem, a critical task is to learn discriminative image representation that captures informative information relevant for localization. We propose a novel representation learning method having higher location-discriminating power. It provides the following contributions: 1) we represent a place (location) as a set of exemplar images depicting the same landmarks and aim to maximize similarities among intra-place images while minimizing similarities among inter-place images; 2) we model a similarity measure as a probability distribution on L_2-metric distances between intra-place and inter-place image representations; 3) we propose a new Stochastic Attraction and Repulsion Embedding (SARE) loss function minimizing the KL divergence between the learned and the actual probability distributions; 4) we give theoretical comparisons between SARE, triplet ranking and contrastive losses. It provides insights into why SARE is better by analyzing gradients. Our SARE loss is easy to implement and pluggable to any CNN. Experiments show that our proposed method improves the localization performance on standard benchmarks by a large margin. Demonstrating the broad applicability of our method, we obtained the third place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our code and model are available at https://github.com/Liumouliu/deepIBL.
Restricted Isometry Property of Gaussian Random Projection for Finite Set of Subspaces<|sep|>Dimension reduction plays an essential role when decreasing the complexity of solving large-scale problems. The well-known Johnson-Lindenstrauss (JL) Lemma and Restricted Isometry Property (RIP) admit the use of random projection to reduce the dimension while keeping the Euclidean distance, which leads to the boom of Compressed Sensing and the field of sparsity related signal processing. Recently, successful applications of sparse models in computer vision and machine learning have increasingly hinted that the underlying structure of high dimensional data looks more like a union of subspaces (UoS). In this paper, motivated by JL Lemma and an emerging field of Compressed Subspace Clustering (CSC), we study for the first time the RIP of Gaussian random matrices for the compression of two subspaces based on the generalized projection $F$-norm distance. We theoretically prove that with high probability the affinity or distance between two projected subspaces are concentrated around their estimates. When the ambient dimension after projection is sufficiently large, the affinity and distance between two subspaces almost remain unchanged after random projection. Numerical experiments verify the theoretical work.
CAMEO: Curiosity Augmented Metropolis for Exploratory Optimal Policies<|sep|>Reinforcement Learning has drawn huge interest as a tool for solving optimal control problems. Solving a given problem (task or environment) involves converging towards an optimal policy. However, there might exist multiple optimal policies that can dramatically differ in their behaviour; for example, some may be faster than the others but at the expense of greater risk. We consider and study a distribution of optimal policies. We design a curiosity-augmented Metropolis algorithm (CAMEO), such that we can sample optimal policies, and such that these policies effectively adopt diverse behaviours, since this implies greater coverage of the different possible optimal policies. In experimental simulations we show that CAMEO indeed obtains policies that all solve classic control problems, and even in the challenging case of environments that provide sparse rewards. We further show that the different policies we sample present different risk profiles, corresponding to interesting practical applications in interpretability, and represents a first step towards learning the distribution of optimal policies itself.
Bending branes for DCFT in two dimensions<|sep|>We consider a holographic dual model for defect conformal field theories (DCFT) in which we include the backreaction of the defect on the dual geometry. In particular, we consider a dual gravity system in which a two-dimensional hypersurface with matter fields, the brane, is embedded into a three-dimensional asymptotically Anti-de Sitter spacetime. Motivated by recent proposals for holographic duals of boundary conformal field theories (BCFT), we assume the geometry of the brane to be determined by Israel junction conditions. We show that these conditions are intimately related to the energy conditions for the brane matter fields, and explain how these energy conditions constrain the possible geometries. This has implications for the holographic entanglement entropy in particular. Moreover, we give exact analytical solutions for the case where the matter content of the brane is a perfect fluid, which in a particular case corresponds to a free massless scalar field. Finally, we describe how our results may be particularly useful for extending a recent proposal for a holographic Kondo model.
Propagation of UHECRs in the Universe<|sep|>The origin, propagation, and mechanisms of acceleration of the ultra-high energy cosmic rays (UHECRs) are not yet well understood. Aiming for a better interpretation of the available experimental data, these data have to be confronted with theoretical models. A realistic simulation of the propagation of UHECRs in the universe should take into account all the relevant energy loss processes due to the interaction with astrophysical backgrounds, as well as the intervening cosmic magnetic fields. Cosmological effects, such as the redshift dependence of the photon backgrounds and the adiabatic expansion of the universe can play an important role in the aforementioned processes. Here we present results of simulations of the propagation of UHECR through the large scale structure of the universe considering cosmological and magnetic field effects simultaneously.
Theoretical seismology in 3D : nonlinear simulations of internal gravity waves in solar-like stars<|sep|>Internal gravity waves (hereafter IGWs) are studied for their impact on the angular momentum transport in stellar radiation zones and the information they provide about the structure and dynamics of deep stellar interiors. We here present the first 3D nonlinear numerical simulations of IGWs excitation and propagation in a solar-like star. The aim is to study the behavior of waves in a realistic 3D nonlinear time dependent model of the Sun and to characterize their properties. We compare our results with theoretical and 1D predictions. It allows us to point out the complementarity between theory and simulation and to highlight the convenience but also the limits of the asymptotic and linear theories. We show that a rich spectrum of IGWs is excited by the convection, representing about 0.4% of the total solar luminosity. We study the spatial and temporal properties of this spectrum, the effect of thermal damping and nonlinear interactions between waves. We give quantitative results about the modes frequencies, evolution with time and rotational splitting and we discuss the amplitude of IGWs considering different regimes of parameters. This work points out the importance of high performance simulation for its complementarity with observation and theory. It opens a large field of investigation concerning IGWs propagating nonlinearly in 3D spherical structures. The extension of this work to other types of stars, with different masses, structures and rotation rates will lead to a deeper and more accurate comprehension of IGWs in stars.
Variations on the Stochastic Shortest Path Problem<|sep|>In this invited contribution, we revisit the stochastic shortest path problem, and show how recent results allow one to improve over the classical solutions: we present algorithms to synthesize strategies with multiple guarantees on the distribution of the length of paths reaching a given target, rather than simply minimizing its expected value. The concepts and algorithms that we propose here are applications of more general results that have been obtained recently for Markov decision processes and that are described in a series of recent papers.
Beyond Lorentzian Symmetry<|sep|>This thesis presents a framework in which to explore kinematical symmetries beyond the standard Lorentzian case. This framework consists of an algebraic classification, a geometric classification, and a derivation of the geometric properties required to define physical theories on the classified spacetime geometries. The work completed in substantiating this framework for kinematical, super-kinematical, and super-Bargmann symmetries constitutes the body of this thesis. To this end, the classification of kinematical Lie algebras in spatial dimension $D = 3$, as presented in [3, 4], is reviewed; as is the classification of spatially-isotropic homogeneous spacetimes of [5]. The derivation of geometric properties such as the non-compactness of boosts, soldering forms and vielbeins, and the space of invariant affine connections is then presented. We move on to classify the $\mathcal{N}=1$ kinematical Lie superalgebras in three spatial dimensions, finding 43 isomorphism classes of Lie superalgebras. Once these algebras are determined, we classify the corresponding simply-connected homogeneous (4|4)-dimensional superspaces and show how the resulting 27 homogeneous superspaces may be related to one another via geometric limits. Finally, we turn our attention to generalised Bargmann superalgebras. In the present work, these will be the $\mathcal{N}=1$ and $\mathcal{N}=2$ super-extensions of the Bargmann and Newton-Hooke algebras, as well as the centrally-extended static kinematical Lie algebra, of which the former three all arise as deformations. Focussing solely on three spatial dimensions, we find $9$ isomorphism classes in the $\mathcal{N}=1$ case, and we identify $22$ branches of superalgebras in the $\mathcal{N}=2$ case.
Harmonics of Solar Radio Spikes at Metric Wavelengths<|sep|>This paper presents the latest observations from the newly-built solar radio spectrograph at the \emph{Chashan Solar Observatory}. On July 18 2016, the spectrograph records a solar spike burst event, which has several episodes showing harmonic structures, with the second, third, and fourth harmonics. The lower harmonic radio spike emissions are observed later than the higher harmonic bands, and the temporal delay of the second (third) harmonic relative to the fourth harmonic is about 30\ --\ 40 (10) ms. Based on the electron cyclotron maser emission mechanism, we analyze possible causes of the temporal delay and further infer relevant coronal parameters, such as the magnetic field strength and the electron density at the radio source.
Entanglement analysis of isotropic spin-1 chains<|sep|>We investigate entanglement spectra of the SO(3) bilinear-biquadratic spin-1 chain, a model with phases exhibiting spontaneous symmetry breaking (both translation and spin rotation), points of enlarged symmetry, and a symmetry-protected topological phase (the Haldane phase). Our analysis reveals how these hallmark features are manifested in the entanglement spectra, and highlights the versatility of entanglement spectra as a tool to study one-dimensional quantum systems via small finite size realisations.
Solving Relational MDPs with Exogenous Events and Additive Rewards<|sep|>We formalize a simple but natural subclass of service domains for relational planning problems with object-centered, independent exogenous events and additive rewards capturing, for example, problems in inventory control. Focusing on this subclass, we present a new symbolic planning algorithm which is the first algorithm that has explicit performance guarantees for relational MDPs with exogenous events. In particular, under some technical conditions, our planning algorithm provides a monotonic lower bound on the optimal value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation for real-valued functions over relational world states. Our planning algorithm uses a set of focus states, which serves as a training set, to simplify and approximate the symbolic solution, and can thus be seen to perform learning for planning. A preliminary experimental evaluation demonstrates the validity of our approach.
Is "My Favorite New Movie" My Favorite Movie? Probing the Understanding of Recursive Noun Phrases<|sep|>Recursive noun phrases (NPs) have interesting semantic properties. For example, "my favorite new movie" is not necessarily my favorite movie, whereas "my new favorite movie" is. This is common sense to humans, yet it is unknown whether language models have such knowledge. We introduce the Recursive Noun Phrase Challenge (RNPC), a dataset of three textual inference tasks involving textual entailment and event plausibility comparison, precisely targeting the understanding of recursive NPs. When evaluated on RNPC, state-of-the-art Transformer models only perform around chance. Still, we show that such knowledge is learnable with appropriate data. We further probe the models for relevant linguistic features that can be learned from our tasks, including modifier semantic category and modifier scope. Finally, models trained on RNPC achieve strong zero-shot performance on an extrinsic Harm Detection evaluation task, showing the usefulness of the understanding of recursive NPs in downstream applications.
Convolutional neural network based hierarchical autoencoder for nonlinear mode decomposition of fluid field data<|sep|>We propose a customized convolutional neural network based autoencoder called a hierarchical autoencoder, which allows us to extract nonlinear autoencoder modes of flow fields while preserving the contribution order of the latent vectors. As preliminary tests, the proposed method is first applied to a cylinder wake at $Re_D$ = 100 and its transient process. It is found that the proposed method can extract the features of these laminar flow fields as the latent vectors while keeping the order of their energy content. The present hierarchical autoencoder is further assessed with a two-dimensional $y-z$ cross-sectional velocity field of turbulent channel flow at $Re_{\tau}$ = 180 in order to examine its applicability to turbulent flows. It is demonstrated that the turbulent flow field can be efficiently mapped into the latent space by utilizing the hierarchical model with a concept of ordered autoencoder mode family. The present results suggest that the proposed concept can be extended to meet various demands in fluid dynamics including reduced order modeling and its combination with linear theory-based methods by using its ability to arrange the order of the extracted nonlinear modes.
A Robust Laser-Inertial Odometry and Mapping Method for Large-Scale Highway Environments<|sep|>In this paper, we propose a novel laser-inertial odometry and mapping method to achieve real-time, low-drift and robust pose estimation in large-scale highway environments. The proposed method is mainly composed of four sequential modules, namely scan pre-processing module, dynamic object detection module, laser-inertial odometry module and laser mapping module. Scan pre-processing module uses inertial measurements to compensate the motion distortion of each laser scan. Then, the dynamic object detection module is used to detect and remove dynamic objects from each laser scan by applying CNN segmentation network. After obtaining the undistorted point cloud without moving objects, the laser inertial odometry module uses an Error State Kalman Filter to fuse the data of laser and IMU and output the coarse pose estimation at high frequency. Finally, the laser mapping module performs a fine processing step and the "Frame-to-Model" scan matching strategy is used to create a static global map. We compare the performance of our method with two state-ofthe-art methods, LOAM and SuMa, using KITTI dataset and real highway scene dataset. Experiment results show that our method performs better than the state-of-the-art methods in real highway environments and achieves competitive accuracy on the KITTI dataset.
Twitter Sentiment Analysis: How To Hedge Your Bets In The Stock Markets<|sep|>Emerging interest of trading companies and hedge funds in mining social web has created new avenues for intelligent systems that make use of public opinion in driving investment decisions. It is well accepted that at high frequency trading, investors are tracking memes rising up in microblogging forums to count for the public behavior as an important feature while making short term investment decisions. We investigate the complex relationship between tweet board literature (like bullishness, volume, agreement etc) with the financial market instruments (like volatility, trading volume and stock prices). We have analyzed Twitter sentiments for more than 4 million tweets between June 2010 and July 2011 for DJIA, NASDAQ-100 and 11 other big cap technological stocks. Our results show high correlation (upto 0.88 for returns) between stock prices and twitter sentiments. Further, using Granger's Causality Analysis, we have validated that the movement of stock prices and indices are greatly affected in the short term by Twitter discussions. Finally, we have implemented Expert Model Mining System (EMMS) to demonstrate that our forecasted returns give a high value of R-square (0.952) with low Maximum Absolute Percentage Error (MaxAPE) of 1.76% for Dow Jones Industrial Average (DJIA). We introduce a novel way to make use of market monitoring elements derived from public mood to retain a portfolio within limited risk state (highly improved hedging bets) during typical market conditions.
H2 molecule in strong magnetic fields<|sep|>The Pauli-Hamiltonian of a molecule with fixed nuclei in a strong constant magnetic field is asymptotic, in norm-resolvent sense, to an effective Hamiltonian which has the form of a multi-particle Schr\"odinger operator with interactions given by one-dimensional \delta-potentials. We study this effective Hamiltonian in the case of the H2 -molecule and establish existence of the ground state. We also show that the inter-nuclear equilibrium distance tends to 0 as the field-strength tends to infinity.
Transport Model Comparison Studies of Intermediate-Energy Heavy-Ion Collisions<|sep|>Transport models are the main method to obtain physics information from low to relativistic-energy heavy-ion collisions. The Transport Model Evaluation Project (TMEP) has been pursued to test the robustness of transport model predictions in reaching consistent conclusions from the same type of physical model. Calculations under controlled conditions of physical input and set-up were performed with various participating codes. These included both calculations of nuclear matter in a box with periodic boundary conditions, and more realistic calculations of heavy-ion collisions. In this intermediate review, we summarize and discuss the present status of the project. We also provide condensed descriptions of the 26 participating codes, which contributed to some part of the project. These include the major codes in use today. We review the main results of the studies completed so far. They show, that in box calculations the differences between the codes can be well understood and a convergence of the results can be reached. These studies also highlight the systematic differences between the two families of transport codes, known as BUU and QMD type codes. However, when the codes were compared in full heavy-ion collisions using different physical models, as recently for pion production, they still yielded substantially different results. This calls for further comparisons of heavy-ion collisions with controlled models and of box comparisons of important ingredients, like momentum-dependent fields, which are currently underway. We often indicate improved strategies in performing transport simulations and thus provide guidance to code developers. Results of transport simulations of heavy-ion collisions from a given code will have more significance if the code can be validated against benchmark calculations such as the ones summarized in this review.
Dynamical corrections to the anomalous holographic softwall model: the pomeron and the odderon<|sep|>In this work we use the holographic softwall AdS/QCD model with anomalous dimension contributions coming from two different QCD beta functions to calculate the masses of higher spin glueball states for both even and odd spins and its respective Regge trajectories, related to the pomeron and the odderon, respectively. We further investigate this model taking into account dynamical corrections due to a dilaton potential consistent with Einstein equations in 5 dimensions. The results found in this work for the Regge trajectories within the anomalous softwall model with dynamical corrections are consistent with those presented in the literature.
The Worrisome Impact of an Inter-rater Bias on Neural Network Training<|sep|>The problem of inter-rater variability is often discussed in the context of manual labeling of medical images. The emergence of data-driven approaches such as Deep Neural Networks (DNNs) brought this issue of raters' disagreement to the front-stage. In this paper, we highlight the issue of inter-rater bias as opposed to random inter-observer variability and demonstrate its influence on DNN training, leading to different segmentation results for the same input images. In fact, lower overlap scores are obtained between the outputs of a DNN trained on annotations of one rater and tested on another. Moreover, we demonstrate that inter-rater bias in the training examples is amplified and becomes more consistent, considering the segmentation predictions of the DNNs' test data. We support our findings by showing that a classifier-DNN trained to distinguish between raters based on their manual annotations performs better when the automatic segmentation predictions rather than the actual raters' annotations were tested. For this study, we used two different datasets: the ISBI 2015 Multiple Sclerosis (MS) challenge dataset, including MRI scans each with annotations provided by two raters with different levels of expertise; and Intracerebral Hemorrhage (ICH) CT scans with manual and semi-manual segmentations. The results obtained allow us to underline a worrisome clinical implication of a DNN bias induced by an inter-rater bias during training. Specifically, we present a consistent underestimate of MS-lesion loads when calculated from segmentation predictions of a DNN trained on input provided by the less experienced rater. In the same manner, the differences in ICH volumes calculated based on outputs of identical DNNs, each trained on annotations from a different source are more consistent and larger than the differences in volumes between the manual and semi-manual annotations used for training.
Quantum and Randomised Algorithms for Non-linearity Estimation<|sep|>Non-linearity of a Boolean function indicates how far it is from any linear function. Despite there being several strong results about identifying a linear function and distinguishing one from a sufficiently non-linear function, we found a surprising lack of work on computing the non-linearity of a function. The non-linearity is related to the Walsh coefficient with the largest absolute value; however, the naive attempt of picking the maximum after constructing a Walsh spectrum requires $\Theta(2^n)$ queries to an $n$-bit function. We improve the scenario by designing highly efficient quantum and randomised algorithms to approximate the non-linearity allowing additive error, denoted $\lambda$, with query complexities that depend polynomially on $\lambda$. We prove lower bounds to show that these are not very far from the optimal ones. The number of queries made by our randomised algorithm is linear in $n$, already an exponential improvement, and the number of queries made by our quantum algorithm is surprisingly independent of $n$. Our randomised algorithm uses a Goldreich-Levin style of navigating all Walsh coefficients and our quantum algorithm uses a clever combination of Deutsch-Jozsa, amplitude amplification and amplitude estimation to improve upon the existing quantum versions of the Goldreich-Levin technique.
Kannada Spell Checker with Sandhi Splitter<|sep|>Spelling errors are introduced in text either during typing, or when the user does not know the correct phoneme or grapheme. If a language contains complex words like sandhi where two or more morphemes join based on some rules, spell checking becomes very tedious. In such situations, having a spell checker with sandhi splitter which alerts the user by flagging the errors and providing suggestions is very useful. A novel algorithm of sandhi splitting is proposed in this paper. The sandhi splitter can split about 7000 most common sandhi words in Kannada language used as test samples. The sandhi splitter was integrated with a Kannada spell checker and a mechanism for generating suggestions was added. A comprehensive, platform independent, standalone spell checker with sandhi splitter application software was thus developed and tested extensively for its efficiency and correctness. A comparative analysis of this spell checker with sandhi splitter was made and results concluded that the Kannada spell checker with sandhi splitter has an improved performance. It is twice as fast, 200 times more space efficient, and it is 90% accurate in case of complex nouns and 50% accurate for complex verbs. Such a spell checker with sandhi splitter will be of foremost significance in machine translation systems, voice processing, etc. This is the first sandhi splitter in Kannada and the advantage of the novel algorithm is that, it can be extended to all Indian languages.
Classification of Breast Lesions Using Quantitative Ultrasound Biomarkers<|sep|>Quantitative ultrasound (QUS) based parameters like the effective scatterer diameter (ESD) and mean scatterer spacing (MSS) are gaining attention recently as non-invasive biomarkers for soft tissue characterization. In this work, we propose a multiple QUS parameter based technique that employs ESD and MSS, for binary classification of breast lesions. In order to produce improved ESD estimates, we propose a modified frequency domain technique for ESD estimation of breast tissues from the diffuse component of backscattered radio-frequency (RF) data. Ensemble empirical mode decomposition (EEMD) is performed to separate the diffuse component from the coherent component by decomposing the RF data into their intrinsic mode functions (IMFs). A non-parametric Kolmogorov-Smirnov (K-S) test is employed for automatic IMF selection along with a multi-step system effect minimization process. The ESD is estimated using a nearest neighborhood average regression line fitting algorithm. Furthermore, we use an ameliorated EEMD domain autoregressive (AR) spectral estimation technique for MSS estimation. On using the ESD for binary classification of 159 lesions, we obtain high sensitivity, specificity, accuracy values of 91.07%, 96.12%, and 94.34%, respectively, with an area under the receiver operating characteristics (ROC) curve of 0.94. On combining ESD with MSS we obtain even more improved sensitivity, specificity, and accuracy values of 96.43%, 95.15%, and 95.60%, respectively, with an area under the ROC of 0.96. Such a high classification performance highlights the potential of these QUS parameters to be used as non-invasive biomarkers for breast cancer detection.
Respecting Time Series Properties Makes Deep Time Series Forecasting Perfect<|sep|>How to handle time features shall be the core question of any time series forecasting model. Ironically, it is often ignored or misunderstood by deep-learning based models, even those baselines which are state-of-the-art. This behavior makes their inefficient, untenable and unstable. In this paper, we rigorously analyze three prevalent but deficient/unfounded deep time series forecasting mechanisms or methods from the view of time series properties, including normalization methods, multivariate forecasting and input sequence length. Corresponding corollaries and solutions are given on both empirical and theoretical basis. We thereby propose a novel time series forecasting network, i.e. RTNet, on the basis of aforementioned analysis. It is general enough to be combined with both supervised and self-supervised forecasting format. Thanks to the core idea of respecting time series properties, no matter in which forecasting format, RTNet shows obviously superior forecasting performances compared with dozens of other SOTA time series forecasting baselines in three real-world benchmark datasets. By and large, it even occupies less time complexity and memory usage while acquiring better forecasting accuracy. The source code is available at https://github.com/OrigamiSL/RTNet.
Study of the Lynx-Cancer void galaxies-V. The extremely isolated galaxy UGC4722<|sep|>We present a detailed study of the extremely isolated Sdm galaxy UGC4722 (M_B = -17.4) located in the nearby Lynx-Cancer void. UGC4722 is a member of the catalogue of isolated galaxies, and has also been identified as one of the most isolated galaxies in the Local Supercluster. Optical images of the galaxy however show that it has a peculiar morphology with an elongated ~ 14 kpc long plume. New observations with the Russian 6-m telescope (BTA) and the Giant Metrewave Radio Telescope (GMRT) of the ionised and neutral gas in UGC4722 reveal the second component responsible for the disturbed morphology of the system. This is a small, almost completely destroyed, very gas-rich dwarf (M_B = -15.2, M_HI/L_B ~4.3). We estimate the oxygen abundance for both galaxies to be 12+log(O/H) ~ 7.5-7.6, which is 2-3 times lower than what is expected from the luminosity-metallicity relation for similar galaxies in denser environments. The ugr colours of the plume derived from Sloan Digital Sky Survey (SDSS) images are consistent with a simple stellar population with a post starburst age of 0.45-0.5 Gyr. This system hence appears to be the first known case of a minor merger with a prominent tidal feature consisting of a young stellar population.
Tractable Lineages on Treelike Instances: Limits and Extensions<|sep|>Query evaluation on probabilistic databases is generally intractable (#P-hard). Existing dichotomy results have identified which queries are tractable (or safe), and connected them to tractable lineages. In our previous work, using different tools, we showed that query evaluation is linear-time on probabilistic databases for arbitrary monadic second-order queries, if we bound the treewidth of the instance. In this paper, we study limitations and extensions of this result. First, for probabilistic query evaluation, we show that MSO tractability cannot extend beyond bounded treewidth: there are even FO queries that are hard on any efficiently constructible unbounded-treewidth class of graphs. This dichotomy relies on recent polynomial bounds on the extraction of planar graphs as minors, and implies lower bounds in non-probabilistic settings, for query evaluation and match counting in subinstance-closed families. Second, we show how to explain our tractability result in terms of lineage: the lineage of MSO queries on bounded-treewidth instances can be represented as bounded-treewidth circuits, polynomial-size OBDDs, and linear-size d-DNNFs. By contrast, we can strengthen the previous dichotomy to lineages, and show that there are even UCQs with disequalities that have superpolynomial OBDDs on all unbounded-treewidth graph classes; we give a characterization of such queries. Last, we show how bounded-treewidth tractability explains the tractability of the inversion-free safe queries: we can rewrite their input instances to have bounded-treewidth.
The Simulation, Fabrication Technology and Characteristic Research of Micro-Pressure Sensor with Isosceles Trapezoidal Beam-Membrane<|sep|>A micro-pressure sensor with an isosceles trapezoidal beam-membrane (ITBM) is proposed in this paper, consisting of a square silicon membrane, four isosceles trapezoidal beams and four piezoresistors.To investigate how the elastic silicon membrane affects pressure sensitive characteristics, a simulation models based on ANSYS 15.0 software were used to analyze the effect of structural dimension on characteristics of pressure sensor. According to that, the chips of micro-pressure sensors were fabricated by micro-electro-mechanical system (MEMS) technology on a silicon wafer with <100> orientation.The experimental results show that the proposed sensor achieves a better sensitivity of 9.64 mV/kPa and an excellent linearity of 0.09%F.S. in the range of 0~3.0 kPa at room temperature and a supply voltage of 5.0 V,with a super temperature coefficient of sensitivity(TCS) about - 684 ppm/K from 235.15 K to 360.15 K and low pressure measurement less than 3.0 kPa.
Generators of finite fields with prescribed traces<|sep|>This paper explores the existence and distribution of primitive elements in finite field extensions with prescribed traces in several intermediate field extensions. Our main result provides an inequality-like condition to ensure the existence of such elements. We then derive concrete existence results for a special class of intermediate extensions.
Climate Cycling on Early Mars Caused by the Carbonate-Silicate Cycle<|sep|>For decades, scientists have tried to explain the evidence for fluvial activity on early Mars, but a consensus has yet to emerge regarding the mechanism for producing it. One hypothesis suggests early Mars was warmed by a thick greenhouse atmosphere. Another suggests that early Mars was generally cold but was warmed occasionally by impacts or by episodes of enhanced volcanism. These latter hypotheses struggle to produce the amounts of rainfall needed to form the martian valleys, but are consistent with inferred low rates of weathering compared to Earth. Here, we provide a geophysical mechanism that could have induced cycles of glaciation and deglaciation on early Mars. Our model produces dramatic climate cycles with extended periods of glaciation punctuated by warm periods lasting up to 10 Myr, much longer than those generated in other episodic warming models. The cycles occur because stellar insolation was low, and because CO2 outgassing is not able to keep pace with CO2 consumption by silicate weathering followed by deposition of carbonates. While CO2 by itself is not able to deglaciate early Mars in our model, we assume that the greenhouse effect is enhanced by substantial amounts of H2 outgassed from Mars' reduced crust and mantle. Our hypothesis can be tested by future Mars exploration that better establishes the time scale for valley formation.
Microscopically-constrained Fock energy density functionals from chiral effective field theory. I. Two-nucleon interactions<|sep|>The density matrix expansion (DME) of Negele and Vautherin is a convenient tool to map finite-range physics associated with vacuum two- and three-nucleon interactions into the form of a Skyme-like energy density functional (EDF) with density-dependent couplings. In this work, we apply the improved formulation of the DME proposed recently in arXiv:0910.4979 by Gebremariam {\it et al.} to the non-local Fock energy obtained from chiral effective field theory (EFT) two-nucleon (NN) interactions at next-to-next-to-leading-order (N$^2$LO). The structure of the chiral interactions is such that each coupling in the DME Fock functional can be decomposed into a cutoff-dependent coupling {\it constant} arising from zero-range contact interactions and a cutoff-independent coupling {\it function} of the density arising from the universal long-range pion exchanges. This motivates a new microscopically-guided Skyrme phenomenology where the density-dependent couplings associated with the underlying pion-exchange interactions are added to standard empirical Skyrme functionals, and the density-independent Skyrme parameters subsequently refit to data. A Mathematica notebook containing the novel density-dependent couplings is provided.
A Compositional Query Algebra for Second-Order Logic and Uncertain Databases<|sep|>World-set algebra is a variable-free query language for uncertain databases. It constitutes the core of the query language implemented in MayBMS, an uncertain database system. This paper shows that world-set algebra captures exactly second-order logic over finite structures, or equivalently, the polynomial hierarchy. The proofs also imply that world-set algebra is closed under composition, a previously open problem.
Measurement of the differential branching fraction of the decay $\Lambda_b^0\rightarrow\Lambda\mu^+\mu^-$<|sep|>The differential branching fraction of the decay $\Lambda_b^0\rightarrow\Lambda\mu^+\mu^-$ is measured as a function of the square of the dimuon invariant mass, $q^2$. A yield of $78\pm12$ $\Lambda_b^0\rightarrow\Lambda\mu^+\mu^-$ decays is observed using data, corresponding to an integrated luminosity of 1.0\,fb$^{-1}$, collected by the LHCb experiment at a centre-of-mass energy of 7\,TeV. A significant signal is found in the $q^2$ region above the square of the $J/\psi$ mass, while at lower-$q^2$ values upper limits are set on the differential branching fraction. Integrating the differential branching fraction over $q^2$, while excluding the $J/\psi$ and $\psi(2S)$ regions, gives a branching fraction of $\BF($\Lambda_b^0\rightarrow\Lambda\mu^+\mu^-$)=(0.96\pm 0.16\stat\pm 0.13\syst\pm 0.21 (\mathrm{norm}))\times 10^{-6}$, where the uncertainties are statistical, systematic and due to the normalisation mode, $$\Lambda_b^0\rightarrow J/psi\Lambda$, respectively.
Design And Fabrication of High Numerical Aperture And Low Aberration Bi-Convex Micro Lens Array<|sep|>Micro lens array is crucial in various kinds of optical and electronic applications. A micro lens array with high numerical aperture (NA) and low aberration is in particular needed. This research is aimed to design and fabricate such a micro lens array with simple structure while keeps the same NA of a same-diameter hemisphere lens. A bi-convex semispherical micro lens array, with corresponding NA 0.379, by PDMS is first designed and analyzed. Experiments are further conducted to fabricate the designed micro lens array by the thermal reflow process. The formed profile is then sputtered with copper to serve as the mold. The front and the rear micro lens array are fabricated by plating PDMS to the mold and then assembled to form the designed micro lens array.
Towards Quantum Simulations in Particle Physics and Beyond on Noisy Intermediate-Scale Quantum Devices<|sep|>We review two algorithmic advances that bring us closer to reliable quantum simulations of model systems in high energy physics and beyond on noisy intermediate-scale quantum (NISQ) devices. The first method is the dimensional expressivity analysis of quantum circuits, which allows for constructing minimal but maximally expressive quantum circuits. The second method is an efficient mitigation of readout errors on quantum devices. Both methods can lead to significant improvements in quantum simulations, e.g., when variational quantum eigensolvers are used.
GPU-Accelerated Algorithms for Compressed Signals Recovery with Application to Astronomical Imagery Deblurring<|sep|>Compressive sensing promises to enable bandwidth-efficient on-board compression of astronomical data by lifting the encoding complexity from the source to the receiver. The signal is recovered off-line, exploiting GPUs parallel computation capabilities to speedup the reconstruction process. However, inherent GPU hardware constraints limit the size of the recoverable signal and the speedup practically achievable. In this work, we design parallel algorithms that exploit the properties of circulant matrices for efficient GPU-accelerated sparse signals recovery. Our approach reduces the memory requirements, allowing us to recover very large signals with limited memory. In addition, it achieves a tenfold signal recovery speedup thanks to ad-hoc parallelization of matrix-vector multiplications and matrix inversions. Finally, we practically demonstrate our algorithms in a typical application of circulant matrices: deblurring a sparse astronomical image in the compressed domain.
Dark Stars: Dark Matter in the First Stars leads to a New Phase of Stellar Evolution<|sep|>The first phase of stellar evolution in the history of the universe may be Dark Stars, powered by dark matter heating rather than by fusion. Weakly interacting massive particles, which are their own antiparticles, can annihilate and provide an important heat source for the first stars in the the universe. This talk presents the story of these Dark Stars. We make predictions that the first stars are very massive ($\sim 800 M_\odot$), cool (6000 K), bright ($\sim 10^6 L_\odot$), long-lived ($\sim 10^6$ years), and probable precursors to (otherwise unexplained) supermassive black holes. Later, once the initial DM fuel runs out and fusion sets in, DM annihilation can predominate again if the scattering cross section is strong enough, so that a Dark Star is born again.
A climate network-based index to discriminate different types of El Ni\~no and La Ni\~na<|sep|>El Ni\~no exhibits distinct Eastern Pacific (EP) and Central Pacific (CP) types which are commonly, but not always consistently, distinguished from each other by different signatures in equatorial climate variability. Here, we propose an index based on evolving climate networks to objectively discriminate between both flavors by utilizing a scalar-valued evolving climate network measure that quantifies spatial localization and dispersion in El Ni\~no's associated teleconnections. Our index displays a sharp peak (high localization) during EP events, whereas during CP events (larger dispersion) it remains close to the baseline observed during normal periods. In contrast to previous classification schemes, our approach specifically account for El Ni\~no's global impacts. We confirm recent El Ni\~no classifications for the years 1951 to 2014 and assign types to those cases were former works yielded ambiguous results. Ultimately, we study La Ni\~na episodes and demonstrate that our index provides a similar discrimination into two types.
Multi-Objective Evolutionary Framework for Non-linear System Identification: A Comprehensive Investigation<|sep|>The present study proposes a multi-objective framework for structure selection of nonlinear systems which are represented by polynomial NARX models. This framework integrates the key components of Multi-Criteria Decision Making (MCDM) which include preference handling, Multi-Objective Evolutionary Algorithms (MOEAs) and a posteriori selection. To this end, three well-known MOEAs such as NSGA-II, SPEA-II and MOEA/D are thoroughly investigated to determine if there exists any significant difference in their search performance. The sensitivity of all these MOEAs to various qualitative and quantitative parameters, such as the choice of recombination mechanism, crossover and mutation probabilities, is also studied. These issues are critically analyzed considering seven discrete-time and a continuous-time benchmark nonlinear system as well as a practical case study of non-linear wave-force modeling. The results of this investigation demonstrate that MOEAs can be tailored to determine the correct structure of nonlinear systems. Further, it has been established through frequency domain analysis that it is possible to identify multiple valid discrete-time models for continuous-time systems. A rigorous statistical analysis of MOEAs via performance sweet spots in the parameter space convincingly demonstrates that these algorithms are robust over a wide range of control parameters.
Transverse Shear Viscosity to Entropy Density for the General Anisotropic Black Brane in Horava-Lifshitz Gravity<|sep|>In this paper we calculate the ratio of transverse shear viscosity to entropy density for the general anisotropic black brane in Horava-Lifshitz gravity. There is a well-known conjecture that states this ratio should be larger than $\frac{1}{4\pi}$. The ratio of shear viscosity to entropy density is proportional to the inverse square coupling of quantum thermal field theory,$\frac{\eta }{s} \sim \frac{1}{\lambda^2 }$. Especially in QFT with gravity dual the stronger coupling means the shear viscosity per entropy density gets closer to the lower bound of $\frac{1}{4\pi}$. The KSS bound preserves in the anisotropic scaling model.
Phase incoherent photonic molecules in V-shaped mode-locked VECSELs<|sep|>Passively mode-locked vertical external-cavity surface-emitting semiconductor lasers (VECSELs) composed of a gain chip and a semiconductor saturable absorber have been drawing much attention due to their excellent performance figures. In this work we investigate how localized structures and incoherent, non-locally bound, pulse molecules emerge in a long cavity VECSELs using a V-shaped cavity geometry. We show that these states are bistable with the laser off state and that they are individually addressable. Using a model based upon delay differential equations, we demonstrate that pulse clusters result from the cavity geometry and from the non-local coupling with the gain medium; this leads to locally independent, yet globally bound phase, incoherent photonic molecules. Using a multiple time-scale analysis, we derive an amplitude equation for the field that allows us to predict analytically the distance between the elements of a cluster.
Calculating tumor trajectory and dose-of-the-day using cone-beam CT projections<|sep|>Purpose: Cone-beam CT (CBCT) projection images provide anatomical data in real-time over several respiratory cycles, forming a comprehensive picture of tumor movement. We developed and validated a method which uses these projections to determine the trajectory of and dose to highly mobile tumors during each fraction of treatment. Methods: CBCT images of a respiration phantom were acquired, the trajectory of which mimicked a lung tumor with high amplitude (up to 2.5 cm) and hysteresis. A template-matching algorithm was used to identify the location of a steel BB in each CBCT projection, and a Gaussian probability density function for the absolute BB position was calculated which best fit the observed trajectory of the BB in the imager geometry. Two modifications of the trajectory reconstruction were investigated: first, using respiratory phase information to refine the trajectory estimation (Phase), and second, using the Monte Carlo (MC) method to sample the estimated Gaussian tumor position distribution. Results: With all methods, the mean position of the BB was determined with accuracy better than 0.1 mm, and trajectory errors averaged 3.8$\pm$1.1% of the marker amplitude. Dosimetric calculations using Phase methods were more accurate, with mean absolute error less than 0.5%, and with error less than 1% in the highest-noise trajectory. MC-based trajectories prevent the over-estimation of dose, but when viewed in an absolute sense, add a small amount of dosimetric error (<0.1%). Conclusions: Marker trajectory and target dose-of-the-day were accurately calculated using CBCT projections. This technique provides a method to evaluate highly-mobile tumors using ordinary CBCT data, and could facilitate better strategies to mitigate or compensate for motion during SBRT.
Dynamics of the Innermost Accretion Flows Around Compact Objects: Magnetosphere-Disc Interface, Global Oscillations and Instabilities<|sep|>We study global non-axisymmetric oscillation modes and instabilities in magnetosphere- disc systems, as expected in neutron star X-ray binaries and possibly also in accreting black hole systems. Our two-dimensional magnetosphere-disc model consists of a Keplerian disc in contact with an uniformly rotating magnetosphere with low plasma density. Two types of global overstable modes exist in such systems, the interface modes and the disc inertial-acoustic modes. We examine various physical effects and parameters that influence the properties of these oscillation modes, particularly their growth rates, including the magnetosphere field configuration, the velocity and density contrasts across the magnetosphere-disc interface, the rotation profile (with Newtonian or General Relativistic potential), the sound speed and magnetic field of the disc. The interface modes are driven unstable by Rayleigh-Taylor and Kelvin-Helmholtz in- stabilities, but can be stabilized by the toroidal field (through magnetic tension) and disc differential rotation (through finite vorticity). General relativity increases their growth rates by modifying the disc vorticity outside the magnetosphere boundary. The interface modes may also be affected by wave absorption associated with corotation resonance in the disc. In the presence of a magnetosphere, the inertial-acoustic modes are effectively trapped at the innermost region of the relativistic disc just outside the interface. They are driven unstable by wave absorption at the corotation resonance, but can be stabilized by modest disc magnetic fields. The overstable oscillation modes studied in this paper have characteristic properties that make them possible candidates for the quasi-periodic oscillations observed in X-ray binaries.
The Involutive Quantaloid of Completely Distributive Lattices<|sep|>Let L be a complete lattice and let Q(L) be the unital quantale of join-continuous endo-functions of L. We prove the following result: Q(L) is an involutive (that is, non-commutative cyclic $\star$-autonomous) quantale if and only if L is a completely distributive lattice. If this is the case, then the dual tensor operation corresponds, via Raney's transforms, to composition in the (dual) quantale of meet-continuous endo-functions of L. Let sLatt be the category of sup-lattices and join-continuous functions and let cdLatt be the full subcategory of sLatt whose objects are the completely distributive lattices. We argue that (i) cdLatt is itself an involutive quantaloid, and therefore it is the largest full-subcategory of sLatt with this property; (ii) cdLatt is closed under the monoidal operations of sLatt and, consequently, if Q(L) is involutive, then Q(L) is completely distributive as well.
AIR-JPMC@SMM4H'22: Classifying Self-Reported Intimate Partner Violence in Tweets with Multiple BERT-based Models<|sep|>This paper presents our submission for the SMM4H 2022-Shared Task on the classification of self-reported intimate partner violence on Twitter (in English). The goal of this task was to accurately determine if the contents of a given tweet demonstrated someone reporting their own experience with intimate partner violence. The submitted system is an ensemble of five RoBERTa models each weighted by their respective F1-scores on the validation data-set. This system performed 13% better than the baseline and was the best performing system overall for this shared task.
Neural Hypernetwork Approach for Pulmonary Embolism diagnosis<|sep|>This work introduces an integrative approach based on Q-analysis with machine learning. The new approach, called Neural Hypernetwork, has been applied to a case study of pulmonary embolism diagnosis. The objective of the application of neural hyper-network to pulmonary embolism (PE) is to improve diagnose for reducing the number of CT-angiography needed. Hypernetworks, based on topological simplicial complex, generalize the concept of two-relation to many-body relation. Furthermore, Hypernetworks provide a significant generalization of network theory, enabling the integration of relational structure, logic and analytic dynamics. Another important results is that Q-analysis stays close to the data, while other approaches manipulate data, projecting them into metric spaces or applying some filtering functions to highlight the intrinsic relations. A pulmonary embolism (PE) is a blockage of the main artery of the lung or one of its branches, frequently fatal. Our study uses data on 28 diagnostic features of 1,427 people considered to be at risk of PE. The resulting neural hypernetwork correctly recognized 94% of those developing a PE. This is better than previous results that have been obtained with other methods (statistical selection of features, partial least squares regression, topological data analysis in a metric space).
Complexity growth rate, grand potential and partition function<|sep|>We examine the complexity/volume conjecture and further investigate the possible connections between complexity and partition function. The complexity/volume 2.0 states that the complexity growth rate $\mathcal{\dot{C}}\sim PV$. In the standard statistics, there is a fundamental relation among $PV$, the grand potential $\Omega$ and the partition function $\mathcal{Z}$. By using this relation, we are able to construct an ansatz between complexity and partition function. The complexity/partition function relation is then utilized to study the complexity of the thermofield double state of extended SYK models for various conditions. The relation between complexity growth rate and black hole phase transition is also discussed.
Optical properties of massive anisotropic tilted Dirac systems<|sep|>We explore the effect of valley-contrasting gaps in the optical response of two-dimensional anisotropic tilted Dirac systems. We study the spectrum of intraband and interband transitions through the joint density of states (JDOS), the optical conductivity tensor, and the Drude spectral weight. The energy bands present an indirect gap in each valley. Thus a new possibility opens for the position of the Fermi level (an ``indirect zone''), and for the momentum space for allowed transitions. The JDOS near each gap displays a set of three van Hove singularities which are in contrast to the case of gapped graphene (an absorption edge only) or 8-$Pmmn$ borophene (two interband critical points due to the tilt). For the Fermi level lying in the gap, the JDOS shows the usual linear dependence on frequency, while when above an indirect zone it looks similar to the borophene case. These spectral characteristics determine the prominent structure of the optical conductivity. The longitudinal conductivity illustrates the strong anisotropy of the optical response. Similarly, the Drude weight is anisotropic and shows regions of nonlinear dependence on the Fermi level. The breaking of valley symmetry leads to a finite Hall response and associated optical properties. The anomalous and valley Hall conductivities present graphene-like behavior with characteristic modifications due to the indirect zones. Almost perfect circular dichroism can be achieved by tuning the exciting frequency with an appropriate Fermi level position. We also calculate the spectra of optical opacity and polarization rotation, which can reach magnitudes of tenths of radians in some cases. The spectral features of the calculated response properties are signatures of the simultaneous presence of tilt and mass and suggest optical ways to determine the formation of different gaps in such class of Dirac systems.
On Integrating Deductive Synthesis and Verification Systems<|sep|>We describe techniques for synthesis and verification of recursive functional programs over unbounded domains. Our techniques build on top of an algorithm for satisfiability modulo recursive functions, a framework for deductive synthesis, and complete synthesis procedures for algebraic data types. We present new counterexample-guided algorithms for constructing verified programs. We have implemented these algorithms in an integrated environment for interactive verification and synthesis from relational specifications. Our system was able to synthesize a number of useful recursive functions that manipulate unbounded numbers and data structures.
Vacuum Alignment in SUSY A4 Models<|sep|>In this note we discuss the vacuum alignment in supersymmetric models with spontaneously broken flavour symmetries in the presence of soft supersymmetry (SUSY) breaking terms. We show that the inclusion of soft SUSY breaking terms can give rise to non-vanishing vacuum expectation values (VEVs) for the auxiliary components of the flavon fields. These non-zero VEVs can have an important impact on the phenomenology of this class of models, since they can induce an additional flavour violating contribution to the sfermion soft mass matrix of right-left (RL) type. We carry out an explicit computation in a class of SUSY A4 models predicting tri-bimaximal mixing in the lepton sector. The flavour symmetry breaking sector is described in terms of flavon and driving supermultiplets. We find non-vanishing VEVs for the auxiliary components of the flavon fields and for the scalar components of the driving fields which are of order m_{SUSY} x <phi> and m_{SUSY}, respectively. Thereby, m_{SUSY} is the generic soft SUSY breaking scale which is expected to be around 1 TeV and <phi> is the VEV of scalar components of the flavon fields. Another effect of these VEVs can be the generation of a mu term.
An \emph{ab initio} study of structural phase transitions of crystalline aluminum under ultrahigh pressures based on ensemble theory<|sep|>It is a long-time pursuit of computations with \emph{ab initio} precision of thermal contributions to phase behaviors of condensed matters under extreme conditions. In this work, the pressure induced structural phase transitions of crystalline aluminum up to $600$ GPa at room temperature are investigated based on the criterion of Gibbs free energy derived directly from the partition function that formulated in the ensemble theory with the interatomic interactions characterized by density functional theory computations. The transition pressures of the FCC$\rightarrow$HCP$\rightarrow$BCC phase transitions are determined at $194$ and $361$ GPa, the axial ratio of the stable HCP structure is found to be equal to $1.62$ and the discontinuities in the equations of states are confirmed to be associated with $-0.67\%$ and $-0.90\%$ volume changes, which are all in an excellent agreement with the measurements by one of the recent experiments but differ from other experimental observations. Compared with the results obtained by the criterion of enthalpy at $0$K, this work further shows the nontrivial thermal impacts on the structural stability of aluminum under ultrahigh-pressure circumstances even at room temperature.
Galactic Pal-eontology: Abundance Analysis of the Disrupting Globular Cluster Palomar 5<|sep|>We present a chemical abundance analysis of the tidally disrupted globular cluster (GC) Palomar 5. By co-adding high-resolution spectra of 15 member stars from the cluster's main body, taken at low signal-to-noise with the Keck/HIRES spectrograph, we were able to measure integrated abundance ratios of 24 species of 20 elements including all major nucleosynthetic channels (namely the light element Na; $\alpha$-elements Mg, Si, Ca, Ti; Fe-peak and heavy elements Sc, V, Cr, Mn, Co, Ni, Cu, Zn; and the neutron-capture elements Y, Zr, Ba, La, Nd, Sm, Eu). The mean metallicity of $-1.56\pm0.02\pm0.06$ dex (statistical and systematic errors) agrees well with the values from individual, low-resolution measurements of individual stars, but it is lower than previous high-resolution results of a small number of stars in the literature. Comparison with Galactic halo stars and other disrupted and unperturbed GCs renders Pal~5 a typical representative of the Milky Way halo population, as has been noted before, emphasizing that the early chemical evolution of such clusters is decoupled from their later dynamical history. We also performed a test as to the detectability of light element variations in this co-added abundance analysis technique and found that this approach is not sensitive even in the presence of a broad range in sodium of $\sim$0.6 dex, a value typically found in the old halo GCs. Thus, while methods of determining the global abundance patterns of such objects are well suited to study their overall enrichment histories, chemical distinctions of their multiple stellar populations is still best obtained from measurements of individual stars.
Improving the Efficiency and Robustness of Deepfakes Detection through Precise Geometric Features<|sep|>Deepfakes is a branch of malicious techniques that transplant a target face to the original one in videos, resulting in serious problems such as infringement of copyright, confusion of information, or even public panic. Previous efforts for Deepfakes videos detection mainly focused on appearance features, which have a risk of being bypassed by sophisticated manipulation, also resulting in high model complexity and sensitiveness to noise. Besides, how to mine the temporal features of manipulated videos and exploit them is still an open question. We propose an efficient and robust framework named LRNet for detecting Deepfakes videos through temporal modeling on precise geometric features. A novel calibration module is devised to enhance the precision of geometric features, making it more discriminative, and a two-stream Recurrent Neural Network (RNN) is constructed for sufficient exploitation of temporal features. Compared to previous methods, our proposed method is lighter-weighted and easier to train. Moreover, our method has shown robustness in detecting highly compressed or noise corrupted videos. Our model achieved 0.999 AUC on FaceForensics++ dataset. Meanwhile, it has a graceful decline in performance (-0.042 AUC) when faced with highly compressed videos.
Derandomizing HSSW Algorithm for 3-SAT<|sep|>We present a (full) derandomization of HSSW algorithm for 3-SAT, proposed by Hofmeister, Sch\"oning, Schuler, and Watanabe in [STACS'02]. Thereby, we obtain an O(1.3303^n)-time deterministic algorithm for 3-SAT, which is currently fastest.
Effect of driving on coarsening dynamics in phase-separating systems<|sep|>We consider the Cahn-Hilliard (CH) equation with a Burgers-type convective term that is used as a model of coarsening dynamics in laterally driven phase-separating systems. In the absence of driving, it is known that solutions to the standard CH equation are characterized by an initial stage of phase separation into regions of one phase surrounded by the other phase (i.e., clusters or drops/holes or islands are obtained) followed by the coarsening process, where the average size of the structures grows in time and their number decreases. Moreover, two main coarsening modes have been identified in the literature, namely, coarsening due to volume transfer and due to translation. In the opposite limit of strong driving, the well-known Kuramoto-Sivashinsky (KS) equation is recovered, which may produce complicated chaotic spatio-temporal oscillations. The primary aim of the present work is to perform a detailed and systematic investigation of the transitions in the solutions of the convective CH (cCH) equation for a wide range of parameter values, and, in particular, to understand in detail how the coarsening dynamics is affected by an increase of the strength of the lateral driving force. Considering symmetric two-drop states, we find that one of the coarsening modes is stabilized at relatively weak driving, and the type of the remaining mode may change as driving increases. Furthermore, there exist intervals in the driving strength where coarsening is completely stabilized. In the intervals where the symmetric two-drop states are unstable they can evolve, for example, into one-drop states, two-drop states of broken symmetry or even time-periodic two-drop states that consist of two traveling drops that periodically exchange mass. We present detailed stability diagrams for symmetric two-drop states in various parameter planes and corroborate our findings by selected time simulations.
Seesaw model in SO(10) with an upper limit on right-handed neutrino masses<|sep|>In the framework of SO(10) gauge unification and the seesaw mechanism, we show that the upper bound on the mass of the heaviest right-handed neutrino $M_{R_3} < 3 \times 10^{11}$ GeV, given by the Pati-Salam intermediate scale of $B-L$ spontaneous symmetry breaking, constrains the observables related to the left-handed light neutrino mass matrix. We assume such an upper limit on the masses of right-handed neutrinos and, as a first approximation, a Cabibbo form for the matrix $V^L$ that diagonalizes the Dirac neutrino matrix $m_D$. Using the inverse seesaw formula, we show that our hypotheses imply a triangular relation in the complex plane of the light neutrino masses with the Majorana phases. We obtain normal hierarchy with an absolute scale for the light neutrino spectrum. Two regions are allowed for the lightest neutrino mass $m_1$ and for the Majorana phases, implying predictions for the neutrino mass measured in Tritium decay and for the double beta decay effective mass $|<m_{ee}>|$.
Upscaled Lattice Boltzmann Method for Simulations of Flows in Heterogeneous Porous Media<|sep|>A upscaled lattice Boltzmann method (LBM) for flow simulations in heterogeneous porous media, at both pore and Darcy scales, is proposed in this paper. In the micro-scale simulations, we model flows using LBM with the modified Guo et al. algorithm where we replace the force model with a simple Shan-Chen force model. The proposed upscaled LBM uses coarser grids to represent the effects of the fine-grid (pore-scale) simulations. For the upscaled LBM, effective properties and reduced-order models are proposed as we coarsen the grid. The effective properties are computed using solutions of local problems (e.g., by performing local LBM simulations) subject to some boundary conditions. A upscaled LBM that can reduce the computational complexity of existing LBM and transfer the information between different scales is implemented. The results of coarse-grid, reduced-order, simulations agree very well with averaged results obtained using a fine grid.
Combining Dynamic Analysis and Visualization to Explore the Distribution of Unit Test Suites<|sep|>As software systems have grown in scale and complexity the test suites built alongside those systems have also become increasingly complex. Understanding key aspects of test suites, such as their coverage of production code, is important when maintaining or reengineering systems. This work investigates the distribution of unit tests in Open Source Software (OSS) systems through the visualization of data obtained from both dynamic and static analysis. Our long-term aim is to support developers in their understanding of test distribution and the relationship of tests to production code. We first obtain dynamic coupling information from five selected OSS systems and we then map the test and production code results. The mapping is shown in graphs that depict both the dependencies between classes and static test information. We analyze these graphs using Centrality metrics derived from graph theory and SNA. Our findings suggest that, for these five systems at least, unit test and dynamic coupling information 'do not match', in that unit tests do not appear to be distributed in line with the systems' dynamic coupling. We contend that, by mapping dynamic coupling data onto unit test information, and through the use of software metrics and visualization, we can locate central system classes and identify to which classes unit testing effort has (or has not) been dedicated.
On Some Features of Color Confinement<|sep|>It is argued that a dual symmetry is needed to naturally explain experimental limits on color confinement. Since color is an exact symmetry the only possibility is that this symmetry be a dual symmetry, related to non trivial spatial homotopy. The sphere at infinity of 3-dimensional space being 2-dimensional, the relevant homotopy is $\Pi_2$, the corresponding configurations monopoles, and the mechanism dual superconductivity. The consistency of the order-disorder nature of the deconfining transition is compared with lattice data . It is also shown that the only dual quantum number is magnetic charge and the key quantity is 't Hooft tensor, independent of the gauge group. The general form of the 't Hooft tensor is computed.
Learning a perceptual manifold with deep features for animation video resequencing<|sep|>We propose a novel deep learning framework for animation video resequencing. Our system produces new video sequences by minimizing a perceptual distance of images from an existing animation video clip. To measure perceptual distance, we utilize the activations of convolutional neural networks and learn a perceptual distance by training these features on a small network with data comprised of human perceptual judgments. We show that with this perceptual metric and graph-based manifold learning techniques, our framework can produce new smooth and visually appealing animation video results for a variety of animation video styles. In contrast to previous work on animation video resequencing, the proposed framework applies to wide range of image styles and does not require hand-crafted feature extraction, background subtraction, or feature correspondence. In addition, we also show that our framework has applications to appealing arrange unordered collections of images.
Nonperturbative resonant strong field ionization of atomic hydrogen<|sep|>We investigate resonant strong field ionization of atomic hydrogen with respect to the 1s-2p-transition. By "strong" we understand that Rabi-periods are executed on a femtosecond time scale. Ionization and AC Stark shifts modify the bound state dynamics severely, leading to nonperturbative signatures in the photoelectron spectra. We introduce an analytical model, capable of predicting qualitative features in the photoelectron spectra such as the positions of the Autler-Townes peaks for modest field strengths. Ab initio solutions of the time-dependent Schroedinger equation show a pronounced shift and broadening of the left Autler-Townes peak as the field strength is increased. The right peak remains rather narrow and shifts less. This result is analyzed and explained with the help of exact AC Stark shifts and ionization rates obtained from Floquet theory. Finally, it is demonstrated that in the case of finite pulses as short as 20fs the Autler-Townes duplet can still be resolved. The fourth generation light sources under construction worldwide will provide bright, coherent radiation with photon energies ranging from a tenth of a meV up to tens of keV, hence covering the regime studied in the paper so that measurements of nonperturbative, relative AC Stark shifts should become feasible with these new light sources.
Relaxing door-to-door matching reduces passenger waiting times: a workflow for the analysis of driver GPS traces in a stochastic carpooling service<|sep|>Carpooling has the potential to transform itself into a mass transportation mode by abandoning its adherence to deterministic passenger-driver matching for door-to-door journeys, and by adopting instead stochastic matching on a network of fixed meeting points. Stochastic matching is where a passenger sends out a carpooling request at a meeting point, and then waits for the arrival of a self-selected driver who is already travelling to the requested meeting point. Crucially there is no centrally dispatched driver. Moreover, the carpooling is assured only between the meeting points, so the onus is on the passengers to travel to/from them by their own means. Thus the success of a stochastic carpooling service relies on the convergence, with minimal perturbation to their existing travel patterns, to the meeting points which are highly frequented by both passengers and drivers. Due to the innovative nature of stochastic carpooling, existing off-the-shelf workflows are largely insufficient for this purpose. To fill the gap in the market, we introduce a novel workflow, comprising of a combination of data science and GIS (Geographic Information Systems), to analyse driver GPS traces. We implement it for an operational stochastic carpooling service in south-eastern France, and we demonstrate that relaxing door-to-door matching reduces passenger waiting times. Our workflow provides additional key operational indicators, namely the driver flow maps, the driver flow temporal profiles and the driver participation rates.
Excitation spectra and wave functions of quasiparticle bound states in bilayer Rashba superconductors<|sep|>We study the excitation spectra and the wave functions of quasiparticle bound states at a vortex and an edge in bilayer Rashba superconductors under a magnetic field. In particular, we focus on the quasiparticle states at the zero energy in the pair-density wave state in a topologically non-trivial phase. We numerically demonstrate that the quasiparticle wave functions with zero energy are localized at both the edge and the vortex core if the magnetic field exceed the critical value.
W-+ H+- production and CP asymmetry at the LHC<|sep|>The dominant contributions to W-+ H+- production at the LHC are the tree-level b anti-b annihilation and the gg fusion. We perform for the case of the complex MSSM a complete calculation of the NLO EW corrections to the b anti-b annihilation channel and a consistent combination with other contributions including the standard and SUSY QCD corrections and the gg fusion, with resummation of the leading radiative corrections to the bottom-Higgs couplings and the neutral Higgs-boson propagators. We observe a large CP-violating asymmetry, arising mainly from the gg channel.
Guided Uncertainty-Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning<|sep|>Traditional robotic approaches rely on an accurate model of the environment, a detailed description of how to perform the task, and a robust perception system to keep track of the current state. On the other hand, reinforcement learning approaches can operate directly from raw sensory inputs with only a reward signal to describe the task, but are extremely sample-inefficient and brittle. In this work, we combine the strengths of model-based methods with the flexibility of learning-based methods to obtain a general method that is able to overcome inaccuracies in the robotics perception/actuation pipeline, while requiring minimal interactions with the environment. This is achieved by leveraging uncertainty estimates to divide the space in regions where the given model-based policy is reliable, and regions where it may have flaws or not be well defined. In these uncertain regions, we show that a locally learned-policy can be used directly with raw sensory inputs. We test our algorithm, Guided Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing peg insertion. Videos are available at https://sites.google.com/view/guapo-rl
Order Handling in Convergent Environments<|sep|>The rapid development of IT&T technology had big impact on the traditional telecommunications market, transforming it from monopolistic market to highly competitive high-tech market where new services are required to be created frequently. This paper aims to describe a design approach that puts order management process (as part of enterprise application integration) in function of rapid service creation. In the text we will present a framework for collaborative order handling supporting convergent services. The design splits the order handling processes in convergent environments in three business process groups: order capture, order management and order fulfillment. The paper establishes abstract framework for order handling and provides design guidelines for transaction handling implementation based on the checkpoint and inverse command strategy. The proposed design approach is based in a convergent telecommunication environment. Same principles are applicable in solving problems of collaboration in function of order processing in any given heterogeneous environment.
A novel black hole mimicker: a boson star and a global monopole nonminimally coupled to gravity<|sep|>A field-theoretic model for a highly compact object that mimicks a black hole is found for the gravitationally interacting system of a boson star and a global monopole which are nonminimally coupled to gravity. According to the strength of the nonlinear gravitational effects and the gravitational backreaction, three distinct coupling regimes are featured: weak, mild and strong. In the strong coupling regime we show that a repulsive monopole stabilizes an attractive boson star and the resulting configuration exhibits large energy density, large (and negative) principal pressures, large compactness, large effective potential, large local forces, and yet exhibits no event horizon. As such a composite system of a boson star and a global monopole represents a convincing microscopic candidate for a black hole mimicker.
The algebraic geometry of Harper operators<|sep|>Following an approach developed by Gieseker, Kn\"orrer and Trubowitz for discretized Schr\"odinger operators, we study the spectral theory of Harper operators in dimension two and one, as a discretized model of magnetic Laplacians, from the point of view of algebraic geometry. We describe the geometry of an associated family of Bloch varieties and compute their density of states. Finally, we also compute some spectral functions based on the density of states. We discuss the difference between the cases with rational or irrational parameters: for the two dimensional Harper operator, the compactification of the Bloch variety is an ordinary variety in the rational case and an ind-pro-variety in the irrational case. This gives rise, at the algebro-geometric level of Bloch varieties, to a phenomenon similar to the Hofstadter butterfly in the spectral theory. In dimension two, the density of states can be expressed in terms of period integrals over Fermi curves, where the resulting elliptic integrals are independent of the parameters. In dimension one, for the almost Mathieu operator, with a similar argument we find the usual dependence of the spectral density on the parameter, which gives rise to the well known Hofstadter butterfly picture.
Magnetic fields and spiral arms in the galaxy M51<|sep|>(Abridged) We use new multi-wavelength radio observations, made with the VLA and Effelsberg telescopes, to study the magnetic field of the nearby galaxy M51 on scales from $200\pc$ to several $\kpc$. Interferometric and single dish data are combined to obtain new maps at \wwav{3}{6} in total and polarized emission, and earlier \wav{20} data are re-reduced. We compare the spatial distribution of the radio emission with observations of the neutral gas, derive radio spectral index and Faraday depolarization maps, and model the large-scale variation in Faraday rotation in order to deduce the structure of the regular magnetic field. We find that the \wav{20} emission from the disc is severely depolarized and that a dominating fraction of the observed polarized emission at \wav{6} must be due to anisotropic small-scale magnetic fields. Taking this into account, we derive two components for the regular magnetic field in this galaxy: the disc is dominated by a combination of azimuthal modes, $m=0+2$, but in the halo only an $m=1$ mode is required to fit the observations. We disuss how the observed arm-interarm contrast in radio intensities can be reconciled with evidence for strong gas compression in the spiral shocks. The average arm--interam contrast, representative of the radii $r>2\kpc$ where the spiral arms are broader, is not compatible with straightforward compression: lower arm--interarm contrasts than expected may be due to resolution effects and \emph{decompression} of the magnetic field as it leaves the arms. We suggest a simple method to estimate the turbulent scale in the magneto-ionic medium from the dependence of the standard deviation of the observed Faraday rotation measure on resolution. We thus obtain an estimate of $50\pc$ for the size of the turbulent eddies.
Rock Climber Distance: Frogs versus Dogs<|sep|>The classical measure of similarity between two polygonal chains in Euclidean space is the Fr\'echet distance, which corresponds to the coordinated motion of two mobile agents along the chains while minimizing their maximum distance. As computing the Fr\'echet distance takes near-quadratic time under the Strong Exponential Time Hypothesis (SETH), we explore two new distance measures, called rock climber distance and $k$-station distance, in which the agents move alternately in their coordinated motion that traverses the polygonal chains. We show that the new variants are equivalent to the Fr\'echet or the Hausdorff distance if the number of moves is unlimited. When the number of moves is limited to a given parameter $k$, we show that it is NP-hard to determine the distance between two curves. We also describe a 2-approximation algorithm to find the minimum $k$ for which the distance drops below a given threshold.
Young Stars Near Earth: The Octans-Near Association and Castor Moving Group<|sep|>All cataloged stellar moving groups and associations with ages <100 Myr and within 100 pc of Earth have Galactic space motions (UVW) situated in a "good box" with dimensions ~20 km/s on a side. Torres et al. defined the Octans Association as a group of 15 stars with age "20 Myr?" and located ~140 pc from Earth, but with average V space velocity -3.6 km/s that is well outside of the good box. We present a list of 14 Hipparcos star systems within 100 pc of Earth that we call "Octans-Near"; these systems have UVW similar to those of the much more distant Octans Association. The Octans-Near stars have apparent ages between about 30 and 100 Myr and their relationship to the Octans Association stars is unclear. Six additional star systems have UVW similar to those of Octans-Near stars and likely ages <200 Myr. These six systems include the late-type binary star EQ Peg -- 6.2 pc from Earth with likely age <100 Myr and thus likely to be the nearest known pre-main sequence star system. The UVW of stars in a previously proposed ~200 Myr old Castor moving group are not too dissimilar from the UVW of Octans-Near stars. However, stars in the Castor group -- if it exists at all -- are mostly substantially older than 200 Myr and thus generally can readily be distinguished from the much younger Octans-Near stars.
Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering<|sep|>Spoken conversational question answering (SCQA) requires machines to model complex dialogue flow given the speech utterances and text corpora. Different from traditional text question answering (QA) tasks, SCQA involves audio signal processing, passage comprehension, and contextual understanding. However, ASR systems introduce unexpected noisy signals to the transcriptions, which result in performance degradation on SCQA. To overcome the problem, we propose CADNet, a novel contextualized attention-based distillation approach, which applies both cross-attention and self-attention to obtain ASR-robust contextualized embedding representations of the passage and dialogue history for performance improvements. We also introduce the spoken conventional knowledge distillation framework to distill the ASR-robust knowledge from the estimated probabilities of the teacher model to the student. We conduct extensive experiments on the Spoken-CoQA dataset and demonstrate that our approach achieves remarkable performance in this task.
Numerical simulation of the optimal two-mode attacks for two-way continuous-variable quantum cryptography in reverse reconciliation<|sep|>We analyze the security of the two-way continuous-variable quantum key distribution protocol in reverse reconciliation against general two-mode attacks, which represent all accessible attacks at fixed channel parameters. Rather than against one specific attack model, the expression of secret key rates of the two-way protocol are derived against all accessible attack models. It is found that there is an optimal two-mode attack to minimize the performance of the protocol in terms of both secret key rates and maximal transmission distances. We identify the optimal two-mode attack, give the specific attack model of the optimal two-mode attack and show the performance of the two-way protocol against the optimal two-mode attack. Even under the optimal two-mode attack, the performances of two-way protocol are still better than the corresponding one-way protocol, which shows the advantage of making a double use of the quantum channel and the potential of long-distance secure communication using two-way protocol.
UNIT project: Universe $N$-body simulations for the Investigation of Theoretical models from galaxy surveys<|sep|>We present the UNIT $N$-body cosmological simulations project, designed to provide precise predictions for nonlinear statistics of the galaxy distribution. We focus on characterizing statistics relevant to emission line and luminous red galaxies in the current and upcoming generation of galaxy surveys. We use a suite of precise particle mesh simulations (FastPM) as well as with full $N$-body calculations with a mass resolution of $\sim 1.2\times10^9\,h^{-1}$M$_{\odot}$ to investigate the recently suggested technique of Angulo & Pontzen 2016 to suppress the variance of cosmological simulations We study redshift space distortions, cosmic voids, higher order statistics from $z=2$ down to $z=0$. We find that both two- and three-point statistics are unbiased. Over the scales of interest for baryon acoustic oscillations and redshift-space distortions, we find that the variance is greatly reduced in the two-point statistics and in the cross correlation between halos and cosmic voids, but is not reduced significantly for the three-point statistics. We demonstrate that the accuracy of the two-point correlation function for a galaxy survey with effective volume of 20 ($h^{-1}$Gpc)$^3$ is improved by about a factor of 40, indicating that two pairs of simulations with a volume of 1 ($h^{-1}$Gpc)$^3$ lead to the equivalent variance of $\sim$150 such simulations. The $N$-body simulations presented here thus provide an effective survey volume of about seven times the effective survey volume of DESI or Euclid. The data from this project, including dark matter fields, halo catalogues, and their clustering statistics, are publicly available at http://www.unitsims.org.
Localized states in coupled Cahn-Hilliard equations<|sep|>The classical Cahn-Hilliard equation corresponds to a gradient dynamics model that describes phase decomposition in a binary mixture. In the spinodal region, an initially homogeneous state spontaneously decomposes via a large-scale instability into drop, hole or labyrinthine concentration patterns of a typical structure length followed by a continuously ongoing coarsening process. Here we consider coupled Cahn-Hilliard dynamics for two concentration fields and show that nonvariational (or active, or nonreciprocal) coupling may induce a small-scale (Turing) instability. At the corresponding primary bifurcation a branch of periodically patterned steady states emerges. Furthermore, there exist localized states that consist of patterned patches coexisting with a homogeneous background. The branches of steady parity-symmetric and -asymmetric localized states form a slanted homoclinic snaking structure typical for systems with a conservation law.
PIETS: Parallelised Irregularity Encoders for Forecasting with Heterogeneous Time-Series<|sep|>Heterogeneity and irregularity of multi-source data sets present a significant challenge to time-series analysis. In the literature, the fusion of multi-source time-series has been achieved either by using ensemble learning models which ignore temporal patterns and correlation within features or by defining a fixed-size window to select specific parts of the data sets. On the other hand, many studies have shown major improvement to handle the irregularity of time-series, yet none of these studies has been applied to multi-source data. In this work, we design a novel architecture, PIETS, to model heterogeneous time-series. PIETS has the following characteristics: (1) irregularity encoders for multi-source samples that can leverage all available information and accelerate the convergence of the model; (2) parallelised neural networks to enable flexibility and avoid information overwhelming; and (3) attention mechanism that highlights different information and gives high importance to the most related data. Through extensive experiments on real-world data sets related to COVID-19, we show that the proposed architecture is able to effectively model heterogeneous temporal data and outperforms other state-of-the-art approaches in the prediction task.
Cross-Correlation in cricket data and RMT<|sep|>We analyze cross-correlation between runs scored over a time interval in cricket matches of different teams using methods of random matrix theory (RMT). We obtain an ensemble of cross-correlation matrices $C$ from runs scored by eight cricket playing nations for (i) test cricket from 1877 -2014 (ii)one-day internationals from 1971 -2014 and (iii) seven teams participating in the Indian Premier league T20 format (2008-2014) respectively. We find that a majority of the eigenvalues of C fall within the bounds of random matrices having joint probability distribution $P(x_1...,x_n)=C_{N \beta} \, \prod_{j<k}w(x_j)| x_j-x_k |^\beta$ where $w(x)=x^{N\beta a}\exp(-N\beta b x)$ and $\beta$ is the Dyson parameter. The corresponding level density gives Marchenko-Pastur (MP) distribution while fluctuations of every participating team agrees with the universal behavior of Gaussian Unitary Ensemble (GUE). We analyze the components of the deviating eigenvalues and find that the largest eigenvalue corresponds to an influence common to all matches played during these periods.
Communication-efficient Decentralized Local SGD over Undirected Networks<|sep|>We consider the distributed learning problem where a network of $n$ agents seeks to minimize a global function $F$. Agents have access to $F$ through noisy gradients, and they can locally communicate with their neighbors a network. We study the Decentralized Local SDG method, where agents perform a number of local gradient steps and occasionally exchange information with their neighbors. Previous algorithmic analysis efforts have focused on the specific network topology (star topology) where a leader node aggregates all agents' information. We generalize that setting to an arbitrary network by analyzing the trade-off between the number of communication rounds and the computational effort of each agent. We bound the expected optimality gap in terms of the number of iterates $T$, the number of workers $n$, and the spectral gap of the underlying network. Our main results show that by using only $R=\Omega(n)$ communication rounds, one can achieve an error that scales as $O({1}/{nT})$, where the number of communication rounds is independent of $T$ and only depends on the number of agents. Finally, we provide numerical evidence of our theoretical results through experiments on real and synthetic data.
The Halo Boltzmann Equation<|sep|>Dark matter halos are the building blocks of the universe as they host galaxies and clusters. The knowledge of the clustering properties of halos is therefore essential for the understanding of the galaxy statistical properties. We derive an effective halo Boltzmann equation which can be used to describe the halo clustering statistics. In particular, we show how the halo Boltzmann equation encodes a statistically biased gravitational force which generates a bias in the peculiar velocities of virialized halos with respect to the underlying dark matter, as recently observed in N-body simulations.
Is Proxima Centauri b habitable? -- A study of atmospheric loss<|sep|>We address the important question of whether the newly discovered exoplanet, Proxima Centauri b (PCb), is capable of retaining an atmosphere over long periods of time. This is done by adapting a sophisticated multi-species MHD model originally developed for Venus and Mars, and computing the ion escape losses from PCb. The results suggest that the ion escape rates are about two orders of magnitude higher than the terrestrial planets of our Solar system if PCb is unmagnetized. In contrast, if the planet does have an intrinsic dipole magnetic field, the rates are lowered for certain values of the stellar wind dynamic pressure, but they are still higher than the observed values for our Solar system's terrestrial planets. These results must be interpreted with due caution, since most of the relevant parameters for PCb remain partly or wholly unknown.
Performance Evaluation of an Extrapolation Method for Ordinary Differential Equations with Error-free Transformation<|sep|>The application of error-free transformation (EFT) is recently being developed to solve ill-conditioned problems. It can reduce the number of arithmetic operations required, compared with multiple precision arithmetic, and also be applied by using functions supported by a well-tuned BLAS library. In this paper, we propose the application of EFT to explicit extrapolation methods to solve initial value problems of ordinary differential equations. Consequently, our implemented routines can be effective for large-sized linear ODE and small-sized nonlinear ODE, especially in the case when harmonic sequence is used.
Twisted self-duality for higher spin gauge fields and prepotentials<|sep|>We show that the equations of motion for (free) integer higher spin gauge fields can be formulated as twisted self-duality conditions on the higher spin curvatures of the spin-$s$ field and its dual. We focus on the case of four spacetime dimensions, but formulate our results in a manner applicable to higher spacetime dimensions. The twisted self-duality conditions are redundant and we exhibit a non-redundant subset of conditions, which have the remarkable property to involve only first-order derivatives with respect to time. This non-redundant subset equates the electric field of the spin-$s$ field (which we define) to the magnetic field of its dual (which we also define), and vice versa. The non-redundant subset of twisted self-duality conditions involve the purely spatial components of the spin-$s$ field and its dual, and also the components of the fields with one zero index. One can get rid of these gauge components by taking the curl of the equations, which does not change their physical content. In this form, the twisted self-duality conditions can be derived from a variational principle that involves prepotentials, which are the higher spin generalizations of the prepotentials previously found in the spins 2 and 3 cases. The prepotentials have again the intriguing feature of possessing both higher spin diffeomorphism invariance and higher spin conformal geometry. The tools introduced in an earlier paper for handling higher spin conformal geometry turn out to be crucial for streamlining the analysis. In four spacetime dimensions where the electric and magnetic fields are tensor fields of the same type, the twisted self-duality conditions enjoy an $SO(2)$ electric-magnetic invariance. We explicitly show that this symmetry is an "off-shell symmetry" (i.e., a symmetry of the action and not just of the equations of motion). Remarks on the extension to higher dimensions are given.
A Semidefinite Programming Based Search Strategy for Feature Selection with Mutual Information Measure<|sep|>Feature subset selection, as a special case of the general subset selection problem, has been the topic of a considerable number of studies due to the growing importance of data-mining applications. In the feature subset selection problem there are two main issues that need to be addressed: (i) Finding an appropriate measure function than can be fairly fast and robustly computed for high-dimensional data. (ii) A search strategy to optimize the measure over the subset space in a reasonable amount of time. In this article mutual information between features and class labels is considered to be the measure function. Two series expansions for mutual information are proposed, and it is shown that most heuristic criteria suggested in the literature are truncated approximations of these expansions. It is well-known that searching the whole subset space is an NP-hard problem. Here, instead of the conventional sequential search algorithms, we suggest a parallel search strategy based on semidefinite programming (SDP) that can search through the subset space in polynomial time. By exploiting the similarities between the proposed algorithm and an instance of the maximum-cut problem in graph theory, the approximation ratio of this algorithm is derived and is compared with the approximation ratio of the backward elimination method. The experiments show that it can be misleading to judge the quality of a measure solely based on the classification accuracy, without taking the effect of the non-optimum search strategy into account.
Hardware Fingerprinting for the ARINC 429 Avionic Bus<|sep|>ARINC 429 is the most common data bus in use today in civil avionics. However, the protocol lacks any form of source authentication. A technician with physical access to the bus is able to replace a transmitter by a rogue device, and the receivers will accept its malicious data as they have no method of verifying the authenticity of messages. Updating the protocol would close off security loopholes in new aircraft but would require thousands of airplanes to be modified. For the interim, until the protocol is replaced, we propose the first intrusion detection system that utilizes a hardware fingerprinting approach for sender identification for the ARINC 429 data bus. Our approach relies on the observation that changes in hardware, such as replacing a transmitter or a receiver with a rogue one, modify the electric signal of the transmission. Because we rely on the analog properties, and not on the digital content of the transmissions, we are able to detect a hardware switch as soon as it occurs, even if the data that is being transmitted is completely normal. Thus, we are able to preempt the attack before any damage is caused. In this paper we describe the design of our intrusion detection system and evaluate its performance against different adversary models. Our analysis includes both a theoretical Markov-chain model and an extensive empirical evaluation. For this purpose, we collected a data corpus of ARINC 429 data traces, which may be of independent interest since, to the best of our knowledge, no public corpus is available. We find that our intrusion detection system is quite realistic: e.g., it achieves near-zero false alarms per second, while detecting a rogue transmitter in under 50ms, and detecting a rogue receiver in under 3 seconds. In other words, technician attacks can be reliably detected during the pre-flight checks, well before the aircraft takes off.
Chandra HETG Observations of the Colliding Stellar Wind System WR 147<|sep|>We present an extended analysis of deep Chandra HETG observations of the WR+OB binary system WR 147 that was resolved into a double X-ray source (Zhekov & Park, 2010, ApJ, 709, L119). Our analysis of the profiles of strong emission lines shows that their centroids are blue-shifted in the spectrum of the northern X-ray source. We find no suppressed forbidden line in the He-like triplets which indicates that the X-ray emitting region is not located near enough to the stars in the binary system to be significantly affected by their UV radiation. The most likely physical picture that emerges from the entire set of HETG data suggests that the northern X-ray source can be associated with the colliding stellar wind region in the wide WR+OB binary system, while the X-rays of its southern counterpart, the WN8 star, are result from stellar wind shocking onto a close companion (a hypothesized third star in the system).
Style Transfer and Extraction for the Handwritten Letters Using Deep Learning<|sep|>How can we learn, transfer and extract handwriting styles using deep neural networks? This paper explores these questions using a deep conditioned autoencoder on the IRON-OFF handwriting data-set. We perform three experiments that systematically explore the quality of our style extraction procedure. First, We compare our model to handwriting benchmarks using multidimensional performance metrics. Second, we explore the quality of style transfer, i.e. how the model performs on new, unseen writers. In both experiments, we improve the metrics of state of the art methods by a large margin. Lastly, we analyze the latent space of our model, and we see that it separates consistently writing styles.
Rayleigh-Taylor instability in an ionized medium<|sep|>We study linear theory of the magnetized Rayleigh-Taylor instability in a system consisting of ions and neutrals. Both components are affected by a uniform vertical gravitational field. We consider ions and neutrals as two separate fluid systems where they can exchange momentum through collisions. However, ions have direct interaction with the magnetic field lines but neutrals are not affected by the field directly. The equations of our two-fluid model are linearized and by applying a set of proper boundary conditions, a general dispersion relation is derived for our two superposed fluids separated by a horizontal boundary. We found two unstable modes for a range of the wavenumbers. It seems that one of the unstable modes corresponds to the ions and the other one is for the neutrals. Both modes are reduced with increasing the collision rate of the particles and the ionization fraction. We show that if the two-fluid nature is considered, RT instability would not be suppressed and also show that the growth time of the perturbations increases. As an example, we apply our analysis to the Local clouds which seems to have arisen because of the RT instability. Assuming that the clouds are partially ionized, we find that the growth rate of these clouds increases in comparison to a fully ionized case.
Stellar Spectroscopy in the Near-infrared with a Laser Frequency Comb<|sep|>The discovery and characterization of exoplanets around nearby stars is driven by profound scientific questions about the uniqueness of Earth and our Solar System, and the conditions under which life could exist elsewhere in our Galaxy. Doppler spectroscopy, or the radial velocity (RV) technique, has been used extensively to identify hundreds of exoplanets, but with notable challenges in detecting terrestrial mass planets orbiting within habitable zones. We describe infrared RV spectroscopy at the 10 m Hobby-Eberly telescope that leverages a 30 GHz electro-optic laser frequency comb with nanophotonic supercontinuum to calibrate the Habitable Zone Planet Finder spectrograph. Demonstrated instrument precision <10 cm/s and stellar RVs approaching 1 m/s open the path to discovery and confirmation of habitable zone planets around M-dwarfs, the most ubiquitous type of stars in our Galaxy.
A Dynamic Parametric Wind Farm Model for Simulating Time-varying Wind Conditions and Floating Platform Motion<|sep|>This paper introduces a dynamic parametric wind farm model that is capable of simulating floating wind turbine platform motion coupled with wake transport under time-varying wind conditions. The simulator is named FOWFSim-Dyn as it is a dynamic extension of the previously developed steady-state Floating Offshore Wind Farm Simulator (FOWFSim). One-dimensional momentum conservation is used to model dynamic propagation of wake centerline locations and average velocities, while momentum recovery is approximated with the assumption of a constant temporal wake expansion rate. Platform dynamics are captured by treating a floating offshore wind farm as a distribution of particles that are subject to aerodynamic, hydrodynamic, and mooring line forces. The finite difference method is used to discretize the momentum conservation equations to yield a nonlinear state-space model. Simulated data are validated against steady-state experimental wind tunnel results obtained from the literature. Predictions of wake centerlines differed from experimental results by at most 8.19% of the rotor diameter. Simulated wake velocity profiles in the far-wake region differed from experimental measurements by less than 3.87% of the free stream wind speed. FOWFSim-Dyn thus possesses a satisfactory level of fidelity for engineering applications. Finally, dynamic simulations are conducted to ensure that time-varying predictions match physical expectations and intuition.
Evolution of faint radio sources in the VIDEO-XMM3 field<|sep|>It has been speculated that low luminosity radio-loud AGN have the potential to serve as an important source of AGN feedback, and may be responsible for suppressing star-formation activity in massive elliptical galaxies at late times. As such the cosmic evolution of these sources is vitally important to understanding the significance of such AGN feedback processes and their influence on the global star-formation history of the universe. In this paper we present a new investigation of the evolution of faint radio sources out to $z{\sim}2.5$. We combine a 1 square degree VLA radio survey, complete to a depth of 100 $\mu$Jy, with accurate 10 band photometric redshifts from the VIDEO and CFHTLS surveys. The results indicate that the radio population experiences mild positive evolution out to $z{\sim}1.2$ increasing their space density by a factor of $\sim$3, consistent with results of several previous studies. Beyond $z$=1.2 there is evidence of a slowing down of this evolution. Star-forming galaxies drive the more rapid evolution at low redshifts, $z{<}$1.2, while more slowly evolving AGN populations dominate at higher redshifts resulting in a decline in the evolution of the radio luminosity function at $z{>}$1.2. The evolution is best fit by pure luminosity evolution with star-forming galaxies evolving as $(1+z)^{2.47\pm0.12}$ and AGN as $(1+z)^{1.18\pm0.21}$.
Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks<|sep|>The occurrence of West Nile Virus (WNV) represents one of the most common mosquito-borne zoonosis viral infections. Its circulation is usually associated with climatic and environmental conditions suitable for vector proliferation and virus replication. On top of that, several statistical models have been developed to shape and forecast WNV circulation: in particular, the recent massive availability of Earth Observation (EO) data, coupled with the continuous advances in the field of Artificial Intelligence, offer valuable opportunities. In this paper, we seek to predict WNV circulation by feeding Deep Neural Networks (DNNs) with satellite images, which have been extensively shown to hold environmental and climatic features. Notably, while previous approaches analyze each geographical site independently, we propose a spatial-aware approach that considers also the characteristics of close sites. Specifically, we build upon Graph Neural Networks (GNN) to aggregate features from neighbouring places, and further extend these modules to consider multiple relations, such as the difference in temperature and soil moisture between two sites, as well as the geographical distance. Moreover, we inject time-related information directly into the model to take into account the seasonality of virus spread. We design an experimental setting that combines satellite images - from Landsat and Sentinel missions - with ground truth observations of WNV circulation in Italy. We show that our proposed Multi-Adjacency Graph Attention Network (MAGAT) consistently leads to higher performance when paired with an appropriate pre-training stage. Finally, we assess the importance of each component of MAGAT in our ablation studies.
AB/BA analysis: A framework for estimating keyword spotting recall improvement while maintaining audio privacy<|sep|>Evaluation of keyword spotting (KWS) systems that detect keywords in speech is a challenging task under realistic privacy constraints. The KWS is designed to only collect data when the keyword is present, limiting the availability of hard samples that may contain false negatives, and preventing direct estimation of model recall from production data. Alternatively, complementary data collected from other sources may not be fully representative of the real application. In this work, we propose an evaluation technique which we call AB/BA analysis. Our framework evaluates a candidate KWS model B against a baseline model A, using cross-dataset offline decoding for relative recall estimation, without requiring negative examples. Moreover, we propose a formulation with assumptions that allow estimation of relative false positive rate between models with low variance even when the number of false positives is small. Finally, we propose to leverage machine-generated soft labels, in a technique we call Semi-Supervised AB/BA analysis, that improves the analysis time, privacy, and cost. Experiments with both simulation and real data show that AB/BA analysis is successful at measuring recall improvement in conjunction with the trade-off in relative false positive rate.
ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation<|sep|>Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithm (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks: 1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re2), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re2 is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual linear policy representations. The state representation conveys expressive common features of the environment learned by all the agents collectively; the linear policy representation provides a favorable space for efficient policy optimization, where novel behavior-level crossover and mutation operations can be performed. Moreover, the linear policy representation allows convenient generalization of policy fitness with the help of Policy-extended Value Function Approximator (PeVFA), further improving the sample efficiency of fitness estimation. The experiments on a range of continuous control tasks show that ERL-Re2 consistently outperforms strong baselines and achieves significant improvement over both its Deep RL and EA components.
Brownian motion of solitons in a Bose-Einstein Condensate<|sep|>For the first time, we observed and controlled the Brownian motion of solitons. We launched solitonic excitations in highly elongated $^{87}\rm{Rb}$ BECs and showed that a dilute background of impurity atoms in a different internal state dramatically affects the soliton. With no impurities and in one-dimension (1-D), these solitons would have an infinite lifetime, a consequence of integrability. In our experiment, the added impurities scatter off the much larger soliton, contributing to its Brownian motion and decreasing its lifetime. We describe the soliton's diffusive behavior using a quasi-1-D scattering theory of impurity atoms interacting with a soliton, giving diffusion coefficients consistent with experiment.
Exploiters-Based Knowledge Extraction in Object-Oriented Knowledge Representation<|sep|>This paper contains the consideration of knowledge extraction mechanisms of such object-oriented knowledge representation models as frames, object-oriented programming and object-oriented dynamic networks. In addition, conception of universal exploiters within object-oriented dynamic networks is also discussed. The main result of the paper is introduction of new exploiters-based knowledge extraction approach, which provides generation of a finite set of new classes of objects, based on the basic set of classes. The methods for calculation of quantity of new classes, which can be obtained using proposed approach, and of quantity of types, which each of them describes, are proposed. Proof that basic set of classes, extended according to proposed approach, together with union exploiter create upper semilattice is given. The approach always allows generating of finitely defined set of new classes of objects for any object-oriented dynamic network. A quantity of these classes can be precisely calculated before the generation. It allows saving of only basic set of classes in the knowledge base.
Structural instabilities and sequence of phase transitions in SrBi$_2$Ta$_2$O$_9$ and SrBi$_2$Nb$_2$O$_9$ from first principles and Monte Carlo simulations<|sep|>Despite their structural similarities, SrBi$_2$Ta$_2$O$_9$ (SBT) and SrBi$_2$Nb$_2$O$_9$ (SBN) undergo a different sequence of phase transitions. The phase diagram of SBT as a function of the temperature includes an intermediate phase between the high-temperature phase and the ferroelectric ground state, while in the Niobium compound the intermediate phase is suppressed and a single transition between the high- and low temperature structures is observed. We present \emph{ab initio} calculations that reveal the relevance of a trilinear coupling between three symmetry-adapted modes to stabilize the ground sate in both compounds, being this coupling much stronger in SBN. Within the framework of the phenomenological Landau theory, it is shown that by solely increasing the strength of the trilinear coupling the topology of the phase diagram of SBT can change up to suppress the intermediate phase. Monte Carlo simulations on an idealized $\phi^4$ Hamiltonian confirm that the trilinear coupling is the key parameter that determines the sequence of phase transitions and that for higher dimensionality of the order parameters the stability region of the intermediate phase is narrower.
A General Optimization-based Framework for Global Pose Estimation with Multiple Sensors<|sep|>Accurate state estimation is a fundamental problem for autonomous robots. To achieve locally accurate and globally drift-free state estimation, multiple sensors with complementary properties are usually fused together. Local sensors (camera, IMU, LiDAR, etc) provide precise pose within a small region, while global sensors (GPS, magnetometer, barometer, etc) supply noisy but globally drift-free localization in a large-scale environment. In this paper, we propose a sensor fusion framework to fuse local states with global sensors, which achieves locally accurate and globally drift-free pose estimation. Local estimations, produced by existing VO/VIO approaches, are fused with global sensors in a pose graph optimization. Within the graph optimization, local estimations are aligned into a global coordinate. Meanwhile, the accumulated drifts are eliminated. We evaluate the performance of our system on public datasets and with real-world experiments. Results are compared against other state-of-the-art algorithms. We highlight that our system is a general framework, which can easily fuse various global sensors in a unified pose graph optimization. Our implementations are open source\footnote{https://github.com/HKUST-Aerial-Robotics/VINS-Fusion}.
3D Room Layout Estimation from a Cubemap of Panorama Image via Deep Manhattan Hough Transform<|sep|>Significant geometric structures can be compactly described by global wireframes in the estimation of 3D room layout from a single panoramic image. Based on this observation, we present an alternative approach to estimate the walls in 3D space by modeling long-range geometric patterns in a learnable Hough Transform block. We transform the image feature from a cubemap tile to the Hough space of a Manhattan world and directly map the feature to the geometric output. The convolutional layers not only learn the local gradient-like line features, but also utilize the global information to successfully predict occluded walls with a simple network structure. Unlike most previous work, the predictions are performed individually on each cubemap tile, and then assembled to get the layout estimation. Experimental results show that we achieve comparable results with recent state-of-the-art in prediction accuracy and performance. Code is available at https://github.com/Starrah/DMH-Net.
Implementation and Analysis of QUIC for MQTT<|sep|>Transport and security protocols are essential to ensure reliable and secure communication between two parties. For IoT applications, these protocols must be lightweight, since IoT devices are usually resource constrained. Unfortunately, the existing transport and security protocols -- namely TCP/TLS and UDP/DTLS -- fall short in terms of connection overhead, latency, and connection migration when used in IoT applications. In this paper, after studying the root causes of these shortcomings, we show how utilizing QUIC in IoT scenarios results in a higher performance. Based on these observations, and given the popularity of MQTT as an IoT application layer protocol, we integrate MQTT with QUIC. By presenting the main APIs and functions developed, we explain how connection establishment and message exchange functionalities work. We evaluate the performance of MQTTw/QUIC versus MQTTw/TCP using wired, wireless, and long-distance testbeds. Our results show that MQTTw/QUIC reduces connection overhead in terms of the number of packets exchanged with the broker by up to 56%. In addition, by eliminating half-open connections, MQTTw/QUIC reduces processor and memory usage by up to 83% and 50%, respectively. Furthermore, by removing the head-of-line blocking problem, delivery latency is reduced by up to 55%. We also show that the throughput drops experienced by MQTTw/QUIC when a connection migration happens is considerably lower than that of MQTTw/TCP.
Higgs production in gluon fusion beyond NNLO<|sep|>We construct an approximate expression for the cross section for Higgs production in gluon fusion at next-to-next-to-next-to-leading order (N$^3$LO) in $\alpha_s$ with finite top mass. We argue that an accurate approximation can be constructed by exploiting the analiticity of the Mellin space cross section, and the information on its singularity structure coming from large N (soft gluon, Sudakov) and small N (high energy, BFKL) all order resummations. We support our argument with an explicit comparison of the approximate and the exact expressions up to the highest (NNLO) order at which the latter are available. We find that the approximate N$^3$LO result amounts to a correction of 16% to the NNLO QCD cross section for production of a 125 GeV Higgs at the LHC (8 TeV), larger than previously estimated, and it significantly reduces the scale dependence of the NNLO result.
Quench dynamics of Fano-like resonances in the presence of the on-dot superconducting pairing<|sep|>We explore the electron dynamics of a system composed of double quantum dot embedded between metallic and superconducting leads in a T-shape geometry. In nanoscopic systems, where electron transfer between electrodes can be realized via different paths, interference effects play an important role. For double quantum dot system in the chosen geometry, interference of electrons transferred between electrodes via the interfacial quantum dot and electrons scattered on the side dot gives rise to Fano-like interference. If such a system is additionally coupled to a superconducting electrode, together with the well-understood Fano resonance an additional resonance appears on the opposite side of the Fermi level. In the recent work [Sci. Rep. 10, 2881 (2020)], we showed that this resonance occurs solely as a result of the local pairing of non-scattered electrons with scattered ones. In this work, considering the quench dynamics, we explore how much time is required for formation of each of these resonances. In particular, (i) we analyze the charge oscillations between subsystems; (ii) we estimate the time required for each resonance to achieve stable equilibrium upon an abrupt change of interdot connection; (iii) we discuss a typical energy and time scales for experiments on similar architectures.
Generalized conformal realizations of Kac-Moody algebras<|sep|>We present a construction which associates an infinite sequence of Kac-Moody algebras, labeled by a positive integer n, to one single Jordan algebra. For n=1, this reduces to the well known Kantor-Koecher-Tits construction. Our generalization utilizes a new relation between different generalized Jordan triple systems, together with their known connections to Jordan and Lie algebras. Applied to the Jordan algebra of hermitian 3x3 matrices over the division algebras R, C, H, O, the construction gives the exceptional Lie algebras f4, e6, e7, e8 for n=2. Moreover, we obtain their infinite-dimensional extensions for n greater or equal to 3. In the case of 2x2 matrices the resulting Lie algebras are of the form so(p+n,q+n) and the concomitant nonlinear realization generalizes the conformal transformations in a spacetime of signature (p,q).
Phenomenological description of the vortex density in rotating BEC superfluids<|sep|>We propose a phenomenological equation for the vortex line density in rotating Bose-Einstein condensates as a function of the angular speed. This equation provides a simple description of the gross features of the increase in vortex number from the appearance of the first vortex to the theoretical rigid-body result for high vortex density, and allows one to compare with analogous situations in superfluid helium, after the suitable changes in the relevant parameters are made.
Generalized sine-Gordon models and quantum braided groups<|sep|>We determine the quantized function algebras associated with various examples of generalized sine-Gordon models. These are quadratic algebras of the general Freidel-Maillet type, the classical limits of which reproduce the lattice Poisson algebra recently obtained for these models defined by a gauged Wess-Zumino-Witten action plus an integrable potential. More specifically, we argue based on these examples that the natural framework for constructing quantum lattice integrable versions of generalized sine-Gordon models is that of affine quantum braided groups.
Semidefinite Relaxations for Best Rank-1 Tensor Approximations<|sep|>This paper studies the problem of finding best rank-1 approximations for both symmetric and nonsymmetric tensors. For symmetric tensors, this is equivalent to optimizing homogeneous polynomials over unit spheres; for nonsymmetric tensors, this is equivalent to optimizing multi-quadratic forms over multi-spheres. We propose semidefinite relaxations, based on sum of squares representations, to solve these polynomial optimization problems. Their properties and structures are studied. In applications, the resulting semidefinite programs are often large scale. The recent Newton-CG augmented Lagrangian method by Zhao, Sun and Toh is suitable for solving these semidefinite relaxations. Extensive numerical experiments are presented to show that this approach is practical in getting best rank-1 approximations.
Intuitionistic Logic is a Connexive Logic<|sep|>We show that intuitionistic logic is deductively equivalent to Connexive Heyting Logic (CHL), hereby introduced as an example of a strong connexive logic with intuitive semantics. We use the reverse algebraisation paradigm: CHL is presented as the assertional logic of a point regular variety (whose structure theory is examined in detail) that turns out to be term equivalent to the variety of Heyting algebras. We provide Hilbert-style and Gentzen-style proof systems for CHL; moreover, we suggest a possible computational interpretation of its connexive conditional, and we revisit Kapsner's idea of superconnexivity.
GAP: Generalizable Approximate Graph Partitioning Framework<|sep|>Graph partitioning is the problem of dividing the nodes of a graph into balanced partitions while minimizing the edge cut across the partitions. Due to its combinatorial nature, many approximate solutions have been developed, including variants of multi-level methods and spectral clustering. We propose GAP, a Generalizable Approximate Partitioning framework that takes a deep learning approach to graph partitioning. We define a differentiable loss function that represents the partitioning objective and use backpropagation to optimize the network parameters. Unlike baselines that redo the optimization per graph, GAP is capable of generalization, allowing us to train models that produce performant partitions at inference time, even on unseen graphs. Furthermore, because we learn the representation of the graph while jointly optimizing for the partitioning loss function, GAP can be easily tuned for a variety of graph structures. We evaluate the performance of GAP on graphs of varying sizes and structures, including graphs of widely used machine learning models (e.g., ResNet, VGG, and Inception-V3), scale-free graphs, and random graphs. We show that GAP achieves competitive partitions while being up to 100 times faster than the baseline and generalizes to unseen graphs.
Standard Model Higgs Combination from CMS with up to 1.7 fb-1 of data<|sep|>The combination is presented of searches for a standard model (SM) Higgs boson in eight decay modes: H->gamma,gamma, H->tau,tau, H->bb, H->WW->2l2nu, H->ZZ->4l, H->ZZ->2l2tau, H->ZZ->2l2nu, and H->ZZ->2l2q. The searches were performed by the CMS Collaboration using 1.1-1.7 fb-1 of integrated luminosity, depending on the analysis. No excess compatible with a SM Higgs signal has been observed; the largest excursion of the observed data from the expected background has a probability of 0.4 after taking into account the look-elsewhere effect. The SM Higgs boson is excluded at 95% C.L. in three mass ranges 145-216, 226-288, and 310-400 GeV/c2, while the expected exclusion range is 130-440 GeV/c2.
Method for generating two coupled Gaussian stochastic processes<|sep|>Most processes in nature are coupled; however, extensive null models for generating such processes still lacks. We present a new method to generate two coupled Gaussian stochastic processes with arbitrary correlation functions. This method is developed by modifying the Fourier filtering method. The robustness of this method is proved by generating two coupled fractional Brownian motions and extending its range of application to Gaussian random fields.
ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues<|sep|>Medical dialogue systems (MDSs) aim to assist doctors and patients with a range of professional medical services, i.e., diagnosis, treatment and consultation. The development of MDSs is hindered because of a lack of resources. In particular. (1) there is no dataset with large-scale medical dialogues that covers multiple medical services and contains fine-grained medical labels (i.e., intents, actions, slots, values), and (2) there is no set of established benchmarks for MDSs for multi-domain, multi-service medical dialogues. In this paper, we present ReMeDi, a set of resource for medical dialogues. ReMeDi consists of two parts, the ReMeDi dataset and the ReMeDi benchmarks. The ReMeDi dataset contains 96,965 conversations between doctors and patients, including 1,557 conversations with fine-gained labels. It covers 843 types of diseases, 5,228 medical entities, and 3 specialties of medical services across 40 domains. To the best of our knowledge, the ReMeDi dataset is the only medical dialogue dataset that covers multiple domains and services, and has fine-grained medical labels. The second part of the ReMeDi resources consists of a set of state-of-the-art models for (medical) dialogue generation. The ReMeDi benchmark has the following methods: (1) pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5) trained, validated, and tested on the ReMeDi dataset, and (2) a self-supervised contrastive learning(SCL) method to expand the ReMeDi dataset and enhance the training of the state-of-the-art pretrained models. We describe the creation of the ReMeDi dataset, the ReMeDi benchmarking methods, and establish experimental results using the ReMeDi benchmarking methods on the ReMeDi dataset for future research to compare against. With this paper, we share the dataset, implementations of the benchmarks, and evaluation scripts.
Adversarial Example Detection by Classification for Deep Speech Recognition<|sep|>Machine Learning systems are vulnerable to adversarial attacks and will highly likely produce incorrect outputs under these attacks. There are white-box and black-box attacks regarding to adversary's access level to the victim learning algorithm. To defend the learning systems from these attacks, existing methods in the speech domain focus on modifying input signals and testing the behaviours of speech recognizers. We, however, formulate the defense as a classification problem and present a strategy for systematically generating adversarial example datasets: one for white-box attacks and one for black-box attacks, containing both adversarial and normal examples. The white-box attack is a gradient-based method on Baidu DeepSpeech with the Mozilla Common Voice database while the black-box attack is a gradient-free method on a deep model-based keyword spotting system with the Google Speech Command dataset. The generated datasets are used to train a proposed Convolutional Neural Network (CNN), together with cepstral features, to detect adversarial examples. Experimental results show that, it is possible to accurately distinct between adversarial and normal examples for known attacks, in both single-condition and multi-condition training settings, while the performance degrades dramatically for unknown attacks. The adversarial datasets and the source code are made publicly available.
Numerical approximation of the 3d hydrostatic Navier-Stokes system with free surface<|sep|>In this paper we propose a stable and robust strategy to approximate the 3d incompressible hydrostatic Euler and Navier-Stokes systems with free surface. Compared to shallow water approximation of the Navier-Stokes system, the idea is to use a Galerkin type approximation of the velocity field with piecewise constant basis functions in order to obtain an accurate description of the vertical profile of the horizontal velocity. Such a strategy has several advantages. It allows - to rewrite the Navier-Stokes equations under the form of a system of conservation lawswith source terms, - the easy handling of the free surface, which does not require moving meshes, - the possibility to take advantage of robust and accurate numerical techniques developed in extensive amount for Shallow Water type systems. Compared to previous works of some of the authors, the three dimensional case is studied in this paper. We show that the model admits a kinetic interpretation including the vertical exchanges terms, and we use this result to formulate a robust finite volume scheme for its numerical approximation. All the aspects of the discrete scheme (fluxes, boundary conditions,. . . ) are completely described and the stability properties of the proposed numerical scheme (well-balancing, positivity of the water depth,. . . ) are discussed. We validate the model and the discrete scheme with some numerical academic examples (3d non stationary analytical solutions) and illustratethe capability of the discrete model to reproduce realistic tsunami waves.
A survey on Blockchain-based applications for reforming data protection, privacy and security<|sep|>The modern society, economy and industry have been changed remarkably by many cutting-edge technologies over the last years, and many more are in development and early implementation that will in turn led even wider spread of adoptions and greater alteration. Blockchain technology along with other rising ones is expected to transform virtually every aspect of global business and individuals' lifestyle in some areas. It has been spreading with multi-sector applications from financial services to healthcare, supply chain, and cybersecurity emerging every passing day. Simultaneously, in the digital world, data protection and privacy are the most enormous issues which customers, companies and policymakers also take seriously into consideration due to the recent increase of security breaches and surveillance in reported incidents. In this case, blockchain has the capability and potential to revolutionize trust, security and privacy of individual data in the online world. Hence, the purpose of this paper is to study the actual cases of Blockchain applied in the reformation of privacy and security field by discussing its impacts as well as the opportunities and challenges.
Anomalous excitons and screenings unveiling strong electronic correlations in SrTi$_{1-x}$Nb$_x$O$_3$, 0$\leq$x$\leq$0.005<|sep|>Electron-electron (e-e) and electron-hole (e-h) interactions are often associated with many exotic phenomena in correlated electron systems. Here, we report an observation of anomalous excitons at 3.75 , 4.67 and 6.11 eV at 4.2 K in bulk-SrTiO$_3$. Fully supported by ab initio GW Bethe-Salpeter equation calculations, these excitons are due to surprisingly strong e-h and e-e interactions with different characters: 4.67 and 6.11 eV are resonant excitons and 3.75 eV is a bound Wannier-like exciton with an unexpectedly higher level of delocalization. Measurements and calculations on SrTi$_{1-x}$Nb$_x$O$_3$ for 0.0001$\leq$x$\leq$0.005 further show that energy and spectral-weight of the excitonic peaks vary as a function of electron doping (x) and temperature, which are attributed to screening effects. Our results show the importance of e-h and e-e interactions yielding to anomalous excitons and thus bring out a new fundamental perspective in SrTiO$_3$.
Exchanging quantum correlations and non-local information between three qubit-syatem<|sep|>The possibility of exchanging the quantum correlations and the non-local information between three qubits interact directly or indirectly via Dzyaloshinskii-Moriya (DM)is discussed. The initial state settings and the interaction strength represent control parameters on the exchanging phenomena. The non-local information that encoded on the different partitions doesn't exceed the initial one. It is shown that, the ability of DM interaction to generate entanglement is larger than that displayed for the dipole interaction. The possibility of maximizing the quantum correlations between the three qubits increases as one increase the strength of interaction and starting with large initial quantum correlations. The long-lived quantum correlations could be achieved by controlling the strength of the dipole interaction.
Structure effect on one neutron removal reaction using relativistic mean field densities in Glauber model<|sep|>We calculate the one neutron removal reaction cross-section $(\sigma_{-1n})$ for few stable and neutron-rich halo nuclei with $^{12}$C as target, using relativistic mean field (RMF) densities, in the frame work of Glauber model.The results are compared with the experimental data. Study of the stable nuclei with the deformed densities have shown a good agreement with the data,however, it differs significantly for the halo nuclei. We observe that while estimating the $\sigma_{-1n}$value from the difference of reaction cross-section of two neighboring nuclei with mass number A and that of A-1 in an isotopic chain, we get good agreement with the known experimental data for the halo cases.
Measuring the spins of heavy binary black holes<|sep|>An accurate and precise measurement of the spins of individual merging black holes is required to understand their origin. While previous studies have indicated that most of the spin information comes from the inspiral part of the signal, the informative spin measurement of the heavy binary black hole system GW190521 suggests that the merger and ringdown can contribute significantly to the spin constraints for such massive systems. We perform a systematic study into the measurability of the spin parameters of individual heavy binary black hole mergers using a numerical relativity surrogate waveform model including the effects of both spin-induced precession and higher-order modes. We find that the spin measurements are driven by the merger and ringdown parts of the signal for GW190521-like systems, but the uncertainty in the measurement increases with the total mass of the system. We are able to place meaningful constraints on the spin parameters even for systems observed at moderate signal-to-noise ratios, but the measurability depends on the exact six-dimensional spin configuration of the system. Finally, we find that the azimuthal angle between the in-plane projections of the component spin vectors at a given reference frequency cannot be well-measured for most of our simulated configurations even for signals observed with high signal-to-noise ratios.
An empirical study of Linespots: A novel past-fault algorithm<|sep|>This paper proposes the novel past-faults fault prediction algorithm Linespots, based on the Bugspots algorithm. We analyze the predictive performance and runtime of Linespots compared to Bugspots with an empirical study using the most significant self-built dataset as of now, including high-quality samples for validation. As a novelty in fault prediction, we use Bayesian data analysis and Directed Acyclic Graphs to model the effects. We found consistent improvements in the predictive performance of Linespots over Bugspots for all seven evaluation metrics. We conclude that Linespots should be used over Bugspots in all cases where no real-time performance is necessary.
A Short Travel for Neutrinos in Large Extra Dimensions<|sep|>Neutrino oscillations successfully explain the flavor transitions observed in neutrinos produced in natural sources like the center of the sun and the earth atmosphere, and also from man-made sources like reactors and accelerators. These oscillations are driven by two mass-squared differences, solar and atmospheric, at the sub-eV scale. However, longstanding anomalies at short-baselines might imply the existence of new oscillation frequencies at the eV-scale and the possibility of this sterile state(s) to mix with the three active neutrinos. One of the many future neutrino programs that are expected to provide a final word on this issue is the Short-Baseline Neutrino Program (SBN) at FERMILAB. In this letter, we consider a specific model of Large Extra Dimensions (LED) which provides interesting signatures of oscillation of extra sterile states. We started re-creating sensitivity analyses for sterile neutrinos in the 3+1 scenario, previously done by the SBN collaboration, by simulating neutrino events in the three SBN detectors from both muon neutrino disappearance and electron neutrino appearance. Then, we implemented neutrino oscillations as predicted in the LED model and also we have performed sensitivity analysis to the LED parameters. Finally, we studied the SBN power of discriminating between the two models, the 3+1 and the LED. We have found that SBN is sensitive to the oscillations predicted in the LED model and have the potential to constrain the LED parameter space better than any other oscillation experiment, for $m_{1}^D<0.1\,\text{eV}$. In case SBN observes a departure from the three active neutrino framework, it also has the power of discriminating between sterile oscillations predicted in the 3+1 framework and the LED ones.
First-principles calculations of graphene nanoribbons in gaseous environments: Structural and electronic properties<|sep|>The stability of graphene nanoribbons in the presence of typical atmospheric molecules is systematically investigated by means of density functional theory. We calculate the edge formation free energy of five different edge configurations passivated by H, H$_2$, O, O$_2$, N$_2$, CO, CO$_2$, and H$_2$O, respectively. In addition to the well known hydrogen passivated armchair and zig-zag edges, we find the zig-zag edge saturated by oxygen atoms to be particularly stable under atmospheric conditions. Saturation by oxygen leads to the formation of metallic states strictly localized on the oxygen atoms. Finally, the vibrational spectrum of the hydrogen and oxygen passivated ribbons are calculated and compared.
Mitigating Docker Security Issues<|sep|>Docker offers an ecosystem that offers a platform for application packaging, distributing, and managing within containers. However, the Docker platform has not yet matured. Presently, Docker is less secured than virtual machines (VM) and most of the other cloud technologies. The key to Dockers inadequate security protocols is container sharing of Linux kernel, which can lead to the risk of privileged escalations. This research will outline some significant security vulnerabilities at Docker and counter solutions to neutralize such attacks. There are a variety of security attacks like insider and outsider. This research will outline both types of attacks and their mitigations strategies. Taking some precautionary measures can save from massive disasters. This research will also present Docker secure deployment guidelines. These guidelines will suggest different configurations to deploy Docker containers in a more secure way.
Assured Neural Network Architectures for Control and Identification of Nonlinear Systems<|sep|>In this paper, we consider the problem of automatically designing a Rectified Linear Unit (ReLU) Neural Network (NN) architecture (number of layers and number of neurons per layer) with the assurance that it is sufficiently parametrized to control a nonlinear system; i.e. control the system to satisfy a given formal specification. This is unlike current techniques, which provide no assurances on the resultant architecture. Moreover, our approach requires only limited knowledge of the underlying nonlinear system and specification. We assume only that the specification can be satisfied by a Lipschitz-continuous controller with a known bound on its Lipschitz constant; the specific controller need not be known. From this assumption, we bound the number of affine functions needed to construct a Continuous Piecewise Affine (CPWA) function that can approximate any Lipschitz-continuous controller that satisfies the specification. Then we connect this CPWA to a NN architecture using the authors' recent results on the Two-Level Lattice (TLL) NN architecture; the TLL architecture was shown to be parameterized by the number of affine functions present in the CPWA function it realizes.
Heterogeneous Anomaly Detection for Software Systems via Attentive Multi-modal Learning<|sep|>Prompt and accurate detection of system anomalies is essential to ensure the reliability of software systems. Unlike manual efforts that exploit all available run-time information, existing approaches usually leverage only a single type of monitoring data (often logs or metrics) or fail to make effective use of the joint information among multi-source data. Consequently, many false predictions occur. To better understand the manifestations of system anomalies, we conduct a comprehensive empirical study based on a large amount of heterogeneous data, i.e., logs and metrics. Our study demonstrates that system anomalies could manifest distinctly in different data types. Thus, integrating heterogeneous data can help recover the complete picture of a system's health status. In this context, we propose HADES, the first work to effectively identify system anomalies based on heterogeneous data. Our approach employs a hierarchical architecture to learn a global representation of the system status by fusing log semantics and metric patterns. It captures discriminative features and meaningful interactions from multi-modal data via a novel cross-modal attention module, enabling accurate system anomaly detection. We evaluate HADES extensively on large-scale simulated and industrial datasets. The experimental results present the superiority of HADES in detecting system anomalies on heterogeneous data. We release the code and the annotated dataset for reproducibility and future research.
Chemical history of isolated dwarf galaxies of the Local Group: I. dSphs: Cetus and Tucana<|sep|>For the first time, we obtain chemical evolution models (CEMs) for Tucana and Cetus, two isolated dwarf spheroidal galaxies (dSphs) of the Local Group. The CEMs have been built from the star formation histories (SFHs) and the metallicity histories, both obtained independently by the LCID project from deep color-magnitude dia- grams. Based on our models, we find that the chemical histories were complex and can be divided into different epochs and scenarios. In particular, during 75 percent of the SFH, the galaxies behaved as closed boxes and, during the remaining 25 percent, either received a lot of primordial gas by accretion or they lost metals through metal-rich winds. In order to discriminate between these two scenarios, abundances ratios in old stars are needed. At t approximately 4.5 Gyr, the galaxies lost most of their gas due to a short-strong, well-mixed wind. We obtain very similar CEMs for both galaxies, although Cetus is twice as massive as Tucana. We conclude that the star formation in both galaxies began with only 1.5 percent of the baryonic mass fraction predicted by lambda CDM.
NG2C: Pretenuring N-Generational GC for HotSpot Big Data Applications<|sep|>Big Data applications suffer from unpredictable and unacceptably high pause times due to Garbage Collection (GC). This is the case in latency-sensitive applications such as on-line credit-card fraud detection, graph-based computing for analysis on social networks, etc. Such pauses compromise latency requirements of the whole application stack and result from applications' aggressive buffering/caching of data, exposing an ill-suited GC design, which assumes that most objects will die young and does not consider that applications hold large amounts of middle-lived data in memory. To avoid such pauses, we propose NG2C, a new GC algorithm that combines pretenuring with an N-Generational heap. By being able to allocate objects into different generations, NG2C is able to group objects with similar lifetime profiles in the same generation. By allocating objects with similar lifetime profiles close to each other, i.e. in the same generation, we avoid object promotion (copying between generations) and heap fragmentation (which leads to heap compactions) both responsible for most of the duration of HotSpot GC pause times. NG2C is implemented for the OpenJDK 8 HotSpot Java Virtual Machine, as an extension of the Garbage First GC. We evaluate NG2C using Cassandra, Lucene, and GraphChi with three different GCs: Garbage First (G1), Concurrent Mark Sweep (CMS), and NG2C. Results show that NG2C decreases the worst observable GC pause time by up to 94.8% for Cassandra, 85.0% for Lucene and 96.45% for GraphChi, when compared to current collectors (G1 and CMS). In addition, NG2C has no negative impact on application throughput or memory usage.
LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting<|sep|>Most NER methods rely on extensive labeled data for model training, which struggles in the low-resource scenarios with limited training data. Existing dominant approaches usually suffer from the challenge that the target domain has different label sets compared with a resource-rich source domain, which can be concluded as class transfer and domain transfer. In this paper, we propose a lightweight tuning paradigm for low-resource NER via pluggable prompting (LightNER). Specifically, we construct the unified learnable verbalizer of entity categories to generate the entity span sequence and entity categories without any label-specific classifiers, thus addressing the class transfer issue. We further propose a pluggable guidance module by incorporating learnable parameters into the self-attention layer as guidance, which can re-modulate the attention and adapt pre-trained weights. Note that we only tune those inserted module with the whole parameter of the pre-trained language model fixed, thus, making our approach lightweight and flexible for low-resource scenarios and can better transfer knowledge across domains. Experimental results show that LightNER can obtain comparable performance in the standard supervised setting and outperform strong baselines in low-resource settings. Code is in https://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot.
Integrating prediction in mean-variance portfolio optimization<|sep|>Many problems in quantitative finance involve both predictive forecasting and decision-based optimization. Traditionally, predictive models are optimized with unique prediction-based objectives and constraints, and are therefore unaware of how those predictions will ultimately be used in the context of their final decision-based optimization. We present a stochastic optimization framework for integrating regression based predictive models in a mean-variance portfolio optimization setting. Closed-form analytical solutions are provided for the unconstrained and equality constrained case. For the general inequality constrained case, we make use of recent advances in neural-network architecture for efficient optimization of batch quadratic-programs. To our knowledge, this is the first rigorous study of integrating prediction in a mean-variance portfolio optimization setting. We present several historical simulations using global futures data and demonstrate the benefits of the integrated approach in comparison to the decoupled alternative.
The perfectly matched layer (PML) for hyperbolic wave propagation problems: A review<|sep|>It is well-known that reliable and efficient domain truncation is crucial to accurate numerical solution of most wave propagation problems. The perfectly matched layer (PML) is a method which, when stable, can provide a domain truncation scheme which is convergent with increasing layer width/damping. The difficulties in using the PML are primarily associated with stability, which can be present at the continuous level or be triggered by numerical approximations. The mathematical and numerical analysis of the PML for hyperbolic wave propagation problems has been an area of active research. It is now possible to construct stable and high order accurate numerical wave solvers by augmenting wave equations with the PML and approximating the equations using summation-by-parts finite difference methods, continuous and discontinuous Galerkin finite element methods. In this review we summarise the progress made, from mathematical, numerical and practical perspectives, point out some open problems and set the stage for future work. We also present numerical experiments of model problems corroborating the theoretical analysis, and numerical simulations of real-world wave propagation demonstrating impact. Stable and parallel implementations of the PML in the high performance computing software packages WaveQLab3D and ExaHyPE allow to sufficiently limit the computational domain of seismological problems with only a few grid points/elements around the computational boundaries where the PML is active, thus saving as much as $96\%$ of the required computational resources for a three space dimensional seismological benchmark problem.
A Generalised Method for Empirical Game Theoretic Analysis<|sep|>This paper provides theoretical bounds for empirical game theoretical analysis of complex multi-agent interactions. We provide insights in the empirical meta game showing that a Nash equilibrium of the meta-game is an approximate Nash equilibrium of the true underlying game. We investigate and show how many data samples are required to obtain a close enough approximation of the underlying game. Additionally, we extend the meta-game analysis methodology to asymmetric games. The state-of-the-art has only considered empirical games in which agents have access to the same strategy sets and the payoff structure is symmetric, implying that agents are interchangeable. Finally, we carry out an empirical illustration of the generalised method in several domains, illustrating the theory and evolutionary dynamics of several versions of the AlphaGo algorithm (symmetric), the dynamics of the Colonel Blotto game played by human players on Facebook (symmetric), and an example of a meta-game in Leduc Poker (asymmetric), generated by the PSRO multi-agent learning algorithm.
First-passage time of run-and-tumble particles<|sep|>We solve the problem of first-passage time for run-and-tumble particles in one dimension. Exact expression is derived for the mean first-passage time in the general case, considering external force-fields and chemotactic-fields, giving rise to space dependent swim-speed and tumble rate. Agreement between theoretical formulae and numerical simulations is obtained in the analyzed case studies -- constant and sinusoidal force fields, constant gradient chemotactic field. Reported findings can be useful to get insights into very different phenomena involving active particles, such as bacterial motion in external fields, intracellular transport, cell migration, animal foraging.
Spectral stability analysis for standing waves of a perturbed Klein-Gordon equation<|sep|>In the present work, we introduce a new $\mathcal{PT}$-symmetric variant of the Klein-Gordon field theoretic problem. We identify the standing wave solutions of the proposed class of equations and analyze their stability. In particular, we obtain an explicit frequency condition, somewhat reminiscent of the classical Vakhitov-Kolokolov criterion, which sharply separates the regimes of spectral stability and instability. Our numerical computations corroborate the relevant theoretical result.
Doping effects of Co, Ni, and Cu in FeTe0.65Se0.35 single crystals<|sep|>The resistivity, magnetoresistance, and magnetic susceptibility are measured in single crystals of FeTe0.65Se0.35 with Cu, Ni, and Co substitutions for Fe. The crystals are grown by Bridgman's method. The resistivity measurements show that superconductivity disappears with the rate which correlates with the nominal valence of the impurity. From magnetoresistance we evaluate doping effect on the basic superconducting parameters, such as upper critical field and coherence length. We find indications that doping leads to two component superconducting behavior, possibly because of local charge depression around impurities.
Deconstructing a galaxy: colour distributions of point sources in Messier 83<|sep|>What do we see when we look at a nearby, well-resolved galaxy? Thousands of individual sources are detected in multiband imaging observations of even a fraction of a nearby galaxy, and characterizing those sources is a complex process. This work analyses a ten-band photometric catalogue of nearly 70 000 point sources in a 7.3 square arcmin region of the nearby spiral galaxy Messier 83, made as part of the Early Release Science programme with the Hubble Space Telescope's Wide Field Camera 3. Colour distributions were measured for both broad-band and broad-and-narrow-band colours; colours made from broad-bands with large wavelength differences generally had broader distributions although B - V was an exception. Two- and three-dimensional colour spaces were generated using various combinations of four bands and clustered with the K-Means and Mean Shift algorithms. Neither algorithm was able to consistently segment the colour distributions: while some distinct features in colour space were apparent in visual examinations, these features were not compact or isolated enough to be recognized as clusters in colour space. K-Means clustering of the UBVI colour space was able to identify a group of objects more likely to be star clusters. Mean Shift was successful in identifying outlying groups at the edges of colour distributions. For identifying objects whose emission is dominated by spectral lines, there was no clear benefit from combining narrow-band photometry in multiple bands compared to a simple continuum subtraction. The clustering analysis results are used to inform recommendations for future surveys of nearby galaxies.
Accountable Off-Policy Evaluation With Kernel Bellman Statistics<|sep|>We consider off-policy evaluation (OPE), which evaluates the performance of a new policy from observed data collected from previous experiments, without requiring the execution of the new policy. This finds important applications in areas with high execution cost or safety concerns, such as medical diagnosis, recommendation systems and robotics. In practice, due to the limited information from off-policy data, it is highly desirable to construct rigorous confidence intervals, not just point estimation, for the policy performance. In this work, we propose a new variational framework which reduces the problem of calculating tight confidence bounds in OPE into an optimization problem on a feasible set that catches the true state-action value function with high probability. The feasible set is constructed by leveraging statistical properties of a recently proposed kernel Bellman loss (Feng et al., 2019). We design an efficient computational approach for calculating our bounds, and extend it to perform post-hoc diagnosis and correction for existing estimators. Empirical results show that our method yields tight confidence intervals in different settings.
Hardless: A Generalized Serverless Compute Architecture for Hardware Processing Accelerators<|sep|>The increasing use of hardware processing accelerators tailored for specific applications, such as the Vision Processing Unit (VPU) for image recognition, further increases developers' configuration, development, and management overhead. Developers have successfully used fully automated elastic cloud services such as serverless computing to counter these additional efforts and shorten development cycles for applications running on CPUs. Unfortunately, current cloud solutions do not yet provide these simplifications for applications that require hardware acceleration. However, as the development of specialized hardware acceleration continues to provide performance and cost improvements, it will become increasingly important to enable ease of use in the cloud. In this paper, we present an initial design and implementation of Hardless, an extensible and generalized serverless computing architecture that can support workloads for arbitrary hardware accelerators. We show how Hardless can scale across different commodity hardware accelerators and support a variety of workloads using the same execution and programming model common in serverless computing today.
PersisDroid: Android Performance Diagnosis via Anatomizing Asynchronous Executions<|sep|>Android applications (apps) grow dramatically in recent years. Apps are user interface (UI) centric typically. Rapid UI responsiveness is key consideration to app developers. However, we still lack a handy tool for profiling app performance so as to diagnose performance problems. This paper presents PersisDroid, a tool specifically designed for this task. The key notion of PersisDroid is that the UI-triggered asynchronous executions also contribute to the UI performance, and hence its performance should be properly captured to facilitate performance diagnosis. However, Android allows tremendous ways to start the asynchronous executions, posing a great challenge to profiling such execution. This paper finds that they can be grouped into six categories. As a result, they can be tracked and profiled according to the specifics of each category with a dynamic instrumentation approach carefully tailored for Android. PersisDroid can then properly profile the asynchronous executions in task granularity, which equips it with low-overhead and high compatibility merits. Most importantly, the profiling data can greatly help the developers in detecting and locating performance anomalies. We code and open-source release PersisDroid. The tool is applied in diagnosing 20 open-source apps, and we find 11 of them contain potential performance problems, which shows its effectiveness in performance diagnosis for Android apps.
Impact of Propagation Environment on Energy-Efficient Relay Placement: Model and Performance Analysis<|sep|>The performance of a relay-based cellular network is greatly affected by the relay location within a cell. Existing results for optimal relay placement do not reflect how the radio propagation environment and choice of the coding scheme can impact system performance. In this paper, we analyze the impact on relaying performance of node distances, relay height and line-of-sight conditions for both uplink and downlink transmissions, using several relay coding schemes. Our first objective is to propose a geometrical model for energy-efficient relay placement that requires only a small number of characteristic distances. Our second objective is to estimate the maximum cell coverage of a relay-aided cell given power constraints, and conversely, the averaged energy consumption given a cell radius. We show that the practical full decode-forward scheme performs close to the energy-optimized partial decode-forward scheme when the relay is ideally located. However, away from this optimum relay location, performance rapidly degrades and more advanced coding scheme, such as partial decode-forward, is needed to maintain good performance and allow more freedom in the relay placement. Finally, we define a trade-off between cell coverage and energy efficiency, and show that there exists a relay location for which increasing the cell coverage has a minimal impact on the average energy consumed per unit area.
Local random quantum circuits are approximate polynomial-designs - numerical results<|sep|>We numerically investigate the statement that local random quantum circuits acting on n qubits composed of polynomially many nearest neighbour two-qubit gates form an approximate unitary poly(n)-design [F.G.S.L. Brandao et al., arXiv:1208.0692]. Using a group theory formalism, spectral gaps that give a ratio of convergence to a given t-design are evaluated for a different number of qubits n (up to 20) and degrees t (t=2,3,4 and 5), improving previously known results for n=2 in the case of t=2 and 3. Their values lead to a conclusion that the previously used lower bound that bounds spectral gaps values may give very little information about the real situation and in most cases, only tells that a gap is closed. We compare our results to the another lower bounding technique, again showing that its results may not be tight.
Feynman Functional Integral in the Fokker Theory<|sep|>The equivalence of two formulations of Fokker's quantum theory is proved - based on the Feynman functional integral representation of the propagator for a system of charges with direct electromagnetic interaction and the quantum principle of least action as an analogue of the Schr\"{o}dinger wave equation. The common basis for the two approaches is the generalized canonical form of Fokker's action.
A VoIP Privacy Mechanism and its Application in VoIP Peering for Voice Service Provider Topology and Identity Hiding<|sep|>Voice Service Providers (VSPs) participating in VoIP peering frequently want to withhold their identity and related privacy-sensitive information from other parties during the VoIP communication. A number of existing documents on VoIP privacy exist, but most of them focus on end user privacy. By summarizing and extending existing work, we present a unified privacy mechanism for both VoIP users and service providers. We also show a case study on how VSPs can use this mechanism for identity and topology hiding in VoIP peering.
Constraining the 0{\nu}2{\beta} matrix elements by nuclear structure observables<|sep|>The discovery that neutrinos have finite rest mass has led to renewed interest in neutrinoless double beta decay. The development of large-scale experiments to search for neutrinoless double beta decay has increased the probability of a credible observation of the process in the near future. The reliability of calculations of the associated nuclear matrix elements is likely soon to become a critical issue. In this paper experimental techniques that access properties of the ground-state wave functions of double beta decay candidates, the occupancies of valence single- particle orbitals and pairing correlations, are summarized and the experimental data for candidate nuclei are reviewed. The results are discussed in relation to questions concerning which aspects of nuclear structure may play an important role in determining the nuclear matrix elements for neutrinoless double beta decay.
Ionized Plasma and Neutral Gas Coupling in the Sun's Chromosphere and Earth's Ionosphere/Thermosphere<|sep|>We review our understanding of ionized plasma and neutral gas coupling in the weakly ionized, stratified, electromagnetically-permeated regions of the Sun's chromosphere and Earth's ionosphere/thermosphere. Using representative models for each environment we derive fundamental descriptions of the coupling of the constituent parts to each other and to the electric and magnetic fields, and we examine the variation in magnetization of the ionized component. Using these descriptions we compare related phenomena in the two environments, and discuss electric currents, energy transfer and dissipation. We present a coupled theoretical and numerical study of plasma instabilities in the two environments that serves as an example of how the chromospheric and ionospheric communities can further collaborate. We also suggest future collaborative studies that will help improve our understanding of these two different atmospheres which share many similarities, but have large disparities in key quantities.
Global Mg/Si and Al/Si Distributions on Lunar Surface Derived from Chang'E-2 X-ray Spectrometer<|sep|>X-ray fluorescence remote sensing technique plays a significant role in the chemical compositions research of the Moon. Here we describe the data analysis method for China's Chang'E-2 X-ray spectrometer (CE2XRS) in detail and present the preliminary results: the first global Mg/Si and Al/Si maps on the lunar surface. Our results show that the distributions of Mg/Si and Al/Si correlate well with the terrains of the Moon. The higher Mg/Si ratio corresponding to the mare regions while the lower value corresponding to the highland terrains. The map of Al/Si ratio shows a reverse relationship with the map of Mg/Si ratio.
Pressure profiles and mass estimates using high-resolution Sunyaev-Zel'dovich effect observations of Zwicky 3146 with MUSTANG-2<|sep|>The galaxy cluster Zwicky 3146 is a sloshing cool core cluster at $z=0.291$ that in X-ray imaging does not appear to exhibit significant pressure substructure in the intracluster medium (ICM). The published $M_{500}$ values range between $3.88^{+0.62}_{-0.58}$ to $22.50 \pm 7.58 \times 10^{14}$ M$_{\odot}$, where ICM-based estimates with reported errors $<20$\% suggest that we should expect to find a mass between $6.53^{+0.44}_{-0.44} \times 10^{14}$ M$_{\odot}$ (from Planck, with an $8.4\sigma$ detection) and $8.52^{+1.77}_{-1.47} \times 10^{14}$ M$_{\odot}$ (from ACT, with a $14\sigma$ detection). This broad range of masses is suggestive that there is ample room for improvement for all methods. Here, we investigate the ability to estimate the mass of Zwicky 3146 via the Sunyaev-Zel'dovich (SZ) effect with data taken at 90 GHz by MUSTANG-2 to a noise level better than $15\ \mu$K at the center, and a cluster detection of $104\sigma$. We derive a pressure profile from our SZ data which is in excellent agreement with that derived from X-ray data. From our SZ-derived pressure profiles, we infer $M_{500}$ and $M_{2500}$ via three methods -- $Y$-$M$ scaling relations, the virial theorem, and hydrostatic equilibrium -- where we employ X-ray constraints from \emph{XMM-Newton} on the electron density profile when assuming hydrostatic equilibrium. Depending on the model and estimation method, our $M_{500}$ estimates range from $6.23 \pm 0.59$ to $10.6 \pm 0.95 \times 10^{14}$ M$_{\odot}$, where our estimate from hydrostatic equilibrium, is $8.29^{+1.93}_{-1.24}$ ($\pm 19.1$\% stat) ${}^{+0.74}_{-0.68}$ ($\pm 8.6$\% sys, calibration) $\times 10^{14}$ M$_{\odot}$. Our fiducial mass, derived from a $Y$-$M$ relation is $8.16^{+0.44}_{-0.54}$ ($\pm 5.5$\% stat) ${}^{+0.46}_{-0.43}$ ($\pm 5.5$\% sys, $Y$-$M$) ${}^{+0.59}_{-0.55}$ ($\pm 7.0$\% sys, cal.) $\times 10^{14}$ M$_{\odot}$.
Interferometric Autocorrelation Measurements of Supercontinuum based on Two-Photon Absorption<|sep|>We report on interferometric autocorrelation measurements of broadband supercontinuum light in the anomalous dispersion regime using two-photon absorption in a GaP photodetector. The method is simple, low-cost, and provides a direct measure of the second-order coherence properties, including quantitative information on the coherence time and average duration of the supercontinuum pulses as well as on the presence of temporally coherent sub-structures. We report measurements in regimes where the supercontinuum is coherent and incoherent. In the former case, the interferometric measurements are similar to what is observed for mode-locked laser pulses while in the latter case the interferometric measurements and coherence properties are shown to have characteristics similar to that of a stationary chaotic light source.
Orthogonality of the mean and error distribution in generalized linear models<|sep|>We show that the mean-model parameter is always orthogonal to the error distribution in generalized linear models. Thus, the maximum likelihood estimator of the mean-model parameter will be asymptotically efficient regardless of whether the error distribution is known completely, known up to a finite vector of parameters, or left completely unspecified, in which case the likelihood is taken to be an appropriate semiparametric likelihood. Moreover, the maximum likelihood estimator of the mean-model parameter will be asymptotically independent of the maximum likelihood estimator of the error distribution. This generalizes some well-known results for the special cases of normal, gamma and multinomial regression models, and, perhaps more interestingly, suggests that asymptotically efficient estimation and inferences can always be obtained if the error distribution is nonparametrically estimated along with the mean. In contrast, estimation and inferences using misspecified error distributions or variance functions are generally not efficient.
Facial Emotion Recognition with Noisy Multi-task Annotations<|sep|>Human emotions can be inferred from facial expressions. However, the annotations of facial expressions are often highly noisy in common emotion coding models, including categorical and dimensional ones. To reduce human labelling effort on multi-task labels, we introduce a new problem of facial emotion recognition with noisy multi-task annotations. For this new problem, we suggest a formulation from the point of joint distribution match view, which aims at learning more reliable correlations among raw facial images and multi-task labels, resulting in the reduction of noise influence. In our formulation, we exploit a new method to enable the emotion prediction and the joint distribution learning in a unified adversarial learning game. Evaluation throughout extensive experiments studies the real setups of the suggested new problem, as well as the clear superiority of the proposed method over the state-of-the-art competing methods on either the synthetic noisy labeled CIFAR-10 or practical noisy multi-task labeled RAF and AffectNet. The code is available at https://github.com/sanweiliti/noisyFER.
News Kaleidoscope: Visual Investigation of Coverage Diversity in News Event Reporting<|sep|>We develop a visual analytics system, NewsKaleidoscope, to investigate the how news reporting of events varies. NewsKaleidoscope combines several backend text language processing techniques with a coordinated visualization interface tailored for visualization non-expert users. To robustly evaluate NewsKaleidoscope, we conduct a trio of user studies. (1) A usability study with news novices assesses the overall system and the specific insights promoted for journalism-agnostic users. (2) A follow-up study with news experts assesses the insights promoted for journalism-savvy users. (3) Based on identified system limitations in these two studies, we amend NewsKaleidoscope design and conduct a third study to validate these improvements. Results indicate that, for both news novice and experts, NewsKaleidoscope supports an effective, task-driven workflow for analyzing the diversity of news coverage about events, though journalism expertise has a significant influence on the user insights and takeaways. Our insights while developing and evaluating NewsKaleidoscope can aid future interface designs that combine visualization with natural language processing to analyze coverage diversity in news event reporting.
Astrophysical interpretation of the anisotropies in the unresolved gamma-ray background<|sep|>Recently, a new measurement of the auto- and cross-correlation angular power spectrum (APS) of the isotropic gamma-ray background was performed, based on 81 months of data of the Fermi Large-Area Telescope (LAT). Here, we fit, for the first time, the new APS data with a model describing the emission of unresolved blazars. These sources are expected to dominate the anisotropy signal. The model we employ in our analysis reproduces well the blazars resolved by Fermi LAT. When considering the APS obtained by masking the sources in the 3FGL catalogue, we find that unresolved blazars under-produce the measured APS below $\sim$1 GeV. Contrary to past results, this suggests the presence of a new contribution to the low-energy APS, with a significance of, at least, 5$\sigma$. The excess can be ascribed to a new class of faint gamma-ray emitters. If we consider the APS obtained by masking the sources in the 2FGL catalogue, there is no under-production of the APS below 1 GeV, but the new source class is still preferred over the blazars-only scenario (with a significance larger than 10$\sigma$). The properties of the new source class and the level of anisotropies induced in the isotropic gamma-ray background are the same, independent of the APS data used. In particular, the new gamma-ray emitters must have a soft energy spectrum, with a spectral index ranging, approximately, from 2.7 to 3.2. This complicates their interpretation in terms of known sources, since, normally, star-forming and radio galaxies are observed with a harder spectrum. The new source class identified here is also expected to contribute significantly to the intensity of the isotropic gamma-ray background.
LipschitzLR: Using theoretically computed adaptive learning rates for fast convergence<|sep|>Optimizing deep neural networks is largely thought to be an empirical process, requiring manual tuning of several hyper-parameters, such as learning rate, weight decay, and dropout rate. Arguably, the learning rate is the most important of these to tune, and this has gained more attention in recent works. In this paper, we propose a novel method to compute the learning rate for training deep neural networks with stochastic gradient descent. We first derive a theoretical framework to compute learning rates dynamically based on the Lipschitz constant of the loss function. We then extend this framework to other commonly used optimization algorithms, such as gradient descent with momentum and Adam. We run an extensive set of experiments that demonstrate the efficacy of our approach on popular architectures and datasets, and show that commonly used learning rates are an order of magnitude smaller than the ideal value.
GrADyS: Exploring movement awareness for efficient routing in Ground-and-Air Dynamic Sensor Networks<|sep|>Several situations exist where a geographic region of some size needs to be scanned or monitored through many sensors. Still, it is either absolutely impossible or prohibitively expensive to deploy and maintain wireless communication infrastructure for the distributed sensors. Either because the region is hidden behind walls, not easily accessible, hard to get through, or infected with some lethal bacteria or virus transmitter. In this case, the best is to scatter (disposable) sensors in the region and let them transmit the collected sensor data by wireless means to an overflying UAV/drone. Which then physically hauls the collected data from the monitored area to a central base station that functions as a gateway to the Internet. The project GrADyS aims to research two sets of problems regarding such data collection. The former aims to coordinate several autonomous UAVs in a distributed manner to collect the generated data while relying only on ad-hoc communication. The latter aims to develop routing protocols to mesh networks Bluetooth Mesh's Low Power Nodes. Both research lines already present preliminary results that are presented in this paper.
Physical parameters of pre-main sequence stars in open clusters<|sep|>Our aims are twofold: To determine the physical parameters of PMS members in young open clusters (YOCs), and to check and compare the performances of different model isochrones. We compare UBVRI photometric observations of YOCs to theoretical isochrones in the photometric diagrams. The comparison simultaneously provides membership assignments for MS and PMS stars, and estimates for the masses, ages, and spatial distribution of the candidate members. The relations found between the different cluster parameters show that the procedure applied to assign cluster membership, and to measure physical parameters for the selected members, is well founded.
A geometric view on local Lorentz transformations in teleparallel gravity<|sep|>Local Lorentz transformations play an important role in teleparallel gravity theories, in which a tetrad is conventionally employed as a fundamental field variable describing the gravitational field. It is commonly understood that modifications of general relativity in the teleparallel framework break a certain notion of local Lorentz invariance, which is present in the pure tetrad formulation of such theories, while another notion present in the covariant formulation is preserved. We illuminate these different notions from a geometric perspective, and distinguish them from what is commonly understood as breaking of local Lorentz invariance in the context of gravity phenomenology. Based on physical arguments, we present a geometric interpretation of the dynamical fields in teleparallel gravity, which unified and refines the conventional approaches.
Quantum Monte Carlo with variable spins: fixed-phase and fixed-node approximations<|sep|>We study several aspects of the recently introduced fixed-phase spin-orbit diffusion Monte Carlo (FPSODMC) method, in particular, its relation to the fixed-node method and its potential use as a general approach for electronic structure calculations. We illustrate constructions of spinor-based wave functions with the full space-spin symmetry without assigning up or down spin labels to particular electrons, effectively "complexifying" even ordinary real-valued wave functions. Interestingly, with proper choice of the simulation parameters and spin variables, such fixed-phase calculations enable one to reach also the fixed-node limit. The fixed-phase solution provides a straightforward interpretation as the lowest bosonic state in a given effective potential generated by the many-body approximate phase. In addition, the divergences present at real wave function nodes are smoothed out to lower dimensionality, decreasing thus the variation of sampled quantities and making the sampling also more straightforward. We illustrate some of these properties on calculations of selected first-row systems that recover the fixed-node results with quantitatively similar levels of the corresponding biases. At the same time, the fixed-phase approach opens new possibilities for more general trial wave functions with further opportunities for increasing accuracy in practical calculations.
Multitask Learning for Sequence Labeling Tasks<|sep|>In this paper, we present a learning method for sequence labeling tasks in which each example sequence has multiple label sequences. Our method learns multiple models, one model for each label sequence. Each model computes the joint probability of all label sequences given the example sequence. Although each model considers all label sequences, its primary focus is only one label sequence, and therefore, each model becomes a task-specific model, for the task belonging to that primary label. Such multiple models are learned {\it simultaneously} by facilitating the learning transfer among models through {\it explicit parameter sharing}. We experiment the proposed method on two applications and show that our method significantly outperforms the state-of-the-art method.
Wavefront sensing from the image domain with the Oxford-SWIFT integral field spectrograph<|sep|>The limits for adaptive-optics (AO) imaging at high contrast and high resolution are determined by residual phase errors from non-common-path aberrations not sensed by the wavefront sensor, especially for integral field spectrographs, where phase diversity techniques are complicated by the image slicer. We present the first application of kernel phase-based wavefront sensing to ground-based AO, where an asymmetric pupil mask and a single image are sufficient to map aberrations up to high order. We push toward internally diffraction-limited performance with the Oxford-SWIFT integral field spectrograph coupled with the PALM-3000 extreme AO system on the Palomar 200-inch telescope. This represents the first observation in which the PALM-3000 + SWIFT internal point-spread-function has closely approached the Airy pattern. While this can only be used on SWIFT with an internal stimulus source, as at short wavelengths the uncorrected atmospheric wavefront errors are still > 1 radian, this nevertheless demonstrates the feasibility of detecting non-common-path errors with this method as an active optics paradigm for near-infrared, AO-corrected instruments at Palomar. We note that this is a particularly promising approach for correcting integral field spectrographs, as the diversity of many narrowband images provides strong constraints on the wavefront error estimate, and the average of reconstructions from many narrow bands can be used to improve overall reconstruction quality.
The SuperNEMO double beta decay experiment<|sep|>The SuperNEMO project studies the feasibility of employing a technique of tracking plus calorimetry to search for neutrinoless double beta decay in 100 kg of enriched isotopes. It aims to reach an effective neutrino mass sensitivity of 50 meV. The current status of the SuperNEMO R&D programme is described, focusing on the main areas of improvement.
Stable interpolation with isotropic and anisotropic Gaussians using Hermite generating function<|sep|>Gaussian kernels can be an efficient and accurate tool for multivariate interpolation. In practice, high accuracies are often achieved in the flat limit where the interpolation matrix becomes increasingly ill-conditioned. Stable evaluation algorithms for isotropic Gaussians (Gaussian radial basis functions) have been proposed based on a Chebyshev expansion of the Gaussians by Fornberg, Larsson & Flyer and based on a Mercer expansion with Hermite polynomials by Fasshauer & McCourt. In this paper, we propose a new stabilization algorithm for the multivariate interpolation with isotropic or anisotropic Gaussians derived from the generating function of the Hermite polynomials. We also derive and analyse a new analytic cut-off criterion for the generating function expansion that allows to automatically adjust the number of the stabilizing basis functions.
The SPARQL2XQuery Interoperability Framework. Utilizing Schema Mapping, Schema Transformation and Query Translation to Integrate XML and the Semantic Web<|sep|>The Web of Data is an open environment consisting of a great number of large inter-linked RDF datasets from various domains. In this environment, organizations and companies adopt the Linked Data practices utilizing Semantic Web (SW) technologies, in order to publish their data and offer SPARQL endpoints (i.e., SPARQL-based search services). On the other hand, the dominant standard for information exchange in the Web today is XML. The SW and XML worlds and their developed infrastructures are based on different data models, semantics and query languages. Thus, it is crucial to develop interoperability mechanisms that allow the Web of Data users to access XML datasets, using SPARQL, from their own working environments. It is unrealistic to expect that all the existing legacy data (e.g., Relational, XML, etc.) will be transformed into SW data. Therefore, publishing legacy data as Linked Data and providing SPARQL endpoints over them has become a major research challenge. In this direction, we introduce the SPARQL2XQuery Framework which creates an interoperable environment, where SPARQL queries are automatically translated to XQuery queries, in order to access XML data across the Web. The SPARQL2XQuery Framework provides a mapping model for the expression of OWL-RDF/S to XML Schema mappings as well as a method for SPARQL to XQuery translation. To this end, our Framework supports both manual and automatic mapping specification between ontologies and XML Schemas. In the automatic mapping specification scenario, the SPARQL2XQuery exploits the XS2OWL component which transforms XML Schemas into OWL ontologies. Finally, extensive experiments have been conducted in order to evaluate the schema transformation, mapping generation, query translation and query evaluation efficiency, using both real and synthetic datasets.
Coronal rain in magnetic arcades: Rebound shocks, Limit cycles, and Shear flows<|sep|>We extend our earlier multidimensional, magnetohydrodynamic simulations of coronal rain occurring in magnetic arcades with higher resolution, grid-adaptive computations covering a much longer ($>6$ hour) timespan. We quantify how in-situ forming blob-like condensations grow along and across field lines and show that rain showers can occur in limit cycles, here demonstrated for the first time in 2.5D setups. We discuss dynamical, multi-dimensional aspects of the rebound shocks generated by the siphon inflows and quantify the thermodynamics of a prominence-corona-transition-region like structure surrounding the blobs. We point out the correlation between condensation rates and the cross-sectional size of loop systems where catastrophic cooling takes place. We also study the variations of the typical number density, kinetic energy and temperature while blobs descend, impact and sink into the transition region. In addition, we explain the mechanisms leading to concurrent upflows while the blobs descend. As a result, there are plenty of shear flows generated with relative velocity difference around 80 km s$^{-1}$ in our simulations. These shear flows are siphon flows set up by multiple blob dynamics and they in turn affect the deformation of the falling blobs. In particular, we show how shear flows can break apart blobs into smaller fragments, within minutes.
Customizable Precision of Floating-Point Arithmetic with Bitslice Vector Types<|sep|>Customizing the precision of data can provide attractive trade-offs between accuracy and hardware resources. We propose a novel form of vector computing aimed at arrays of custom-precision floating point data. We represent these vectors in bitslice format. Bitwise instructions are used to implement arithmetic circuits in software that operate on customized bit-precision. Experiments show that this approach can be efficient for vectors of low-precision custom floating point types, while providing arbitrary bit precision.
Sterile neutrinos and neutrinoless double beta decay in effective field theory<|sep|>We investigate neutrinoless double beta decay ($0\nu\beta\beta$) in the presence of sterile neutrinos with Majorana mass terms. These gauge-singlet fields are allowed to interact with Standard-Model (SM) fields via renormalizable Yukawa couplings as well as higher-dimensional gauge-invariant operators up to dimension seven in the Standard Model Effective Field Theory extended with sterile neutrinos. At the GeV scale, we use Chiral effective field theory involving sterile neutrinos to connect the operators at the level of quarks and gluons to hadronic interactions involving pions and nucleons. This allows us to derive an expression for $0\nu\beta\beta$ rates for various isotopes in terms of phase-space factors, hadronic low-energy constants, nuclear matrix elements, the neutrino masses, and the Wilson coefficients of higher-dimensional operators. The needed hadronic low-energy constants and nuclear matrix elements depend on the neutrino masses, for which we obtain interpolation formulae grounded in QCD and chiral perturbation theory that improve existing formulae that are only valid in a small regime of neutrino masses. The resulting framework can be used directly to assess the impact of $0\nu\beta\beta$ experiments on scenarios with light sterile neutrinos and should prove useful in global analyses of sterile-neutrino searches. We perform several phenomenological studies of $0\nu\beta\beta$ in the presence of sterile neutrinos with and without higher-dimensional operators. We find that non-standard interactions involving sterile neutrinos have a dramatic impact on $0\nu\beta\beta$ phenomenology, and next-generation experiments can probe such interactions up to scales of $\mathcal O(100)$ TeV.
Water in Low-Mass Star-Forming Regions with Herschel: The Link Between Water Gas and Ice in Protostellar Envelopes<|sep|>Aims: Our aim is to determine the critical parameters in water chemistry and the contribution of water to the oxygen budget by observing and modelling water gas and ice for a sample of eleven low-mass protostars, for which both forms of water have been observed. Methods: A simplified chemistry network, which is benchmarked against more sophisticated chemical networks, is developed that includes the necessary ingredients to determine the water vapour and ice abundance profiles in the cold, outer envelope in which the temperature increases towards the protostar. Comparing the results from this chemical network to observations of water emission lines and previously published water ice column densities, allows us to probe the influence of various agents (e.g., FUV field, initial abundances, timescales, and kinematics). Results: The observed water ice abundances with respect to hydrogen nuclei in our sample are 30-80ppm, and therefore contain only 10-30% of the volatile oxygen budget of 320ppm. The keys to reproduce this result are a low initial water ice abundance after the pre-collapse phase together with the fact that atomic oxygen cannot freeze-out and form water ice in regions with T(dust)>15 K. This requires short prestellar core lifetimes of less than about 0.1Myr. The water vapour profile is shaped through the interplay of FUV photodesorption, photodissociation, and freeze-out. The water vapour line profiles are an invaluable tracer for the FUV photon flux and envelope kinematics. Conclusions: The finding that only a fraction of the oxygen budget is locked in water ice can be explained either by a short pre-collapse time of less than 0.1 Myr at densities of n(H)~1e4 cm-3, or by some other process that resets the initial water ice abundance for the post-collapse phase. A key for the understanding of the water ice abundance is the binding energy of atomic oxygen on ice.
On the optimality of tree-reweighted max-product message-passing<|sep|>Tree-reweighted max-product (TRW) message passing is a modified form of the ordinary max-product algorithm for attempting to find minimal energy configurations in Markov random field with cycles. For a TRW fixed point satisfying the strong tree agreement condition, the algorithm outputs a configuration that is provably optimal. In this paper, we focus on the case of binary variables with pairwise couplings, and establish stronger properties of TRW fixed points that satisfy only the milder condition of weak tree agreement (WTA). First, we demonstrate how it is possible to identify part of the optimal solution|i.e., a provably optimal solution for a subset of nodes| without knowing a complete solution. Second, we show that for submodular functions, a WTA fixed point always yields a globally optimal solution. We establish that for binary variables, any WTA fixed point always achieves the global maximum of the linear programming relaxation underlying the TRW method.
The Quantum Closet<|sep|>The equivalence postulate approach to quantum mechanics entails a derivation of quantum mechanics from a fundamental geometrical principle. Underlying the formalism there exists a basic cocycle condition, which is invariant under D-dimensional finite Mobius transformations. The invariance of the cocycle condition under finite Mobius transformations implies that space is compact. Additionally, it implies energy quantisation and the undefinability of quantum trajectories. I argue that the decompactification limit coincides with the classical limit. Evidence for the compactness of the universe may exist in the Cosmic Microwave Background Radiation.
Structural Guidance for Transformer Language Models<|sep|>Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The "Generative Parsing" idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The "Structural Scaffold" idea guides the language model's representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models' syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.
Building Robust Ensembles via Margin Boosting<|sep|>In the context of adversarial robustness, a single model does not usually have enough power to defend against all possible adversarial attacks, and as a result, has sub-optimal robustness. Consequently, an emerging line of work has focused on learning an ensemble of neural networks to defend against adversarial attacks. In this work, we take a principled approach towards building robust ensembles. We view this problem from the perspective of margin-boosting and develop an algorithm for learning an ensemble with maximum margin. Through extensive empirical evaluation on benchmark datasets, we show that our algorithm not only outperforms existing ensembling techniques, but also large models trained in an end-to-end fashion. An important byproduct of our work is a margin-maximizing cross-entropy (MCE) loss, which is a better alternative to the standard cross-entropy (CE) loss. Empirically, we show that replacing the CE loss in state-of-the-art adversarial training techniques with our MCE loss leads to significant performance improvement.
Fourier Theory on the Complex Plane I: Conjugate Pairs of Fourier Series and Inner Analytic Functions<|sep|>A correspondence between arbitrary Fourier series and certain analytic functions on the unit disk of the complex plane is established. The expression of the Fourier coefficients is derived from the structure of complex analysis. The orthogonality and completeness relations of the Fourier basis are derived in the same way. It is shown that the limiting function of any Fourier series is also the limit to the unit circle of an analytic function in the open unit disk. An alternative way to recover the original real functions from the Fourier coefficients, which works even when the Fourier series are divergent, is thus presented. The convergence issues are discussed up to a certain point. Other possible uses of the correspondence established are pointed out.
On the Linear Extension Complexity of Regular n-gons<|sep|>In this paper, we propose new lower and upper bounds on the linear extension complexity of regular $n$-gons. Our bounds are based on the equivalence between the computation of (i) an extended formulation of size $r$ of a polytope $P$, and (ii) a rank-$r$ nonnegative factorization of a slack matrix of the polytope $P$. The lower bound is based on an improved bound for the rectangle covering number (also known as the boolean rank) of the slack matrix of the $n$-gons. The upper bound is a slight improvement of the result of Fiorini, Rothvoss and Tiwary [Extended Formulations for Polygons, Discrete Comput. Geom. 48(3), pp. 658-668, 2012]. The difference with their result is twofold: (i) our proof uses a purely algebraic argument while Fiorini et al. used a geometric argument, and (ii) we improve the base case allowing us to reduce their upper bound $2 \left\lceil \log_2(n) \right\rceil$ by one when $2^{k-1} < n \leq 2^{k-1}+2^{k-2}$ for some integer $k$. We conjecture that this new upper bound is tight, which is suggested by numerical experiments for small $n$. Moreover, this improved upper bound allows us to close the gap with the best known lower bound for certain regular $n$-gons (namely, $9 \leq n \leq 13$ and $21 \leq n \leq 24$) hence allowing for the first time to determine their extension complexity.
Classically conformal U(1)$^\prime$ extended Standard Model and Higgs vacuum stability<|sep|>We consider the minimal U(1)$^\prime$ extension of the Standard Model (SM) with conformal invariance at the classical level, where in addition to the SM particle contents, three generations of right-handed neutrinos and a U(1)$^\prime$ Higgs field are introduced. In the presence of the three right-handed neutrinos, which are responsible for the seesaw mechanism, this model is free from all the gauge and gravitational anomalies. The U(1)$^\prime$ gauge symmetry is radiatively broken via the Coleman-Weinberg mechanism, by which the U(1)$^\prime$ gauge boson ($Z^\prime$ boson) mass as well as the Majorana mass for the right-handed neutrinos are generated. The radiative U(1)$^\prime$ symmetry breaking also induces a negative mass squared for the SM Higgs doublet to trigger the electroweak symmetry breaking. In this context, we investigate a possibility to solve the SM Higgs vacuum instability problem. The model includes only three free parameters (U(1)$^\prime$ charge of the SM Higgs doublet, U(1)$^\prime$ gauge coupling and $Z^\prime$ boson mass), for which we perform parameter scan, and identify a parameter region resolving the SM Higgs vacuum instability. We also examine naturalness of the model. The heavy states associated with the U(1)$^\prime$ symmetry breaking contribute to the SM Higgs self-energy. We find an upper bound on $Z^\prime$ boson mass, $m_{Z^\prime} \lesssim 6$ TeV, in order to avoid a fine-tuning severer than 10 % level. The $Z^\prime$ boson in this mass range can be discovered at the LHC Run-2 in the near future.
Testing the Detectability of Extraterrestrial $\mathrm{O}_2$ with the ELTs using Real Data with Real Noise<|sep|>The future extremely large telescopes (ELTs) are expected to be powerful tools to probe the atmospheres of extrasolar planets using high-dispersion spectroscopy, with the potential to detect molecular oxygen in Earth-like planets transiting nearby, late-type stars. So far, simulations have concentrated on the optical 7600 \AA{} A-band of oxygen using synthetic noise distributions. In this paper, we build upon previous work to predict the detectability of molecular oxygen in nearby, temperate planets by using archival, time-series data of Proxima Centauri from the high-dispersion UVES spectrograph on ESO's Very Large Telescope (VLT). The brightest transiting M-dwarfs are expected to be about 25 times fainter than Proxima, a factor that is similar to the difference in light-gathering power between the VLT and the future ELTs. By injecting synthetic oxygen transmission signals into the UVES data, the $\mathrm{O}_2$ detectability can be studied in the presence of real data with real noise properties. Correcting for the relatively low throughput ($\sim$4%) of the Proxima spectra to an assumed 20% throughput for a high-dispersion spectrograph on the European ELT, we find that the molecular oxygen signature of an Earth-twin transiting a nearby ($d \approx 7 \,\mathrm{pc}$) M5V star can be detected in 20-50 transits (a total of 70-175 hours of observing time). This estimate using more realistic simulations is close to previous predictions. Novel concepts that increase the instrumental throughput can further reduce the time span over which such observations need to be taken.
Comparison of different methods of spatial disaggregation of electricity<|sep|>Energy system models involve various input data sets representing the generation, consumption and transport infrastructure of electricity. Especially energy system models with a focus on the transmission grid require time series of electricity feed-in and consumption in a high spatial resolution. In general, there are two approaches to obtain regionalized time series: top-down and bottom-up. In many cases, both methodologies may be combined to aggregate or disaggregate input data. Furthermore, there exist various approaches to assign regionalized feed-in of renewable energy sources and electrical load to the model's grid connection points. The variety in the regionalization process leads to significant differences on a regional scope, even if global values are the same. We develop a methodology to compare regionalization techniques of input data for photovoltaics, wind and electrical load between various models as well as data assignment techniques to the power grid nodes. We further define two invariants to evaluate the outcome of the regionalization process at the NUTS 3 level, one invariant for the annual profiles and one for the installed capacities. This methodology enabled us to compare different regionalization and assignment workflows using simple parameters, without explicit knowledge of grid topology. Our results show that the resolution of the input data and the use of a top-down or a bottom-up approach are the most determinant factors in the regionalization process.
Simultaneous Embeddability of Two Partitions<|sep|>We study the simultaneous embeddability of a pair of partitions of the same underlying set into disjoint blocks. Each element of the set is mapped to a point in the plane and each block of either of the two partitions is mapped to a region that contains exactly those points that belong to the elements in the block and that is bounded by a simple closed curve. We establish three main classes of simultaneous embeddability (weak, strong, and full embeddability) that differ by increasingly strict well-formedness conditions on how different block regions are allowed to intersect. We show that these simultaneous embeddability classes are closely related to different planarity concepts of hypergraphs. For each embeddability class we give a full characterization. We show that (i) every pair of partitions has a weak simultaneous embedding, (ii) it is NP-complete to decide the existence of a strong simultaneous embedding, and (iii) the existence of a full simultaneous embedding can be tested in linear time.
Topology of atomically thin soft ferroelectric membranes at finite temperature<|sep|>One account of two-dimensional (2D) structural transformations in 2D ferroelectrics predicts an evolution from a structure with Pnm2$_1$ symmetry into a structure with square P4/nmm symmetry and is consistent with experimental evidence, while another argues for a transformation into a structure with rectangular Pnmm symmetry. An analysis of the assumptions made in these models is provided here, and six fundamental results concerning these transformations are contributed as follows: (i) Softened phonon modes produce rotational modes in these materials. (ii) The transformation to a structure with P4/nmm symmetry occurs at the lowest critical temperature $T_c$. (iii) The hypothesis that one unidirectional optical vibrational mode underpins the 2D transformation is unwarranted. (iv) Being successively more constrained, a succession of critical temperatures ($T_c<T_c'<T_c''$) occurs in going from molecular dynamics calculations with the NPT and NVT ensembles onto the model with unidirectional oscillations. (v) The choice of exchange-correlation functional impacts the estimate of the critical temperature. (vi) Crucially, the correct physical picture of these transformations is one in which rotational modes confer a topological character to the 2D transformation via the proliferation of vortices.
New word analogy corpus for exploring embeddings of Czech words<|sep|>The word embedding methods have been proven to be very useful in many tasks of NLP (Natural Language Processing). Much has been investigated about word embeddings of English words and phrases, but only little attention has been dedicated to other languages. Our goal in this paper is to explore the behavior of state-of-the-art word embedding methods on Czech, the language that is characterized by very rich morphology. We introduce new corpus for word analogy task that inspects syntactic, morphosyntactic and semantic properties of Czech words and phrases. We experiment with Word2Vec and GloVe algorithms and discuss the results on this corpus. The corpus is available for the research community.
Probing the Slope of Cluster Mass Profile with Gravitational Einstein Rings: Application to Abell 1689<|sep|>The strong lensing modelling of gravitational ``rings'' formed around massive galaxies is sensitive to the amplitude of the external shear and convergence produced by nearby mass condensations. In current wide field surveys, it is now possible to find out a large number of rings, typically 10 gravitational rings per square degree. We propose here, to systematically study gravitational rings around galaxy clusters to probe the cluster mass profile beyond the cluster strong lensing regions. For cluster of galaxies with multiple arc systems, we show that rings found at various distances from the cluster centre can improve the modelling by constraining the slope of the cluster mass profile. We outline the principle of the method with simple numerical simulations and we apply it to 3 rings discovered recently in Abell~1689. In particular, the lens modelling of the 3 rings confirms that the cluster is bimodal, and favours a slope of the mass profile steeper than isothermal at a cluster radius $\sim 300 \kpc$. These results are compared with previous lens modelling of Abell~1689 including weak lensing analysis. Because of the difficulty arising from the complex mass distribution in Abell~1689, we argue that the ring method will be better implemented on simpler and relaxed clusters.
Detection of frequency spacings in the young O-type binary HD 46149 from CoRoT photometry<|sep|>Using the CoRoT space based photometry of the O-type binary HD46149, stellar atmospheric effects related to rotation can be separated from pulsations, because they leave distinct signatures in the light curve. This offers the possibility of characterising and exploiting any pulsations seismologically. Combining high-quality space based photometry, multi-wavelength photometry, spectroscopy and constraints imposed by binarity and cluster membership, the detected pulsations in HD46149 are analyzed and compared with those for a grid of stellar evolutionary models in a proof-of-concept approach. We present evidence of solar-like oscillations in a massive O-type star, and show that the observed frequency range and spacings are compatible with theoretical predictions. Thus, we unlock and confirm the strong potential of this seismically unexplored region in the HR diagram.
A Hierarchical Reinforcement Learning Method for Persistent Time-Sensitive Tasks<|sep|>Reinforcement learning has been applied to many interesting problems such as the famous TD-gammon and the inverted helicopter flight. However, little effort has been put into developing methods to learn policies for complex persistent tasks and tasks that are time-sensitive. In this paper, we take a step towards solving this problem by using signal temporal logic (STL) as task specification, and taking advantage of the temporal abstraction feature that the options framework provide. We show via simulation that a relatively easy to implement algorithm that combines STL and options can learn a satisfactory policy with a small number of training cases
Imaging the formation of high-energy dispersion anomalies in the actinide UCoGa$_5$<|sep|>We use angle-resolved photoemission spectroscopy (ARPES) to image the emergence of substaintial dispersion anomalies in the electronic renormalization of the actinide compound UCoGa$_5$ which was presumed to belong to a conventional Fermi liquid family. Kinks or abrupt breaks in the slope of the quasiparticle dispersion are detected both at low ($\sim$130 meV) and high ($\sim$1 eV) binding energies below the Fermi energy, ruling out any significant contribution of phonons. We perform numerical calculations to demonstrate that the anomalies are adequately described by coupling between itinerant fermions and spin fluctuations arising from the particle-hole continuum of the spin-orbit split $5f$ states of uranium. These anomalies are resemble the `waterfall' phenomenon of the high-temperature copper-oxide superconductors, suggesting that spin fluctuations are a generic route toward multiform electronic phases in correlated materials as different as high-temperature superconductors and actinides.
Gaussian Process Topic Models<|sep|>We introduce Gaussian Process Topic Models (GPTMs), a new family of topic models which can leverage a kernel among documents while extracting correlated topics. GPTMs can be considered a systematic generalization of the Correlated Topic Models (CTMs) using ideas from Gaussian Process (GP) based embedding. Since GPTMs work with both a topic covariance matrix and a document kernel matrix, learning GPTMs involves a novel component-solving a suitable Sylvester equation capturing both topic and document dependencies. The efficacy of GPTMs is demonstrated with experiments evaluating the quality of both topic modeling and embedding.
An alternative approach to the quasi-Periodic solutions of the Hunter-Saxton hierarchy<|sep|>This paper is dedicated to provide the global solutions of algebro-geometric type for all the equations of a new commuting hierarchy containing the Hunter-Saxton (HS) equation. Our main tools include the zero curvature method to derive the HS hierarchy, the generalized Jacobian variety, the generalized Riemann theta function, the Weyl $m$-fucntions $m_\pm(x,t,z)$, and the pole motion obtained by solving an inverse problem for the Sturm-Liouville equation $L(\psi_1)=-\psi_1^{\prime\prime}=zy\psi_1$. Based on these tools and the theory of nonautonomous differential systems, topological dynamics and ergodic theory, the algebro-geometric solutions are obtained for the entire HS hierarchy.
Overview of EIREX 2010: Computing<|sep|>The first Information Retrieval Education through Experimentation track (EIREX 2010) was run at the University Carlos III of Madrid, during the 2010 spring semester. EIREX 2010 is the first in a series of experiments designed to foster new Information Retrieval (IR) education methodologies and resources, with the specific goal of teaching undergraduate IR courses from an experimental perspective. For an introduction to the motivation behind the EIREX experiments, see the first sections of [Urbano et al., 2011]. For information on other editions of EIREX and related data, see the website at http://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a) to help students get a view of the Information Retrieval process as they would find it in a real-world scenario, either industrial or academic; b) to make students realize the importance of laboratory experiments in Computer Science and have them initiated in their execution and analysis; c) to create a public repository of resources to teach Information Retrieval courses; d) to seek the collaboration and active participation of other Universities in this endeavor. This overview paper summarizes the results of the EIREX 2010 track, focusing on the creation of the test collection and the analysis to assess its reliability.
UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated Input Degradation<|sep|>The fusion of multiple sensor modalities, especially through deep learning architectures, has been an active area of study. However, an under-explored aspect of such work is whether the methods can be robust to degradations across their input modalities, especially when they must generalize to degradations not seen during training. In this work, we propose an uncertainty-aware fusion scheme to effectively fuse inputs that might suffer from a range of known and unknown degradations. Specifically, we analyze a number of uncertainty measures, each of which captures a different aspect of uncertainty, and we propose a novel way to fuse degraded inputs by scaling modality-specific output softmax probabilities. We additionally propose a novel data-dependent spatial temperature scaling method to complement these existing uncertainty measures. Finally, we integrate the uncertainty-scaled output from each modality using a probabilistic noisy-or fusion method. In a photo-realistic simulation environment (AirSim), we show that our method achieves significantly better results on a semantic segmentation task, compared to state-of-art fusion architectures, on a range of degradations (e.g. fog, snow, frost, and various other types of noise), some of which are unknown during training. We specifically improve upon the state-of-art[1] by 28% in mean IoU on various degradations. [1] Abhinav Valada, Rohit Mohan, and Wolfram Burgard. Self-Supervised Model Adaptation for Multimodal Semantic Segmentation. In: arXiv e-prints, arXiv:1808.03833 (Aug. 2018), arXiv:1808.03833. arXiv: 1808.03833 [cs.CV].
On Multi-rate Sequential Data Transmission<|sep|>In this report, we investigate the data transmission model in which a sequence of data is broadcasted to a number of receivers. The receivers, which have different channel capacities, wish to decode the data sequentially at different rates. Our results are applicable to a wide range of scenarios. For instance, it can be employed in the broadcast streaming of a video clip through the internet, so that receivers with different bandwidths can play the video at different speed. Receivers with greater bandwidths can provide a smooth playback, while receivers with smaller bandwidths can play the video at a slower speed, or with short pauses or rebuffering.
Balancing forward and feedback error correction for erasure channels with unreliable feedback<|sep|>The traditional information theoretic approach to studying feedback is to consider ideal instantaneous high-rate feedback of the channel outputs to the encoder. This was acceptable in classical work because the results were negative: Shannon pointed out that even perfect feedback often does not improve capacity and in the context of symmetric DMCs, Dobrushin showed that it does not improve the fixed block-coding error exponents in the interesting high rate regime. However, it has recently been shown that perfect feedback does allow great improvements in the asymptotic tradeoff between end-to-end delay and probability of error, even for symmetric channels at high rate. Since gains are claimed with ideal instantaneous feedback, it is natural to wonder whether these improvements remain if the feedback is unreliable or otherwise limited. Here, packet-erasure channels are considered on both the forward and feedback links. First, the feedback channel is considered as a given and a strategy is given to balance forward and feedback error correction in the suitable information-theoretic limit of long end-to-end delays. At high enough rates, perfect-feedback performance is asymptotically attainable despite having only unreliable feedback! Second, the results are interpreted in the zero- sum case of "half-duplex" nodes where the allocation of bandwidth or time to the feedback channel comes at the direct expense of the forward channel. It turns out that even here, feedback is worthwhile since dramatically lower asymptotic delays are possible by appropriately balancing forward and feedback error correction. The results easily generalize to channels with strictly positive zero-undeclared-error capacities.
Benchmarking Neural Network Robustness to Common Corruptions and Perturbations<|sep|>In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.
On fixed-parameter algorithms for Split Vertex Deletion<|sep|>In the Split Vertex Deletion problem, given a graph G and an integer k, we ask whether one can delete k vertices from the graph G to obtain a split graph (i.e., a graph, whose vertex set can be partitioned into two sets: one inducing a clique and the second one inducing an independent set). In this paper we study fixed-parameter algorithms for Split Vertex Deletion parameterized by k: we show that, up to a factor quasipolynomial in k and polynomial in n, the Split Vertex Deletion problem can be solved in the same time as the well-studied Vertex Cover problem. Plugging the currently best fixed-parameter algorithm for Vertex Cover due to Chen et al. [TCS 2010], we obtain an algorithm that solves Split Vertex Deletion in time O(1.2738^k * k^O(log k) + n^O(1)). To achieve our goal, we prove the following structural result that may be of independent interest: for any graph G we may compute a family P of size n^O(log n) containing partitions of V(G) into two parts, such for any two disjoint subsets X_C, X_I of V(G) where G[X_C] is a clique and G[X_I] is an independent set, there is a partition in P which contains all vertices of X_C on one side and all vertices of X_I on the other.
The spins of compact objects born from helium stars in binary systems<|sep|>The angular momentum (AM) content of massive stellar cores helps to determine the natal spin rates of neutron stars and black holes. Asteroseismic measurements of low-mass stars have proven that stellar cores rotate slower than predicted by most prior work, so revised models are necessary. In this work, we apply an updated AM transport model based on the Tayler instability to massive helium stars in close binaries, in which tidal spin-up can greatly increase the star's AM. Consistent with prior work, these stars can produce highly spinning black holes upon core-collapse if the orbital period is less than $P_{\rm orb} \lesssim \! 1 \, {\rm day}$. For neutron stars, we predict a strong correlation between the pre-explosion mass and the neutron star rotation rate, with millisecond periods ($P_{\rm NS} \lesssim 5 \, {\rm ms}$) only achievable for massive ($M \gtrsim 10 \, M_\odot$) helium stars in tight ($P_{\rm orb} \lesssim 1 \, {\rm day}$) binaries. Finally, we discuss our models in relation to type Ib/c supernovae, superluminous supernove, gamma-ray bursts, and LIGO measurements of black hole spins. Our models are roughly consistent with the rates and energetics of these phenomena, with the exception of broad-lined Ic supernovae, whose high rates and ejecta energies are difficult to explain.
On-line Joint Limit Avoidance for Torque Controlled Robots by Joint Space Parametrization<|sep|>This paper proposes control laws ensuring the stabilization of a time-varying desired joint trajectory, as well as joint limit avoidance, in the case of fully-actuated manipulators. The key idea is to perform a parametrization of the feasible joint space in terms of exogenous states. It follows that the control of these states allows for joint limit avoidance. One of the main outcomes of this paper is that position terms in control laws are replaced by parametrized terms, where joint limits must be avoided. Stability and convergence of time-varying reference trajectories obtained with the proposed method are demonstrated to be in the sense of Lyapunov. The introduced control laws are verified by carrying out experiments on two degrees-of-freedom of the humanoid robot iCub.
The effect of heterogeneity on flocking behavior and systemic risk<|sep|>The goal of this paper is to study organized flocking behavior and systemic risk in heterogeneous mean-field interacting diffusions. We illustrate in a number of case studies the effect of heterogeneity in the behavior of systemic risk in the system, i.e., the risk that several agents default simultaneously as a result of interconnections. We also investigate the effect of heterogeneity on the "flocking behavior" of different agents, i.e., when agents with different dynamics end up following very similar paths and follow closely the mean behavior of the system. Using Laplace asymptotics, we derive an asymptotic formula for the tail of the loss distribution as the number of agents grows to infinity. This characterizes the tail of the loss distribution and the effect of the heterogeneity of the network on the tail loss probability.
Search for magnetic monopoles with the MoEDAL prototype trapping detector in 8 TeV proton-proton collisions at the LHC<|sep|>The MoEDAL experiment is designed to search for magnetic monopoles and other highly-ionising particles produced in high-energy collisions at the LHC. The largely passive MoEDAL detector, deployed at Interaction Point 8 on the LHC ring, relies on two dedicated direct detection techniques. The first technique is based on stacks of nuclear-track detectors with surface area $\sim$18 m$^2$, sensitive to particle ionisation exceeding a high threshold. These detectors are analysed offline by optical scanning microscopes. The second technique is based on the trapping of charged particles in an array of roughly 800 kg of aluminium samples. These samples are monitored offline for the presence of trapped magnetic charge at a remote superconducting magnetometer facility. We present here the results of a search for magnetic monopoles using a 160 kg prototype MoEDAL trapping detector exposed to 8 TeV proton-proton collisions at the LHC, for an integrated luminosity of 0.75 fb$^{-1}$. No magnetic charge exceeding $0.5g_{\rm D}$ (where $g_{\rm D}$ is the Dirac magnetic charge) is measured in any of the exposed samples, allowing limits to be placed on monopole production in the mass range 100 GeV$\leq m \leq$ 3500 GeV. Model-independent cross-section limits are presented in fiducial regions of monopole energy and direction for $1g_{\rm D}\leq|g|\leq 6g_{\rm D}$, and model-dependent cross-section limits are obtained for Drell-Yan pair production of spin-1/2 and spin-0 monopoles for $1g_{\rm D}\leq|g|\leq 4g_{\rm D}$. Under the assumption of Drell-Yan cross sections, mass limits are derived for $|g|=2g_{\rm D}$ and $|g|=3g_{\rm D}$ for the first time at the LHC, surpassing the results from previous collider experiments.
TeV-scale seesaw mechanism catalyzed by the electron mass<|sep|>We construct a model in which the neutrino Dirac mass terms are of order the electron mass and the seesaw mechanism proceeds via right-handed neutrinos with masses of order TeV. In our model the spectra of the three light and of the three heavy neutrinos are closely related. Since the mixing between light and heavy neutrinos is small, the model predicts no effects in pp and p \bar p colliders. Possible signatures of the model are the lepton-number-violating process e- e- --> H- H-, where H- is a charged scalar particle, lepton-flavour-violating decays like mu- --> e- e+ e-, or a sizable contribution to the anomalous magnetic dipole moment of the muon.
Cost Efficient Repository Management for Cloud-Based On-Demand Video Streaming<|sep|>Video transcoding is the process of converting a video to the format supported by the viewer's device. Video transcoding requires huge storage and computational resources, thus, many video stream providers choose to carry it out on the cloud. Video streaming providers generally need to prepare several formats of the same video (termed pre-transcoding) and stream the appropriate format to the viewer. However, pre-transcoding requires enormous storage space and imposes a significant cost to the stream provider. More importantly, pre-transcoding proven to be inefficient due to the long-tail access pattern to video streams in a repository. To reduce the incurred cost, in this research, we propose a method to partially pre-transcode video streams and re-transcode the rest of it in an on-demand manner. We will develop a method to strike a trade-off between pre-transcoding and on-demand transcoding of video streams to reduce the overall cost. Experimental results show the efficiency of our approach, particularly, when a high percentage of videos are accessed frequently. In such repositories, the proposed approach reduces the incurred cost by up to 70\%.
Towards Better Analysis of Deep Convolutional Neural Networks<|sep|>Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable.
Symmetry aspects of nonholonomic field theories<|sep|>The developments in this paper are concerned with nonholonomic field theories in the presence of symmetries. Having previously treated the case of vertical symmetries, we now deal with the case where the symmetry action can also have a horizontal component. As a first step in this direction, we derive a new and convenient form of the field equations of a nonholonomic field theory. Nonholonomic symmetries are then introduced as symmetry generators whose virtual work is zero along the constraint submanifold, and we show that for every such symmetry, there exists a so-called momentum equation, describing the evolution of the associated component of the momentum map. Keeping up with the underlying geometric philosophy, a small modification of the derivation of the momentum lemma allows us to treat also generalized nonholonomic symmetries, which are vector fields along a projection. Such symmetries arise for example in practical examples of nonholonomic field theories such as the Cosserat rod, for which we recover both energy conservation (a previously known result), as well as a modified conservation law associated with spatial translations.
Predicting the Output Structure of Sparse Matrix Multiplication with Sampled Compression Ratio<|sep|>Sparse general matrix multiplication (SpGEMM) is a fundamental building block in numerous scientific applications. One critical task of SpGEMM is to compute or predict the structure of the output matrix (i.e., the number of nonzero elements per output row) for efficient memory allocation and load balance, which impact the overall performance of SpGEMM. Existing work either precisely calculates the output structure or adopts upper-bound or sampling-based methods to predict the output structure. However, these methods either take much execution time or are not accurate enough. In this paper, we propose a novel sampling-based method with better accuracy and low costs compared to the existing sampling-based method. The proposed method first predicts the compression ratio of SpGEMM by leveraging the number of intermediate products (denoted as FLOP) and the number of nonzero elements (denoted as NNZ) of the same sampled result matrix. And then, the predicted output structure is obtained by dividing the FLOP per output row by the predicted compression ratio. We also propose a reference design of the existing sampling-based method with optimized computing overheads to demonstrate the better accuracy of the proposed method. We construct 625 test cases with various matrix dimensions and sparse structures to evaluate the prediction accuracy. Experimental results show that the absolute relative errors of the proposed method and the reference design are 1.56\% and 8.12\%, respectively, on average, and 25\% and 156\%, respectively, in the worst case.
Minkowski functionals for phase behavior under confinement<|sep|>In this work, the Minkowski functionals are used as a framework to study how morphology (i.e. the shape of a structure) and topology (i.e. how different structures are connected) influence wall adsorption and capillary condensation under tight confinement. Numerical simulations based on classical density functional theory (DFT) are run for a wide variety of geometries using both hard-sphere and Lennard-Jones fluids. These DFT computations are compared to results obtained using the Minkowski functionals. It is found that the Minkowski functionals can provide a good description of the behavior of Lennard-Jones fluids down to small system sizes. In addition, through decomposition of the free energy, the Minkowski functionals provide a good framework to better understand what are the dominant contributions to the physics of a system. Lastly, while studying the phase envelope shift as a function of the Minkowski functionals it is found that topology has a different effect depending on whether the phase transition under consideration is a first- or a second-order transition.
Low-Level Physiological Implications of End-to-End Learning of Speech Recognition<|sep|>Current speech recognition architectures perform very well from the point of view of machine learning, hence user interaction. This suggests that they are emulating the human biological system well. We investigate whether the inference can be inverted to provide insights into that biological system; in particular the hearing mechanism. Using SincNet, we confirm that end-to-end systems do learn well known filterbank structures. However, we also show that wider band-width filters are important in the learned structure. Whilst some benefits can be gained by initialising both narrow and wide-band filters, physiological constraints suggest that such filters arise in mid-brain rather than the cochlea. We show that standard machine learning architectures must be modified to allow this process to be emulated neurally.
A New Method of Accelerated Bayesian Inference for Comparable Mass Binaries in both Ground and Space-Based Gravitational Wave Astronomy<|sep|>With the advance in computational resources, Bayesian inference is increasingly becoming the standard tool of practise in GW astronomy. However, algorithms such as Markov Chain Monte Carlo (MCMC) require a large number of iterations to guarantee convergence to the target density. Each chain demands a large number of evaluations of the likelihood function, and in the case of a Hessian MCMC, calculations of the Fisher information matrix for use as a proposal distribution. As each iteration requires the generation of at least one gravitational waveform, we very quickly reach a point of exclusion for current Bayesian algorithms, especially for low mass systems where the length of the waveforms is large and the waveform generation time is on the order of seconds. This suddenly demands a timescale of many weeks for a single MCMC. As each likelihood and Fisher information matrix calculation requires the evaluation of noise-weighted scalar products, we demonstrate that by using the linearity of integration, and the fact that more than 90% of the generation time is spent at frequencies less that one third of the maximum, we can construct composite integrals that speed up the MCMCs for comparable mass binaries by a factor of between 3.5 and 5.5, depending on the waveform length. This method is both source and detector type independent, and can be applied to any waveform that displays significant frequency evolution, such as stellar mass binaries with Advanced LIGO/Virgo, as well as supermassive black holes with eLISA
Parity qubits and poor man's Majorana bound states in double quantum dots<|sep|>We study a double quantum dot connected via a common superconducting lead and show that this system can be tuned to host one Majorana bound state (MBS) on each dot. We call them "poor man's Majorana bound states" since they are not topologically protected, but otherwise share the properties of MBS formed in topological superconductors. We describe the conditions for the existence of the two spatially separated MBS, which include breaking of spin degeneracy in the two dots, with the spins polarized in different directions. Therefore, we propose to use a magnetic field configuration where the field directions on the two dot form an angle. By control of this angle the cross Andreev reflection and the tunnel amplitudes can be tuned to be approximately equal, which is a requirement for the formation of the MBS. We show that the fermionic state encoded in the two Majoranas constitutes a parity qubit, which is non-local and can only be measured by probing both dots simultaneously. Using a many-particle basis for the MBS, we discuss the role of interactions and show that inter-dot interactions always lift the degeneracy. We also show how the MBS can be probed by transport measurements and discuss how the combination of several such double dot systems allows for entanglement of parity qubits and measurement of their dephasing times.
Trapped fermions in a synthetic non-Abelian gauge field<|sep|>On increasing the coupling strength ($\lambda$) of a non-Abelian gauge field that induces a generalized Rashba spin-orbit interaction, the topology of the Fermi surface of a homogeneous gas of noninteracting fermions of density $\rho \sim \kf^3$ undergoes a change at a critical value, $\lambda_T \approx \kf$ [Phys. Rev. B {\bf 84}, 014512 (2011)]. In this paper we analyze how this phenomenon affects the size and shape of a cloud of spin-$\half$ fermions trapped in a harmonic potential such as those used in cold atom experiments. We develop an adiabatic formulation, including the concomitant Pancharatnam-Berry phase effects, for the one particle states in the presence of a trapping potential and the gauge field, obtaining approximate analytical formulae for the energy levels for some high symmetry gauge field configurations of interest. An analysis based on the local density approximation reveals that, for a given number of particles, the cloud shrinks in a {\em characteristic fashion with increasing $\lambda$}. For an isotropic harmonic trap, the local density approximation predicts a spherical cloud for all gauge field configurations, which are anisotropic in general. We show, via a calculation of the cloud shape using exact eigenstates, that for certain gauge field configurations there is systematic and observable anisotropy in the cloud shape that increases with increasing gauge coupling $\lambda$. These results should be useful in the design of cold atom experiments with fermions in non-Abelian gauge fields. An important spin-off of our adiabatic formulation is that it reveals exciting possibilities for the cold-atom realization of interesting condensed matter Hamiltonians (eg. quantum hall spherical geometry) by using a non-Abelian gauge field in conjunction with another potential.
Discrete-Time Fractional-Order PID Controller: Definition, Tuning, Digital Realization and Experimental Results<|sep|>In some of the complicated control problems we have to use the controllers that apply nonlocal operators to the error signal to generate the control. Currently, the most famous controller with nonlocal operators is the fractional-order PID (FOPID). Commonly, after tuning the parameters of FOPID controller, its transfer function is discretized (for realization purposes) using the so-called generating function. This discretization is the origin of some errors and unexpected results in feedback systems. It may even happen that the controller obtained by discretizing a FOPID controller works worse than a directly-tuned discrete-time classical PID controller. Moreover, FOPID controllers cannot directly be applied to the processes modeled by, e.g., the ARMA or ARMAX model. The aim of this paper is to propose a discrete-time version of the FOPID controller and discuss on its properties and applications. Similar to the FOPID controller, the proposed structure applies nonlocal operators (with adjustable memory length) to the error signal. Two methods for tuning the parameters of the proposed controller are developed and it is shown that the proposed controller has the capacity of solving complicated control problems with a high performance.
Statistical Properties of Microstructure Noise<|sep|>We study the estimation of moments and joint moments of microstructure noise. Estimators of arbitrary order of (joint) moments are provided, for which we establish consistency as well as central limit theorems. In particular, we provide estimators of auto-covariances and auto-correlations of the noise. Simulation studies demonstrate excellent performance of our estimators even in the presence of jumps and irregular observation times. Empirical studies reveal (moderate) positive auto-correlation of the noise for the stocks tested.
Continuum limit of the $D$ meson, $D_s$ meson and charmonium spectrum from $N_f=2+1+1$ twisted mass lattice QCD<|sep|>We compute masses of $D$ meson, $D_s$ meson and charmonium states using $N_f=2+1+1$ Wilson twisted mass lattice QCD. All results are extrapolated to physical light quark masses, physical strange and charm quark masses and to the continuum. Our analysis includes states with spin $J = 0,1,2$, parity $\mathcal{P} = -,+$ and in case of charmonium also charge conjugation $\mathcal{C} = -,+$. Computations are based on a large set of quark-antiquark meson creation operators. We investigate and quantify all sources of systematic errors, including fitting range uncertainties, finite volume effects, isospin breaking effects and the choice of the fitting ansatz for the combined chiral and continuum extrapolation such that the resulting meson masses can be compared directly and in a meaningful way to experimental results. Within combined statistical and systematic errors, which are between below two per mille and three percent, our results agree with available experimental results for most of the states. In the few cases where we observe discrepancies, we discuss possible reasons.
Laplace-Laplace analysis of the fractional Poisson process<|sep|>We generate the fractional Poisson process by subordinating the standard Poisson process to the inverse stable subordinator. Our analysis is based on application of the Laplace transform with respect to both arguments of the evolving probability densities.
Data-driven Modelling of Ship Maneuvers in Waves via Dynamic Mode Decomposition<|sep|>A data-driven and equation-free approach is proposed and discussed to model ships maneuvers in waves, based on the dynamic mode decomposition (DMD). DMD is a dimensionality-reduction/reduced-order modeling method, which provides a linear finite-dimensional representation of a possibly nonlinear system dynamics by means of a set of modes with associated oscillation frequencies and decay/growth rates. DMD also allows for short-term future estimates of the system's state, which can be used for real-time prediction and control. Here, the objective of the DMD is the analysis and forecast of the trajectories/motions/forces of ships operating in waves, offering a complementary efficient method to equation-based system identification approaches. Results are presented for the course keeping of a free-running naval destroyer (5415M) in irregular stern-quartering waves and for the free-running KRISO Container Ship (KCS) performing a turning circle in regular waves. Results are overall promising and show how DMD is able to identify the most important modes and forecast the system's state with reasonable accuracy up to two wave encounter periods.
Processes $\tau^- \to \pi^- \pi^0 \nu_{\tau}$ and $e^{+}e^{-} \to \pi^{+} \pi^{-}$ in the chiral NJL model with final state interactions taken into account<|sep|>The processes $\tau^- \to \pi^- \pi^0 \nu_{\tau}$ and $e^{+}e^{-} \to \pi^{+} \pi^{-}$ are considered within the chiral Nambu--Jona-Lasinio model with taking into account the pion final state interactions beyond the leading $1/N_c$ approximation. The contribution of the loop correction caused by a $\rho$ meson exchange between the pions, which gives the main contribution in the P-wave channel, is caclulated. The results for both processes are in a satisfactory agreement with experimental data.
Dark Radiation Constraints on Mixed Axion/Neutralino Dark Matter<|sep|>Recent analyses of WMAP9 data show that dark radiation-- parametrized by the apparent number of additional neutrinos \Delta N_{eff} contributing to the cosmic expansion-- is consistent with the Standard Model and bounded from above by about \Delta N_{eff} ~< 0.5 at 95% CL. We consider the mixed axion/neutralino cold dark matter scenario which arises in R-parity conserving supersymmetric (SUSY) models wherein the strong CP problem is solved by hadronic axions with a concommitant axion(a)/saxion(s)/axino(\ta) supermultiplet. Our new results include improved calculations of thermal axion and saxion production and include effects of saxion decay to axinos and axions. We show that the above bound on \Delta N_{eff} is easily satisfied if saxions are mainly thermally produced and m_{LSP} < m_{\ta} \lesssim m_s. However, if the dominant mechanism of saxion production is through coherent oscillations, the WMAP9 data provides a strong bound on saxion production followed by saxion decays to axions. Furthermore we show that scenarios with mixed neutralino/axion dark matter are highly constrained by combined WMAP9, BBN and Xe-100 constraints. In particular, supersymmetric models with a standard overabundance of neutralino dark matter are excluded for all values of the Peccei-Quinn breaking scale. Next generation WIMP direct detection experiments may be able to discover or exclude mixed axion-neutralino CDM scenarios where $s\to aa$ is the dominant saxion decay mode.
Single-charge rotating black holes in four-dimensional gauged supergravity<|sep|>We consider four-dimensional U(1)^4 gauged supergravity, and obtain asymptotically AdS_4, non-extremal, charged, rotating black holes with one non-zero U(1) charge. The thermodynamic quantities are computed. We obtain a generalization that includes a NUT parameter. The general solution has a discrete symmetry involving inversion of the rotation parameter, and has a string frame metric that admits a rank-2 Killing-Stackel tensor.
Efficient Continuous Top-$k$ Geo-Image Search on Road Network<|sep|>With the rapid development of mobile Internet and cloud computing technology, large-scale multimedia data, e.g., texts, images, audio and videos have been generated, collected, stored and shared. In this paper, we propose a novel query problem named continuous top-$k$ geo-image query on road network which aims to search out a set of geo-visual objects based on road network distance proximity and visual content similarity. Existing approaches for spatial textual query and geo-image query cannot address this problem effectively because they do not consider both of visual content similarity and road network distance proximity on road network. In order to address this challenge effectively and efficiently, firstly we propose the definition of geo-visual objects and continuous top-$k$ geo-visual objects query on road network, then develop a score function for search. To improve the query efficiency in a large-scale road network, we propose the search algorithm named geo-visual search on road network based on a novel hybrid indexing framework called VIG-Tree, which combines G-Tree and visual inverted index technique. In addition, an important notion named safe interval and results updating rule are proposed, and based on them we develop an efficient algorithm named moving monitor algorithm to solve continuous query. Experimental evaluation on real multimedia dataset and road network dataset illustrates that our solution outperforms state-of-the-art method.
Quantum-Space Attacks<|sep|>Theoretical quantum key distribution (QKD) protocols commonly rely on the use of qubits (quantum bits). In reality, however, due to practical limitations, the legitimate users are forced to employ a larger quantum (Hilbert) space, say a quhexit (quantum six-dimensional) space, or even a much larger quantum Hilbert space. Various specific attacks exploit of these limitations. Although security can still be proved in some very special cases, a general framework that considers such realistic QKD protocols, as well as} attacks on such protocols, is still missing. We describe a general method of attacking realistic QKD protocols, which we call the `quantum-space attack'. The description is based on assessing the enlarged quantum space actually used by a protocol, the `quantum space of the protocol'. We demonstrate these new methods by classifying various (known) recent attacks against several QKD schemes, and by analyzing a novel attack on interferometry-based QKD.
Numerical Simulation of Grating Couplers for Mode Multiplexed Systems<|sep|>A numerical investigation of a two dimensional integrated fiber grating coupler capable of exciting several LP fiber modes in both TE and TM polarization is presented. Simulation results and an assessment of the numerical complexity of the 3D, fully vectorial finite element model of the device are shown.
Shallow vs deep learning architectures for white matter lesion segmentation in the early stages of multiple sclerosis<|sep|>In this work, we present a comparison of a shallow and a deep learning architecture for the automated segmentation of white matter lesions in MR images of multiple sclerosis patients. In particular, we train and test both methods on early stage disease patients, to verify their performance in challenging conditions, more similar to a clinical setting than what is typically provided in multiple sclerosis segmentation challenges. Furthermore, we evaluate a prototype naive combination of the two methods, which refines the final segmentation. All methods were trained on 32 patients, and the evaluation was performed on a pure test set of 73 cases. Results show low lesion-wise false positives (30%) for the deep learning architecture, whereas the shallow architecture yields the best Dice coefficient (63%) and volume difference (19%). Combining both shallow and deep architectures further improves the lesion-wise metrics (69% and 26% lesion-wise true and false positive rate, respectively).
Noncommutative Galois Extension and Graded q-Differential Algebra<|sep|>We show that a semi-commutative Galois extension of a unital associative algebra can be endowed with the structure of a graded q-differential algebra. We study the first and higher order noncommutative differential calculus of semi-commutative Galois extension induced by the graded q-differential algebra. As an example we consider the quaternions which can be viewed as the semi-commutative Galois extension of complex numbers.
A matrix theoretic characterization of the strongly reachable subspace<|sep|>In this paper, we provide novel characterizations of the weakly unobservable and the strongly reachable subspaces corresponding to a given state-space system. These characterizations provide closed-form representations for the said subspaces. In this process, we establish that the strongly reachable subspace is intimately related to the space of admissible impulsive inputs. We also show how to calculate the dimensions of these subspaces from the transfer matrix of the system.
Investigation of Bose condensation in ideal Bose gas trapped under generic power law potential in $d$ dimension<|sep|>The changes in characteristics of Bose condensation of ideal Bose gas due to an external generic power law potential $U=\sum_{i=1} ^d c_i |\frac{x_i}{a_i}|^{n_i}$ are studied carefully. Detailed calculation of Kim $et$ $al.$ (S. H. Kim, C. K. Kim and K. Nahm, J Phys. Condens. Matter 11 10269 (1999).) yielded the hierarchy of condensation transitions with changing fractional dimensionality. In this manuscript, some theorems regarding specific heat at constant volume $C_V$ are presented. Careful examination of these theorems reveal the existence of hidden hierarchy of the condensation transition in trapped systems as well.
Quenched lattice calculation of semileptonic heavy-light meson form factors<|sep|>We calculate, in the continuum limit of quenched lattice QCD, the matrix elements of the heavy-heavy vector current between heavy-light pseudoscalar meson states. We present the form factors for different values of the initial and final meson masses at finite momentum transfer. In particular, we calculate the non-perturbative correction to the differential decay rate of the process B --> D l nu including the case of a non-vanishing lepton mass.
Interference Reduction in Music Recordings Combining Kernel Additive Modelling and Non-Negative Matrix Factorization<|sep|>In live and studio recordings unexpected sound events often lead to interferences in the signal. For non-stationary interferences, sound source separation techniques can be used to reduce the interference level in the recording. In this context, we present a novel approach combining the strengths of two algorithmic families: NMF and KAM. The recent KAM approach applies robust statistics on frames selected by a source-specific kernel to perform source separation. Based on semi-supervised NMF, we extend this approach in two ways. First, we locate the interference in the recording based on detected NMF activity. Second, we improve the kernel-based frame selection by incorporating an NMF-based estimate of the clean music signal. Further, we introduce a temporal context in the kernel, taking some musical structure into account. Our experiments show improved separation quality for our proposed method over a state-of-the-art approach for interference reduction.
Axial quasi-normal modes of Einstein-Gauss-Bonnet-dilaton neutron stars<|sep|>We investigate axial quasi-normal modes of realistic neutron stars in Einstein-Gauss-Bonnet-dilaton gravity. We consider 8 realistic equations of state containing nuclear, hyperonic, and hybrid matter. We focus on the fundamental curvature mode and compare the results with those of pure Einstein theory. We observe that the frequency of the modes is increased by the presence of the Gauss-Bonnet-dilaton, while the impact on the damping time is typically smaller. Interestingly, we obtain that universal relations valid in pure Einstein theory still hold for Einstein-Gauss-Bonnet-dilaton gravity, and we propose a method to use these phenomenological relations to constrain the value of the Gauss-Bonnet coupling.
Disentangling and modeling interactions in fish with burst-and-coast swimming<|sep|>We combine extensive data analyses with a modeling approach to measure, disentangle, and reconstruct the actual functional form of interactions involved in the coordination of swimming in Rummy-nose tetra (Hemigrammus rhodostomus). This species of fish performs burst-and-coast swimming behavior that consists of sudden heading changes combined with brief accelerations followed by quasi-passive, straight decelerations. We quantify the spontaneous stochastic behavior of a fish and the interactions that govern wall avoidance and the attraction and alignment to a neighboring fish, the latter by exploiting general symmetry constraints for the interactions. In contrast with previous experimental works, we find that both attraction and alignment behaviors control the reaction of fish to a neighbor. We then exploit these results to build a model of spontaneous burst-and-coast swimming and interactions of fish, with all parameters being estimated or directly measured from experiments. This model quantitatively reproduces the key features of the motion and spatial distributions observed in experiments with a single fish and with two fish. This demonstrates the power of our method that exploits large amounts of data for disentangling and fully characterizing the interactions that govern collective behaviors in animals groups. Moreover, we introduce the notions of "dumb" and "intelligent" active matter and emphasize and clarify the strong differences between them.
Out of equilibrium dynamics with Matrix Product States<|sep|>Theoretical understanding of strongly correlated systems in one spatial dimension (1D) has been greatly advanced by the density-matrix renormalization group (DMRG) algorithm, which is a variational approach using a class of entanglement-restricted states called Matrix Product States (MPSs). However, DRMG suffers from inherent accuracy restrictions when multiple states are involved due to multi-state targeting and also the approximate representation of the Hamiltonian and other operators. By formulating the variational approach of DMRG explicitly for MPSs one can avoid errors inherent in the multi-state targeting approach. Furthermore, by using the Matrix Product Operator (MPO) formalism, one can exactly represent the Hamiltonian and other operators. The MPO approach allows 1D Hamiltonians to be templated using a small set of finite state automaton rules without reference to the particular microscopic degrees of freedom. We present two algorithms which take advantage of these properties: eMPS to find excited states of 1D Hamiltonians and tMPS for the time evolution of a generic time-dependent 1D Hamiltonian. We properly account for time-ordering of the propagator such that the error does not depend on the rate of change of the Hamiltonian. Our algorithms use only the MPO form of the Hamiltonian, and so are applicable to microscopic degrees of freedom of any variety, and do not require Hamiltonian-specialized implementation. We benchmark our algorithms with a case study of the Ising model, where the critical point is located using entanglement measures. We then study the dynamics of this model under a time-dependent quench of the transverse field through the critical point. Finally, we present studies of a dipolar, or long-range Ising model, again using entanglement measures to find the critical point and study the dynamics of a time-dependent quench through the critical point.
Structure and dynamics in low density regions: galaxy-galaxy correlations inside cosmic voids<|sep|>We compute the galaxy-galaxy correlation function of low-luminosity SDSS-DR7 galaxies $(-20 < M_{\rm r} - 5\log_{10}(h) < -18)$ inside cosmic voids identified in a volume limited sample of galaxies at $z=0.085$. To identify voids, we use bright galaxies with $M_{\rm r} - 5\log_{10}(h) < -20.0$. We find that structure in voids as traced by faint galaxies is mildly non-linear as compared with the general population of galaxies with similar luminosities. This implies a redshift-space correlation function with a similar shape than the real-space correlation albeit a normalization factor. The redshift space distortions of void galaxies allow to calculate pairwise velocity distributions which are consistent with an exponential model with a pairwise velocity dispersion of $w \sim 50-70$ km/s, significantly lower than the global value of $w \sim 500$ km/s. We also find that the internal structure of voids as traced by faint galaxies is independent of void environment, namely the correlation functions of galaxies residing in void-in-void or void-in-shell regions are identical within uncertainties. We have tested all our results with the semi-analytic catalogue MDPL2-\textsc{Sag} finding a suitable agreement with the observations in all topics studied.
New very massive stars in Cygnus OB2<|sep|>The compact association Cygnus OB2 is known to contain a large population of massive stars, but its total mass is currently a matter of debate. While recent surveys have uncovered large numbers of OB stars in the area around Cyg OB2, detailed study of the optically brightest among them suggests that most are not part of the association. We observed an additional sample of optically faint OB star candidates, with the aim of checking if more obscured candidates are correspondingly more likely to be members of Cyg OB2. Low resolution spectra of 9 objects allow the rejection of one foreground star and the selection of four O-type stars, which were later observed at higher resolution. In a subsequent run, we observed three more stars in the classification region and three other stars in the far red. We identify five (perhaps six) new evolved very massive stars and three main sequence O-type stars, all of which are likely to be members of Cyg OB2. The new findings allow a much better definition of the upper HR diagram, suggesting an age ~2.5Myr for the association and hinting that the O3-5 supergiants in the association are blue stragglers, either younger or following a different evolutionary path from other cluster members. Though the bulk of the early stars seems to belong to an (approximately) single-age population, there is ample evidence for the presence of somewhat older stars at the same distance. Our results suggest that, even though Cyg OB2 is unlikely to contain as many as 100 O-type stars, it is indeed substantially more massive than was thought prior to recent infrared surveys.
It's Time: OS Mechanisms for Enforcing Asymmetric Temporal Integrity<|sep|>Mixed-criticality systems combine real-time components of different levels of criticality, i.e. severity of failure, on the same processor, in order to obtain good resource utilisation. They must guarantee deadlines of highly-critical tasks at the expense of lower-criticality ones in the case of overload. Present operating systems provide inadequate support for this kind of system, which is of growing importance in avionics and other verticals. We present an approach that provides the required asymmetric integrity and its implementation in the high-assurance seL4 microkernel.
Combining SOS and Moment Relaxations with Branch and Bound to Extract Solutions to Global Polynomial Optimization Problems<|sep|>In this paper, we present a branch and bound algorithm for extracting approximate solutions to Global Polynomial Optimization (GPO) problems with bounded feasible sets. The algorithm is based on a combination of SOS/Moment relaxations and successively bisecting a hyper-rectangle containing the feasible set of the GPO problem. At each iteration, the algorithm makes a comparison between the volume of the hyper-rectangles and their associated lower bounds to the GPO problem obtained by SOS/Moment relaxations to choose and subdivide an existing hyper-rectangle. For any desired accuracy, if we use sufficiently large order of SOS/Moment relaxations, then the algorithm is guaranteed to return a suboptimal point in a certain sense. For a fixed order of SOS/Moment relaxations, the complexity of the algorithm is linear in the number of iterations and polynomial in the number of constrains. We illustrate the effectiveness of the algorithm for a 6-variable, 5-constraint GPO problem for which the ideal generated by the equality constraints is not zero-dimensional - a case where the existing Moment-based approach for extracting the global minimizer might fail.
Inference for Stellar Opacities from Seismic Studies of the Hybrid $\beta$ Cep/SPB pulsators<|sep|>We present a comprehensive seismic study of the three pulsating stars of $\beta$ Cep/SPB type: $\nu$ Eridani, 12 Lacertae and $\gamma$ Pegasi. Models with the modified mean opacity profile are constructed in order to account for both the observed frequency range and the values of some individual frequencies. To decrease the number of possible solutions, we make use of the non-adiabatic parameter $f$, whose value is very sensitive to subphotospheric layers where pulsations are driven. This complex seismic modelling show the need for a significant modification of the opacity profile.
The Effect of Pair-Instability Mass Loss on Black Hole Mergers<|sep|>Mergers of two stellar origin black holes are a prime source of gravitational waves and are under intensive investigations. One crucial ingredient in their modeling has so far been neglected. Pair-instability pulsation supernovae with associated severe mass loss may suppress formation of massive black holes, decreasing black hole merger rates for the highest black hole masses. The mass loss associated with pair-instability pulsation supernovae limits the Population I/II stellar-origin black hole mass to 50 Msun, in tension with earlier predictions that the maximum black hole mass could be as high as 100 Msun. Suppression of double black hole merger rates by pair-instability pulsation supernovae is negligible for our evolutionary channel. Our standard evolutionary model with inclusion of pair-instability pulsation supernovae and with pair-instability supernovae is fully consistent with the LIGO detections of black hole mergers: GW150914, GW151226 and LVT151012. The LIGO observations seem to exclude high (>400 km/s) BH natal kicks. We predict the detection of several up to 60 BH-BH mergers with total mass 10--150 Msun (most likely range: 20--80 Msun) in the forthcoming 60 effective days of LIGO O2 observations.
Interferometric Observations of Rapidly Rotating Stars<|sep|>Optical interferometry provides us with a unique opportunity to improve our understanding of stellar structure and evolution. Through direct observation of rotationally distorted photospheres at sub-milliarcsecond scales, we are now able to characterize latitude dependencies of stellar radius, temperature structure, and even energy transport. These detailed new views of stars are leading to revised thinking in a broad array of associated topics, such as spectroscopy, stellar evolution, and exoplanet detection. As newly advanced techniques and instrumentation mature, this topic in astronomy is poised to greatly expand in depth and influence.
Quantum tunneling and black hole spectroscopy<|sep|>The entropy-area spectrum of a black hole has been a long-standing and unsolved problem. Based on a recent methodology introduced by two of the authors, for the black hole radiation (Hawking effect) as tunneling effect, we obtain the entropy spectrum of a black hole. In Einstein's gravity, we show that both entropy and area spectrum are evenly spaced. But in more general theories (like Einstein-Gauss-Bonnet gravity), although the entropy spectrum is equispaced, the corresponding area spectrum is not.
The effects of multiply quantum wells (MQW) on optical and electrical characteristics of AlGaAs lasers with separate confinement heterostructures (SCH)<|sep|>Optical and electrical characteristics of AlGaAs lasers with separate confinement heterostructures are modeled by using Synopsys's Sentaurus TCAD, and open source software. The results for cases of 2-QW (2 Quantum Wells) and 3-QW structures are compared with these for 1-QW. A significant improvement of useful laser parameters is obtained by increasing the number of Quantum Wells and optimizing the width of waveguides. In particular, the maximum optical efficiency is shown to reach 88% for a 3-QW structure with optimal width of waveguides. The width of optical intensity profile of MQW lasers increases, leading to lowering maximal light power density passing through laser facets, decreasing the risk of catastrophic damage of mirrors.
Mesoscopic simulation of diffusive contaminant spreading in gas flows at low pressure<|sep|>Many modern production and measurement facilities incorporate multiphase systems at low pressures. In this region of flows at small, non-zero Knudsen- and low Mach numbers the classical mesoscopic Monte Carlo methods become increasingly numerically costly. To increase the numerical efficiency of simulations hybrid models are promising. In this contribution, we propose a novel efficient simulation approach for the simulation of two phase flows with a large concentration imbalance in a low pressure environment in the low intermediate Knudsen regime. Our hybrid model comprises a lattice-Boltzmann method corrected for the lower intermediate Kn regime proposed by Zhang et al. for the simulation of an ambient flow field. A coupled event-driven Monte-Carlo-style Boltzmann solver is employed to describe particles of a second species of low concentration. In order to evaluate the model, standard diffusivity and diffusion advection systems are considered.
Unified Analysis of Periodization-Based Sampling Methods for Mat\'ern Covariances<|sep|>The periodization of a stationary Gaussian random field on a sufficiently large torus comprising the spatial domain of interest is the basis of various efficient computational methods, such as the classical circulant embedding technique using the fast Fourier transform for generating samples on uniform grids. For the family of Mat\'ern covariances with smoothness index $\nu$ and correlation length $\lambda$, we analyse the nonsmooth periodization (corresponding to classical circulant embedding) and an alternative procedure using a smooth truncation of the covariance function. We solve two open problems: the first concerning the $\nu$-dependent asymptotic decay of eigenvalues of the resulting circulant in the nonsmooth case, the second concerning the required size in terms of $\nu$, $\lambda$ of the torus when using a smooth periodization. In doing this we arrive at a complete characterisation of the performance of these two approaches. Both our theoretical estimates and the numerical tests provided here show substantial advantages of smooth truncation.
Zoomed cosmological simulations of Milky Way sized halos in f(R)-gravity<|sep|>We investigate the impact of f(R) modified gravity on the internal properties of Milky Way sized dark matter halos in a set of cosmological zoom simulations of seven halos from the Aquarius suite, carried out with our code MG-GADGET in the Hu & Sawicki f(R) model. Also, we calculate the fifth forces in ideal NFW-halos as well as in our cosmological simulations and compare them against analytic model predictions for the fifth force inside spherical objects. We find that these theoretical predictions match the forces in the ideal halos very well, whereas their applicability is somewhat limited for realistic cosmological halos. Our simulations show that f(R) gravity significantly affects the dark matter density profile of Milky Way sized objects as well as their circular velocities. In unscreened regions, the velocity dispersions are increased by up to 40% with respect to LCDM for viable f(R) models. This difference is larger than reported in previous works. The Solar circle is fully screened in $f_{R0} = -10^{-6}$ models for Milky Way sized halos, while this location is unscreened for slightly less massive objects. Within the scope of our limited halo sample size, we do not find a clear dependence of the concentration parameter of dark matter halos on $f_{R0}$.
Thermodynamics, density profiles and correlation functions of the inhomogeneous one-dimensional spinor Bose gas<|sep|>We investigate the finite temperature properties of the one-dimensional two-component Bose gas (2CBG) with repulsive contact interaction in a harmonic trap. Making use of a new lattice embedding for the 2CBG and the quantum transfer matrix we derive a system of two nonlinear integral equations characterizing the thermodynamics of the uniform system for all values of the relevant parameters: temperature, strength of the interaction, chemical potential and magnetic field. This system allows for an easy numerical implementation in stark contrast with the infinite number of equations obtained by employing the thermodynamic Bethe ansatz. We use this exact solution coupled with the local density approximation to compute the density profiles and local density correlation function of the inhomogeneous gas for a wide range of coupling strengths and temperatures. Our results show that the polarization in the center of the trap influences heavily the local correlator especially in the experimentally accessible Tonks-Girardeau regime.
A new method to determine the grain size of planetary regolith<|sep|>Airless planetary bodies are covered by a dusty layer called regolith. The grain size of the regolith determines the temperature and the mechanical strength of the surface layers. Thus, knowledge of the grain size of planetary regolith helps to prepare future landing and/or sample-return missions. In this work, we present a method to determine the grain size of planetary regolith by using remote measurements of the thermal inertia. We found that small bodies in the Solar System (diameter less than ~100 km) are covered by relatively coarse regolith grains with typical particle sizes in the millimeter to centimeter regime, whereas large objects possess very fine regolith with grain sizes between 10 and 100 micrometer.
Cross-domain Generalization for AMR Parsing<|sep|>Abstract Meaning Representation (AMR) parsing aims to predict an AMR graph from textual input. Recently, there has been notable growth in AMR parsing performance. However, most existing work focuses on improving the performance in the specific domain, ignoring the potential domain dependence of AMR parsing systems. To address this, we extensively evaluate five representative AMR parsers on five domains and analyze challenges to cross-domain AMR parsing. We observe that challenges to cross-domain AMR parsing mainly arise from the distribution shift of words and AMR concepts. Based on our observation, we investigate two approaches to reduce the domain distribution divergence of text and AMR features, respectively. Experimental results on two out-of-domain test sets show the superiority of our method.
User-specific Adaptive Fine-tuning for Cross-domain Recommendations<|sep|>Making accurate recommendations for cold-start users has been a longstanding and critical challenge for recommender systems (RS). Cross-domain recommendations (CDR) offer a solution to tackle such a cold-start problem when there is no sufficient data for the users who have rarely used the system. An effective approach in CDR is to leverage the knowledge (e.g., user representations) learned from a related but different domain and transfer it to the target domain. Fine-tuning works as an effective transfer learning technique for this objective, which adapts the parameters of a pre-trained model from the source domain to the target domain. However, current methods are mainly based on the global fine-tuning strategy: the decision of which layers of the pre-trained model to freeze or fine-tune is taken for all users in the target domain. In this paper, we argue that users in RS are personalized and should have their own fine-tuning policies for better preference transfer learning. As such, we propose a novel User-specific Adaptive Fine-tuning method (UAF), selecting which layers of the pre-trained network to fine-tune, on a per-user basis. Specifically, we devise a policy network with three alternative strategies to automatically decide which layers to be fine-tuned and which layers to have their parameters frozen for each user. Extensive experiments show that the proposed UAF exhibits significantly better and more robust performance for user cold-start recommendation.
The MuPix Telescope: A Thin, high Rate Tracking Telescope<|sep|>The MuPix Telescope is a particle tracking telescope, optimized for tracking low momentum particles and high rates. It is based on the novel High-Voltage Monolithic Active Pixel Sensors (HV-MAPS), designed for the Mu3e tracking detector. The telescope represents a first application of the HV-MAPS technology and also serves as test bed of the Mu3e readout chain. The telescope consists of up to eight layers of the newest prototypes, the MuPix7 sensors, which send data self-triggered via fast serial links to FPGAs, where the data is time-ordered and sent to the PC. A particle hit rate of 1 MHz per layer could be processed. Online tracking is performed with a subset of the incoming data. The general concept of the telescope, chip architecture, readout concept and online reconstruction are described. The performance of the sensor and of the telescope during test beam measurements are presented.
ESR evidence for partial melting of the orbital order in LaMnO$_3$ below the Jahn-Teller transition<|sep|>We report on high-temperature electron spin resonance studies of a detwinned LaMnO$_3$ single crystal across the Jahn-Teller transition at $T_{\rm JT}$ = 750 K. The anisotropy of the linewidth and g-factor reflects the local Jahn-Teller distortions in the orbitally ordered phase. A clear jump in the linewidth accompanies the Jahn-Teller transition at $T_{\rm JT}$ = 750 K confirming that the transition is of first order. Already at $T^*$ = 550 K a significant decrease of the reduced linewidth is observed. This temperature scale is discussed with respect to the interaction of the $e_g$-electrons of the Mn$^{3+}$-ions and the elastic field of the cooperative distortions. Our results support a partial melting of the orbital order along the antiferromagnetically coupled $b$-axis at $T^*$. The remaining two-dimensional orbital ordering within the ferromagnetically coupled $ac$-plane finally disappears together with the cooperative distortion at $T_{\rm JT}$. Moreover in our discussion we show that elastic strain field interactions can explain the melting of the orbital order and, thus, has to be taken into account to explain the orbital ordering in LaMnO$_3$.
The Potluck Problem<|sep|>This paper proposes the Potluck Problem as a model for the behavior of independent producers and consumers under standard economic assumptions, as a problem of resource allocation in a multi-agent system in which there is no explicit communication among the agents.
Lithospheric delamination beneath the southern Puna plateau resolved by local earthquake tomography<|sep|>We present a local earthquake tomography to illuminate the crustal and uppermost mantle structure beneath the southern Puna plateau and to test the delamination hypothesis. Vp and Vp/Vs ratios were obtained using travel time variations recorded by 75 temporary seismic stations between 2007 and 2009. In the upper crust, prominent low Vp anomalies are found beneath the main volcanic centers, indicating the presence of magma and melt beneath the southern Puna plateau. In the lowlands to the southeast of the Puna plateau, below the Sierras Pampeanas, a high Vp body is observed in the crust. Beneath the Moho at around 90 km depth, a strong high Vp anomaly is detected just west of the giant backarc Cerro Galan Ignimbrite caldera with the robustness of this feature being confirmed by multiple synthetic tests. This high velocity body can be interpreted as a delaminated block of lower crust and uppermost mantle lithosphere under the southern Puna plateau. The low velocities in the crust can be interpreted as having been induced by the delamination event that triggered the rise of fluids and melts into the crust and induced the high topography in this part of the plateau. The tomography also reveals low velocity anomalies that link arc magnetism at the Ojos del Salado volcanic center with slab seismicity clusters at depths of about 100 and 150 km and support fluid fluxing in the mantle wedge due to dehydration reaction within the subducted slab.
Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation<|sep|>This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA). Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture. Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP. Extensive experiments on different data sets demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces more than 80% communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm.
Model Independent Predictions for Rare Top Decays with Weak Coupling<|sep|>Measurements at B factories have provided important constraints on new physics in several rare processes involving the B meson. New Physics, if present in the b quark sector may also affect the top sector. In an effective Lagrangian approach, we write down operators where effects in the bottom and the top sector are related. Assuming the couplings of the operators to be of the same size as the weak coupling g of the Standard Model and taking into account constraints on new physics from the bottom sector as well as top branching ratios, we make predictions for the rare top decays t -> cV where V = \gamma, Z. We find branching fractions for these decays within possible reach of the LHC. Predictions are also made for t -> sW.
Radiative hydrodynamics simulations of red supergiant stars. III. Spectro-photocentric variability, photometric variability, and consequences on Gaia measurements<|sep|>Context. It has been shown that convection in red supergiant stars gives rise to large granules causing surface inhomogeneities together with shock waves in the photosphere. The resulting motion of the photocenter (on time scales ranging from months to years) could possibly have adverse effects on the parallax determination with Gaia. Aims. We explore the impact of the granulation on the photocentric and photometric variability. We quantify these effects in order to better characterize the error possibly altering the parallax. Methods. We use 3D radiative-hydrodynamics simulations of convection with CO5BOLD and the post-processing radiative transfer code OPTIM3D to compute intensity maps and spectra in the Gaia G band [325-1030 nm]. Results. We provide astrometric and photometric predictions from 3D simulations of RSGs that are used to evaluate the degradation of the astrometric parameters of evolved stars derived by Gaia. We show from RHD simulations that a supergiant like Betelgeuse exhibits a photocentric noise characterised by a standard deviation of the order of 0.1 AU. The number of bright giant and supergiant stars whose Gaia parallaxes will be altered by the photocentric noise ranges from a few tens to several thousandths. The degradation of the astrometric fit due to the presence of this photocentric noise will be noticeable up to about 5 kpc for the brightest supergiants. Moreover, parallaxes of supergiants are affected by a error of the order of a few percents. We show that the photocentric noise, as predicted by the 3D simulation, does account for a substantial part of the supplementary 'cosmic noise' that affects Hipparcos measurements of Betelgeuse and Antares.
Galaxy Zoo: Bar Lengths in Nearby Disk Galaxies<|sep|>We present an analysis of bar length measurements of 3150 local galaxies in a volume limited sample of low redshift (z < 0.06) disk galaxies. Barred galaxies were initially selected from the Galaxy Zoo 2 project, and the lengths and widths of the bars were manually drawn by members of the Galaxy Zoo community using a Google Maps interface. Bars were measured independently by different observers, multiple times per galaxy (>=3), and we find that observers were able to reproduce their own bar lengths to 3% and each others' to better than 20%. We find a "color bimodality" in our disk galaxy population with bar length, i.e., longer bars inhabit redder disk galaxies and the bars themselves are redder, and that the bluest galaxies host the smallest galactic bars (< 5 kpc/h). We also find that bar and disk colors are clearly correlated, and for galaxies with small bars, the disk is, on average, redder than the bar colors, while for longer bars the bar then itself is redder, on average, than the disk. We further find that galaxies with a prominent bulge are more likely to host longer bars than those without bulges. We categorise our galaxy populations by how the bar and/or ring are connected to the spiral arms. We find that galaxies whose bars are directly connected to the spiral arms are preferentially bluer and that these galaxies host typically shorter bars. Within the scatter, we find that stronger bars are found in galaxies which host a ring (and only a ring). The bar length and width measurements used herein are made publicly available for others to use (http://data.galaxyzoo.org).
Polynomial algorithm for exact calculation of partition function for binary spin model on planar graphs<|sep|>In this paper we propose and realize (the code is publicly available at https://github.com/Thrawn1985/2D-Partition-Function) an algorithm for exact calculation of partition function for planar graph models with binary spins. The complexity of the algorithm is O(N^2). Test experiments shows good agreement with Onsager's analytical solution for two-dimensional Ising model of infinite size.
Estimation and control of oscillators through short-range noisy proximity measurements<|sep|>In this paper, we present a novel estimation and control strategy to balance a formation of discrete-time oscillators on a circle. We consider the case in which each oscillator only gathers noisy proximity measurements, whose range is lower than the desired spacing along the circle, implying total disconnectedness of the balanced formation. These restrictions pose relevant challenges that are overcome through the symbiotic combination of an estimator that borrows tools from interval analysis and a three-level bang-bang controller. We prove that the formation can be balanced, with an accuracy that can be regulated by tuning a controller parameter. The effectiveness of the proposed strategy is further illustrated through a set of numerical simulations.
Anti-BRST in the Causal Approach<|sep|>It is known that the elimination of the anomalies in all orders of perturbation theory is an open problem. The constrains given by usual invariance properties and the Wess-Zumino identities are not enough to eliminate the anomalies in the general case of an Yang-Mills theory. So, any new symmetry of the model could restrict further the anomalies and be a solution of the problem. We consider the anti-BRST transform of Ojima in the causal approach and investigate if such new restrictions are obtained. Unfortunately, the result is negative: if we have BRST invariance up to the second order of the perturbation theory, we also have anti-BRST invariance up to the same order. Probably, this result is true in all orders of the perturbation theory. So, anti-BRST transform gives nothing new, and we have to find other ideas to restrict and eventually eliminate the anomalies for a general Yang-Mills theory.
Interior Matter Estimates of Rapidly Rotating Compact Stars<|sep|>The authors try to probe the inner components of rapidly rotating compact stars such as the millisecond pulsar SAX J1808.4-3658 and the possible sub-millisecond pulsar XTE J1739-285 in their own way by comparing the genuine rotation frequencies under different theoretical models with the observational data, which may exert more stringent constraint on matter composition of compact stars. According to their treatment, the SAX J1808.4-3658 is a star with exotic matter and XTE J1739-285 a hybrid star.
Jet charge modification in dense QCD matter<|sep|>Jet production and jet substructure modification in heavy-ion collisions have played an essential role in revealing the in-medium evolution of parton showers and the determination of the properties of strongly-interacting matter under extreme conditions. It is imperative to extend these studies to include flavor tagging and to devise observables that are sensitive to the partonic origin of jets. The average jet charge, defined as the momentum-weighted sum of the electric charges of particles inside the jet, is a proxy of the electric charge of the quark or gluon that initiates the jet. We demonstrate how the factorization framework of soft-collinear effective theory can be generalized to evaluate the jet charge in a dense strongly-interacting matter environment, such as the one produced in nuclear reactions at collider energies. Observables that can separate the contribution of in-medium branching from the trivial isospin effects are identified and their connection to established jet quenching effects is elucidated. We present predictions for the transverse momentum dependence of the jet charge distribution in nucleus-nucleus collisions and its modification relative to the proton case.
Spectroscopic investigations of a Ti:Tm:LiNbO3 waveguide for photon-echo quantum memory<|sep|>We report the fabrication and characterization of a Ti$^{4+}$:Tm$^{3+}$:LiNbO$_3$ optical waveguide in view of photon-echo quantum memory applications. In particular, we investigated room- and cryogenic-temperature properties via absorption, spectral hole burning, photon echo, and Stark spectroscopy. We found radiative lifetimes of 82 $\mu$s and 2.4 ms for the $^3$H$_4$ and $^3$F$_4$ levels, respectively, and a 44% branching ratio from the $^3$H$_{4}$ to the $^3$F$_4$ level. We also measured an optical coherence time of 1.6 $\mu$s for the $^3$H$_6\leftrightarrow{}^3$H$_4$, 795 nm wavelength transition, and investigated the limitation of spectral diffusion to spectral hole burning. Upon application of magnetic fields of a few hundred Gauss, we observed persistent spectral holes with lifetimes up to seconds. Furthermore, we measured a linear Stark shift of 25 kHz$\cdot$cm/V. Our results are promising for integrated, electro-optical, waveguide quantum memory for photons.
Vision Transformer with Convolutional Encoder-Decoder for Hand Gesture Recognition using 24 GHz Doppler Radar<|sep|>Transformers combined with convolutional encoders have been recently used for hand gesture recognition (HGR) using micro-Doppler signatures. We propose a vision-transformer-based architecture for HGR with multi-antenna continuous-wave Doppler radar receivers. The proposed architecture consists of three modules: a convolutional encoderdecoder, an attention module with three transformer layers, and a multi-layer perceptron. The novel convolutional decoder helps to feed patches with larger sizes to the attention module for improved feature extraction. Experimental results obtained with a dataset corresponding to a two-antenna continuous-wave Doppler radar receiver operating at 24 GHz (published by Skaria et al.) confirm that the proposed architecture achieves an accuracy of 98.3% which substantially surpasses the state-of-the-art on the used dataset.
Chimera states in ring-star network of Chua circuits<|sep|>We investigate the emergence of amplitude and frequency chimera states in ring-star networks consisting of identical Chua circuits connected via nonlocal diffusive, bidirectional coupling. We first identify single-well chimera patterns in a ring network under nonlocal coupling schemes. When a central node is added to the network, forming a ring-star network, the central node acts as the distributor of information, increasing the chances of synchronization. Numerical simulations show that the radial coupling strength $k$ between the central and the peripheral nodes acts as an order parameter leading from a lower to a higher frequency domain. The transition between the domains takes place for intermediate coupling values, $0.5 <k< 2$, where the frequency chimera states prevail. The transition region (width and boundaries) depends on the Chua oscillator parameters and the network specifics. Potential applications of star connectivity can be found in the control of Chua networks and in other coupled chaotic dynamical systems. By adding one central node and without further modifications to the individual network parameters it is possible to entrain the system to lower or higher frequency domains as desired by the particular applications.
Top-down Visual Saliency Guided by Captions<|sep|>Neural image/video captioning models can generate accurate descriptions, but their internal process of mapping regions to words is a black box and therefore difficult to explain. Top-down neural saliency methods can find important regions given a high-level semantic task such as object classification, but cannot use a natural language sentence as the top-down input for the task. In this paper, we propose Caption-Guided Visual Saliency to expose the region-to-word mapping in modern encoder-decoder networks and demonstrate that it is learned implicitly from caption training data, without any pixel-level annotations. Our approach can produce spatial or spatiotemporal heatmaps for both predicted captions, and for arbitrary query sentences. It recovers saliency without the overhead of introducing explicit attention layers, and can be used to analyze a variety of existing model architectures and improve their design. Evaluation on large-scale video and image datasets demonstrates that our approach achieves comparable captioning performance with existing methods while providing more accurate saliency heatmaps. Our code is available at visionlearninggroup.github.io/caption-guided-saliency/.
Vector meson production at HERA<|sep|>The rich experimental landscape of exclusive vector meson production at the high energy electron-proton collider HERA is reviewed, with emphasis on the transition from soft to hard diffraction and QCD interpretations.
Effect of compositional fluctuation on the survival of bet-hedging species<|sep|>Understanding the coexistence of diverse species in a changing environment is an important problem in community ecology. Bet-hedging is a strategy that helps species survive in such changing environments. However, studies of bet-hedging have often focused on the expected long-term growth rate of the species by itself, neglecting competition with other coexisting species. Here we study the extinction risk of a bet-hedging species in competition with others. We show that there are three contributions to the extinction risk. The first is the usual demographic fluctuation due to stochastic reproduction and selection processes in finite populations. The second, due to the fluctuation of population growth rate caused by environmental changes, may counterintuitively reduce the extinction risk for small populations. Besides those two, we reveal a third contribution, which is unique to bet-hedging species that diversify into multiple phenotypes: The phenotype composition of the population will fluctuate over time, resulting in increased extinction risk. We compare such compositional fluctuation to the demographic and environmental contributions, showing how they have different effects on the extinction risk depending on the population size, generation overlap, and environmental correlation.
A Re-interpretation of the STEREO/STE Observations and it's Consequences<|sep|>We present an alternate interpretation of recent STEREO/STE observations that were originally attributed to energetic neutral atoms (ENA) from the heliosheath. The signal attributed to the diffuse ENA source instead shows the characteristics of a point source. We point out that the peak intensity seen by STEREO/STE is centered at the ecliptic longitude of the bright X-ray source Sco X-1. The observed energy spectrum and intensity are also consistent with the X-rays from Sco X-1. The problem of energy dissipation at the solar wind termination shock remains unsolved while current understanding of the interaction between the solar wind and interstellar wind awaits future observations.
Kinetic and mean field description of Gibrat's law<|sep|>We introduce and analyze a linear kinetic model that describes the evolution of the probability density of the number of firms in a society, in which the microscopic rate of change obeys to the so-called law of proportional effect proposed by Gibrat. Despite its apparent simplicity, the possible mean field limits of the kinetic model are varied. In some cases, the asymptotic limit can be described by a first-order partial differential equation. In other cases, the mean field equation is a linear diffusion with a non constant diffusion coefficient that models also the geometric Brownian motion and can be studied analytically. In this case, it is shown that the large-time behavior of the solution is represented, for a large class of initial data, by a lognormal distribution with constant mean value and variance increasing exponentially in time at a precise rate. The relationship between the kinetic and the diffusion models allow to introduce an easy-to- implement expression for computing the Fourier transform of the lognormal distribution.
The Data Processor of the EUSO-SPB2 Telescopes<|sep|>In this paper we present the Data Processor (DP) of EUSO-SPB2 (Extreme Universe Space Observatory on a Super Pressure Balloon, mission two) telescopes . The EUSO-SPB2 is the continuation of the JEM-EUSO science program on ultra-long duration balloons, started with the EUSO-SPB1 mission. The EUSO-SPB2 will host on-board two telescopes. One is a fluorescence telescope designed to detect high energy cosmic rays via the UV fluorescence emission of the showers in the atmosphere; the other one measures direct Cherenkov light emission from lower energy cosmic rays and other optical backgrounds for cosmogenic tau neutrino detection. The DP is the component of the electronics system which performs data management and instrument control for each of the two telescopes. The DP controls front-end electronics, tags events with arrival time and payload position through a GPS system, provides signals for time synchronization of the event and measures live and dead time of the telescope. Furthermore it manages mass memory for data storage and performs housekeeping monitor and controls the power on and power off sequences. Since a super pressure balloon may remain airborne up to 100 days, the requirements on the electronics and data handling are quite severe. The DP operates at high altitude in unpressurised environment which represents a technological challenge for heat dissipation. In this paper we describe the main components of the system and the design developed for the new mission.
A generalized precession parameter $\chi_\mathrm{p}$ to interpret gravitational-wave data<|sep|>Originally designed for waveform approximants, the effective precession parameter $\chi_\mathrm{p}$ is the most commonly used quantity to characterize spin-precession effects in gravitational-wave observations of black-hole binary coalescences. We point out that the current definition of $\chi_\mathrm{p}$ retains some, but not all, variations taking place on the precession timescale. We rectify this inconsistency and propose more general definitions that either fully consider or fully average those oscillations. Our generalized parameter $\chi_\mathrm{p}\in[0,2]$ presents an exclusive region $\chi_\mathrm{p}>1$ that can only be populated by binaries with two precessing spins. We apply our prescriptions to current LIGO/Virgo events and find that posterior distributions of $\chi_\mathrm{p}$ tend to show longer tails at larger values. This appears to be a generic feature, implying that (i) current $\chi_\mathrm{p}$ measurement errors might be underestimated, but also that (ii) evidence for spin precession in current data might be stronger than previously inferred. Among the gravitational-wave events released to date, that which shows the most striking behavior is GW190521.
An Extensible Timing Infrastructure for Adaptive Large-scale Applications<|sep|>Real-time access to accurate and reliable timing information is necessary to profile scientific applications, and crucial as simulations become increasingly complex, adaptive, and large-scale. The Cactus Framework provides flexible and extensible capabilities for timing information through a well designed infrastructure and timing API. Applications built with Cactus automatically gain access to built-in timers, such as gettimeofday and getrusage, system-specific hardware clocks, and high-level interfaces such as PAPI. We describe the Cactus timer interface, its motivation, and its implementation. We then demonstrate how this timing information can be used by an example scientific application to profile itself, and to dynamically adapt itself to a changing environment at run time.
Heat transfer and flow regimes in quasi-static magnetoconvection with a vertical magnetic field<|sep|>Numerical simulations of quasi-static magnetoconvection with a vertical magnetic field are carried out up to a Chandrasekhar number of $Q=10^8$ over a broad range of Rayleigh numbers $Ra$. Three magnetoconvection regimes are identified: two of the regimes are magnetically-constrained in the sense that a leading-order balance exists between the Lorentz and buoyancy forces, whereas the third regime is characterized by unbalanced dynamics that is similar to non-magnetic convection. Each regime is distinguished by flow morphology, momentum and heat equation balances, and heat transport behavior. One of the magnetically-constrained regimes appears to represent an `ultimate' magnetoconvection regime in the dual limit of asymptotically-large buoyancy forcing and magnetic field strength; this regime is characterized by an interconnected network of anisotropic, spatially-localized fluid columns aligned with the direction of the imposed magnetic field that remain quasi-laminar despite having large flow speeds. As for non-magnetic convection, heat transport is controlled primarily by the thermal boundary layer. Empirically, the scaling of the heat transport and flow speeds with $Ra$ appear to be independent of the thermal Prandtl number within the magnetically-constrained, high-$Q$ regimes.
Individual Preference Aware Caching Policy Design in Wireless D2D Networks<|sep|>Cache-aided wireless device-to-device (D2D) networks allow significant throughput increase, depending on the concentration of the popularity distribution of files. Many studies assume that all users have the same preference distribution; however, this may not be true in practice. This work investigates whether and how the information about individual preferences can benefit cache-aided D2D networks. We examine a clustered network and derive a network utility that considers both the user distribution and channel fading effects into the analysis. We also formulate a utility maximization problem for designing caching policies. This maximization problem can be applied to optimize several important quantities, including throughput, energy efficiency (EE), cost, and hit-rate, and to solve different tradeoff problems. We provide a general approach that can solve the proposed problem under the assumption that users coordinate, then prove that the proposed approach can obtain the stationary point under a mild assumption. Using simulations of practical setups, we show that performance can improve significantly with proper exploitation of individual preferences. We also show that different types of tradeoffs exist between different performance metrics and that they can be managed through caching policy and cooperation distance designs.
Do cement nanoparticles exist in space ?<|sep|>The calcium-silicate-hydrate is used to model properties of cement on Earth. We study cementitious nanoparticles and propose these structures as components of cosmic dust grains. Quantum density functional theory methods are applied for the calculation of infrared spectra of Ca4Si4O14H4, Ca6Si3O13H2, and Ca12Si6O26H4 clusters. We find bands distributed over the near, mid and far-infrared region. A specific calcium-silicate-hydrate spectral feature at 14 microns, together with the bands at 10 and 18 microns which exist for other silicates as well, could be used for a detection of cosmic cement. We compare calculated bands with the 14 microns features in the spectra of HD 45677, HD 44179, and IRC+10420 which were observed by Infrared Space Observatory and classified as remaining. High abundance of oxygen atoms in cementitious nanoparticles could partially explain observed depletion of this element from the interstellar medium into dust grains.
F-SIOL-310: A Robotic Dataset and Benchmark for Few-Shot Incremental Object Learning<|sep|>Deep learning has achieved remarkable success in object recognition tasks through the availability of large scale datasets like ImageNet. However, deep learning systems suffer from catastrophic forgetting when learning incrementally without replaying old data. For real-world applications, robots also need to incrementally learn new objects. Further, since robots have limited human assistance available, they must learn from only a few examples. However, very few object recognition datasets and benchmarks exist to test incremental learning capability for robotic vision. Further, there is no dataset or benchmark specifically designed for incremental object learning from a few examples. To fill this gap, we present a new dataset termed F-SIOL-310 (Few-Shot Incremental Object Learning) which is specifically captured for testing few-shot incremental object learning capability for robotic vision. We also provide benchmarks and evaluations of 8 incremental learning algorithms on F-SIOL-310 for future comparisons. Our results demonstrate that the few-shot incremental object learning problem for robotic vision is far from being solved.
Threshold resummation for pair production of coloured heavy (s)particles at hadron colliders<|sep|>We derive a factorization formula for the production of pairs of heavy coloured particles in hadronic collisions near the production threshold that establishes factorization of soft and Coulomb effects. This forms the basis for a combined resummation of Coulomb and soft corrections, including the non-trivial interference of the two effects. We develop a resummation formalism valid at NNLL accuracy using the momentum-space approach to soft gluon resummation. We present numerical results for the NLL resummed squark-antisquark production cross section at the LHC and Tevatron, including also the contribution of squark-antisquark bound states below threshold. The total correction on top of the next-to-leading order approximation is found to be sizeable, and amounts to (4-20)% in the squark mass region 200 GeV-3 TeV at the 14 TeV LHC. The scale dependence of the total cross section is also reduced.
Towards an Efficient ML System: Unveiling a Trade-off between Task Accuracy and Engineering Efficiency in a Large-scale Car Sharing Platform<|sep|>Upon the significant performance of the supervised deep neural networks, conventional procedures of developing ML system are \textit{task-centric}, which aims to maximize the task accuracy. However, we scrutinized this \textit{task-centric} ML system lacks in engineering efficiency when the ML practitioners solve multiple tasks in their domain. To resolve this problem, we propose an \textit{efficiency-centric} ML system that concatenates numerous datasets, classifiers, out-of-distribution detectors, and prediction tables existing in the practitioners' domain into a single ML pipeline. Under various image recognition tasks in the real world car-sharing platform, our study illustrates how we established the proposed system and lessons learned from this journey as follows. First, the proposed ML system accomplishes supreme engineering efficiency while achieving a competitive task accuracy. Moreover, compared to the \textit{task-centric} paradigm, we discovered that the \textit{efficiency-centric} ML system yields satisfactory prediction results on multi-labelable samples, which frequently exist in the real world. We analyze these benefits derived from the representation power, which learned broader label spaces from the concatenated dataset. Last but not least, our study elaborated how we deployed this \textit{efficiency-centric} ML system is deployed in the real world live cloud environment. Based on the proposed analogies, we highly expect that ML practitioners can utilize our study to elevate engineering efficiency in their domain.
Open Source CRM Systems for SMEs<|sep|>Customer Relationship Management (CRM) systems are very common in large companies. However, CRM systems are not very common in Small and Medium Enterprises (SMEs). Most SMEs do not implement CRM systems due to several reasons, such as lack of knowledge about CRM or lack of financial resources to implement CRM systems. SMEs have to start implementing Information Systems (IS) technology into their business operations in order to improve business values and gain more competitive advantage over rivals. CRM system has the potential to help improve the business value and competitive capabilities of SMEs. Given the high fixed costs of normal activity of companies, we intend to promote free and viable solutions for small and medium businesses. In this paper, we explain the reasons why SMEs do not implement CRM system and the benefits of using open source CRM system in SMEs. We also describe the functionalities of top open source CRM systems, examining the applicability of these tools in fitting the needs of SMEs.
Sensitivity and Generalization in Neural Networks: an Empirical Study<|sep|>In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets. We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization $-$ such as full-batch training or using random labels $-$ correspond to lower robustness, while factors associated with good generalization $-$ such as data augmentation and ReLU non-linearities $-$ give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.
Towards Flexible Sparsity-Aware Modeling: Automatic Tensor Rank Learning Using The Generalized Hyperbolic Prior<|sep|>Tensor rank learning for canonical polyadic decomposition (CPD) has long been deemed as an essential yet challenging problem. In particular, since the tensor rank controls the complexity of the CPD model, its inaccurate learning would cause overfitting to noise or underfitting to the signal sources, and even destroy the interpretability of model parameters. However, the optimal determination of a tensor rank is known to be a non-deterministic polynomial-time hard (NP-hard) task. Rather than exhaustively searching for the best tensor rank via trial-and-error experiments, Bayesian inference under the Gaussian-gamma prior was introduced in the context of probabilistic CPD modeling, and it was shown to be an effective strategy for automatic tensor rank determination. This triggered flourishing research on other structured tensor CPDs with automatic tensor rank learning. On the other side of the coin, these research works also reveal that the Gaussian-gamma model does not perform well for high-rank tensors and/or low signal-to-noise ratios (SNRs). To overcome these drawbacks, in this paper, we introduce a more advanced generalized hyperbolic (GH) prior to the probabilistic CPD model, which not only includes the Gaussian-gamma model as a special case, but also is more flexible to adapt to different levels of sparsity. Based on this novel probabilistic model, an algorithm is developed under the framework of variational inference, where each update is obtained in a closed-form. Extensive numerical results, using synthetic data and real-world datasets, demonstrate the significantly improved performance of the proposed method in learning both low as well as high tensor ranks even for low SNR cases.
The WFI Halpha spectroscopic survey of the Magellanic Clouds: Be stars in SMC open clusters<|sep|>At low metallicity, B-type stars show lower loss of mass and, therefore, angular momentum so that it is expected that there are more Be stars in the Magellanic Clouds than in the Milky Way. However, till now, searches for Be stars were only performed in a very small number of open clusters in the Magellanic Clouds. Using the ESO/WFI in its slitless spectroscopic mode, we performed a Halpha survey of the Large and Small Magellanic Cloud. Eight million low-resolution spectra centered on Halpha were obtained. For their automatic analysis, we developed the ALBUM code. Here, we present the observations, the method to exploit the data and first results for 84 open clusters in the SMC. In particular, cross-correlating our catalogs with OGLE positional and photometric data, we classified more than 4000 stars and were able to find the B and Be stars in them. We show the evolution of the rates of Be stars as functions of area density, metallicity, spectral type, and age.
On the AdS/QCD estimates of the scalar glueball mass<|sep|>We review standard holographic derivations of the scalar glueball spectra and emphasize computational assumptions which rely strongly on the original AdS/CFT correspondence, and those which are more arbitrary. In the soft-wall model, we show explicitly that the dual of the spectral mass constraint in the Kallen-Lehmann representation of the 2-point scalar gluonic correlator is an eigenvalue constraint on the motion equation of the bulk field corresponding this correlator.
Time delay estimator for predetermined repeated signal robust to narrowband interference<|sep|>In this paper, time delay estimation techniques robust to narrowband interference (NBI) are proposed. Owing to the deluge of wireless signal interference these days, narrowband interference is a common problem for communication and positioning systems. To mitigate the effect of this narrow band interference, we propose a robust time delay estimator for a predetermined repeated synchronization signal in an NBI environment. We exploit an ensemble of average and sample covariance matrices to estimate the noise profile. In addition, to increase the detection probability, we suppress the variance of likelihood value by employing a von-Mises distribution in the time-delay estimator. Our proposed time delay estimator shows a better performance in an NBI environment compared to a typical time delay estimator.
Classically Scale-invariant B-L Model and Conformal Gravity<|sep|>We consider a coupling of conformal gravity to the classically scale-invariant B-L extended standard model which has been recently proposed as a phenomenologically viable model realizing the Coleman-Weinberg mechanism of breakdown of the electro-weak symmetry. As in a globally scale-invariant dilaton gravity, it is also shown in a locally scale-invariant conformal gravity that without recourse to the Coleman-Weinberg mechanism, the B-L gauge symmetry is broken in the process of spontaneous symmetry breakdown of the local scale invariance (Weyl invariance) at the tree level and as a result the B-L gauge field becomes massive via the Higgs mechanism. As a bonus of conformal gravity, the massless dilaton field does not appear and the parameters in front of the non-minimal coupling of gravity are completely fixed in the present model. This observation clearly shows that the conformal gravity has a practical application even if the scalar field does not possess any dynamical degree of freedom owing to the local scale symmetry.
Strong-coupling Superconductivity in the Cuprate Oxide<|sep|>Superconductivity in the cuprate oxide is studied by Kondo-lattice theory based on the t-J model with the el-ph interaction arising from the modulation of the superexchange interaction by phonons. The self-energy of electrons is decomposed into the single-site and multisite ones. It is proved by using the mapping of the single-site one in the t-J model to its corresponding one in the Anderson model that the single-site self-energy is that of a normal Fermi liquid, even if a superconducting (SC) order parameter appears or the multisite one is anomalous. The electron liquid characterized by the single-site self-energy is a normal Fermi liquid. The Fermi liquid is further stabilized by the RVB mechanism. The stabilized Fermi liquid is a relevant unperturbed state that can be used to study superconductivity and anomalous Fermi-liquid behaviors. The so-called spin-fluctuation-mediated exchange interaction, which includes the superexchange interaction as a part, is the attractive interaction that binds d-wave Cooper pairs. An analysis of the spin susceptibility implies that, because of the el-ph interaction, the imaginary part of the exchange interaction has a sharp peak or dip at \pm\omega^*, where \omega^*\simeq \omega_ph in the normal state and \epsilon_G/2 \lessim \omega^* \lessim \epsilon_G /2+ \omega_ph in the SC state, where \omega_ph is the energy of relevant phonons and \epsilon_G is the SC gap. If the imaginary part has a sharp peak or dip at \pm\omega^*, the dispersion relation of quasi-particles has kink structures near \pm\omega^* above and below the chemical potential, the density of states has dip-and-hump structures near \pm \omega^* outside the coherence peaks in the SC state, and the anisotropy of the gap deviates from the simple d-wave anisotropy.
The Future Asymptotic Behaviour of a Non-Tilted Bianchi Type IV Viscous Model<|sep|>The future asymptotic behaviour of a non-titled Bianchi Type IV viscous fluid model is analyzed. In particular, we consider the case of a viscous fluid without heat conduction, and constant expansion-normalized bulk and shear viscosity coefficients. We show using dynamical systems theory that the only future attracting equilibrium points are the flat Friedmann-LeMaitre (FL) solution, the open FL solution and the isotropic Milne universe solution. We also show the bifurcations exist with respect to an increasing expansion-normalized bulk viscosity coefficient. It is finally shown through an extensive numerical analysis, that the dynamical system isotropizes at late times.
Nonlinear Weighted Directed Acyclic Graph and A Priori Estimates for Neural Networks<|sep|>In an attempt to better understand structural benefits and generalization power of deep neural networks, we firstly present a novel graph theoretical formulation of neural network models, including fully connected, residual network (ResNet) and densely connected networks (DenseNet). Secondly, we extend the error analysis of the population risk for two layer network \cite{ew2019prioriTwo} and ResNet \cite{e2019prioriRes} to DenseNet, and show further that for neural networks satisfying certain mild conditions, similar estimates can be obtained. These estimates are a priori in nature since they depend sorely on the information prior to the training process, in particular, the bounds for the estimation errors are independent of the input dimension.
An update on moduli stabilization with antibrane uplift<|sep|>It was recently shown that in warped compactifications based on a Klebanov-Strassler throat there is a light complex structure field, governing the size of the throat and the redshift at its tip. We show that after uplift of the cosmological constant by an anti-D3 brane at the tip of the throat, the contribution to supersymmetry breaking coming from the new light field is large. We work out the mass scales, in particular the condition for this field to be heavier than the K\"ahler modulus. We check that for the range of parameters relevant for the destabilization we find agreement with de Sitter swampland conjecture. Adding matter fields on distant branes, we discuss the effects on supersymmetry breaking in the observable sector. A hierarchically small scale of supersymmetry breaking translates generically into large values of localized D3 charges in the manifold.
Phase-noise protection in quantum-enhanced differential interferometry<|sep|>Differential interferometry (DI) with two coupled sensors is a most powerful approach for precision measurements in presence of strong phase noise. However DI has been studied and implemented only with classical resources. Here we generalize the theory of differential interferometry to the case of entangled probe states. We demonstrate that, for perfectly correlated interferometers and in the presence of arbitrary large phase noise, sub-shot noise sensitivities -- up to the Heisenberg limit -- are still possible with a special class of entangled states in the ideal lossless scenario. These states belong to a decoherence free subspace where entanglement is passively protected. Our work pave the way to the full exploitation of entanglement in precision measurements in presence of strong phase noise.
Purified SASE undulator configuration to enhance the performance of the soft x-ray beamline at the European XFEL<|sep|>The purified SASE (pSASE) undulator configuration recently proposed at SLAC promises an increase in the output spectral density of XFELs. In this article we study a straightforward implementation of this configuration for the soft x-ray beamline at the European XFEL. A few undulator cells, resonant at a subharmonic of the FEL radiation, are used in the middle of the exponential regime to amplify the radiation, while simultaneously reducing the FEL bandwidth. Based on start-to-end simulations, we show that with the proposed configuration the spectral density in the photon energy range between 1.3 keV and 3 keV can be enhanced of an order of magnitude compared to the baseline mode of operation. This option can be implemented into the tunable-gap SASE3 baseline undulator without additional hardware, and it is complementary to the self-seeding option with grating monochromator proposed for the same undulator line, which can cover the photon energy range between about 0.26 keV and 1 keV.
The Unheralded Value of the Multiway Rendezvous: Illustration with the Production Cell Benchmark<|sep|>The multiway rendezvous introduced in Theoretical CSP is a powerful paradigm to achieve synchronization and communication among a group of (possibly more than two) processes. We illustrate the advantages of this paradigm on the production cell benchmark, a model of a real metal processing plant, for which we propose a compositional software controller, which is written in LNT and LOTOS, and makes intensive use of the multiway rendezvous.
Data processing over single-port homodyne detection to realize super-resolution and super-sensitivity<|sep|>Performing homodyne detection at one port of squeezed-state light interferometer and then binarzing measurement data are important to achieve super-resolving and super-sensitive phase measurements. Here we propose a new data-processing technique by dividing the measurement quadrature into three bins (equivalent to a multi-outcome measurement), which leads to a higher improvement in the phase resolution and the phase sensitivity under realistic experimental condition. Furthermore, we develop a new phase-estimation protocol based on a combination of the inversion estimators of each outcome and show that the estimator can saturate the Cramer-Rao lower bound, similar to asymptotically unbiased maximum likelihood estimator.
Ultrasensitive and fast single wavelength plasmonic hydrogen sensing with anisotropic nanostructured Pd films<|sep|>Anisotropic nanostructured porous Pd films are fabricated using oblique angle deposition in vacuum on a glass substrate. They display a dichroic response, due to localised surface plasmon resonances (LSPR) within the nanoparticles forming the film, dependent on the incident light polarisation. Ultrasensitive hydrogen sensing is reached by using these films in conjunction with a differential optical technique derived from the reflectance anisotropy spectroscopy. The evolution of the samples optical responses is monitored during the formation of Pd hydride in both the dilute alpha-phase and the dense beta-phase, whilst the samples are exposed to different concentration of H2 in Ar (from 100 percent H2 to a few ppm). The measurements are performed at a single wavelength in the visible range and at 22 deg C. The results show that a quantitative measurement of the hydrogen concentration in a carrier gas can be measured throughout the concentration range. The limit of detection is 10 ppm and the time for detecting the presence of H2 in the carrying gas is below one second at concentration down to 0.25 percent of H2 in Ar. Furthermore, the optical anisotropy of the samples and its evolution with exposure to H2 are correctly reproduced with an effective medium theory.
Life in the fast lane: a direct view of the dynamics, formation, and evolution of the Milky Way's bar<|sep|>Studies of the ages, abundances, and motions of individual stars in the Milky Way provide one of the best ways to study the evolution of disk galaxies over cosmic time. The formation of the Milky Way's barred inner region in particular is a crucial piece of the puzzle of disk galaxy evolution. Using data from APOGEE and Gaia, we present maps of the kinematics, elemental abundances, and age of the Milky Way bulge and disk that show the barred structure of the inner Milky Way in unprecedented detail. The kinematic maps allow a direct, purely kinematic determination of the bar's pattern speed of 41+/-3 km/s/kpc and of its shape and radial profile. We find the bar's age, metallicity, and abundance ratios to be the same as those of the oldest stars in the disk that are formed in its turbulent beginnings, while stars in the bulge outside of the bar are younger and more metal-rich. This implies that the bar likely formed ~8 Gyr ago, when the decrease in turbulence in the gas disk allowed a thin disk to form that quickly became bar-unstable. The bar's formation therefore stands as a crucial epoch in the evolution of the Milky Way, a picture that is in line with the evolutionary path that emerges from observations of the gas kinematics in external disk galaxies over the last ~10 Gyr.
Computing the confidence levels for a root-mean-square test of goodness-of-fit<|sep|>The classic chi-squared statistic for testing goodness-of-fit has long been a cornerstone of modern statistical practice. The statistic consists of a sum in which each summand involves division by the probability associated with the corresponding bin in the distribution being tested for goodness-of-fit. Typically this division should precipitate rebinning to uniformize the probabilities associated with the bins, in order to make the test reasonably powerful. With the now widespread availability of computers, there is no longer any need for this. The present paper provides efficient black-box algorithms for calculating the asymptotic confidence levels of a variant on the classic chi-squared test which omits the problematic division. In many circumstances, it is also feasible to compute the exact confidence levels via Monte Carlo simulation.
PRIMA: Planner-Reasoner Inside a Multi-task Reasoning Agent<|sep|>We consider the problem of multi-task reasoning (MTR), where an agent can solve multiple tasks via (first-order) logic reasoning. This capability is essential for human-like intelligence due to its strong generalizability and simplicity for handling multiple tasks. However, a major challenge in developing effective MTR is the intrinsic conflict between reasoning capability and efficiency. An MTR-capable agent must master a large set of "skills" to tackle diverse tasks, but executing a particular task at the inference stage requires only a small subset of immediately relevant skills. How can we maintain broad reasoning capability and also efficient specific-task performance? To address this problem, we propose a Planner-Reasoner framework capable of state-of-the-art MTR capability and high efficiency. The Reasoner models shareable (first-order) logic deduction rules, from which the Planner selects a subset to compose into efficient reasoning paths. The entire model is trained in an end-to-end manner using deep reinforcement learning, and experimental studies over a variety of domains validate its effectiveness.
Multi-stage Multi-recursive-input Fully Convolutional Networks for Neuronal Boundary Detection<|sep|>In the field of connectomics, neuroscientists seek to identify cortical connectivity comprehensively. Neuronal boundary detection from the Electron Microscopy (EM) images is often done to assist the automatic reconstruction of neuronal circuit. But the segmentation of EM images is a challenging problem, as it requires the detector to be able to detect both filament-like thin and blob-like thick membrane, while suppressing the ambiguous intracellular structure. In this paper, we propose multi-stage multi-recursive-input fully convolutional networks to address this problem. The multiple recursive inputs for one stage, i.e., the multiple side outputs with different receptive field sizes learned from the lower stage, provide multi-scale contextual boundary information for the consecutive learning. This design is biologically-plausible, as it likes a human visual system to compare different possible segmentation solutions to address the ambiguous boundary issue. Our multi-stage networks are trained end-to-end. It achieves promising results on two public available EM segmentation datasets, the mouse piriform cortex dataset and the ISBI 2012 EM dataset.
Sink strength calculations of dislocations and loops using OKMC<|sep|>We calculate the sink strength of dislocations and toroidal absorbers using Object Kinetic Monte Carlo and compare with the theoretical expressions. We get good agreement for dislocations and loop-shaped absorbers of 3D migrating defects, provided that the volume fraction is low, and fair agreements for dislocations with 1D migrating defects. The master curve for the 3D to 1D transition is well reproduced with loop-shaped absorbers and fairly well with dislocations. We conclude that, on the one hand, the master curve is correct for a wide range of sinks and that, on the other, OKMC techniques inherently take correctly into account the strengths of sinks of any shape, provided that an effective way of appropriately inserting the sinks to be studied can be found.
Signal Processing with Pulse Trains: An Algebraic Approach- Part II<|sep|>The integrate and fire converter (IFC) enables an alternative to digital signal processing. IFC converts analog signal voltages into time between pulses and it is possible to reconstruct the analog signal from the IFC pulses with an error as small as required. In this paper, we present the definition of multiplication in pulse trains created by the IFC based on time domain operations and prove that it constitutes an Abelian group in the space of IFC pulse trains. We also show that pulse domain multiplication corresponds to pointwise multiplication of analog signals. It is further proved that pulse domain multiplication is distributive over pulse domain addition and hence it forms a field in the space of IFC pulse trains, which is an important property for linear signal processing.
Data-Efficient Methods for Dialogue Systems<|sep|>Conversational User Interface (CUI) has become ubiquitous in everyday life, in consumer-focused products like Siri and Alexa or business-oriented solutions. Deep learning underlies many recent breakthroughs in dialogue systems but requires very large amounts of training data, often annotated by experts. Trained with smaller data, these methods end up severely lacking robustness (e.g. to disfluencies and out-of-domain input), and often just have too little generalisation power. In this thesis, we address the above issues by introducing a series of methods for training robust dialogue systems from minimal data. Firstly, we study two orthogonal approaches to dialogue: linguistically informed and machine learning-based - from the data efficiency perspective. We outline the steps to obtain data-efficient solutions with either approach. We then introduce two data-efficient models for dialogue response generation: the Dialogue Knowledge Transfer Network based on latent variable dialogue representations, and the hybrid Generative-Retrieval Transformer model (ranked first at the DSTC 8 Fast Domain Adaptation task). Next, we address the problem of robustness given minimal data. As such, propose a multitask LSTM-based model for domain-general disfluency detection. For the problem of out-of-domain input, we present Turn Dropout, a data augmentation technique for anomaly detection only using in-domain data, and introduce autoencoder-augmented models for efficient training with Turn Dropout. Finally, we focus on social dialogue and introduce a neural model for response ranking in social conversation used in Alana, the 3rd place winner in the Amazon Alexa Prize 2017 and 2018. We employ a novel technique of predicting the dialogue length as the main ranking objective and show that this approach improves upon the ratings-based counterpart in terms of data efficiency while matching it in performance.
Gravitational redshift profiles in the $f(R)$ and symmetron models<|sep|>Aims. We investigate the gravitational redshift in clusters of galaxies in the symmetron and Hu-Sawicky $f(R)$ models. The characteristic feature of both models is the screening mechanism that hides the fifth force in dense environments recovering general relativity. Methods. We use $N$-body simulations that were run using the code \texttt{Isis}, which includes scalar fields, to analyse the deviation of observables in modified gravity models with respect to $\Lambda$CDM. Results. We find that due to the presence of the screening, the deviation is highly dependent on the halo mass. For instance, the $f(R)$ parameters $|f_{R0}|=10^{-5}$, $n=1$ cause an enhancement of the gravitational signal by up to 50% for halos with masses between $10^{13}\,M_\odot h^{-1}$ and $10^{14}\,M_\odot h^{-1}$. The characteristic mass range where the fifth force is most active varies with the model parameters. The usual assumption is that the presence of a fifth force leads to a deeper potential well and thus, a stronger gravitational redshift. However, we find that in cases in which only the central regions of the halos are screened, there could also be a weaker gravitational redshift.
Staircase of crystal phases of hard-core bosons on the Kagome lattice<|sep|>We study the quantum phase diagram of a system of hard-core bosons on the Kagome lattice with nearest-neighbor repulsive interactions, for arbitrary densities, by means of the hierarchical mean field theory and exact diagonalization techniques. This system is isomorphic to the spin S=1/2 XXZ model in presence of an external magnetic field, a paradigmatic example of frustrated quantum magnetism. In the non-frustrated regime, we find two crystal phases at densities 1/3 and 2/3 that melt into a superfluid phase when increasing the hopping amplitude, in semi-quantitative agreement with quantum Monte Carlo computations. In the frustrated regime and away from half-filling, we find a series of plateaux with densities commensurate with powers of 1/3. The broader density plateaux (at densities 1/3 and 2/3) are remnants of the classical degeneracy in the Ising limit. For densities near half-filling, this staircase of crystal phases melts into a superfluid, which displays finite chiral currents when computed with clusters having an odd number of sites. Both the staircase of crystal phases and the superfluid phase prevail in the non-interacting limit, suggesting that the lowest dispersionless single-particle band may be at the root of this phenomenon.
Limits on the Mass and Initial Entropy of 51 Eri b from Gaia EDR3 Astrometry<|sep|>51 Eri b is one of the only young planets consistent with a wide range of possible initial entropy states, including the cold-start scenario associated with some models of planet formation by core accretion. The most direct way to constrain the initial entropy of a planet is by measuring its luminosity and mass at a sufficiently young age that the initial conditions still matter. We present the tightest upper limit on 51 Eri b's mass yet (M < 11 Mjup at 2$\sigma$) using a cross-calibration of Hipparcos and Gaia EDR3 astrometry and the orbit-fitting code orvara. We also reassess its luminosity using a direct, photometric approach, finding log(Lbol/Lsun) = -5.5$\pm$0.2 dex. Combining this luminosity with the 24$\pm$3 Myr age of the $\beta$ Pic moving group, of which 51 Eri is a member, we derive mass distributions from a grid of evolutionary models that spans a wide range of initial entropies. We find that 51 Eri b is inconsistent with the coldest-start scenarios, requiring an initial entropy of >8 $k_B$/baryon at 97% confidence. This result represents the first observational constraint on the initial entropy of a potentially cold-start planet, and it continues the trend of dynamical masses for directly imaged planets pointing to warm- or hot-start formation scenarios.
Wearable Assistive Devices for the Blind<|sep|>Assistive devices are a key aspect in wearable systems for biomedical applications, as they represent potential aids for people with physical and sensory disabilities that might lead to improvements in the quality of life. This chapter focuses on wearable assistive devices for the blind. It intends to review the most significant work done in this area, to present the latest approaches for assisting this population and to understand universal design concepts for the development of wearable assistive devices and systems for the blind.
Global-Local Propagation Network for RGB-D Semantic Segmentation<|sep|>Depth information matters in RGB-D semantic segmentation task for providing additional geometric information to color images. Most existing methods exploit a multi-stage fusion strategy to propagate depth feature to the RGB branch. However, at the very deep stage, the propagation in a simple element-wise addition manner can not fully utilize the depth information. We propose Global-Local propagation network (GLPNet) to solve this problem. Specifically, a local context fusion module(L-CFM) is introduced to dynamically align both modalities before element-wise fusion, and a global context fusion module(G-CFM) is introduced to propagate the depth information to the RGB branch by jointly modeling the multi-modal global context features. Extensive experiments demonstrate the effectiveness and complementarity of the proposed fusion modules. Embedding two fusion modules into a two-stream encoder-decoder structure, our GLPNet achieves new state-of-the-art performance on two challenging indoor scene segmentation datasets, i.e., NYU-Depth v2 and SUN-RGBD dataset.
Modern consumerism and the waste problem<|sep|>With the advance of industrial mass production, modern micro-electronics and computers, the intervals between the release of new generations of consumer products have been dramatically reduced and so have their lifetime cycles. While it was very natural in the post-war era, that sophisticated consumer products like television sets and stereo equipment would not be replaced with a new product until they break, and usually beyond that point since it was very common to have a broken television set serviced, the habits of consumers have changed during the last quarter of the 20th century. A modern consumer product, like Apple's famous iPhone has a market life of approximately one year until a successor is announced and subsequently pushed into the market. Usually these new generations bring a bunch of new features, have a higher performance while maintaining the price or becoming even cheaper, thus the consumer greatly benefits from the reduced lifetime cycle of these products. However, electronic devices not only require a lot of of Earth's limited resources for their production, but their production processes are a major source for harmful climate gases like carbon dioxide and toxic waste like heavy metal alloys, acids and alkalis. And last but not least is every obsoleted iPhone a candidate for waste facilities unless consumers are going to sell them on the second hand market. While we can not expect consumers and manufacturers to go back to the early days of consumer products where lifetime cycles reached up to 20 years, the world record being the famous "Centennial Lightbulb" in Livermore, CA in the US, which has been lit for over 100 years, it is certainly about time to rethink modern consumerism with regard to responsibility to future generations.
Inverse scattering at fixed energy for radial magnetic Schr{\"o}dinger operators with obstacle in dimension two<|sep|>We study an inverse scattering problem at fixed energy for radial magnetic Schr{\"o}dinger operators on R^2 \ B(0, r\_0), where r\_0 is a positive and arbitrarily small radius. We assume that the magnetic potential A satisfies a gauge condition and we consider the class C of smooth, radial and compactly supported electric potentials and magnetic fields denoted by V and B respectively. If (V, B) and (\tilde{V} , \tilde{B}) are two couples belonging to C, we then show that if the corresponding phase shifts $\delta$\_l and \tilde{$\delta$}\_l (i.e. the scattering data at fixed energy) coincide for all l $\in$ L, where L $\subset$ N^$\star$ satisfies the M{\"u}ntz condition \sum\_{l$\in$L} \frac{1}{l} = +$\infty$, then V (x) = \tilde{V}(x) and B(x) = \tilde{B}(x) outside the obstacle B(0, r\_0). The proof use the Complex Angular Momentum method and is close in spirit to the celebrated B{\"o}rg-Marchenko uniqueness Theorem.
Linear and integrable nonlinear evolution of the qutrit<|sep|>The nonlinear generalization of the von Neumann equation preserving convexity of the state space is studied in the nontrivial case of the qutrit. This equation can be cast into the integrable classical Riccati system of nonlinear ordinary differential equations. The solutions of such system are investigated in both the linear case corresponding to the standard von Neumann equation and the nonlinear one referring to the generalization of this equation. The analyzed dynamics of the qutrit is rich and includes quasiperiodic motion, multiple equilibria and limit cycles.
Alias Resolution Based on ICMP Rate Limiting<|sep|>Alias resolution techniques (e.g., Midar) associate, mostly through active measurement, a set of IP addresses as belonging to a common router. These techniques rely on distinct router features that can serve as a signature. Their applicability is affected by router support of the features and the robustness of the signature. This paper presents a new alias resolution tool called Limited Ltd. that exploits ICMP rate limiting, a feature that is increasingly supported by modern routers that has not previously been used for alias resolution. It sends ICMP probes toward target interfaces in order to trigger rate limiting, extracting features from the probe reply loss traces. It uses a machine learning classifier to designate pairs of interfaces as aliases. We describe the details of the algorithm used by Limited Ltd. and illustrate its feasibility and accuracy. Limited Ltd. not only is the first tool that can perform alias resolution on IPv6 routers that do not generate monotonically increasing fragmentation IDs (e.g., Juniper routers) but it also complements the state-of-the-art techniques for IPv4 alias resolution. All of our code and the collected dataset are publicly available.
Graphs cospectral with NU$(n + 1, q^2)$, $n \ne 3$<|sep|>Let $H(n, q^2)$ be a non-degenerate Hermitian variety of $PG(n, q^2)$, $n \ge 2$. Let NU$(n+1, q^2)$ be the graph whose vertices are the points of $PG(n, q^2) \setminus H(n, q^2)$ and two vertices $P_1$, $P_2$ are adjacent if the line joining $P_1$ and $P_2$ is tangent to $H(n, q^2)$. Then NU$(n + 1, q^2)$ is a strongly regular graph. In this paper we show that NU$(n + 1, q^2)$, $n \ne 3$, is not determined by its spectrum.
Deep NMF Topic Modeling<|sep|>Nonnegative matrix factorization (NMF) based topic modeling methods do not rely on model- or data-assumptions much. However, they are usually formulated as difficult optimization problems, which may suffer from bad local minima and high computational complexity. In this paper, we propose a deep NMF (DNMF) topic modeling framework to alleviate the aforementioned problems. It first applies an unsupervised deep learning method to learn latent hierarchical structures of documents, under the assumption that if we could learn a good representation of documents by, e.g. a deep model, then the topic word discovery problem can be boosted. Then, it takes the output of the deep model to constrain a topic-document distribution for the discovery of the discriminant topic words, which not only improves the efficacy but also reduces the computational complexity over conventional unsupervised NMF methods. We constrain the topic-document distribution in three ways, which takes the advantages of the three major sub-categories of NMF -- basic NMF, structured NMF, and constrained NMF respectively. To overcome the weaknesses of deep neural networks in unsupervised topic modeling, we adopt a non-neural-network deep model -- multilayer bootstrap network. To our knowledge, this is the first time that a deep NMF model is used for unsupervised topic modeling. We have compared the proposed method with a number of representative references covering major branches of topic modeling on a variety of real-world text corpora. Experimental results illustrate the effectiveness of the proposed method under various evaluation metrics.
High Performance CNFET-based Ternary Full Adders<|sep|>This paper investigates the use of carbon nanotube field effect transistors (CNFETs) for the design of ternary full adder cells. The proposed circuits have been designed based on the unique properties of CNFETs such as having desired threshold voltages by adjusting diameter of the CNFETs gate nanotubes. The proposed circuits are examined using HSPICE simulator with the standard 32 nm CNFET technology. The proposed methods are simulated at different conditions such as different supply voltages, different temperature and operational frequencies. Simulation results show that the proposed designs are faster than the state of the art CNFET based ternary full adders.
Observational studies of Cepheid amplitudes. II Metallicity dependence of pulsation amplitudes<|sep|>Physical and phenomenological properties (radius, luminosity, shape of the light curve, etc.) of Cepheids strongly depend on the pulsation period, with the exception of the pulsation amplitude. A possible factor causing a wide range of pulsation amplitudes might be the different atmospheric metallicities of individual Cepheids. We studied the influence exerted by the atmospheric iron content, [Fe/H], on the pulsational amplitude of Galactic Cepheids. We searched for correlations between the [Fe/H] value and both the observed amplitudes and amplitude related parameters. The amplitude of the Cepheid pulsation slightly decreases with increasing iron abundance. This effect is more pronounced for the radial velocity variations and for the shorter pulsation periods. The wavelength dependence of photometric amplitudes is also found to be sensitive to the metallicity. Some of these effects are not consequences of differential line blanketing. Based on the calibrations of the metallicity sensitivity relationships, we derived photometric iron abundance for 21 Galactic Cepheids. The dichotomic behaviour dividing Galactic Cepheids that pulsate in the fundamental mode into short- and long-period groups at the period of 10.47 d can be noticed in some diagrams that show metallicity-related dependences. We confirm that variety in atmospheric metallicity in Cepheids contributes to the finite range of pulsation amplitudes at a given period. Effects of metallicity on the amplitudes revealed from observational data and the occurrence of the dichotomy also derived from phenomenological data have to be confirmed by appropriate theoretical models of stellar structure and pulsation.
Computing the Testing Error without a Testing Set<|sep|>Deep Neural Networks (DNNs) have revolutionized computer vision. We now have DNNs that achieve top (performance) results in many problems, including object recognition, facial expression analysis, and semantic segmentation, to name but a few. The design of the DNNs that achieve top results is, however, non-trivial and mostly done by trail-and-error. That is, typically, researchers will derive many DNN architectures (i.e., topologies) and then test them on multiple datasets. However, there are no guarantees that the selected DNN will perform well in the real world. One can use a testing set to estimate the performance gap between the training and testing sets, but avoiding overfitting-to-the-testing-data is almost impossible. Using a sequestered testing dataset may address this problem, but this requires a constant update of the dataset, a very expensive venture. Here, we derive an algorithm to estimate the performance gap between training and testing that does not require any testing dataset. Specifically, we derive a number of persistent topology measures that identify when a DNN is learning to generalize to unseen samples. This allows us to compute the DNN's testing error on unseen samples, even when we do not have access to them. We provide extensive experimental validation on multiple networks and datasets to demonstrate the feasibility of the proposed approach.
Non-relativistic Holography and Singular Black Hole<|sep|>We provide a framework for non-relativistic holography so that a covariant action principle ensuring the Galilean symmetry for dual conformal field theory is given. This framework is based on the Bargmann lift of the Newton-Cartan gravity to the one-dimensional higher Einstein gravity, or reversely, the null-like Kaluza-Klein reduction. We reproduce the previous zero temperature results, and our framework provides a natural explanation about why the holography is co-dimension 2. We then construct the black hole solution dual to the thermal CFT, and find the horizon is curvature singular. However, we are able to derive the sensible thermodynamics for the dual non-relativistic CFT with correct thermodynamical relations. Besides, our construction admits a null Killing vector in the bulk such that the Galilean symmetry is preserved under the holographic RG flow. Finally, we evaluate the viscosity and find it zero if we neglect the back reaction of the singular horizon, otherwise, it could be nonzero.
Out-of-equilibrium protocol for R\'enyi entropies via the Jarzynski equality<|sep|>In recent years entanglement measures, such as the von Neumann and the R\'enyi entropies, provided a unique opportunity to access elusive feature of quantum many-body systems. However, extracting entanglement properties analytically, experimentally, or in numerical simulations can be a formidable task. Here, by combining the replica trick and the Jarzynski equality we devise a new effective out-of-equilibrium protocol for measuring the equilibrium R\'enyi entropies. The key idea is to perform a quench in the geometry of the replicas. The R\'enyi entropies are obtained as the exponential average of the work performed during the quench. We illustrate an application of the method in classical Monte Carlo simulations, although it could be useful in different contexts, such as in Quantum Monte Carlo, or experimentally in cold-atom systems. The method is most effective in the quasi-static regime, i.e., for a slow quench, where it allows to obtain the R\'enyi entropies in a single realization of the protocol. As a benchmark, we present results for the R\'enyi entropies in the Ising universality class in $1$$+$$1$ dimensions, which are found in perfect agreement with the well-known Conformal Field Theory (CFT) predictions.
Observables, gravitational dressing, and obstructions to locality and subsystems<|sep|>Quantum field theory - our basic framework for describing all non-gravitational physics - conflicts with general relativity: the latter precludes the standard definition of the former's essential principle of locality, in terms of commuting local observables. We examine this conflict more carefully, by investigating implications of gauge (diffeomorphism) invariance for observables in gravity. We prove a dressing theorem, showing that any operator with nonzero Poincare charges, and in particular any compactly-supported operator, in flat-spacetime quantum field theory must be gravitationally dressed once coupled to gravity, i.e. it must depend on the metric at arbitrarily long distances, and we put lower bounds on this nonlocal dependence. This departure from standard locality occurs in the most severe way possible: in perturbation theory about flat spacetime, at leading order in Newton's constant. The physical observables in a gravitational theory therefore do not organize themselves into local commuting subalgebras: the principle of locality must apparently be reformulated or abandoned, and in fact we lack a clear definition of the coarser and more basic notion of a quantum subsystem of the Universe. We discuss relational approaches to locality based on diffeomorphism-invariant nonlocal operators, and reinforce arguments that any such locality is state-dependent and approximate. We also find limitations to the utility of bilocal diffeomorphism-invariant operators that are considered in cosmological contexts. An appendix provides a concise review of the canonical covariant formalism for gravity, instrumental in the discussion of Poincare charges and their associated long-range fields.
Entropy-difference based stereo error detection<|sep|>Stereo depth estimation is error-prone; hence, effective error detection methods are desirable. Most such existing methods depend on characteristics of the stereo matching cost curve, making them unduly dependent on functional details of the matching algorithm. As a remedy, we propose a novel error detection approach based solely on the input image and its depth map. Our assumption is that, entropy of any point on an image will be significantly higher than the entropy of its corresponding point on the image's depth map. In this paper, we propose a confidence measure, Entropy-Difference (ED) for stereo depth estimates and a binary classification method to identify incorrect depths. Experiments on the Middlebury dataset show the effectiveness of our method. Our proposed stereo confidence measure outperforms 17 existing measures in all aspects except occlusion detection. Established metrics such as precision, accuracy, recall, and area-under-curve are used to demonstrate the effectiveness of our method.
Search for radio pulsations in LS I +61 303<|sep|>Context. LS I +61 303 is a member of the select group of gamma-ray binaries: galactic binary systems that contain a massive star and a compact object, show a changing milliarcsecond morphology and a similar broad spectral energy distribution (SED) that peaks at MeV-TeV energies and is modulated by the orbital motion. The nature of the compact object is unclear in LS I +61 303, LS 5039 and HESS J0632+057, whereas PSR B1259-63 harbours a 47.74 ms radio pulsar. Aims. A scenario in which a young pulsar wind interacts with the stellar wind has been proposed to explain the very high energy (VHE, E > 100 GeV) gamma-ray emission detected from LS I +61 303, although no pulses have been reported from this system at any wavelength. We aim to find evidence of the pulsar nature of the compact object. Methods. We performed phased array observations with the Giant Metrewave Radio Telescope (GMRT) at 1280 MHz centred at phase 0.54. Simultaneous data from the multi-bit phased array (PA) back-end with a sampling time of tsamp = 128 microsec and from the polarimeter (PMT) back-end with tsamp = 256 microsec where taken. Results. No pulses have been found in the data set, with a minimum detectable mean flux density of \sim 0.38 mJy at 8-sigma level for the pulsed emission from a putative pulsar with period P >2 ms and duty cycle D = 10% in the direction of LS I +61 303. Conclusions. The detection of posible radio pulsations will require deep and sensitive observations at frequencies \sim0.5-5 GHz and orbital phases 0.6-0.7. However, it may be unfeasible to detect pulses if the putative pulsar is not beamed at the Earth or if there is a strong absorption within the binary system.
Inflationary spectra with inverse-volume corrections in loop quantum cosmology and their observational constraints from Planck 2015 data<|sep|>We first derive the primordial power spectra, spectral indices and runnings of both scalar and tensor perturbations of a flat inflationary universe to the second-order approximations of the slow-roll parameters, in the framework of loop quantum cosmology with the inverse-volume quantum corrections. This represents an extension of our previous work in which the parameter $\sigma$ was assumed to be an integer, where $\sigma$ characterizes the quantum corrections and in general can take any of values from the range $\sigma \in (0, 6]$. Restricting to the first-order approximations of the slow-roll parameters, we find corrections to the results obtained previously in the literature, and point out the causes for such errors. To our best knowledge, these represent the most accurate calculations of scalar and tensor perturbations given so far in the literature. Then, fitting the perturbations to the recently released data by Planck (2015), we obtain the most severe constraints for various values of $\sigma$. Using these constraints as our referring point, we discuss whether these quantum gravitational corrections can lead to measurable signatures in the future cosmological observations. We show that, depending on the value of $\sigma$, the scale-dependent contributions to the relativistic inflationary spectra due to the inverse-volume corrections could be well within the range of the detectability of the forthcoming generations of experiments, such as the Stage IV experiments.
Facial Affect Analysis: Learning from Synthetic Data & Multi-Task Learning Challenges<|sep|>Facial affect analysis remains a challenging task with its setting transitioned from lab-controlled to in-the-wild situations. In this paper, we present novel frameworks to handle the two challenges in the 4th Affective Behavior Analysis In-The-Wild (ABAW) competition: i) Multi-Task-Learning (MTL) Challenge and ii) Learning from Synthetic Data (LSD) Challenge. For MTL challenge, we adopt the SMM-EmotionNet with a better ensemble strategy of feature vectors. For LSD challenge, we propose respective methods to combat the problems of single labels, imbalanced distribution, fine-tuning limitations, and choice of model architectures. Experimental results on the official validation sets from the competition demonstrated that our proposed approaches outperformed baselines by a large margin. The code is available at https://github.com/sylyoung/ABAW4-HUST-ANT.
Higgs boson mass and electroweak observables in the MRSSM<|sep|>R-symmetry is a fundamental symmetry which can solve the SUSY flavor problem and relax the search limits on SUSY masses. Here we provide a complete next-to-leading order computation and discussion of the lightest Higgs boson mass, the W boson mass and muon decay in the minimal R-symmetric SUSY model (MRSSM). This model contains non-MSSM particles including a Higgs triplet, Dirac gauginos and higgsinos, and leads to significant new tree-level and one-loop contributions to these observables. We show that the model can accommodate the measured values of the observables for interesting regions of parameter space with stop masses of order 1 TeV in spite of the absence of stop mixing. We characterize these regions and provide typical benchmark points, which are also checked against further experimental constraints. A detailed exposition of the model, its mass matrices and its Feynman rules relevant for computations in this paper is also provided.
Spatiotemporal Residual Networks for Video Action Recognition<|sep|>Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping these with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.
AdS Black Hole Solutions in the Extended New Massive Gravity<|sep|>We have obtained (warped) AdS black hole solutions in the three dimensional extended new massive gravity. We investigate some properties of black holes and obtain central charges of the two dimensional dual CFT. To obtain the central charges, we use the relation between entropy and temperature according to the AdS/CFT dictionary. For AdS black holes, one can also use the central charge function formalism which leads to the same results.
On the Combinatorial Multi-Armed Bandit Problem with Markovian Rewards<|sep|>We consider a combinatorial generalization of the classical multi-armed bandit problem that is defined as follows. There is a given bipartite graph of $M$ users and $N \geq M$ resources. For each user-resource pair $(i,j)$, there is an associated state that evolves as an aperiodic irreducible finite-state Markov chain with unknown parameters, with transitions occurring each time the particular user $i$ is allocated resource $j$. The user $i$ receives a reward that depends on the corresponding state each time it is allocated the resource $j$. The system objective is to learn the best matching of users to resources so that the long-term sum of the rewards received by all users is maximized. This corresponds to minimizing regret, defined here as the gap between the expected total reward that can be obtained by the best-possible static matching and the expected total reward that can be achieved by a given algorithm. We present a polynomial-storage and polynomial-complexity-per-step matching-learning algorithm for this problem. We show that this algorithm can achieve a regret that is uniformly arbitrarily close to logarithmic in time and polynomial in the number of users and resources. This formulation is broadly applicable to scheduling and switching problems in networks and significantly extends prior results in the area.
DLHub: Model and Data Serving for Science<|sep|>While the Machine Learning (ML) landscape is evolving rapidly, there has been a relative lag in the development of the "learning systems" needed to enable broad adoption. Furthermore, few such systems are designed to support the specialized requirements of scientific ML. Here we present the Data and Learning Hub for science (DLHub), a multi-tenant system that provides both model repository and serving capabilities with a focus on science applications. DLHub addresses two significant shortcomings in current systems. First, its selfservice model repository allows users to share, publish, verify, reproduce, and reuse models, and addresses concerns related to model reproducibility by packaging and distributing models and all constituent components. Second, it implements scalable and low-latency serving capabilities that can leverage parallel and distributed computing resources to democratize access to published models through a simple web interface. Unlike other model serving frameworks, DLHub can store and serve any Python 3-compatible model or processing function, plus multiple-function pipelines. We show that relative to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides greater capabilities, comparable performance without memoization and batching, and significantly better performance when the latter two techniques can be employed. We also describe early uses of DLHub for scientific applications.
Comparison of interface capturing methods for the simulation of two-phase flow in a unified low-Mach framework<|sep|>This paper proposes a comparison of four popular interface capturing methods : the volume of fluid (VOF), the standard level set (SLS), the accurate conservative level set (ACLS) and the coupled level set and volume of fluid (CLSVOF). All methods are embedded into a unified low-Mach framework based on a Cartesian-grid finite-volume discretization. This framework includes a sharp transport of the interface, a wellbalanced surface tension discretization and a consistent mass and momentum transport which allows capillary-driven simulations with high density ratio. The comparison relies on shared metrics for geometrical accuracy, mass and momentum conservation which exposes the weakness and strengths of each method. Finally, the versatility and capabilities of the proposed solver are demonstrated on the simulation of a 3D head-on collision of two water droplets. Overall, all methods manage to retrieve reasonable results for all test cases presented. VOF, CLSVOF and ACLS tend to artificially create little structures while SLS suffers from conservation issues in the mesh resolution limit. This study leads us to the conclusion that CLSVOF is the most promising method for two-phase flow simulations in our specific framework because of its inherent conservation properties and topology accuracy.
Juliet: a versatile modelling tool for transiting and non-transiting exoplanetary systems<|sep|>Here we present juliet, a versatile tool for the analysis of transits, radial-velocities, or both. juliet is built over many available tools for the modelling of transits, radial-velocities and stochastic processes (here modelled as Gaussian Processes; GPs) in order to deliver a tool/wrapper which can be used for the analysis of transit photometry and radial-velocity measurements from multiple instruments at the same time, using nested sampling algorithms which allows it to not only perform a thorough sampling of the parameter space, but also to perform model comparison via bayesian evidences. In addition, juliet allows to fit transiting and non-transiting multi-planetary systems, and to fit GPs which might share hyperparameters between the photometry and radial-velocities simultaneously (e.g., stellar rotation periods), which might be useful for disentangling stellar activity in radial-velocity measurements. Nested Sampling, Importance Nested Sampling and Dynamic Nested Sampling is performed with publicly available codes which in turn give juliet multi-threading options, allowing it to scale the computing time of complicated multi-dimensional problems. We make juliet publicly available via GitHub.
Improving Transferability of Adversarial Patches on Face Recognition with Generative Models<|sep|>Face recognition is greatly improved by deep convolutional neural networks (CNNs). Recently, these face recognition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically realizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the attacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we observe that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indicating the overfitting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face images. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transferability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well.
$k$-Sum Decomposition of Strongly Unimodular Matrices<|sep|>Networks are frequently studied algebraically through matrices. In this work, we show that networks may be studied in a more abstract level using results from the theory of matroids by establishing connections to networks by decomposition results of matroids. First, we present the implications of the decomposition of regular matroids to networks and related classes of matrices, and secondly we show that strongly unimodular matrices are closed under $k$-sums for $k=1,2$ implying a decomposition into highly connected network-representing blocks, which are also shown to have a special structure.
Jointly Optimizing Sensing Pipelines for Multimodal Mixed Reality Interaction<|sep|>Natural human interactions for Mixed Reality Applications are overwhelmingly multimodal: humans communicate intent and instructions via a combination of visual, aural and gestural cues. However, supporting low-latency and accurate comprehension of such multimodal instructions (MMI), on resource-constrained wearable devices, remains an open challenge, especially as the state-of-the-art comprehension techniques for each individual modality increasingly utilize complex Deep Neural Network models. We demonstrate the possibility of overcoming the core limitation of latency--vs.--accuracy tradeoff by exploiting cross-modal dependencies -- i.e., by compensating for the inferior performance of one model with an increased accuracy of more complex model of a different modality. We present a sensor fusion architecture that performs MMI comprehension in a quasi-synchronous fashion, by fusing visual, speech and gestural input. The architecture is reconfigurable and supports dynamic modification of the complexity of the data processing pipeline for each individual modality in response to contextual changes. Using a representative "classroom" context and a set of four common interaction primitives, we then demonstrate how the choices between low and high complexity models for each individual modality are coupled. In particular, we show that (a) a judicious combination of low and high complexity models across modalities can offer a dramatic 3-fold decrease in comprehension latency together with an increase 10-15% in accuracy, and (b) the right collective choice of models is context dependent, with the performance of some model combinations being significantly more sensitive to changes in scene context or choice of interaction.
Does Collaborative Editing Help Mitigate Security Vulnerabilities in Crowd-Shared IoT Code Examples?<|sep|>Background: With the proliferation of crowd-sourced developer forums, software developers are increasingly sharing more coding solutions to programming problems with others in forums. The decentralized nature of knowledge sharing on sites has raised the concern of sharing security vulnerable code, which then can be reused into mission critical software systems - making those systems vulnerable in the process. Collaborative editing has been introduced in forums like Stack Overflow to improve the quality of the shared contents. Aim: In this paper, we investigate whether code editing can mitigate shared vulnerable code examples by analyzing IoT code snippets and their revisions in three Stack Exchange sites: Stack Overflow, Arduino, and Raspberry Pi. Method:We analyze the vulnerabilities present in shared IoT C/C++ code snippets, as C/C++ is one of the most widely used languages in mission-critical devices and low-powered IoT devices. We further analyse the revisions made to these code snippets, and their effects. Results: We find several vulnerabilities such as CWE 788 - Access of Memory Location After End of Buffer, in 740 code snippets . However, we find the vast majority of posts are not revised, or revisions are not made to the code snippets themselves (598 out of 740). We also find that revisions are most likely to result in no change to the number of vulnerabilities in a code snippet rather than deteriorating or improving the snippet. Conclusions: We conclude that the current collaborative editing system in the forums may be insufficient to help mitigate vulnerabilities in the shared code.
Memristor model based on fuzzy window function<|sep|>Memristor (memory-resistor) is the fourth passive circuit element. We introduce a memristor model based on a fuzzy logic window function. Fuzzy models are flexible, which enables the capture of the pinched hysteresis behavior of the memristor. The introduced fuzzy model avoids common problems associated with window-function based memristor models, such as the terminal state problem, and the symmetry issues. The model captures the memristor behavior with a simple rule-base which gives an insight of how memristors work. Because of the flexibility offered by the fuzzy system, shape and distribution of input and output membership functions can be tuned to capture the behavior of various real memristors.
Trapped Fermi gases with Rashba spin-orbit coupling in two dimensions<|sep|>We use the Bogoliubov-de Gennes formalism to analyze harmonically trapped Fermi gases with Rashba-type spin-orbit coupling in two dimensions. We consider both population-balanced and -imbalanced Fermi gases throughout the BCS-BEC evolution, and study the effects of spin-orbit coupling on the spontaneously induced countercirculating mass currents and the associated intrinsic angular momentum. In particular, we find that even a small spin-orbit coupling destabilizes Fulde-Ferrel-Larkin-Ovchinnikov (FFLO)-type spatially modulated superfluid phases as well as the phase-separated states against the polarized superfluid phase. We also show that the continuum of quasiparticle and quasihole excitation spectrum can be connected by zero, one or two discrete branches of interface modes, depending on the number of interfaces between a topologically trivial phase (e.g. locally unpolarized/low-polarized superfluid or spin-polarized normal) and a topologically nontrivial one (e.g. locally high-polarized superfluid) that may be present in a trapped system.
Tight Global Linear Convergence Rate Bounds for Douglas-Rachford Splitting<|sep|>Recently, several authors have shown local and global convergence rate results for Douglas-Rachford splitting under strong monotonicity, Lipschitz continuity, and cocoercivity assumptions. Most of these focus on the convex optimization setting. In the more general monotone inclusion setting, Lions and Mercier showed a linear convergence rate bound under the assumption that one of the two operators is strongly monotone and Lipschitz continuous. We show that this bound is not tight, meaning that no problem from the considered class converges exactly with that rate. In this paper, we present tight global linear convergence rate bounds for that class of problems. We also provide tight linear convergence rate bounds under the assumptions that one of the operators is strongly monotone and cocoercive, and that one of the operators is strongly monotone and the other is cocoercive. All our linear convergence results are obtained by proving the stronger property that the Douglas-Rachford operator is contractive.
Detection of high Lyman continuum leakage from four low-redshift compact star-forming galaxies<|sep|>Following our first detection reported in Izotov et al. (2016), we present the detection of Lyman continuum (LyC) radiation of four other compact star-forming galaxies observed with the Cosmic Origins Spectrograph (COS) onboard the Hubble Space Telescope (HST). These galaxies, at redshifts of z~0.3, are characterized by high emission-line flux ratios [OIII]5007/[OII]3727 > 5. The escape fractions of the LyC radiation fesc(LyC) in these galaxies are in the range of ~6%-13%, the highest values found so far in low-redshift star-forming galaxies. Narrow double-peaked Lyalpha emission lines are detected in the spectra of all four galaxies, compatible with predictions for Lyman continuum leakers. We find escape fractions of Lyalpha, fesc(Lyalpha) ~20%-40%, among the highest known for Lyalpha emitters (LAEs). Surface brightness profiles produced from the COS acquisition images reveal bright star-forming regions in the center and exponential discs in the outskirts with disc scale lengths alpha in the range ~0.6-1.4 kpc. Our galaxies are characterized by low metallicity, ~1/8-1/5 solar, low stellar mass ~(0.2 - 4)e9 Msun, high star formation rates SFR~14-36 Msun/yr, and high SFR densities Sigma~2-35 Msun/yr/kpc^2. These properties are comparable to those of high-redshift star-forming galaxies. Finally, our observations, combined with our first detection reported in Izotov et al. (2016), reveal that a selection for compact star-forming galaxies showing high [OIII]5007/[OII]3727 ratios appears to pick up very efficiently sources with escaping Lyman continuum radiation: all five of our selected galaxies are LyC leakers.
Preview of Comet C/2021 A1 (Leonard) and Its Encounter with Venus<|sep|>Long period comet C/2021 A1 (Leonard) will approach Venus to within 0.029 au on 2021 December 18 and may subsequently graze the planet with its dust trail less than two days later. We observed C/2021 A1 with the Lowell Discovery Telescope on 2021 January 13 and March 3, as well as with the Palomar Hale Telescope on 2021 March 20, while the comet was inbound at heliocentric distances of r=4.97 au, 4.46 au, and 4.28 au, respectively. Tail morphology suggests that the dust is optically dominated by ~0.1-1 mm radius grains produced in the prior year. Neither narrowband imaging photometry nor spectrophotometry reveal any definitive gas emission, placing 3-sigma upper bounds on CN production of <1e23 molec/s at both of the latter two epochs. Trajectory analysis indicates that large (>1 mm) grains ejected at extremely large heliocentric distances (r>30 au) are most strongly favored to reach Venus. The flux of such meteors on Venus, and thus their potential direct or indirect observability, is highly uncertain as the comet's dust production history is poorly constrained at these distances, but will likely fall well below the meteor flux from comet C/2013 A1 (Siding Spring)'s closer encounter to Mars in 2014, and thus poses negligible risk to any spacecraft in orbit around Venus. Dust produced in previous apparitions will not likely contribute substantially to the meteor flux, nor will dust from any future activity apart from an unlikely high speed (>0.5 km/s) dust outburst prior to the comet reaching r~2 au in 2021 September.
An Open Model for Researching the Role of Culture in Online Self-Disclosure<|sep|>The analysis of consumers' personal information (PI) is a significant source to learn about consumers. In online settings, many consumers disclose PI abundantly -- this is particularly true for information provided on social network services. Still, people manage the privacy level they want to maintain by disclosing by disclosing PI accordingly. In addition, studies have shown that consumers' online self-disclosure (OSD) differs across cultures. Therefore, intelligent systems should consider cultural issues when collecting, processing, storing or protecting data from consumers. However, existing studies typically rely on a comparison of two cultures, providing valuable insights but not drawing a comprehensive picture. We introduce an open research model for cultural OSD research, based on the privacy calculus theory. Our open research model incorporates six cultural dimensions, six predictors, and 24 structured propositions. It represents a comprehensive approach that provides a basis to explain possible cultural OSD phenomena in a systematic way.
Auctioning with Strategically Reticent Bidders<|sep|>Classic mechanism design often assumes that a bidder's action is restricted to report a type or a signal, possibly untruthfully. In today's digital economy, bidders are holding increasing amount of private information about the auctioned items. And due to legal or ethical concerns, they would demand to reveal partial but truthful information, as opposed to report untrue signal or misinformation. To accommodate such bidder behaviors in auction design, we propose and study a novel mechanism design setup where each bidder holds two kinds of information: (1) private \emph{value type}, which can be misreported; (2) private \emph{information variable}, which the bidder may want to conceal or partially reveal, but importantly, \emph{not} to misreport. We show that in this new setup, it is still possible to design mechanisms that are both \emph{Incentive and Information Compatible} (IIC). We develop two different black-box transformations, which convert any mechanism $\mathcal{M}$ for classic bidders to a mechanism $\mathcal{M}'$ for strategically reticent bidders, based on either outcome of expectation or expectation of outcome, respectively. We identify properties of the original mechanism $\mathcal{M}$ under which the transformation leads to IIC mechanisms $\mathcal{M}'$. Interestingly, as corollaries of these results, we show that running VCG with expected bidder values maximizes welfare whereas the mechanism using expected outcome of Myerson's auction maximizes revenue. Finally, we study how regulation on the auctioneer's usage of information may lead to more robust mechanisms.
A proposed method using GPU based SDO to optimize retail warehouses<|sep|>Research in warehouse optimization has gotten increased attention in the last few years due to e-commerce. The warehouse contains a waste range of different products. Due to the nature of the individual order, it is challenging to plan the picking list to optimize the material flow in the process. There are also challenges in minimizing costs and increasing production capacity, and this complexity can be defined as a multidisciplinary optimization problem with an IDF nature. In recent years the use of parallel computing using GPGPUs has become increasingly popular due to the introduction of CUDA C and accompanying applications in, e.g., Python. In the case study at the company in the field of retail, a case study including a system design optimization (SDO) resulted in an increase in throughput with well over 20% just by clustering different categories and suggesting in which sequence the orders should be picked during a given time frame. The options provided by implementing a distributed high-performance computing network based on GPUs for subsystem optimization have shown to be fruitful in developing a functioning SDO for warehouse optimization. The toolchain can be used for designing new warehouses or evaluating and tuning existing ones.
Reduction without reduction: Adding KK-monopoles to five dimensional stationary axisymmetric solutions<|sep|>We present a general method to add KK-monopole charge to any asymptotically flat stationary axisymmetric solution of five dimensional General Relativity. The technique exploits the underlying SL(3,R) invariance of the system by identifying a particular element of the symmetry group which changes the asymptotic boundary condition and adds KK-monopole charge. Furthermore, we develop a set of technical tools which allow us to apply the SL(3,R) transformations to solutions produced by the Inverse Scattering method. As an example of our methods, we construct the exact solution describing a static black ring carrying KK-monopole charge.
Curriculum Meta-Learning for Few-shot Classification<|sep|>We propose an adaptation of the curriculum training framework, applicable to state-of-the-art meta learning techniques for few-shot classification. Curriculum-based training popularly attempts to mimic human learning by progressively increasing the training complexity to enable incremental concept learning. As the meta-learner's goal is learning how to learn from as few samples as possible, the exact number of those samples (i.e. the size of the support set) arises as a natural proxy of a given task's difficulty. We define a simple yet novel curriculum schedule that begins with a larger support size and progressively reduces it throughout training to eventually match the desired shot-size of the test setup. This proposed method boosts the learning efficiency as well as the generalization capability. Our experiments with the MAML algorithm on two few-shot image classification tasks show significant gains with the curriculum training framework. Ablation studies corroborate the independence of our proposed method from the model architecture as well as the meta-learning hyperparameters
No Evidence for a Aystematic FEII Emission Line Redshift in Type 1 AGN<|sep|>We test the recent claim by Hu et al. (2008) that FeII emission in Type 1 AGN shows a systematic redshift relative to the local source rest frame and broad-line Hbeta. We compile high s/n median composites using SDSS spectra from both the Hu et al. sample and our own sample of the 469 brightest DR5 spectra. Our composites are generated in bins of FWHM Hbeta and FeII strength as defined in our 4D Eigenvector 1 (4DE1) formalism. We find no evidence for a systematic FeII redshift and consistency with previous assumptions that FeII shift and width (FWHM) follow Hbeta shift and FWHM in virtually all sources. This result is consistent with the hypothesis that FeII emission (quasi-ubiquitous in type 1 sources) arises from a broad-line region with geometry and kinematics the same as that producing the Balmer lines.
Energy Saving Techniques for Phase Change Memory (PCM)<|sep|>In recent years, the energy consumption of computing systems has increased and a large fraction of this energy is consumed in main memory. Towards this, researchers have proposed use of non-volatile memory, such as phase change memory (PCM), which has low read latency and power; and nearly zero leakage power. However, the write latency and power of PCM are very high and this, along with limited write endurance of PCM present significant challenges in enabling wide-spread adoption of PCM. To address this, several architecture-level techniques have been proposed. In this report, we review several techniques to manage power consumption of PCM. We also classify these techniques based on their characteristics to provide insights into them. The aim of this work is encourage researchers to propose even better techniques for improving energy efficiency of PCM based main memory.
Linear gyrokinetic stability of a high $\beta$ non-inductive spherical tokamak<|sep|>Spherical tokamaks (STs) have been shown to possess properties desirable for a fusion power plant such as achieving high plasma ? and having increased vertical stability. To understand the confinement properties that might be expected in the conceptual design for a high $\beta$ ST fusion reactor, a 1GW ST plasma equilibrium was analysed using local linear gyrokinetics to determine the type of micro-instabilities that arise. Kinetic ballooning modes (KBMs) and micro-tearing modes (MTMs) are found to be the dominant instabilities. The parametric dependence of these linear modes was determined and from the insights gained, the equilibrium was tuned to find a regime marginally stable to all micro-instabilities at $\theta_0$ = 0:0. This work identifies the most important micro-instabilities expected to generate turbulent transport in high $\beta$ STs. The impact of such modes must be faithfully captured in first principles based reduced models of anomalous transport that are needed for predictive simulations.
Interaction and thermodynamics of spinons in the XX chain<|sep|>The mapping between the fermion and spinon compositions of eigenstates in the one-dimensional spin-1/2 XX model on a lattice with N sites is used to describe the spinon interaction from two different perspectives: (i) For finite N the energy of all eigenstates is expressed as a function of spinon momenta and spinon spins, which, in turn, are solutions of a set of Bethe ansatz equations. The latter are the basis of an exact thermodynamic analysis in the spinon representation of the XX model. (ii) For N -> infinity the energy per site of spinon configurations involving any number of spinon orbitals is expressed as a function of reduced variables representing momentum, filling, and magnetization of each orbital. The spins of spinons in a single orbital are found to be coupled in a manner well described by an Ising-like equivalent-neighbor interaction, switching from ferromagnetic to antiferromagnetic as the filling exceeds a critical level. Comparisons are made with results for the Haldane-Shastry model.
The role of the chemical potential in the BCS theory<|sep|>We study the effect of the chemical potential on the results of the BCS theory of superconductivity. We assume that the pairing interaction is manifested between electrons of single-particle energies in an interval $[\mu - \hbar\omega_c, \mu + \hbar\omega_c]$, where $\mu$ and $\omega_c$ are parameters of the model--$\mu$ needs not be equal to the chemical potential of the system, denoted here by $\mu_R$. The BCS results are recovered if $\mu = \mu_R$. If $\mu \ne \mu_R$ the physical properties change significantly: the energy gap $\Delta$ is smaller than the BCS gap, a population imbalance appears, and the superconductor-normal metal phase transition is of the first order. The quasiparticle imbalance is an equilibrium property that appears due to the asymmetry with respect to $\mu_R$ of the single-particle energy interval in which the pairing potential is manifested. For $\mu_R - \mu$ taking values in some ranges, the equation for $\Delta$ may have more than one solution at the same temperature, forming branches of solutions when $\Delta$ is plotted vs $\mu_R-\mu$ at fixed $T$. The solution with the highest energy gap, which corresponds to the BCS solution when $\mu = \mu_R$, cease to exist if $|\mu-\mu_R| \ge 2\Delta_0$ ($\Delta_0$ is the BCS gap at zero temperature). Therefore the superconductivity is conditioned by the existence of the pairing interaction and also by the value of $\mu_R - \mu$.
Disappearance of stretch-induced wrinkles of thin sheets: a study of orthotropic films<|sep|>A recent paper (Healey et al., J. Nonlin. Sci., 2013, 23:777-805.) predicted the disappearance of the stretch-induced wrinkled pattern of thin, clamped, elastic sheets by numerical simulation of the F\"oppl-von K\'arm\'an equations extended to the finite in-plane strain regime. It has also been revealed that for some aspect ratios of the rectangular domain wrinkles do not occur at all regardless of the applied extension. To verify these predictions we carried out experiments on thin 20 micrometer thick adhesive covered), previously prestressed elastomer sheets with different aspect ratios under displacement controlled pull tests. On one hand the the adjustment of the material properties during prestressing is highly advantageous as in targeted strain regime the film becomes substantially linearly elastic (which is far not the case without prestress). On the other hand a significant, non-ignorable orthotropy develops during this first extension. To enable quantitative comparisons we abandoned the assumption about material isotropy inherent in the original model and derived the governing equations for an orthotropic medium. In this way we found good agreement between numerical simulations and experimental data. Analysis of the negativity of the second Piola-Kirchhoff stress tensor revealed that the critical stretch for a bifurcation point at which the wrinkles disappear must be finite for any aspect ratio. On the contrary there is no such a bound for the aspect ratio as a bifurcation parameter. Physically this manifests as complicated wrinkled patterns with more than one highly wrinkled zones on the surface in case of elongated rectangles. These arrangements have been found both numerically and experimentally. These findings also support the new, finite strain model, since the F\"oppl-von K\'arm\'an equations based on infinitesimal strains do not exhibit such a behavior.
Automatised full one-loop renormalisation of the MSSM I: The Higgs sector, the issue of tan(beta) and gauge invariance<|sep|>We give an extensive description of the renormalisation of the Higgs sector of the minimal supersymmetric model in SloopS. SloopS is an automatised code for the computation of one-loop processes in the MSSM. In this paper, the first in a series, we study in detail the non gauge invariance of some definitions of tan(beta). We rely on a general non-linear gauge fixing constraint to make the gauge parameter dependence of different schemes for tan(beta) at one-loop explicit. In so doing, we update, within these general gauges, an important Ward-Slavnov-Taylor identity on the mixing between the pseudo-scalar Higgs, A^0, and the Z^0. We then compare the tan(beta) scheme dependence of a few observables. We find that the best tan(beta) scheme is the one based on the decay A^0 -> tau^+ tau^- because of its gauge invariance, being unambiguously defined from a physical observable, and because it is numerically stable. The oft used DRbar scheme performs almost as well on the last count, but is usually defined from non-gauge invariant quantities in the Higgs sector. The use of the heavier scalar Higgs mass in lieu of tan(beta) though related to a physical parameter induces too large radiative corrections in many instances and is therefore not recommended.
Quantum phase slip phenomenon in superconducting nanowires with low-Ohmic environment<|sep|>In a number of recent experiments it has been demonstrated that in ultra-narrow superconducting channels quantum fluctuations of the order parameter, alternatively called quantum phase slips, are responsible for the finite resistance well below the critical temperature. The acceptable agreement between those experiments and the models describing quantum fluctuations in quasi-one-dimensional superconductors has been established. However the very concept of the phase slip is justified when these fluctuations are the relatively rare events, meaning that the effective resistance of the system should be much smaller than the normal state equivalent. In this paper we study the limit of the strong quantum fluctuations where the existing models are not applicable. In particular case of ultra-thin titanium nanowires it is demonstrated that below the expected critical temperature the resistance does not demonstrate any trend towards the conventional for a superconductor zero-resistivity state even at negligibly small measuring currents. Application of a small magnetic field leads to an unusual negative magnetoresistance, which becomes more pronounced at lower temperatures. The origin of the negative magnetoresistance effect is not clear.
ALIFE: Adaptive Logit Regularizer and Feature Replay for Incremental Semantic Segmentation<|sep|>We address the problem of incremental semantic segmentation (ISS) recognizing novel object/stuff categories continually without forgetting previous ones that have been learned. The catastrophic forgetting problem is particularly severe in ISS, since pixel-level ground-truth labels are available only for the novel categories at training time. To address the problem, regularization-based methods exploit probability calibration techniques to learn semantic information from unlabeled pixels. While such techniques are effective, there is still a lack of theoretical understanding of them. Replay-based methods propose to memorize a small set of images for previous categories. They achieve state-of-the-art performance at the cost of large memory footprint. We propose in this paper a novel ISS method, dubbed ALIFE, that provides a better compromise between accuracy and efficiency. To this end, we first show an in-depth analysis on the calibration techniques to better understand the effects on ISS. Based on this, we then introduce an adaptive logit regularizer (ALI) that enables our model to better learn new categories, while retaining knowledge for previous ones. We also present a feature replay scheme that memorizes features, instead of images directly, in order to reduce memory requirements significantly. Since a feature extractor is changed continually, memorized features should also be updated at every incremental stage. To handle this, we introduce category-specific rotation matrices updating the features for each category separately. We demonstrate the effectiveness of our approach with extensive experiments on standard ISS benchmarks, and show that our method achieves a better trade-off in terms of accuracy and efficiency.
Thermalization and dynamical spectral properties in the quark-meson model<|sep|>We investigate the nonequilibrium evolution of the quark-meson model using two-particle irreducible effective action techniques. Our numerical simulations, which include the full dynamics of the order parameter of chiral symmetry, show how the model thermalizes into different regions of its phase diagram. In particular, by studying quark and meson spectral functions, we shed light on the real-time dynamics approaching the crossover transition, revealing e.g. the emergence of light effective fermionic degrees of freedom in the infrared. At late times in the evolution, the fluctuation-dissipation relation emerges naturally among both meson and quark degrees of freedom, confirming that the simulation successfully reaches thermal equilibrium.
Circumnuclear Multi-phase Gas in Circinus Galaxy IV: absorption owing to high-$J$ CO rotational transitions<|sep|>We studied the absorption features of CO lines against the continuum originating from the heated dust in the obscuring tori around active galactic nuclei (AGNs). We investigated the formation of absorption lines corresponding to the CO rotational transitions using three-dimensional non-LTE line transfer simulations considering the dust thermal emission. As in Papers I--III of this series, we performed post-processed radiative transfer calculations using the "radiation-driven fountain model" (wada2016}, which yields a geometrically thick obscuring structure around the nucleus. This model is consistent with the spectral energy distribution of the nearest type-2 Seyfert galaxy, the Circinus galaxy. We found that the continuum-subtracted channel maps of $J = 4-3$ and higher transitions show absorption regions along the disk mid-plane for an edge-on viewing angle. The spectra consist of multiple absorption and emission features, reflecting the internal inhomogeneous and turbulent structure of the torus. The deepest absorption feature is caused by the gas on the near-side of the torus between $r =10$ and 15 pc, which is located in front of the AGN-heated dust inside $r \simeq 5$ pc. We also found that a spatial resolution of 0.5--1.0 pc is necessary to resolve the absorption features. Moreover, the inclination angle must be close to the edge-on angle (i.e., $\sim 85^\circ$) to observe the absorption features. The findings of the present study imply that combining our radiation-hydrodynamic model with high-resolution observations of CO (7-6) by ALMA can provide new information about the internal structure of the molecular tori in nearby AGNs.
Nonlinear Schr\"odinger equation from generalized exact uncertainty principle<|sep|>Inspired by the generalized uncertainty principle (GUP), which adds gravitational effects to the standard description of quantum uncertainty, we extend the exact uncertainty principle (EUP) approach by Hall and Reginatto [J. Phys. A: Math. Gen. (2002) 35 3289], and obtain a (quasi)nonlinear Schr\"odinger equation. This quantum evolution equation of unusual form, enjoys several desired properties like separation of non-interacting subsystems or planewave solutions for free particles. Starting with the harmonic oscillator example, we show that every solution of this equation respects the gravitationally-induced minimal position uncertainty proportional to the Planck length. Quite surprisingly, our result successfully merges the core of classical physics with non-relativistic quantum mechanics in its extremal form. We predict that the commonly accepted phenomenon, namely a modification of a free-particle dispersion relation due to quantum gravity might not occur in reality.
Learning to Play against Any Mixture of Opponents<|sep|>Intuitively, experience playing against one mixture of opponents in a given domain should be relevant for a different mixture in the same domain. We propose a transfer learning method, Q-Mixing, that starts by learning Q-values against each pure-strategy opponent. Then a Q-value for any distribution of opponent strategies is approximated by appropriately averaging the separately learned Q-values. From these components, we construct policies against all opponent mixtures without any further training. We empirically validate Q-Mixing in two environments: a simple grid-world soccer environment, and a complicated cyber-security game. We find that Q-Mixing is able to successfully transfer knowledge across any mixture of opponents. We next consider the use of observations during play to update the believed distribution of opponents. We introduce an opponent classifier -- trained in parallel to Q-learning, using the same data -- and use the classifier results to refine the mixing of Q-values. We find that Q-Mixing augmented with the opponent classifier function performs comparably, and with lower variance, than training directly against a mixed-strategy opponent.
A False Sense of Security? Revisiting the State of Machine Learning-Based Industrial Intrusion Detection<|sep|>Anomaly-based intrusion detection promises to detect novel or unknown attacks on industrial control systems by modeling expected system behavior and raising corresponding alarms for any deviations.As manually creating these behavioral models is tedious and error-prone, research focuses on machine learning to train them automatically, achieving detection rates upwards of 99%. However, these approaches are typically trained not only on benign traffic but also on attacks and then evaluated against the same type of attack used for training. Hence, their actual, real-world performance on unknown (not trained on) attacks remains unclear. In turn, the reported near-perfect detection rates of machine learning-based intrusion detection might create a false sense of security. To assess this situation and clarify the real potential of machine learning-based industrial intrusion detection, we develop an evaluation methodology and examine multiple approaches from literature for their performance on unknown attacks (excluded from training). Our results highlight an ineffectiveness in detecting unknown attacks, with detection rates dropping to between 3.2% and 14.7% for some types of attacks. Moving forward, we derive recommendations for further research on machine learning-based approaches to ensure clarity on their ability to detect unknown attacks.
The composition of transiting giant extrasolar planets<|sep|>In principle, the combined measurements of the mass and radius a giant exoplanet allow one to determine the relative fraction of hydrogen and helium and of heavy elements in the planet. However, uncertainties on the underlying physics imply that some known transiting planets appear anomalously large, and this generally prevent any firm conclusion when a planet is considered on an individual basis. On the basis of a sample of 9 transiting planets known at the time, Guillot et al. A&A 453, L21 (1996), concluded that all planets could be explained with the same set of hypotheses, either by large but plausible modifications of the equations of state, opacities, or by the addition of an energy source, probably related to the dissipation of kinetic energy by tides. On this basis, they concluded that the amount of heavy elements in close-in giant planets is correlated with the metallicity of the parent star. Furthermore they showed that planets around metal-rich stars can possess large amounts of heavy elements, up to 100 Earth masses. These results are confirmed by studying the present sample of 18 transiting planets with masses between that of Saturn and twice the mass of Jupiter.
Multigrid methods for two-player zero-sum stochastic games<|sep|>We present a fast numerical algorithm for large scale zero-sum stochastic games with perfect information, which combines policy iteration and algebraic multigrid methods. This algorithm can be applied either to a true finite state space zero-sum two player game or to the discretization of an Isaacs equation. We present numerical tests on discretizations of Isaacs equations or variational inequalities. We also present a full multi-level policy iteration, similar to FMG, which allows to improve substantially the computation time for solving some variational inequalities.
Nonlinear Dynamics of Parity-Even Tricritical Gravity in Three and Four Dimensions<|sep|>Recently proposed "multicritical" higher-derivative gravities in Anti de Sitter space carry logarithmic representations of the Anti de Sitter isometry group. While generically non-unitary already at the quadratic, free-theory level, in special cases these theories admit a unitary subspace. The simplest example of such behavior is "tricritical" gravity. In this paper, we extend the study of parity-even tricritical gravity in d = 3, 4 to the first nonlinear order. We show that the would-be unitary subspace suffers from a linearization instability and is absent in the full non-linear theory.
Hamiltonian-connectedness of triangulations with few separating triangles<|sep|>We prove that 3-connected triangulations with at most one separating triangle are hamiltonian-connected. In order to show bounds on the strongest form of this theorem, we proved that for any $s\geq4$ there are 3-connected triangulation with $s$ separating triangles that are not hamiltonian-connected. We also present computational results which show that all `small' 3-connected triangulations with at most 3 separating triangles are hamiltonian-connected.
Poking the Beehive From Space: K2 Rotation Periods For Praesepe<|sep|>We analyze {\it K2} light curves for 794 low-mass ($1 > M_* > 0.1$ $M_{\odot}$) members of the $\approx$650-Myr-old open cluster Praesepe, and measure rotation periods ($P_{rot}$) for 677 of these stars. We find that half of the rapidly rotating $>$0.3 $M_{\odot}$ stars are confirmed or candidate binary systems. The remaining $>0.3$ $M_{\odot}$ fast rotators have not been searched for companions, and are therefore not confirmed single stars. We found previously that nearly all rapidly rotating $>$0.3 $M_{\odot}$ stars in the Hyades are binaries, but we require deeper binary searches in Praesepe to confirm whether binaries in these two co-eval clusters have different $P_{rot}$ distributions. We also compare the observed $P_{rot}$ distribution in Praesepe to that predicted by models of angular-momentum evolution. We do not observe the clear bimodal $P_{rot}$ distribution predicted by Brown (2014) for $>$0.5 $M_{\odot}$ stars at the age of Praesepe, but 0.25$-$0.5 $M_{\odot}$ stars do show stronger bimodality. In addition, we find that $>$60\% of early M dwarfs in Praesepe rotate more slowly than predicted at 650 Myr by Matt et al. (2015), which suggests an increase in braking efficiency for these stars relative to solar-type stars and fully convective stars. The incompleteness of surveys for binaries in open clusters likely impacts our comparison with these models, since the models only attempt to describe the evolution of isolated single stars.
Electrodynamics in accelerated frames revisited<|sep|>Maxwell's equations are formulated in arbitrary moving frames by means of tetrad fields, which are interpreted as reference frames adapted to observers in space-time. We assume the existence of a general distribution of charges and currents in an inertial frame. Tetrad fields are used to project the electromagnetic fields and sources on accelerated frames. The purpose is to study several configurations of fields and observers that in the literature are understood as paradoxes. For instance, are the two situations, (i) an accelerated charge in an inertial frame, and (ii) a charge at rest in an inertial frame described from the perspective of an accelerated frame, physically equivalent? Is the electromagnetic radiation the same in both frames? Normally in the analysis of these paradoxes the electromagnetic fields are transformed to (uniformly) accelerated frames by means of a coordinate transformation of the Faraday tensor. In the present approach coordinate and frame transformations are disentangled, and the electromagnetic field in the accelerated frame is obtained through a frame (local Lorentz) transformation. Consequently the fields in the inertial and accelerated frames are described in the same coordinate system. This feature allows the investigation of paradoxes such as the one mentioned above.
Blur Robust Optical Flow using Motion Channel<|sep|>It is hard to estimate optical flow given a realworld video sequence with camera shake and other motion blur. In this paper, we first investigate the blur parameterization for video footage using near linear motion elements. we then combine a commercial 3D pose sensor with an RGB camera, in order to film video footage of interest together with the camera motion. We illustrates that this additional camera motion/trajectory channel can be embedded into a hybrid framework by interleaving an iterative blind deconvolution and warping based optical flow scheme. Our method yields improved accuracy within three other state-of-the-art baselines given our proposed ground truth blurry sequences; and several other realworld sequences filmed by our imaging system.
Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and Benchmark<|sep|>Perhaps surprisingly sewerage infrastructure is one of the most costly infrastructures in modern society. Sewer pipes are manually inspected to determine whether the pipes are defective. However, this process is limited by the number of qualified inspectors and the time it takes to inspect a pipe. Automatization of this process is therefore of high interest. So far, the success of computer vision approaches for sewer defect classification has been limited when compared to the success in other fields mainly due to the lack of public datasets. To this end, in this work we present a large novel and publicly available multi-label classification dataset for image-based sewer defect classification called Sewer-ML. The Sewer-ML dataset consists of 1.3 million images annotated by professional sewer inspectors from three different utility companies across nine years. Together with the dataset, we also present a benchmark algorithm and a novel metric for assessing performance. The benchmark algorithm is a result of evaluating 12 state-of-the-art algorithms, six from the sewer defect classification domain and six from the multi-label classification domain, and combining the best performing algorithms. The novel metric is a class-importance weighted F2 score, $\text{F}2_{\text{CIW}}$, reflecting the economic impact of each class, used together with the normal pipe F1 score, $\text{F}1_{\text{Normal}}$. The benchmark algorithm achieves an $\text{F}2_{\text{CIW}}$ score of 55.11% and $\text{F}1_{\text{Normal}}$ score of 90.94%, leaving ample room for improvement on the Sewer-ML dataset. The code, models, and dataset are available at the project page https://vap.aau.dk/sewer-ml/
Effect of Temperature on the Complexity of Solid Argon System<|sep|>We study the measure of complexity in solid Argon system from the time series data of kinetic energy of single Argon atoms at different equilibrated temperatures. To account the inherent multi-scale dependence of the complexity, the multi-scale entropy of the time series of kinetic energy of individual Argon atoms are computed at different equilibrated temperatures. The multi-scale entropy study reveals that the dynamics of an atom becomes more complex at higher temperatures and the result corroborates well with the variation of the pair correlation function of the atoms in the solid Argon crystal. Also, we repeat the multi-scale entropy analysis for program generated Levy noise time series and for time series data obtained from the outcomes of exponential decay with noise dx(t) = -x(t) dt + sigma dB(t) (Langevin equation). Our study establishes that the scale dependence of sample entropy for time series of kinetic energy of individual atoms in solid Argon system has similar tendency as that of Levy noise time series and the outcomes of exponential decay with noise (Langevin equation).
Psamathe: A DSL with Flows for Safe Blockchain Assets<|sep|>Blockchains host smart contracts for crowdfunding, tokens, and many other purposes. Vulnerabilities in contracts are often discovered, leading to the loss of large quantities of money. Psamathe is a new language we are designing around a new flow abstraction, reducing asset bugs and making contracts more concise than in existing languages. We present an overview of Psamathe, including a partial formalization. We also discuss several example contracts in Psamathe, and compare the Psamathe examples to the same contracts written in Solidity.
Active Galactic Nuclei: Boon or Bane for Biota?<|sep|>Active Galactic Nuclei (AGNs) emit substantial fluxes of high-energy electromagnetic radiation, and have therefore attracted some recent attention for their negative impact on galactic habitability. In this paper, we propose that AGNs may also engender the following beneficial effects: (i) prebiotic synthesis of biomolecular building blocks mediated by ultraviolet (UV) radiation, and (ii) powering photosynthesis on certain free-floating planets and moons. We also reassess the harmful biological impact of UV radiation originating from AGNs, and find that their significance could have been overestimated. Our calculations suggest that neither the positive nor negative ramifications stemming from a hypothetical AGN in the Milky Way are likely to affect putative biospheres in most of our Galaxy. On the other hand, we find that a sizable fraction of all planetary systems in galaxies with either disproportionately massive black holes ($\sim 10^{9-10}\,M_\odot$) or high stellar densities (e.g., compact dwarf galaxies) might be susceptible to both the beneficial and detrimental consequences of AGNs, with the former potentially encompassing a greater spatial extent than the latter.
Positrons and antiprotons from inert doublet model dark matter<|sep|>In the framework of the Inert Doublet Model, a very simple extension of the Standard Model, we study the production and propagation of antimatter in cosmic rays coming from annihilation of a scalar dark matter particle. We consider three benchmark candidates, all consistent with the WMAP cosmic abundance and existing direct detection experiments, and confront the predictions of the model with the recent PAMELA, ATIC and HESS data. For a light candidate, M_{DM} = 10 GeV, we argue that the positron and anti-proton fluxes may be large, but still consistent with expected backgrounds, unless there is an enhancement (boost factor) in the local density of dark matter. There is also a substantial anti-deuteron flux which might be observable by future experiments. For a candidate with M_{DM} = 70 GeV, the contribution to positron and anti-proton fluxes is much smaller than the expected backgrounds. Even if a boost factor is invoked to enhance the signals, the candidate is unable to explain the observed positron and anti-proton excesses. Finally, for a heavy candidate, M_{DM} = 10 TeV, it is possible to fit the PAMELA excess (but, unfortunately, not the ATIC one) provided there is a large enhancement, either in the local density of dark matter or through the Sommerfeld effect.
On the shape of a D-brane bound state and its topology change<|sep|>As is well known, coordinates of D-branes are described by NxN matrices. From generic non-commuting matrices, it is difficult to extract physics, for example, the shape of the distribution of positions of D-branes. To overcome this problem, we generalize and elaborate on a simple prescription, first introduced by Hotta, Nishimura and Tsuchiya, which determines the most appropriate gauge to make the separation between diagonal components (D-brane positions) and off-diagonal components. This prescription makes it possible to extract the distribution of D-branes directly from matrices. We verify the power of it by applying it to Monte-Carlo simulations for various lower dimensional Yang-Mills matrix models. In particular, we detect the topology change of the D-brane bound state for a phase transition of a matrix model; the existence of this phase transition is expected from the gauge/gravity duality, and the pattern of the topology change is strikingly similar to the counterpart in the gravity side, the black hole/black string transition. We also propose a criterion, based on the behavior of the off-diagonal components, which determines when our prescription gives a sensible definition of D-brane positions. We provide numerical evidence that our criterion is satisfied for the typical distance between D-branes. For a supersymmetric model, positions of D-branes can be defined even at a shorter distance scale. The behavior of off-diagonal elements found in this analysis gives some support for previous studies of D-brane bound states.
Computational molecular field theory for nematic liquid crystals<|sep|>Nematic liquid crystals exhibit configurations in which the underlying ordering changes markedly on macroscopic length scales. Such structures include topological defects in the nematic phase and tactoids within nematic-isotropic coexistence. We discuss a computational study of inhomogeneous configurations that is based on a field theory extension of the Maier-Saupe molecular model of a uniaxial, nematic liquid crystal. A tensor order parameter is defined as the second moment of an orientational probability distribution, leading to a free energy that is not convex within the isotropic-nematic coexistence region, and that goes to infinity if the eigenvalues of the order parameter become non-physical. Computations of the spatial profile of the order parameter are presented for an isotropic-nematic interface in one dimension, a tactoid in two dimensions, and a nematic disclination in two dimensions. We compare our results to those given by the Landau de-Gennes free energy for the same configurations and discuss the advantages of such a model over the latter.
Averaging in cosmology based on Cartan scalars<|sep|>We present a new approach for averaging in general relativity and cosmology. After a short review of the theory originally taken from the equivalence problem, we consider two ways how to deal with averaging based on Cartan scalars. We apply the theory for two different LTB models. In the first one, correlation term behaves as a positive cosmological constant, in the second example leading correlation term behaves like spatial curvature. We also show nontriviality of averaging for linearized monochromatic gravitational wave.
On Co-Maximal Subgroup Graph of a Group<|sep|>The co-maximal subgroup graph $\Gamma(G)$ of a group $G$ is a graph whose vertices are non-trivial proper subgroups of $G$ and two vertices $H$ and $K$ are adjacent if $HK=G$. In this paper, we continue the study of $\Gamma(G)$, especially when $\Gamma(G)$ has isolated vertices. We define a new graph $\Gamma^*(G)$, which is obtained by removing isolated vertices from $\Gamma(G)$. We characterize when $\Gamma^*(G)$ is connected, a complete graph, star graph, has an universal vertex etc. We also find various graph parameters like diameter, girth, bipartiteness etc. in terms of properties of $G$.
Characterization of RF Inhomogeneity via NOON states<|sep|>We describe a method for quantitative characterization of radio-frequency (RF) inhomogeneity using NOON states. NOON states are special multiple-quantum coherences which can be easily prepared in star-topology spin-systems. In this method we exploit the high sensitivity of the NOON states for z-rotations. As a result, Torrey oscillations with NOON states decay much faster than that of single quantum coherences. Therefore the present method requires shorter pulse durations, and enables the study of inhomogeneity at higher RF powers. To model such a inhomogeneity profile, we propose a two-parameter asymmetric Lorentzian function. The experiments are carried out in 1H channels of two different high-resolution NMR probes and the results are compared. Finally, we also extend the NOON state method to characterize the inhomogeneity correlations between two RF channels. We obtain the correlation profile between 1H and 31P channels of an NMR probe by constructing a five-parameter asymmetric-3D Lorentzian model.
High-Q Dual-Band Graphene Absorbers by Selective Excitation of Graphene Plasmon Polaritons: Circuit Model Analysis<|sep|>This article presents the design of two dual-band graphene-based absorbers for terahertz frequencies. The absorbers are composed of two-dimensional (2D)arrays of ribbons and disks printed on a ground plane backed dielectric spacer. The design is based on the excitation of a specific plasmon polariton of the graphene patterned array at each resonance band. An analytical circuit model is used to derive closed-form relations for the geometrical parameters of the absorber and graphene parameters. The graphene patterned array appears as a surface admittance made of an infinite parallel array of series RLC branches. Each branch is equivalent to a graphene plasmon polariton (GPP) providing a distinct resonance mode. The design procedure is based on selectively exciting the first two GPPs. This means that two of the parallel RLC branches are selectively used in the circuit model analysis. The results obtained using the analytical solution are compared with the full-wave simulations in HFSS. The agreement between the results validates the developed design method.
On the propagation of singularities for a class of linearised hybrid inverse problems<|sep|>For a general formulation of linearised hybrid inverse problems in impedance tomography, the qualitative properties of the solutions are analysed. Using an appropriate scalar pseudo-differential formulation, the problems are shown to permit propagating singularities under certain non-elliptic conditions, and the associated directions of propagation are precisely identified relative to the directions in which ellipticity is lost. The same result is found in the setting for the corresponding normal formulation of the scalar pseudo-differential equations. A numerical reconstruction procedure based of the least squares finite element method is derived, and a series of numerical experiments visualise exactly how the loss of ellipticity manifests itself as propagating singularities.
Medium effects on heavy-flavour observables in high-energy nuclear collisions<|sep|>The peculiar role of heavy-flavour observables in relativistic heavy-ion collisions is discussed. Produced in the early stage, $c$ and $b$ quarks cross the hot medium arising from the collision, interacting strongly with the latter, until they hadronize. Depending on the strength of the interaction heavy quarks may or not approach kinetic equilibrium with the plasma, tending in the first case to follow the collective flow of the expanding fireball. The presence of a hot deconfined medium may also affect heavy-quark hadronization, being possible for them to recombine with the surrounding light thermal partons, so that the final heavy-flavour hadrons inherit part of the flow of the medium. Here we show how it is possible to develop a complete transport setup allowing one to describe heavy-flavour production in high-energy nuclear collisions, displaying some major results one can obtain. Finally, the possibility that the formation of a hot deconfined medium even in small systems (high-multiplicity p-Au and d-Au collisions, so far) may affect also heavy-flavour observables is investigated.
Spatially Aware Melanoma Segmentation Using Hybrid Deep Learning Techniques<|sep|>In this paper, we proposed using a hybrid method that utilises deep convolutional and recurrent neural networks for accurate delineation of skin lesion of images supplied with ISBI 2017 lesion segmentation challenge. The proposed method was trained using 1800 images and tested on 150 images from ISBI 2017 challenge.
Unsupervised and interpretable scene discovery with Discrete-Attend-Infer-Repeat<|sep|>In this work we present Discrete Attend Infer Repeat (Discrete-AIR), a Recurrent Auto-Encoder with structured latent distributions containing discrete categorical distributions, continuous attribute distributions, and factorised spatial attention. While inspired by the original AIR model andretaining AIR model's capability in identifying objects in an image, Discrete-AIR provides direct interpretability of the latent codes. We show that for Multi-MNIST and a multiple-objects version of dSprites dataset, the Discrete-AIR model needs just one categorical latent variable, one attribute variable (for Multi-MNIST only), together with spatial attention variables, for efficient inference. We perform analysis to show that the learnt categorical distributions effectively capture the categories of objects in the scene for Multi-MNIST and for Multi-Sprites.
Large-scale Multi-granular Concept Extraction Based on Machine Reading Comprehension<|sep|>The concepts in knowledge graphs (KGs) enable machines to understand natural language, and thus play an indispensable role in many applications. However, existing KGs have the poor coverage of concepts, especially fine-grained concepts. In order to supply existing KGs with more fine-grained and new concepts, we propose a novel concept extraction framework, namely MRC-CE, to extract large-scale multi-granular concepts from the descriptive texts of entities. Specifically, MRC-CE is built with a machine reading comprehension model based on BERT, which can extract more fine-grained concepts with a pointer network. Furthermore, a random forest and rule-based pruning are also adopted to enhance MRC-CE's precision and recall simultaneously. Our experiments evaluated upon multilingual KGs, i.e., English Probase and Chinese CN-DBpedia, justify MRC-CE's superiority over the state-of-the-art extraction models in KG completion. Particularly, after running MRC-CE for each entity in CN-DBpedia, more than 7,053,900 new concepts (instanceOf relations) are supplied into the KG. The code and datasets have been released at https://github.com/fcihraeipnusnacwh/MRC-CE
Hunting for vampires and other unlikely forms of parity violation at the Large Hadron Collider<|sep|>Non-Standard-Model parity violation may be occurring in LHC collisions. Any such violation would go unseen, however, as searches are for it are not currently performed. One barrier to searches for parity violation is the lack of model-independent methods sensitive to all of its forms. We remove this barrier by demonstrating an effective and model-independent way to search for parity-violating physics at the LHC. The method is data-driven and makes no reference to any particular parity-violating model. Instead, it inspects data to construct sensitive parity-odd event variables (using machine learning tools), and uses these variables to test for parity asymmetry in independent data. We demonstrate the efficacy of this method by testing it on data simulated from the Standard Model and from a non-standard parity-violating model. This result enables the possibility of investigating a variety of previously unexplored forms of parity violation in particle physics. Data and software are shared at https://zenodo.org/record/6827724
Detecting local earthquakes via fiber-optic cables in telecommunication conduits under Stanford University campus using deep learning<|sep|>With fiber-optic seismic acquisition development, continuous dense seismic monitoring is becoming increasingly more accessible. Repurposing fiber cables in telecommunication conduits makes it possible to run seismic studies at low cost, even in locations where traditional seismometers are not easily installed, such as in urban areas. However, due to the large volume of continuous streaming data, data collected in such a manner will go to waste unless we significantly automate the processing workflow. We train a convolutional neural network (CNN) for earthquake detection using data acquired over three years by fiber cables in telecommunication conduits under Stanford University campus. We demonstrate that fiber-optic systems can effectively complement sparse seismometer networks to detect local earthquakes. The CNN allows for reliable earthquake detection despite a low signal-to-noise ratio and even detects small-amplitude previously-uncataloged events.
Recombination Lines and Molecular Gas from Hypercompact HII regions in W51 A<|sep|>We present a detailed characterization of the population of compact radio-continuum sources in W51 A using subarcsecond VLA and ALMA observations. We analyzed their 2-cm continuum, the recombination lines (RL's) H77$\alpha$ and H30$\alpha$, and the lines of $\rm H_{2}CO(3_{0,3}-2_{0,2})$, $\rm H_{2}CO(3_{2,1}-2_{2,0})$, and $\rm SO(6_{5}-5_{4})$. We derive diameters for 10/20 sources in the range $D \sim 10^{-3}$ to $\sim 10^{-2}$ pc, thus placing them in the regime of hypercompact HII regions (HC HII's). Their continuum-derived electron densities are in the range $n_{\rm e} \sim 10^4$ to $10^5$ cm$^{-3}$, lower than typically considered for HC HII's. We combined the RL measurements and independently derived $n_{\rm e}$, finding the same range of values but significant offsets for individual measurements between the two methods. We found that most of the sources in our sample are ionized by early B-type stars, and a comparison of $n_{\rm e}$ vs $D$ shows that they follow the inverse relation previously derived for ultracompact (UC) and compact HII's. When determined, the ionized-gas kinematics is always (7/7) indicative of outflow. Similarly, 5 and 3 out of the 8 HC HII's still embedded in a compact core show evidence for expansion and infall motions in the molecular gas, respectively. We hypothesize that there could be two different types of $hypercompact$ ($D< 0.05$ pc) HII regions: those that essentially are smaller, expanding UC HII's; and those that are also $hyperdense$ ($n_{\rm e} > 10^6$ cm$^{-3}$), probably associated with O-type stars in a specific stage of their formation or early life.
Predicting the variance of a measurement with 1/f noise<|sep|>Measurement devices always add noise to the signal of interest and it is necessary to evaluate the variance of the results. This article focuses on stationary random processes whose Power Spectrum Density is a power law of frequency. For flicker noise, behaving as $1/f$ and which is present in many different phenomena, the usual way to compute the variance leads to infinite values. This article proposes an alternative definition of the variance which takes into account the fact that measurement devises need to be calibrated. This new variance, which depends on the calibration duration, the measurement duration and the duration between the calibration and the measurement, allows avoiding infinite values when computing the variance of a measurement.
Detonations and deflagrations in cosmological phase transitions<|sep|>We study the steady state motion of bubble walls in cosmological phase transitions. Taking into account the boundary and continuity conditions for the fluid variables, we calculate numerically the wall velocity as a function of the nucleation temperature, the latent heat, and a friction parameter. We determine regions in the space of these parameters in which detonations and/or deflagrations are allowed. In order to apply the results to a physical case, we calculate these quantities in a specific model, which consists of an extension of the Standard Model with singlet scalar fields. We also obtain analytic approximations for the wall velocity, both in the case of deflagrations and of detonations.
Pix2Point: Learning Outdoor 3D Using Sparse Point Clouds and Optimal Transport<|sep|>Good quality reconstruction and comprehension of a scene rely on 3D estimation methods. The 3D information was usually obtained from images by stereo-photogrammetry, but deep learning has recently provided us with excellent results for monocular depth estimation. Building up a sufficiently large and rich training dataset to achieve these results requires onerous processing. In this paper, we address the problem of learning outdoor 3D point cloud from monocular data using a sparse ground-truth dataset. We propose Pix2Point, a deep learning-based approach for monocular 3D point cloud prediction, able to deal with complete and challenging outdoor scenes. Our method relies on a 2D-3D hybrid neural network architecture, and a supervised end-to-end minimisation of an optimal transport divergence between point clouds. We show that, when trained on sparse point clouds, our simple promising approach achieves a better coverage of 3D outdoor scenes than efficient monocular depth methods.
Scalable, Fast Cloud Computing with Execution Templates<|sep|>Large scale cloud data analytics applications are often CPU bound. Most of these cycles are wasted: benchmarks written in C++ run 10-51 times faster than frameworks such as Naiad and Spark. However, calling faster implementations from those frameworks only sees moderate (3-5x) speedups because their control planes cannot schedule work fast enough. This paper presents execution templates, a control plane abstraction for CPU-bound cloud applications, such as machine learning. Execution templates leverage highly repetitive control flow to cache scheduling decisions as {\it templates}. Rather than reschedule hundreds of thousands of tasks on every loop execution, nodes instantiate these templates. A controller's template specifies the execution across all worker nodes, which it partitions into per-worker templates. To ensure that templates execute correctly, controllers dynamically patch templates to match program control flow. We have implemented execution templates in Nimbus, a C++ cloud computing framework. Running in Nimbus, analytics benchmarks can run 16-43 times faster than in Naiad and Spark. Nimbus's control plane can scale out to run these faster benchmarks on up to 100 nodes (800 cores).
Collectibility for Mixed Quantum States<|sep|>Bounds analogous to entropic uncertainty relations allow one to design practical tests to detect quantum entanglement by a collective measurement performed on several copies of the state analyzed. This approach, initially worked out for pure states only [Phys. Rev. Lett. 107, 150502 (2011)], is extended here for mixed quantum states. We define collectibility for any mixed states of a multipartite system. Deriving bounds for collectibility for positive partially transposed states of given purity provides a new insight into the structure of entangled quantum states. In case of two qubits the application of complementary measurements and coincidence based detections leads to a new test of entanglement of pseudopure states.
Nematic fluctuations and their wave vector in two-dimensional metals<|sep|>We revisit the problem of electrons on a square lattice below half filling close to an Ising-nematic quantum critical point. For Fermi surfaces with sufficiently strong antinodal nesting, the static nematic susceptibility is maximal at the antinodal nesting wave vector within a simple RPA calculation. We present a detailed analysis of the nematic susceptibility within Eliashberg theory and show that the strong interaction between Fermions in the antinodal regions shifts the maximum of the nematic susceptibility to slightly larger wave vectors. The corresponding order is akin to the incommensurate charge-density wave with d-wave form factor found recently in some underdoped cuprate materials. At sufficiently high temperatures around $T/t \sim 0.1$ nematic fluctuations are strongest at zero wave vector.
Operators versus functions: from quantum dynamical semigroups to tomographic semigroups<|sep|>Quantum mechanics can be formulated in terms of phase-space functions, according to Wigner's approach. A generalization of this approach consists in replacing the density operators of the standard formulation with suitable functions, the so-called generalized Wigner functions or (group-covariant) tomograms, obtained by means of group-theoretical methods. A typical problem arising in this context is to express the evolution of a quantum system in terms of tomograms. In the case of a (suitable) open quantum system, the dynamics can be described by means of a quantum dynamical semigroup 'in disguise', namely, by a semigroup of operators acting on tomograms rather than on density operators. We focus on a special class of quantum dynamical semigroups, the twirling semigroups, that have interesting applications, e.g., in quantum information science. The 'disguised counterparts' of the twirling semigroups, i.e., the corresponding semigroups acting on tomograms, form a class of semigroups of operators that we call tomographic semigroups. We show that the twirling semigroups and the tomographic semigroups can be encompassed in a unique theoretical framework, a class of semigroups of operators including also the probability semigroups of classical probability theory, so achieving a deeper insight into both the mathematical and the physical aspects of the problem.
Empirical Analysis of Overfitting and Mode Drop in GAN Training<|sep|>We examine two key questions in GAN training, namely overfitting and mode drop, from an empirical perspective. We show that when stochasticity is removed from the training procedure, GANs can overfit and exhibit almost no mode drop. Our results shed light on important characteristics of the GAN training procedure. They also provide evidence against prevailing intuitions that GANs do not memorize the training set, and that mode dropping is mainly due to properties of the GAN objective rather than how it is optimized during training.
Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order<|sep|>Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a set of downstream NLU tasks.
Conductivities from attractors<|sep|>In the context of applications of the AdS/CFT correspondence to condensed matter physics, we compute conductivities for field theory duals of dyonic planar black holes in 3+1-dimensional Einstein-Maxwell-dilaton theories at zero temperature. We combine the near-horizon data obtained via Sen's entropy function formalism with known expressions for conductivities. In this way we express the conductivities in terms of the extremal black hole charges. We apply our approach to three different examples for dilaton theories for which the background geometry is not known explicitly. For a constant scalar potential, the thermoelectric conductivity explicitly scales as $\alpha_{xy}\sim N^{3/2}$, as expected. For the same model, our approach yields a finite result for the heat conductivity $\kappa/T \propto N^{3/2}$ even for $T \rightarrow 0$.
Coverage and Throughput Analysis with a Non-Uniform Small Cell Deployment<|sep|>Small cell network (SCN) offers, for the first time, a low-cost and scalable mechanism to meet the forecast data-traffic demand. In this paper, we propose a non-uniform SCN deployment scheme. The small cell base stations (BSs) in this scheme will not be utilized in the region within a prescribed distance away from any macrocell BSs, defined as the inner region. Based upon the analytical framework provided in this work, the downlink coverage and single user throughput are precisely characterized. Provided that the inner region size is appropriately chosen, we find that the proposed non-uniform SCN deployment scheme can maintain the same level of cellular coverage performance even with 50% less small cell BSs used than the uniform SCN deployment, which is commonly considered in the literature. Furthermore, both the coverage and the single user throughput performance will significantly benefit from the proposed scheme, if its average small cell density is kept identical to the uniform SCN deployment. This work demonstrates the benefits obtained from a simple non-uniform SCN deployment, thus highlighting the importance of deploying small cells selectively.
A Denoising Autoencoder that Guides Stochastic Search<|sep|>An algorithm is described that adaptively learns a non-linear mutation distribution. It works by training a denoising autoencoder (DA) online at each generation of a genetic algorithm to reconstruct a slowly decaying memory of the best genotypes so far. A compressed hidden layer forces the autoencoder to learn hidden features in the training set that can be used to accelerate search on novel problems with similar structure. Its output neurons define a probability distribution that we sample from to produce offspring solutions. The algorithm outperforms a canonical genetic algorithm on several combinatorial optimisation problems, e.g. multidimensional 0/1 knapsack problem, MAXSAT, HIFF, and on parameter optimisation problems, e.g. Rastrigin and Rosenbrock functions.
Construction of Diffeomorphisms with Prescribed Jacobian Determinant and Curl<|sep|>The variational principle (VP) is designed to generate non-folding grids (diffeomorphisms) with prescribed Jacobian determinant (JD) and curl. Its solution pool of the original VP is based on an additive formulation and, consequently, is not invariant in the diffeomorphic Lie algebra. The original VP works well when the prescribed pair of JD and curl is calculated from a diffeomorphism, but not necessarily when the prescribed JD and curl are not known to come from a diffeomorphism. This issue is referred as the mismatched pair problem. In spite of that, the original VP works effectively in 2D grid generations. To resolve these issues, in this paper, we describe a new version of VP (revised VP), which is based on composition of transformations and, therefore, is invariant in the Lie algebra. The revised VP seems have overcome the inaccuracy of original VP in 3D grid generations. In the following sections, the mathematical derivations are presented. It is shown that the revised VP can calculate the inverse transformation of a known diffeomorphism. Its inverse consistency and transitivity of transformations are also demonstrated numerically. Moreover, a computational strategy is formulated based on the new version of VP to handle the mismatch issue and is demonstrated with preliminary result.
On hyperbolicity violations in cosmological models with vector fields<|sep|>Cosmological models with vector fields received much attention in recent years. Unfortunately, most of them are plagued with severe instabilities or other problems. In particular, it was noted by G. Esposito-Farese, C. Pitrou and J.-Ph. Uzan in arXiv:0912.0481 that the models with a non-linear function of the Maxwellian kinetic term do always imply violations of hyperbolicity somewhere in the phase space. In this work we make this statement more precise in several respects and show that those violations may not be present around spatially homogeneous configurations of the vector field.
Analytical results for the quantum non-Markovianity of spin ensembles undergoing pure dephasing dynamics<|sep|>We study analytically the non-Markovianity of a spin ensemble, with arbitrary number of spins and spin quantum number, undergoing a pure dephasing dynamics. The system is considered as a part of a larger spin ensemble of any geometry with pairwise interactions. We derive exact formulas for the reduced dynamics of the system and for its non-Markovianity as assessed by the witness of Lorenzo \emph{et al.} [Phys.~Rev.~A \textbf{88}, 020102(R) (2013)]. The non-Markovianity is further investigated in the thermodynamic limit when the environment's size goes to infinity. In this limit and for finite-size systems, we find that the Markovian's character of the system's dynamics crucially depends on the range of the interactions. We also show that, when the system and its environment are initially in a separable state, the appearance of non-Markovianity is independent of the entanglement generation between the system and its environment.
Adaptive iterative linearised finite element methods for implicitly constituted incompressible fluid flow problems and its application to Bingham fluids<|sep|>In this work, we introduce an iterative linearised finite element method for the solution of Bingham fluid flow problems. The proposed algorithm has the favourable property that a subsequence of the sequence of iterates generated converges weakly to a solution of the problem. This will be illustrated by two numerical experiments.
Decoupling constant for $\alpha_s$ and the effective gluon-Higgs coupling to three loops in supersymmetric QCD<|sep|>We compute the three-loop QCD corrections to the decoupling constant for $\alpha_s$ which relates the Minimal Supersymmetric Standard Model to Quantum Chromodynamics with five or six active flavours. The new results can be used to study the stability of $\alpha_s$ evaluated at a high scale from the knowledge of its value at $M_Z$. We furthermore derive a low-energy theorem which allows the calculation of the coefficient function of the effective Higgs boson-gluon operator from the decoupling constant. This constitutes the first independent check of the matching coefficient to three loops.
Thermodynamics of Non-Commutative Scalar-Tensor-Vector Gravity Black holes<|sep|>In this paper, we analyze the thermodynamic stability of Schwarzschild Modified Gravity (MOG) black holes in a non-commutative framework. We show that, unlike a commutative MOG black hole, in the coherent state picture of non-commutativity MOG black holes are thermodynamically stable. At the final stage of evaporation, a stable remnant with zero temperature and finite entropy is left in this noncommutative framework. Also, we consider Parikh-Wilczek tunneling mechanism of massive particles from non-commutative MOG black holes and demonstrate that information leaks out of non-commutative MOG black holes in the form of some non-thermal correlations.
Competing Orders in a Dipolar Bose-Fermi Mixture on a Square Optical Lattice: Mean-Field Perspective<|sep|>We consider a mixture of a two-component Fermi gas and a single-component dipolar Bose gas in a square optical lattice and reduce it into an effective Fermi system where the Fermi-Fermi interaction includes the attractive interaction induced by the phonons of a uniform dipolar Bose-Einstein condensate. Focusing on this effective Fermi system in the parameter regime that preserves the symmetry of $D_4$, the point group of a square, we explore, within the Hartree-Fock-Bogoliubov mean-field theory, the phase competition among density wave orderings and superfluid pairings. We construct the matrix representation of the linearized gap equation in the irreducible representations of $D_4$. We show that in the weak coupling regime, each matrix element, which is a four-dimensional (4D) integral in momentum space, can be put in a separable form involving a 1D integral, which is only a function of temperature and the chemical potential, and a pairing-specific "effective" interaction, which is an analytical function of the parameters that characterize the Fermi-Fermi interactions in our system. We analyze the critical temperatures of various competing orders as functions of different system parameters in both the absence and presence of the dipolar interaction. We find that close to half filling, the d_{x^{2}-y^{2}}-wave pairing with a critical temperature in the order of a fraction of Fermi energy (at half filling) may dominate all other phases, and at a higher filling factor, the p-wave pairing with a critical temperature in the order of a hundredth of Fermi energy may emerge as a winner. We find that tuning a dipolar interaction can dramatically enhance the pairings with $d_{xy}$- and g-wave symmetries but not enough for them to dominate other competing phases.
A re--analysis of the iron line in the XMM-Newton data from the low/hard state in GX339--4<|sep|>The detection of an extremely broad iron line in XMM-Newton MOS data from the low/hard state of the black hole binary GX339-4 is the only piece of evidence which unambiguously conflicts with the otherwise extremely successful truncated disc interpretation of this state. However, it also conflicts with some aspect of observational data for all other alternative geometries of the low/hard state, including jet models, making it very difficult to understand. We re-analyse these data and show that they are strongly affected by pileup even with extensive centroid removal as the source is ~200x brighter than the recommended maximum countrate. Instead, we extract the simultaneous PN timing mode data which should not be affected by pileup. These show a line which is significantly narrower than in the MOS data. Thus these data are easily consistent with a truncated disc, and indeed, strongly support such an interpretation.
Sim-to-real reinforcement learning applied to end-to-end vehicle control<|sep|>In this work, we study vision-based end-to-end reinforcement learning on vehicle control problems, such as lane following and collision avoidance. Our controller policy is able to control a small-scale robot to follow the right-hand lane of a real two-lane road, while its training was solely carried out in a simulation. Our model, realized by a simple, convolutional network, only relies on images of a forward-facing monocular camera and generates continuous actions that directly control the vehicle. To train this policy we used Proximal Policy Optimization, and to achieve the generalization capability required for real performance we used domain randomization. We carried out thorough analysis of the trained policy, by measuring multiple performance metrics and comparing these to baselines that rely on other methods. To assess the quality of the simulation-to-reality transfer learning process and the performance of the controller in the real world, we measured simple metrics on a real track and compared these with results from a matching simulation. Further analysis was carried out by visualizing salient object maps.
Multiquark interactions and heavy hybrid stars<|sep|>We introduce a two flavor Nambu--Jona-Lasinio (NJL) model with 8-quark interactions in the scalar and the vector channel. With the lower density region described by the density-dependent relativistic mean field model we construct a hybrid equation of state. We especially focus on the 4-quark vector couplings and the 8-quark vector NJL couplings and allocate a region in this parameter subspace where hybrid stars with masses larger than $2M_\odot$ exist.
Comprehensive Analysis of Coronal Mass Ejection Mass and Energy Properties Over a Full Solar Cycle<|sep|>The LASCO coronagraphs, in continuous operation since 1995, have observed the evolution of the solar corona and coronal mass ejections (CMEs) over a full solar cycle with high quality images and regular cadence. This is the first time that such a dataset becomes available and constitutes a unique resource for the study of CMEs. In this paper, we present a comprehensive investigation of the solar cycle dependence on the CME mass and energy over a full solar cycle (1996-2009) including the first in-depth discussion of the mass and energy analysis methods and their associated errors. Our analysis provides several results worthy of further studies. It demonstrates the possible existence of two event classes; 'normal' CMEs reaching constant mass for $>10$ R$_{\sun}$ and 'pseudo' CMEs which disappear in the C3 FOV. It shows that the mass and energy properties of CME reach constant levels, and therefore should be measured, only above $\sim 10 R_\sun$. The mass density ($g/R_\sun^2$) of CMEs varies relatively little ($<$ order of magnitude) suggesting that the majority of the mass originates from a small range in coronal heights. We find a sudden reduction in the CME mass in mid-2003 which may be related to a change in the electron content of the large scale corona and we uncover the presence of a six-month periodicity in the ejected mass from 2003 onwards.
The Future of Artificial Intelligence and its Social, Economic and Ethical Consequences<|sep|>Recent development in AI has enabled the expansion of its application to multiple domains. From medical treatment, gaming, manufacturing to daily business processes. A huge amount of money has been poured into AI research due to its exciting discoveries. Technology giants like Google, Facebook, Amazon, and Baidu are the driving forces in the field today. But the rapid growth and excitement that the technology offers obscure us from looking at the impact it brings on our society. This short paper gives a brief history of AI and summarizes various social, economic and ethical issues that are impacting our society today. We hope that this work will provide a useful starting point and perhaps reference for newcomers and stakeholders of the field.
Online network coding for optimal throughput and delay -- the three-receiver case<|sep|>For a packet erasure broadcast channel with three receivers, we propose a new coding algorithm that makes use of feedback to dynamically adapt the code. Our algorithm is throughput optimal, and we conjecture that it also achieves an asymptotically optimal average decoding delay at the receivers. We consider heavy traffic asymptotics, where the load factor \rho approaches 1 from below with either the arrival rate (\lambda) or the channel parameter (\mu) being fixed at a number less than 1. We verify through simulations that our algorithm achieves an asymptotically optimal decoding delay of O(1/(1-\rho)).
Quiver varieties and quantum Knizhnik--Zamolodchikov equation<|sep|>We show how equivariant volumes of tensor product quiver varieties of type A are given by matrix elements of vertex operators of centrally extended doubles of Yangians, and how they satisfy in some cases the rational, level 1, quantum Knizhnik--Zamolodchikov equation.
Double scaling limits of random matrices and minimal (2m,1) models: the merging of two cuts in a degenerate case<|sep|>In this article, we show that the double scaling limit correlation functions of a random matrix model when two cuts merge with degeneracy $2m$ (i.e. when $y\sim x^{2m}$ for arbitrary values of the integer $m$) are the same as the determinantal formulae defined by conformal $(2m,1)$ models. Our approach follows the one developed by Berg\`{e}re and Eynard in \cite{BergereEynard} and uses a Lax pair representation of the conformal $(2m,1)$ models (giving Painlev\'e II integrable hierarchy) as suggested by Bleher and Eynard in \cite{BleherEynard}. In particular we define Baker-Akhiezer functions associated to the Lax pair to construct a kernel which is then used to compute determinantal formulae giving the correlation functions of the double scaling limit of a matrix model near the merging of two cuts.
Topological background discrimination in the PandaX-III neutrinoless double beta decay experiment<|sep|>The PandaX-III experiment plans to search for neutrinoless double beta decay (0$\nu\beta\beta$) of $^{136}$Xe in the China JinPing underground Laboratory (CJPL). The experiment will use a high pressure gaseous Time Projection Chamber (TPC) to register both the energy and the electron track topology of an event. This article is devoted to the software side of the experiment. As software tool we use REST, a framework developed for the reconstruction and simulation of TPC-based detector systems. We study the potential for background reduction by introducing appropiate parameters based on the properties of 0$\nu\beta\beta$ events. We exploit for the first time not only the energy density of the electron track-ends, but also the electron scattering angles produced by an electron near the end of its trajectory. To implement this, we have added new algorithms for detector signal and track processing inside REST. Their assessment shows that background can be reduced by about 7 orders of magnitude while keeping 0$\nu\beta\beta$ efficiency above 20% for the PandaX-III baseline readout scheme, a 2-dimensional 3mm-pitch stripped readout. More generally, we use the potential of REST to handle 2D/3D data to assess the impact on signal-to-background significance at different detector granularities, and to validate the PandaX-III baseline choice. Finally, we demonstrate the potential to discriminate surface background events generated at the readout plane in the absence of $t_o$, by making use of event parameters related with the diffusion of electrons.
Recent Advances in Transient Imaging: A Computer Graphics and Vision Perspective<|sep|>Transient imaging has recently made a huge impact in the computer graphics and computer vision fields. By capturing, reconstructing, or simulating light transport at extreme temporal resolutions, researchers have proposed novel techniques to show movies of light in motion, see around corners, detect objects in highly-scattering media, or infer material properties from a distance, to name a few. The key idea is to leverage the wealth of information in the temporal domain at the pico or nanosecond resolution, information usually lost during the capture-time temporal integration. This paper presents recent advances in this field of transient imaging from a graphics and vision perspective, including capture techniques, analysis, applications and simulation.
A New Fuzzy $H_{\infty}$ Filter Design for Nonlinear Time-Delay Systems with Mismatched Premise Membership Functions<|sep|>This paper is concerned with the fuzzy $H_{\infty}$ filter design issue for nonlinear systems with time-varying delay. To overcome the shortcomings of the conventional methods with matched preconditions, the fuzzy $H_{\infty}$ filter to be designed and the T-S fuzzy model are assumed to have different premise membership functions and number of rules, thus, greater design flexibility and robustness to uncertainty can be achieved. However, such design will also make the derived results conservative, to relax the result, a novel integral inequality which is tighter than the traditional inequalities derived from the Leibniz-Newton formula is applied, besides, a fuzzy Lypunov function and the information of the membership functions are also introduced. All the design methods are presented in LMI-based conditions. Finally, two numerical examples are given to prove the effectiveness and superiority of the proposed approach.
The Metallicity Dependence of the Minimum Mass for Core-Collapse Supernovae<|sep|>Understanding the progenitors of core collapse supernovae and their population statistics is a key ingredient for many current studies in astronomy but as yet this remains elusive. Using the MESA stellar evolution code we study the dependence of the lower mass limit for making core collapse supernovae (SNe) as function of initial stellar metallicity. We find that this mass limit is smallest at approximately [Z] = -2 with a value of ~ 8.3 Msun. At [Z] = 0 the limit is ~ 9.5 Msun and continues to rise with higher metallicity. As a consequence, for a fixed initial mass function the supernova rate may be 20% to 25% higher at [Z] = -2. This affects the association of observed SN rates as a probe for the cosmological star formation rate, rate predictions for supernova surveys, and population synthesis studies.
The N2HDM under Theoretical and Experimental Scrutiny<|sep|>The N2HDM is based on the CP-conserving 2HDM extended by a real scalar singlet field. Its enlarged parameter space and its fewer symmetry conditions as compared to supersymmetric models allow for an interesting phenomenology compatible with current experimental constraints, while adding to the 2HDM sector the possibility of Higgs-to-Higgs decays with three different Higgs bosons. In this paper the N2HDM is subjected to detailed scrutiny. Regarding the theoretical constraints we implement tests of tree-level perturbativity and vacuum stability. Moreover, we present, for the first time, a thorough analysis of the global minimum of the N2HDM. The model and the theoretical constraints have been implemented in ScannerS, and we provide N2HDECAY, a code based on HDECAY, for the computation of the N2HDM branching ratios and total widths including the state-of-the-art higher order QCD corrections and off-shell decays. We then perform an extensive parameter scan in the N2HDM parameter space, with all theoretical and experimental constraints applied, and analyse its allowed regions. We find that large singlet admixtures are still compatible with the Higgs data and investigate which observables will allow to restrict the singlet nature most effectively in the next runs of the LHC. Similarly to the 2HDM, the N2HDM exhibits a wrong-sign parameter regime, which will be constrained by future Higgs precision measurements.
Topology of Entanglement Evolution of Two Qubits<|sep|>The dynamics of a two-qubit system is considered with the aim of a general categorization of the different ways in which entanglement can disappear in the course of the evolution, e.g., entanglement sudden death. The dynamics is described by the function ~n(t), where ~n is the 15-dimensional polarization vector. This representation is particularly useful because the components of ~n are direct physical observables, there is a meaningful notion of orthogonality, and the concurrence C can be computed for any point in the space. We analyze the topology of the space S of separable states (those having C = 0), and the often lower-dimensional linear dynamical subspace D that is characteristic of a specific physical model. This allows us to give a rigorous characterization of the four possible kinds of entanglement evolution. Which evolution is realized depends on the dimensionality of D and of D \cap S, the position of the asymptotic point of the evolution, and whether or not the evolution is "distance-Markovian", a notion we define. We give several examples to illustrate the general principles, and to give a method to compute critical points. We construct a model that shows all four behaviors.
Information Systems Playground - The Target Infrastructure, Scaling Astro-WISE into the Petabyte range<|sep|>The Target infrastructure has been specially built as a storage and compute infrastructure for the information systems derived from Astro-WISE. This infrastructure will be used by several applications that collaborate in the area of information systems within the Target project. It currently consists of 10 PB of storage and thousands of computational cores. The infrastructure has been constructed based on the requirements of the applications. The storage is controlled by the Global Parallel File System of IBM. This file system takes care of the required flexibility by combining storage hardware with different characteristics into a single file system. It is also very scalable, which allows the system to be extended into the future, while replacing old hardware with new technology.
Transition from a phase-segregated state to single-phase incommensurate sodium ordering in Na_xCoO_2 with x \approx 0.53<|sep|>Synchrotron X-ray diffraction investigations of two single crystals of Na_xCoO_2 from different batches with composition x = 0.525-0.530 reveal homogeneous incommensurate sodium ordering with propagation vector (0.53 0.53 0) at room-temperature. The incommensurate (qq0) superstructure exists between 220 K and 430 K. The value of q varies between q = 0.514 and 0.529, showing a broad plateau at the latter value between 260 K and 360 K. On cooling, unusual reversible phase segregation into two volume fractions is observed. Below 220 K, one volume fraction shows the well-known commensurate orthorhombic x = 0.50 superstructure, while a second volume fraction with x = 0.55 exhibits another commensurate superstructure, presumably with a 6a x 6a x c hexagonal supercell. We argue that the commensurate-to-incommensurate transition is an intrinsic feature of samples with Na concentrations x = 0.5 + d with d ~ 0.03.
KPZ equation with short-range correlated noise: emergent symmetries and non-universal observables<|sep|>We investigate the stationary-state fluctuations of a growing one-dimensional interface described by the KPZ dynamics with a noise featuring smooth spatial correlations of characteristic range $\xi$. We employ Non-perturbative Functional Renormalization Group methods in order to resolve the properties of the system at all scales. We show that the physics of the standard (uncorrelated) KPZ equation emerges on large scales independently of $\xi$. Moreover, the Renormalization Group flow is followed from the initial condition to the fixed point, that is from the microscopic dynamics to the large-distance properties. This provides access to the small-scale features (and their dependence on the details of the noise correlations) as well as to the universal large-scale physics. In particular, we compute the kinetic energy spectrum of the stationary state as well as its non-universal amplitude. The latter is experimentally accessible by measurements at large scales and retains a signature of the microscopic noise correlations. Our results are compared to previous analytical and numerical results from independent approaches. They are in agreement with direct numerical simulations for the kinetic energy spectrum as well as with the prediction, obtained with the replica trick by Gaussian variational method, of a crossover in $\xi$ of the non-universal amplitude of this spectrum.
PlaneSDF-based Change Detection for Long-term Dense Mapping<|sep|>The ability to process environment maps across multiple sessions is critical for robots operating over extended periods of time. Specifically, it is desirable for autonomous agents to detect changes amongst maps of different sessions so as to gain a conflict-free understanding of the current environment. In this paper, we look into the problem of change detection based on a novel map representation, dubbed Plane Signed Distance Fields (PlaneSDF), where dense maps are represented as a collection of planes and their associated geometric components in SDF volumes. Given point clouds of the source and target scenes, we propose a three-step PlaneSDF-based change detection approach: (1) PlaneSDF volumes are instantiated within each scene and registered across scenes using plane poses; 2D height maps and object maps are extracted per volume via height projection and connected component analysis. (2) Height maps are compared and intersected with the object map to produce a 2D change location mask for changed object candidates in the source scene. (3) 3D geometric validation is performed using SDF-derived features per object candidate for change mask refinement. We evaluate our approach on both synthetic and real-world datasets and demonstrate its effectiveness via the task of changed object detection. Supplementary video: https://youtu.be/oh-MQPWTwZI
A Methodology for Player Modeling based on Machine Learning<|sep|>AI is gradually receiving more attention as a fundamental feature to increase the immersion in digital games. Among the several AI approaches, player modeling is becoming an important one. The main idea is to understand and model the player characteristics and behaviors in order to develop a better AI. In this work, we discuss several aspects of this new field. We proposed a taxonomy to organize the area, discussing several facets of this topic, ranging from implementation decisions up to what a model attempts to describe. We then classify, in our taxonomy, some of the most important works in this field. We also presented a generic approach to deal with player modeling using ML, and we instantiated this approach to model players' preferences in the game Civilization IV. The instantiation of this approach has several steps. We first discuss a generic representation, regardless of what is being modeled, and evaluate it performing experiments with the strategy game Civilization IV. Continuing the instantiation of the proposed approach we evaluated the applicability of using game score information to distinguish different preferences. We presented a characterization of virtual agents in the game, comparing their behavior with their stated preferences. Once we have characterized these agents, we were able to observe that different preferences generate different behaviors, measured by several game indicators. We then tackled the preference modeling problem as a binary classification task, with a supervised learning approach. We compared four different methods, based on different paradigms (SVM, AdaBoost, NaiveBayes and JRip), evaluating them on a set of matches played by different virtual agents. We conclude our work using the learned models to infer human players' preferences. Using some of the evaluated classifiers we obtained accuracies over 60% for most of the inferred preferences.
Fast Differential Emission Measure Inversion of Solar Coronal Data<|sep|>We present a fast method for reconstructing Differential Emission Measures (DEMs) using solar coronal data. On average, the method computes over 1000 DEMs per second for a sample active region observed by the Atmospheric Imaging Assembly (AIA) on the Solar Dynamics Observatory (SDO), and achieves reduced chi-squared of order unity with no negative emission in all but a few test cases. The high performance of this method is especially relevant in the context of AIA, which images of order one million solar pixels per second. This paper describes the method, analyzes its fidelity, compares its performance and results with other DEM methods, and applies it to an active region and loop observed by AIA and by the Extreme-ultraviolet Imaging Spectrometer (EIS) on Hinode.
Phase-type mixture-of-experts regression for loss severities<|sep|>The task of modeling claim severities is addressed when data is not consistent with the classical regression assumptions. This framework is common in several lines of business within insurance and reinsurance, where catastrophic losses or heterogeneous sub-populations result in data difficult to model. Their correct analysis is required for pricing insurance products, and some of the most prevalent recent specifications in this direction are mixture-of-experts models. This paper proposes a regression model that generalizes the latter approach to the phase-type distribution setting. More specifically, the concept of mixing is extended to the case where an entire Markov jump process is unobserved and where states can communicate with each other. The covariates then act on the initial probabilities of such underlying chain, which play the role of expert weights. The basic properties of such a model are computed in terms of matrix functionals, and denseness properties are derived, demonstrating their flexibility. An effective estimation procedure is proposed, based on the EM algorithm and multinomial logistic regression, and subsequently illustrated using simulated and real-world datasets. The increased flexibility of the proposed models does not come at a high computational cost, and the motivation and interpretation are equally transparent to simpler MoE models.
Robustness Verification for Transformers<|sep|>Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the first robustness verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis.
The Abelian Higgs model under a gauge invariant looking glass: exploiting new Ward identities for gauge invariant operators and the Equivalence Theorem<|sep|>The content of two additional Ward identities exhibited by the $U(1)$ Higgs model is exploited. These novel Ward identities can be derived only when a pair of local composite operators providing a gauge invariant setup for the Higgs particle and the massive vector boson is introduced in the theory from the beginning. Among the results obtained from the above mentioned Ward identities, we underline a new exact relationship between the stationary condition for the vacuum energy, the vanishing of the tadpoles and the vacuum expectation value of the gauge invariant scalar operator. We also present a characterization of the two-point correlation function of the composite operator corresponding to the vector boson in terms of the two-point function of the elementary gauge fields. Finally, a discussion on the connection between the cartesian and the polar parametrization of the complex scalar field is presented in the light of the Equivalence Theorem. The latter can in the current case be understood in the language of a constrained cohomology, which also allows to rewrite the action in terms of the aforementioned gauge invariant operators. We also comment on the diminished role of the global $U(1)$ symmetry and its breaking.
Real-Time Steering of Curved Sound Beams in a Feedback-based Topological Acoustic Metamaterial<|sep|>We present the concept of a feedback-based topological acoustic metamaterial as a tool for realizing autonomous and active guiding of sound beams along arbitrary curved paths in free two-dimensional space. The metamaterial building blocks are acoustic transducers, embedded in a slab waveguide. The transducers generate a desired dispersion profile in closed-loop by processing real-time pressure field measurements through preprogrammed controllers. In particular, the metamaterial can be programmed to exhibit analogies of quantum topological wave phenomena, which enables unconventional and exceptionally robust sound beam guiding. As an example, we realize the quantum valley Hall effect by creating, using a collocated pressure feedback, an alternating acoustic impedance pattern across the waveguide. The pattern is traversed by artificial trajectories of different shapes, which are reconfigurable in real-time. Due to topological protection, the sound waves between the plates remain localized on the trajectories, and do not back-scatter by the sharp corners or imperfections in the design. The feedback-based design can be used to realize arbitrary physical interactions in the metamaterial, including non-local, nonlinear, time-dependent, or non-reciprocal couplings, paving the way to new unconventional acoustic wave guiding on the same reprogrammable platform. We then present a non-collocated control algorithm, which mimics another quantum effect, rendering the sound beams uni-directional.
Structure and Recognition of 3,4-leaf Powers of Galled Phylogenetic Networks in Polynomial Time<|sep|>A graph is a $k$-leaf power of a tree $T$ if its vertices are leaves of $T$ and two vertices are adjacent in $T$ if and only if their distance in $T$ is at most $k$. Then $T$ is a $k$-leaf root of $G$. This notion was introduced by Nishimura, Ragde, and Thilikos [2002] motivated by the search for underlying phylogenetic trees. We study here an extension of the $k$-leaf power graph recognition problem. This extension is motivated by a new biological question for the evaluation of the latteral gene transfer on a population of viruses. We allow the host graph to slightly differs from a tree and allow some cycles. In fact we study phylogenetic galled networks in which cycles are pairwise vertex disjoint. We show some structural results and propose polynomial algorithms for the cases $k=3$ and $k=4$. As a consequence, squares of galled networks can also be recognized in polynomial time.
Robust Directional Modulation Design for Secrecy Rate Maximization in Multi-User Networks<|sep|>In this paper, based on directional modulation (DM), robust beamforming matrix design for sum secrecy rate maximization is investigated in multi-user systems. The base station (BS) is assumed to have the imperfect knowledge of the direction angle toward each eavesdropper, with the estimation error following the Von Mises distribution. To this end, a Von Mises distribution-Sum Secrecy Rate Maximization (VMD-SSRM) method is proposed to maximize the sum secrecy rate by employing semi-definite relaxation and first-order approximation based on Taylor expansion to solve the optimization problem. Then in order to optimize the sum secrecy rate in the case of the worst estimation error of direction angle toward each eavesdropper, we propose a maximum angle estimation error-SSRM (MAEE-SSRM) method. The optimization problem is constructed based on the upper and lower bounds of the estimated eavesdropping channel related coefficient and then solved by the change of the variable method. Simulation results show that our two proposed methods have better sum secrecy rate than zero-forcing (ZF) method and signal-to-leakage-and-noise ratio (SLNR) method. Furthermore, the sum secrecy rate performance of our VMD-SSRM method is better than that of our MAEE-SSRM method.
Eye-Tracking-Based Design of Mixed Reality Learning Environments in STEM<|sep|>With the advent of commercially available Mixed-Reality(MR)-headsets in recent years MR-assisted learning started to play a vital role in educational research, especially related to STEM (science, technology, engineering and mathematics) education. Along with these developments it seems viable to further frameworks and structured design processes for MR-based learning environments. Instead of a widely applicable framework for designing educational MR applications, we here consider the case of virtually enhancing physical hands-on experiments in STEM, where students are given a certain problem to solve, and how to design these. For this focused realm, we suggest an empirically driven problemand user-centred design process for MR applications to get novices to act more like experts and exemplify it for a specific experiment and problem set containing a non-trivial electric circuit with capacitors and coils.
Robustness of quantum randomness expansion protocols in the presence of noise<|sep|>In this paper we investigate properties of several randomness generation protocols in the device independent framework. Using Bell-type inequalities it is possible to certify that the numbers generated by an untrusted device are indeed random. We present a selection of certificates which guarantee two bits of randomness for each run of the experiment in the noiseless case and require the parties to share a maximally entangled state. To compare them we study their efficiency in the presence of white noise. We find that for different amounts of noise different operators are optimal for certifying most randomness. Therefore the vendor of the device should use different protocols depending on the amount of noise expected to occur. Another of our results that we find particularly interesting is that using a single Bell operator as a figure of merit is rarely optimal.
The binary Be star $\delta$ Scorpii at high spectral and spatial resolution : II The circumstellar disk evolution after the periastron<|sep|>Classical Be stars are hot non-supergiant stars surrounded by a gaseous circumstellar disk that is responsible for the observed infrared (IR) excess and emission lines. The influence of binarity on these phenomena remains controversial. We followed the evolution of the environment surrounding the binary Be star $\delta$ Scorpii one year before and one year after the 2011 periastron to check for any evidence of a strong interaction between its companion and the primary circumstellar disk. We used the VLTI/AMBER spectro-interferometric instrument operating in the K band in high (12000) spectral resolution to obtain information on both the disk geometry and kinematics. Observations were carried out in two emission lines: Br$\gamma$ (2.172\,$\mu$m) and $\ion{He}{i}$ (2.056\,$\mu$m). We detected some important changes in $\delta$ Scorpii's circumstellar disk geometry between the first observation made in April 2010 and the new observation made in June 2012. During the last two years the disk has grown at a mean velocity of 0.2\,km\,s$^{-1}$. This is compatible with the expansion velocity previously found during the 2001-2007 period. The disk was also found to be asymmetric at both epochs, but with a different morphology in 2010 and 2012. Considering the available spectroscopic data showing that the main changes in the emission-line profiles occurred quickly during the periastron, it is probable that the differences between the 2010 and 2012 disk geometry seen in our interferometric data stem from a disk perturbation caused by the companion tidal effects. However, taking into account that no significant changes have occurred in the disk since the end of the 2011 observing season, it is difficult to understand how this induced inhomogeneity has been "frozen" in the disk for such a long period.
Scanning-gate-induced effects and spatial mapping of a cavity<|sep|>Tailored electrostatic potentials are the foundation of scanning gate microscopy. We present several aspects of the tip-induced potential on the two-dimensional electron gas. First, we give methods on how to estimate the size of the tip-induced potential. Then, a ballistic cavity is formed and studied as a function of the bias-voltage of the metallic top gates and probed with the tip-induced potential. It is shown how the potential of the cavity changes by tuning the system to a regime where conductance quantization in the constrictions formed by the tip and the top gates occurs. This conductance quantization leads to a unprecedented rich fringe pattern over the entire structure. Finally, the effect of electrostatic screening of the metallic top gates is discussed.
Dynamics of feed forward induced interference training<|sep|>Preceptron model updating with back propagation has become the routine of deep learning. Continuous feed forward procedure is required in order for backward propagate to function properly. Doubting the underlying physical interpretation on transformer based models such as GPT brought about by the routine explaination, a new method of training is proposed in order to keep self-consistency of the physics. By treating the GPT model as a space-time diagram, and then trace the worldlines of signals, identifing the possible paths of signals in order fot a self-attention event to occure. With a slight modification, self-attention can be viewed as an ising model interaction, which enables the goal to be designed as energy of system. Target is treated as an external magnetic field inducing signals modeled as magnetic dipoles. A probability network is designed to pilot input signals travelling for different durations through different routes. A rule of updating the probabilities is designed in order to form constructive interference at target locations so that instantaneous energy can be maximised. Experiment was conducted on a 4-class classification problem extracted from MNIST. The results exhibit interesting but expected behavours, which do not exist in a bp updated network, but more like learning in a real human, especially in the few-shot scenario.
Feynman graphs and renormalization in quantum diffusion<|sep|>We review our proof that in a scaling limit, the time evolution of a quantum particle in a static random environment leads to a diffusion equation. In particular, we discuss the role of Feynman graph expansions and of renormalization.
X-ray spectroscopy of the z=6.4 quasar J1148+5251<|sep|>We present the 78-ks Chandra observations of the $z=6.4$ quasar SDSS J1148+5251. The source is clearly detected in the energy range 0.3-7 keV with 42 counts (with a significance $\gtrsim9\sigma$). The X-ray spectrum is best-fitted by a power-law with photon index $\Gamma=1.9$ absorbed by a gas column density of $\rm N_{\rm H}=2.0^{+2.0}_{-1.5}\times10^{23}\,\rm cm^{-2}$. We measure an intrinsic luminosity at 2-10 keV and 10-40 keV equal to $\sim 1.5\times 10^{45}~\rm erg~s^{-1}$, comparable with luminous local and intermediate-redshift quasar properties. Moreover, the X-ray to optical power-law slope value ($\alpha_{\rm OX}=-1.76\pm 0.14$) of J1148 is consistent with the one found in quasars with similar rest-frame 2500 \AA ~luminosity ($L_{\rm 2500}\sim 10^{32}~\rm erg~s^{-1}$\AA$^{-1}$). Then we use Chandra data to test a physically motivated model that computes the intrinsic X-ray flux emitted by a quasar starting from the properties of the powering black hole and assuming that X-ray emission is attenuated by intervening, metal-rich ($Z\geq \rm Z_{\odot}$) molecular clouds distributed on $\sim$kpc scales in the host galaxy. Our analysis favors a black hole mass $M_{\rm BH} \sim 3\times 10^9 \rm M_\odot$ and a molecular hydrogen mass $M_{\rm H_2}\sim 2\times 10^{10} \rm M_\odot$, in good agreement with estimates obtained from previous studies. We finally discuss strengths and limits of our analysis.
Small-scale resolving simulations of the turbulent mixing in confined planar jets using one-dimensional turbulence<|sep|>Small-scale effects of turbulent mixing are numerically investigated by applying the map-based, stochastic, one-dimensional turbulence (ODT) model to confined planar jets. The model validation is carried out for the momentum transport by comparing ODT results to available reference data for the bulk Reynolds numbers $Re=20\,000$ and $40\,000$. Various pointwise statistical quantities are computed and compared to the available reference data. We show that these quantities can be captured well, or at least to a reasonable extent, by the stand-alone model formulation and for fixed model parameters. Only the root-mean-square velocity fluctuations remain systematically underestimated in the ODT results (by an approximate factor of $1.5$). Afterwards, the turbulent transport of a passive scalar is addressed for the Schmidt numbers $Sc=1$ and $1250$. For the high Schmidt number and in contrast to the velocity fluctuations, it is shown that the scalar fluctuation variance is up to ten times larger in the ODT simulations resolving the Batchelor scale. The fluctuation variance is notably smaller for the lower Schmidt number, but exhibits better agreement with the references at a nominally higher Schmidt number. We suggest that this is due to implicit filtering in the references, which barely resolve the Kolmogorov scale. ODT turbulence spectra support this interpretation since a Batchelor-like scalar turbulence spectrum is only observed for the higher Schmidt number. With the aid of these spectra and the fluctuation statistics we conclude that implicit filtering has a similar effect as a reduction of the Schmidt number.
Topological Data Analysis: Concepts, Computation, and Applications in Chemical Engineering<|sep|>A primary hypothesis that drives scientific and engineering studies is that data has structure. The dominant paradigms for describing such structure are statistics (e.g., moments, correlation functions) and signal processing (e.g., convolutional neural nets, Fourier series). Topological Data Analysis (TDA) is a field of mathematics that analyzes data from a fundamentally different perspective. TDA represents datasets as geometric objects and provides dimensionality reduction techniques that project such objects onto low-dimensional spaces that are composed of elementary geometric objects. Key property of these elementary objects (also known as topological features) are that they persist at different scales and that they are stable under perturbations (e.g., noise, stretching, twisting, and bending). In this work, we review key mathematical concepts and methods of TDA and present different applications in chemical engineering.
The Casimir-Polder effect for a stack of conductive planes<|sep|>The Casimir-Polder interaction between an atom and a multilayered system composed of infinitely thin planes is considered using the zeta-function regularization approach with summation of the zero-point energies. As a prototype material, each plane is represented by a graphene sheet whose optical response is described by a constant conductivity or Drude-Lorentz model conductivity. Asymptotic expressions for various separations are derived and compared to numerical calculations. We distinguish between large atom/plane limit, where retardation effects are prominent, and small atom/plane limit, where the typical van der Waals coefficient is found to be dependent on the number of graphenes and characteristic distances. The calculated energies for different atoms and graphene conductivity models brings forward the basic science of the Casimir-Polder effect and suggests ways to manipulate this interaction experimentally.
Neural Network-based Power Flow Model<|sep|>Power flow analysis is used to evaluate the flow of electricity in the power system network. Power flow calculation is used to determine the steady-state variables of the system, such as the voltage magnitude/phase angle of each bus and the active/reactive power flow on each branch. The DC power flow model is a popular linear power flow model that is widely used in the power industry. Although it is fast and robust, it may lead to inaccurate line flow results for some transmission lines. Since renewable energy sources such as solar farms or offshore wind farms are usually located far away from the main grid, accurate line flow results on these critical lines are essential for power flow analysis due to the unpredictable nature of renewable energy. Data-driven methods can be used to partially address these inaccuracies by taking advantage of historical grid profiles. In this paper, a neural network (NN) model is trained to predict power flow results using historical power system data. Although the training process may take time, once trained, it is very fast to estimate line flows. A comprehensive performance analysis between the proposed NN-based power flow model and the traditional DC power flow model is conducted. It can be concluded that the proposed NN-based power flow model can find solutions quickly and more accurately than DC power flow model.
Nonlinear electrochemical impedance spectroscopy for lithium-ion battery model parameterization<|sep|>In this work we analyse the local nonlinear electrochemical impedance spectroscopy (NLEIS) response of a lithium-ion battery and estimate model parameters from measured NLEIS data. The analysis assumes a single-particle model including nonlinear diffusion of lithium within the electrode particles and asymmetric charge transfer kinetics at their surface. Based on this model and assuming a moderately-small excitation amplitude, we systematically derive analytical formulae for the impedances up to the second harmonic response, allowing the meaningful interpretation of each contribution in terms of physical processes and nonlinearities in the model. The implications of this for parameterization are explored, including structural identifiability analysis and parameter estimation using maximum likelihood, with both synthetic and experimentally measured impedance data. Accurate fits to impedance data are possible, however inconsistencies in the fitted diffusion timescales suggest that a nonlinear diffusion model may not be appropriate for the cells considered. Model validation is also demonstrated by predicting time-domain voltage response using the parameterized model and this is shown to have excellent agreement with measured voltage time-series data (11.1 mV RMSE).
Electron-scale shear instabilities: magnetic field generation and particle acceleration in astrophysical jets<|sep|>Strong shear flow regions found in astrophysical jets are shown to be important dissipation regions, where the shear flow kinetic energy is converted into electric and magnetic field energy via shear instabilities. The emergence of these self-consistent fields make shear flows significant sites for radiation emission and particle acceleration. We focus on electron-scale instabilities, namely the collisionless, unmagnetized Kelvin-Helmholtz instability (KHI) and a large-scale dc magnetic field generation mechanism on the electron scales. We show that these processes are important candidates to generate magnetic fields in the presence of strong velocity shears, which may naturally originate in energetic matter outburst of active galactic nuclei and gamma-ray bursters. We show that the KHI is robust to density jumps between shearing flows, thus operating in various scenarios with different density contrasts. Multidimensional particle-in-cell (PIC) simulations of the KHI, performed with OSIRIS, reveal the emergence of a strong and large-scale dc magnetic field component, which is not captured by the standard linear fluid theory. This dc component arises from kinetic effects associated with the thermal expansion of electrons of one flow into the other across the shear layer, whilst ions remain unperturbed due to their inertia. The electron expansion forms dc current sheets, which induce a dc magnetic field. Our results indicate that most of the electromagnetic energy developed in the KHI is stored in the dc component, reaching values of equipartition on the order of $10^{-3}$ in the electron time-scale, and persists longer than the proton time-scale. Particle scattering/acceleration in the self generated fields of these shear flow instabilities is also analyzed.
Cold Mode Accretion in Galaxy Formation<|sep|>A generic expectation for gas accreted by high mass haloes is that it is shock heated to the virial temperature of the halo. In low mass haloes, or at high redshift, however, the gas cooling rate is sufficiently rapid that an accretion shock is unlikely to form. Instead, gas can accrete directly into the centre of the halo in a `cold mode' of accretion. Although semi-analytic models have always made a clear distinction between hydrostatic and rapid cooling they have not made a distinction between whether or not an accretion shock forms. Starting from the well-established Galform code, we investigate the effect of explicitly accounting for cold mode accretion using the shock stability model of Birnboim & Dekel. When we modify the code so that there is no effective feedback from galaxy formation, we find that cold mode accretion is the dominant channel for feeding gas into the galaxies at high redshifts. However, this does not translate into a significant difference in the star formation history of the universe compared to the previous code. When effective feedback is included in the model, we find that the the cold mode is much less apparent because of the presence of gas ejected from the galaxy. Thus the inclusion of the additional cold mode physics makes little difference to basic results from earlier semi-analytic models which used a simpler treatment of gas accretion. For more sophisticated predictions of its consequences, we require a better understanding of how the cold mode delivers angular momentum to galaxies and how it interacts with outflows.
Unveiling hidden properties of young star clusters: differential reddening, star-formation spread and binary fraction<|sep|>Usually, important parameters of young, low-mass star clusters are very difficult to obtain by means of photometry, especially when differential reddening and/or binaries occur in large amounts. We present a semi-analytical approach that, applied to the Hess diagram of a young star cluster, is able to retrieve the values of mass, age, star-formation spread, distance modulus, foreground and differential reddening, and binary fraction. The global optimisation method known as adaptive simulated annealing (ASA) is used to minimise the residuals between the observed and simulated Hess diagrams of a star cluster. The simulations are realistic and take the most relevant parameters of young clusters into account. Important features of the simulations are: a normal (Gaussian) differential reddening distribution, a time-decreasing star-formation rate, the unresolved binaries, and the smearing effect produced by photometric uncertainties on Hess diagrams. Free parameters are: cluster mass, age, distance modulus, star-formation spread, foreground and differential reddening, and binary fraction.
Notes on the integration of numerical relativity waveforms<|sep|>A primary goal of numerical relativity is to provide estimates of the wave strain, $h$, from strong gravitational wave sources, to be used in detector templates. The simulations, however, typically measure waves in terms of the Weyl curvature component, $\psi_4$. Assuming Bondi gauge, transforming to the strain $h$ reduces to integration of $\psi_4$ twice in time. Integrations performed in either the time or frequency domain, however, lead to secular non-linear drifts in the resulting strain $h$. These non-linear drifts are not explained by the two unknown integration constants which can at most result in linear drifts. We identify a number of fundamental difficulties which can arise from integrating finite length, discretely sampled and noisy data streams. These issues are an artifact of post-processing data. They are independent of the characteristics of the original simulation, such as gauge or numerical method used. We suggest, however, a simple procedure for integrating numerical waveforms in the frequency domain, which is effective at strongly reducing spurious secular non-linear drifts in the resulting strain.
A Robust Secure Hybrid Analog and Digital Receive Beamforming Scheme for Efficient Interference Reduction<|sep|>Medium-scale or large-scale receive antenna array with digital beamforming can be employed at receiver to make a significant interference reduction, but leads to expensive cost and high complexity of the RF-chain circuit. To deal with this issue, a classic analog-and-digital beamforming (ADB) structure was proposed in the literature for greatly reducing the number of RF-chains. Based on the ADB structure, we in this paper propose a robust hybrid ADB scheme to resist directions of arrival (DOAs) estimation errors. The key idea of our scheme is to employ null space projection (NSP) in analog beamforming domain and diagonal loading (DL) method in digital beamforming domain. Simulation results show that the proposed scheme performs more robustly, and moreover, has a significant improvement on the receive signal to interference plus noise ratio (SINR) compared to NSP ADB scheme and DL method.
Finite volume effects in pion-kaon scattering and reconstruction of the kappa(800) resonance<|sep|>Simulating the kappa(800) on the lattice is a challenging task that starts to become feasible due to the rapid progress in recent-years lattice QCD calculations. As the resonance is broad, special attention to finite-volume effects has to be paid, because no sharp resonance signal as from avoided level crossing can be expected. In the present article, we investigate the finite volume effects in the framework of unitarized chiral perturbation theory using next-to-leading order terms. After a fit to meson-meson partial wave data, lattice levels for piK scattering are predicted. In addition, levels are shown for the quantum numbers in which the sigma(600), f_0(980), a_0(980), phi(1020), K*(892), and rho(770) appear, as well as the repulsive channels. Methods to extract the kappa(800) signal from the lattice spectrum are presented. Using pseudo-data, we estimate the precision that lattice data should have to allow for a clear-cut extraction of this resonance. To put the results into context, in particular the required high precision on the lattice data, the sigma(600), the P-wave resonances K*(892) and rho(770), and the repulsive piK, pipi phases are analyzed as well.
A Quasi-Newton method for physically-admissible simulation of Poiseuille flow under fracture propagation<|sep|>Coupled hydro-mechanical processes are of great importance to numerous engineering systems, e.g., hydraulic fracturing, geothermal energy, and carbon sequestration. Fluid flow in fractures is modeled after a Poiseuille law that relates the conductivity to the aperture by a cubic relation. Newton's method is commonly employed to solve the resulting discrete, nonlinear algebraic systems. It is demonstrated, however, that Newton's method will likely converge to nonphysical numerical solutions, resulting in estimates with a negative fracture aperture. A Quasi-Newton approach is developed to ensure global convergence to the physical solution. A fixed-point stability analysis demonstrates that both physical and nonphysical solutions are stable for Newton's method, whereas only physical solutions are stable for the proposed Quasi-Newton method. Additionally, it is also demonstrated that the Quasi-Newton method offers a contraction mapping along the iteration path. Numerical examples of fluid-driven fracture propagation demonstrate that the proposed solution method results in robust and computationally efficient performance.
A sensitivity study of the neutral-neutral reactions C + C3 and C + C5 in cold dense interstellar clouds<|sep|>Chemical networks used for models of interstellar clouds contain many reactions, some of them with poorly determined rate coefficients and/or products. In this work, we report a method for improving the predictions of molecular abundances using sensitivity methods and ab initio calculations. Based on the chemical network osu.2003, we used two different sensitivity methods to determine the most important reactions as a function of time for models of dense cold clouds. Of these reactions, we concentrated on those between C and C3 and between C and C5, both for their effect on specific important species such as CO and for their general effect on large numbers of species. We then used ab initio and kinetic methods to determine an improved rate coefficient for the former reaction and a new set of products, plus a slightly changed rate coefficient for the latter. Putting our new results in a pseudo-time-dependent model of cold dense clouds, we found that the abundances of many species are altered at early times, based on large changes in the abundances of CO and atomic C. We compared the effect of these new rate coefficients/products on the comparison with observed abundances and found that they shift the best agreement from 3e4 yr to (1-3)e5 yr.
Yield enhancement in whispering gallery mode biosensors: microfluidics and optical forces<|sep|>A microfluidic whispering gallery mode (WGM) biosensing system is proposed for enhanced delivery and detection of target molecules. A microtoroid resonator coupled to a tapered optical fiber is immersed within a microfluidic channel, and supplied with target molecules at various flow rates. We show through Monte Carlo simulations that the flow characteristics and resonantly enhanced optical forces of the sensor substantially improve both the sensing time and yield. When compared to a diffusion-limited sensing modality, the average time required to detect a single molecule is reduced from more than 100 minutes to less than 10 seconds, and the overall yield of the device is enhanced from less than 5% to a maximum of 70.6% for femtomolar concentrations of analyte.
Space Alternating Variational Estimation Based Sparse Bayesian Learning for Complex-value Sparse Signal Recovery Using Adaptive Laplace Priors<|sep|>Due to its self-regularizing nature and its ability to quantify uncertainty, the Bayesian approach has achieved excellent recovery performance across a wide range of sparse signal recovery applications. However, most existing methods are based on the real-value signal model, with the complex-value signal model rarely considered. Motivated by the adaptive least absolute shrinkage and selection operator (LASSO) and the sparse Bayesian learning (SBL) framework, a hierarchical model with adaptive Laplace priors is proposed in this paper for recovery of complex sparse signals. Moreover, the space alternating approach is integrated into the algorithm to reduce the computational complexity of the proposed method. In experiments, the proposed algorithm is studied for complex Gaussian random dictionaries and different types of complex signals. These experiments show that the proposed algorithm offers better recovery performance for different types of complex signals than state-of-the-art methods.
In-Vivo Bytecode Instrumentation for Improving Privacy on Android Smartphones in Uncertain Environments<|sep|>In this paper we claim that an efficient and readily applicable means to improve privacy of Android applications is: 1) to perform runtime monitoring by instrumenting the application bytecode and 2) in-vivo, i.e. directly on the smartphone. We present a tool chain to do this and present experimental results showing that this tool chain can run on smartphones in a reasonable amount of time and with a realistic effort. Our findings also identify challenges to be addressed before running powerful runtime monitoring and instrumentations directly on smartphones. We implemented two use-cases leveraging the tool chain: BetterPermissions, a fine-grained user centric permission policy system and AdRemover an advertisement remover. Both prototypes improve the privacy of Android systems thanks to in-vivo bytecode instrumentation.
Two-loop $N$-jettiness soft function for $pp \to 2j$ production<|sep|>We calculate the two-loop soft function for $pp \to jj$, $e^+e^- \to 4j$ or $e p \to 3j$ when the $N$-jettiness observable is measured. The result presented here makes up the necessary piece for realizing the full next-to-next-to-leading order predictions for these processes using the $N$-jettiness subtraction scheme.
On the analysis of scheduling algorithms for structured parallel computations<|sep|>Algorithms for scheduling structured parallel computations have been widely studied in the literature. For some time now, Work Stealing is one of the most popular for scheduling such computations, and its performance has been studied in both theory and practice. Although it delivers provably good performances, the effectiveness of its underlying load balancing strategy is known to be limited for certain classes of computations, particularly the ones exhibiting irregular parallelism (e.g. depth first searches). Many studies have addressed this limitation from a purely load balancing perspective, viewing computations as sets of independent tasks, and then analyzing the expected amount of work attached to each processor as the execution progresses. However, these studies make strong assumptions regarding work generation which, despite being standard from a queuing theory perspective --- where work generation can be assumed to follow some random distribution --- do not match the reality of structured parallel computations --- where the work generation is not random, only depending on the structure of a computation. In this paper, we introduce a formal framework for studying the performance of structured computation schedulers, define a criterion that is appropriate for measuring their performance, and present a methodology for analyzing the performance of randomized schedulers. We demonstrate the convenience of this methodology by using it to prove that the performance of Work Stealing is limited, and to analyze the performance of a Work Stealing and Spreading algorithm, which overcomes Work Stealing's limitation.
An X-ray and Radio Study of the Hubble Frontier Field Cluster Abell S1063<|sep|>We present results from \textit{Chandra} X-ray observations and 325 MHz Giant Metrewave Radio Telescope (GMRT) observations of the massive and X-ray luminous cluster of galaxies Abell S1063. We report the detection of large-scale \lq\lq excess brightness\rq\rq\ in the residual \textit{Chandra} X-ray surface brightness map, which extends at least 2.7 Mpc towards the north-east from the center of the cluster. We also present a high fidelity X-ray flux and temperature map using \textit{Chandra} archival data of 122 ksec, which shows the disturbed morphology in the cluster. The residual flux map shows the first observational confirmation of the merging axis proposed by earlier simulation by \citet{Gomez2012AJ....144...79G}. The average temperature within $R_{500}$ is $11.7 \pm 0.56$ keV, which makes AS1063 one of the hottest clusters in the nearby Universe. The integrated radio flux density at 325 MHz is found to be $62.0\pm6.3$ mJy. The integrated spectrum of the radio halo follows a power-law with a spectral index $\alpha=-1.43\pm 0.13$. The radio halo is found to be significantly under-luminous, which favored for both the hadronic as well as the turbulent re-acceleration mechanism for its origin.
Reconfiguration Algorithms for High Precision Communications in Time Sensitive Networks: Time-Aware Shaper Configuration with IEEE 802.1Qcc (Extended Version)<|sep|>As new networking paradigms emerge for different networking applications, e.g., cyber-physical systems, and different services are handled under a converged data link technology, e.g., Ethernet, certain applications with mission critical traffic cannot coexist on the same physical networking infrastructure using traditional Ethernet packet-switched networking protocols. The IEEE 802.1Q Time Sensitive Networking (TSN) task group is developing protocol standards to provide deterministic properties on Ethernet based packet-switched networks. In particular, the IEEE 802.1Qcc, centralized management and control, and the IEEE 802.1Qbv, Time-Aware Shaper, can be used to manage and control scheduled traffic streams with periodic properties along with best-effort traffic on the same network infrastructure. In this paper, we investigate the effects of using the IEEE 802.1Qcc management protocol to accurately and precisely configure TAS enabled switches (with transmission windows governed by gate control lists (GCLs) with gate control entries (GCEs)) ensuring ultra-low latency, zero packet loss, and minimal jitter for scheduled TSN traffic. We examine both a centralized network/distributed user model (hybrid model) and a fully-distributed (decentralized) 802.1Qcc model on a typical industrial control network with the goal of maximizing scheduled traffic streams.
Solar Atmospheric Neutrinos and the Sensitivity Floor for Solar Dark Matter Annihilation Searches<|sep|>Cosmic rays interacting in the solar atmosphere produce showers that result in a flux of high-energy neutrinos from the Sun. These form an irreducible background to indirect solar WIMP co-annihilation searches, which look for heavy dark matter particles annihilating into final states containing neutrinos in the Solar core. This background will eventually create a sensitivity floor for indirect WIMP co-annihilation searches analogous to that imposed by low-energy solar neutrino interactions for direct dark matter detection experiments. We present a new calculation of the flux of solar atmospheric neutrinos with a detailed treatment of systematic uncertainties inherent in solar atmospheric shower evolution, and we use this to derive the sensitivity floor for indirect solar WIMP annihilation analyses. We find that the floor lies less than one order of magnitude beyond the present experimental limits on spin-dependent WIMP-proton cross sections for some mass points, and that the high-energy solar atmospheric neutrino flux may be observable with running and future neutrino telescopes.
Multi-criteria Anomaly Detection using Pareto Depth Analysis<|sep|>We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.
Generalized modularity matrices<|sep|>Various modularity matrices appeared in the recent literature on network analysis and algebraic graph theory. Their purpose is to allow writing as quadratic forms certain combinatorial functions appearing in the framework of graph clustering problems. In this paper we put in evidence certain common traits of various modularity matrices and shed light on their spectral properties that are at the basis of various theoretical results and practical spectral-type algorithms for community detection.
New H2 collision-induced absorption and NH3 opacity and the spectra of the coolest brown dwarfs<|sep|>We present new cloudy and cloudless model atmospheres for brown dwarfs using recent ab initio calculations of the line list of ammonia (NH3) and of the collision-induced absorption of molecular hydrogen (H2). We compare the new synthetic spectra with models based on an earlier description of the H2 and NH3 opacities. We find a significant improvement in fitting the nearly complete spectral energy distribution of the T7p dwarf Gliese 570D and in near infrared color-magnitude diagrams of field brown dwarfs. We apply these new models to the identification of NH3 absorption in the H band peak of very late T dwarfs and the new Y dwarfs and discuss the observed trend in the NH3-H spectral index. The new NH3 line list also allows a detailed study of the medium resolution spectrum of the T9/T10 dwarf UGPS J072227.51-054031.2 where we identify several specific features caused by NH3.
Goal-Driven Dynamics Learning via Bayesian Optimization<|sep|>Real-world robots are becoming increasingly complex and commonly act in poorly understood environments where it is extremely challenging to model or learn their true dynamics. Therefore, it might be desirable to take a task-specific approach, wherein the focus is on explicitly learning the dynamics model which achieves the best control performance for the task at hand, rather than learning the true dynamics. In this work, we use Bayesian optimization in an active learning framework where a locally linear dynamics model is learned with the intent of maximizing the control performance, and used in conjunction with optimal control schemes to efficiently design a controller for a given task. This model is updated directly based on the performance observed in experiments on the physical system in an iterative manner until a desired performance is achieved. We demonstrate the efficacy of the proposed approach through simulations and real experiments on a quadrotor testbed.
Simons Observatory Small Aperture Telescope overview<|sep|>The Simons Observatory (SO) is a cosmic microwave background (CMB) experiment from the Atacama Desert in Chile comprising three small-aperture telescopes (SATs) and one large-aperture telescope (LAT). In total, SO will field over 60,000 transition-edge sensor (TES) bolometers in six spectral bands centered between 27 and 280 GHz in order to achieve the sensitivity necessary to measure or constrain numerous cosmological quantities. In this work, we focus on the SATs which are optimized to search for primordial gravitational waves that are detected as parity-odd polarization patterns called a B-modes on degree scales in the CMB. Each SAT employs a single optics tube with TES arrays operating at 100 mK. The high throughput optics system has a 42 cm aperture and a 35-degree field of view coupled to a 36 cm diameter focal plane. The optics consist of three metamaterial anti-re ection coated silicon lenses. Cryogenic ring baffles with engineered blackbody absorbers are installed in the optics tube to minimize the stray light. The entire optics tube is cooled to 1 K. A cryogenic continuously rotating half-wave plate near the sky side of the aperture stop helps to minimize the effect of atmospheric uctuations. The telescope warm baffling consists of a forebaffle, an elevation stage mounted co-moving shield, and a fixed ground shield that together control the far side-lobes and mitigates ground-synchronous systematics. We present the status of the SAT development.
An intent-based approach for creating assistive robots' control systems<|sep|>The current research standards in robotics demand general approaches to robots' controllers development. In the assistive robotics domain, the human-machine interaction plays a substantial role. Especially, the humans generate intents that affect robot control system. In the article an approach is presented for creating control systems for assistive robots, which reacts to users' intents delivered by voice commands, buttons, or an operator console. The whole approach was applied to the real system consisting of customised TIAGo robot and additional hardware components. The exemplary experiments performed on the platform illustrate the motivation for diversification of human-machine interfaces in assistive robots.
Fast Packed String Matching for Short Patterns<|sep|>Searching for all occurrences of a pattern in a text is a fundamental problem in computer science with applications in many other fields, like natural language processing, information retrieval and computational biology. In the last two decades a general trend has appeared trying to exploit the power of the word RAM model to speed-up the performances of classical string matching algorithms. In this model an algorithm operates on words of length w, grouping blocks of characters, and arithmetic and logic operations on the words take one unit of time. In this paper we use specialized word-size packed string matching instructions, based on the Intel streaming SIMD extensions (SSE) technology, to design very fast string matching algorithms in the case of short patterns. From our experimental results it turns out that, despite their quadratic worst case time complexity, the new presented algorithms become the clear winners on the average for short patterns, when compared against the most effective algorithms known in literature.
Pointing the Unknown Words<|sep|>The problem of rare and unknown words is an important issue that can potentially influence the performance of many NLP systems, including both the traditional count-based and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each time-step, the decision of which softmax layer to use choose adaptively made by an MLP which is conditioned on the context.~We motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known.~We observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset using our proposed model.
Ground state pseudoscalar mesons on the light front: from the light to heavy sector<|sep|>We extract the leading Fock-state light front wave functions (LF-LFWFs) of both the light and heavy pseudoscalar mesons, e.g., the pion (at masses of 130 MeV, 310 MeV and 690 MeV), $\eta_c$ and $\eta_b$, from their covariant Bethe-Salpeter wave functions within the rainbow-ladder (RL) truncation. It is shown that the LF-LFWFs get narrower in $x$ (the longitudinal momentum fraction of meson carried by the quark) with the increasing current quark mass, and the leading twist parton distribution amplitudes (PDAs) inherit this feature. Meanwhile, we find in the pion the LF-LFWFs only contribute around 30\% to the total Fock-state normalization, indicating the presence of significant higher Fock-states within. In contrast, in the $\eta_c$ and $\eta_b$ the LF-LFWFs contribute more than $90$\%, suggesting the $Q\bar{Q}$ valence Fock-state truncation as a good approximation for heavy mesons. We thus study the 3-dimensional parton distributions of the $\eta_c$ and $\eta_b$ with the unpolarized generalized parton distribution function (GPD) and the transverse momentum dependent parton distribution function (TMD). Through the gravitational form factors in connection with the GPD, the mass radii of the $\eta_c$ and $\eta_b$ in the light-cone frame are determined to be $r_{E,{\rm LC}}^{\eta_c} =0.150$ fm and $r_{E,{\rm LC}}^{\eta_b} =0.089$ fm respectively.
Sharding-Based Proof-of-Stake Blockchain Protocols: Security Analysis<|sep|>Blockchain technology has been gaining great interest from a variety of sectors, including healthcare, supply chain and cryptocurrencies. However, Blockchain suffers from its limited ability to scale (i.e. low throughput and high latency). Several solutions have been appeared to tackle this issue. In particular, sharding proved that it is one of the most promising solutions to Blockchain scalability. Sharding can be divided into two major categories: (1) Sharding-based Proof-of-Work (PoW) Blockchain protocols, and (2) Sharding-based Proof-of-Stake (PoS) Blockchain protocols. The two categories achieve a good performances (i.e. good throughput with a reasonable latency), but raise security issues. This article attends that analyze the security of the second category. More specifically, we compute the probability of committing a faulty block and measure the security by computing the number of years to fail. Finally, to show the effectiveness of the proposed model, we conduct a numerical analysis and evaluate the results obtained.
New Hamiltonian expansions adapted to the Trojan problem<|sep|>A number of studies, referring to the observed Trojan asteroids of various planets in our Solar System, or to hypothetical Trojan bodies in extrasolar planetary systems, have emphasized the importance of so-called secondary resonances in the problem of the long term stability of Trojan motions. Such resonances describe commensurabilities between the fast, synodic, and secular frequency of the Trojan body, and, possibly, additional slow frequencies produced by more than one perturbing bodies. The presence of secondary resonances sculpts the dynamical structure of the phase space. Hence, identifying their location is a relevant task for theoretical studies. In the present paper we combine the methods introduced in two recent papers (Paez & Efthymiopoulos, 2015, Paez & Locatelli, 2015) in order to analytically predict the location of secondary resonances in the Trojan problem (SEE FILE FOR COMPLETE ABSTRACT)
Determination of an onset of superconducting diamagnetism by scaling of the normal-state magnetization<|sep|>We propose a simple scaling procedure for the normal-state magnetization Mn data collected as functions of temperature T in different magnetic fields H. As a result, the Mn(T) curves collected in different fields collapse on to a single Msc(T) line. In this representation, the onset of superconducting diamagnetism manifests itself by a sharp divergence of the Msc(T) curves for different H values. As will be demonstrated, this allows for a reliable determination of temperature Tonset, at which superconducting diamagnetism become observable.
Resource Allocation in Mobility-Aware Federated Learning Networks: A Deep Reinforcement Learning Approach<|sep|>Federated learning allows mobile devices, i.e., workers, to use their local data to collaboratively train a global model required by the model owner. Federated learning thus addresses the privacy issues of traditional machine learning. However, federated learning faces the energy constraints of the workers and the high network resource cost due to the fact that a number of global model transmissions may be required to achieve the target accuracy. To address the energy constraint, a power beacon can be used that recharges energy to the workers. However, the model owner may need to pay an energy cost to the power beacon for the energy recharge. To address the high network resource cost, the model owner can use a WiFi channel, called default channel, for the global model transmissions. However, communication interruptions may occur due to the instability of the default channel quality. For this, special channels such as LTE channels can be used, but this incurs channel cost. As such, the problem of the model owner is to decide amounts of energy recharged to the workers and to choose channels used to transmit its global model to the workers to maximize the number of global model transmissions while minimizing the energy and channel costs. This is challenging for the model owner under the uncertainty of the channel, energy and mobility states of the workers. In this paper, we thus propose to employ the Deep Q-Network (DQN) that enables the model owner to find the optimal decisions on the energy and the channels without any a priori network knowledge. Simulation results show that the proposed DQN always achieves better performance compared to the conventional algorithms.
The Adapted Ordering Method for Lie Algebras and Superalgebras and their Generalizations<|sep|>In 1998 the Adapted Ordering Method was developed for the representation theory of the superconformal algebras in two dimensions. It allows: to determine maximal dimensions for a given type of space of singular vectors, to identify all singular vectors by only a few coefficients, to spot subsingular vectors and to set the basis for constructing embedding diagrams. In this article we present the Adapted Ordering Method for general Lie algebras and superalgebras, and their generalizations, provided they can be triangulated. We also review briefly the results obtained for the Virasoro algebra and for the N=2 and Ramond N=1 superconformal algebras.
Magnetic field diagnostics and spatio-temporal variability of the solar transition region<|sep|>Magnetic field diagnostics of the transition region from the chromosphere to the corona faces us with the problem that one has to apply extreme UV spectro-polarimetry. While for coronal diagnostic techniques already exist through infrared coronagraphy above the limb and radio observations on the disk, for the transition region one has to investigate extreme UV observations. However, so far the success of such observations has been limited, but there are various projects to get spectro-polarimetric data in the extreme UV in the near future. Therefore it is timely to study the polarimetric signals we can expect for such observations through realistic forward modeling. We employ a 3D MHD forward model of the solar corona and synthesize the Stokes I and Stokes V profiles of C IV 1548 A. A signal well above 0.001 in Stokes V can be expected, even when integrating for several minutes in order to reach the required signal-to-noise ratio, despite the fact that the intensity in the model is rapidly changing (just as in observations). Often this variability of the intensity is used as an argument against transition region magnetic diagnostics which requires exposure times of minutes. However, the magnetic field is evolving much slower than the intensity, and thus when integrating in time the degree of (circular) polarization remains rather constant. Our study shows the feasibility to measure the transition region magnetic field, if a polarimetric accuracy on the order of 0.001 can be reached, which we can expect from planned instrumentation.
Reinforcement learning for multi-item retrieval in the puzzle-based storage system<|sep|>Nowadays, fast delivery services have created the need for high-density warehouses. The puzzle-based storage system is a practical way to enhance the storage density, however, facing difficulties in the retrieval process. In this work, a deep reinforcement learning algorithm, specifically the Double&Dueling Deep Q Network, is developed to solve the multi-item retrieval problem in the system with general settings, where multiple desired items, escorts, and I/O points are placed randomly. Additionally, we propose a general compact integer programming model to evaluate the solution quality. Extensive numerical experiments demonstrate that the reinforcement learning approach can yield high-quality solutions and outperforms three related state-of-the-art heuristic algorithms. Furthermore, a conversion algorithm and a decomposition framework are proposed to handle simultaneous movement and large-scale instances respectively, thus improving the applicability of the PBS system.
The impact of mass map truncation on strong lensing simulations<|sep|>Strong gravitational lensing is a powerful tool to measure cosmological parameters and to study galaxy evolution mechanisms. However, quantitative strong lensing studies often require mock observations. To capture the full complexity of galaxies, the lensing galaxy is often drawn from high resolution, dark matter only or hydro-dynamical simulations. These have their own limitations, but the way we use them to emulate mock lensed systems may also introduce significant artefacts. In this work we identify and explore the specific impact of mass truncation on simulations of strong lenses by applying different truncation schemes to a fiducial density profile with conformal isodensity contours. Our main finding is that improper mass truncation can introduce undesired artificial shear. The amplitude of the spurious shear depends on the shape and size of the truncation area as well as on the slope and ellipticity of the lens density profile. Due to this effect, the value of H0 or the shear amplitude inferred by modelling those systems may be biased by several percents. However, we show that the effect becomes negligible provided that the lens projected map extends over at least 50 times the Einstein radius.
A deep search for molecular gas in two massive Lyman break galaxies at z=3 and 4: vanishing CO-emission due to low metallicity?<|sep|>We present deep IRAM Plateau de Bure Interferometer (PdBI) observations, searching for CO-emission toward two massive, non-lensed Lyman break galaxies (LBGs) at z=3.216 and 4.058. With one low significance CO detection (3.5 sigma) and one sensitive upper limit, we find that the CO lines are >~ 3-4 times weaker than expected based on the relation between IR and CO luminosities followed by similarly, massive galaxies at z=0-2.5. This is consistent with a scenario in which these galaxies have low metallicity, causing an increased CO-to-H_2 conversion factor, i.e., weaker CO-emission for a given molecular (H_2) mass. The required metallicities at z>3 are lower than predicted by the fundamental metallicity relation (FMR) at these redshifts, consistent with independent evidence. Unless our galaxies are atypical in this respect, detecting molecular gas in normal galaxies at z>3 may thus remain challenging even with ALMA.
A new nearby pulsar wind nebula overlapping the RX J0852.0-4622 supernova remnant<|sep|>Energetic pulsars can be embedded in a nebula of relativistic leptons which is powered by the dissipation of the rotational energy of the pulsar. The object PSR J0855-4644 is an energetic and fast-spinning pulsar (Edot = 1.1x10^36 erg/s, P=65 ms) discovered near the South-East rim of the supernova remnant (SNR) RX J0852.0-4622 (aka Vela Jr) by the Parkes multibeam survey. The position of the pulsar is in spatial coincidence with an enhancement in X-rays and TeV gamma-rays, which could be due to its putative pulsar wind nebula (PWN). The purpose of this study is to search for diffuse non-thermal X-ray emission around PSR J0855-4644 to test for the presence of a PWN and to estimate the distance to the pulsar. An X-ray observation was carried out with the XMM-Newton satellite to constrain the properties of the pulsar and its nebula. The absorption column density derived in X-rays from the pulsar and from different regions of the rim of the SNR was compared with the absorption derived from the atomic (HI) and molecular (12CO) gas distribution along the corresponding lines of sight to estimate the distance of the pulsar and of the SNR. The observation has revealed the X-ray counterpart of the pulsar together with surrounding extended emission thus confirming the existence of a PWN. The comparison of column densities provided an upper limit to the distance of the pulsar PSR J0855-4644 and the SNR RX J0852.0-4622 (d<900 pc). Although both objects are at compatible distances, we rule out that the pulsar and the SNR are associated. With this revised distance, PSR J0855-4644 is the second most energetic pulsar, after the Vela pulsar, within a radius of 1 kpc and could therefore contribute to the local cosmic-ray e-/e+ spectrum.
Noether symmetric classical and quantum scalar field cosmology<|sep|>We study the evolution of a two dimensional minisuperspace cosmological model in classical and quantum levels by the Noether symmetry approach. The phase space variables turn out to correspond to the scale factor of a Friedmann-Robertson-Walker (FRW) model and a scalar field with which the action of the model is augmented. It is shown that the minisuperspace of such a model is a two dimensional manifold with vanishing Ricci scalar. We present a coordinate transformation which cast the corresponding minisuper metric to a Minkowskian or Euclidean one according to the choices of an ordinary or phantom model for the scalar field. Then, the Noether symmetry of such a cosmological model is investigated by utilizing the behavior of the corresponding Lagrangian under the infinitesimal generators of the desired symmetry. We explicitly calculate the form of the scalar field potential functions for which such symmetries exist. For these potential functions, the exact classical and quantum solutions in the cases where the scalar field is an ordinary or a phantom one, are presented and compared.
Renaissance of the ~1 TeV Fixed-Target Program<|sep|>This document describes the physics potential of a new fixed-target program based on a ~1 TeV proton source. Two proton sources are potentially available in the future: the existing Tevatron at Fermilab, which can provide 800 GeV protons for fixed-target physics, and a possible upgrade to the SPS at CERN, called SPS+, which would produce 1 TeV protons on target. In this paper we use an example Tevatron fixed-target program to illustrate the high discovery potential possible in the charm and neutrino sectors. We highlight examples which are either unique to the program or difficult to accomplish at other venues.
Liquid crystal defects in the Landau-de Gennes theory in two dimensions-beyond the one-constant approximation<|sep|>We consider the two-dimensional Landau-de Gennes energy with several elastic constants, subject to general $k$-radially symmetric boundary conditions. We show that for generic elastic constants the critical points consistent with the symmetry of the boundary conditions exist only in the case $k=2$. In this case we identify three types of radial profiles: with two, three of full five components and numerically investigate their minimality and stability depending on suitable parameters. We also numerically study the stability properties of the critical points of the Landau-de Gennes energy and capture the intricate dependence of various qualitative features of these solutions on the elastic constants and the physical regimes of the liquid crystal system.
Adding Interactivity to Education of Complex Wireless Networks Using Digital Game-Based Learning<|sep|>Can we make undergraduate engineering education easier and more fun? This research aims to see if we can answer the ambitious question! The digital game-based learning (DGBL) has been found to increase the efficacy of learning when applied in engineering classes thanks to its ability to make students feel easy and fun. However, the state-of-the-art DGBL schemes still observe challenges in various aspects including cost, efficacy, readiness of instructors and students, etc. Motivated from the challenges, this research proposes to design a DGBL platform that features visualized and systematic views. Specifically, we identify the blockchain applied to wireless communications and networking systems as a key ecosystem that we capitalize the benefit of the proposed platform.. As such, in this paper, we lay out a comprehensive DGBL pedagogy that includes (i) creation of relevant assignment activities and class materials in a relevant course and (ii) evaluation of the pedagogical efficacy. In a long-term view, a successful delivery of this research will increase the confidence of undergraduate engineering students on the "in-concert" dynamics of various factors determining the performance of a blockchain system built on a wireless network.
Driving solar coronal MHD simulations on high-performance computers<|sep|>The quality of today's research is often tightly limited to the available computing power and scalability of codes to many processors. For example, tackling the problem of heating the solar corona requires a most realistic description of the plasma dynamics and the magnetic field. Numerically solving such a magneto-hydrodynamical (MHD) description of a small active region (AR) on the Sun requires millions of computation hours on current high-performance computing (HPC) hardware. The aim of this work is to describe methods for an efficient parallelization of boundary conditions and data input/output (IO) strategies that allow for a better scaling towards thousands of processors (CPUs). The Pencil Code is tested before and after optimization to compare the performance and scalability of a coronal MHD model above an AR. We present a novel boundary condition for non-vertical magnetic fields in the photosphere, where we approach the realistic pressure increase below the photosphere. With that, magnetic flux bundles become narrower with depth and the flux density increases accordingly. The scalability is improved by more than one order of magnitude through the HPC-friendly boundary conditions and IO strategies. This work describes also the necessary nudging methods to drive the MHD model with observed magnetic fields from the Sun's photosphere. In addition, we present the upper and lower atmospheric boundary conditions (photospheric and towards the outer corona), including swamp layers to diminish perturbations before they reach the boundaries. Altogether, these methods enable more realistic 3D MHD simulations than previous models regarding the coronal heating problem above an AR -- simply because of the ability to use a large amount of CPUs efficiently in parallel.
Search for the Standard Model Higgs boson in WH -> lnubb and H -> WW(*) -> lnulnu channels at ATLAS<|sep|>Results for the Standard Model Higgs boson search by the ATLAS experiment in the WH -> lnubb and H -> WW(*) -> lnulnu channels are presented. The results are based on 1.04 fb^-1 of data from pp collisions at sqrt(s) = 7 TeV produced by the LHC in 2011. No evidence is found for the Standard Model Higgs boson in either decay mode. The WH -> lnubb channel is not yet sensitive to the Standard Model Higgs, while the H -> WW(*) -> lnulnu channel excludes the Standard Model Higgs in the range of 158 < m_H < 186 GeV at the 95% confidence level.
Soft Gravitons Screen Couplings in de Sitter Space<|sep|>The scale invariance of the quantum fluctuations in de Sitter space leads to the appearance of de Sitter symmetry breaking infra-red logarithms in the graviton propagator. We investigate physical effects of soft gravitons on the local dynamics of matter fields well inside the cosmological horizon. We show that the IR logarithms do not spoil Lorentz invariance in scalar and Dirac field theory. The leading IR logarithms can be absorbed by a time dependent wave function renormalization factor in the both cases. In the interacting field theory with $\lambda \phi^4$ and Yukawa interaction, we find that the couplings become time dependent with definite scaling exponents. We argue that the relative scaling exponents of the couplings are gauge invariant and physical as we can use the evolution of a coupling as a physical time.
Interpreting Deep Classifier by Visual Distillation of Dark Knowledge<|sep|>Interpreting black box classifiers, such as deep networks, allows an analyst to validate a classifier before it is deployed in a high-stakes setting. A natural idea is to visualize the deep network's representations, so as to "see what the network sees". In this paper, we demonstrate that standard dimension reduction methods in this setting can yield uninformative or even misleading visualizations. Instead, we present DarkSight, which visually summarizes the predictions of a classifier in a way inspired by notion of dark knowledge. DarkSight embeds the data points into a low-dimensional space such that it is easy to compress the deep classifier into a simpler one, essentially combining model compression and dimension reduction. We compare DarkSight against t-SNE both qualitatively and quantitatively, demonstrating that DarkSight visualizations are more informative. Our method additionally yields a new confidence measure based on dark knowledge by quantifying how unusual a given vector of predictions is.
Effects of Model Parameters in Thermodynamics of the PNJL Model<|sep|>The thermodynamic behavior of the two-flavor($N_f=$2) three-color ($N_c=3$) Polyakov-loop-extended Nambu-Jona-Lasinio model at the finite chemical potential is investigated. New lattice gluon data for gluon thermodynamics are used defining the effective potential within polynomial and logarithmic forms of its approximation. We study the effects of using different sets of data and different forms of the potential on thermodynamic properties of hot and dense matter. It is found that the PNJL thermodynamics depends stronger on the form of the effective potential than on the used lattice data set. Particular attention is paid to the phase diagram in the $(T,\mu)$ plane.
Embedding Four-directional Paths on Convex Point Sets<|sep|>A directed path whose edges are assigned labels "up", "down", "right", or "left" is called \emph{four-directional}, and \emph{three-directional} if at most three out of the four labels are used. A \emph{direction-consistent embedding} of an \mbox{$n$-vertex} four-directional path $P$ on a set $S$ of $n$ points in the plane is a straight-line drawing of $P$ where each vertex of $P$ is mapped to a distinct point of $S$ and every edge points to the direction specified by its label. We study planar direction-consistent embeddings of three- and four-directional paths and provide a complete picture of the problem for convex point sets.
Measurement of the pure dissolution rate constant of a mineral in water<|sep|>We present here a methodology, using holographic interferometry, enabling to measure the pure surface reaction rate constant of the dissolution of a mineral in water, unambiguously free from the influence of mass transport. We use that technique to access to this value for gypsum and we demonstrate that it was never measured before but could be deduced a posteriori from the literature results if hydrodynamics is taken into account with accuracy. It is found to be much smaller than expected. This method enables to provide reliable rate constants for the test of dissolution models and the interpretation of in situ measurements, and gives clues to explain the inconsistency between dissolution rates of calcite and aragonite, for instance, in the literature.
Improving BPSO-based feature selection applied to offline WI handwritten signature verification through overfitting control<|sep|>This paper investigates the presence of overfitting when using Binary Particle Swarm Optimization (BPSO) to perform the feature selection in a context of Handwritten Signature Verification (HSV). SigNet is a state of the art Deep CNN model for feature representation in the HSV context and contains 2048 dimensions. Some of these dimensions may include redundant information in the dissimilarity representation space generated by the dichotomy transformation (DT) used by the writer-independent (WI) approach. The analysis is carried out on the GPDS-960 dataset. Experiments demonstrate that the proposed method is able to control overfitting during the search for the most discriminant representation.
Spacetime mappings of the Brown-York quasilocal energy<|sep|>In several areas of theoretical physics it is useful to know how a quasilocal energy transforms under conformal rescalings or generalized Kerr-Schild mappings. We derive the transformation properties of the Brown-York quasilocal energy in spherical symmetry and we contrast them with those of the Misner-Sharp-Hernandez energy.
Flavor and CP-violating Higgs sector in two Higgs doublet models with $U(1)'$<|sep|>We investigate the role of a local $U(1)'$ symmetry for the problem of CP violation in the effective theory for two Higgs doublet models and its microscopic counterparts. First, in two Higgs doublet models with $U(1)'$, we show that the higher-dimensional operators in the scalar potential violate the CP symmetry with an interplay with the mixing mass parameter, giving rise to small mixings between CP-even and CP-odd scalars. Motivated by the $B$-meson anomalies in recent years, we take the flavored $U(1)'$ to be a benchmark model for specifying the flavor structure. Then, we calculate the electric dipole moment of electron (eEDM) at two loops due to the CP-violating higher-dimensional operators and identify the correlation between the masses of heavy Higgs bosons and the cutoff scale from the bound on eEDM. We also comment on the possibility of making an independent test of the CP violation in the collider searches for heavy Higgs bosons. Finally, we show how the obtained eEDM results in the effective theory can be used to constrain microscopic models with an explicit CP violation in the partially decoupled or dark sectors.
Numerically evaluating the bispectrum in curved field-space - with PyTransport 2.0<|sep|>We extend the transport framework for numerically evaluating the power spectrum and bispectrum in multi-field inflation to the case of a curved field-space metric. This method naturally accounts for all sub- and super-horizon tree level effects, including those induced by the curvature of the field-space. We present an open source implementation of our equations in an extension of the publicly available PyTransport code. Finally we illustrate how our technique is applied to examples of inflationary models with a non-trivial field-space metric.
Discovery of hard-spectrum \gamma-ray emission from the BL Lac object 1ES 0414+009<|sep|>1ES 0414+009 (z = 0.287) is a distant high-frequency-peaked BL Lac object, and has long been considered a likely emitter of very-high energy (VHE, E>100 GeV) gamma-rays due to its high X-ray and radio flux. Observations in the VHE gamma-ray band and across the electromagnetic spectrum can provide insights into the origin of highly energetic particles present in the source and the radiation processes at work. Because of the distance of the source, the gamma-ray spectrum might provide further limits on the level of the Extragalactic Background Light (EBL). We report observations made between October 2005 and December 2009 with H.E.S.S., an array of four imaging atmospheric Cherenkov telescopes. Observations at high energies (HE, 100 MeV - 100 GeV) with the Fermi-LAT instrument in the first 20 months of its operation are also reported. To complete the multi-wavelength picture, archival UV and X-ray observations with the Swift satellite and optical observations with the ATOM telescope are also used. Based on the observations with H.E.S.S., 1ES 0414+009 is detected for the first time in the VHE band. An excess of 224 events is measured, corresponding to a significance of 7.8 sigma. The photon spectrum of the source is well described by a power law, with photon index of 3.45 \pm 0.25stat \pm 0.20syst. The integral flux above 200 GeV is (1.88 \pm 0.20stat \pm 0.38syst) \times10-12 cm-2 s-1. Observations with the Fermi-LAT in the first 20 months of operation show a flux between 200 MeV and 100 GeV of (2.3 \pm 0.2stat) \times 10-9 erg cm-2 s-1, and a spectrum well described by a power-law function with a photon index 1.85 \pm 0.18. Swift/XRT observations show an X-ray flux between 2 and 10 keV of (0.8 - 1) \times 10-11 erg cm-2 s-1, and a steep spectrum (2.2 - 2.3). Combining X-ray with optical-UV data, a fit with a log-parabolic function locates the synchrotron peak around 0.1 keV. ...
On the Evolution of the Star Formation Rate Function of Massive Galaxies. Constraints at 0.4<z<1.8 from the GOODS-MUSIC Catalogue<|sep|>[abridged] We study the evolution of the Star Formation Rate Function (SFRF) of massive galaxies over the 0.4<z<1.8 redshift range and its implications for our understanding of the physical processes responsible for galaxy evolution. We use multiwavelength observations included in the GOODS-MUSIC catalogue, which provides a suitable coverage of the spectral region from 0.3 to 24 micron and either spectroscopic or photometric redshifts for each object. Individual SFRs have been obtained by combining UV and 24 micron observations, when the latter were available. For all other sources an "SED fitting" SFR estimate has been considered. We then define a stellar mass limited sample, complete in the Mstar>1.e10 Msun range and determine the SFRF using the 1/Vmax algorithm. We define simulated galaxy catalogues based on three different semi-analytical models of galaxy formation and evolution. We show that the theoretical SFRFs are well described by a double power law functional form and its redshift evolution is approximated with high accuracy by a pure evolution of the typical SFR. We find good agreement between model predictions and the high-SFR end of the SFRF, when the observational errors on the SFR are taken into account. However, the observational SFRF is characterised by a double peaked structure, which is absent in its theoretical counterparts. At z>1.0 the observed SFRF shows a relevant density evolution, which is not reproduced by SAMs, due to the well known overprediction of intermediate mass galaxies at z~2. The agreement at the low-SFR end is poor: all models overpredict the space density of SFR~1 Msun/yr and no model reproduces the double peaked shape of the observational SFRF. If confirmed by deeper IR observations, this discrepancy will provide a key constraint on theoretical modelling of star formation and stellar feedback.
Laser induced electron diffraction: a tool for molecular orbital imaging<|sep|>We explore the laser-induced ionization dynamics of N2 and CO2 molecules subjected to a few-cycle, linearly polarized, 800\,nm laser pulse using effective two-dimensional single active electron time-dependent quantum simulations. We show that the electron recollision process taking place after an initial tunnel ionization stage results in quantum interference patterns in the energy resolved photo-electron signals. If the molecule is initially aligned perpendicular to the field polarization, the position and relative heights of the associated fringes can be related to the molecular geometrical and orbital structure, using a simple inversion algorithm which takes into account the symmetry of the initial molecular orbital from which the ionized electron is produced. We show that it is possible to extract inter-atomic distances in the molecule from an averaged photon-electron signal with an accuracy of a few percents.
Hall drift and the braking indices of young pulsars<|sep|>Braking index measurements of young radio pulsars are all smaller than the value expected for spin down by magnetic dipole braking. We investigate magnetic field evolution in the neutron star crust due to Hall drift as an explanation for observed braking indices. Using numerical simulations and a semi-analytic model, we show that a $\approx 10^{14}\ {\rm G}$ quadrupolar toroidal field in the neutron star crust at birth leads to growth of the dipole moment at a rate large enough to agree with measured braking indices. A key factor is the density at which the crust yields to magnetic stresses that build up during the evolution, which sets a characteristic minimum Hall timescale. The observed braking indices of pulsars with inferred dipole fields of $\lesssim 10^{13}\ {\rm G}$ can be explained in this picture, although with a significant octupole component needed in some cases. For the stronger field pulsars, those with $B_d\gtrsim 10^{13}\ {\rm G}$, we find that the magnetic stresses in the crust exceed the maximum shear stress before the pulsar reaches its current age, likely quenching the Hall effect. This may have implications for the magnetar activity seen in the high magnetic field radio pulsar PSR~J1846-0258. Observations of braking indices may therefore be a new piece of evidence that neutron stars contain subsurface toroidal fields that are significantly stronger than the dipole field, and may indicate that the Hall effect is important in a wider range of neutron stars than previously thought.
Latest Results of the OSQAR Photon Regeneration Experiment for Axion-Like Particle Search<|sep|>The OSQAR photon regeneration experiment searches for pseudoscalar and scalar axion-like particles by the method of "Light Shining Through a Wall", based on the assumption that these weakly interacting sub-eV particles couple to two photons to give rise to quantum oscillations with optical photons in strong magnetic field. No excess of events has been observed, which constrains the di-photon coupling strength of both pseudoscalar and scalar particles down to $5.7 \cdot 10^{-8}$ GeV$^{-1}$ in the massless limit. This result is the most stringent constraint on the di-photon coupling strength ever achieved in laboratory experiments.
Radiative and Non-Radiative Exciton Energy Transfer in Monolayers of Two-Dimensional Transition Metal Dichalcogenides<|sep|>We present results on the rates of interlayer energy transfer between excitons in two-dimensional transition metal dichalcogenides (TMDs). We consider both radiative (mediated by real photons) and non-radiative (mediated by virtual photons) mechanisms of energy transfer using a unified Green's function approach that takes into account modification of the exciton energy dispersions as a result of interactions. The large optical oscillator strengths associated with excitons in TMDs result in very fast energy transfer rates. The energy transfer times depend on the exciton momentum, exciton linewidth, and the interlayer separation and can range from values less than 100 femtoseconds to more than tens of picoseconds. Whereas inside the light cone the energy transfer rates of longitudinal and transverse excitons are comparable, outside the light cone the energy transfer rates of longitudinal excitons far exceed those of transverse excitons. Average energy transfer times for a thermal ensemble of longitudinal and transverse excitons is temperature dependent and can be smaller than a picosecond at room temperature for interlayer separations smaller than 10 nm. Energy transfer times of localized excitons range from values less than a picosecond to several tens of picoseconds. When the exciton scattering and dephasing rates are small, energy transfer dynamics exhibit coherent oscillations. Our results show that electromagnetic interlayer energy transfer can be an efficient mechanism for energy exchange between TMD monolayers.
A Fully Numerical Approach to One-Loop Amplitudes<|sep|>We suggest a new approach for the automatic and fully numerical evaluation of one-loop scattering amplitudes in perturbative quantum field theory. We use suitably formulated dispersion relations to perform the calculation as a convolution of tree-level amplitudes. This allows to take advantage of the iterative numerical algorithms for the evaluation of leading order matrix elements.
Towards Fine-grained Image Classification with Generative Adversarial Networks and Facial Landmark Detection<|sep|>Fine-grained classification remains a challenging task because distinguishing categories needs learning complex and local differences. Diversity in the pose, scale, and position of objects in an image makes the problem even more difficult. Although the recent Vision Transformer models achieve high performance, they need an extensive volume of input data. To encounter this problem, we made the best use of GAN-based data augmentation to generate extra dataset instances. Oxford-IIIT Pets was our dataset of choice for this experiment. It consists of 37 breeds of cats and dogs with variations in scale, poses, and lighting, which intensifies the difficulty of the classification task. Furthermore, we enhanced the performance of the recent Generative Adversarial Network (GAN), StyleGAN2-ADA model to generate more realistic images while preventing overfitting to the training set. We did this by training a customized version of MobileNetV2 to predict animal facial landmarks; then, we cropped images accordingly. Lastly, we combined the synthetic images with the original dataset and compared our proposed method with standard GANs augmentation and no augmentation with different subsets of training data. We validated our work by evaluating the accuracy of fine-grained image classification on the recent Vision Transformer (ViT) Model.
Coulomb solutions from improper pseudo-unitary free gauge field operator translations<|sep|>Fundamental problems of quantum field theory related to the representation problem of canonical commutation relations are discussed within a gauge field version of a van Hove-type model. The Coulomb field generated by a static charge distribution is described as a formal superposition of time-like pseudo-photons in Fock space with a Krein structure. In this context, a generalization of operator gauge transformations is introduced to generate coherent states of abelian gauge fields interacting with a charged background.
On the Study of Sample Complexity for Polynomial Neural Networks<|sep|>As a general type of machine learning approach, artificial neural networks have established state-of-art benchmarks in many pattern recognition and data analysis tasks. Among various kinds of neural networks architectures, polynomial neural networks (PNNs) have been recently shown to be analyzable by spectrum analysis via neural tangent kernel, and particularly effective at image generation and face recognition. However, acquiring theoretical insight into the computation and sample complexity of PNNs remains an open problem. In this paper, we extend the analysis in previous literature to PNNs and obtain novel results on sample complexity of PNNs, which provides some insights in explaining the generalization ability of PNNs.
Intelligent GPS Spoofing Attack Detection in Power Grids<|sep|>The GPS is vulnerable to GPS spoofing attack (GSA), which leads to disorder in time and position results of the GPS receiver. In power grids, phasor measurement units (PMUs) use GPS to build time-tagged measurements, so they are susceptible to this attack. As a result of this attack, sampling time and phase angle of the PMU measurements change. In this paper, a neural network GPS spoofing detection (NNGSD) with employing PMU data from the dynamic power system is presented to detect GSAs. Numerical results in different conditions show the real-time performance of the proposed detection method.
Thermal luminosities of cooling neutron stars<|sep|>Ages and thermal luminosities of neutron stars, inferred from observations, can be interpreted with the aid of the neutron star cooling theory to gain information on the properties of superdense matter in neutron-star interiors. We present a survey of estimated ages, surface temperatures and thermal luminosities of middle-aged neutron stars with relatively weak or moderately strong magnetic fields, which can be useful for these purposes. The catalogue includes results selected from the literature, supplemented with new results of spectral analysis of a few cooling neutron stars. The data are compared with the theory. We show that overall agreement of theoretical cooling curves with observations improves substantially for models where neutron superfluidity in stellar core is weak.
Thermal machines beyond the weak coupling regime<|sep|>How much work can be extracted from a heat bath using a thermal machine? The study of this question has a very long tradition in statistical physics in the weak-coupling limit, applied to macroscopic systems. However, the assumption that thermal heat baths remain uncorrelated with physical systems at hand is less reasonable on the nano-scale and in the quantum setting. In this work, we establish a framework of work extraction in the presence of quantum correlations. We show in a mathematically rigorous and quantitative fashion that quantum correlations and entanglement emerge as a limitation to work extraction compared to what would be allowed by the second law of thermodynamics. At the heart of the approach are operations that capture naturally non-equilibrium dynamics encountered when putting physical systems into contact with each other. We discuss various limits that relate to known results and put our work into context of approaches to finite-time quantum thermodynamics.
High dimensional generalized empirical likelihood for moment restrictions with dependent data<|sep|>This paper considers the maximum generalized empirical likelihood (GEL) estimation and inference on parameters identified by high dimensional moment restrictions with weakly dependent data when the dimensions of the moment restrictions and the parameters diverge along with the sample size. The consistency with rates and the asymptotic normality of the GEL estimator are obtained by properly restricting the growth rates of the dimensions of the parameters and the moment restrictions, as well as the degree of data dependence. It is shown that even in the high dimensional time series setting, the GEL ratio can still behave like a chi-square random variable asymptotically. A consistent test for the over-identification is proposed. A penalized GEL method is also provided for estimation under sparsity setting.
Depthwise Separable Convolutional ResNet with Squeeze-and-Excitation Blocks for Small-footprint Keyword Spotting<|sep|>One difficult problem of keyword spotting is how to miniaturize its memory footprint while maintain a high precision. Although convolutional neural networks have shown to be effective to the small-footprint keyword spotting problem, they still need hundreds of thousands of parameters to achieve good performance. In this paper, we propose an efficient model based on depthwise separable convolution layers and squeeze-and-excitation blocks. Specifically, we replace the standard convolution by the depthwise separable convolution, which reduces the number of the parameters of the standard convolution without significant performance degradation. We further improve the performance of the depthwise separable convolution by reweighting the output feature maps of the first convolution layer with a so-called squeeze-and-excitation block. We compared the proposed method with five representative models on two experimental settings of the Google Speech Commands dataset. Experimental results show that the proposed method achieves the state-of-the-art performance. For example, it achieves a classification error rate of 3.29% with a number of parameters of 72K in the first experiment, which significantly outperforms the comparison methods given a similar model size. It achieves an error rate of 3.97% with a number of parameters of 10K, which is also slightly better than the state-of-the-art comparison method given a similar model size.
A Framework of Sparse Online Learning and Its Applications<|sep|>The amount of data in our society has been exploding in the era of big data today. In this paper, we address several open challenges of big data stream classification, including high volume, high velocity, high dimensionality, high sparsity, and high class-imbalance. Many existing studies in data mining literature solve data stream classification tasks in a batch learning setting, which suffers from poor efficiency and scalability when dealing with big data. To overcome the limitations, this paper investigates an online learning framework for big data stream classification tasks. Unlike some existing online data stream classification techniques that are often based on first-order online learning, we propose a framework of Sparse Online Classification (SOC) for data stream classification, which includes some state-of-the-art first-order sparse online learning algorithms as special cases and allows us to derive a new effective second-order online learning algorithm for data stream classification. In addition, we also propose a new cost-sensitive sparse online learning algorithm by extending the framework with application to tackle online anomaly detection tasks where class distribution of data could be very imbalanced. We also analyze the theoretical bounds of the proposed method, and finally conduct an extensive set of experiments, in which encouraging results validate the efficacy of the proposed algorithms in comparison to a family of state-of-the-art techniques on a variety of data stream classification tasks.
Optimistic Temporal Difference Learning for 2048<|sep|>Temporal difference (TD) learning and its variants, such as multistage TD (MS-TD) learning and temporal coherence (TC) learning, have been successfully applied to 2048. These methods rely on the stochasticity of the environment of 2048 for exploration. In this paper, we propose to employ optimistic initialization (OI) to encourage exploration for 2048, and empirically show that the learning quality is significantly improved. This approach optimistically initializes the feature weights to very large values. Since weights tend to be reduced once the states are visited, agents tend to explore those states which are unvisited or visited few times. Our experiments show that both TD and TC learning with OI significantly improve the performance. As a result, the network size required to achieve the same performance is significantly reduced. With additional tunings such as expectimax search, multistage learning, and tile-downgrading technique, our design achieves the state-of-the-art performance, namely an average score of 625 377 and a rate of 72% reaching 32768 tiles. In addition, for sufficiently large tests, 65536 tiles are reached at a rate of 0.02%.
Gaia eclipsing binary and multiple systems. Supervised classification and self-organizing maps<|sep|>Large surveys producing tera- and petabyte-scale databases require machine-learning and knowledge discovery methods to deal with the overwhelming quantity of data and the difficulties of extracting concise, meaningful information with reliable assessment of its uncertainty. This study investigates the potential of a few machine-learning methods for the automated analysis of eclipsing binaries in the data of such surveys. We aim to aid the extraction of samples of eclipsing binaries from such databases and to provide basic information about the objects. We estimate class labels according to two classification systems, one based on the light curve morphology (EA/EB/EW classes) and the other based on the physical characteristics of the binary system (system morphology classes; detached through overcontact systems). Furthermore, we explore low-dimensional surfaces along which the light curves of eclipsing binaries are concentrated, to use in the characterization of the binary systems and in the exploration of biases of the full unknown Gaia data with respect to the training sets. We explore the performance of principal component analysis (PCA), linear discriminant analysis (LDA), random forest classification and self-organizing maps (SOM). We pre-process the photometric time series by combining a double Gaussian profile fit and a smoothing spline, in order to de-noise and interpolate the observed light curves. We achieve further denoising, and selected the most important variability elements from the light curves using PCA. We perform supervised classification using random forest and LDA based on the PC decomposition, while SOM gives a continuous 2-dimensional manifold of the light curves arranged by a few important features. We estimate the uncertainty of the supervised methods due to the specific finite training set using ensembles of models constructed on randomized training sets.
Uncertainties in the solar photospheric oxygen abundance<|sep|>The purpose of this work is to better understand the confidence limits of the photospheric solar oxygen abundance derived from three-dimensional models using the forbidden [OI] line at 6300 \AA , including correlations with other parameters involved. We worked with a three-dimensional empirical model and two solar intensity atlases. We employed Bayesian inference as a tool to determine the most probable value for the solar oxygen abundance given the model chosen. We considered a number of error sources, such as uncertainties in the continuum derivation, in the wavelength calibration and in the abundance/strength of Ni. Our results shows correlations between the effects of several parameters employed in the derivation. The Bayesian analysis provides robust confidence limits taking into account all of these factors in a rigorous manner. We obtain that, given the empirical three-dimensional model and the atlas observations employed here, the most probable value for the solar oxygen abundance is $\log(\epsilon_O) = 8.86\pm0.04$. However, we note that this uncertainty does not consider possible sources of systematic errors due to the model choice.
VLBI constraints on the "jet-line" of Cygnus X-1<|sep|>Results are presented from recent VLBI observations of Cygnus X-1 during X-ray spectral state changes. Using the EVN in e-VLBI mode and the VLBA with disk recording, we observed the X-ray binary at very high angular resolution and studied changes in the compact jets as the source made transitions from hard X-ray states to softer states. The radio light curves show that these transitions were accompanied by radio flaring events followed by a quenching of the radio emission, as expected from the current paradigm for disc-jet coupling in X-ray binaries. While we see structural changes in the compact jets during these transitions, there was no evidence for the expected ejection of bright, relativistically-moving jet knots. However, we find strong evidence that the jet does not switch off completely in the soft X-ray state of Cygnus X-1, such that a weak, compact jet persists during this phase of radio quenching.
Evidence for a Preferred Handedness of Spiral Galaxies<|sep|>In this article I extend an earlier study of spiral galaxies in the Sloan Digital Sky Survey (SDSS) to investigate whether the universe has an overall handedness. A preference for spiral galaxies in one sector of the sky to be left-handed or right-handed spirals would indicate a parity-violating asymmetry in the overall universe and a preferred axis. The previous study used 2616 spiral galaxies with redshifts <0.04 and identified handedness. The new study uses 15158 with redshifts <0.085 and obtains very similar results to the first with a signal exceeding 5 sigma, corresponding to a probability ~2.5x10-7 for occurring by chance. A similar asymmetry is seen in the Southern Galaxy spin catalog of Iye and Sugai. The axis of the dipole asymmetry lies at approx. (l, b) =(52 d, 68.5 d), roughly along that of our Galaxy and close to alignments observed in the WMAP cosmic microwave background distributions.
Large-girth roots of graphs<|sep|>We study the problem of recognizing graph powers and computing roots of graphs. We provide a polynomial time recognition algorithm for r-th powers of graphs of girth at least 2r+3, thus improving a bound conjectured by Farzad et al. (STACS 2009). Our algorithm also finds all r-th roots of a given graph that have girth at least 2r+3 and no degree one vertices, which is a step towards a recent conjecture of Levenshtein that such root should be unique. On the negative side, we prove that recognition becomes an NP-complete problem when the bound on girth is about twice smaller. Similar results have so far only been attempted for r=2,3.
Spectral Parameters for Scattering Amplitudes in N=4 Super Yang-Mills Theory<|sep|>Planar N=4 Super Yang-Mills theory appears to be a quantum integrable four-dimensional conformal theory. This has been used to find equations believed to describe its exact spectrum of anomalous dimensions. Integrability seemingly also extends to the planar space-time scattering amplitudes of the N=4 model, which show strong signs of Yangian invariance. However, in contradistinction to the spectral problem, this has not yet led to equations determining the exact amplitudes. We propose that the missing element is the spectral parameter, ubiquitous in integrable models. We show that it may indeed be included into recent on-shell approaches to scattering amplitude integrands, providing a natural deformation of the latter. Under some constraints, Yangian symmetry is preserved. Finally we speculate that the spectral parameter might also be the regulator of choice for controlling the infrared divergences appearing when integrating the integrands in exactly four dimensions.
Microsecond Consensus for Microsecond Applications<|sep|>We consider the problem of making apps fault-tolerant through replication, when apps operate at the microsecond scale, as in finance, embedded computing, and microservices apps. These apps need a replication scheme that also operates at the microsecond scale, otherwise replication becomes a burden. We propose Mu, a system that takes less than 1.3 microseconds to replicate a (small) request in memory, and less than a millisecond to fail-over the system - this cuts the replication and fail-over latencies of the prior systems by at least 61% and 90%. Mu implements bona fide state machine replication/consensus (SMR) with strong consistency for a generic app, but it really shines on microsecond apps, where even the smallest overhead is significant. To provide this performance, Mu introduces a new SMR protocol that carefully leverages RDMA. Roughly, in Mu a leader replicates a request by simply writing it directly to the log of other replicas using RDMA, without any additional communication. Doing so, however, introduces the challenge of handling concurrent leaders, changing leaders, garbage collecting the logs, and more - challenges that we address in this paper through a judicious combination of RDMA permissions and distributed algorithmic design. We implemented Mu and used it to replicate several systems: a financial exchange app called Liquibook, Redis, Memcached, and HERD. Our evaluation shows that Mu incurs a small replication latency, in some cases being the only viable replication system that incurs an acceptable overhead.
Analysis of air pollution time series using complexity-invariant distance and information measures<|sep|>Air pollution is known to be a major threat for human and ecosystem health. A proper understanding of the factors generating pollution and of the behavior of air pollution in time is crucial to support the development of effective policies aiming at the reduction of pollutant concentration. This paper considers the hourly time series of three pollutants, namely NO$_2$, O$_3$ and PM$_{2.5}$, collected on sixteen measurement stations in Switzerland. The air pollution patterns due to the location of measurement stations and their relationship with anthropogenic activities, and specifically land use, are studied using two approaches: Fisher-Shannon information plane and complexity-invariant distance between time series. A clustering analysis is used to recognize within the measurements of a same pollutant group of stations behaving in a similar way. The results clearly demonstrate the relationship between the air pollution probability densities and land use activities.
Plane Pair Matching for Efficient 3D View Registration<|sep|>We present a novel method to estimate the motion matrix between overlapping pairs of 3D views in the context of indoor scenes. We use the Manhattan world assumption to introduce lightweight geometric constraints under the form of planes into the problem, which reduces complexity by taking into account the structure of the scene. In particular, we define a stochastic framework to categorize planes as vertical or horizontal and parallel or non-parallel. We leverage this classification to match pairs of planes in overlapping views with point-of-view agnostic structural metrics. We propose to split the motion computation using the classification and estimate separately the rotation and translation of the sensor, using a quadric minimizer. We validate our approach on a toy example and present quantitative experiments on a public RGB-D dataset, comparing against recent state-of-the-art methods. Our evaluation shows that planar constraints only add low computational overhead while improving results in precision when applied after a prior coarse estimate. We conclude by giving hints towards extensions and improvements of current results.
A First Runtime Analysis of the NSGA-II on a Multimodal Problem<|sep|>Very recently, the first mathematical runtime analyses of the multi-objective evolutionary optimizer NSGA-II have been conducted (AAAI 2022, GECCO 2022 (to appear), arxiv 2022). We continue this line of research with a first runtime analysis of this algorithm on a benchmark problem consisting of two multimodal objectives. We prove that if the population size $N$ is at least four times the size of the Pareto front, then the NSGA-II with four different ways to select parents and bit-wise mutation optimizes the OneJumpZeroJump benchmark with jump size~$2 \le k \le n/4$ in time $O(N n^k)$. When using fast mutation, a recently proposed heavy-tailed mutation operator, this guarantee improves by a factor of $k^{\Omega(k)}$. Overall, this work shows that the NSGA-II copes with the local optima of the OneJumpZeroJump problem at least as well as the global SEMO algorithm.
Driven Transport on open filaments with inter-filament switching processes<|sep|>We study a two filament driven lattice gas model with oppositely directed species of particles moving on two parallel filaments with filament switching processes and particle inflow and outflow at filament ends. The filament switching process is {\it correlated} such that particles switch filaments with finite probability only when oppositely directed particles meet on the same filament. This model mimics some of the coarse grained features observed in context of microtubule (MT) based intracellular transport, wherein cellular cargo loaded and off-loaded at filament ends are transported on multiple parallel microtubule (MT) filaments and can switch between the parallel microtubule filaments. We focus on a regime where the filaments are weakly coupled, such that filament switching rates scale inversely as the length of the filament. We find that the interplay (off)loading processes at the boundaries and the filament switching process leads to some distinctive features of the system. These features includes occurrence of variety of phases in the system with inhomogeneous density profiles including localized density shocks, density difference across the filaments and bidirectional current flows in the system. We analyze the system by developing a mean field (MF) theory and comparing the results obtained from the MF theory with the Monte Carlo (MC) simulations of the dynamics of the system. We find that the steady state density and current profiles of particles and the phase diagram obtained within the MF picture matches quite well with MC simulation results. These findings maybe useful for studying multi-filament intracellular transport.
Prema: A Tool for Precise Requirements Editing, Modeling and Analysis<|sep|>We present Prema, a tool for Precise Requirement Editing, Modeling and Analysis. It can be used in various fields for describing precise requirements using formal notations and performing rigorous analysis. By parsing the requirements written in formal modeling language, Prema is able to get a model which aptly depicts the requirements. It also provides different rigorous verification and validation techniques to check whether the requirements meet users' expectation and find potential errors. We show that our tool can provide a unified environment for writing and verifying requirements without using tools that are not well inter-related. For experimental demonstration, we use the requirements of the automatic train protection (ATP) system of CASCO signal co. LTD., the largest railway signal control system manufacturer of China. The code of the tool cannot be released here because the project is commercially confidential. However, a demonstration video of the tool is available at https://youtu.be/BX0yv8pRMWs.
Debugging Memory Issues In Embedded Linux: A Case Study<|sep|>Debugging denotes the process of detecting root causes of unexpected observable behaviors in programs, such as a program crash, an unexpected output value being produced or an assertion violation. Debugging of program errors is a difficult task and often takes a significant amount of time in the software development life cycle. In the context of embedded software, the probability of bugs is quite high. Due to requirements of low code size and less resource consumption, embedded softwares typically do away with a lot of sanity checks during development time. This leads to high chance of errors being uncovered in the production code at run time. In this paper we propose a methodology for debugging errors in BusyBox, a de-facto standard for Linux in embedded systems. Our methodology works on top of Valgrind, a popular memory error detector and Daikon, an invariant analyzer. We have experimented with two published errors in BusyBox and report our findings in this paper.
Gradient Projection for Solving Quadratic Programs with Standard Simplex Constraints<|sep|>An important method to optimize a function on standard simplex is the active set algorithm, which requires the gradient of the function to be projected onto a hyperplane, with sign constraints on the variables that lie in the boundary of the simplex. We propose a new algorithm to efficiently project the gradient for this purpose. Furthermore, we apply the proposed gradient projection method to quadratic programs (QP) with standard simplex constraints, where gradient projection is used to explore the feasible region and, when we believe the optimal active set is identified, we switch to constrained conjugate gradient to accelerate convergence. Specifically, two different directions of gradient projection are used to explore the simplex, namely, the projected gradient and the reduced gradient. We choose one of the two directions according to the angle between the directions. Moreover, we propose two conditions for guessing the optimal active set heuristically. The first condition is that the working set remains unchanged for many iterations, and the second condition is that the angle between the projected gradient and the reduced gradient is small enough. Based on these strategies, a new active set algorithm for solving quadratic programs on standard simplex is proposed.
Modeling the Lukewarm Corino Phase - is L1527 unique?<|sep|>Sakai et al. have observed long-chain unsaturated hydrocarbons and cyanopolyynes in the low-mass star-forming region L1527, and have attributed this result to a gas-phase ion-molecule chemistry, termed ``Warm Carbon Chain Chemistry'', which occurs during and after the evaporation of methane from warming grains. The source L1527 is an envelope surrounding a Class 0/I protostar with regions that possess a slightly elevated temperature of ~30 K. The molecules detected by Sakai et al. are typically associated only with dark molecular clouds, and not with the more evolved hot corino phase. In order to determine if L1527 is chemically distinct from a dark cloud, we compute models including various degrees of heating. The results indicate that the composition of L1527 is somewhat more likely to be due to ``Warm Carbon Chain Chemistry'' than to be a remnant of a colder phase. If so, the molecular products provide a signature of a previously uncharacterized early phase of low mass star formation, which can be characterized as a ``lukewarm'' corino. We also include predictions for other molecular species that might be observed toward candidate lukewarm corino sources. Although our calculations show that unsaturated hydrocarbons and cyanopolyynes can be produced in the gas phase as the grains warm up to 30 K, they also show that such species do not disappear rapidly from the gas as the temperature reaches 200 K, implying that such species might be detected in hot corinos and hot cores.
Quantum plasmonic excitation in graphene and robust-to-loss propagation<|sep|>We investigate the excitation of quantum plasmonic states of light in graphene using end-fire and prism coupling. In order to model the excitation process quantum mechanically we quantize the transverse-electric and transverse-magnetic surface plasmon polariton (SPP) modes in graphene. A selection of regimes are then studied that enable the excitation of SPPs by photons and we show that efficient coupling of photons to graphene SPPs is possible at the quantum level. Futhermore, we study the excitation of quantum states and their propagation under the effects of loss induced from the electronic degrees of freedom in the graphene. Here, we investigate whether it is possible to protect quantum information using quantum error correction techniques. We find that these techniques provide a robust-to-loss method for transferring quantum states of light in graphene over large distances.
Mean field dynamo action in renovating shearing flows<|sep|>We study mean field dynamo action in renovating flows with finite and non zero correlation time ($\tau$) in the presence of shear. Previous results obtained when shear was absent are generalized to the case with shear. The question of whether the mean magnetic field can grow in the presence of shear and non helical turbulence, as seen in numerical simulations, is examined. We show in a general manner that, if the motions are strictly non helical, then such mean field dynamo action is not possible. This result is not limited to low (fluid or magnetic) Reynolds numbers nor does it use any closure approximation; it only assumes that the flow renovates itself after each time interval $\tau$. Specifying to a particular form of the renovating flow with helicity, we recover the standard dispersion relation of the $\alpha^2 \Omega$ dynamo, in the small $\tau$ or large wavelength limit. Thus mean fields grow even in the presence of rapidly growing fluctuations, surprisingly, in a manner predicted by the standard quasilinear closure, even though such a closure is not strictly justified. Our work also suggests the possibility of obtaining mean field dynamo growth in the presence of helicity fluctuations, although having a coherent helicity will be more efficient.
Improving Neural Machine Translation Models with Monolingual Data<|sep|>Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.
Stable chaos<|sep|>Stable chaos is a generalization of the chaotic behaviour exhibited by cellular automata to continuous-variable systems and it owes its name to an underlying irregular and yet linearly stable dynamics. In this review we discuss analogies and differences with the usual deterministic chaos and introduce several tools for its characterization. Some examples of transitions from ordered behavior to stable chaos are also analyzed to further clarify the underlying dynamical properties. Finally, two models are specifically discussed: the diatomic hard-point gas chain and a network of globally coupled neurons.
The quantum Allan variance<|sep|>The instability of an atomic clock is characterized by the Allan variance, a measure widely used to describe the noise of frequency standards. We provide an explicit method to find the ultimate bound on the Allan variance of an atomic clock in the most general scenario where N atoms are prepared in an arbitrarily entangled state and arbitrary measurement and feedback are allowed, including those exploiting coherences between succeeding interrogation steps. While the method is rigorous and general, it becomes numerically challenging for large N and long averaging times.
Embeddings for Schwarzschild metric: classification and new results<|sep|>We suggest a method to search the embeddings of Riemannian spaces with a high enough symmetry in a flat ambient space. It is based on a procedure of construction surfaces with a given symmetry. The method is used to classify the embeddings of the Schwarzschild metric which have the symmetry of this solution, and all such embeddings in a six-dimensional ambient space (i.e. a space with a minimal possible dimension) are constructed. Four of the six possible embeddings are already known, while the two others are new. One of the new embeddings is asymptotically flat, while the other embeddings in a six-dimensional ambient space do not have this property. The asymptotically flat embedding can be of use in the analysis of the many-body problem, as well as for the development of gravity description as a theory of a surface in a flat ambient space.
Automated Speed and Lane Change Decision Making using Deep Reinforcement Learning<|sep|>This paper introduces a method, based on deep reinforcement learning, for automatically generating a general purpose decision making function. A Deep Q-Network agent was trained in a simulated environment to handle speed and lane change decisions for a truck-trailer combination. In a highway driving case, it is shown that the method produced an agent that matched or surpassed the performance of a commonly used reference model. To demonstrate the generality of the method, the exact same algorithm was also tested by training it for an overtaking case on a road with oncoming traffic. Furthermore, a novel way of applying a convolutional neural network to high level input that represents interchangeable objects is also introduced.
Summary of the CKM 2010 Working Group on Rare Decays<|sep|>Rare decays were essential in the discovery of the CKM mechanism of flavour and CP violation and are highly sensitive probes of physics beyond the Standard Model. In this summary the current status and future prospects of experimental measurements and the Standard Model theory predictions of various rare B, D and K decay observables are discussed. The specific new physics sensitivities of each mode are also briefly reviewed.
Investigating the Role of Prior Disambiguation in Deep-learning Compositional Models of Meaning<|sep|>This paper aims to explore the effect of prior disambiguation on neural network- based compositional models, with the hope that better semantic representations for text compounds can be produced. We disambiguate the input word vectors before they are fed into a compositional deep net. A series of evaluations shows the positive effect of prior disambiguation for such deep models.
A Linear Belief Function Approach to Portfolio Evaluation<|sep|>By elaborating on the notion of linear belief functions (Dempster 1990; Liu 1996), we propose an elementary approach to knowledge representation for expert systems using linear belief functions. We show how to use basic matrices to represent market information and financial knowledge, including complete ignorance, statistical observations, subjective speculations, distributional assumptions, linear relations, and empirical asset pricing models. We then appeal to Dempster's rule of combination to integrate the knowledge for assessing an overall belief of portfolio performance, and updating the belief by incorporating additional information. We use an example of three gold stocks to illustrate the approach.
Gravitational wave signatures from kink proliferation on cosmic (super-) strings<|sep|>Junctions on cosmic string loops give rise to the proliferation of sharp kinks. We study the effect of this proliferation on the gravitational wave (GW) signals emitted from string networks with junctions, assuming a scaling solution. We calculate the rate of occurrence and the distribution in amplitude of the GW bursts emitted at cusps and kinks in the frequency bands of LIGO and LISA as a function of the string tension, the number of sharp kinks on loops with junctions and the fraction of loops in the cosmological network which have junctions. Combining our results with current observational constraints, we find that pulsar data rule out a significant number of kinks on loops for strings with tensions G\mu > 10^{-12}. By contrast, for smaller tensions current observations allow for a large number of kinks on loops. If this is the case, the incoherent superposition of small bursts emitted at kink-kink encounters leads to an enhanced GW background that hides the strong individual bursts from kinks and cusps.
On the onset of runaway stellar collisions in dense star clusters I. Dynamics of the first collision<|sep|>We study the circumstances under which first collisions occur in young and dense star clusters. The initial conditions for our direct $N$-body simulations are chosen such that the clusters experience core collapse within a few million years, before the most massive stars have left the main-sequence. It turns out that the first collision is typically driven by the most massive stars in the cluster. Upon arrival in the cluster core, by dynamical friction, massive stars tend to form binaries. The enhanced cross section of the binary compared to a single star causes other stars to engage the binary. A collision between one of the binary components and the incoming third star is then mediated by the encounters between the binary and other cluster members. Due to the geometry of the binary-single star engagement the relative velocity at the moment of impact is substantially different than in a two-body encounter. This may have profound consequences for the further evolution of the collision product.
Gaugephobic Higgs Signals at the LHC<|sep|>The Gaugephobic Higgs model provides an interpolation between three different models of electroweak symmetry breaking: Higgsless models, Randall-Sundrum models, and the Standard Model. At parameter points between the extremes, Standard Model Higgs signals are present at reduced rates, and Higgsless Kaluza-Klein excitations are present with shifted masses and couplings, as well as signals from exotic quarks necessary to protect the Zbb coupling. Using a new implementation of the model in SHERPA, we show the LHC signals which differentiate the generic Gaugephobic Higgs model from its limiting cases. These are all signals involving a Higgs coupling to a Kaluza-Klein gauge boson or quark. We identify the clean signal $p p \to W^(i) \to W H$ mediated by a Kaluza-Klein W, which can be present at large rates and is enhanced for even Kaluza-Klein numbers. Due to the very hard lepton coming from the W decay, this signature has little background, and provides a better discovery channel for the Higgs than any of the Standard Model modes, over its entire mass range. A Higgs radiated from new heavy quarks also has large rates, but is much less promising due to very high multiplicity final states.
COPAR - Multivariate time series modeling using the COPula AutoRegressive model<|sep|>Analysis of multivariate time series is a common problem in areas like finance and economics. The classical tool for this purpose are vector autoregressive models. These however are limited to the modeling of linear and symmetric dependence. We propose a novel copula-based model which allows for non-linear and asymmetric modeling of serial as well as between-series dependencies. The model exploits the flexibility of vine copulas which are built up by bivariate copulas only. We describe statistical inference techniques for the new model and demonstrate its usefulness in three relevant applications: We analyze time series of macroeconomic indicators, of electricity load demands and of bond portfolio returns.
Extensional Models of Untyped Lambda-mu Calculus<|sep|>This paper proposes new mathematical models of the untyped Lambda-mu calculus. One is called the stream model, which is an extension of the lambda model, in which each term is interpreted as a function from streams to individual data. The other is called the stream combinatory algebra, which is an extension of the combinatory algebra, and it is proved that the extensional equality of the Lambda-mu calculus is equivalent to equality in stream combinatory algebras. In order to define the stream combinatory algebra, we introduce a combinatory calculus SCL, which is an abstraction-free system corresponding to the Lambda-mu calculus. Moreover, it is shown that stream models are algebraically characterized as a particular class of stream combinatory algebras.
Climbing the N-shell resonance ladder of xenon<|sep|>The dependency on the excitation energy of ultrafast multi-photon ionization of xenon by intense, short extreme ultraviolet pulses (XUV) was investigated in the vicinity of the 4$d$ 'giant' resonance using ion time-of-flight spectroscopy. The yields of the high charge states of xenon show strong variations with the excitation energy. With reference to simulated absorption spectra, we can link the photon energy dependency to resonance structures of single-electron excitations mainly in the xenon N-shell and purely sequential multi-photon absorption.
Dynamic Role Binding in Blockchain-Based Collaborative Business Processes<|sep|>Blockchain technology enables the execution of collaborative business processes involving mutually untrusted parties. Existing platforms allow such processes to be modeled using high-level notations and compiled into smart contracts that can be deployed on blockchain platforms. However, these platforms brush aside the question of who is allowed to execute which tasks in the process, either by deferring the question altogether or by adopting a static approach where all actors are bound to roles upon process instantiation. Yet, a key advantage of blockchains is their ability to support dynamic sets of actors. This paper presents a model for dynamic binding of actors to roles in collaborative processes and an associated binding policy specification language. The proposed language is endowed with a Petri net semantics, thus enabling policy consistency verification. The paper also outlines an approach to compile policy specifications into smart contracts for enforcement. An experimental evaluation shows that the cost of policy enforcement increases linearly with the number of roles and constraints.
Prediction-Correction for Nonsmooth Time-Varying Optimization via Forward-Backward Envelopes<|sep|>We present an algorithm for minimizing the sum of a strongly convex time-varying function with a time-invariant, convex, and nonsmooth function. The proposed algorithm employs the prediction-correction scheme alongside the forward-backward envelope, and we are able to prove the convergence of the solutions to a neighborhood of the optimizer that depends on the sampling time. Numerical simulations for a time-varying regression problem with elastic net regularization highlight the effectiveness of the algorithm.
Electromagnetic Field and Cylindrical Compact Objects in Modified Gravity<|sep|>In this paper, we have investigated the role of different fluid parameters particularly electromagnetic field and $f(R)$ corrections on the evolution of cylindrical compact object. We have explored the modified field equations, kinematical quantities and dynamical equations. An expression for the mass function has been found in comparison with the Misner-Sharp formalism in modified gravity, after which different mass radius diagrams are drawn. The coupled dynamical transport equation have been formulated to discuss the role of thermoinertial effects on the inertial mass density of the cylindrical relativistic interior. Finally, we have presented a framework, according to which all possible solutions of the metric $f(R)$-Maxwell field equations coupled with static fluid can be written through set of scalar functions. It is found that modified gravity induced by Lagrangians $f(R)=\alpha R^2,~f(R)=\alpha R^2-\beta R$ and $f(R)=\frac{\alpha R^2-\beta R}{1+\gamma R}$ are likely to host more massive cylindrical compact objects with smaller radii as compared to GR.
Data Fusion on Motion and Magnetic Sensors embedded on Mobile Devices for the Identification of Activities of Daily Living<|sep|>Several types of sensors have been available in off-the-shelf mobile devices, including motion, magnetic, vision, acoustic, and location sensors. This paper focuses on the fusion of the data acquired from motion and magnetic sensors, i.e., accelerometer, gyroscope and magnetometer sensors, for the recognition of Activities of Daily Living (ADL) using pattern recognition techniques. The system developed in this study includes data acquisition, data processing, data fusion, and artificial intelligence methods. Artificial Neural Networks (ANN) are included in artificial intelligence methods, which are used in this study for the recognition of ADL. The purpose of this study is the creation of a new method using ANN for the identification of ADL, comparing three types of ANN, in order to achieve results with a reliable accuracy. The best accuracy was obtained with Deep Learning, which, after the application of the L2 regularization and normalization techniques on the sensors data, reports an accuracy of 89.51%.
A self-normalized approach to confidence interval construction in time series<|sep|>We propose a new method to construct confidence intervals for quantities that are associated with a stationary time series, which avoids direct estimation of the asymptotic variances. Unlike the existing tuning-parameter-dependent approaches, our method has the attractive convenience of being free of choosing any user-chosen number or smoothing parameter. The interval is constructed on the basis of an asymptotically distribution-free self-normalized statistic, in which the normalizing matrix is computed using recursive estimates. Under mild conditions, we establish the theoretical validity of our method for a broad class of statistics that are functionals of the empirical distribution of fixed or growing dimension. From a practical point of view, our method is conceptually simple, easy to implement and can be readily used by the practitioner. Monte-Carlo simulations are conducted to compare the finite sample performance of the new method with those delivered by the normal approximation and the block bootstrap approach.
Linear-Complexity Overhead-Optimized Random Linear Network Codes<|sep|>Sparse random linear network coding (SRLNC) is an attractive technique proposed in the literature to reduce the decoding complexity of random linear network coding. Recognizing the fact that the existing SRLNC schemes are not efficient in terms of the required reception overhead, we consider the problem of designing overhead-optimized SRLNC schemes. To this end, we introduce a new design of SRLNC scheme that enjoys very small reception overhead while maintaining the main benefit of SRLNC, i.e., its linear encoding/decoding complexity. We also provide a mathematical framework for the asymptotic analysis and design of this class of codes based on density evolution (DE) equations. To the best of our knowledge, this work introduces the first DE analysis in the context of network coding. Our analysis method then enables us to design network codes with reception overheads in the order of a few percent. We also investigate the finite-length performance of the proposed codes and through numerical examples we show that our proposed codes have significantly lower reception overheads compared to all existing linear-complexity random linear network coding schemes.
Limits on the Mass and Abundance of Primordial Black Holes from Quasar Gravitational Microlensing<|sep|>The idea that dark matter can be made of intermediate-mass primordial black holes in the $10M_\odot \lesssim M \lesssim 200M_\odot$ range has recently been reconsidered, particularly in the light of the detection of gravitational waves by the LIGO experiment. The existence of even a small fraction of dark matter in black holes should nevertheless result in noticeable quasar gravitational microlensing. Quasar microlensing is sensitive to any type of compact objects in the lens galaxy, to their abundance, and to their mass. We have analyzed optical and X-ray microlensing data from 24 gravitationally lensed quasars to estimate the abundance of compact objects in a very wide range of masses. We conclude that the fraction of mass in black holes or any type of compact objects is negligible outside of the $0.05 M_\odot \lesssim M \lesssim 0.45 M_\odot$ mass range and that it amounts to $20 \pm5$% of the total matter, in agreement with the expected masses and abundances of the stellar component. Consequently, the existence of a significant population of intermediate-mass primordial black holes appears to be inconsistent with current microlensing observations. Therefore, primordial massive black holes are a very unlikely source of the gravitational radiation detected by LIGO.
Quantum supremacy with spin squeezed atomic ensembles<|sep|>We propose a method to achieve quantum supremacy using ensembles of qubits, using only spin squeezing, basis rotations, and Fock state measurements. Each ensemble is assumed to be controllable only with its total spin. Using a repeated sequence of random basis rotations followed by squeezing, we show that the probability distribution of the final measurements quickly approaches a Porter-Thomas distribution. We show that the sampling probability can be related to a #P-hard problem with a complexity scaling as $(N+1)^M$, where $N$ is the number of qubits in an ensemble and $ M $ is the number of ensembles. The scheme can be implemented with hot or cold atomic ensembles. Due to the large number of atoms in typical atomic ensembles, this allows access to the quantum supremacy regime with a modest number of ensembles or gate depth.
Perturbative criteria for Anderson localization in long-ranged 1D tight-binding models<|sep|>We develop an alternative scaling approach to determine the criteria for Anderson localization in one-dimensional tight-binding models with random site energies having a bandwidth that decays as a power law in space, $H_{ij} \propto |i - j|^{-\alpha}$. At the first order in perturbation theory the scale dependence of the exchange-narrowed energy of the disorder is compared to the energy level spacing of the ideal system to establish whether or not the disorder has a perturbative effect on the Bloch states. We find that at $\alpha =1$, the perturbative condition is satisfied and for sufficiently weak disorder strength all states are extended. For $\alpha > 1$, all states are localized for arbitrary disorder strength, in agreement with the earlier renormalization group treatment by Levitov.
Analysis of Contractions in System Graphs: Application to State Estimation<|sep|>Observability and estimation are closely tied to the system structure, which can be visualized as a system graph--a graph that captures the inter-dependencies within the state variables. For example, in social system graphs such inter-dependencies represent the social interactions of different individuals. It was recently shown that contractions, a key concept from graph theory, in the system graph are critical to system observability, as (at least) one state measurement in every contraction is necessary for observability. Thus, the size and number of contractions are critical in recovering for loss of observability. In this paper, the correlation between the average-size/number of contractions and the global clustering coefficient (GCC) of the system graph is studied. Our empirical results show that estimating systems with high GCC requires fewer measurements, and in case of measurement failure, there are fewer possible options to find substitute measurement that recovers the system's observability. This is significant as by tuning the GCC, we can improve the observability properties of large-scale engineered networks, such as social networks and smart grid.
A Unified Plug-and-Play Framework for Effective Data Denoising and Robust Abstention<|sep|>The success of Deep Neural Networks (DNNs) highly depends on data quality. Moreover, predictive uncertainty makes high performing DNNs risky for real-world deployment. In this paper, we aim to address these two issues by proposing a unified filtering framework leveraging underlying data density, that can effectively denoise training data as well as avoid predicting uncertain test data points. Our proposed framework leverages underlying data distribution to differentiate between noise and clean data samples without requiring any modification to existing DNN architectures or loss functions. Extensive experiments on multiple image classification datasets and multiple CNN architectures demonstrate that our simple yet effective framework can outperform the state-of-the-art techniques in denoising training data and abstaining uncertain test data.
An Overview of Direct Diagnosis and Repair Techniques in the WeeVis Recommendation Environment<|sep|>Constraint-based recommenders support users in the identification of items (products) fitting their wishes and needs. Example domains are financial services and electronic equipment. In this paper we show how divide-and-conquer based (direct) diagnosis algorithms (no conflict detection is needed) can be exploited in constraint-based recommendation scenarios. In this context, we provide an overview of the MediaWiki-based recommendation environment WeeVis.
Localised Dirac eigenmodes and Goldstone's theorem at finite temperature<|sep|>I show that a finite density of near-zero localised Dirac modes can lead to the disappearance of the massless excitations predicted by the finite-temperature version of Goldstone's theorem in the chirally broken phase of a gauge theory.
Quasienergy spectrum and tunneling current in ac-driven triple quantum dot shuttles<|sep|>The dynamics of electrons in ac driven double quantum dots have been extensively analyzed by means of Floquet theory. In these systems, coherent destruction of tunneling has been shown to occur for certain ac field parameters. In the present work we analyze, by means of Floquet theory, the electron dynamics of a triple quantum dot in series attached to electric contacts, where the central dot position oscillates. In particular, we analyze the quasienergy spectrum of this ac driven nanoelectromechanical system, as a function of the intensity and frequency of the ac field and of external dc voltages. For strong driving fields, we derive, by means of perturbation theory, analytical expressions for the quasienergies of the driven oscillator system. From this analysis we discuss the conditions for coherent destruction of tunneling (CDT) to occur as a function of detuning and field parameters. For zero detuning, and from the invariance of the Floquet Hamiltonian under a generalized parity transformation, we find analytical expressions describing the symmetry properties of the Fourier components of the Floquet states under such transformation. By using these expressions, we show that in the vicinity of the CDT condition, the quasienergy spectrum exhibits exact crossings which can be characterized by the parity properties of the corresponding eigenvectors.
Constraining the Mass Profiles of Stellar Systems: Schwarzschild Modeling of Discrete Velocity Datasets<|sep|>(ABRIDGED) We present a new Schwarzschild orbit-superposition code designed to model discrete datasets composed of velocities of individual kinematic tracers in a dynamical system. This constitutes an extension of previous implementations that can only address continuous data in the form of (the moments of) velocity distributions, thus avoiding potentially important losses of information due to data binning. Furthermore, the code can handle any combination of available velocity components, i.e., only line-of-sight velocities, only proper motions, or a combination of both. It can also handle a combination of discrete and continuous data. The code finds the distribution function (DF, a function of the three integrals of motion E, Lz, and I3) that best reproduces the available kinematic and photometric observations in a given axisymmetric gravitational potential. The fully numerical approach ensures considerable freedom on the form of the DF f(E,Lz,I3). This allows a very general modeling of the orbital structure, thus avoiding restrictive assumptions about the degree of (an)isotropy of the orbits. We describe the implementation of the discrete code and present a series of tests of its performance based on the modeling of simulated datasets generated from a known DF. We find that the discrete Schwarzschild code recovers the original orbital structure, M/L ratios, and inclination of the input datasets to satisfactory accuracy, as quantified by various statistics. The code will be valuable, e.g., for modeling stellar motions in Galactic globular clusters, and those of individual stars, planetary nebulae, or globular clusters in nearby galaxies. This can shed new light on the total mass distributions of these systems, with central black holes and dark matter halos being of particular interest.
Fast robustness quantification with variational Bayes<|sep|>Bayesian hierarchical models are increasing popular in economics. When using hierarchical models, it is useful not only to calculate posterior expectations, but also to measure the robustness of these expectations to reasonable alternative prior choices. We use variational Bayes and linear response methods to provide fast, accurate posterior means and robustness measures with an application to measuring the effectiveness of microcredit in the developing world.
GS100-02-41: a new large HI shell in the outer part of the Galaxy<|sep|>Massive stars have a profound effect on the surrounding interstellar medium. They ionize and heat the neutral gas, and due to their strong winds, they swept the gas up forming large HI shells. In this way, they generate a dense shell where the physical conditions for the formation of new stars are given. The aim of this study is to analyze the origin and evolution of the large HI shell GS100-02-41 and its role in triggering star forming processes.To characterize the shell and its environs, we carry out a multi-wavelength study. We analyze he HI 21 cm line, the radio continuum, and infrared emission distributions. The analysis of the HI data shows an expanding shell structure centred at (l, b) = (100.6 deg, -2.04 deg) in the velocity range from -29 to -51.7 km/s. We infer for GS100-02-41, a kinematical distance of 2.8 +/- 0.6 kpc. Several massive stars belonging to Cep OB1 are located in projection within the large HI, shell boundaries. The analysis of the radio continuum and infrared data reveal that there is no continuum counterpart of the HI shell. On the other hand, three slightly extended radio continuum sources are observed in projection onto the dense HI shell. From their flux density determinations we infer that they are thermal in nature. An analysis of the HI emission distribution in the environs of these sources shows, for each of them, a region of low emissivity having a good morphological correlation with the ionized gas in a velocity range similar to the one where GS100-02-41 is detected. The origin of GS100-02-41 could have been mainly due to the action of the Cep OB1 massive stars located inside the HI shell. The obtained age difference between the HI shell and the HII regions, together with their relative location, led us to conclude that the ionizing stars could have been created as a consequence of the shell evolution.
Slow Thermalization of Few-Body Dipole-Dipole Interactions<|sep|>We simulate the dynamics of Rydberg atoms resonantly exchanging energy via two-, three-, and four-body dipole-dipole interactions in a one-dimensional array. Using a simplified model of a realistic experimental system, we study the initial state survival probability, the level spacing statistics, the spread of entanglement, and the properties of the energy eigenstates. By exploring a range of disorders and interaction strengths, we find regions in parameter space where the three- and four-body dynamics exhibit nonergodic behavior and either fail to reach thermodynamic equilibrium or do so slowly. The interplay between the always-resonant and field-tuned interactions gives rise to quantum many-body scar states, which play a critical role in slowing the dynamics of the three- and four-body interactions.
Interference of Spontaneous Emission of Light from two Solid-State Atomic Ensembles<|sep|>We report an interference experiment of spontaneous emission of light from two distant solid-state ensembles of atoms that are coherently excited by a short laser pulse. The ensembles are Erbium ions doped into two LiNbO3 crystals with channel waveguides, which are placed in the two arms of a Mach-Zehnder interferometer. The light that is spontaneously emitted after the excitation pulse shows first-order interference. By a strong collective enhancement of the emission, the atoms behave as ideal two-level quantum systems and no which-path information is left in the atomic ensembles after emission of a photon. This results in a high fringe visibility of 95%, which implies that the observed spontaneous emission is highly coherent.
Associated production of Higgs boson and heavy quarks at the LHC: predictions with the kt-factorization<|sep|>In the framework of the kt-factorization approach, we study the production of Higgs bosons associated with a heavy (beauty or top) quark pair at the CERN LHC collider conditions. Our consideration is based mainly on the off-shell gluon-gluon fusion suprocess g + g -> Q + Q + H. The corresponding matrix element squared have been calculated for the first time. We investigate the total and differential cross sections of bbH and ttH production taking into account also the non-negligible contribution from the q + q -> Q + Q + H mechanism. In the numerical calculations we use the unintegrated gluon distributions obtained from the CCFM evolution equation. Our results are compared with the leading and next-to-leading order predictions of the collinear factorization of QCD.
Finite Interface Dissipation Phase Field Modeling of Ni-Nb under Additive Manufacturing Conditions<|sep|>During the laser powder bed fusion (L-PBF) process, the built part undergoes multiple rapid heating-cooling cycles, leading to complex microstructures with nonuniform properties. In the present work, a computational framework, which weakly couples a finite element thermal model to a non-equilibrium PF model was developed to investigate the rapid solidification microstructure of a Ni-Nb alloy during L-PBF. The framework is utilized to predict the spatial variation of the morphology and size of cellular segregation structure as well as the microsegregation in single-track melt pool microstructures obtained under different process conditions. A solidification map demonstrating the variation of microstructural features as a function of the temperature gradient and growth rate is presented. A planar to cellular transition is predicted in the majority of keyhole mode melt pools, while a planar interface is predominant in conduction mode melt pools. The predicted morphology and size of the cellular segregation structure agrees well with experimental measurements.
Strong and weak coupling limits in optics of quantum well excitons<|sep|>A transition between the strong (coherent) and weak (incoherent) coupling limits of resonant interaction between quantum well (QW) excitons and bulk photons is analyzed and quantified as a function of the incoherent damping rate caused by exciton-phonon and exciton-exciton scattering. For confined QW polaritons, a second, anomalous, damping-induced dispersion branch arises and develops with increasing damping. In this case, the strong-weak coupling transition is attributed to a critical damping rate, when the intersection of the normal and damping-induced dispersion branches occurs. For the radiative states of QW excitons, i.e., for radiative QW polaritons, the transition is described as a qualitative change of the photoluminescence spectrum at grazing angles along the QW structure. Furthermore, we show that the radiative corrections to the QW exciton states with in-plane wavevector approaching the photon cone are universally scaled by an energy parameter rather than diverge. The strong-weak coupling transition rates are also proportional to the same energy parameter. The numerical evaluations are given for a GaAs single quantum well with realistic parameters.
The Riemannian geometry is not sufficient for the geometrization of the Maxwell's equations<|sep|>The transformation optics uses geometrized Maxwell's constitutive equations to solve the inverse problem of optics, namely to solve the problem of finding the parameters of the medium along the paths of the electromagnetic field propagation. The quadratic Riemannian geometry is usually used for the geometrization of Maxwell's constitutive equations, because of the usage of the general relativity approaches. However, the problem of the insufficiency of the Riemannian structure for describing the constitutive tensor of the Maxwell's equations arises. The authors analyze the structure of the constitutive tensor and correlate it with the structure of the metric tensor of Riemannian geometry. It was concluded that the use of the quadratic metric for the geometrization of Maxwell's equations is insufficient, since the number of components of the metric tensor is less than the number of components of the constitutive tensor. The possible solution to this problem may be a transition to Finslerian geometry, in particular, the use of the Berwald-Moor metric to establish the structural correspondence between the field tensors of the electromagnetic field.
A Generally Applicable, Highly Scalable Measurement Computation and Optimization Approach to Sequential Model-Based Diagnosis<|sep|>Model-Based Diagnosis deals with the identification of the real cause of a system's malfunction based on a formal system model and observations of the system behavior. When a malfunction is detected, there is usually not enough information available to pinpoint the real cause and one needs to discriminate between multiple fault hypotheses (called diagnoses). To this end, Sequential Diagnosis approaches ask an oracle for additional system measurements. This work presents strategies for (optimal) measurement selection in model-based sequential diagnosis. In particular, assuming a set of leading diagnoses being given, we show how queries (sets of measurements) can be computed and optimized along two dimensions: expected number of queries and cost per query. By means of a suitable decoupling of two optimizations and a clever search space reduction the computations are done without any inference engine calls. For the full search space, we give a method requiring only a polynomial number of inferences and show how query properties can be guaranteed which existing methods do not provide. Evaluation results using real-world problems indicate that the new method computes (virtually) optimal queries instantly independently of the size and complexity of the considered diagnosis problems and outperforms equally general methods not exploiting the proposed theory by orders of magnitude.
Cyclic codes over $\mathbb{Z}_4[u]/\langle u^k\rangle$ of odd length<|sep|>Let $R=\mathbb{Z}_{4}[u]/\langle u^k\rangle=\mathbb{Z}_{4}+u\mathbb{Z}_{4}+\ldots+u^{k-1}\mathbb{Z}_{4}$ ($u^k=0$) where $k\in \mathbb{Z}^{+}$ satisfies $k\geq 2$. For any odd positive integer $n$, it is known that cyclic codes over $R$ of length $n$ are identified with ideals of the ring $R[x]/\langle x^{n}-1\rangle$. In this paper, an explicit representation for each cyclic code over $R$ of length $n$ is provided and a formula to count the number of codewords in each code is given. Then a formula to calculate the number of cyclic codes over $R$ of length $n$ is obtained. Precisely, the dual code of each cyclic code and self-dual cyclic codes over $R$ of length $n$ are investigated. When $k=4$, some optimal quasi-cyclic codes over $\mathbb{Z}_{4}$ of length $28$ and index $4$ are obtained from cyclic codes over $R=\mathbb{Z}_{4} [u]/\langle u^4\rangle$.
Bispinor Auxiliary Fields in Duality-Invariant Electrodynamics Revisited: The U(N) Case<|sep|>We update and detail the formulation of the duality-invariant systems of N interacting abelian gauge fields with N auxiliary bispinor fields added. In this setting, the self-duality amounts to U(N) invariance of the nonlinear interaction of the auxiliary fields. The U(N) self-dual Lagrangians arise after solving the nonlinear equations of motion for the auxiliary fields. We also elaborate on a new extended version of the bispinor field formulation involving some additional scalar auxiliary fields and study U(N) invariant interactions with derivatives of the auxiliary bispinor fields. Such interactions generate higher-derivative U(N) self-dual theories.
Latency Measurement of 100 km Fiber Using Correlation-OTDR<|sep|>By means of C-OTDR (Correlation - Optical Time Domain Reflectometry), we measured the latency of100 km fiber with an accuracy of a few picoseconds. Based on iterating 49 measurements, we calculated a standard deviation of 12 ps between the round-trip latency values. To verify the reflection measurements, we used a single pass setup without reflector, which showed a maximum difference of only 11 ps.
Processing and Characterization of Precision Microparts from Nickel-based Materials<|sep|>The objective of this research was to study the influence of electroplating parameters on electrodeposit characteristics for the production of nickel (Ni) and nickel-iron (Ni-Fe) microparts by photoelectroforming. The research focused on the most relevant parameter for industry, which is the current density, because it determines the process time and the consumed energy. The results of the Ni and Ni-Fe characterisations can be divided into two aspects closely linked with each other ; the morphology and the hardness.
Measurement of Cloud Properties Using a Self-Designed Cloud Chamber<|sep|>Present paper examines the dependency of ambient parameters such as humidity, and turbulence to determine the conditions on raindrop formation with the help of a self-designed cloud chamber. The research methods are experimental and observational in nature, where atmospheric phenomena are recreated through the usage of appropriate substitutes. Miniature droplets were created inside a box-like setup through the use of dry ice to cool the water vapor rising up, so as to create suspended water droplets, and to induce precipitation of heavier droplets. The experiment resulted in the creation of precipitated droplets, which were found at the base of the chamber at 99-100% relative humidity. The suspended droplets were used to study factors such as luminosity and variation of droplet sizes with turbulence. It was found that up to 14.4 m/s of turbulence, the droplet sizes increase with an increase in turbulence, with the luminosity decreasing with increase in turbulence. The gaussian profile of droplet size distribution has also been obtained, with a standard deviation of 2.83, 3.01 and 3.18 for low, medium and high turbulence speeds respectively. The experiment can be extended to incorporate a higher number of variables, so as to include a wider range of atmospheric phenomena.
Human Activity Recognition using Multi-Head CNN followed by LSTM<|sep|>This study presents a novel method to recognize human physical activities using CNN followed by LSTM. Achieving high accuracy by traditional machine learning algorithms, (such as SVM, KNN and random forest method) is a challenging task because the data acquired from the wearable sensors like accelerometer and gyroscope is a time-series data. So, to achieve high accuracy, we propose a multi-head CNN model comprising of three CNNs to extract features for the data acquired from different sensors and all three CNNs are then merged, which are followed by an LSTM layer and a dense layer. The configuration of all three CNNs is kept the same so that the same number of features are obtained for every input to CNN. By using the proposed method, we achieve state-of-the-art accuracy, which is comparable to traditional machine learning algorithms and other deep neural network algorithms.
Estimating meteor rates using Bayesian inference<|sep|>A method for estimating the true meteor rate \lambda\ from a small number of observed meteors n is derived. We employ Bayesian inference with a Poissonian likelihood function. We discuss the choice of a suitable prior and propose the adoption of Jeffreys prior, P(\lambda)=\lambda^{-0.5}, which yields an expectation value E(\lambda) = n+0.5 for any n \geq 0. We update the ZHR meteor activity formula accordingly, and explain how 68%- and 95%-confidence intervals can be computed.
Computing the Death Rate of COVID-19<|sep|>The Infection Fatality Rate (IFR) of COVID-19 is difficult to estimate because the number of infections is unknown and there is a lag between each infection and the potentially subsequent death. We introduce a new approach for estimating the IFR by first estimating the entire sequence of daily infections. Unlike prior approaches, we incorporate existing data on the number of daily COVID-19 tests into our estimation; knowing the test rates helps us estimate the ratio between the number of cases and the number of infections. Also unlike prior approaches, rather than determining a constant lag from studying a group of patients, we treat the lag as a random variable, whose parameters we determine empirically by fitting our infections sequence to the sequence of deaths. Our approach allows us to narrow our estimation to smaller time intervals in order to observe how the IFR changes over time. We analyze a 250 day period starting on March 1, 2020. We estimate that the IFR in the U.S. decreases from a high of $0.68\%$ down to $0.24\%$ over the course of this time period. We also provide IFR and lag estimates for Italy, Denmark, and the Netherlands, all of which also exhibit decreasing IFRs but to different degrees.
Centre Vortex Structure of QCD-Vacuum Fields and Confinement<|sep|>The non-trivial ground-state vacuum fields of QCD form the foundation of matter. Using modern visualisation techniques, this presentation examines the microscopic structure of these fields. Of particular interest are the centre vortices identified within the ground-state fields of lattice QCD. Our current focus is on understanding the manner in which dynamical fermions in the QCD vacuum alter the centre-vortex structure. The impact of dynamical fermions is significant and provides new insights into the role of centre vortices in underpinning both confinement and dynamical chiral symmetry breaking in QCD.
A novel method for component separation of extended sources in X-ray astronomy<|sep|>In high-energy astronomy, spectro-imaging instruments such as X-ray detectors allow investigation of the spatial and spectral properties of extended sources including galaxy clusters, galaxies, diffuse interstellar medium, supernova remnants and pulsar wind nebulae. In these sources, each physical component possesses a different spatial and spectral signature, but the components are entangled. Extracting the intrinsic spatial and spectral information of the individual components from this data is a challenging task. Current analysis methods do not fully exploit the 2D-1D (x,y,E) nature of the data, as the spatial and spectral information are considered separately. Here we investigate the application of a Blind Source Separation algorithm that jointly exploits the spectral and spatial signatures of each component in order to disentangle them. We explore the capabilities of a new BSS method (General Morphological Component Analysis; GMCA), initially developed to extract an image of the Cosmic Microwave Background from Planck data, in an X-ray context. The performance of GMCA on X-ray data is tested using Monte-Carlo simulations of supernova remnant toy models, designed to represent typical science cases. We find that GMCA is able to separate highly entangled components in X-ray data even in high contrast scenarios, and can extract with high accuracy the spectrum and map of each physical component. A modification is proposed to improve the spectral fidelity in the case of strongly overlapping spatial components, and we investigate a resampling method to derive realistic uncertainties associated to the results of the algorithm. Applying the modified algorithm to the deep Chandra observations of Cassiopeia A, we are able to produce detailed maps of the synchrotron emission at low energies (0.6-2.2 keV), and of the red/blue shifted distributions of a number of elements including Si and Fe K.
Transplanckian Censorship and Global Cosmic Strings<|sep|>Large field excursions are required in a number of axion models of inflation. These models also possess global cosmic strings, around which the axion follows a path mirroring the inflationary trajectory. Cosmic strings are thus an interesting theoretical laboratory for the study of transplanckian field excursions. We describe connections between various effective field theory models of axion monodromy and study the classical spacetimes around their supercritical cosmic strings. For small decay constants $f<M_p$ and large winding numbers $n>M_p/f$, the EFT is under control and the string cores undergo topological inflation, which may be either of exponential or power-law type. We show that the exterior spacetime is nonsingular and equivalent to a decompactifying cigar geometry, with the radion rolling in a potential generated by axion flux. Signals are able to circumnavigate infinite straight strings in finite but exponentially long time, $t\sim e^{\Delta a/M_p}$. For finite loops of supercritical string in asymptotically flat space, we argue that if topological inflation occurs, then topological censorship implies transplanckian censorship, or that external observers are forbidden from threading the loop and observing the full excursion of the axion.
Smooth Exploration for Robotic Reinforcement Learning<|sep|>Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.
Three-point correlation functions from pulsating strings in AdS$_5\times S^5$<|sep|>One of the most important problems in any conformal field theory is the calculation of three-point functions of primary operators. In this paper we provide explicit examples of correlators with two scalar operators in $\,{\cal N}=4$ super-Yang--Mills theory at large $N$, corresponding to pulsating semiclassical strings in AdS$_5\times S^5$, and an operator with small quantum numbers at strong coupling.
R-modes and neutron star recycling scenario<|sep|>To put new constraints on the r-mode instability window, we analyse the formation of millisecond pulsars (MSPs) within the recycling scenario, making use of three sets of observations: (a) X-ray observations of neutron stars (NSs) in low-mass X-ray binaries; (b) timing of millisecond pulsars; and (c) X-ray and UV observations of MSPs. As shown in previous works, r-mode dissipation by shear viscosity is not sufficient to explain observational set (a), and enhanced r-mode dissipation at the red-shifted internal temperatures $T^\infty\sim 10^8$ K is required to stabilize the observed NSs. Here, we argue that models with enhanced bulk viscosity can hardly lead to a self-consistent explanation of observational set (a) due to strong neutrino emission, which is typical for these models (unrealistically powerful energy source is required to keep NSs at the observed temperatures). We also demonstrate that the observational set (b), combined with the theory of internal heating and NS cooling, provides evidence of enhanced r-mode dissipation at low temperatures, $T^\infty\sim 2\times 10^7$ K. Observational set (c) allows us to set an upper limit on the internal temperatures of MSPs, $T^\infty<2\times 10^7$ K (assuming a canonical NS with the accreted crust). Recycling scenario can produce MSPs at these temperatures only if r-mode instability is suppressed in the whole MSP spin frequency range ($\nu\lesssim 750$ Hz) at temperatures $2\times 10^7\lesssim T^\infty\lesssim 3 \times 10^7$ K, providing thus a new constraint on the r-mode instability window. These observational constraints are analysed in more details in application to the resonance uplift scenario of Gusakov et al. [Phys. Rev. Lett., 112 (2014), 151101].
Is it Possible to Extract Metabolic Pathway Information from In Vivo H Nuclear Magnetic Resonance Spectroscopy Data?<|sep|>In vivo H nuclear magnetic resonance (NMR) spectroscopy is an important tool for performing non-invasive quantitative assessments of brain tumour glucose metabolism. Brain tumours are considered fast-growth tumours because of their high rate of proliferation. In addition, tumour cells exhibit profound genetic, biochemical and histological differences with respect to the original non-transformed cell types. Therefore, there is strong interest from the clinical investigator's point of view in understanding the role of brain metabolites under normal and pathological conditions and especially in the development of early tumour detection techniques. Unfortunately, current diagnosis techniques ignore the dynamic aspects of these signals. It is largely believed that temporal variations of NMR Spectra are simply due to noise or do not carry enough information to be exploited by any reliable diagnosis procedure. Thus, current diagnosis procedures are mainly based on empirical observations extracted from single averaged spectra. In this paper, firstly a machine learning framework for the analysis of NMR spectroscopy signals which can exploit both static and dynamic aspects of these signals is introduced. Secondly, the dynamics of the signals are further analyzed using elements from chaos theory in order to understand their underlying structure. Furthermore, we show that they exhibit rich chaotic dynamics suggesting the encoding of metabolic pathway information.
Dispersion in two dimensional channels - the Fick-Jacobs approximation revisited<|sep|>We examine the dispersion of Brownian particles in a symmetric two dimensional channel, this classical problem has been widely studied in the literature using the so called Fick-Jacobs' approximation and its various improvements. Most studies rely on the reduction to an effective one dimensional diffusion equation, here we drive an explicit formula for the diffusion constant which avoids this reduction. Using this formula the effective diffusion constant can be evaluated numerically without resorting to Brownian simulations. In addition a perturbation theory can be developed in $\varepsilon = h_0/L$ where $h_0$ is the characteristic channel height and $L$ the period. This perturbation theory confirms the results of Kalinay and Percus (Phys. Rev. E 74, 041203 (2006)), based on the reduction, to one dimensional diffusion are exact at least to ${\cal O}(\varepsilon^6)$. Furthermore, we show how the Kalinay and Percus pseudo-linear approximation can be straightforwardly recovered. The approach proposed here can also be exploited to yield exact results an appropriate limit $\varepsilon \to \infty$, we show that here the diffusion constant remains finite and show how the result can be obtained with a simple physical argument. Moreover we show that the correction to the effective diffusion constant is of order $1/\varepsilon$ and remarkably has a some universal characteristics. Numerically we compare the analytical results obtained with exact numerical calculations for a number of interesting channel geometries.
Barrier-crossing times for different non-Markovian friction in well and barrier $-$ A numerical study<|sep|>We introduce a generalized Langevin model system for different non-Markovian effects in the well and barrier regions of a potential, and use it to numerically study the dependence of the barrier-crossing time. In the appropriate limits, our model interpolates between the theoretical barrier-crossing-time predictions by Grote and Hynes (GH), as well as by Pollak et al., which for a single barrier memory time can differ by several orders of magnitude. Our model furthermore allows to test an analytic rate theory for space-inhomogeneous memory, which disagrees with our numerical results in the long well-memory regime. In this regime, we find that short barrier memory decreases the barrier-crossing time as compared to long barrier memory. This is in contrast with the short well-memory regime, where both our numerical results and GH theory predict an acceleration of the barrier crossing time with increasing barrier memory time. Both effects, the `Markovian-barrier acceleration' and GH `non-Markovian-barrier acceleration' can be understood from a committor analysis. Our model combines finite relaxation times of orthogonal degrees of freedom with a space-inhomogeneous coupling to such degrees, and represents a step towards more realistic modeling of physical reaction coordinates.
Mining Generalized Patterns from Large Databases using Ontologies<|sep|>Formal Concept Analysis (FCA) is a mathematical theory based on the formalization of the notions of concept and concept hierarchies. It has been successfully applied to several Computer Science fields such as data mining,software engineering, and knowledge engineering, and in many domains like medicine, psychology, linguistics and ecology. For instance, it has been exploited for the design, mapping and refinement of ontologies. In this paper, we show how FCA can benefit from a given domain ontology by analyzing the impact of a taxonomy (on objects and/or attributes) on the resulting concept lattice. We willmainly concentrate on the usage of a taxonomy to extract generalized patterns (i.e., knowledge generated from data when elements of a given domain ontology are used) in the form of concepts and rules, and improve navigation through these patterns. To that end, we analyze three generalization cases and show their impact on the size of the generalized pattern set. Different scenarios of simultaneous generalizations on both objects and attributes are also discussed
An Algebraic Approach for the MIMO Control of Small Scale Helicopter<|sep|>The control of small-scale helicopter is a MIMO problem. To use of classical control approach to formally solve a MIMO problem, one needs to come up with multidimensional Root Locus diagram to tune the control parameters. The problem with the required dimension of the RL diagram for MIMO design has forced the design procedure of classical approach to be conducted in cascaded multi-loop SISO system starting from the innermost loop outward. To implement this control approach for a helicopter, a pitch and roll attitude control system is often subordinated to a, respectively, longitudinal and lateral velocity control system in a nested architecture. The requirement for this technique to work is that the inner attitude control loop must have a higher bandwidth than the outer velocity control loop which is not the case for high performance mini helicopter. To address the above problems, an algebraic design approach is proposed in this work. The designed control using s-CDM approach is demonstrated for hovering control of small-scale helicopter simultaneously subjected to plant parameter uncertainties and wind disturbances.
Resonance width distribution for open quantum systems<|sep|>Recent measurements of resonance widths for low-energy neutron scattering off heavy nuclei show large deviations from the standard Porter-Thomas distribution. We propose a new resonance width distribution based on the random matrix theory for an open quantum system. Two methods of derivation lead to a single analytical expression; in the limit of vanishing continuum coupling, we recover the Porter-Thomas distribution. The result depends on the ratio of typical widths $\Gamma$ to the energy level spacing $D$ via the dimensionless parameter $\kappa=(\pi\Gamma/2D)$. The new distribution suppresses small widths and increases the probabilities of larger widths.
The signature of primordial black holes in the dark matter halos of galaxies<|sep|>The aim of this paper is to investigate the claim that stars in the lensing galaxy of a gravitationally lensed quasar system can always account for the observed microlensing of the individual quasar images. A small sample of gravitationally lensed quasar systems was chosen where the quasar images appear to lie on the fringe of the stellar distribution of the lensing galaxy. As with most quasar systems, all the individual quasar images were observed to be microlensed. The surface brightness of the lensing galaxy at the positions of the quasar images was measured from HST frames, and converted to stellar surface mass density. The surface density of smoothly distributed dark matter at the image positions was obtained from lensing models of the quasar systems and applied to the stellar surface mass density to give the optical depth to microlensing. This was then used to assess the probability that the stars in the lensing galaxy could be responsible for the observed microlensing. The results were supported by microlensing simulations of the star fields around the quasar images combined with values of convergence and shear from the lensing models. Taken together, the probability that all the observed microlensing is due to stars was found to be ~0.0003. Errors resulting from surface brightness measurement, mass-to-light ratio and the contribution of the dark matter halo do not significantly affect this result. It is argued that the most plausible candidates for the microlenses are primordial black holes, either in the dark matter halos of the lensing galaxies, or more generally distributed along the lines of sight to the quasars.
Models of coupled dark matter to dark energy<|sep|>We present three distinct types of models of dark energy in the form of a scalar field which is explicitly coupled to dark matter. Our construction draws from the pull-back formalism for fluids and generalises the fluid action to involve couplings to the scalar field. We investigate the cosmology of each class of model both at the background and linearly perturbed level. We choose a potential for the scalar field and a specific coupling function for each class of models and we compute the Cosmic Microwave Background and matter power spectra.
Star Formation Efficiencies and Lifetimes of Giant Molecular Clouds in the Milky Way<|sep|>We use a sample of the 13 most luminous WMAP Galactic free-free sources, responsible for 33% of the free- free emission of the Milky Way, to investigate star formation. The sample contains 40 star forming complexes; we combine this sample with giant molecular cloud (GMC) catalogs in the literature, to identify the host GMCs of 32 of the complexes. We estimate the star formation efficiency epsilon_GMC and star formation rate per free-fall time epsilon_ff. We find that epsilon_GMC ranges from 0.002 to 0.2, with an ionizing luminosity-weighted average epsilon_GMC = 0.08, compared to the Galactic average = 0.005. Turning to the star formation rate per free-fall time, we find values that range up to epsilon_ff = 1. Weighting by ionizing luminosity, we find an average of epsilon_ff = 0.16 - 0.24 depending on the estimate of the age of the system. Once again, this is much larger than the Galaxy-wide average value epsilon_ff = 0.008. We show that the lifetimes of giant molecular clouds at the mean mass found in our sample is 17 plus or minus 4 Myr, about two free-fall times. The GMCs hosting the most luminous clusters are being disrupted by those clusters. Accordingly, we interpret the range in epsilon_ff as the result of a time-variable star formation rate; the rate of star formation increases with the age of the host molecular cloud, until the stars disrupt the cloud. These results are inconsistent with the notion that the star formation rate in Milky Way GMCs is determined by the properties of supersonic turbulence
An exhaustive survey of trust models in p2p network<|sep|>Most of the peers accessing the services are under the assumption that the service accessed in a P2P network is utmost secured. By means of prevailing hard security mechanisms, security goals like authentication, authorization, privacy, non repudiation of services and other hard security issues are resolved. But these mechanisms fail to provide soft security. An exhaustive survey of existing trust and reputation models in P2P network regarding service provisioning is presented and challenges are listed.p2p Trust issues like trust bootstrapping, trust evidence procurement, trust assessment, trust interaction outcome evaluation and other trust based classification of peers behaviour into trusted, inconsistent, un trusted, malicious, betraying, redemptive are discussed.
Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation<|sep|>Most existing vision-language pre-training methods focus on understanding tasks and use BERT-like objectives (masked language modeling and image-text matching) during pretraining. Although they perform well in many understanding downstream tasks, e.g., visual question answering, image-text retrieval and visual entailment, they do not possess the ability to generate. To tackle this problem, we propose Unified multimodal pre-training for both Vision-Language understanding and generation (UniVL). The proposed UniVL is capable of handling both understanding tasks and generative tasks. We augment existing pretraining paradigms that only use random masks with causal masks, i.e., triangular masks that mask out future tokens, such that the pre-trained models can have autoregressive generation abilities by design. We formulate several previous understanding tasks as a text generation task and propose to use prompt-based method for fine-tuning on different downstream tasks. Our experiments show that there is a trade-off between understanding tasks and generation tasks while using the same model, and a feasible way to improve both tasks is to use more data. Our UniVL framework attains comparable performance to recent vision-language pre-training methods on both understanding tasks and generation tasks. Moreover, we demostrate that prompt-based finetuning is more data-efficient - it outperforms discriminative methods in few-shot scenarios.
A prediction of $D^*$-multi-$\rho$ states<|sep|>We present a study of the many-body interaction between a $D^*$ and multi-$\rho$. We use an extrapolation to SU(4) of the hidden gauge formalism, which produced dynamically the resonances $f_2(1270)$ in the $\rho\rho$ interaction and $D^*_2(2460)$ in the $\rho D^*$ interaction. Then let a third particle, $\rho$, $D^*$, or a resonance collide with them, evaluating the scattering amplitudes in terms of the Fixed Center Approximation of the Faddeev equations. We find several clear resonant structures above $2800\mev$ in the multibody scattering amplitudes. They would correspond to new charmed resonances, $D^*_3$, $D^*_4$, $D^*_5$ and $D^*_6$, which are not yet listed in the PDG, which would be analogous to the $\rho_3(1690)$, $f_4(2050)$, $\rho_5(2350)$, $f_6(2510)$ and $K^*_3(1780)$, $K^*_4(2045)$, $K^*_5(2380)$ described before as multi-$\rho$ and $K^*$-multi-$\rho$ states respectively.
Sufficiency, Separability and Temporal Probabilistic Models<|sep|>Suppose we are given the conditional probability of one variable given some other variables.Normally the full joint distribution over the conditioning variablesis required to determine the probability of the conditioned variable.Under what circumstances are the marginal distributions over the conditioning variables sufficient to determine the probability ofthe conditioned variable?Sufficiency in this sense is equivalent to additive separability ofthe conditional probability distribution.Such separability structure is natural and can be exploited forefficient inference.Separability has a natural generalization to conditional separability.Separability provides a precise notion of weaklyinteracting subsystems in temporal probabilistic models.Given a system that is decomposed into separable subsystems, exactmarginal probabilities over subsystems at future points in time can becomputed by propagating marginal subsystem probabilities, rather thancomplete system joint probabilities.Thus, separability can make exact prediction tractable.However, observations can break separability,so exact monitoring of dynamic systems remains hard.
Modelling stellar activity with Gaussian process regression networks<|sep|>Stellar photospheric activity is known to limit the detection and characterisation of extra-solar planets. In particular, the study of Earth-like planets around Sun-like stars requires data analysis methods that can accurately model the stellar activity phenomena affecting radial velocity (RV) measurements. Gaussian Process Regression Networks (GPRNs) offer a principled approach to the analysis of simultaneous time-series, combining the structural properties of Bayesian neural networks with the non-parametric flexibility of Gaussian Processes. Using HARPS-N solar spectroscopic observations encompassing three years, we demonstrate that this framework is capable of jointly modelling RV data and traditional stellar activity indicators. Although we consider only the simplest GPRN configuration, we are able to describe the behaviour of solar RV data at least as accurately as previously published methods. We confirm the correlation between the RV and stellar activity time series reaches a maximum at separations of a few days, and find evidence of non-stationary behaviour in the time series, associated with an approaching solar activity minimum.
Charged Ising Model of Neutron Star Matter<|sep|>Background: The inner crust of a neutron star is believed to consist of Coulomb-frustrated complex structures known as "nuclear pasta" that display interesting and unique low-energy dynamics. Purpose: To elucidate the structure and composition of the neutron-star crust as a function of temperature, density, and proton fraction. Methods: A new lattice-gas model, the "Charged-Ising Model" (CIM), is introduced to simulate the behavior of neutron-star matter. Preliminary Monte Carlo simulations on 30^3 lattices are performed for a variety of temperatures, densities, and proton fractions. Results: Results are obtained for the heat capacity, pair-correlation function, and static structure factor for a variety of conditions appropriate to the inner stellar crust. Conclusions: Although relatively simple, the CIM captures the essence of Coulomb frustration that is required to simulate the subtle dynamics of the inner stellar crust. Moreover, the computationally demanding long-range Coulomb interactions have been pre-computed at the appropriate lattice sites prior to the start of the simulation resulting in enormous computational gains. This work demonstrates the feasibility of future CIM simulations involving a large number of particles as a function of density, temperature, and proton fraction.
Distance statistics in quadrangulations with no multiple edges and the geometry of minbus<|sep|>We present a detailed calculation of the distance-dependent two-point function for quadrangulations with no multiple edges. Various discrete observables measuring this two-point function are computed and analyzed in the limit of large maps. For large distances and in the scaling regime, we recover the same universal scaling function as for general quadrangulations. We then explore the geometry of "minimal neck baby universes" (minbus), which are the outgrowths to be removed from a general quadrangulation to transform it into a quadrangulation with no multiple edges, the "mother universe". We give a number of distance-dependent characterizations of minbus, such as the two-point function inside a minbu or the law for the distance from a random point to the mother universe.
Superpixel Hierarchy<|sep|>Superpixel segmentation is becoming ubiquitous in computer vision. In practice, an object can either be represented by a number of segments in finer levels of detail or included in a surrounding region at coarser levels of detail, and thus a superpixel segmentation hierarchy is useful for applications that require different levels of image segmentation detail depending on the particular image objects segmented. Unfortunately, there is no method that can generate all scales of superpixels accurately in real-time. As a result, a simple yet effective algorithm named Super Hierarchy (SH) is proposed in this paper. It is as accurate as the state-of-the-art but 1-2 orders of magnitude faster. The proposed method can be directly integrated with recent efficient edge detectors like the structured forest edges to significantly outperforms the state-of-the-art in terms of segmentation accuracy. Quantitative and qualitative evaluation on a number of computer vision applications was conducted, demonstrating that the proposed method is the top performer.
Data-driven Rollout for Deterministic Optimal Control<|sep|>We consider deterministic infinite horizon optimal control problems with nonnegative stage costs. We draw inspiration from learning model predictive control scheme designed for continuous dynamics and iterative tasks, and propose a rollout algorithm that relies on sampled data generated by some base policy. The proposed algorithm is based on value and policy iteration ideas, and applies to deterministic problems with arbitrary state and control spaces, and arbitrary dynamics. It admits extensions to problems with trajectory constraints, and a multiagent structure.
Universal Programmable Quantum Circuit Schemes to Emulate an Operator<|sep|>Unlike fixed designs, programmable circuit designs support an infinite number of operators. The functionality of a programmable circuit can be altered by simply changing the angle values of the rotation gates in the circuit. Here, we present a new quantum circuit design technique resulting in two general programmable circuit schemes. The circuit schemes can be used to simulate any given operator by setting the angle values in the circuit. This provides a fixed circuit design whose angles are determined from the elements of the given matrix-which can be non-unitary-in an efficient way. We also give both the classical and quantum complexity analysis for these circuits and show that the circuits require a few classical computations. They have almost the same quantum complexities as non-general circuits. Since the presented circuit designs are independent from the matrix decomposition techniques and the global optimization processes used to find quantum circuits for a given operator, high accuracy simulations can be done for the unitary propagators of molecular Hamiltonians on quantum computers. As an example, we show how to build the circuit design for the hydrogen molecule.
Near-Infrared properties of the X-ray emitting young stellar objects in the Carina Nebula<|sep|>Abbreviated Abstract: The near-infrared study of the Carina Nebula in this paper builds on the results of the Chandra Carina Complex Project (CCCP), that detected 14368 X-ray sources in the 1.4 square-degree survey region, an automatic source classification study that classified 10714 of these as very likely young stars in Carina, and an analysis of their clustering properties. We used HAWK-I at the ESO VLT to conduct a very deep near-IR survey with sub-arcsecond angular resolution, covering about 1280 square-arcminutes. The HAWK-I images reveal more than 600000 individual infrared sources, whereby objects as faint as J ~ 23, H ~ 22, and Ks ~ 21 are detected at S/N >= 3. While less than half of the Chandra X-ray sources have counterparts in the 2MASS catalog, the ~5 mag deeper HAWK-I data reveal infrared counterparts to 6636 (= 88.8%) of the 7472 Chandra X-ray sources in the HAWK-I field. We analyze near-infrared color-color and color-magnitude diagrams to derive information about the extinctions, infrared excesses, ages, and masses of the X-ray selected objects. The near-infrared properties agree well with the results of the automatic X-ray source classification. The shape of the K-band luminosity function of the X-ray selected Carina members agrees well with that derived for the Orion Nebula Cluster, suggesting that, down to the X-ray detection limit around 0.5-1 Msun, the shape of the IMF in Carina is consistent with that in Orion (and thus the field IMF). The fraction of stars with near-infrared excesses is rather small, <=10%, but shows considerable variations between individual parts of the complex. The distribution of extinctions for the diskless stars ranges from ~1.6 mag to ~6.2 mag (central 80th percentile), clearly showing a considerable range of differential extinction between individual stars in the complex.
Spectral Implications of Atomic Uncertainties in Optically-thin Hot Plasmas<|sep|>Two new high-resolution X-ray spectroscopy missions, XRISM and Athena, will observe deeper and with higher X-ray resolution than ever before possible. Interpreting these new X-ray spectra will require understanding the impact that uncertainties on fundamental atomic quantities such as collisional cross sections, transition rates, and wavelengths have on spectral models. As millions of values are required to generate even a simple model of an optically-thin hot plasma, most such rates exist only as theoretical calculations. We have developed methods to estimate the uncertainty in the final spectral calculations based on published experimental data and plausible approximations to the uncertainties in the underlying atomic data. We present an extension to the pyatomdb code that implements these methods and investigate the sensitivity of selected strong diagnostic lines in the X-ray bandpass (0.3-12 keV).
Multiple Component Analysis of Time Resolved Spectra of GRB041006: A Clue to the Nature of Underlying Soft Component of GRBs<|sep|>GRB 041006 was detected by HETE-2 at 12:18:08 UT on 06 October 2004. This GRB displays a soft X-ray emission, a precursor before the onset of the main event, and also a soft X-ray tail after the end of the main peak. The light curves in four different energy bands display different features; At higher energy bands several peaks are seen in the light curve, while at lower energy bands a single broader bump dominates. It is expected that these different features are the result of a mixture of several components each of which has different energetics and variability. To reveal the nature of each component, we analysed the time resolved spectra and they are successfully resolved into several components. We also found that these components can be classified into two distinct classes; One is a component which has an exponential decay of $E_{p}$ with a characteristic timescale shorter than $\sim$ 30 sec, and its spectrum is well represented by a broken power law function, which is frequently observed in many prompt GRB emissions, so it should have an internal-shock origin. Another is a component whose $E_{p}$ is almost unchanged with characteristic timescale longer than $\sim$ 60 sec, and shows a very soft emission and slower variability. The spectrum of the soft component is characterized by either a broken power law or a black body spectrum. This component might originate from a relatively wider and lower velocity jet or a photosphere of the fireball. By assuming that the soft component is a thermal emission, the radiation radius is initially $4.4 \times 10^{6}$ km, which is a typical radius of a blue supergiant, and its expansion velocity is $2.4 \times 10^{5}$ km/s in the source frame.
Human Behavior Recognition Method Based on CEEMD-ES Radar Selection<|sep|>In recent years, the millimeter-wave radar to identify human behavior has been widely used in medical,security, and other fields. When multiple radars are performing detection tasks, the validity of the features contained in each radar is difficult to guarantee. In addition, processing multiple radar data also requires a lot of time and computational cost. The Complementary Ensemble Empirical Mode Decomposition-Energy Slice (CEEMD-ES) multistatic radar selection method is proposed to solve these problems. First, this method decomposes and reconstructs the radar signal according to the difference in the reflected echo frequency between the limbs and the trunk of the human body. Then, the radar is selected according to the difference between the ratio of echo energy of limbs and trunk and the theoretical value. The time domain, frequency domain and various entropy features of the selected radar are extracted. Finally, the Extreme Learning Machine (ELM) recognition model of the ReLu core is established. Experiments show that this method can effectively select the radar, and the recognition rate of three kinds of human actions is 98.53%.
Polyakov Loop in Non-covariant Operator Formalism<|sep|>We discuss a Polyakov loop in non-covariant operator formalism which consists of only physical degrees of freedom at finite temperature. It is pointed out that although the Polyakov loop is expressed by a Euclidean time component of gauge fields in a covariant path integral formalism, there is no direct counterpart of the Polyakov loop operator in the operator formalism because the Euclidean time component of gauge fields is not a physical degree of freedom. We show that by starting with an operator which is constructed in terms of only physical operators in the non-covariant operator formalism, the vacuum expectation value of the operator calculated by trace formula can be rewritten into a familiar form of an expectation value of Polyakov loop in a covariant path integral formalism at finite temperature for the cases of axial and Coulomb gauge.
New Supersymmetric Localizations from Topological Gravity<|sep|>Supersymmetric field theories can be studied exactly on suitable off-shell supergravity backgrounds. We show that in two dimensions such backgrounds are identifiable with BRST invariant backgrounds of topological gravity coupled to an abelian topological gauge multiplet. This latter background is required for the consistent coupling of the topological `matter' YM theory to topological gravity. We make use of this topological point of view to obtain, in a simple and straightforward way, a complete classification of localizing supersymmetric backgrounds in two dimensions. The BRST invariant topological backgrounds are parametrized by both Killing vectors and $S^1$-equivariant cohomology of the 2-dimensional world-sheet. We reconstruct completely the supergravity backgrounds from the topological data: some of the supergravity fields are twisted versions of the topological backgrounds, but others are "composite", i.e. they are non-linear functionals of them. We recover all the known localizing 2-dimensional backgrounds and (infinitely) many more that have not been explored so far. We show that the supersymmetric $\Omega$-deformation is nothing but the background value of the ghost-for-ghost of topological gravity, a result which holds for other dimensions too. The new localizing backgrounds are characterized by non-trivial fluxes for both the graviphotons of the supergravity multiplet.
Quantum magnetism of ultra-cold fermion systems with the symplectic symmetry<|sep|>We numerically study quantum magnetism of ultra-cold alkali and alkaline-earth fermion systems with large hyperfine spin $F=3/2$, which are characterized by a generic $Sp(N)$ symmetry with N=4. The methods of exact diagonalization (ED) and density-matrix-renormalization-group are employed for the large size one-dimensional (1D) systems, and ED is applied to a two-dimensional (2D) square lattice on small sizes. We focus on the magnetic exchange models in the Mott-insulating state at quarter-filling. Both 1D and 2D systems exhibit rich phase diagrams depending on the ratio between the spin exchanges $J_0$ and $J_2$ in the bond spin singlet and quintet channels, respectively. In 1D, the ground states exhibit a long-range-ordered dimerization with a finite spin gap at $J_0/J_2>1$, and a gapless spin liquid state at $J_0/J_2 \le 1$, respectively. In the former and latter cases, the correlation functions exhibit the two-site and four-site periodicities, respectively. In 2D, various spin correlation functions are calculated up to the size of $4\times 4$. The Neel-type spin correlation dominates at large values of $J_0/J_2$, while a $2\times 2$ plaquette correlation is prominent at small values of this ratio. Between them, a columnar spin-Peierls dimerization correlation peaks. We infer the competitions among the plaquette ordering, the dimer ordering, and the Neel ordering in the 2D system.
Evaluation of accurate uncertainty of measurement in L subshell ionization cross-section<|sep|>To have a better understanding of a physical process, a comparison of ex-perimental data with theoretical values is mandatory. The comparison ismeaningful if the uncertainty in the experiment is accounted well. However,it is seldom seen, especially for a complex phenomenon. We take a test casethrough L subshell ionization of atoms by particle impact. Experimentally,x-ray production cross-sections are measured, but ionization cross-sectionsare calculated theoretically. Furthermore, the uncertainty of the x-ray pro-duction cross-section is mainly statistics and detector-efficiency driven. Butionization cross-section involves many other factors because of the relation-ship between the production and ionization cross section, having wide uncer-tainty spectrum. Consequently, determining the measurement uncertainty inL subshell ionization cross-section is always difficult. We have studied thisissue in the simplest way, where the rule of weighted propagation of rela-tive uncertainty is utilised. We notice that larger uncertainties are involvedin atomic parameters relevant to L1(2s1/2) subshell than those associatedwith the other two L2(2p1/2) and L3(2p3/2) subshells. Hence, comparisonbetween theory and experiment would give higher emphasis onL2andL3subshell ionization cross sections. We believe this work aware us that theappropriate uncertainty evaluation is extremely important for providing theright judgment on the data.
A Parallel Encryption Algorithm for Block Ciphers Based on Reversible Programmable Cellular Automata<|sep|>A Cellular Automata (CA) is a computing model of complex System using simple rule. In CA the problem space into number of cell and each cell can be one or several final state. Cells are affected by neighbours' to the simple rule. Cellular Automata are highly parallel and discrete dynamical systems, whose behaviour is completely specified in terms of a local relation. This paper deals with the Cellular Automata (CA) in cryptography for a class of Block Ciphers through a new block encryption algorithm based on Reversible Programmable Cellular Automata Theory. The proposed algorithm belongs to the class of symmetric key systems.
Reply to "Comment on 'Isotope effect in high-Tc superconductors' "<|sep|>Our paper on the isotope effect in high-temperature superconductors with cation substitutions presents a comprehensive analysis rooted completely in the experimental evidence. In this Reply we show that pair-breaking disorder, isotope effects, doping-induced variations in Tc and in the magnetic penetration depth, Coulomb's law, and Anderson's theorem are treated with correct physical and mathematical fundamentals. In contrast, the theory fostered in the Comment by Alexandrov and Zhao contradicts several specific experimental facts, eight of which are briefly discussed. Their Comment also uncritically repeats a previously discredited assertion of an isotope effect in the superconducting carrier mass, incorrectly assumes that cation doping continuously varies intrinsic superconducting parameters, unjustifiably assigns importance to data from samples with serious quality problems, and renders a false estimate of the pair-breaking strength.
MCMC Guided CNN Training and Segmentation for Pancreas Extraction<|sep|>Efficient organ segmentation is the precondition of various quantitative analysis. Segmenting the pancreas from abdominal CT images is a challenging task because of its high anatomical variability in shape, size and location. What's more, the pancreas only occupies a small portion in abdomen, and the organ border is very fuzzy. All these factors make the segmentation methods of other organs less suitable for the pancreas segmentation. In this report, we propose a Markov Chain Monte Carlo (MCMC) sampling guided convolutional neural network (CNN) approach, in order to handle such difficulties in morphological and photometric variabilities. Specifically, the proposed method mainly contains three steps: First, registration is carried out to mitigate the body weight and location variability. Then, an MCMC sampling is employed to guide the sampling of 3D patches, which are fed to the CNN for training. At the same time, the pancreas distribution is also learned for the subsequent segmentation. Third, sampled from the learned distribution, an MCMC process guides the segmentation process. Lastly, the patches based segmentation is fused using a Bayesian voting scheme. This method is evaluated on the NIH pancreatic datasets which contains 82 abdominal contrast-enhanced CT volumes. Finally, we achieved a competitive result of 78.13% Dice Similarity Coefficient value and 82.65% Recall value in testing data.
Kinematic Discovery of a Stellar Stream located in Pisces<|sep|>We report the kinematic discovery of the Pisces Stellar Stream (PSS), at Galactic longitude of roughly l=135 degrees and longitude between -39 < b < -36 deg. We originally identified this halo substructure from velocities of red giant branch stars in Sloan Digital Sky Survey (SDSS) Data Release 8, and confirmed its presence in turnoff stars from SDSS photometric data. The PSS is a narrow, kinematically cold tidal stream, with a velocity dispersion of 8 km/s. Its metallicity is [Fe/H]=-2.2, with ~0.3 dex dispersion. The color-magnitude signature of the stream turnoff, combined with our measured metallicity, places the PSS at a distance of 35+/-3 kpc. The Pisces Stellar Stream is the same as the previously announced "Triangulum stream" and part of the proposed "stream a". We rule out an association of the PSS with other previously known Milky Way substructures in the same region of the sky.
Millisecond speed deep learning based proton dose calculation with Monte Carlo accuracy<|sep|>Next generation online and real-time adaptive radiotherapy workflows require precise particle transport simulations in sub-second times, which is unfeasible with current analytical pencil beam algorithms (PBA) or stochastic Monte Carlo (MC) methods. We present a data-driven millisecond speed dose calculation algorithm (DoTA) accurately predicting the dose deposited by mono-energetic proton pencil beams for arbitrary energies and patient geometries. Given the forward-scattering nature of protons, we frame 3D particle transport as modeling a sequence of 2D geometries in the beam's eye view. DoTA combines convolutional neural networks extracting spatial features (e.g., tissue and density contrasts) with a transformer self-attention backbone that routes information between the sequence of geometry slices and a vector representing the beam's energy, and is trained to predict low noise MC simulations of proton beamlets using 80,000 different head and neck, lung, and prostate geometries. Predicting beamlet doses in 5 ms with a very high gamma pass rate of 99.37% (1%, 3 mm) compared to the ground truth MC calculations, DoTA significantly improves upon analytical pencil beam algorithms both in precision and speed. Offering MC accuracy 100 times faster than PBAs for pencil beams, our model calculates full treatment plan doses in 10 to 15 s depending on the number of beamlets, achieving a 99.70% (2%, 2 mm) gamma pass rate across 9 test patients. Outperforming all previous analytical pencil beam and deep learning based approaches, DoTA represents a new state of the art in data-driven dose calculation and can directly compete with the speed of even commercial GPU MC approaches. Providing the sub-second speed required for adaptive treatments, straightforward implementations could offer similar benefits to other steps of the radiotherapy workflow or other modalities such as helium or carbon treatments.
Rendering Nighttime Image Via Cascaded Color and Brightness Compensation<|sep|>Image signal processing (ISP) is crucial for camera imaging, and neural networks (NN) solutions are extensively deployed for daytime scenes. The lack of sufficient nighttime image dataset and insights on nighttime illumination characteristics poses a great challenge for high-quality rendering using existing NN ISPs. To tackle it, we first built a high-resolution nighttime RAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert professionals. Meanwhile, to best capture the characteristics of nighttime illumination light sources, we develop the CBUnet, a two-stage NN ISP to cascade the compensation of color and brightness attributes. Experiments show that our method has better visual quality compared to traditional ISP pipeline, and is ranked at the second place in the NTIRE 2022 Night Photography Rendering Challenge for two tracks by respective People's and Professional Photographer's choices. The code and relevant materials are avaiable on our website: https://njuvision.github.io/CBUnet.
Relative Error Control in Bivariate Interpolatory Cubature<|sep|>We describe an algorithm for controlling the relative error in the numerical evaluation of a bivariate integral, without prior knowledge of the magnitude of the integral. In the event that the magnitude of the integral is less than unity, absolute error control is preferred. The underlying quadrature rule is positive-weight interpolatory and composite. Some numerical examples demonstrate the algorithm.
Extension of the Uhlenbeck-Ford Model with an Attraction<|sep|>The Uhlenbeck-Ford model for soft repulsion, which has only a repulsive interaction, is extended by inclusion of an attraction. This extension still allows an analytical evaluation of the virial coefficients. The integrals over the graph contributions are reduced to a combinatorial problem. We have calculated the virial coefficients to order 6 in the density. A link is made between this model and more common interactions, like the 12-6 Lennard-Jones potential.
The multiobjective multidimensional knapsack problem: a survey and a new approach<|sep|>The knapsack problem (KP) and its multidimensional version (MKP) are basic problems in combinatorial optimization. In this paper we consider their multiobjective extension (MOKP and MOMKP), for which the aim is to obtain or to approximate the set of efficient solutions. In a first step, we classify and describe briefly the existing works, that are essentially based on the use of metaheuristics. In a second step, we propose the adaptation of the two-phase Pareto local search (2PPLS) to the resolution of the MOMKP. With this aim, we use a very-large scale neighborhood (VLSN) in the second phase of the method, that is the Pareto local search. We compare our results to state-of-the-art results and we show that we obtain results never reached before by heuristics, for the biobjective instances. Finally we consider the extension to three-objective instances.
A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting Action-Oriented and Goal-Oriented Instructions<|sep|>Robots operating alongside humans in diverse, stochastic environments must be able to accurately interpret natural language commands. These instructions often fall into one of two categories: those that specify a goal condition or target state, and those that specify explicit actions, or how to perform a given task. Recent approaches have used reward functions as a semantic representation of goal-based commands, which allows for the use of a state-of-the-art planner to find a policy for the given task. However, these reward functions cannot be directly used to represent action-oriented commands. We introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding Network (DRAGGN), for task grounding and execution that handles natural language from either category as input, and generalizes to unseen environments. Our robot-simulation results demonstrate that a system successfully interpreting both goal-oriented and action-oriented task specifications brings us closer to robust natural language understanding for human-robot interaction.
Two spectroscopically confirmed galaxy structures at z=0.61 and 0.74 in the CFHTLS Deep~3 field<|sep|>Adami et al. (2010) have detected several cluster candidates at z>0.5 as part of a systematic search for clusters in the Canada France Hawaii Telescope Legacy Survey, based on photometric redshifts. We focus here on two of them, located in the D3 field: D3-6 and D3-43. We have obtained spectroscopy with Gemini/GMOS and measured redshifts for 23 and 14 galaxies in the two structures. These redshifts were combined with those available in the literature. A dynamical and a weak lensing analysis were also performed, together with the study of X-ray Chandra archive data. Cluster D3-6 is found to be a single structure of 8 spectroscopically confirmed members at an average redshift z=0.607, with a velocity dispersion of 423 km/s. It appears to be a relatively low mass cluster. D3-43-S3 has 46 spectroscopically confirmed members at an average redshift z=0.739. It can be decomposed into two main substructures, having a velocity dispersion of about 600 and 350 km/s. An explanation to the fact that D3-43-S3 is detected through weak lensing (only marginally, at the ~3sigma level) but not in X-rays could be that the two substructures are just beginning to merge more or less along the line of sight. We also show that D3-6 and D3-43-S3 have similar global galaxy luminosity functions, stellar mass functions, and star formation rate (SFR) distributions. The only differences are that D3-6 exhibits a lack of faint early type galaxies, a deficit of extremely high stellar mass galaxies compared to D3-43-S3, and an excess of very high SFR galaxies. This study shows the power of techniques based on photometric redshifts to detect low to moderately massive structures, even at z~0.75.
Structure and decays of nuclear three-body systems: the Gamow coupled-channel method in Jacobi coordinates<|sep|>${\bf Background:}$ Weakly bound and unbound nuclear states appearing around particle thresholds are prototypical open quantum systems. Theories of such states must take into account configuration mixing effects in the presence of strong coupling to the particle continuum space. ${\bf Purpose:}$ To describe structure and decays of three-body systems, we developed a Gamow coupled-channel (GCC) approach in Jacobi coordinates by employing the complex-momentum formalism. We benchmarked the new framework against the complex-energy Gamow Shell Model (GSM). ${\bf Methods:}$ The GCC formalism is expressed in Jacobi coordinates, so that the center-of-mass motion is automatically eliminated. To solve the coupled-channel equations, we use hyperspherical harmonics to describe the angular wave functions while the radial wave functions are expanded in the Berggren ensemble, which includes bound, scattering and Gamow states. ${\bf Results:}$ We show that the GCC method is both accurate and robust. Its results for energies, decay widths, and nucleon-nucleon angular correlations are in good agreement with the GSM results. ${\bf Conclusions:}$ We have demonstrated that a three-body GSM formalism explicitly constructed in cluster-orbital shell model coordinates provides similar results to a GCC framework expressed in Jacobi coordinates, provided that a large configuration space is employed. Our calculations for $A=6$ systems and $^{26}$O show that nucleon-nucleon angular correlations are sensitive to the valence-neutron interaction. The new GCC technique has many attractive features when applied to bound and unbound states of three-body systems: it is precise, efficient, and can be extended by introducing a microscopic model of the core.
An ADMM Based Method for Computation Rate Maximization in Wireless Powered Mobile-Edge Computing Networks<|sep|>In this paper, we consider a wireless powered mobile edge computing (MEC) network, where the distributed energy-harvesting wireless devices (WDs) are powered by means of radio frequency (RF) wireless power transfer (WPT). In particular, the WDs follow a binary computation offloading policy, i.e., data set of a computing task has to be executed as a whole either locally or remotely at the MEC server via task offloading. We are interested in maximizing the (weighted) sum computation rate of all the WDs in the network by jointly optimizing the individual computing mode selection (i.e., local computing or offloading) and the system transmission time allocation (on WPT and task offloading). The major difficulty lies in the combinatorial nature of multi-user computing mode selection and its strong coupling with transmission time allocation. To tackle this problem, we propose a joint optimization method based on the ADMM (alternating direction method of multipliers) decomposition technique. Simulation results show that the proposed method can efficiently achieve near-optimal performance under various network setups, and significantly outperform the other representative benchmark methods considered. Besides, using both theoretical analysis and numerical study, we show that the proposed method enjoys low computational complexity against the increase of networks size.
Detection and extraction of signals from the epoch of reionization using higher order one-point statistics<|sep|>Detecting redshifted 21cm emission from neutral hydrogen in the early Universe promises to give direct constraints on the epoch of reionization (EoR). It will, though, be very challenging to extract the cosmological signal (CS) from foregrounds and noise which are orders of magnitude larger. Fortunately, the signal has some characteristics which differentiate it from the foregrounds and noise, and we suggest that using the correct statistics may tease out signatures of reionization. We generate mock datacubes simulating the output of the Low Frequency Array (LOFAR) EoR experiment. These cubes combine realistic models for Galactic and extragalactic foregrounds and the noise with three different simulations of the CS. We fit out the foregrounds, which are smooth in the frequency direction, to produce residual images in each frequency band. We denoise these images and study the skewness of the one-point distribution in the images as a function of frequency. We find that, under sufficiently optimistic assumptions, we can recover the main features of the redshift evolution of the skewness in the 21cm signal. We argue that some of these features - such as a dip at the onset of reionization, followed by a rise towards its later stages - may be generic, and give us a promising route to a statistical detection of reionization.
Energy Efficient Node Deployment in Wireless Ad-hoc Sensor Networks<|sep|>We study a wireless ad-hoc sensor network (WASN) where $N$ sensors gather data from the surrounding environment and transmit their sensed information to $M$ fusion centers (FCs) via multi-hop wireless communications. This node deployment problem is formulated as an optimization problem to make a trade-off between the sensing uncertainty and energy consumption of the network. Our primary goal is to find an optimal deployment of sensors and FCs to minimize a Lagrange combination of the sensing uncertainty and energy consumption. To support arbitrary routing protocols in WASNs, the routing-dependent necessary conditions for the optimal deployment are explored. Based on these necessary conditions, we propose a routing-aware Lloyd algorithm to optimize node deployment. Simulation results show that, on average, the proposed algorithm outperforms the existing deployment algorithms.
KIC 3858884: a hybrid {\delta} Sct pulsator in a highly eccentric eclipsing binary<|sep|>The analysis of eclipsing binaries containing non-radial pulsators allows: i) to combine two different and independent sources of information on the internal structure and evolutionary status of the components, and ii) to study the effects of tidal forces on pulsations. KIC 3858884 is a bright Kepler target whose light curve shows deep eclipses, complex pulsation patterns with pulsation frequencies typical of {\delta} Sct, and a highly eccentric orbit. We present the result of the analysis of Kepler photometry and of high resolution phaseresolved spectroscopy. Spectroscopy yielded both the radial velocity curves and, after spectral disentangling, the primary component effective temperature and metallicity, and line-of-sight projected rotational velocities. The Kepler light curve was analyzed with an iterative procedure devised to disentangle eclipses from pulsations which takes into account the visibility of the pulsating star during eclipses. The search for the best set of binary parameters was performed combining the synthetic light curve models with a genetic minimization algorithm, which yielded a robust and accurate determination of the system parameters. The binary components have very similar masses (1.88 and 1.86 Msun) and effective temperatures (6800 and 6600 K), but different radii (3.45 and 3.05 Rsun). The comparison with the theoretical models evidenced a somewhat different evolutionary status of the components and the need of introducing overshooting in the models. The pulsation analysis indicates a hybrid nature of the pulsating (secondary) component, the corresponding high order g-modes might be excited by an intrinsic mechanism or by tidal forces.
Long-term continuous assessment of SRAM PUF and source of random numbers<|sep|>The qualities of Physical Unclonable Functions (PUFs) suffer from several noticeable degradations due to silicon aging. In this paper, we investigate the long-term effects of silicon aging on PUFs derived from the start-up behavior of Static Random Access Memories (SRAM). Previous research on SRAM aging is based on transistor-level simulation or accelerated aging test at high temperature and voltage to observe aging effects within a short period of time. In contrast, we have run a long-term continuous power-up test on 16 Arduino Leonardo boards under nominal conditions for two years. In total, we collected around 175 million measurements for reliability, uniqueness and randomness evaluations. Analysis shows that the number of bits that flip with respect to the reference increased by 19.3% while min-entropy of SRAM PUF noise improves by 19.3% on average after two years of aging. The impact of aging on reliability is smaller under nominal conditions than was previously assessed by the accelerated aging test. The test we conduct in this work more closely resembles the conditions of a device in the field, and therefore we more accurately evaluate how silicon aging affects SRAM PUFs.
Analysis of High Impedance Coils both in Transmission and Reception Regimes<|sep|>Theory of a high impedance coil (HIC) - a cable loop antenna with a modified shield - is discussed comprehensively for both in transmitting and receiving regimes. Understanding a weakness of the previously reported HIC in transmitting regime, we suggest another HIC which is advantageous in both transmitting and receiving regimes compared to a conventional loop antenna. In contrast with a claim of previous works, only this HIC is a practical transceiver HIC. Using the perturbation approach and adding gaps to both shield and inner wire of the cable, we tune the resonance frequency to be suitable for ultra-high field (UHF) magnetic resonance imaging (MRI). Our theoretical model is verified by simulations. Designing the HIC theoretically, we have fabricated an array of three HICs operating at 298 MHz. The operation of the array has been experimentally studied in presence of different phantoms used in UHF MRI and the results compared with those obtained for a conventional array.
ACK-Less Rate Adaptation for IEEE 802.11bc Enhanced Broadcast Services Using Sim-to-Real Deep Reinforcement Learning<|sep|>In IEEE 802.11bc, the broadcast mode on wireless local area networks (WLANs), data rate control that is based on acknowledgement (ACK) mechanism similar to the one in the current IEEE 802.11 WLANs is not applicable because ACK mechanism is not implemented. This paper addresses this challenge by proposing ACK-less data rate adaptation methods by capturing non-broadcast uplink frames of STAs. In IEEE 802.11bc, an use case is assumed, where a part of STAs in the broadcast recipients is also associated with non-broadcast APs, and such STAs periodically transmit uplink frames including ACK frames. The proposed method is based on the idea that by overhearing such uplink frames, the broadcast AP surveys channel conditions at partial STAs, thereby setting appropriate data rates for the STAs. Furthermore, in order to avoid reception failures in a large portion of STAs, this paper proposes deep reinforcement learning (DRL)-based data rate adaptation framework that uses a sim-to-real approach. Therein, information of reception success/failure at broadcast recipient STAs, that could not be notified to the broadcast AP in real deployments, are made available by simulations beforehand, thereby forming data rate adaptation strategies. Numerical results show that utilizing overheard uplink frames of recipients makes it feasible to manage data rates in ACK-less broadcast WLANs, and using the sim-to-real DRL framework can decrease reception failures.
All-coupling polaron optical response: analytic approaches beyond the adiabatic approximation<|sep|>In the present work, the problem of an all-coupling analytic description for the optical conductivity of the Froehlich polaron is treated, with the goal being to bridge the gap in validity range that exists between two complementary methods: on the one hand the memory function formalism and on the other hand the strong-coupling expansion based on the Franck-Condon picture for the polaron response. At intermediate coupling, both methods were found to fail as they do not reproduce Diagrammatic Quantum Monte Carlo results. To resolve this, we modify the memory function formalism with respect to the Feynman-Hellwarth-Iddings-Platzman (FHIP) approach, in order to take into account a non-quadratic interaction in a model system for the polaron. The strong-coupling expansion is extended beyond the adiabatic approximation, by including into the treatment non-adiabatic transitions between excited polaron states. The polaron optical conductivity that we obtain by combining the two extended methods agree well, both qualitatively and quantitatively, with the Diagrammatic Quantum Monte Carlo results in the whole available range of the electron-phonon coupling strength.
Reinforcement Learning approach for Real Time Strategy Games Battle city and S3<|sep|>In this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning and SARSA algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on two real-time strategy games called BattleCity and S3. There are two main advantages of having such an approach as compared to other works in RTS. (1) We can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of RTS games (2) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works. Keywords : Reinforcement learning, Machine learning, Real time strategy, Artificial intelligence.
Effect of hydrophobic solutes on the liquid-liquid critical point<|sep|>Jagla ramp particles, interacting through a ramp potential with two characteristic length scales, are known to show in their bulk phase thermodynamic and dynamic anomalies, similar to what is found in water. Jagla particles also exhibit a line of phase transitions separating a low density liquid phase and a high density liquid phase, terminating in a liquid-liquid critical point in a region of the phase diagram that can be studied by simulations. Employing molecular dynamics computer simulations, we study the thermodynamics and the dynamics of solutions of hard spheres (HS) in a solvent formed by Jagla ramp particles. We consider the cases of HS mole fraction x = 0.10, 0.15 and 0.20, and also the case x = 0.50 (a 1:1 mixture of HS and Jagla particles). We find a liquid-liquid critical point, up to the highest HS mole fraction; its position shifts to higher pressures and lower temperatures upon increasing x. We also find that the diffusion coefficient anomalies appear to be preserved for all the mole fractions studied.
Deep Reinforcement Learning based Robot Navigation in Dynamic Environments using Occupancy Values of Motion Primitives<|sep|>This paper presents a Deep Reinforcement Learning based navigation approach in which we define the occupancy observations as heuristic evaluations of motion primitives, rather than using raw sensor data. Our method enables fast mapping of the occupancy data, generated by multi-sensor fusion, into trajectory values in 3D workspace. The computationally efficient trajectory evaluation allows dense sampling of the action space. We utilize our occupancy observations in different data structures to analyze their effects on both training process and navigation performance. We train and test our methodology on two different robots within challenging physics-based simulation environments including static and dynamic obstacles. We benchmark our occupancy representations with other conventional data structures from state-of-the-art methods. The trained navigation policies are also validated successfully with physical robots in dynamic environments. The results show that our method not only decreases the required training time but also improves the navigation performance as compared to other occupancy representations. The open-source implementation of our work and all related info are available at \url{https://github.com/RIVeR-Lab/tentabot}.
Cosmic Ray Acceleration in Supernova Remnants<|sep|>We review the main observational and theoretical facts about acceleration of Galactic cosmic rays in supernova remnants, discussing the arguments in favor and against a connection between cosmic rays and supernova remnants, the so-called supernova remnant paradigm for the origin of Galactic cosmic rays. Recent developments in the modeling of the mechanism of diffusive shock acceleration are discussed, with emphasis on the role of 1) magnetic field amplification, 2) acceleration of nuclei heavier than hydrogen, 3) presence of neutrals in the circumstellar environment. The status of the supernova-cosmic ray connection in the time of Fermi-LAT and Cherenkov telescopes is also discussed.
Giant Amplification in Degenerate Band Edge Slow-Wave Structures Interacting with an Electron Beam<|sep|>We propose a new amplification regime based on synchronous operation of four degenerate electromagnetic (EM) modes in a slow-wave structure and the electron beam, referred to as super synchronization. These four EM modes arise in a Fabry-Perot cavity (FPC) when degenerate band edge (DBE) condition is satisfied. The modes interact constructively with the electron beam resulting in superior amplification. In particular, much larger gains are achieved for smaller beam currents compared to conventional structures based on synchronization with only a single EM mode. We demonstrate giant gain scaling with respect to the length of the slow-wave structure compared to conventional Pierce type single mode traveling wave tube amplifiers. We construct a coupled transmission line (CTL) model for a loaded waveguide slow-wave structure exhibiting a DBE, and investigate the phenomenon of giant gain via super synchronization using the Pierce model generalized to multimode interaction.
A novel delayed-choice experimental proposal testing local decisions<|sep|>Entangled states are notoriously non-separable, their sub-ensembles being only statistical mixtures yielding no coherences and no quantum interference phenomena. The interesting features of entangled states can be revealed only by coincidence counts over the (typically) two sub-ensembles of the system. In this paper we show that this feature extends to properties thought to be local, for example the transmissivity coefficient of a beam splitter. We discuss a well-known experimental setup and propose modifications, so that delayed-choice can be added and this new feature of entanglement tested.
Footprints of Supersymmetry on Higgs Decay<|sep|>Motivated by future collider proposals that aim to measure the Higgs properties precisely, we study the partial decay widths of the lightest Higgs boson in the minimal supersymmetric standard model with an emphasis on the parameter region where all superparticles and heavy Higgs bosons are not accessible at the LHC. Taking account of phenomenological constraints such as the Higgs mass, flavor constraints, vacuum stability, and perturbativity of coupling constants up to the grand unification scale, we discuss how large the deviations of the partial decay widths from the standard model predictions can be. These constraints exclude large fraction of the parameter region where the Higgs widths show significant deviation from the standard model predictions. Nevertheless, even if superparticles and the heavy Higgses are out of the reach of 14TeV LHC, the deviation may be large enough to be observed at future $e^+e^-$ collider experiments.
Author-topic profiles for academic search<|sep|>We implemented and evaluated a two-stage retrieval method for personalized academic search in which the initial search results are re-ranked using an author-topic profile. In academic search tasks, the user's own data can help optimizing the ranking of search results to match the searcher's specific individual needs. The author-topic profile consists of topic-specific terms, stored in a graph. We re-rank the top-1000 retrieved documents using ten features that represent the similarity between the document and the author-topic graph. We found that the re-ranking gives a small but significant improvement over the reproduced best method from the literature. Storing the profile as a graph has a number of advantages: it is flexible with respect to node and relation types; it is a visualization of knowledge that is interpretable by the user, and it offers the possibility to view relational characteristics of individual nodes.
Dermatologist Level Dermoscopy Skin Cancer Classification Using Different Deep Learning Convolutional Neural Networks Algorithms<|sep|>In this paper, the effectiveness and capability of convolutional neural networks have been studied in the classification of 8 skin diseases. Different pre-trained state-of-the-art architectures (DenseNet 201, ResNet 152, Inception v3, InceptionResNet v2) were used and applied on 10135 dermoscopy skin images in total (HAM10000: 10015, PH2: 120). The utilized dataset includes 8 diagnostic categories - melanoma, melanocytic nevi, basal cell carcinoma, benign keratosis, actinic keratosis and intraepithelial carcinoma, dermatofibroma, vascular lesions, and atypical nevi. The aim is to compare the ability of deep learning with the performance of highly trained dermatologists. Overall, the mean results show that all deep learning models outperformed dermatologists (at least 11%). The best ROC AUC values for melanoma and basal cell carcinoma are 94.40% (ResNet 152) and 99.30% (DenseNet 201) versus 82.26% and 88.82% of dermatologists, respectively. Also, DenseNet 201 had the highest macro and micro averaged AUC values for overall classification (98.16%, 98.79%, respectively).
A higher rank extension of the Askey-Wilson Algebra<|sep|>A novel generalization of the Askey-Wilson algebra is presented and shown to be associated with coproducts in the quantum algebra $U_q(su(1,1))$. This algebra has 15 non-commuting generators given by $Q^{(A)}$, with $A\subset \{1,2,3,4\}$ and their 5 linearly independent inversions generated by the algebra automorphism $q\rightarrow q^{-1}$, $E\leftrightarrow F.$ The set of generators can be split into operators fixed under inversion, and those with an orientation under this inversion. We then show that the generators will either commute or satisfy q-commutator relations linear in the generators, with the restriction that parity operators commute with generators of opposite parity and the q-commutation relations are between those with the same parity. Finally, we give a novel algebra expression satisfied by the generators involving only the natural generators, i.e. those arising from the coupling scheme.
Energy-efficient Routing of Hybrid Vehicles<|sep|>We consider a constrained shortest path problem with two resources. These two resources can be converted into each other in a particular manner. Our practical application is the energy optimal routing of hybrid vehicles. Due to the possibility of converting fuel into electric energy this setting adds new characteristics and new combinatorial possibilities to the common constrained shortest path problem (CSP). We formulate the resulting problem as a generalization of CSP. We show that optimal paths in this model may contain cycles and we state conditions to prevent them. The main contribution is a polynomial-time approximation scheme and a simpler approximation algorithm for computing energy-optimal paths in graphs.
Simple approach for calculating the binding free energy of a multivalent particle<|sep|>We present a simple yet accurate numerical approach to compute the free energy of binding of multivalent objects on a receptor-coated surface. The method correctly accounts for the fact that one ligand can bind to at most one receptor. The numerical approach is based on a saddle-point approximation to the computation of a complex residue. We compare our theory with the powerful Valence-Limited Interaction Theory (VLIT) (J. Chem. Phys. 137, 094108(2012), J. Chem. Phys. 138, 021102(2013)) and find excellent agreement in the regime where that theory is expected to work. However, the present approach even works for low receptor/ligand densities, where VLIT breaks down.
Improved Total Variation based Image Compressive Sensing Recovery by Nonlocal Regularization<|sep|>Recently, total variation (TV) based minimization algorithms have achieved great success in compressive sensing (CS) recovery for natural images due to its virtue of preserving edges. However, the use of TV is not able to recover the fine details and textures, and often suffers from undesirable staircase artifact. To reduce these effects, this letter presents an improved TV based image CS recovery algorithm by introducing a new nonlocal regularization constraint into CS optimization problem. The nonlocal regularization is built on the well known nonlocal means (NLM) filtering and takes advantage of self-similarity in images, which helps to suppress the staircase effect and restore the fine details. Furthermore, an efficient augmented Lagrangian based algorithm is developed to solve the above combined TV and nonlocal regularization constrained problem. Experimental results demonstrate that the proposed algorithm achieves significant performance improvements over the state-of-the-art TV based algorithm in both PSNR and visual perception.
The Herschel view of the dominant mode of galaxy growth from z=4 to the present day<|sep|>We present an analysis of the deepest Herschel images in four major extragalactic fields GOODS-North, GOODS-South, UDS and COSMOS obtained within the GOODS-Herschel and CANDELS-Herschel key programs. The picture provided by 10497 individual far-infrared detections is supplemented by the stacking analysis of a mass-complete sample of 62361 star-forming galaxies from the CANDELS-HST H band-selected catalogs and from two deep ground-based Ks band-selected catalogs in the GOODS-North and the COSMOS-wide fields, in order to obtain one of the most accurate and unbiased understanding to date of the stellar mass growth over the cosmic history. We show, for the first time, that stacking also provides a powerful tool to determine the dispersion of a physical correlation and describe our method called "scatter stacking" that may be easily generalized to other experiments. We demonstrate that galaxies of all masses from z=4 to 0 follow a universal scaling law, the so-called main sequence of star-forming galaxies. We find a universal close-to-linear slope of the logSFR-logM* relation with evidence for a flattening of the main sequence at high masses (log(M*/Msun) > 10.5) that becomes less prominent with increasing redshift and almost vanishes by z~2. This flattening may be due to the parallel stellar growth of quiescent bulges in star-forming galaxies. Within the main sequence, we measure a non varying SFR dispersion of 0.3 dex. The specific SFR (sSFR=SFR/M*) of star-forming galaxies is found to continuously increase from z=0 to 4. Finally we discuss the implications of our findings on the cosmic SFR history and show that more than 2/3 of present-day stars must have formed in a regime dominated by the main sequence mode. As a consequence we conclude that, although omnipresent in the distant Universe, galaxy mergers had little impact in shaping the global star formation history over the last 12.5 Gyr.
DeFlow: Learning Complex Image Degradations from Unpaired Data with Conditional Flows<|sep|>The difficulty of obtaining paired data remains a major bottleneck for learning image restoration and enhancement models for real-world applications. Current strategies aim to synthesize realistic training data by modeling noise and degradations that appear in real-world settings. We propose DeFlow, a method for learning stochastic image degradations from unpaired data. Our approach is based on a novel unpaired learning formulation for conditional normalizing flows. We model the degradation process in the latent space of a shared flow encoder-decoder network. This allows us to learn the conditional distribution of a noisy image given the clean input by solely minimizing the negative log-likelihood of the marginal distributions. We validate our DeFlow formulation on the task of joint image restoration and super-resolution. The models trained with the synthetic data generated by DeFlow outperform previous learnable approaches on three recent datasets. Code and trained models are available at: https://github.com/volflow/DeFlow
Isotropic MIMO Interference Channels without CSIT: The Loss of Degrees of Freedom<|sep|>This paper studies two-user MIMO interference channel with isotropic fading. We assume that users are equipped with arbitrary number of antennas and the channel state information (CSI) is available at receivers only. An outer bound is obtained for the degree of freedom region, which suggests the loss of degrees of freedom due to the lack of CSI at transmitters under many circumstances.
Electrical Properties of Selective-Area-Grown Superconductor-Semiconductor Hybrid Structures on Silicon<|sep|>We present a superconductor-semiconductor material system that is both scalable and monolithically integrated on a silicon substrate. It uses selective area growth of Al-InAs hybrid structures on a planar III-V buffer layer, grown directly on a high resistivity silicon substrate. We characterized the electrical properties of this material system at millikelvin temperatures and observed a high average field-effect mobility of $\mu \approx 3200\,\mathrm{cm^2/Vs}$ for the InAs channel, and a hard induced superconducting gap. Josephson junctions exhibited a high interface transmission, $\mathcal{T} \approx 0.75 $, gate voltage tunable switching current with a product of critical current and normal state resistance, $I_{\mathrm{C}}R_{\mathrm{N}} \approx 83\,\mathrm{\mu V}$, and signatures of multiple Andreev reflections. These results pave the way for scalable and high coherent gate voltage tunable transmon devices and other superconductor-semiconductor hybrids fabricated directly on silicon.
Direct detection of pixel-level myocardial infarction areas via a deep-learning algorithm<|sep|>Accurate detection of the myocardial infarction (MI) area is crucial for early diagnosis planning and follow-up management. In this study, we propose an end-to-end deep-learning algorithm framework (OF-RNN ) to accurately detect the MI area at the pixel level. Our OF-RNN consists of three different function layers: the heart localization layers, which can accurately and automatically crop the region-of-interest (ROI) sequences, including the left ventricle, using the whole cardiac magnetic resonance image sequences; the motion statistical layers, which are used to build a time-series architecture to capture two types of motion features (at the pixel-level) by integrating the local motion features generated by long short-term memory-recurrent neural networks and the global motion features generated by deep optical flows from the whole ROI sequence, which can effectively characterize myocardial physiologic function; and the fully connected discriminate layers, which use stacked auto-encoders to further learn these features, and they use a softmax classifier to build the correspondences from the motion features to the tissue identities (infarction or not) for each pixel. Through the seamless connection of each layer, our OF-RNN can obtain the area, position, and shape of the MI for each patient. Our proposed framework yielded an overall classification accuracy of 94.35% at the pixel level, from 114 clinical subjects. These results indicate the potential of our proposed method in aiding standardized MI assessments.
Modelling dynamic programming problems by generalized d-graphs<|sep|>In this paper we introduce the concept of generalized d-graph (admitting cycles) as special dependency-graphs for modelling dynamic programming (DP) problems. We describe the d-graph versions of three famous single-source shortest algorithms (The algorithm based on the topological order of the vertices, Dijkstra algorithm and Bellman-Ford algorithm), which can be viewed as general DP strategies in the case of three different class of optimization problems. The new modelling method also makes possible to classify DP problems and the corresponding DP strategies in term of graph theory.
Verifying Programs via Intermediate Interpretation<|sep|>We explore an approach to verification of programs via program transformation applied to an interpreter of a programming language. A specialization technique known as Turchin's supercompilation is used to specialize some interpreters with respect to the program models. We show that several safety properties of functional programs modeling a class of cache coherence protocols can be proved by a supercompiler and compare the results with our earlier work on direct verification via supercompilation not using intermediate interpretation. Our approach was in part inspired by an earlier work by De E. Angelis et al. (2014-2015) where verification via program transformation and intermediate interpretation was studied in the context of specialization of constraint logic programs.
Reinforcement Learning of POMDPs using Spectral Methods<|sep|>We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.
Evanescent Wave Approximation for Non-Hermitian Hamiltonians<|sep|>The counterpart of the rotating wave approximation for non-Hermitian Hamiltonians is considered, which allows for the derivation of a suitable effective Hamiltonian for systems with some states undergoing decays. In the limit of very high decay rates, on the basis of this effective description we can predict the occurrence of a quantum Zeno dynamics which is interpreted as the removal of some coupling terms and the vanishing of an operatorial pseudo-Lamb shift.
Hybrid synchronization in coupled ultracold atomic gases<|sep|>We study the time evolution of two coupled many-body quantum systems one of which is assumed to be Bose condensed. Specifically, we consider two ultracold atomic clouds populating each two localized single-particle states, i.e. a two-component Bosonic Josephson junction. The cold atoms cloud can retain its coherence when coupled to the condensate and displays synchronization with the latter, differing from usual entrainment. We term this effect among the ultracold and the condensed clouds as {\it hybrid synchronization}. The onset of synchronization, which we observe in the evolution of average properties of both gases when increasing their coupling, is found to be related to the many-body properties of the quantum gas, e.g. condensed fraction, quantum fluctuations of the particle number differences. We discuss the effects of different initial preparations, the influence of unequal particle numbers for the two clouds, and explore the dependence on the initial quantum state, e.g. coherent state, squeezed state and Fock state, finding essentially the same phenomenology in all cases.
Iterative Pseudo-Labeling with Deep Feature Annotation and Confidence-Based Sampling<|sep|>Training deep neural networks is challenging when large and annotated datasets are unavailable. Extensive manual annotation of data samples is time-consuming, expensive, and error-prone, notably when it needs to be done by experts. To address this issue, increased attention has been devoted to techniques that propagate uncertain labels (also called pseudo labels) to large amounts of unsupervised samples and use them for training the model. However, these techniques still need hundreds of supervised samples per class in the training set and a validation set with extra supervised samples to tune the model. We improve a recent iterative pseudo-labeling technique, Deep Feature Annotation (DeepFA), by selecting the most confident unsupervised samples to iteratively train a deep neural network. Our confidence-based sampling strategy relies on only dozens of annotated training samples per class with no validation set, considerably reducing user effort in data annotation. We first ascertain the best configuration for the baseline -- a self-trained deep neural network -- and then evaluate our confidence DeepFA for different confidence thresholds. Experiments on six datasets show that DeepFA already outperforms the self-trained baseline, but confidence DeepFA can considerably outperform the original DeepFA and the baseline.
Microwave-controlled coupling of Majorana bound states<|sep|>We propose microwave-controlled rotations for qubits realized as Majorana bound states. To this end we study an inhomogeneous Kitaev chain in a microwave cavity. The chain consists of two topologically nontrivial regions separated by a topologically trivial, gapped region. The Majorana bound states at the interfaces between the left (right) regions and the central region are coupled, and their energies are split by virtual cotunneling processes. The amplitude for these cotunneling processes decreases exponentially in the number of sites of the gapped region, and the decay length diverges as the gap of the topologically trivial region closes. We demonstrate that microwave radiation can exponentially enhance the coupling between the Majorana bound states, both for classical and quantized electric fields. By solving the appropriate Liouville equation numerically we show that microwaves can drive Rabi oscillations in the Majorana sector. Our model emerges as an effective description for a topological semiconductor nanowire in a microwave cavity. Thus, our proposal provides an experimentally feasible way to obtain full single-qubit control necessary for universal quantum computation with Majorana qubits.
"Distance mapping" and the 3-D structure of BB +30 3639<|sep|>BD +30 3639 is a member of a group of uncommon planetary nebula with Wolf-Rayet central star and higher expansion velocities in [O III] than in [N II] lines. Images and high-resolution spectra from the literature are used in order to construct a 3-D model of the nebula using the morpho-kinematic code SHAPE. We find that two homologous expansion laws are needed for the [N II] and [O III] shell. We conclude that the internal velocity field of BD +30 3639 decreases with the distance from the central star at least between the [O III] and [N II] shells. A cylindrical velocity component is used to replicate the high-speed bipolar collimated outflows. We also present a new kinematic analysis technique called "distance mapping". It uses the observed proper motion vectors and the 3-D velocity field to generate maps that can be used as a constraint to the morpho-kinematic modeling with SHAPE as well as improve the accuracy for distance determination. It is applied to BD+30 3639 using 178 internal proper motion vectors from Li et al. (2002) and our 3-D velocity field to determine a distance of 1.52 \pm 0.21 kpc. Finally, we find evidence for an interaction between the eastern part of nebula and the ambient H2 molecular gas.
A Tentative Detection of a Starspot During Consecutive Transits of an Extrasolar Planet from the Ground: No Evidence of a Double Transiting Planet System Around TrES-1<|sep|>There have been numerous reports of anomalies during transits of the planet TrES-1b. Recently, Rabus and coworkers' analysis of HST observations lead them to claim brightening anomalies during transit might be caused by either a second transiting planet or a cool starspot. Observations of two consecutive transits are presented here from the University of Arizona's 61-inch Kuiper Telescope on May 12 and May 15, 2008 UT. A 5.4 +/- 1.7 mmag (0.54 +/- 0.17%) brightening anomaly was detected during the first half of the transit on May 12 and again in the second half of the transit on May 15th. We conclude that this is a tentative detection of a r greater than or equal to 6 earth radii starspot rotating on the surface of the star. We suggest that all evidence to date suggest TrES-1 has a spotty surface and there is no need to introduce a second transiting planet in this system to explain these anomalies. We are only able to constrain the rotational period of the star to 40.2 +22.9 -14.6 days, due to previous errors in measuring the alignment of the stellar spin axis with the planetary orbital axis. This is consistent with the previously observed P_obs = 33.2 +22.3 -14.3 day period. We note that this technique could be applied to other transiting systems for which starspots exist on the star in the transit path of the planet in order to constrain the rotation rate of the star. (abridged)
The 2D pn Junction Driven Out-of-Equilibrium<|sep|>The pn junction is a fundamental electrical component in modern electronics and optoelectronics. Currently, there is a great deal of interest in the two-dimensional (2D) pn junction. Although many experiments have demonstrated the working principle, there is a lack of fundamental understanding of its basic properties and expected performances, in particular when the device is driven out of equilibrium. To fill the current gap in understanding, we investigate the electrostatics and electronic transport of 2D lateral pn junctions. To do so we implement a physics-based simulator that selfconsistently solves the 2D Poisson's equation coupled to the drift-diffusion and continuity equations. Notably, the simulator takes into account the strong influence of the out of plane electric field through the surrounding dielectric, capturing the weak screening of charge carriers. Supported by simulations, we propose a Shockley-like equation for the ideal current voltage characteristics, in full analogy to the bulk junction after defining an effective depletion layer (EDL). We also discuss the impact of recombination generation processes inside the EDL, which actually produce a significant deviation with respect to the ideal behavior, consistently with experimental data. Moreover, we analyze the capacitances and conductance of the 2D lateral pn junction. Based on its equivalent circuit we investigate its cut-off frequency targeting RF applications. To gain deeper insight into the role played by material dimensionality, we benchmark the performances of single-layer MoS2 (2D) lateral pn junctions against those of the Si (3D) junction. Finally, a practical discussion on the short length 2D junction case together with the expected impact of interface states has been provided. Given the available list of 2D materials, this work opens the door to a wider exploration of material dependent performances.
Effective Combination of Language and Vision Through Model Composition and the R-CCA Method<|sep|>We address the problem of integrating textual and visual information in vector space models for word meaning representation. We first present the Residual CCA (R-CCA) method, that complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared, max correlation, space. We then show that constructing visual and textual representations and then post-processing them through composition of common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (a.k.a sequential modeling) yields high quality models. On five standard semantic benchmarks our sequential models outperform recent multimodal representation learning alternatives, including ones that rely on joint representation learning. For two of these benchmarks our R-CCA method is part of the Best configuration our algorithm yields.
Stability of resonant configurations during the migration of planets and constraints on disk-planet interactions<|sep|>We study the stability of mean-motion resonances (MMR) between two planets during their migration in a protoplanetary disk. We use an analytical model of resonances, and describe the effect of the disk by a migration timescale (T_{m,i}) and an eccentricity damping timescale (T_{e,i}) for each planet (i=1,2 respectively for the inner and outer planet). We show that the resonant configuration is stable if T_{e,1}/T_{e,2} > (e_1/e_2)^2. This general result can be used to put constraints on specific models of disk-planet interactions. For instance, using classical prescriptions for type I migration, we show that when the angular momentum deficit (AMD) of the inner orbit is larger than the outer's orbit AMD, resonant systems must have a locally inverted disk density profile to stay locked in resonance during the migration. This inversion is very untypical of type I migration and our criterion can thus provide an evidence against classical type I migration. That is indeed the case for the Jupiter-mass resonant systems HD 60532b, c (3:1 MMR), GJ 876b, c (2:1 MMR), and HD 45364b, c (3:2 MMR). This result may be an evidence for type II migration (gap opening planets), which is compatible with the large masses of these planets.
PCNNA: A Photonic Convolutional Neural Network Accelerator<|sep|>Convolutional Neural Networks (CNN) have been the centerpiece of many applications including but not limited to computer vision, speech processing, and Natural Language Processing (NLP). However, the computationally expensive convolution operations impose many challenges to the performance and scalability of CNNs. In parallel, photonic systems, which are traditionally employed for data communication, have enjoyed recent popularity for data processing due to their high bandwidth, low power consumption, and reconfigurability. Here we propose a Photonic Convolutional Neural Network Accelerator (PCNNA) as a proof of concept design to speedup the convolution operation for CNNs. Our design is based on the recently introduced silicon photonic microring weight banks, which use broadcast-and-weight protocol to perform Multiply And Accumulate (MAC) operation and move data through layers of a neural network. Here, we aim to exploit the synergy between the inherent parallelism of photonics in the form of Wavelength Division Multiplexing (WDM) and sparsity of connections between input feature maps and kernels in CNNs. While our full system design offers up to more than 3 orders of magnitude speedup in execution time, its optical core potentially offers more than 5 order of magnitude speedup compared to state-of-the-art electronic counterparts.
Self-Attentional Acoustic Models<|sep|>Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.
Fast and Furious: Shock Heated Gas as the Origin of Spatially Resolved Hard X-ray Emission in the Central 5 kpc of the Galaxy Merger NGC 6240<|sep|>We have obtained a deep, sub-arcsecond resolution X-ray image of the nuclear region of the luminous galaxy merger NGC 6240 with Chandra, which resolves the X-ray emission from the pair of active nuclei and the diffuse hot gas in great detail. We detect extended hard X-ray emission from kT~6 keV (~70 million K) hot gas over a spatial scale of 5 kpc, indicating the presence of fast shocks with velocity of ~2200 km/s. For the first time we obtain the spatial distribution of this highly ionized gas emitting FeXXV, which shows a remarkable correspondence to the large scale morphology of H_2(1-0) S(1) line emission and H\alpha filaments. Propagation of fast shocks originated in the starburst driven wind into the ambient dense gas can account for this morphological correspondence. With an observed L(0.5-8 keV)=5.3E+41 erg/s, the diffuse hard X-ray emission is 100 times more luminous than that observed in the classic starburst galaxy M82. Assuming a filling factor of 1% for the 70 MK temperature gas, we estimate its total mass (M_{hot}=1.8E+8 Msun) and thermal energy (E_{th}=6.5E+57 ergs). The total iron mass in the highly ionized plasma is M_{Fe}=4.6E+5 Msun. Both the energetics and the iron mass in the hot gas are consistent with the expected injection from the supernovae explosion during the starburst that is commensurate with its high star formation rate. No evidence for fluorescent Fe I emission is found in the CO filament connecting the two nuclei.
Role of Secondary Attributes to Boost the Prediction Accuracy of Students Employability Via Data Mining<|sep|>Data Mining is best-known for its analytical and prediction capabilities. It is used in several areas such as fraud detection, predicting client behavior, money market behavior, bankruptcy prediction. It can also help in establishing an educational ecosystem, which discovers useful knowledge, and assist educators to take proactive decisions to boost student performance and employability. This paper presents an empirical study that compares varied classification algorithms on two datasets of MCA (Masters in Computer Applications) students collected from various affiliated colleges of a reputed state university in India. One dataset includes only primary attributes, whereas other dataset is feeded with secondary psychometric attributes in it. The results showcase that solely primary academic attributes do not lead to smart prediction accuracy of students employability, once they square measure within the initial year of their education. The study analyzes and stresses the role of secondary psychometric attributes for better prediction accuracy and analysis of students performance. Timely prediction and analysis of students performance can help Management, Teachers and Students to work on their gray areas for better results and employment opportunities.
Improved Feature Importance Computations for Tree Models: Shapley vs. Banzhaf<|sep|>Shapley values are one of the main tools used to explain predictions of tree ensemble models. The main alternative to Shapley values are Banzhaf values that have not been understood equally well. In this paper we make a step towards filling this gap, providing both experimental and theoretical comparison of these model explanation methods. Surprisingly, we show that Banzhaf values offer several advantages over Shapley values while providing essentially the same explanations. We verify that Banzhaf values: (1) have a more intuitive interpretation, (2) allow for more efficient algorithms, and (3) are much more numerically robust. We provide an experimental evaluation of these theses. In particular, we show that on real world instances. Additionally, from a theoretical perspective we provide new and improved algorithm computing the same Shapley value based explanations as the algorithm of Lundberg et al. [Nat. Mach. Intell. 2020]. Our algorithm runs in $O(TLD+n)$ time, whereas the previous algorithm had $O(TLD^2+n)$ running time bound. Here, $T$ is the number of trees, $L$ is the maximum number of leaves in a tree, and $D$ denotes the maximum depth of a tree in the ensemble. Using the computational techniques developed for Shapley values we deliver an optimal $O(TL+n)$ time algorithm for computing Banzhaf values based explanations. In our experiments these algorithms give running times smaller even by an order of magnitude.
The sequence space bv and some applications<|sep|>In this work, we give well-known results related to some properties, dual spaces and matrix transformations of the sequence space bv and introduce the matrix domain of space bv with arbitrary triangle matrix A. Afterward, we choose the matrix A as Ces\`aro mean of order one, generalized weighted mean and Riesz mean and compute alpha-; beta- gamma-duals of these spaces. And also, we characterize the matrix classes of the new spaces.
Building powerful and equivariant graph neural networks with structural message-passing<|sep|>Message-passing has proved to be an effective way to design graph neural networks, as it is able to leverage both permutation equivariance and an inductive bias towards learning local structures in order to achieve good generalization. However, current message-passing architectures have a limited representation power and fail to learn basic topological properties of graphs. We address this problem and propose a powerful and equivariant message-passing framework based on two ideas: first, we propagate a one-hot encoding of the nodes, in addition to the features, in order to learn a local context matrix around each node. This matrix contains rich local information about both features and topology and can eventually be pooled to build node representations. Second, we propose methods for the parametrization of the message and update functions that ensure permutation equivariance. Having a representation that is independent of the specific choice of the one-hot encoding permits inductive reasoning and leads to better generalization properties. Experimentally, our model can predict various graph topological properties on synthetic data more accurately than previous methods and achieves state-of-the-art results on molecular graph regression on the ZINC dataset.
From Laurent Series to Exact Meromorphic Solutions: the Kawahara equation<|sep|>Nonlinear waves are studied in a mixture of liquid and gas bubbles. Influence of viscosity and heat transfer is taken into consideration on propagation of the pressure waves. Nonlinear evolution equations of the second and the third order for describing nonlinear waves in gas-liquid mixtures are derived. Exact solutions of these nonlinear evolution equations are found. Properties of nonlinear waves in a liquid with gas bubbles are discussed.
A clustering-based biased Monte Carlo approach to protein titration curve prediction<|sep|>In this work, we developed an efficient approach to compute ensemble averages in systems with pairwise-additive energetic interactions between the entities. Methods involving full enumeration of the configuration space result in exponential complexity. Sampling methods such as Markov Chain Monte Carlo (MCMC) algorithms have been proposed to tackle the exponential complexity of these problems; however, in certain scenarios where significant energetic coupling exists between the entities, the efficiency of the such algorithms can be diminished. We used a strategy to improve the efficiency of MCMC by taking advantage of the cluster structure in the interaction energy matrix to bias the sampling. We pursued two different schemes for the biased MCMC runs and show that they are valid MCMC schemes. We used both synthesized and real-world systems to show the improved performance of our biased MCMC methods when compared to the regular MCMC method. In particular, we applied these algorithms to the problem of estimating protonation ensemble averages and titration curves of residues in a protein.
Background Free Quantum Gravity based on Conformal Gravity and Conformal Field Theory on M^4<|sep|>We study four dimensional quantum gravity formulated as a certain conformal field theory at the ultraviolet fixed point, whose dynamics is described by the combined system of Riegert-Wess-Zumino and Weyl actions. Background free nature comes out as quantum diffeomorphism symmetry by quantizing the conformal factor of the metric field nonperturbatively. In this paper, Minkowski background M^4 is employed in practice. The generator of quantum diffeomorphism that forms conformal algebra is constructed. Using it, we study the composite scalar operator that becomes a good conformal field. We find that physical fields are described by such scalar fields with conformal dimension 4. Consequently, tensor fields outside the unitarity bound are excluded. Computations of quantum algebra on M^4 are carried out in the coordinate space using operator products of the fields. The nilpotent BRST operator is also constructed.
Energy benchmarks for water clusters and ice structures from an embedded many-body expansion<|sep|>We show how an embedded many-body expansion (EMBE) can be used to calculate accurate \emph{ab initio} energies of water clusters and ice structures using wavefunction-based methods. We use the EMBE described recently by Bygrave \emph{et al.} (J. Chem. Phys. \textbf{137}, 164102 (2012)), in which the terms in the expansion are obtained from calculations on monomers, dimers, etc. acted on by an approximate representation of the embedding field due to all other molecules in the system, this field being a sum of Coulomb and exchange-repulsion fields. Our strategy is to separate the total energy of the system into Hartree-Fock and correlation parts, using the EMBE only for the correlation energy, with the Hartree-Fock energy calculated using standard molecular quantum chemistry for clusters and plane-wave methods for crystals. Our tests on a range of different water clusters up to the 16-mer show that for the second-order M\o{}ller-Plesset (MP2) method the EMBE truncated at 2-body level reproduces to better than 0.1 m$E_{\rm h}$/monomer the correlation energy from standard methods. The use of EMBE for computing coupled-cluster energies of clusters is also discussed. For the ice structures Ih, II and VIII, we find that MP2 energies near the complete basis-set limit reproduce very well the experimental values of the absolute and relative binding energies, but that the use of coupled-cluster methods for many-body correlation (non-additive dispersion) is essential for a full description. Possible future applications of the EMBE approach are suggested.
Magneto-elastic modes and lifetime of magnons in thin yttrium-iron garnet films<|sep|>We calculate the effects of the spin-lattice coupling on the magnon spectrum of thin ferromagnetic films consisting of the magnetic insulator yttrium-iron garnet. The magnon-phonon hybridisation generates a characteristic minimum in the spin dynamic structure factor which quantitatively agrees with recent Brillouin light scattering experiments. We also show that at room temperature the phonon contribution to the magnon damping exhibits a rather complicated momentum dependence: In the exchange regime the magnon damping is dominated by Cherenkov type scattering processes, while in the long-wavelength dipolar regime these processes are subdominant and the magnon damping is two orders of magnitude smaller. We supplement our calculations by actual measurements of the magnon relaxation in the dipolar regime. Our theory provides a simple explanation of a recent experiment probing the different temperatures of the magnon and phonon gases in yttrium-iron garnet.
The debts' clearing problem: a new approach<|sep|>The debts' clearing problem is about clearing all the debts in a group of $n$ entities (e.g. persons, companies) using a minimal number of money transaction operations. In our previous works we studied the problem, gave a dynamic programming solution solving it and proved that it is NP-hard. In this paper we adapt the problem to dynamic graphs and give a data structure to solve it. Based on this data structure we develop a new algorithm, that improves our previous one for the static version of the problem.
X-ray emission from high-redshift miniquasars: self-regulating the population of massive black holes through global warming<|sep|>Observations of high-redshift quasars at z>6 imply that supermassive black holes (SMBHs) with masses over a billion solar masses were in place less than 1 Gyr after the Big Bang. If these SMBHs assembled from "seed" BHs left behind by the first stars, then they must have accreted gas at close to the Eddington limit during a large fraction (>50%) of the time. A generic problem with this scenario, however, is that the mass density in million-solar-mass SMBHs at z=6 already exceeds the locally observed SMBH mass density by several orders of magnitude; in order to avoid this overproduction, BH seed formation and growth must become significantly less efficient in less massive protogalaxies, while proceeding uninterrupted in the most massive galaxies that formed first. Using Monte-Carlo realizations of the merger and growth history of BHs, we show that X-rays from the earliest accreting BHs can provide such a feedback mechanism. Our calculations paint a self-consistent picture of black-hole-made climate change, in which the first miniquasars---among them the ancestors of the z>6 quasar SMBHs---globally warm the IGM and suppress the formation and growth of subsequent generations of BHs. We present two specific models with global miniquasar feedback that provide excellent agreement with recent estimates of the z=6 SMBH mass function. For each of these models, we estimate the rate of BH mergers at z>6 that could be detected by the proposed gravitational-wave observatory eLISA/NGO.
Difficulty-aware Image Super Resolution via Deep Adaptive Dual-Network<|sep|>Recently, deep learning based single image super-resolution(SR) approaches have achieved great development. The state-of-the-art SR methods usually adopt a feed-forward pipeline to establish a non-linear mapping between low-res(LR) and high-res(HR) images. However, due to treating all image regions equally without considering the difficulty diversity, these approaches meet an upper bound for optimization. To address this issue, we propose a novel SR approach that discriminately processes each image region within an image by its difficulty. Specifically, we propose a dual-way SR network that one way is trained to focus on easy image regions and another is trained to handle hard image regions. To identify whether a region is easy or hard, we propose a novel image difficulty recognition network based on PSNR prior. Our SR approach that uses the region mask to adaptively enforce the dual-way SR network yields superior results. Extensive experiments on several standard benchmarks (e.g., Set5, Set14, BSD100, and Urban100) show that our approach achieves state-of-the-art performance.
Non-equilibrium noise in the (non-)Abelian fractional quantum Hall effect<|sep|>We analyse the noise of the edge current of a generic fractional quantum Hall state in a tunnelling point contact system. We show that the non-symmetrized noise in the edge current for the system out-of-equilibrium is completely determined by the noise in the tunnelling current and the Nyquist-Johnson (equilibrium) noise of the edge current. Simply put, the noise in the tunnelling current does not simply add up the equilibrium noise of the edge current. A correction term arises associated with the correlation between the tunnelling current and the edge current. We show, using a non-equilibrium Ward identity, that this correction term is determined by the anti-symmetric part of the noise in the tunnelling current. This leads to a non-equilibrium fluctuation-dissipation theorem and related expressions for the excess and shot noise of the noise in the edge current. Our approach makes use of simple properties of the edge, such as charge conservation and chirality, and applies to generic constructions of the edge theory which includes edges of non-Abelian states and edges with multiple charged channels. Two important tools we make use of are the non-equilibrium Kubo formula and the non-equilibrium Ward identity. We discuss these identities in the appendix.
Determination of the Jet Energy Scale<|sep|>The uncertainty in jet energy scale is one of the dominating systematic errors for many measurements at hadron colliders - most notably for the measurement of the top-quark-mass, inclusive jet cross section measurements and last but not least for events with large missing transverse energy as expected in searches beyond the standard model. This talk will review the approaches taken at Tevatron towards controlling the jet energy scale and discuss prospects for the LHC experiments.
A Uniform Retrieval Analysis of Ultracool Dwarfs. III. Properties of Y-Dwarfs<|sep|>Ultra-cool brown dwarfs offer a unique window into understanding substellar atmospheric physics and chemistry. Their strong molecular absorption bands at infrared wavelengths, Jupiter-like radii, cool temperatures, and lack of complicating stellar irradiation, make them ideal test-beds for understanding Jovian-like atmospheres. Here we report the findings of a uniform atmospheric retrieval analysis on a set of 14 Y and T-dwarfs observed with the Hubble Space Telescope Wide Field Camera 3 instrument. From our retrieval analysis, we find the temperature-structures to be largely consistent with radiative-convective equilibrium in most objects. We also determine the abundances of water, methane, and ammonia and upper limits on the alkali metals sodium and potassium. The constraints on water and methane are consistent with predictions from chemical equilibrium models, while those of ammonia may be affected by vertical disequilibrium mixing, consistent with previous works. Our key result stems from the constraints on the alkali metal abundances where we find their continued depletion with decreasing effective temperature, consistent with the trend identified in a previous retrieval analysis on a sample of slightly warmer late T-dwarfs in Line et al. (2017). These constraints show that the previously observed Y-J color trend across the T/Y transition is most likely due to the depletion of these metals in accordance with predictions from equilibrium condensate rainout chemistry. Finally, we simulate future James Webb Space Telescope observations of ultra-cool dwarfs and find that the NIRSpec PRISM offers the best chance at developing high-precision constraints on fundamental atmospheric characteristics.
Supernovae and Cosmology with Future European Facilities<|sep|>Prospects for future supernova surveys are discussed, focusing on the ESA Euclid mission and the European Extremely Large Telescope(E-ELT), both expected to be in operation around the turn of the decade. Euclid is a 1.2m space survey telescope that will operate at visible and near-infrared wavelengths, and has the potential to find and obtain multi-band lightcurves for thousands of distant supernovae. The E-ELT is a planned general-purpose ground-based 40m-class optical-IR telescope with adaptive optics built in, which will be capable of obtaining spectra of Type Ia supernovae to redshifts of at least four. The contribution to supernova cosmology with these facilities will be discussed in the context of other future supernova programs such as those proposed for DES, JWST, LSST and WFIRST.
Deep Diving into BitTorrent Locality<|sep|>A substantial amount of work has recently gone into localizing BitTorrent traffic within an ISP in order to avoid excessive and often times unnecessary transit costs. Several architectures and systems have been proposed and the initial results from specific ISPs and a few torrents have been encouraging. In this work we attempt to deepen and scale our understanding of locality and its potential. Looking at specific ISPs, we consider tens of thousands of concurrent torrents, and thus capture ISP-wide implications that cannot be appreciated by looking at only a handful of torrents. Secondly, we go beyond individual case studies and present results for the top 100 ISPs in terms of number of users represented in our dataset of up to 40K torrents involving more than 3.9M concurrent peers and more than 20M in the course of a day spread in 11K ASes. We develop scalable methodologies that permit us to process this huge dataset and answer questions such as: "\emph{what is the minimum and the maximum transit traffic reduction across hundreds of ISPs?}", "\emph{what are the win-win boundaries for ISPs and their users?}", "\emph{what is the maximum amount of transit traffic that can be localized without requiring fine-grained control of inter-AS overlay connections?}", "\emph{what is the impact to transit traffic from upgrades of residential broadband speeds?}".
Funnel Libraries for Real-Time Robust Feedback Motion Planning<|sep|>We consider the problem of generating motion plans for a robot that are guaranteed to succeed despite uncertainty in the environment, parametric model uncertainty, and disturbances. Furthermore, we consider scenarios where these plans must be generated in real-time, because constraints such as obstacles in the environment may not be known until they are perceived (with a noisy sensor) at runtime. Our approach is to pre-compute a library of "funnels" along different maneuvers of the system that the state is guaranteed to remain within (despite bounded disturbances) when the feedback controller corresponding to the maneuver is executed. We leverage powerful computational machinery from convex optimization (sums-of-squares programming in particular) to compute these funnels. The resulting funnel library is then used to sequentially compose motion plans at runtime while ensuring the safety of the robot. A major advantage of the work presented here is that by explicitly taking into account the effect of uncertainty, the robot can evaluate motion plans based on how vulnerable they are to disturbances. We demonstrate and validate our method using extensive hardware experiments on a small fixed-wing airplane avoiding obstacles at high speed (~12 mph), along with thorough simulation experiments of ground vehicle and quadrotor models navigating through cluttered environments. To our knowledge, these demonstrations constitute one of the first examples of provably safe and robust control for robotic systems with complex nonlinear dynamics that need to plan in real-time in environments with complex geometric constraints.
Quasi-Rip and Pseudo-Rip Universes Induced by the Fluid Inhomogeneous Equation of State<|sep|>We investigate specific models for a dark energy universe leading to Quasi-Rip and Pseudo-Rip cosmologies. In the Quasi-Rip model the equation of state parameter w is less than -1 in the first stage, but becomes larger than -1 in the second stage. In the Pseudo-Rip model the Hubble parameter tends to a constant value in the remote future, although w is always less than -1. Conditions for the appearance of the Quasi-Rip and the Pseudo-Rip in terms of the parameters in the equation of state are determined. Analogies with the theory of viscous cosmology are discussed.
Origin of optically passive spiral galaxies with dusty star-forming regions: Outside-in truncation of star formation?<|sep|>Recent observations have revealed that red, optically--passive spiral galaxies with little or no optical emission lines, harbour significant amounts of dust-obscured star formation. We propose that these observational results can be explained if the spatial distributions of the cold gas and star-forming regions in these spiral galaxies are significantly more compact than those in blue star-forming spirals. Our numerical simulations show that if the sizes of star-forming regions in spiral galaxies with disk sizes of R_d are ~ 0.3R_d, such galaxies appear to have lower star formation rates as well as higher degrees of dust extinction. This is mainly because star formation in these spirals occurs only in the inner regions where both the gas densities and metallicities are higher, and hence the dust extinction is also significantly higher. We discuss whether star formation occurring preferentially in the inner regions of spirals is closely associated with the stripping of halo and disk gas via some sort of environmental effect. We suggest that the "outside-in truncation of star formation" is the key to a better understanding of apparently optically--passive spirals with dusty star-forming regions.
Correlators in tensor models from character calculus<|sep|>We explain how the calculations of arXiv:1704.08648, which provided the first evidence for non-trivial structures of Gaussian correlators in tensor models, are efficiently performed with the help of the (Hurwitz) character calculus. This emphasizes a close similarity between technical methods in matrix and tensor models and supports a hope to understand the emerging structures in very similar terms. We claim that the $2m$-fold Gaussian correlators of rank $r$ tensors are given by $r$-linear combinations of dimensions with the Young diagrams of size $m$. The coefficients are made from the characters of the symmetric group $S_m$ and their exact form depends on the symmetries of the model. As the simplest application of this new knowledge, we provide simple expressions for correlators in the Aristotelian tensor model as tri-linear combinations of dimensions.
Accuracy Enhancement of Pickett Tunnelling Barrier Memristor Model<|sep|>Titanium dioxide (TiO2) memristors exhibit complex conduction mechanism. Several models of different complexity have been developed in order to mimic the experimental results for physical behaviors observed in memristor devices. Pickett's tunneling barrier model describes the TiO2 memristors, and utilizes complex derivative of tunnel barrier width. It attains a large error in the ON switching region. Variety of research consider it as the reference model for the TiO2 memristors. In this paper, we first analyze the theory of operation of the memristor and discuss Pickett's model. Then, we propose a modification to its derivative functions to provide a lower error and closer agreement with physical behavior. This modification is represented by two additional fitting parameters to damp or accelerate the tunnel width derivative. Also, we incorporate a hard limiter term to limit the tunnel width to its physical extremes 1 nm and 2 nm. We run simulations to test the model modifications and we compare the results to the experimental and original Pickett's model results. The modified model more closely resembles the experimental behavior of TiO2 memristors and potentially enables the memristor to be used as a multilevel memory.
Gradient Noise Convolution (GNC): Smoothing Loss Function for Distributed Large-Batch SGD<|sep|>Large-batch stochastic gradient descent (SGD) is widely used for training in distributed deep learning because of its training-time efficiency, however, extremely large-batch SGD leads to poor generalization and easily converges to sharp minima, which prevents naive large-scale data-parallel SGD (DP-SGD) from converging to good minima. To overcome this difficulty, we propose gradient noise convolution (GNC), which effectively smooths sharper minima of the loss function. For DP-SGD, GNC utilizes so-called gradient noise, which is induced by stochastic gradient variation and convolved to the loss function as a smoothing effect. GNC computation can be performed by simply computing the stochastic gradient on each parallel worker and merging them, and is therefore extremely easy to implement. Due to convolving with the gradient noise, which tends to spread along a sharper direction of the loss function, GNC can effectively smooth sharp minima and achieve better generalization, whereas isotropic random noise cannot. We empirically show this effect by comparing GNC with isotropic random noise, and show that it achieves state-of-the-art generalization performance for large-scale deep neural network optimization.
Overlapping domains for topology optimization of large-area metasurfaces<|sep|>We introduce an overlapping-domain approach to large-area metasurface design, in which each simulated domain consists of a unit cell and overlapping regions from the neighboring cells plus PML absorbers. We show that our approach generates greatly improved metalens quality compared to designs produced using a locally periodic approximation, thanks to $\sim 10\times$ better accuracy with similar computational cost. We use the new approach with topology optimization to design large-area ($200\lambda$) high-NA (0.71) multichrome and broadband achromatic lenses with high focusing efficiency ($\sim 50\%$), greatly improving upon previously reported works.
A Radiatively induced Elementary Goldstone Higgs in SU(4)/Sp(4)<|sep|>Using a $SU(4)\rightarrow Sp(4)$ pattern of chiral symmetry breaking, we investigate the pseudo-Goldstone nature of the Higgs boson in an elementary realisation that at the same time provides an ultraviolet completion. The renormalizability of the model together with the perturbative corrections determine dynamically the direction of vacuum of the theory and the corresponding Higgs chiral symmetry breaking scale $ f \sim 14~$TeV. The Higgs boson is radiatively generated and the scalar mass spectrum, together with a second massive Higgs boson, lie in the multi-TeV range.
Scientific Goals of the Kunlun Infrared Sky Survey (KISS)<|sep|>The high Antarctic plateau provides exceptional conditions for conducting infrared observations of the cosmos on account of the cold, dry and stable atmosphere above the ice surface. This paper describes the scientific goals behind the first program to examine the time-varying universe in the infrared from Antarctica - the Kunlun Infrared Sky Survey (KISS). This will employ a small (50 cm aperture) telescope to monitor the southern skies in the 2.4um Kdark window from China's Kunlun station at Dome A, on the summit of the Antarctic plateau, through the uninterrupted 4-month period of winter darkness. An earlier paper discussed optimisation of the Kdark filter for the best sensitivity (Li et al 2016). This paper examines the scientific program for KISS. We calculate the sensitivity of the camera for the extrema of observing conditions that will be encountered. We present the parameters for sample surveys that could then be carried out for a range of cadences and sensitivities. We then discuss several science programs that could be conducted with these capabilities, involving star formation, brown dwarfs and hot Jupiters, exoplanets around M dwarfs, the terminal phases of stellar evolution, discovering fast transients as part of multi-wavelength campaigns, embedded supernova searches, reverberation mapping of active galactic nuclei, gamma ray bursts and the detection of the cosmic infrared background. Accepted for publication in PASA, 04/08/16.
Heating the coffee by looking at it. Or why quantum measurements are physical processes<|sep|>Using a very simple Gedankenexperiment, I remind the reader that (contrary to what happens in classical mechanics) the energy of a quantum system is inevitably increased just by performing (some) textbook measurements on it. As a direct conclusion, this means that some measurements require the expenditure of a finite amount of energy to be carried out. I also argue that this makes it very difficult to regard measurements as disembodied, immaterial, informational operations, and it forces us to look at them as physical processes just like any other one.
DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range Data<|sep|>We introduce the DROW detector, a deep learning based detector for 2D range data. Laser scanners are lighting invariant, provide accurate range data, and typically cover a large field of view, making them interesting sensors for robotics applications. So far, research on detection in laser range data has been dominated by hand-crafted features and boosted classifiers, potentially losing performance due to suboptimal design choices. We propose a Convolutional Neural Network (CNN) based detector for this task. We show how to effectively apply CNNs for detection in 2D range data, and propose a depth preprocessing step and voting scheme that significantly improve CNN performance. We demonstrate our approach on wheelchairs and walkers, obtaining state of the art detection results. Apart from the training data, none of our design choices limits the detector to these two classes, though. We provide a ROS node for our detector and release our dataset containing 464k laser scans, out of which 24k were annotated.
Induced magnetic moment in the magnetic catalysis of chiral symmetry breaking<|sep|>The chiral symmetry breaking in a Nambu-Jona-Lasinio effective model of quarks in the presence of a magnetic field is investigated. We show that new interaction tensor channels open up via Fierz identities due to the explicit breaking of the rotational symmetry by the magnetic field. We demonstrate that the magnetic catalysis of chiral symmetry breaking leads to the generation of two independent condensates, the conventional chiral condensate and a spin-one condensate. While the chiral condensate generates, as usual, a dynamical fermion mass, the new condensate enters as a dynamical anomalous magnetic moment in the dispersion of the quasiparticles. Since the pair, formed by a quark and an antiquark with opposite spins, possesses a resultant magnetic moment, an external magnetic field can align it giving rise to a net magnetic moment for the ground state. The two condensates contribute to the effective mass of the LLL quasiparticles in such a way that the critical temperature for chiral symmetry restoration becomes enhanced.
Semantic Unification A sheaf theoretic approach to natural language<|sep|>Language is contextual and sheaf theory provides a high level mathematical framework to model contextuality. We show how sheaf theory can model the contextual nature of natural language and how gluing can be used to provide a global semantics for a discourse by putting together the local logical semantics of each sentence within the discourse. We introduce a presheaf structure corresponding to a basic form of Discourse Representation Structures. Within this setting, we formulate a notion of semantic unification --- gluing meanings of parts of a discourse into a coherent whole --- as a form of sheaf-theoretic gluing. We illustrate this idea with a number of examples where it can used to represent resolutions of anaphoric references. We also discuss multivalued gluing, described using a distributions functor, which can be used to represent situations where multiple gluings are possible, and where we may need to rank them using quantitative measures. Dedicated to Jim Lambek on the occasion of his 90th birthday.
Dust reddening in star-forming galaxies<|sep|>We present empirical relations between the global dust reddening and other physical galaxy properties including the Halpha luminosity, Halpha surface brightness, metallicity and axial ratio for star-forming disc galaxies. The study is based on a large sample of ~22 000 well-defined star-forming galaxies selected from the Sloan Digital Sky Survey (SDSS). The reddening parameterized by color excess E(B-V) is derived from the Balmer decrement. Besides the dependency of reddening on Halpha luminosity / surface brightness and gas phase metallicity, it is also correlated with the galaxy inclination, in the sense that edge-on galaxies are more attenuated than face-on galaxies at a give intrinsic luminosity. In light of these correlations, we present the empirical formulae of E(B-V) as a function of these galaxy properties, with a scatter of only 0.07 mag. The empirical relation can be reproduced if most dust attenuation to the HII region is due to diffuse interstellar dust distributing in a disc thicker than that of HII regions. The empirical formulae can be incorporated into semi-analytical models of galaxy formation and evolution to estimate the dust reddening and enable comparison with observations more practically.
Speech Synthesis with Mixed Emotions<|sep|>Emotional speech synthesis aims to synthesize human voices with various emotional effects. The current studies are mostly focused on imitating an averaged style belonging to a specific emotion type. In this paper, we seek to generate speech with a mixture of emotions at run-time. We propose a novel formulation that measures the relative difference between the speech samples of different emotions. We then incorporate our formulation into a sequence-to-sequence emotional text-to-speech framework. During the training, the framework does not only explicitly characterize emotion styles, but also explores the ordinal nature of emotions by quantifying the differences with other emotions. At run-time, we control the model to produce the desired emotion mixture by manually defining an emotion attribute vector. The objective and subjective evaluations have validated the effectiveness of the proposed framework. To our best knowledge, this research is the first study on modelling, synthesizing and evaluating mixed emotions in speech.
State Estimation in Electric Power Systems Leveraging Graph Neural Networks<|sep|>The goal of the state estimation (SE) algorithm is to estimate complex bus voltages as state variables based on the available set of measurements in the power system. Because phasor measurement units (PMUs) are increasingly being used in transmission power systems, there is a need for a fast SE solver that can take advantage of high sampling rates of PMUs. This paper proposes training a graph neural network (GNN) to learn the estimates given the PMU voltage and current measurements as inputs, with the intent of obtaining fast and accurate predictions during the evaluation phase. GNN is trained using synthetic datasets, created by randomly sampling sets of measurements in the power system and labelling them with a solution obtained using a linear SE with PMUs solver. The presented results display the accuracy of GNN predictions in various test scenarios and tackle the sensitivity of the predictions to the missing input data.
Graphical Generative Adversarial Networks<|sep|>We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive dependency functions. We introduce a structured recognition model to infer the posterior distribution of latent variables given observations. We generalize the Expectation Propagation (EP) algorithm to learn the generative model and recognition model jointly. Finally, we present two important instances of Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN), which can successfully learn the discrete and temporal structures on visual datasets, respectively.
Assessing the efficiency of first-principles basin-hopping sampling<|sep|>We present a systematic performance analysis of first-principles basin-hopping (BH) runs, with the target to identify all low-energy isomers of small Si and Cu clusters described within density-functional theory. As representative and widely employed move classes we focus on single-particle and collective moves, in which one or all atoms in the cluster at once are displaced in a random direction by some prescribed move distance, respectively. The analysis provides detailed insights into the bottlenecks and governing factors for the sampling efficiency, as well as simple rules-of-thumb for near-optimum move settings, that are intriguingly independent of the distinctly different chemistry of Si and Cu. At corresponding settings, the observed performance of the BH algorithm employing two simple, general-purpose move classes is already very good, and for the small systems studied essentially limited by frequent revisits to a few dominant isomers.
How to Detect the Signatures of Self-Gravitating Circumstellar Discs with the Atacama Large Millimetre/sub-millimetre Array<|sep|>In this paper we present simulated Atacama Large Millimetre/sub-millimetre Array (ALMA) observations of self-gravitating circumstellar discs with different properties in size, mass and inclination, located in four of the most extensively studied and surveyed star-forming regions. Starting from a Smoothed Particle Hydrodynamics (SPH) simulation and representative dust opacities, we have initially constructed maps of the expected emission at sub-mm wavelengths of a large sample of discs with different properties. We have then simulated realistic observations of discs as they may appear with ALMA using the Common Astronomy Software Application ALMA simulator. We find that, with a proper combination of antenna configuration and integration time, the spiral structure characteristic of self-gravitating discs is readily detectable by ALMA over a wide range of wavelengths at distances comparable to TW Hydrae ($\sim 50 \,$pc), Taurus - Auriga and Ophiucus ($\sim 140 \,$pc) star-forming regions. However, for discs located in Orion complex ($\sim 400 \,$pc) only the largest discs in our sample (outer radius of 100 au) show a spatially resolved structure while the smaller ones (outer radius of 25 au) are characterized by a spiral structure that is not conclusively detectable with ALMA.
Model- and calibration-independent test of cosmic acceleration<|sep|>We present a calibration-independent test of the accelerated expansion of the universe using supernova type Ia data. The test is also model-independent in the sense that no assumptions about the content of the universe or about the parameterization of the deceleration parameter are made and that it does not assume any dynamical equations of motion. Yet, the test assumes the universe and the distribution of supernovae to be statistically homogeneous and isotropic. A significant reduction of systematic effects, as compared to our previous, calibration-dependent test, is achieved. Accelerated expansion is detected at significant level (4.3 sigma in the 2007 Gold sample, 7.2 sigma in the 2008 Union sample) if the universe is spatially flat. This result depends, however, crucially on supernovae with a redshift smaller than 0.1, for which the assumption of statistical isotropy and homogeneity is less well established.
Advanced Signal Reconstruction in Tunka-Rex with Matched Filtering and Deep Learning<|sep|>The Tunka Radio Extension (Tunka-Rex) is a digital antenna array operating in the frequency band of 30-80 MHz, measuring the radio emission of air-showers induced by ultra-high energy cosmic rays. Tunka-Rex is co-located with the TAIGA experiment in Siberia and consists of 63 antennas, 57 of them in a densely instrumented area of about 1km2. The signals from the air showers are short pulses, which have a duration of tens of nanoseconds and are recorded in traces of about 5{\mu}s length. The Tunka-Rex analysis of cosmic-ray events is based on the reconstruction of these signals, in particular, their positions in the traces and amplitudes. This reconstruction suffers at low signal-to-noise ratios, i.e. when the recorded traces are dominated by background. To lower the threshold of the detection and increase the efficiency, we apply advanced methods of signal reconstruction, namely matched filtering and deep neural networks with autoencoder architecture. In the present work we show the comparison between the signal reconstructions obtained with these techniques, and give an example of the first reconstruction of the Tunka-Rex signals obtained with a deep neural networks.
Exploring Speech Cues in Web-mined COVID-19 Conversational Vlogs<|sep|>The COVID-19 pandemic caused by the novel SARS-Coronavirus-2 (n-SARS-CoV-2) has impacted people's lives in unprecedented ways. During the time of the pandemic, social vloggers have used social media to actively share their opinions or experiences in quarantine. This paper collected videos from YouTube to track emotional responses in conversational vlogs and their potential associations with events related to the pandemic. In particular, vlogs uploaded from locations in New York City were analyzed given that this was one of the first epicenters of the pandemic in the United States. We observed some common patterns in vloggers' acoustic and linguistic features across the time span of the quarantine, which is indicative of changes in emotional reactivity. Additionally, we investigated fluctuations of acoustic and linguistic patterns in relation to COVID-19 events in the New York area (e.g. the number of daily new cases, number of deaths, and extension of stay-at-home order and state of emergency). Our results indicate that acoustic features, such as zero-crossing-rate, jitter, and shimmer, can be valuable for analyzing emotional reactivity in social media videos. Our findings further indicate that some of the peaks of the acoustic and linguistic indices align with COVID-19 events, such as the peak in the number of deaths and emergency declaration.
Still Wrong Use of Pairings in Cryptography<|sep|>Several pairing-based cryptographic protocols are recently proposed with a wide variety of new novel applications including the ones in emerging technologies like cloud computing, internet of things (IoT), e-health systems and wearable technologies. There have been however a wide range of incorrect use of these primitives. The paper of Galbraith, Paterson, and Smart (2006) pointed out most of the issues related to the incorrect use of pairing-based cryptography. However, we noticed that some recently proposed applications still do not use these primitives correctly. This leads to unrealizable, insecure or too inefficient designs of pairing-based protocols. We observed that one reason is not being aware of the recent advancements on solving the discrete logarithm problems in some groups. The main purpose of this article is to give an understandable, informative, and the most up-to-date criteria for the correct use of pairing-based cryptography. We thereby deliberately avoid most of the technical details and rather give special emphasis on the importance of the correct use of bilinear maps by realizing secure cryptographic protocols. We list a collection of some recent papers having wrong security assumptions or realizability/efficiency issues. Finally, we give a compact and an up-to-date recipe of the correct use of pairings.
Quiet-Sun imaging asymmetries in NaI D1 compared with other strong Fraunhofer lines<|sep|>Imaging spectroscopy of the solar atmosphere using the NaI D1 line yields marked asymmetry between the blue and red line wings: sampling a quiet-Sun area in the blue wing displays reversed granulation, whereas sampling in the red wing displays normal granulation. The MgI b2 line of comparable strength does not show this asymmetry, nor does the stronger CaII 8542 line. We demonstrate the phenomenon with near-simultaneous spectral images in NaI D1, MgI b2, and CaII 8542 from the Swedish 1-m Solar Telescope. We then explain it with line-formation insights from classical 1D modeling and with a 3D magnetohydrodynamical simulation combined with NLTE spectral line synthesis that permits detailed comparison with the observations in a common format. The cause of the imaging asymmetry is the combination of correlations between intensity and Dopplershift modulation in granular overshoot and the sensitivity to these of the steep profile flanks of the NaI D1 line. The MgI b2 line has similar core formation but much wider wings due to larger opacity buildup and damping in the photosphere. Both lines obtain marked core asymmetry from photospheric shocks in or near strong magnetic concentrations, less from higher-up internetwork shocks that produce similar asymmetry in the spatially averaged CaII 8542 profile.
Perturbative Approach on Financial Markets<|sep|>We study the point of transition between complete and incomplete financial models thanks to Dirichlet Forms methods. We apply recent techniques, developped by Bouleau, to hedging procedures in order to perturbate parameters and stochastic processes, in the case of a volatility parameter fixed but uncertain for traders; we call this model Perturbed Black Scholes (PBS) Model. We show that this model can reproduce at the same time a smile effect and a bid-ask spread; we exhibit the volatility function associated to the local-volatility model equivalent to PBS model when vanilla options are concerned. Lastly, we present a connection between Error Theory using Dirichlet Forms and Utility Function Theory.
Search for nonlinear memory from subsolar mass compact binary mergers<|sep|>We present the first results of the search for nonlinear memory from subsolar mass binary black hole (BBH) mergers during the second observing run (O2) of the LIGO and Virgo detectors. The oscillatory chirp signal from the inspiral and merger of low mass BBHs ($M_\mathrm{Tot} \leq 0.4 M_\odot$) are at very high frequencies and fall outside the sensitivity band of the current ground-based detectors. However, the non-oscillatory memory signal during the merger saturates towards the lower frequencies and can be detected for those proposed BBHs. We show in this work that the morphology of the memory signal depends minimally upon the source parameters of the binary, thus only the overall amplitude of the signal changes and hence the result can be interpolated for extremely low mass BBH mergers. We did not find any signal which can be interpreted as a memory signal and we place upper limits on the rate of BBH mergers with $M_\mathrm{Tot} \leq 0.4 M_\odot$ for the first time.
Optimal resource diffusion for suppressing disease spreading in multiplex networks<|sep|>Resource diffusion is an ubiquitous phenomenon, but how it impacts epidemic spreading has received little study. We propose a model that couples epidemic spreading and resource diffusion in multiplex networks. The spread of disease in a physical contact layer and the recovery of the infected nodes are both strongly dependent upon resources supplied by their counterparts in the social layer. The generation and diffusion of resources in the social layer are in turn strongly dependent upon the state of the nodes in the physical contact layer. Resources diffuse preferentially or randomly in this model. To quantify the degree of preferential diffusion, a bias parameter that controls the resource diffusion is proposed. We conduct extensive simulations and find that the preferential resource diffusion can change phase transition type of the fraction of infected nodes. When the degree of interlayer correlation is below a critical value, increasing the bias parameter changes the phase transition from double continuous to single continuous. When the degree of interlayer correlation is above a critical value, the phase transition changes from multiple continuous to first discontinuous and then to hybrid. We find hysteresis loops in the phase transition. We also find that there is an optimal resource strategy at each fixed degree of interlayer correlation where the threshold reaches a maximum and under which the disease can be maximally suppressed. In addition, the optimal controlling parameter increases as the degree of inter-layer correlation increases.
Decentralized Multi-player Multi-armed Bandits with No Collision Information<|sep|>The decentralized stochastic multi-player multi-armed bandit (MP-MAB) problem, where the collision information is not available to the players, is studied in this paper. Building on the seminal work of Boursier and Perchet (2019), we propose error correction synchronization involving communication (EC-SIC), whose regret is shown to approach that of the centralized stochastic MP-MAB with collision information. By recognizing that the communication phase without collision information corresponds to the Z-channel model in information theory, the proposed EC-SIC algorithm applies optimal error correction coding for the communication of reward statistics. A fixed message length, as opposed to the logarithmically growing one in Boursier and Perchet (2019), also plays a crucial role in controlling the communication loss. Experiments with practical Z-channel codes, such as repetition code, flip code and modified Hamming code, demonstrate the superiority of EC-SIC in both synthetic and real-world datasets.
Approximate observability and back and forth observer of a PDE model of crystallisation process<|sep|>In this paper, we are interested in the estimation of Particle Size Distributions (PSDs) during a batch crystallization process in which particles of two different shapes coexist and evolve simultaneously. The PSDs are estimated thanks to a measurement of an apparent Chord Length Distribution (CLD), a measure that we model for crystals of spheroidal shape. Our main result is to prove the approximate observability of the infinite-dimensional system in any positive time. Under this observability condition, we are able to apply a Back and Forth Nudging (BFN) algorithm to reconstruct the PSD.
A Connected Enterprise - Transformation through Mobility and Social Networks<|sep|>Due to rapid changes in business dynamics, there is a growing demand to encourage social conversations/exchanges and the ability to connect and communicate with peers, partners, customers and other stakeholders anytime, anywhere which drives the need of mobile-enable, the existing enterprise applications. This paper highlights a distinct set of needs and key customer challenges that must be considered and addressed for deployment of Social Collaboration applications and Mobility services in enterprises. It not only addresses the Critical Success Factors for enterprise mobility enablement but also outlines the unique business requirements to rapidly create social collaboration culture and the discipline of turning social data into meaningful insights to drive business decisions in real-time. Moreover, the paper emphasizes on developing composite offerings on social enterprise and Mobile networks that not only offer the value proposition in terms of financially oriented results, but also help customer to maximize return on investment (ROI).
Quantum Cognition based on an Ambiguous Representation Derived from a Rough Set Approximation<|sep|>Over the last years, in a series papers by Arrechi and others, a model for the cognitive processes involved in decision making has been proposed and investigated. The key element of this model is the expression of apprehension and judgement, basic cognitive process of decision making, as an inverse Bayes inference classifying the information content of neuron spike trains. For successive plural stimuli, it has been shown that this inference, equipped with basic non-algorithmic jumps, is affected by quantum-like characteristics. We show here that such a decision making process is related consistently with ambiguous representation by an observer within a universe of discourse. In our work ambiguous representation of an object or a stimuli is defined by a pair of maps from objects of a set to their representations, where these two maps are interrelated in a particular structure. The a priori and a posteriori hypotheses in Bayes inference are replaced by the upper and lower approximation, correspondingly, for the initial data sets each derived with respect to a map. We show further that due to the particular structural relation between the two maps, the logical structure of such combined approximations can only be expressed as an orthomodular lattice and therefore can be represented by a quantum rather than a Boolean logic. To our knowledge, this is the first investigation aiming to reveal the concrete logic structure of inverse Bayes inference in cognitive processes.
Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information<|sep|>Adversarial attacks against commercial black-box speech platforms, including cloud speech APIs and voice control devices, have received little attention until recent years. The current "black-box" attacks all heavily rely on the knowledge of prediction/confidence scores to craft effective adversarial examples, which can be intuitively defended by service providers without returning these messages. In this paper, we propose two novel adversarial attacks in more practical and rigorous scenarios. For commercial cloud speech APIs, we propose Occam, a decision-only black-box adversarial attack, where only final decisions are available to the adversary. In Occam, we formulate the decision-only AE generation as a discontinuous large-scale global optimization problem, and solve it by adaptively decomposing this complicated problem into a set of sub-problems and cooperatively optimizing each one. Our Occam is a one-size-fits-all approach, which achieves 100% success rates of attacks with an average SNR of 14.23dB, on a wide range of popular speech and speaker recognition APIs, including Google, Alibaba, Microsoft, Tencent, iFlytek, and Jingdong, outperforming the state-of-the-art black-box attacks. For commercial voice control devices, we propose NI-Occam, the first non-interactive physical adversarial attack, where the adversary does not need to query the oracle and has no access to its internal information and training data. We combine adversarial attacks with model inversion attacks, and thus generate the physically-effective audio AEs with high transferability without any interaction with target devices. Our experimental results show that NI-Occam can successfully fool Apple Siri, Microsoft Cortana, Google Assistant, iFlytek and Amazon Echo with an average SRoA of 52% and SNR of 9.65dB, shedding light on non-interactive physical attacks against voice control devices.
Disformal vectors and anisotropies on a warped brane<|sep|>The Maxwell action is conformally invariant and classically ignorant of conformally flat metrics. However, if the vector lives in a disformal metric--- as it does if residing upon a moving brane---this is no longer true. The disformal coupling is then mediated by a Dirac-Born-Infeld scalar field. Here a systematic dynamical system analysis is developed for anisotropic Bianchi I cosmology with a massive disformally coupled vector field. Several new fixed points are found, including anisotropic scaling solutions. The presented formalism here presented can be conveniently applied to general scenarios with or without extra dimensional motivations. This is illustrated here by performing a complete analysis with simple assumption that both the potentials and the warp factor for the brane are (nearly) exponential. In that case, the anisotropic fixed points are either not attractors, do not describe accelerating expansion or else they feature too large anisotropies to be compatible with observations. Nonetheless, viable classes of models exist where isotropy is retained due to rapid oscillations of the vector field, thus providing a possible realisation of disformally interacting massive dark matter.
Markov Chain Methods For Analyzing Complex Transport Networks<|sep|>We have developed a steady state theory of complex transport networks used to model the flow of commodity, information, viruses, opinions, or traffic. Our approach is based on the use of the Markov chains defined on the graph representations of transport networks allowing for the effective network design, network performance evaluation, embedding, partitioning, and network fault tolerance analysis. Random walks embed graphs into Euclidean space in which distances and angles acquire a clear statistical interpretation. Being defined on the dual graph representations of transport networks random walks describe the equilibrium configurations of not random commodity flows on primary graphs. This theory unifies many network concepts into one framework and can also be elegantly extended to describe networks represented by directed graphs and multiple interacting networks.
Harmonic Coding: An Optimal Linear Code for Privacy-Preserving Gradient-Type Computation<|sep|>We consider the problem of distributedly computing a general class of functions, referred to as gradient-type computation, while maintaining the privacy of the input dataset. Gradient-type computation evaluates the sum of some `partial gradients', defined as polynomials of subsets of the input. It underlies many algorithms in machine learning and data analytics. We propose Harmonic Coding, which universally computes any gradient-type function, while requiring the minimum possible number of workers. Harmonic Coding strictly improves computing schemes developed based on prior works, such as Shamir's secret sharing and Lagrange Coded Computing, by injecting coded redundancy using harmonic progression. It enables the computing results of the workers to be interpreted as the sum of partial gradients and some redundant results, which then allows the cancellation of non-gradient terms in the decoding process. By proving a matching converse, we demonstrate the optimality of Harmonic Coding, even compared to the schemes that are non-universal (i.e., can be designed based on a specific gradient-type function).
Scalable Private Decision Tree Evaluation with Sublinear Communication<|sep|>Private decision tree evaluation (PDTE) allows a decision tree holder to run a secure protocol with a feature provider. By running the protocol, the feature provider will learn a classification result. Nothing more is revealed to either party. In most existing PDTE protocols, the required communication grows exponentially with the tree's depth $d$, which is highly inefficient for large trees. This shortcoming motivated us to design a sublinear PDTE protocol with $O(d)$ communication complexity. The core of our construction is a shared oblivious selection (SOS) functionality, allowing two parties to perform a secret-shared oblivious read operation from an array. We provide two SOS protocols, both of which achieve sublinear communication and propose optimizations to further improve their efficiency. Our sublinear PDTE protocol is based on the proposed SOS functionality and we prove its security under a semi-honest adversary. We compare our protocol with the state-of-the-art, in terms of communication and computation, under various network settings. The performance evaluation shows that our protocol is practical and more scalable over large trees than existing solutions.
Renormalization of the GT operator within the realistic shell model<|sep|>In nuclear structure calculations, the choice of a limited model space, due to computational needs, leads to the necessity to renormalize the Hamiltonian as well as any transition operator. Here, we present a study of the renormalization procedure and effects of the Gamow-Teller operator within the framework of the realistic shell model. Our effective shell-model operators are obtained, starting from a realistic nucleon-nucleon potential, by way of the many-body perturbation theory in order to take into account the degrees of freedom that are not explicitly included in the chosen model space. The theoretical effective shell-model Hamiltonian and transition operators are then employed in shell-model calculations, whose results are compared with data of Gamow-Teller transition strengths and double-beta half-lives for nuclei which are currently of interest for the detection of the neutrinoless double-beta decay process, in a mass interval ranging from A=48 up to A=136. We show that effective operators are able to reproduce quantitatively the spectroscopic and decay properties without resorting to an empirical quenching neither of the axial coupling constant gA, nor of the spin and orbital gyromagnetic factors. This should assess the reliability of applying present theoretical tools to this problematic.
Misleading Deep-Fake Detection with GAN Fingerprints<|sep|>Generative adversarial networks (GANs) have made remarkable progress in synthesizing realistic-looking images that effectively outsmart even humans. Although several detection methods can recognize these deep fakes by checking for image artifacts from the generation process, multiple counterattacks have demonstrated their limitations. These attacks, however, still require certain conditions to hold, such as interacting with the detection method or adjusting the GAN directly. In this paper, we introduce a novel class of simple counterattacks that overcomes these limitations. In particular, we show that an adversary can remove indicative artifacts, the GAN fingerprint, directly from the frequency spectrum of a generated image. We explore different realizations of this removal, ranging from filtering high frequencies to more nuanced frequency-peak cleansing. We evaluate the performance of our attack with different detection methods, GAN architectures, and datasets. Our results show that an adversary can often remove GAN fingerprints and thus evade the detection of generated images.
Nondeterminism and an abstract formulation of Ne\v{c}iporuk's lower bound method<|sep|>A formulation of "Ne\v{c}iporuk's lower bound method" slightly more inclusive than the usual complexity-measure-specific formulation is presented. Using this general formulation, limitations to lower bounds achievable by the method are obtained for several computation models, such as branching programs and Boolean formulas having access to a sublinear number of nondeterministic bits. In particular, it is shown that any lower bound achievable by the method of Ne\v{c}iporuk for the size of nondeterministic and parity branching programs is at most $O(n^{3/2}/\log n)$.
An Adjustable Heat Conduction based KNN Approach for Session-based Recommendation<|sep|>The KNN approach, which is widely used in recommender systems because of its efficiency, robustness and interpretability, is proposed for session-based recommendation recently and outperforms recurrent neural network models. It captures the most recent co-occurrence information of items by considering the interaction time. However, it neglects the co-occurrence information of items in the historical behavior which is interacted earlier and cannot discriminate the impact of items and sessions with different popularity. Due to these observations, this paper presents a new contextual KNN approach to address these issues for session-based recommendation. Specifically, a diffusion-based similarity method is proposed for considering the popularity of vertices in session-item bipartite network, and a candidate selection method is proposed to capture the items that are co-occurred with different historical clicked items in the same session efficiently. Comprehensive experiments are conducted to demonstrate the effectiveness of our KNN approach over the state-of-the-art KNN approach for session-based recommendation on three benchmark datasets.
A local non-negative initial data scalar characterisation of the Kerr solution<|sep|>For any vacuum initial data set, we define a local, non-negative scalar quantity which vanishes at every point of the data hypersurface if and only if the data are {\em Kerr initial} data. Our scalar quantity only depends on the quantities used to construct the vacuum initial data set which are the Riemannian metric defined on the initial data hypersurface and a symmetric tensor which plays the role of the second fundamental form of the embedded initial data hypersurface. The dependency is {\em algorithmic} in the sense that given the initial data one can compute the scalar quantity by algebraic and differential manipulations, being thus suitable for an implementation in a numerical code. The scalar could also be useful in studies of the non-linear stability of the Kerr solution because it serves to measure the deviation of a vacuum initial data set from the Kerr initial data in a local and algorithmic way.
Expropriations, Property Confiscations and New Offshore Entities: Evidence from the Panama Papers<|sep|>Using the Panama Papers, we show that the beginning of media reporting on expropriations and property confiscations in a country increases the probability that offshore entities are incorporated by agents from the same country in the same month. This result is robust to the use of country-year fixed effects and the exclusion of tax havens. Further analysis shows that the effect is driven by countries with non-corrupt and effective governments, which supports the notion that offshore entities are incorporated when reasonably well-intended and well-functioning governments become more serious about fighting organized crime by confiscating proceeds of crime.
Interference Mitigation Using Dynamic Frequency Re-use for Dense Femtocell Network Architectures<|sep|>The next generation network aims to efficiently deploy low cost and low power cellular base station in the subscriber's home environment. For the femtocell deployment, frequency allocation among femtocells and macrocell is big concern to mitigate the interference, and to ensure the best use of the expensive spectrum. There are many sources of interference in integrated femtocell/macrocell networks. Lagging in proper management of interference reduces the system capacity, increases the outage probability, and finally users feel bad quality of experience (QoE). The cost effective interference management technique depends on the size of femtocells deployment. In this paper, firstly we present deployable various possible femtocell network scenarios. We propose the dynamic frequency re-use scheme to mitigate interference for femtocell deployment. For highly dense femtocells, we propose the functionalities of self organizing network (SON) based femtocell network architecture. The outage probability of a femtocell user is analyzed in details. The performances of the proposed schemes for various femtocell deployments are performed using numerical analysis.
Non-Simplified SUSY: Stau-Coannihilation at LHC and ILC<|sep|>If new phenomena beyond the Standard Model will be discovered at the LHC, the properties of the new particles could be determined with data from the High-Luminosity LHC and from a future linear collider like the ILC. We discuss the possible interplay between measurements at the two accelerators in a concrete example, namely a full SUSY model which features a small stau_1-LSP mass difference. Various channels have been studied using the Snowmass 2013 combined LHC detector implementation in the Delphes simulation package, as well as simulations of the ILD detector concept from the Technical Design Report. We investigate both the LHC and ILC capabilities for discovery, separation and identification of various parts of the spectrum. While some parts would be discovered at the LHC, there is substantial room for further discoveries at the ILC. We finally highlight examples where the precise knowledge about the lower part of the mass spectrum which could be acquired at the ILC would enable a more in-depth analysis of the LHC data with respect to the heavier states.
Bi-LSTM Scoring Based Similarity Measurement with Agglomerative Hierarchical Clustering (AHC) for Speaker Diarization<|sep|>Majority of speech signals across different scenarios are never available with well-defined audio segments containing only a single speaker. A typical conversation between two speakers consists of segments where their voices overlap, interrupt each other or halt their speech in between multiple sentences. Recent advancements in diarization technology leverage neural network-based approaches to improvise multiple subsystems of speaker diarization system comprising of extracting segment-wise embedding features and detecting changes in the speaker during conversation. However, to identify speaker through clustering, models depend on methodologies like PLDA to generate similarity measure between two extracted segments from a given conversational audio. Since these algorithms ignore the temporal structure of conversations, they tend to achieve a higher Diarization Error Rate (DER), thus leading to misdetections both in terms of speaker and change identification. Therefore, to compare similarity of two speech segments both independently and sequentially, we propose a Bi-directional Long Short-term Memory network for estimating the elements present in the similarity matrix. Once the similarity matrix is generated, Agglomerative Hierarchical Clustering (AHC) is applied to further identify speaker segments based on thresholding. To evaluate the performance, Diarization Error Rate (DER%) metric is used. The proposed model achieves a low DER of 34.80% on a test set of audio samples derived from ICSI Meeting Corpus as compared to traditional PLDA based similarity measurement mechanism which achieved a DER of 39.90%.
Use of Ensembles of Fourier Spectra in Capturing Recurrent Concepts in Data Streams<|sep|>In this research, we apply ensembles of Fourier encoded spectra to capture and mine recurring concepts in a data stream environment. Previous research showed that compact versions of Decision Trees can be obtained by applying the Discrete Fourier Transform to accurately capture recurrent concepts in a data stream. However, in highly volatile environments where new concepts emerge often, the approach of encoding each concept in a separate spectrum is no longer viable due to memory overload and thus in this research we present an ensemble approach that addresses this problem. Our empirical results on real world data and synthetic data exhibiting varying degrees of recurrence reveal that the ensemble approach outperforms the single spectrum approach in terms of classification accuracy, memory and execution time.
Event patterns extracted from anisotropic spectra of charged particles produced in Pb-Pb collisions at 2.76 TeV<|sep|>Event patterns extracted from anisotropic spectra of charged particles produced in lead-lead collisions at 2.76 TeV are investigated. We use an inverse power-law resulted from the QCD calculus to describe the transverse momentum spectrum in the hard scattering process, and a revised Erlang distribution resulted from a multisource thermal model to describe the transverse momentum spectrum and anisotropic flow in the soft excitation process. The pseudorapidity distribution is described by a three-Gaussian function which is a revision of the Landau hydrodynamic model. Thus, the event patterns at the kinetic freeze-out are displayed by the scatter plots of the considered particles in the three-dimensional velocity, momentum, and rapidity spaces.
Interferometric signatures of the temperature dependence of the specific shear viscosity in heavy-ion collisions<|sep|>Recent work has shown that a temperature dependence of the shear viscosity to entropy ratio, $\eta/s$, influences the collective flow pattern in heavy-ion collisions in characteristic ways that can be measured by studying hadron transverse momentum spectra and their anisotropies. Here we point out that it also affects the pair momentum dependence of the Hanbury-Brown$-$Twiss (HBT) radii (the source size parameters extracted from two-particle intensity interferometry) and the variance of their event-by-event fluctuations. This observation establishes interferometric signatures as useful observables to complement the constraining power of single-particle spectra on the temperature dependence of $\eta/s$.
Equivalence of the Hawking temperature in conformal frames<|sep|>The conformal invariance of the Hawking temperature, conjectured for the asymptotically flat and stationary black holes by Jacobson and Kang, is semiclassically evaluated for a simple particular case of symmetrical spherically and non asymptotically flat black hole. By using the Bogoliubov coefficients, the metric euclideanization, the reflection coefficient and the gravitational anomaly, as methods of calculating the Hawking temperature, we find that it is invariant under a specific conformal transformation of the metric. We discuss briefly the results for each method.
An argument in favor of strong scaling for deep neural networks with small datasets<|sep|>In recent years, with the popularization of deep learning frameworks and large datasets, researchers have started parallelizing their models in order to train faster. This is crucially important, because they typically explore many hyperparameters in order to find the best ones for their applications. This process is time consuming and, consequently, speeding up training improves productivity. One approach to parallelize deep learning models followed by many researchers is based on weak scaling. The minibatches increase in size as new GPUs are added to the system. In addition, new learning rates schedules have been proposed to fix optimization issues that occur with large minibatch sizes. In this paper, however, we show that the recommendations provided by recent work do not apply to models that lack large datasets. In fact, we argument in favor of using strong scaling for achieving reliable performance in such cases. We evaluated our approach with up to 32 GPUs and show that weak scaling not only does not have the same accuracy as the sequential model, it also fails to converge most of time. Meanwhile, strong scaling has good scalability while having exactly the same accuracy of a sequential implementation.
Buoyancy statistics in moist turbulent Rayleigh-Benard convection<|sep|>We study shallow moist Rayleigh-Benard convection in the Boussinesq approximation in three-dimensional direct numerical simulations. The thermodynamics of phase changes is approximated by a piecewise linear equation of state close to the phase boundary. The impact of phase changes on the turbulent fluctuations and the transfer of buoyancy through the layer is discussed as a function of the Rayleigh number and the ability to form liquid water. The enhanced buoyancy flux due to phase changes is compared with dry convection reference cases and related to the cloud cover in the convection layer. This study indicates that the moist Rayleigh-Benard problem offers a practical framework for the development and evaluation of parametrizations for atmospheric convection.
The Natural Language Decathlon: Multitask Learning as Question Answering<|sep|>Deep learning has improved performance on many natural language processing (NLP) tasks individually. However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task. We introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. We cast all tasks as question answering over a context. Furthermore, we present a new Multitask Question Answering Network (MQAN) jointly learns all tasks in decaNLP without any task-specific modules or parameters in the multitask setting. MQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification. We demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and performance further improves with an anti-curriculum training strategy. Though designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting. We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.
Thermal Global Expansion Coefficient Measurement for a Harmonic Trapped Gas Across Bose-Einstein Condensation<|sep|>We report the measurement of the global thermal expansion coefficient of a confined Bose gas of $^{87}\rm Rb$ in a harmonic potential around the Bose-Einstein condensation transition temperature. We use the concept of global thermodynamic variables, previously introduced and appropriated for a non-homogeneous system. We observe the behavior of the thermal expansion coefficient above and below the critical temperature showing the lambda-like shape present in superfluid helium. The study demonstrates the potentiality of global thermodynamic variables for the investigation of properties across the critical temperature and a new way to study the thermodynamic properties of the quantum systems.
Decoupling Learning Rules from Representations<|sep|>In the artificial intelligence field, learning often corresponds to changing the parameters of a parameterized function. A learning rule is an algorithm or mathematical expression that specifies precisely how the parameters should be changed. When creating an artificial intelligence system, we must make two decisions: what representation should be used (i.e., what parameterized function should be used) and what learning rule should be used to search through the resulting set of representable functions. Using most learning rules, these two decisions are coupled in a subtle (and often unintentional) way. That is, using the same learning rule with two different representations that can represent the same sets of functions can result in two different outcomes. After arguing that this coupling is undesirable, particularly when using artificial neural networks, we present a method for partially decoupling these two decisions for a broad class of learning rules that span unsupervised learning, reinforcement learning, and supervised learning.
