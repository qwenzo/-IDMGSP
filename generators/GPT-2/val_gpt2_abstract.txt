Climate & BCG: Effects on COVID-19 Death Growth Rates<|sep|>Multiple studies have suggested the spread of COVID-19 is affected by factors such as climate, BCG vaccinations, pollution and blood type. We perform a joint study of these factors using the death growth rates of 40 regions worldwide with both machine learning and Bayesian methods. We find weak, non-significant (< 3$\sigma$) evidence for temperature and relative humidity as factors in the spread of COVID-19 but little or no evidence for BCG vaccination prevalence or $\text{PM}_{2.5}$ pollution. The only variable detected at a statistically significant level (>3$\sigma$) is the rate of positive COVID-19 tests, with higher positive rates correlating with higher daily growth of deaths.
Provable Self-Representation Based Outlier Detection in a Union of Subspaces<|sep|>Many computer vision tasks involve processing large amounts of data contaminated by outliers, which need to be detected and rejected. While outlier detection methods based on robust statistics have existed for decades, only recently have methods based on sparse and low-rank representation been developed along with guarantees of correct outlier detection when the inliers lie in one or more low-dimensional subspaces. This paper proposes a new outlier detection method that combines tools from sparse representation with random walks on a graph. By exploiting the property that data points can be expressed as sparse linear combinations of each other, we obtain an asymmetric affinity matrix among data points, which we use to construct a weighted directed graph. By defining a suitable Markov Chain from this graph, we establish a connection between inliers/outliers and essential/inessential states of the Markov chain, which allows us to detect outliers by using random walks. We provide a theoretical analysis that justifies the correctness of our method under geometric and connectivity assumptions. Experimental results on image databases demonstrate its superiority with respect to state-of-the-art sparse and low-rank outlier detection methods.
Full Diversity Space-Time Block Codes with Low-Complexity Partial Interference Cancellation Group Decoding<|sep|>Partial interference cancellation (PIC) group decoding proposed by Guo and Xia is an attractive low-complexity alternative to the optimal processing for multiple-input multiple-output (MIMO) wireless communications. It can well deal with the tradeoff among rate, diversity and complexity of space-time block codes (STBC). In this paper, a systematic design of full-diversity STBC with low-complexity PIC group decoding is proposed. The proposed code design is featured as a group-orthogonal STBC by replacing every element of an Alamouti code matrix with an elementary matrix composed of multiple diagonal layers of coded symbols. With the PIC group decoding and a particular grouping scheme, the proposed STBC can achieve full diversity, a rate of $(2M)/(M+2)$ and a low-complexity decoding for $M$ transmit antennas. Simulation results show that the proposed codes can achieve the full diversity with PIC group decoding while requiring half decoding complexity of the existing codes.
Skeleton-based Gait Index Estimation with LSTMs<|sep|>In this paper, we propose a method that estimates a gait index for a sequence of skeletons. Our system is a stack of an encoder and a decoder that are formed by Long Short-Term Memories (LSTMs). In the encoding stage, the characteristics of an input are automatically determined and are compressed into a latent space. The decoding stage then attempts to reconstruct the input according to such intermediate representation. The reconstruction error is thus considered as a weak gait index. By combining such weak indices over a long-time movement, our system can provide a good estimation for the gait index. Our experiments on a large dataset (nearly one hundred thousand skeletons) showed that the index given by the proposed method outperformed some recent works on gait analysis.
Aggregates of clusters in the Gaia data<|sep|>The precision of the parallax measurements by Gaia is unprecedented. As of Gaia Data Release 2, the number of known nearby open clusters has increased. Some of the clusters appear to be relatively close to each other and form aggregates, which makes them interesting objects to study. We study the aggregates of clusters which share several of the assigned member stars in relatively narrow volumes of the phase space. Using the most recent list of open clusters, we compare the cited central parallaxes with the histograms of parallax distributions of cluster aggregates. The aggregates were chosen based on the member stars which are shared by multiple clusters. Many of the clusters in the aggregates have been assigned parallaxes which coincide with the histograms. However, clusters that share a large number of members in a small volume of the phase space display parallax distributions which do not coincide with the values from the literature. This is the result of ignoring a possibility of assigning multiple probabilities to a single star. We propose that this small number of clusters should be analysed anew.
3C273 variability at 7 mm: Evidences of shocks and precession in the jet<|sep|>We report 4 years of observations of 3C273 at 7 mm obtained with the Itapetinga Radiotelescope, in Brazil, between 2009 and 2013. We detected a flare in 2010 March, when the flux density increased by 50% and reached 35 Jy. After the flare, the flux density started to decrease and reached values lower than 10 Jy. We suggest that the 7 mm flare is the radio counterpart of the $\gamma$-ray flare observed by Fermi/LAT in 2009 September, in which the flux density at high energies reached a factor of fifty of its average value. A delay of 170 days between the radio and $\gamma$-ray flares was revealed using the Discrete Correlation Function (DCF) that can be interpreted in the context of a shock model, in which each flare corresponds to the formation of a compact superluminal component that expands and becomes optically thin at radio frequencies at latter epochs. The difference in flare intensity between frequencies and at a different times, is explained as a consequence of an increase in the Doppler factor $\delta$, as predicted by the 16 year precession model proposed by Abraham & Romero, which has a large effect on boosting at high frequencies while does not affect too much the observed optically thick radio emission. We discuss other observable effects of the variation in $\delta$, as the increase in the formation rate of superluminal components, the variations in the time delay between flares and the periodic behaviour of the radio light curve that we found compatible with changes in the Doppler factor.
Material Dependence of the Wire-Particle Casimir Interaction<|sep|>We study the Casimir interaction between a metallic cylindrical wire and a metallic spherical particle by employing the scattering formalism. At large separations, we derive the asymptotic form of the interaction. In addition, we find the interaction between a metallic wire and an isotropic atom, both in the non-retarded and retarded limits. We identify the conditions under which the asymptotic Casimir interaction does not depend on the material properties of the metallic wire and the particle. Moreover, we compute the exact Casimir interaction between the particle and the wire numerically. We show that there is a complete agreement between the numerics and the asymptotic energies at large separations. For short separations, our numerical results show good agreement with the proximity force approximation.
The luminosities of cool supergiants in the Magellanic Clouds, and the Humphreys-Davidson limit revisited<|sep|>The empirical upper luminosity boundary $L_{\rm max}$ of cool supergiants, often referred to as the Humphreys-Davidson limit, is thought to encode information on the general mass-loss behaviour of massive stars. Further, it delineates the boundary at which single stars will end their lives stripped of their hydrogen-rich envelope, which in turn is a key factor in the relative rates of Type-II to Type-Ibc supernovae from single star channels. In this paper we have revisited the issue of $L_{\rm max}$ by studying the luminosity distributions of cool supergiants (SGs) in the Large and Small Magellanic Clouds (LMC/SMC). We assemble samples of cool SGs in each galaxy which are highly-complete above $\log L/L_{\odot}$=5.0, and determine their spectral energy distributions from the optical to the mid-infrared using modern multi-wavelength survey data. We show that in both cases $L_{\rm max}$ appears to be lower than previously quoted, and is in the region of $\log L/L_{\odot}$=5.5. There is no evidence for $L_{\rm max}$ being higher in the SMC than in the LMC, as would be expected if metallicity-dependent winds were the dominant factor in the stripping of stellar envelopes. We also show that $L_{\rm max}$ aligns with the lowest luminosity of single nitrogen-rich Wolf-Rayet stars, indicating of a change in evolutionary sequence for stars above a critical mass. From population synthesis analysis we show that the Geneva evolutionary models greatly over-predict the numbers of cool SGs in the SMC. We also argue that the trend of earlier average spectral types of cool SGs in lower metallicity environments represents a genuine shift to hotter temperatures. Finally, we use our new bolometric luminosity measurements to provide updated bolometric corrections for cool supergiants.
Physical Parameters of Asteroids Estimated from the WISE 3 Band Data and NEOWISE Post-Cryogenic Survey<|sep|>Enhancements to the science data processing pipeline of NASA's Wide-field Infrared Explorer (WISE) mission, collectively known as NEOWISE, resulted in the detection of $>$158,000 minor planets in four infrared wavelengths during the fully cryogenic portion of the mission. Following the depletion of its cryogen, NASA's Planetary Science Directorate funded a four month extension to complete the survey of the inner edge of the Main Asteroid Belt and to detect and discover near-Earth objects (NEOs). This extended survey phase, known as the NEOWISE Post-Cryogenic Survey, resulted in the detection of $\sim$6500 large Main Belt asteroids and 88 NEOs in its 3.4 and 4.6 $\mu$m channels. During the Post-Cryogenic Survey, NEOWISE discovered and detected a number of asteroids co-orbital with the Earth and Mars, including the first known Earth Trojan. We present preliminary thermal fits for these and other NEOs detected during the 3-Band Cryogenic and Post-Cryogenic Surveys.
Projection-based reduced order models for a cut finite element method in parametrized domains<|sep|>This work presents a reduced order modelling technique built on a high fidelity embedded mesh finite element method. Such methods, and in particular the CutFEM method, are attractive in the generation of projection-based reduced order models thanks to their capabilities to seamlessly handle large deformations of parametrized domains. The combination of embedded methods and reduced order models allows us to obtain fast evaluation of parametrized problems, avoiding remeshing as well as the reference domain formulation, often used in the reduced order modelling for boundary fitted finite element formulations. The resulting novel methodology is presented on linear elliptic and Stokes problems, together with several test cases to assess its capability. The role of a proper extension and transport of embedded solutions to a common background is analyzed in detail.
Accelerating PageRank using Partition-Centric Processing<|sep|>PageRank is a fundamental link analysis algorithm that also functions as a key representative of the performance of Sparse Matrix-Vector (SpMV) multiplication. The traditional PageRank implementation generates fine granularity random memory accesses resulting in large amount of wasteful DRAM traffic and poor bandwidth utilization. In this paper, we present a novel Partition-Centric Processing Methodology (PCPM) to compute PageRank, that drastically reduces the amount of DRAM communication while achieving high sustained memory bandwidth. PCPM uses a Partition-centric abstraction coupled with the Gather-Apply-Scatter (GAS) programming model. By carefully examining how a PCPM based implementation impacts communication characteristics of the algorithm, we propose several system optimizations that improve the execution time substantially. More specifically, we develop (1) a new data layout that significantly reduces communication and random DRAM accesses, and (2) branch avoidance mechanisms to get rid of unpredictable data-dependent branches. We perform detailed analytical and experimental evaluation of our approach using 6 large graphs and demonstrate an average 2.7x speedup in execution time and 1.7x reduction in communication volume, compared to the state-of-the-art. We also show that unlike other GAS based implementations, PCPM is able to further reduce main memory traffic by taking advantage of intelligent node labeling that enhances locality. Although we use PageRank as the target application in this paper, our approach can be applied to generic SpMV computation.
A two-stage formalism for common-envelope phases of massive stars<|sep|>We propose a new simple formalism to predict the orbital separations after common-envelope phases with massive star donors. We focus on the fact that massive red supergiants tend to have a sizeable radiative layer between the dense helium core and the convective envelope. Our formalism treats the common-envelope phase in two stages: dynamical in-spiral through the outer convective envelope and thermal timescale mass transfer from the radiative intershell. With fiducial choices of parameters, the new formalism typically predicts much wider separations compared to the classical energy formalism. Moreover, our formalism predicts that final separations strongly depend on the donor evolutionary stage and companion mass. Our formalism provides a physically-motivated alternative option for population synthesis studies to treat common-envelope evolution. This treatment will impact on predictions for massive-star binaries, including gravitational-wave sources, X-ray binaries and stripped-envelope supernovae.
Multiclass Common Spatial Pattern for EEG based Brain Computer Interface with Adaptive Learning Classifier<|sep|>In Brain Computer Interface (BCI), data generated from Electroencephalogram (EEG) is non-stationary with low signal to noise ratio and contaminated with artifacts. Common Spatial Pattern (CSP) algorithm has been proved to be effective in BCI for extracting features in motor imagery tasks, but it is prone to overfitting. Many algorithms have been devised to regularize CSP for two class problem, however they have not been effective when applied to multiclass CSP. Outliers present in data affect extracted CSP features and reduces performance of the system. In addition to this non-stationarity present in the features extracted from the CSP present a challenge in classification. We propose a method to identify and remove artifact present in the data during pre-processing stage, this helps in calculating eigenvectors which in turn generates better CSP features. To handle the non-stationarity, Self-Regulated Interval Type-2 Neuro-Fuzzy Inference System (SRIT2NFIS) was proposed in the literature for two class EEG classification problem. This paper extends the SRIT2NFIS to multiclass using Joint Approximate Diagonalization (JAD). The results on standard data set from BCI competition IV shows significant increase in the accuracies from the current state of the art methods for multiclass classification.
"I have no idea what they're trying to accomplish:" Enthusiastic and Casual Signal Users' Understanding of Signal PINs<|sep|>We conducted an online study with $n = 235$ Signal users on their understanding and usage of PINs in Signal. In our study, we observe a split in PIN management and composition strategies between users who can explain the purpose of the Signal PINs (56%; enthusiasts) and users who cannot (44%; casual users). Encouraging adoption of PINs by Signal appears quite successful: only 14% opted-out of setting a PIN entirely. Among those who did set a PIN, most enthusiasts had long, complex alphanumeric PINs generated by and saved in a password manager. Meanwhile more casual Signal users mostly relied on short numeric-only PINs. Our results suggest that better communication about the purpose of the Signal PIN could help more casual users understand the features PINs enable (such as that it is not simply a personal identification number). This communication could encourage a stronger security posture.
Open quantum systems with delayed coherent feedback<|sep|>We present an elementary derivation and generalisation of a recently reported method of simulating feedback in open quantum systems. We use our generalised method to simulate systems with multiple delays, as well as cascaded systems with delayed backscatter. In addition, we derive a generalisation of the quantum regression formula that applies to systems with delayed feedback, and show how to use the formula to compute two-time correlation functions of the system as well as output field properties. Finally, we show that delayed coherent feedback can be simulated as a quantum teleportation protocol that requires only Markovian resources, pre-shared entanglement, and time travel. The requirement for time travel can be avoided by using a probabilistic protocol.
The nature of the SDSS galaxies in various classes based on morphology, colour and spectral features - II. Multi-wavelength properties<|sep|>We present a multi-wavelength study of the nature of the SDSS galaxies divided into fine classes based on their morphology, colour and spectral features. The SDSS galaxies are classified into early-type and late-type; red and blue; passive, HII, Seyfert and LINER, which returns a total of 16 fine classes of galaxies. The properties of galaxies in each fine class are investigated from radio to X-ray, using 2MASS, IRAS, FIRST, NVSS, GALEX and ROSAT data. The UV - optical - NIR colours of blue early-type galaxies (BEGs) seem to result from the combination of old stellar population and recent star formation (SF). Non-passive red early-type galaxies (REGs) have larger metallicity and younger age than passive REGs, which implies that non-passive REGs have suffered recent SF adding young and metal-rich stars to them. The radio detection fraction of REGs strongly depends on their optical absolute magnitudes, while that of most late-type galaxies does not, implying the difference in their radio sources: AGN and SF. The UV - optical colours and the radio detection fraction of passive RLGs show that they have properties similar to REGs rather than non-passive RLGs. Dust extinction may not be a dominant factor making RLGs red, because RLGs are detected in the mid- and far-infrared bands less efficiently than blue late-type galaxies (BLGs). The passive BLGs have very blue UV - optical - NIR colours, implying either recent SF quenching or current SF in their outskirts. Including star formation rate, other multi-wavelength properties in each fine class are investigated, and their implication on the identity of each fine class is discussed (abridged).
A $Gaia$ Data Release 2 catalogue of white dwarfs and a comparison with SDSS<|sep|>We present a catalogue of white dwarf candidates selected from the second data release of $Gaia$ (DR2). We used a sample of spectroscopically confirmed white dwarfs from the Sloan Digital Sky Survey (SDSS) to map the entire space spanned by these objects in the $Gaia$ Hertzsprung-Russell diagram. We then defined a set of cuts in absolute magnitude, colour, and a number of $Gaia$ quality flags to remove the majority of contaminating objects. Finally, we adopt a method analogous to the one presented in our earlier SDSS photometric catalogues to calculate a probability of being a white dwarf ($P_{\mathrm{WD}}$) for all $Gaia$ sources which passed the initial selection. The final catalogue is composed of $486\,641$ stars with calculated $P_{\mathrm{WD}}$ from which it is possible to select a sample of $\simeq 260\,000$ high-confidence white dwarf candidates in the magnitude range $8<G<21$. By comparing this catalogue with a sample of SDSS white dwarf candidates we estimate an upper limit in completeness of $85$ per cent for white dwarfs with $G \leq 20$ mag and $T_{\mathrm{eff}} > 7000$ K, at high Galactic latitudes ($|b|>20^{\circ}$). However, the completeness drops at low Galactic latitudes, and the magnitude limit of the catalogue varies significantly across the sky as a function of $Gaia$'s scanning law. We also provide the list of objects within our sample with available SDSS spectroscopy. We use this spectroscopic sample to characterise the observed structure of the white dwarf distribution in the H-R diagram.
A search for ultrahigh-energy neutrinos associated with astrophysical sources using the third flight of ANITA<|sep|>The ANtarctic Impulsive Transient Antenna (ANITA) long-duration balloon experiment is sensitive to interactions of ultra high-energy (E > 10^{18} eV) neutrinos in the Antarctic ice sheet. The third flight of ANITA, lasting 22 days, began in December 2014. We develop a methodology to search for energetic neutrinos spatially and temporally coincident with potential source classes in ANITA data. This methodology is applied to several source classes: the TXS 0506+056 blazar and NGC 1068, the first potential TeV neutrino sources identified by IceCube, flaring high-energy blazars reported by the Fermi All-Sky Variability Analysis, gamma-ray bursts, and supernovae. Among searches within the five source classes, one candidate was identified as associated with SN 2015D, although not at a statistically significant level. We proceed to place upper limits on the source classes. We further comment on potential applications of this methodology to more sensitive future instruments.
A jigsaw puzzle framework for homogenization of high porosity foams<|sep|>An approach to homogenization of high porosity metallic foams is explored. The emphasis is on the \Alporas{} foam and its representation by means of two-dimensional wire-frame models. The guaranteed upper and lower bounds on the effective properties are derived by the first-order homogenization with the uniform and minimal kinematic boundary conditions at heart. This is combined with the method of Wang tilings to generate sufficiently large material samples along with their finite element discretization. The obtained results are compared to experimental and numerical data available in literature and the suitability of the two-dimensional setting itself is discussed.
Geometrical approach to SU(2) navigation with Fibonacci anyons<|sep|>Topological quantum computation with Fibonacci anyons relies on the possibility of efficiently generating unitary transformations upon pseudoparticles braiding. The crucial fact that such set of braids has a dense image in the unitary operations space is well known; in addition, the Solovay-Kitaev algorithm allows to approach a given unitary operation to any desired accuracy. In this paper, the latter task is fulfilled with an alternative method, in the SU(2) case, based on a generalization of the geodesic dome construction to higher dimension.
Star Formation and Gas Phase History of the Cosmic Web<|sep|>We present a new method of tracking and characterizing the environment in which galaxies and their associated circumgalactic medium evolve. We use a structure finding algorithm we developed to self-consistently parse and follow the evolution of poor clusters, filaments and voids in large scale simulations. We trace the complete evolution of the baryons in the gas phase and the star formation history within each structure in our simulated volume. We vary the structure measure threshold to probe the complex inner structure of star forming regions in poor clusters, filaments and voids. We find the majority of star formation occurs in cold, condensed gas in filaments at intermediate redshifts (z ~ 3). We also show that much of the star formation above a redshift z = 3 occurs in low contrast regions of filaments, but as the density contrast increases at lower redshift star formation switches to the high contrast regions, or inner parts, of filaments. Since filaments bridge the void and cluster regions, it suggests that the majority of star formation occurs in galaxies in intermediate density regions prior to the accretion onto poor clusters. We find that at the present epoch, the gas phase distribution is 43.1%, 30.0%, 24.7% and 2.2% in the diffuse, WHIM, hot halo and condensed phases, respectively. The majority of the WHIM is associated with filaments. However, their multiphase nature and the fact that the star formation occurs predominantly in the condensed gas both point to the importance of not conflating the filamentary environment with the WHIM. Moreover, in our simulation volume 8.77%, 79.1%, 2.11% of the gas at z = 0 is located in poor clusters, filaments, and voids, respectively. We find that both filaments and poor clusters are multiphase environments distinguishing themselves by different distribution of gas phases.
Learn to Forget: Machine Unlearning via Neuron Masking<|sep|>Nowadays, machine learning models, especially neural networks, become prevalent in many real-world applications.These models are trained based on a one-way trip from user data: as long as users contribute their data, there is no way to withdraw; and it is well-known that a neural network memorizes its training data. This contradicts the "right to be forgotten" clause of GDPR, potentially leading to law violations. To this end, machine unlearning becomes a popular research topic, which allows users to eliminate memorization of their private data from a trained machine learning model.In this paper, we propose the first uniform metric called for-getting rate to measure the effectiveness of a machine unlearning method. It is based on the concept of membership inference and describes the transformation rate of the eliminated data from "memorized" to "unknown" after conducting unlearning. We also propose a novel unlearning method calledForsaken. It is superior to previous work in either utility or efficiency (when achieving the same forgetting rate). We benchmark Forsaken with eight standard datasets to evaluate its performance. The experimental results show that it can achieve more than 90\% forgetting rate on average and only causeless than 5\% accuracy loss.
Mean-square convergence of a semi-discrete scheme for stochastic nonlinear Maxwell equations<|sep|>In this paper, we propose a semi-implicit Euler scheme to discretize the stochastic nonlinear Maxwell equations with multiplicative Ito noise, which is implicit in the drift term and explicit in the diffusion term of the equations, in order to suited to Ito product. Uniform bounds with high regularities of solutions for both the continuous and the discrete problems are obtained, which are crucial properties to derive the mean-square convergence with certain order. Allowing sufficient spatial regularity and utilizing the energy estimate technique, the convergence order 1/2 in mean-square sense is obtained.
Efficient Bayesian phase estimation using mixed priors<|sep|>We describe an efficient implementation of Bayesian quantum phase estimation in the presence of noise and multiple eigenstates. The main contribution of this work is the dynamic switching between different representations of the phase distributions, namely truncated Fourier series and normal distributions. The Fourier-series representation has the advantage of being exact in many cases, but suffers from increasing complexity with each update of the prior. This necessitates truncation of the series, which eventually causes the distribution to become unstable. We derive bounds on the error in representing normal distributions with a truncated Fourier series, and use these to decide when to switch to the normal-distribution representation. This representation is much simpler, and was proposed in conjunction with rejection filtering for approximate Bayesian updates. We show that, in many cases, the update can be done exactly using analytic expressions, thereby greatly reducing the time complexity of the updates. Finally, when dealing with a superposition of several eigenstates, we need to estimate the relative weights. This can be formulated as a convex optimization problem, which we solve using a gradient-projection algorithm. By updating the weights at exponentially scaled iterations we greatly reduce the computational complexity without affecting the overall accuracy.
The new semianalytic code GalICS 2.0 - Reproducing the galaxy stellar mass function and the Tully-Fisher relation simultaneously<|sep|>GalICS 2.0 is a new semianalytic code to model the formation and evolution of galaxies in a cosmological context. N-body simulations based on a Planck cosmology are used to construct halo merger trees, track subhaloes, compute spins and measure concentrations. The accretion of gas onto galaxies and the morphological evolution of galaxies are modelled with prescriptions derived from hydrodynamic simulations. Star formation and stellar feedback are described with phenomenological models (as in other semianalytic codes). GalICS 2.0 computes rotation speeds from the gravitational potential of the dark matter, the disc and the central bulge. As the rotation speed depends not only on the virial velocity but also on the ratio of baryons to dark matter within a galaxy, our calculation predicts a different Tully-Fisher relation from models in which the rotation speed is proportional to the virial velocity. This is why GalICS 2.0 is able to reproduce the galaxy stellar mass function and the Tully-Fisher relation simultaneously. Our results are also in agreement with halo masses from weak lensing and satellite kinematics, gas fractions, the relation between star formation rate (SFR) and stellar mass, the evolution of the cosmic SFR density, bulge-to-disc ratios, disc sizes and the Faber-Jackson relation.
Towards Ultra-Reliable Low-Latency Communications: Typical Scenarios, Possible Solutions, and Open Issues<|sep|>Ultra-reliable low-latency communications (URLLC) has been considered as one of the three new application scenarios in the \emph{5th Generation} (5G) \emph {New Radio} (NR), where the physical layer design aspects have been specified. With the 5G NR, we can guarantee the reliability and latency in radio access networks. However, for communication scenarios where the transmission involves both radio access and wide area core networks, the delay in radio access networks only contributes to part of the \emph{end-to-end} (E2E) delay. In this paper, we outline the delay components and packet loss probabilities in typical communication scenarios of URLLC, and formulate the constraints on E2E delay and overall packet loss probability. Then, we summarize possible solutions in the physical layer, the link layer, the network layer, and the cross-layer design, respectively. Finally, we discuss the open issues in prediction and communication co-design for URLLC in wide area large scale networks.
Alterations And Rearrangements Of A Non-Autonomous Dynamical System<|sep|>In this paper, we discuss the dynamics of alterations and rearrangements of a non-autonomous dynamical system generated by the family $\mathbb{F}$. We prove that while insertion/deletion of a map in the family $\mathbb{F}$ can disturb the dynamics of a system, the dynamics of the system does not change if the map inserted/deleted is feeble open. In the process, we prove that if the inserted/deleted map is feeble open, the altered system exhibits any form of mixing/sensitivity if and only if the original system exhibits the same. We extend our investigations to properties like equicontinuity, minimality and proximality for the two systems. We prove that any finite rearrangement of a non-autonomous dynamical system preserves the dynamics of original system if the family $\mathbb{F}$ is feeble open. We also give examples to show that the dynamical behavior of a system need be not be preserved under infinite rearrangement.
Green-aware Mobile Edge Computing for IoT: Challenges, Solutions and Future Directions<|sep|>The development of Internet of Things (IoT) technology enables the rapid growth of connected smart devices and mobile applications. However, due to the constrained resources and limited battery capacity, there are bottlenecks when utilizing the smart devices. Mobile edge computing (MEC) offers an attractive paradigm to handle this challenge. In this work, we concentrate on the MEC application for IoT and deal with the energy saving objective via offloading workloads between cloud and edge. In this regard, we firstly identify the energy-related challenges in MEC. Then we present a green-aware framework for MEC to address the energy-related challenges, and provide a generic model formulation for the green MEC. We also discuss some state-of-the-art workloads offloading approaches to achieve green IoT and compare them in comprehensive perspectives. Finally, some future research directions related to energy efficiency in MEC are given.
The Fun is Finite: Douglas-Rachford and Sudoku Puzzle -- Finite Termination and Local Linear Convergence<|sep|>In recent years, the Douglas-Rachford splitting method has been shown to be effective at solving many non-convex optimization problems. In this paper we present a local convergence analysis for non-convex feasibility problems and show that both finite termination and local linear convergence are obtained. For a generalization of the Sudoku puzzle, we prove that the local linear rate of convergence of Douglas-Rachford is exactly $\frac{\sqrt{5}}{5}$ and independent of puzzle size. For the $s$-queens problem we prove that Douglas-Rachford converges after a finite number of iterations. Numerical results on solving Sudoku puzzles and $s$-queens puzzles are provided to support our theoretical findings.
Radiative heat transfer in nonlinear Kerr media<|sep|>We obtain a fluctuation--dissipation theorem describing thermal electromagnetic fluctuation effects in nonlinear media that we exploit in conjunction with a stochastic Langevin framework to study thermal radiation from Kerr ($\chi^{(3)}$) photonic cavities coupled to external environments at and out of equilibrium. We show that that in addition to thermal broadening due to two-photon absorption,the emissivity of such cavities can exhibit asymmetric,non-Lorentzian lineshapes due to self-phase modulation. When the local temperature of the cavity is larger than that of the external bath, we find that the heat transfer into the bath exceeds the radiation from a corresponding linear black body at the same local temperature. We predict that these temperature-tunable thermal processes can be observed in practical, nanophotonic cavities operating at relatively small temperatures.
Redshift Space Distortion of 21cm line at 1<z<5 with Cosmological Hydrodynamic Simulations<|sep|>We measure the scale dependence and redshift dependence of 21 cm line emitted from the neutral hydrogen gas at redshift 1<z<5 using full cosmological hydrodynamic simulations by taking the ratios between the power spectra of HI-dark matter cross correlation and dark matter auto-correlation. The neutral hydrogen distribution is computed in full cosmological hydrodynamic simulations including star formation and supernova feedback under a uniform ultra-violet background radiation. We find a significant scale dependence of HI bias at z>3 on scales of k>1h/Mpc, but it is roughly constant at lower redshift z<3. The redshift evolution of HI bias is relatively slow compared to that of QSOs at similar redshift range. We also measure a redshift space distortion (RSD) of HI gas to explore the properties of HI clustering. Fitting to a widely applied theoretical prediction, we find that the constant bias is consistent with that measured directly from the real-space power spectra, and the velocity dispersion is marginally consistent with the linear perturbation prediction. Finally we compare the results obtained from our simulation and the Illustris simulation, and conclude that the detailed astrophysical effects do not affect the scale dependence of HI bias very much, which implies that the cosmological analysis using 21 cm line of HI will be robust against the uncertainties arising from small-scale astrophysical processes such as star formation and supernova feedback.
Stellar models in Brane Worlds<|sep|>We consider here a full study of stellar dynamics from the brane-world point of view in the case of constant density and of a polytropic fluid. We start our study cataloguing the minimal requirements to obtain a compact object with a Schwarszchild exterior, highlighting the low and high energy limit, the boundary conditions, and the appropriate behavior of Weyl contributions inside and outside of the star. Under the previous requirements we show an extensive study of stellar behavior, starting with stars of constant density and its extended cases with the presence of nonlocal contributions. Finally, we focus our attention to more realistic stars with a polytropic equation of state, specially in the case of white dwarfs, and study their static configurations numerically. One of the main results is that the inclusion of the Weyl functions from braneworld models allow the existence of more compact configurations than within General Relativity.
Two-parton Light-cone Distribution Amplitudes of Tensor Mesons<|sep|>We present a detailed study of the two-parton light-cone distribution amplitudes for $1\,^3P_2$ nonet tensor mesons. The asymptotic two-parton distribution amplitudes of twist-2 and twist-3 are given. The decay constants $f_T$ and $f_T^\bot$ defined by the matrices of non-local operators on the lightcone are estimated using the QCD sum rule techniques. We also study the decay constants for $f_2(1270)$ and $f_2^\prime(1525)$ based on the hypothesis of tensor meson dominance together with the data of $\Gamma(f_2\to \pi\pi)$ and $\Gamma(f'_2\to K\bar K)$ and find that the results are in accordance with the sum rule predictions.
Towards precision distances and 3D dust maps using broadband Period--Magnitude relations of RR Lyrae stars<|sep|>We determine the period-magnitude relations of RR Lyrae stars in 13 photometric bandpasses from 0.4 to 12 {\mu}m using timeseries observations of 134 stars. The Bayesian formalism, extended from our previous work to include the effects of line-of-sight dust extinction, allows for the simultaneous inference of the posterior distribution of the mean absolute magnitude, slope of the period-magnitude power-law, and intrinsic scatter about a perfect power-law for each bandpass. In addition, the distance modulus and line-of-sight dust extinction to each RR Lyrae star in the calibration sample is determined, yielding a sample median fractional distance error of 0.66%. The intrinsic scatter in all bands appears to be larger than the photometric errors, except in WISE W1 (3.4 {\mu}m) and W2 (4.6 {\mu}m) where the photometric error ($\sigma \approx 0.05$ mag) is to be comparable or larger than the intrinsic scatter. Additional observations at these wavelengths could improve the inferred distances to these sources further. As an application of the methodology, we infer the distance to the RRc-type star RZCep at low Galactic latitude ($b = 5.5^\circ$) to be $\mu=8.0397\pm0.0123$ mag ($405.4\pm2.3$ pc) with colour excess $E(B-V)=0.2461\pm0.0089$ mag. This distance, equivalent to a parallax of $2467\pm14$ microarcsec, is consistent with the published HST parallax measurement but with an uncertainty that is 13 times smaller than the HST measurement. If our measurements (and methodology) hold up to scrutiny, the distances to these stars have been determined to an accuracy comparable to those expected with Gaia. As RR Lyrae are one of the primary components of the cosmic distance ladder, the achievement of sub-1% distance errors within a formalism that accounts for dust extinction may be considered a strong buttressing of the path to eventual 1% uncertainties in Hubble's constant.
UV Direct-Writing of Metals on Polyimide<|sep|>Conductive micro-patterned copper tracks were fabricated by UV direct-writing of a nanoparticle silver seed layer followed by selective electroless copper deposition. Silver ions were first incorporated into a hydrolyzed polyimide surface layer by wet chemical treatment. A photoreactive polymer coating, methoxy poly(ethylene glycol) (MPEG) was coated on top of the substrate prior to UV irradiation. Electrons released through the interaction between the MPEG molecules and UV photons allowed the reduction of the silver ions across the MPEG/doped polyimide interface. The resultant silver seed layer has a cluster morphology which is suitable for the initiation of electroless plating. Initial results showed that the deposited copper tracks were in good agreement with the track width on the photomask and laser direct-writing can also fabricate smaller line width metal tracks with good accuracy. The facile fabrication presented here can be carried out in air, at atmospheric pressure, and on contoured surfaces.
Tunable scattering cancellation of light using anisotropic cylindrical cavities<|sep|>Engineered core-shell cylinders are good candidates for applications in invisibility and cloaking.In particular, hyperbolic nanotubes demonstrate tunable ultra-low scattering cross section in the visible spectral range. In this work we investigate the limits of validity of the condition for invisibility, which was shown to rely on reaching an epsilon near zero in one of the components of the effective permittivity tensor of the anisotropic metamaterial cavity. For incident light polarized perpendicularly to the scatterer axis, critical deviations are found in low-birefringent arrangements and also with high-index cores. We demonstrate that the ability of anisotropic metallodielectric nanocavities to dramatically reduce the scattered light is associated with a multiple Fano-resonance phenomenon. We extensively explore such resonant effect to identify tunable windows of invisibility.
The shortest period detached white dwarf + main-sequence binary<|sep|>We present high-speed ULTRACAM and SOFI photometry and X-shooter spectroscopy of the recently discovered 94 minute orbital period eclipsing white dwarf / main-sequence binary SDSS J085746.18+034255.3 (CSS 03170) and use these observations to measure the system parameters. We detect a shallow secondary eclipse and hence are able to determine an orbital inclination of 85.5 +/- 0.2 deg. The white dwarf has a mass of 0.51 +/- 0.05 Msun and a radius of 0.0247 +/- 0.0008 Rsun. With a temperature of 35,300 +/- 400K the white dwarf is highly over-inflated if it has a carbon-oxygen core, however if it has a helium core then its mass and radius are consistent with evolutionary models. Therefore, the white dwarf in SDSS J085746.18+034255.3 is most likely a helium core white dwarf with a mass close to the upper limit expected from evolution. The main-sequence star is an M8 dwarf with a mass of 0.09 +/- 0.01 Msun and a radius of 0.110 +/- 0.004 Rsun placing it close to the hydrogen burning limit. The system emerged from a common envelope ~20 million years ago and will reach a semi-detached configuration in ~400 million years, becoming a cataclysmic variable with a period of 66 minutes, below the period minimum.
High-z dusty star-forming galaxies: a top-heavy initial mass function?<|sep|>Recent estimates point to abundances of z > 4 sub-millimeter (sub-mm) galaxies far above model predictions. The matter is still debated. According to some analyses the excess may be substantially lower than initially thought and perhaps accounted for by flux boosting and source blending. However, there is no general agreement on this conclusion. An excess of z > 6 dusty galaxies has also been reported albeit with poor statistics. On the other hand, evidence of a top-heavy initial mass function (IMF) in high-z starburst galaxies has been reported in the past decades. This would translate into a higher sub-mm luminosity of dusty galaxies at fixed star formation rate, i.e., into a higher abundance of bright high-z sub-mm galaxies than expected for a universal Chabrier IMF. Exploiting our physical model for high-z proto-spheroidal galaxies, we find that part of the excess can be understood in terms of an IMF somewhat top-heavier than Chabrier. Such IMF is consistent with that recently proposed to account for the low 13C/18O abundance ratio in four dusty starburst galaxies at z = 2-3. However, extreme top-heavy IMFs are inconsistent with the sub-mm counts at z > 4.
Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels<|sep|>A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model (MGLM). MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels. MGLM endows flexible inference, including unconditional generation, conditional generation (where 1 channel is observed and other channels are generated), and partially observed generation (where incomplete observations are spread across all the channels). We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.
Asymmetry and structural information in preferential attachment graphs<|sep|>Graph symmetries intervene in diverse applications, from enumeration, to graph structure compression, to the discovery of graph dynamics (e.g., node arrival order inference). Whereas Erd\H{o}s-R\'enyi graphs are typically asymmetric, real networks are highly symmetric. So a natural question is whether preferential attachment graphs, where in each step a new node with $m$ edges is added, exhibit any symmetry. In recent work it was proved that preferential attachment graphs are symmetric for $m=1$, and there is some non-negligible probability of symmetry for $m=2$. It was conjectured that these graphs are asymmetric when $m \geq 3$. We settle this conjecture in the affirmative, then use it to estimate the structural entropy of the model. To do this, we also give bounds on the number of ways that the given graph structure could have arisen by preferential attachment. These results have further implications for information theoretic problems of interest on preferential attachment graphs.
Disease Normalization with Graph Embeddings<|sep|>The detection and normalization of diseases in biomedical texts are key biomedical natural language processing tasks. Disease names need not only be identified, but also normalized or linked to clinical taxonomies describing diseases such as MeSH. In this paper we describe deep learning methods that tackle both tasks. We train and test our methods on the known NCBI disease benchmark corpus. We propose to represent disease names by leveraging MeSH's graphical structure together with the lexical information available in the taxonomy using graph embeddings. We also show that combining neural named entity recognition models with our graph-based entity linking methods via multitask learning leads to improved disease recognition in the NCBI corpus.
Photometric observation of HAT-P-16b in the near-UV<|sep|>We present the first primary transit light curve of the hot Jupiter HAT-P-16b in the near-UV photometric band. We observed this object on December 29,2012 in order to update the transit ephemeris, constrain its planetary parameters and search for magnetic field interference. Vidotto et al. (2011a) postulate that the magnetic field of HAT-P-16b can be constrained if its near-UV light curve shows an early ingress compared to its optical light curve, while its egress remains unchanged. However, we did not detect an early ingress in our night of observing when using a cadence of 60 seconds and an average photometric precision of 2.26mmag. We find a near-UV planetary radius of Rp=1.274+-0.057RJup which is consistent with its near-IR radius of Rp=1.289+-0.066RJup (Buchhave et al., 2010). We developed an automated reduction pipeline and modeling package to process our data. The data reduction package synthesizes a set of IRAF scripts to calibrate images and perform aperture photometry. The modeling package utilizes the Levenberg-Marquardt minimization algorithm to find a least-squares best fit and a differential evolution Markov Chain Monte Carlo algorithm to find the best fit to the light curve. To constrain the red noise in both fitting models we use the residual permutation (rosary bead) method and time-averaging method.
Dust dynamics and evolution in expanding HII regions. I. Radiative drift of neutral and charged grains<|sep|>We consider dust drift under the influence of stellar radiation pressure during the pressure-driven expansion of an HII region using the chemo-dynamical model MARION. Dust size distribution is represented by four dust types: conventional polycyclic aromatic hydrocarbons (PAHs), very small grains (VSGs), big grains (BGs) and also intermediate-sized grains (ISGs), which are larger than VSGs and smaller than BGs. The dust is assumed to move at terminal velocity determined locally from the balance between the radiation pressure and gas drag. As Coulomb drag is an important contribution to the overall gas drag, we evaluate a grain charge evolution within the HII region for each dust type. BGs are effectively swept out of the HII region. The spatial distribution of ISGs within the HII region has a double peak structure, with a smaller inner peak and a higher outer peak. PAHs and VSGs are mostly coupled to the gas. The mean charge of PAHs is close to zero, so they can become neutral from time to time because of charge fluctuations. These periods of neutrality occur often enough to cause the removal of PAHs from the very interior of the HII region. For VSGs, the effect of charge fluctuations is less pronounced but still significant. We conclude that accounting for charge dispersion is necessary to describe the dynamics of small grains.
Dissecting Energy Consumption of NB-IoT Devices Empirically<|sep|>3GPP has recently introduced NB-IoT, a new mobile communication standard offering a robust and energy efficient connectivity option to the rapidly expanding market of Internet of Things (IoT) devices. To unleash its full potential, end-devices are expected to work in a plug and play fashion, with zero or minimal parameters configuration, still exhibiting excellent energy efficiency. We perform the most comprehensive set of empirical measurements with commercial IoT devices and different operators to date, quantifying the impact of several parameters to energy consumption. Our campaign proves that parameters setting does impact energy consumption, so proper configuration is necessary. We shed light on this aspect by first illustrating how the nominal standard operational modes map into real current consumption patterns of NB-IoT devices. Further, we investigate which device reported metadata metrics better reflect performance and implement an algorithm to automatically identify device state in current time series logs. Then, we provide a measurement-driven analysis of the energy consumption and network performance of two popular NB-IoT boards under different parameter configurations and with two major western European operators. We observed that energy consumption is mostly affected by the paging interval in Connected state, set by the base station. However, not all operators correctly implement such settings. Furthermore, under the default configuration, energy consumption in not strongly affected by packet size nor by signal quality, unless it is extremely bad. Our observations indicate that simple modifications to the default parameters settings can yield great energy savings.
Measurement of the muon-induced neutron seasonal modulation with LVD<|sep|>Cosmic ray muons with the average energy of 280 GeV and neutrons produced by muons are detected with the Large Volume Detector at LNGS. We present an analysis of the seasonal variation of the neutron flux on the basis of the data obtained during 15 years. The measurement of the seasonal variation of the specific number of neutrons generated by muons allows to obtaine the variation magnitude of of the average energy of the muon flux at the depth of the LVD location. The source of the seasonal variation of the total neutron flux is a change of the intensity and the average energy of the muon flux.
Analytical properties of Einasto dark matter haloes<|sep|>Recent high-resolution N-body CDM simulations indicate that nonsingular three-parameter models such as the Einasto profile perform better than the singular two-parameter models, e.g. the Navarro, Frenk and White, in fitting a wide range of dark matter haloes. While many of the basic properties of the Einasto profile have been discussed in previous studies, a number of analytical properties are still not investigated. In particular, a general analytical formula for the surface density, an important quantity that defines the lensing properties of a dark matter halo, is still lacking to date. To this aim, we used a Mellin integral transform formalism to derive a closed expression for the Einasto surface density and related properties in terms of the Fox H and Meijer G functions, which can be written as series expansions. This enables arbitrary-precision calculations of the surface density and the lensing properties of realistic dark matter halo models. Furthermore, we compared the S\'ersic and Einasto surface mass densities and found differences between them, which implies that the lensing properties for both profiles differ.
The cavity method: from exact solutions to algorithms<|sep|>The goal of this chapter is to review the main ideas that underlie the cavity method for disordered models defined on random graphs, as well as present some of its outcomes, focusing on the random constraint satisfaction problems for which it provided both a better understanding of the phase transitions they undergo, and suggestions for the development of algorithms to solve them.
Complex-valued information entropy measure for networks with directed links (digraphs). Application to citations by community agents with opposite opinions<|sep|>The notion of complex-valued information entropy measure is presented. It applies in particular to directed networks (digraphs). The corresponding statistical physics notions are outlined. The studied network, serving as a case study, in view of illustrating the discussion, concerns citations by agents belonging to two distinct communities which have markedly different opinions: the Neocreationist and Intelligent Design Proponents, on one hand, and the Darwinian Evolution Defenders, on the other hand. The whole, intra- and inter-community adjacency matrices, resulting from quotations of published work by the community agents, are elaborated and eigenvalues calculated. Since eigenvalues can be complex numbers, the information entropy may become also complex-valued. It is calculated for the illustrating case. The role of the imaginary part finiteness is discussed in particular and given some physical sense interpretation through local interaction range consideration. It is concluded that such generalizations are not only interesting and necessary for discussing directed networks, but also may give new insight into conceptual ideas about directed or other networks. Notes on extending the above to Tsallis entropy measure are found in an Appendix.
Evidence of Particle Acceleration in the Superbubble 30 Doradus C with NuSTAR<|sep|>We present evidence of diffuse, non-thermal X-ray emission from the superbubble 30 Doradus C (30 Dor C) using hard X-ray images and spectra from NuSTAR observations. For this analysis, we utilize data from a 200 ks targeted observation of 30 Dor C as well as 2.8 Ms of serendipitous off-axis observations from the monitoring of nearby SN 1987A. The complete shell of 30 Dor C is detected up to 20 keV, and the young supernova remnant MCSNR J0536-6913 in the southeast of 30 Dor C is not detected above 8 keV. Additionally, six point sources identified in previous Chandra and XMM-Newton investigations have hard X-ray emission coincident with their locations. Joint spectral fits to the NuSTAR and XMM-Newton spectra across the 30 Dor C shell confirm the non-thermal nature of the diffuse emission. Given the best-fit rolloff frequencies of the X-ray spectra, we find maximum electron energies of 70-110 TeV (assuming a B-field strength of 4$\mu$G), suggesting 30 Dor C is accelerating particles. Particles are either accelerated via diffusive shock acceleration at locations where the shocks have not stalled behind the H$\alpha$ shell, or cosmic-rays are accelerated through repeated acceleration of low-energy particles via turbulence and magnetohydrodynamic waves in the bubble's interior.
Logistic map with memory from economic model<|sep|>A generalization of the economic model of logistic growth, which takes into account the effects of memory and crises, is suggested. Memory effect means that the economic factors and parameters at any given time depend not only on their values at that time, but also on their values at previous times. For the mathematical description of the memory effects, we use the theory of derivatives of non-integer order. Crises are considered as sharp splashes (bursts) of the price, which are mathematically described by the delta-functions. Using the equivalence of fractional differential equations and the Volterra integral equations, we obtain discrete maps with memory that are exact discrete analogs of fractional differential equations of economic processes. We derive logistic map with memory, its generalizations, and "economic" discrete maps with memory from the fractional differential equations, which describe the economic natural growth with competition, power-law memory and crises.
Molecular line mapping of the giant molecular cloud associated with RCW 106 - II. Column density and dynamical state of the clumps<|sep|>We present a fully sampled C^{18}O (1-0) map towards the southern giant molecular cloud (GMC) associated with the HII region RCW 106, and use it in combination with previous ^{13}CO (1-0) mapping to estimate the gas column density as a function of position and velocity. We find localized regions of significant ^{13}CO optical depth in the northern part of the cloud, with several of the high-opacity clouds in this region likely associated with a limb-brightened shell around the HII region G333.6-0.2. Optical depth corrections broaden the distribution of column densities in the cloud, yielding a log-normal distribution as predicted by simulations of turbulence. Decomposing the ^{13}CO and C^{18}O data cubes into clumps, we find relatively weak correlations between size and linewidth, and a more sensitive dependence of luminosity on size than would be predicted by a constant average column density. The clump mass spectrum has a slope near -1.7, consistent with previous studies. The most massive clumps appear to have gravitational binding energies well in excess of virial equilibrium; we discuss possible explanations, which include magnetic support and neglect of time-varying surface terms in the virial theorem. Unlike molecular clouds as a whole, the clumps within the RCW 106 GMC, while elongated, appear to show random orientations with respect to the Galactic plane.
An estimation of the Moon radius by counting craters: a generalization of Monte-Carlo calculation of $\pi $ to spherical geometry<|sep|>By applying Monte-Carlo method, the Moon radius is obtained by counting craters in a spherical square over the surface of it. As it is well known, approximate values for $\pi $ can be obtained by counting random numbers in a square and in a quarter of circle inscribed in it in Euclidean geometry. This procedure can be extend it to spherical geometry, where new relations between the areas of a spherical square and the quarter of circle inscribed in it are obtained. When the radius of the sphere is larger than the radius of the quarter of circle, Euclidean geometry is recovered and the ratio of the areas tends to $\pi $. Using these results, theoretical deviations of $\pi $ due to the Moon radius $R$ are computed. In order to obtain this deviation, a spherical square is selected located in a great circle of the Moon. The random points over the spherical square are given by a specific zone of the Moon where craters are distributed almost randomly. Computing the ratio of the areas, the deviation of $\pi $ allows us to obtain the Moon radius with an intrinsic error given by the finite number of random craters.
Utilizing UNet for the future traffic map prediction task Traffic4cast challenge 2020<|sep|>This paper describes our UNet based experiments on the Traffic4cast challenge 2020. Similar to the Traffic4cast challenge 2019, the task is to predict traffic flow volume, direction and speed on a high resolution map of three large cities worldwide. We mainly experimented with UNet based deep convolutional networks with various compositions of densely connected convolution layers, average pooling layers and max pooling layers. Three base UNet model types are tried and predictions are combined by averaging prediction scores or taking median value. Our method achieved best performance in this years newly built challenge dataset.
Interference Effects in Quantum Belief Networks<|sep|>Probabilistic graphical models such as Bayesian Networks are one of the most powerful structures known by the Computer Science community for deriving probabilistic inferences. However, modern cognitive psychology has revealed that human decisions could not follow the rules of classical probability theory, because humans cannot process large amounts of data in order to make judgements. Consequently, the inferences performed are based on limited data coupled with several heuristics, leading to violations of the law of total probability. This means that probabilistic graphical models based on classical probability theory are too limited to fully simulate and explain various aspects of human decision making. Quantum probability theory was developed in order to accommodate the paradoxical findings that the classical theory could not explain. Recent findings in cognitive psychology revealed that quantum probability can fully describe human decisions in an elegant framework. Their findings suggest that, before taking a decision, human thoughts are seen as superposed waves that can interfere with each other, influencing the final decision. In this work, we propose a new Bayesian Network based on the psychological findings of cognitive scientists. We made experiments with two very well known Bayesian Networks from the literature. The results obtained revealed that the quantum like Bayesian Network can affect drastically the probabilistic inferences, specially when the levels of uncertainty of the network are very high (no pieces of evidence observed). When the levels of uncertainty are very low, then the proposed quantum like network collapses to its classical counterpart.
A Lagrangian model of copepod dynamics: Clustering by escape jumps in turbulence<|sep|>Planktonic copepods are small crustaceans that have the ability to swim by quick powerful jumps. Such an aptness is used to escape from high shear regions, which may be caused either by flow per- turbations, produced by a large predator (i.e., fish larvae), or by the inherent highly turbulent dynamics of the ocean. Through a combined experimental and numerical study, we investigate the impact of jumping behaviour on the small-scale patchiness of copepods in a turbulent environment. Recorded velocity tracks of copepods displaying escape response jumps in still water are here used to define and tune a Lagrangian Copepod (LC) model. The model is further employed to simulate the behaviour of thousands of copepods in a fully developed hydrodynamic turbulent flow obtained by direct numerical simulation of the Navier-Stokes equations. First, we show that the LC velocity statistics is in qualitative agreement with available experimental observations of copepods in tur- bulence. Second, we quantify the clustering of LC, via the fractal dimension $D_2$. We show that $D_2$ can be as low as ~ 2.3 and that it critically depends on the shear-rate sensitivity of the proposed LC model, in particular it exhibits a minimum in a narrow range of shear-rate values. We further investigate the effect of jump intensity, jump orientation and geometrical aspect ratio of the copepods on the small-scale spatial distribution. At last, possible ecological implications of the observed clustering on encounter rates and mating success are discussed
Mitigation of Civilian-to-Military Interference in DSRC for Urban Operations<|sep|>Dedicated short-range communications (DSRC) attracts popularity in the military applications thanks to its easiness in establishment, no need for paid subscription, and wide compatibility with any other IEEE 802.11 standards. The main challenge in DSRC is congestion due to existence of only 7 channels, which may not be enough to accommodate the increased number of transmitters expected to be deployed in the near future. Recently, there are a myriad of urban operation scenarios for the military including urban warfare and humanitarian assistance/disaster relief (HA/DR). The key challenge is that the communications among the military vehicles can be interfered by civilian users. It is the desire that the messages transmitted by the military vehicles hold a higher significance so that they can avoid the interference coming from the civilian users, which is not supported in the current version of DSRC. As a remedy, this paper proposes a protocol that prioritizes the military DSRC users while muffling the civilian DSRC users. Our results show that this prioritization method achieves higher communications performances for the military DSRC users.
Triple collinear emissions in parton showers<|sep|>A framework to include triple collinear splitting functions into parton showers is presented, and the implementation of flavor-changing NLO splitting kernels is discussed as a first application. The correspondence between the Monte-Carlo integration and the analytic computation of NLO DGLAP evolution kernels is made explicit for both timelike and spacelike parton evolution. Numerical simulation results are obtained with two independent implementations of the new algorithm, using the two independent event generation frameworks Pythia and Sherpa.
Anisotropic CR diffusion and gamma-ray production close to supernova remnants, with an application to W28<|sep|>Cosmic rays that escape their acceleration site interact with the ambient medium and produce gamma rays as the result of inelastic proton-proton collisions. The detection of such diffuse emission may reveal the presence of an accelerator of cosmic rays, and also constrain the cosmic ray diffusion coefficient in its vicinity. Preliminary results in this direction have been obtained in the last years from studies of the gamma-ray emission from molecular clouds located in the vicinity of supernova remnants, which are the prime candidate for cosmic ray production. Hints have been found for a significant suppression of the diffusion coefficient with respect to the average one in the Galaxy. However, most of these studies rely on the assumption of isotropic diffusion, which may not be very well justified. Here, we extend this study to the case in which cosmic rays that escape an accelerator diffuse preferentially along the magnetic field lines. As a first approximation, we further assume that particles are strongly magnetized and that their transport perpendicular to the magnetic field is mainly due to the wandering of the field lines. The resulting spatial distribution of runaway cosmic rays around the accelerator is, in this case, strongly anisotropic. An application of the model to the case of the supernova remnant W28 demonstrates how the estimates of the diffusion coefficient from gamma-ray observations strongly depend on the assumptions made on the isotropy (or anisotropy) of diffusion. For higher levels of anisotropy of the diffusion, larger values of the diffusion coefficient are found to provide a good fit to data. Thus, detailed models for the propagation of cosmic rays are needed in order to interpret in a correct way the gamma-ray observations.
SelectVisAR: Selective Visualisation of Virtual Environments in Augmented Reality<|sep|>When establishing a visual connection between a virtual reality user and an augmented reality user, it is important to consider whether the augmented reality user faces a surplus of information. Augmented reality, compared to virtual reality, involves two, not one, planes of information: the physical and the virtual. We propose SelectVisAR, a selective visualisation system of virtual environments in augmented reality. Our system enables an augmented reality spectator to perceive a co-located virtual reality user in the context of four distinct visualisation conditions: Interactive, Proximity, Everything, and Dollhouse. We explore an additional two conditions, Context and Spotlight, in a follow-up study. Our design uses a human-centric approach to information filtering, selectively visualising only parts of the virtual environment related to the interactive possibilities of a virtual reality user. The research investigates how selective visualisations can be helpful or trivial for the augmented reality user when observing a virtual reality user.
Almost-Orthogonal Layers for Efficient General-Purpose Lipschitz Networks<|sep|>It is a highly desirable property for deep networks to be robust against small input changes. One popular way to achieve this property is by designing networks with a small Lipschitz constant. In this work, we propose a new technique for constructing such Lipschitz networks that has a number of desirable properties: it can be applied to any linear network layer (fully-connected or convolutional), it provides formal guarantees on the Lipschitz constant, it is easy to implement and efficient to run, and it can be combined with any training objective and optimization method. In fact, our technique is the first one in the literature that achieves all of these properties simultaneously. Our main contribution is a rescaling-based weight matrix parametrization that guarantees each network layer to have a Lipschitz constant of at most 1 and results in the learned weight matrices to be close to orthogonal. Hence we call such layers almost-orthogonal Lipschitz (AOL). Experiments and ablation studies in the context of image classification with certified robust accuracy confirm that AOL layers achieve results that are on par with most existing methods. Yet, they are simpler to implement and more broadly applicable, because they do not require computationally expensive matrix orthogonalization or inversion steps as part of the network architecture. We provide code at https://github.com/berndprach/AOL.
Clustering of Galaxies with Dynamical Dark Energy<|sep|>In this paper, we study thermodynamics of the cluster of galaxies under the effect of dynamical dark energy. We evaluate the configurational integral for interacting system of galaxies in an expanding universe by including the effects produced by the varying $\Lambda$. The gravitational partition function is obtained using this configuration integral. We obtain thermodynamics quantities in canonical ensemble which depend on time and investigate the second law of thermodynamics. We also calculate the distribution function in grand canonical ensemble. The time evolution of the clustering parameter of galaxies is investigated for the time dependent (dynamical) dark energy. We conclude that the second law of thermodynamics is valid for the total system of cluster of galaxies and dynamical dark energy. We calculate correlation function and show that our model is very close to Peebles's power law, in agreement with the N-body simulation. It is observed that thermodynamics quantities depend on the modified clustering parameter for this system of galaxies.
Min-Max Regret Problems with Ellipsoidal Uncertainty Sets<|sep|>We consider robust counterparts of uncertain combinatorial optimization problems, where the difference to the best possible solution over all scenarios is to be minimized. Such minmax regret problems are typically harder to solve than their nominal, non-robust counterparts. While current literature almost exclusively focuses on simple uncertainty sets that are either finite or hyperboxes, we consider problems with more flexible and realistic ellipsoidal uncertainty sets. We present complexity results for the unconstrained combinatorial optimization problem and for the shortest path problem. To solve such problems, two types of cuts are introduced, and compared in a computational experiment.
Multi-Tubal Rank of Third Order Tensor and Related Low Rank Tensor Completion Problem<|sep|>Recently, a tensor factorization based method for a low tubal rank tensor completion problem of a third order tensor was proposed, which performed better than some existing methods. Tubal rank is only defined on one mode of third order tensor without low rank structure in the other two modes. That is, low rank structures on the other two modes are missing. Motivated by this, we first introduce multi-tubal rank, and then establish a relationship between multi-tubal rank and Tucker rank. Based on the multi-tubal rank, we propose a novel low rank tensor completion model. For this model, a tensor factorization based method is applied and the corresponding convergence anlysis is established. In addition, spatio-temporal characteristics are intrinsic features in video and internet traffic tensor data. To get better performance, we make full use of such features and improve the established tensor completion model. Then we apply tensor factorization based method for the improved model. Finally, numerical results are reported on the completion of image, video and internet traffic data to show the efficiency of our proposed methods. From the reported numerical results, we can assert that our methods outperform the existing methods.
A parsec-scale outflow from the luminous YSO IRAS 17527-2439<|sep|>Imaging observations of IRAS 17527-2439 are obtained in the near-IR JHK photometric bands and in a narrow-band filter centred at the wavelength of the H_2 1-0 S(1) line. The continuum-subtracted H_2 image is used to identify outflows. The data obtained in this study are used in conjunction with Spitzer, AKARI, and IRAS data. A parsec-scale bipolar outflow is discovered in our H_2 line image, which is supported by the detection in the archival Spitzer images. The H_2 image exhibits signs of precession of the main jet and shows tentative evidence for a second outflow. These suggest the possibility of a companion to the outflow source. There is a strong component of continuum emission in the direction of the outflow, which supports the idea that the outflow cavity provides a path for radiation to escape, thereby reducing the radiation pressure on the accreted matter. The bulk of the emission observed close to the outflow in the WFCAM and Spitzer bands is rotated counter clockwise with respect to the outflow traced in H_2, which may be due to precession. The YSO driving the outflow is identified in the Spitzer images. The spectral energy distribution (SED) of the YSO is studied using available radiative transfer models. A model fit to the SED of the central source tells us that the YSO has a mass of 12.23 M_sun and that it is in an early stage of evolution.
Countering Misinformation on Social Networks Using Graph Alterations<|sep|>We restrict the propagation of misinformation in a social-media-like environment while preserving the spread of correct information. We model the environment as a random network of users in which each news item propagates in the network in consecutive cascades. Existing studies suggest that the cascade behaviors of misinformation and correct information are affected differently by user polarization and reflexivity. We show that this difference can be used to alter network dynamics in a way that selectively hinders the spread of misinformation content. To implement these alterations, we introduce an optimization-based probabilistic dropout method that randomly removes connections between users to achieve minimal propagation of misinformation. We use disciplined convex programming to optimize these removal probabilities over a reduced space of possible network alterations. We test the algorithm's effectiveness using simulated social networks. In our tests, we use both synthetic network structures based on stochastic block models, and natural network structures that are generated using random sampling of a dataset collected from Twitter. The results show that on average the algorithm decreases the cascade size of misinformation content by up to $70\%$ in synthetic network tests and up to $45\%$ in natural network tests while maintaining a branching ratio of at least $1.5$ for correct information.
Maximum Number of Modes of Gaussian Mixtures<|sep|>Gaussian mixture models are widely used in Statistics. A fundamental aspect of these distributions is the study of the local maxima of the density, or modes. In particular, it is not known how many modes a mixture of $k$ Gaussians in $d$ dimensions can have. We give a brief account of this problem's history. Then, we give improved lower bounds and the first upper bound on the maximum number of modes, provided it is finite.
Trail of the Higgs in the primordial spectrum<|sep|>We study the effects of the Higgs directly coupled to the inflaton on the primordial power spectrum. The quadratic coupling between the Higgs and the inflaton stabilizes the Higgs in the electroweak vacuum during inflation by inducing a large effective mass for the Higgs, which also leads to oscillatory features in the primordial power spectrum due to the oscillating classical background. Meanwhile, the features from quantum fluctuations exhibit simple monotonic k-dependence and are subleading compared to the classical contributions. We also comment on the collider searches.
Local Explanation of Dialogue Response Generation<|sep|>In comparison to the interpretation of classification models, the explanation of sequence generation models is also an important problem, however it has seen little attention. In this work, we study model-agnostic explanations of a representative text generation task -- dialogue response generation. Dialog response generation is challenging with its open-ended sentences and multiple acceptable responses. To gain insights into the reasoning process of a generation model, we propose a new method, local explanation of response generation (LERG) that regards the explanations as the mutual interaction of segments in input and output sentences. LERG views the sequence prediction as uncertainty estimation of a human response and then creates explanations by perturbing the input and calculating the certainty change over the human response. We show that LERG adheres to desired properties of explanations for text generation including unbiased approximation, consistency and cause identification. Empirically, our results show that our method consistently improves other widely used methods on proposed automatic- and human- evaluation metrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can extract both explicit and implicit relations between input and output segments.
Population I Cepheids and star formation history of the Large Magellanic Cloud<|sep|>In this paper we study the Cepheids distribution in the Large Magellanic Cloud (LMC) as a function of their ages using data from the OGLE III photometric catalogue. To determine age of the Pop I Cepheids, we derived a period-age (PA) relationship using the Cepheids found in the LMC star clusters. We find two peaks in the period distribution at logP =0.49+/-0.01 and logP =0.28+/-0.01 days which correspond to fundamental and first overtone pulsation modes, respectively. Ages of the Cepheids are used to understand star formation scenario in the LMC in last 30-600 Myr. The age distribution of the LMC Cepheids is found to have a peak at log(Age)=8.2+/-0.1. This suggests that major star formation event took place at about 125-200 Myr ago which may have been triggered by a close encounter between the SMC and the LMC. Cepheids are found to be asymmetrically distributed throughout the LMC and many of them lie in clumpy structures along the bar. The frequency distribution of Cepheids suggests that most of the clumps are located to the eastern side of the LMC optical center.
Maximum of N Independent Brownian Walkers till the First Exit From the Half Space<|sep|>We consider the one-dimensional target search process that involves an immobile target located at the origin and $N$ searchers performing independent Brownian motions starting at the initial positions $\vec x = (x_1,x_2,..., x_N)$ all on the positive half space. The process stops when the target is first found by one of the searchers. We compute the probability distribution of the maximum distance $m$ visited by the searchers till the stopping time and show that it has a power law tail: $P_N(m|\vec x)\sim B_N (x_1x_2... x_N)/m^{N+1}$ for large $m$. Thus all moments of $m$ up to the order $(N-1)$ are finite, while the higher moments diverge. The prefactor $B_N$ increases with $N$ faster than exponentially. Our solution gives the exit probability of a set of $N$ particles from a box $[0,L]$ through the left boundary. Incidentally, it also provides an exact solution of the Laplace's equation in an $N$-dimensional hypercube with some prescribed boundary conditions. The analytical results are in excellent agreement with Monte Carlo simulations.
M2-branes Coupled to Antisymmetric Fluxes<|sep|>By turning on antisymmetric background fluxes, we study how multiple M2-branes are coupled to them. Our investigation concentrates on the gauge invariance conditions for the Myers-Chern-Simons action. Furthermore, the dimensional reduction of M2-branes to D2-branes introduces more constraints on the newly introduced tensors. Particularly, for the theory based on A_4 algebra, we are able to fix all components of them up to an overall normalization constant. These results can not be simply obtained from the previously proposed cubic matrix representation for this algebra. We also comment on cubic matrices as the representations of 3-algebras.
Magnetic complexity as an explanation for bimodal rotation populations among young stars<|sep|>Observations of young open clusters have revealed a bimodal distribution of fast and slower rotation rates that has proven difficult to explain with predictive models of spin down that depend on rotation rates alone. The Metastable Dynamo Model proposed recently by Brown, employing a stochastic transition probability from slow to more rapid spin down regimes, appears to be more successful but lacks a physical basis for such duality. Using detailed 3D MHD wind models computed for idealized multipole magnetic fields, we show that surface magnetic field complexity can provide this basis. Both mass and angular momentum losses decline sharply with increasing field complexity. Combined with observation evidence for complex field morphologies in magnetically active stars, our results support a picture in which young, rapid rotators lose angular momentum in an inefficient way because of field complexity. During this slow spin-down phase, magnetic complexity is eroded, precipitating a rapid transition from weak to strong wind coupling.
Physics-based Shadow Image Decomposition for Shadow Removal<|sep|>We propose a novel deep learning method for shadow removal. Inspired by physical models of shadow formation, we use a linear illumination transformation to model the shadow effects in the image that allows the shadow image to be expressed as a combination of the shadow-free image, the shadow parameters, and a matte layer. We use two deep networks, namely SP-Net and M-Net, to predict the shadow parameters and the shadow matte respectively. This system allows us to remove the shadow effects from images. We then employ an inpainting network, I-Net, to further refine the results. We train and test our framework on the most challenging shadow removal dataset (ISTD). Our method improves the state-of-the-art in terms of root mean square error (RMSE) for the shadow area by 20\%. Furthermore, this decomposition allows us to formulate a patch-based weakly-supervised shadow removal method. This model can be trained without any shadow-free images (that are cumbersome to acquire) and achieves competitive shadow removal results compared to state-of-the-art methods that are trained with fully paired shadow and shadow-free images. Last, we introduce SBU-Timelapse, a video shadow removal dataset for evaluating shadow removal methods.
Removing Qualified Names in Modular Languages<|sep|>Although the notion of qualified names is popular in module systems, it causes severe complications. In this paper, we propose an alternative to qualified names. The key idea is to import the declarations in other modules to the current module before they are used. In this way, all the declarations can be accessed locally. However, this approach is not efficient in memory usage. Our contribution is the {\it module weakening} scheme which allows us to import the minimal parts. As an example of this approach, we propose a module system for functional languages.
Bounding Box Disparity: 3D Metrics for Object Detection With Full Degree of Freedom<|sep|>The most popular evaluation metric for object detection in 2D images is Intersection over Union (IoU). Existing implementations of the IoU metric for 3D object detection usually neglect one or more degrees of freedom. In this paper, we first derive the analytic solution for three dimensional bounding boxes. As a second contribution, a closed-form solution of the volume-to-volume distance is derived. Finally, the Bounding Box Disparity is proposed as a combined positive continuous metric. We provide open source implementations of the three metrics as standalone python functions, as well as extensions to the Open3D library and as ROS nodes.
High order semi-Lagrangian methods for the BGK equation<|sep|>A new class of high-order accuracy numerical methods for the BGK model of the Boltzmann equation is presented. The schemes are based on a semi-lagrangian formulation of the BGK equation; time integration is dealt with DIRK (Diagonally Implicit Runge Kutta) and BDF methods; the latter turn out to be accurate and computationally less expensive than the former. Numerical results and examples show that the schemes are reliable and efficient for the investigation of both rarefied and fluid regimes in gasdynamics.
Stochastically excited oscillations on the upper main sequence<|sep|>Convective envelopes in stars on the main sequence are usually connected only with stars of spectral types F5 or later. However, observations as well as theory indicate that the convective outer layers in earlier stars, despite being shallow, are still effective and turbulent enough to stochastically excite oscillations. Because of the low amplitudes, exploring stochastically excited pulsations became possible only with space missions such as Kepler and CoRoT. Here I review the recent results and discuss among others, pulsators such as delta Scuti, gamma Doradus, roAp, beta Cephei, Slowly Pulsating B and Be stars, all in the context of solar-like oscillations.
Planar wiggler as a tool for generating hard twisted photons<|sep|>Simple formulas for the probability of radiation of twisted photons by scalar and Dirac particles with quantum recoil taken into account are derived. We show that the quantum recoil does not spoil the selection rule for the forward radiation of twisted photons in the planar undulator: $m+n$ is an even number, where $n$ is the harmonic number and $m$ is the projection of the total angular momentum of the radiated twisted photon. The explicit formulas for the radiation probability of twisted photons produced in the planar wiggler are obtained with account for the quantum recoil. The radiation of twisted photons by GeV electrons in the planar wiggler and in the crystalline undulator is investigated.
Can we measure the neutrino mass hierarchy in the sky?<|sep|>Cosmological probes are steadily reducing the total neutrino mass window, resulting in constraints on the neutrino-mass degeneracy as the most significant outcome. In this work we explore the discovery potential of cosmological probes to constrain the neutrino hierarchy, and point out some subtleties that could yield spurious claims of detection. This has an important implication for next generation of double beta decay experiments, that will be able to achieve a positive signal in the case of degenerate or inverted hierarchy of Majorana neutrinos. We find that cosmological experiments that nearly cover the whole sky could in principle distinguish the neutrino hierarchy by yielding 'substantial' evidence for one scenario over the another, via precise measurements of the shape of the matter power spectrum from large scale structure and weak gravitational lensing.
The feature of shadow images and observed luminosity of the Bardeen black hole surrounded by different accretions<|sep|>In this paper, by exploring the photon motion in the region near the Bardeen black hole, the shadow and observation properties of the black hole surrounded by various accretion models are studied. We analyzed the changes in shadow imaging and observation luminosity when the relevant physical parameters are changed. For the different spherical accretions background, one can find that the radius of shadow and the position of photon sphere do not change, but the observation intensity of shadow in the infalling accretion model is significantly lower than that of the static case. When the black hole is surrounded by an optically and thin disk accretion, the contribution of the photon rings, lensing rings and direct emission to the total observed flux has also been studied. Under the different forms of the emission modes, the result shows that the observed brightness is mainly determined by direct emission, while the lensing rings will provide a small part of the observation flux and the photon ring can provide a negligible observation flux. By comparing our results with the Schwarzschild spacetime, it is found that the existence or change of relevant status parameters will greatly affect the shape and observation intensity of black hole shadow. These results support that the change of state parameter will affect the spacetime structure, thus affecting the observation feature of black hole shadows.
Mild solutions to the dynamic programming equation for stochastic optimal control problems<|sep|>We show via the nonlinear semigroup theory in $L^1(\mathbb{R})$ that the $1$-D dynamic programming equation associated with a stochastic optimal control problem with multiplicative noise has a unique mild solution $\varphi\in C([0,T];W^{1,\infty}(\mathbb{R}))$ with $\varphi_{xx}\in C([0,T];L^1(\mathbb{R}))$. The $n$-dimensional case is also investigated.
Weak associativity and deformation quantization<|sep|>Non-commutativity and non-associativity are quite natural in string theory. For open strings it appear due to the presence of non-vanishing background two-form in the world volume of Dirichlet brane, while in closed string theory the flux compactifications with non-vanishing three-form also lead to non-geometric backgrounds. In this paper, working in the framework of deformation quantization, we study the violation of associativity imposing the condition that the associator of three elements should vanish whenever each two of them are equal. The corresponding star products are called alternative and satisfy an important for physical applications properties like the Moufang identities, alternative identities, Artin's theorem, etc. The condition of alternativity is invariant under the gauge transformations, just like it happens in the associative case. The price to pay is the restriction on the non-associative algebra which can be represented by the alternative star product, it should satisfy the Malcev identity. The example of nontrivial Malcev algebra is the algebra of imaginary octonions. For this case we construct an explicit expression of the non-associative and alternative star product. We also discuss the quantization of Malcev-Poisson algebras of general form, study its properties and provide the lower order expression for the alternative star product. To conclude we define the integration on the algebra of the alternative star products and show that the integrated associator vanishes.
Scintillation properties of pure and Ce$^{3+}$-doped SrF$_2$ crystals<|sep|>In this paper results of scintillation properties measurements of pure and Ce3+-doped strontium fluoride crystals are presented. We measure light output, scintillation decay time profile and temperature stability of light output. X-ray excited luminescence outputs corrected for spectral response of monochromator and photomultiplier for pure SrF2 and SrF2-0.3 mol.% Ce3+ are approximately 95% and 115% of NaI-Tl emission output, respectively. A photopeak with a 10% full width at half maximum is observed at approximately 84% the light output of a NaI-Tl crystal after correction for spectral response of photomultiplier, when sample 10x10 mm of pure SrF2 crystal is excited with 662 KeV photons. Corrected light output of SrF2-0.3 mol.% Ce3+ under 662 KeV photon excitation is found at approximately 64% the light output of the NaI-Tl crystal.
Interplay between destructive quantum interference and symmetry-breaking phenomena in graphene quantum junctions<|sep|>We study the role of electronic spin and valley symmetry in the quantum interference (QI) patterns of the transmission function in graphene quantum junctions. In particular, we link it to the position of the destructive QI anti-resonances. When the spin or valley symmetry is preserved, electrons with opposite spin or valley display the same interference pattern. On the other hand, when a symmetry is lifted the anti-resonances are split, with a consequent dramatic differentiation of the transport properties in the respective channel. We demonstrate rigorously this link in terms of the analytical structure of the electronic Green function which follows from the symmetries of the microscopic model and we confirm the result with numerical calculations for graphene nanoflakes. We argue that this is a generic and robust feature that can be exploited in different ways for the realization of nanoelectronic QI devices, generalizing the recent proposal of a QI-assisted spin-filtering effect [A. Valli et al. Nano Lett. 18, 2158 (2018)].
Darboux Transformation and Exact Solutions of the Myrzakulov-Lakshmanan-II Equation<|sep|>The Myrzakulov-Lakshmanan-II (ML-II) equation is one of a (2+1)-dimensional generalizations of the Heisenberg ferromagnetic equation. It is integrable and has a non-isospectral Lax representation. In this paper, the Darboux transformation (DT) for the ML-II equation is constructed. Using the DT, the 1-soliton and 2-soliton solutions of the ML-II equation are presented.
Radio polarimetry of compact steep spectrum sources at sub-arcsecond resolution<|sep|>Aims - We report new Very Large Array polarimetric observations of Compact Steep-Spectrum (CSS) sources at 8.4, 15, and 23GHz. Methods - Using multi-frequency VLA observations we have derived sub-arcsecond resolution images of the total intensity, polarisation, and rotation measure (RM) distributions. Results heading - We present multi-frequency VLA polarisation observations of CSS sources. About half of the sources are point-like even at the resolution of about 0.1x0.1 arcseconds. The remaining sources have double or triple structure. Low values for the percentage of polarised emission in CSS sources is confirmed. On the average, quasars are more polarised than galaxies. A wide range of RM values have been measured. There are clear indications of very large RMs up to 5\,585 rad m**(-2). CSS galaxies are characterized by RM values that are larger than CSS quasars. The majority of the objects show very large values of RM. Conclusions - The available data on sub-arcsecond-scale rest-frame RM estimates for CSS sources show that these have a wide range of values extending up to about 36,000 rad m**(-2). RM estimates indicate an overall density of the magneto-ionic medium larger than classical radio sources.
Rapid estimation of drifting parameters in continuously measured quantum systems<|sep|>We investigate the determination of a Hamiltonian parameter in a quantum system undergoing continuous measurement. We demonstrate a computationally rapid yet statistically optimal method to estimate an unknown and possibly time-dependent parameter, where we maximize the likelihood of the observed stochastic readout. By dealing directly with the raw measurement record rather than the quantum state trajectories, the estimation can be performed while the data is being acquired, permitting continuous tracking of the parameter during slow drifts in real time. Furthermore, we incorporate realistic nonidealities, such as decoherence processes and measurement inefficiency. As an example, we focus on estimating the value of the Rabi frequency of a continuously measured qubit, and compare maximum likelihood estimation to a simpler fast Fourier transform. Using this example, we discuss how the quality of the estimation depends on both the strength and duration of the measurement; we also discuss the trade-off between the accuracy of the estimate and the sensitivity to drift as the estimation duration is varied.
Ionised gas abundances in barred spiral galaxies<|sep|>This is the third paper of a series devoted to study the properties of bars from long slit spectroscopy to understand their formation, evolution and their influence on the evolution of disk galaxies. In this work we aim to determine the gas metallicity distribution of a sample of 20 barred early-type galaxies. We compare the nebular and stellar metallicity distributions to conclude about the origin of the warm gas. We compare the results of nebular emission metallicities using different semi-empirical methods. We carry out AGN diagnostic diagrams along the radius to determine the radius of influence of the AGN and the nuclei nature of the studied galaxies. We then derive the gas metallicities along the bars and compare the results to the distribution of stellar metallicities in the same regions. Most of the gas emission is centrally concentrated, although 15 galaxies also show emission along the bar. In the central regions, gas oxygen abundances are in the range 12+$\log$(O/H)= 8.4-9.1. The nebular metallicity gradients are very shallow in the bulge and bar regions. For three galaxies (one of them a LINER), the gas metallicities lie well below the stellar ones in the bulge region. These results do not depend on the choice of the semi-empirical calibration used to calculate the abundances. We see that the galaxies with the lowest abundances are those with the largest rotational velocities. The presence of gas of significantly lower metallicity than the stellar abundances in three of our galaxies, points to an external origin as the source of the gas that fuels the present star formation in the centre of some early-type barred galaxies. The fact that the bar/disk nebular metallicities are higher than the central ones might be indicating that the gas could be accreted via cooling flows instead of radial accretion from gas sitting in the outer parts of the disk.
Hiding neutrino mass in modified gravity cosmologies<|sep|>Cosmological observables show a dependence with the neutrino mass, which is partially degenerate with parameters of extended models of gravity. We study and explore this degeneracy in Horndeski generalized scalar-tensor theories of gravity. Using forecasted cosmic microwave background and galaxy power spectrum datasets, we find that a single parameter in the linear regime of the effective theory dominates the correlation with the total neutrino mass. For any given mass, a particular value of this parameter approximately cancels the power suppression due to the neutrino mass at a given redshift. The extent of the cancellation of this degeneracy depends on the cosmological large-scale structure data used at different redshifts. We constrain the parameters and functions of the effective gravity theory and determine the influence of gravity on the determination of the neutrino mass from present and future surveys.
Particle Acceleration in Collapsing Magnetic Traps with a Braking Plasma Jet<|sep|>Collapsing magnetic traps (CMTs) are one proposed mechanism for generating non-thermal particle populations in solar flares. CMTs occur if an initially stretched magnetic field structure relaxes rapidly into a lower-energy configuration, which is believed to happen as a by-product of magnetic reconnection. A similar mechanism for energising particles has also been found to operate in the Earth's magnetotail. One particular feature proposed to be of importance for particle acceleration in the magnetotail is that of a braking plasma jet, i.e. a localised region of strong flow encountering stronger magnetic field which causes the jet to slow down and stop. Such a feature has not been included in previously proposed analytical models of CMTs for solar flares. In this work we incorporate a braking plasma jet into a well studied CMT model for the first time. We present results of test particle calculations in this new CMT model. We observe and characterise new types of particle behaviour caused by the magnetic structure of the jet braking region, which allows electrons to be trapped both in the braking jet region and the loop legs. We compare and contrast the behaviour of particle orbits for various parameter regimes of the underlying trap by examining particle trajectories, energy gains and the frequency with which different types of particle orbit are found for each parameter regime.
An experimental data-driven mass-spring model of flexible Calliphora wings<|sep|>Insect wings can undergo significant deformation during flapping motion owing to inertial, elastic and aerodynamic forces. Changes in shape then alter aerodynamic forces, resulting in a fully coupled Fluid-Structure Interaction (FSI) problem. Here, we present detailed three-dimensional FSI simulations of deformable blowfly (Calliphora vomitoria) wings in flapping flight. A wing model is proposed using a multi-parameter mass-spring approach, chosen for its implementation simplicity and computational efficiency. We train the model to reproduce static elasticity measurements by optimizing its parameters using a genetic algorithm with covariance matrix adaptation (CMA-ES). Wing models trained with experimental data are then coupled to a high-performance flow solver run on massively parallel supercomputers. Different features of the modeling approach and the intra-species variability of elastic properties are discussed. We found that individuals with different wing stiffness exhibit similar aerodynamic properties characterized by dimensionless forces and power at the same Reynolds number. We further study the influence of wing flexibility by comparing between the flexible wings and their rigid counterparts. Under equal prescribed kinematic conditions for rigid and flexible wings, wing flexibility improves lift-to-drag ratio as well as lift-to-power ratio and reduces peak force observed during wing rotation.
Risk Measurement, Risk Entropy, and Autonomous Driving Risk Modeling<|sep|>It has been for a long time to use big data of autonomous vehicles for perception, prediction, planning, and control of driving. Naturally, it is increasingly questioned why not using this big data for risk management and actuarial modeling. This article examines the emerging technical difficulties, new ideas, and methods of risk modeling under autonomous driving scenarios. Compared with the traditional risk model, the novel model is more consistent with the real road traffic and driving safety performance. More importantly, it provides technical feasibility for realizing risk assessment and car insurance pricing under a computer simulation environment.
Multiple-isotope pellet cycles captured by turbulent transport modelling in the JET tokamak<|sep|>For the first time the pellet cycle of a multiple-isotope plasma is successfully reproduced with reduced turbulent transport modelling, within an integrated simulation framework. Future nuclear fusion reactors are likely to be fuelled by cryogenic pellet injection, due to higher penetration and faster response times. Accurate pellet cycle modelling is crucial to assess fuelling efficiency and burn control. In recent JET tokamak experiments, deuterium pellets with reactor-relevant deposition characteristics were injected into a pure hydrogen plasma. Measurements of the isotope ratio profile inferred a Deuterium penetration time comparable to the energy confinement time. The modelling successfully reproduces the plasma thermodynamic profiles and the fast deuterium penetration timescale. The predictions of the reduced turbulence model QuaLiKiz in the presence of a negative density gradient following pellet deposition are compared with GENE linear and nonlinear higher fidelity modelling. The results are encouraging with regard to reactor fuelling capability and burn control.
Automatic Segmentation, Localization, and Identification of Vertebrae in 3D CT Images Using Cascaded Convolutional Neural Networks<|sep|>This paper presents a method for automatic segmentation, localization, and identification of vertebrae in arbitrary 3D CT images. Many previous works do not perform the three tasks simultaneously even though requiring a priori knowledge of which part of the anatomy is visible in the 3D CT images. Our method tackles all these tasks in a single multi-stage framework without any assumptions. In the first stage, we train a 3D Fully Convolutional Networks to find the bounding boxes of the cervical, thoracic, and lumbar vertebrae. In the second stage, we train an iterative 3D Fully Convolutional Networks to segment individual vertebrae in the bounding box. The input to the second networks have an auxiliary channel in addition to the 3D CT images. Given the segmented vertebra regions in the auxiliary channel, the networks output the next vertebra. The proposed method is evaluated in terms of segmentation, localization, and identification accuracy with two public datasets of 15 3D CT images from the MICCAI CSI 2014 workshop challenge and 302 3D CT images with various pathologies introduced in [1]. Our method achieved a mean Dice score of 96%, a mean localization error of 8.3 mm, and a mean identification rate of 84%. In summary, our method achieved better performance than all existing works in all the three metrics.
Space-compatible cavity-enhanced single-photon generation with hexagonal boron nitride<|sep|>Sources of pure and indistinguishable single-photons are critical for near-future optical quantum technologies. Recently, color centers hosted by two-dimensional hexagonal boron nitride (hBN) have emerged as a promising platform for high luminosity room temperature single-photon sources. Despite the brightness of the emitters, the spectrum is rather broad and the single-photon purity is not sufficient for practical quantum information processing. Here, we report integration of such a quantum emitter hosted by hBN into a tunable optical microcavity. A small mode volume of the order of $\lambda^3$ allows us to Purcell enhance the fluorescence, with the observed excited state lifetime shortening. The cavity significantly narrows the spectrum and improves the single-photon purity by suppression of off-resonant noise. We explore practical applications by evaluating the performance of our single-photon source for quantum key distribution and quantum computing. The complete device is compact and implemented on a picoclass satellite platform, enabling future low-cost satellite-based long-distance quantum networks.
Randomness for Free<|sep|>We consider two-player zero-sum games on graphs. These games can be classified on the basis of the information of the players and on the mode of interaction between them. On the basis of information the classification is as follows: (a) partial-observation (both players have partial view of the game); (b) one-sided complete-observation (one player has complete observation); and (c) complete-observation (both players have complete view of the game). On the basis of mode of interaction we have the following classification: (a) concurrent (both players interact simultaneously); and (b) turn-based (both players interact in turn). The two sources of randomness in these games are randomness in transition function and randomness in strategies. In general, randomized strategies are more powerful than deterministic strategies, and randomness in transitions gives more general classes of games. In this work we present a complete characterization for the classes of games where randomness is not helpful in: (a) the transition function probabilistic transition can be simulated by deterministic transition); and (b) strategies (pure strategies are as powerful as randomized strategies). As consequence of our characterization we obtain new undecidability results for these games.
Deep high-resolution X-ray spectra from cool-core clusters<|sep|>We examine deep XMM-Newton Reflection Grating Spectrometer (RGS) spectra from the cores of three X-ray bright cool core galaxy clusters, Abell 262, Abell 3581 and HCG 62. Each of the RGS spectra show Fe XVII emission lines indicating the presence of gas around 0.5 keV. There is no evidence for O VII emission which would imply gas at still cooler temperatures. The range in detected gas temperature in these objects is a factor of 3.7, 5.6 and 2 for Abell 262, Abell 3581 and HCG 62, respectively. The coolest detected gas only has a volume filling fraction of 6 and 3 per cent for Abell 262 and Abell 3581, but is likely to be volume filling in HCG 62. Chandra spatially resolved spectroscopy confirms the low volume filling fractions of the cool gas in Abell 262 and Abell 3581, indicating this cool gas exists as cold blobs. Any volume heating mechanism aiming to prevent cooling would overheat the surroundings of the cool gas by a factor of 4. If the gas is radiatively cooling below 0.5 keV, it is cooling at a rate at least an order of magnitude below that at higher temperatures in Abell 262 and Abell 3581 and two-orders of magnitude lower in HCG 62. The gas may be cooling non-radiatively through mixing in these cool blobs, where the energy released by cooling is emitted in the infrared. We find very good agreement between smooth particle inference modelling of the cluster and conventional spectral fitting. Comparing the temperature distribution from this analysis with that expected in a cooling flow, there appears to be a even larger break below 0.5 keV as compared with previous empirical descriptions of the deviations of cooling flow models.
A thermodynamically consistent quasi-particle model without density-dependent infinity of the vacuum zero point energy<|sep|>In this paper, we generalize the improved quasi-particle model proposed in J. Cao et al., [ Phys. Lett. B {\bf711}, 65 (2012)] from finite temperature and zero chemical potential to the case of finite chemical potential and zero temperature, and calculate the equation of state (EOS) for (2+1) flavor Quantum Chromodynamics (QCD) at zero temperature and high density. We first calculate the partition function at finite temperature and chemical potential, then go to the limit $T=0$ and obtain the equation of state (EOS) for cold and dense QCD, which is important for the study of neutron stars. Furthermore, we use this EOS to calculate the quark-number density, the energy density, the quark-number susceptibility and the speed of sound at zero temperature and finite chemical potential and compare our results with the corresponding ones in the existing literature.
Effective carrying capacity and analytical solution of a particular case of the Richards-like two-species population dynamics model<|sep|>We consider a generalized two-species population dynamic model and analytically solve it for the amensalism and commensalism ecological interactions. These two-species models can be simplified to a one-species model with a time dependent extrinsic growth factor. With a one-species model with an effective carrying capacity one is able to retrieve the steady state solutions of the previous one-species model. The equivalence obtained between the effective carrying capacity and the extrinsic growth factor is complete only for a particular case, the Gompertz model. Here we unveil important aspects of sigmoid growth curves, which are relevant to growth processes and population dynamics.
The Atlas3D Project -- XI. Dense molecular gas properties of CO-luminous early-type galaxies<|sep|>Surveying eighteen 12CO-bright galaxies from the ATLAS3D early-type galaxy sample with the Institut de Radio Astronomie Millim\'etrique (IRAM) 30m telescope, we detect 13CO(1-0) and 13CO(2-1) in all eighteen galaxies, HCN(1-0) in 12/18 and HCO+(1-0) in 10/18. We find that the line ratios 12CO(1-0)/13CO(1-0) and 12CO(1-0)/HCN(1-0) are clearly correlated with several galaxy properties: total stellar mass, luminosity-weighted mean stellar age, molecular to atomic gas ratio, dust temperature and dust morphology. We suggest that these correlations are primarily governed by the optical depth in the 12CO lines; interacting, accreting and/or starbursting early-type galaxies have more optically thin molecular gas while those with settled dust and gas discs host optically thick molecular gas. The ranges of the integrated line intensity ratios generally overlap with those of spirals, although we note some outliers in the 12CO(1- 0)/13CO(1-0), 12CO(2-1)/13CO(2-1) and HCN/HCO+(1-0) ratios. In particular, three galaxies are found to have very low 12CO(1-0)/13CO(1-0) and 12CO(2-1)/13CO(2-1) ratios. Such low ratios may signal particularly stable molecular gas which creates stars less efficiently than 'normal' (i.e. below Schmidt-Kennicutt prediction), consistent with the low dust temperatures seen in these galaxies.
Mechanical energy fluxes associated with saturated coronal heating in M dwarfs: comparison with predictions of a turbulent dynamo<|sep|>Empirically, the X-ray luminosity LX from M dwarfs has been found to have an upper limit of about 0.2% of the bolometric flux Lbol. In the limit where magnetic fields in M dwarfs are generated in equipartition with convective motions, we use stellar models to calculate the energy flux of Alfven waves FA as a function of depth in the sub-surface convection zone. Since Alfven waves have the optimal opportunity for wave modes to reach the corona, we suggest that FA sets an upper limit on the mechanical flux Fmech which causes coronal heating. This suggestion accounts quantitatively for the saturated values of LX/Lbol which have been reported empirically for M dwarfs.
Using Partial Monotonicity in Submodular Maximization<|sep|>Over the last two decades, submodular function maximization has been the workhorse of many discrete optimization problems in machine learning applications. Traditionally, the study of submodular functions was based on binary function properties. However, such properties have an inherit weakness, namely, if an algorithm assumes functions that have a particular property, then it provides no guarantee for functions that violate this property, even when the violation is very slight. Therefore, recent works began to consider continuous versions of function properties. Probably the most significant among these (so far) are the submodularity ratio and the curvature, which were studied extensively together and separately. The monotonicity property of set functions plays a central role in submodular maximization. Nevertheless, and despite all the above works, no continuous version of this property has been suggested to date (as far as we know). This is unfortunate since submoduar functions that are almost monotone often arise in machine learning applications. In this work we fill this gap by defining the monotonicity ratio, which is a continues version of the monotonicity property. We then show that for many standard submodular maximization algorithms one can prove new approximation guarantees that depend on the monotonicity ratio; leading to improved approximation ratios for the common machine learning applications of movie recommendation, quadratic programming and image summarization.
Liquid velocity fluctuations and energy spectra in three-dimensional buoyancy driven bubbly flows<|sep|>We present a direct numerical simulation (DNS) study of pseudo-turbulence in buoyancy driven bubbly flows for a range of Reynolds ($\Rey$) and Atwood ($\At$) numbers. We study the probability distribution function of the horizontal and vertical liquid velocity fluctuations and find them to be in quantitative agreement with the experiments. The energy spectrum shows the $k^{-3}$ scaling at high $\Rey$ and becomes steeper on reducing the $\Rey$. To investigate the spectral transfers in the flow, we derive the scale-by-scale energy budget equation. Our analysis shows that, for scales smaller than the bubble diameter, the net production because of the surface tension and the kinetic energy flux balances viscous dissipation to give the $k^{-3}$ scaling of the energy spectrum for both low and high $\At$.
Location Aided Energy Balancing Strategy in Green Cellular Networks<|sep|>Most cellular network communication strategies are focused on data traffic scenarios rather than energy balance and efficient utilization. Thus mobile users in hot cells may suffer from low throughput due to energy loading imbalance problem. In state of art cellular network technologies, relay stations extend cell coverage and enhance signal strength for mobile users. However, busy traffic makes the relay stations in hot area run out of energy quickly. In this paper, we propose an energy balancing strategy in which the mobile nodes are able to dynamically select and hand over to the relay station with the highest potential energy capacity to resume communication. Key to the strategy is that each relay station merely maintains two parameters that contains the trend of its previous energy consumption and then predicts its future quantity of energy, which is defined as the relay station potential energy capacity. Then each mobile node can select the relay station with the highest potential energy capacity. Simulations demonstrate that our approach significantly increase the aggregate throughput and the average life time of relay stations in cellular network environment.
Dual Smoothing and Level Set Techniques for Variational Matrix Decomposition<|sep|>We focus on the robust principal component analysis (RPCA) problem, and review a range of old and new convex formulations for the problem and its variants. We then review dual smoothing and level set techniques in convex optimization, present several novel theoretical results, and apply the techniques on the RPCA problem. In the final sections, we show a range of numerical experiments for simulated and real-world problems.
Non-Markovian finite-temperature two-time correlation functions of system operators of a pure-dephasing model<|sep|>We evaluate the non-Markovian finite-temperature two-time correlation functions (CF's) of system operators of a pure-dephasing spin-boson model in two different ways, one by the direct exact operator technique and the other by the recently derived evolution equations, valid to second order in the system-environment interaction Hamiltonian. This pure-dephasing spin-boson model that is exactly solvable has been extensively studied as a simple decoherence model. However, its exact non-Markovian finite-temperature two-time system operator CF's, to our knowledge, have not been presented in the literature. This may be mainly due to the fact, illustrated in this article, that in contrast to the Markovian case, the time evolution of the reduced density matrix of the system (or the reduced quantum master equation) alone is not sufficient to calculate the two-time system operator CF's of non-Markovian open systems. The two-time CF's obtained using the recently derived evolution equations in the weak system-environment coupling case for this non-Markovian pure-dephasing model happen to be the same as those obtained from the exact evaluation. However, these results significantly differ from the non-Markovian two-time CF's obtained by wrongly directly applying the quantum regression theorem (QRT), a useful procedure to calculate the two-time CF's for weak-coupling Markovian open systems. This demonstrates clearly that the recently derived evolution equations generalize correctly the QRT to non-Markovian finite-temperature cases. It is believed that these evolution equations will have applications in many different branches of physics.
UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning<|sep|>We present an unsupervised learning approach for optical flow estimation by improving the upsampling and learning of pyramid network. We design a self-guided upsample module to tackle the interpolation blur problem caused by bilinear upsampling between pyramid levels. Moreover, we propose a pyramid distillation loss to add supervision for intermediate levels via distilling the finest flow as pseudo labels. By integrating these two components together, our method achieves the best performance for unsupervised optical flow learning on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. In particular, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015, which outperform the previous state-of-the-art methods by 22.2% and 15.7%, respectively.
Micropattern gas detector technologies and applications, the work of the RD51 collaboration<|sep|>The RD51 collaboration was founded in April 2008 to coordinate and facilitate efforts for development of micropattern gaseous detectors (MPGDs). The 75 institutes from 25 countries bundle their effort, experience and resources to develop these emerging micropattern technologies. MPGDs are already employed in several nuclear and high-energy physics experiments, medical imaging instruments and photodetection applications; many more applications are foreseen. They outperform traditional wire chambers in terms of rate capability, time and position resolution, granularity, stability and radiation hardness. RD51 supports efforts to make MPGDs also suitable for large areas, increase cost-efficiency, develop portable detectors and improve ease-of-use. The collaboration is organized in working groups which develop detectors with new geometries, study and simulate their properties, and design optimized electronics. Among the common supported projects are creation of test infrastructure such as beam test and irradiation facilities, and the production workshop.
LP Formulations of sufficient statistic based strategies in Finite Horizon Two-Player Zero-Sum Stochastic Bayesian games<|sep|>This paper studies two-player zero-sum stochastic Bayesian games where each player has its own dynamic state that is unknown to the other player. Using typical techniques, we provide the recursive formulas and sufficient statistics in both the primal game and its dual games. It's also shown that with a specific initial parameter, the optimal strategy of one player in a dual game is also the optimal strategy of the player in the primal game. To deal with the long finite Bayesian game we have provided an algorithm to compute the sub-optimal strategies of the players step by step to avoid the LP complexity. For this, we computed LPs to find the special initial parameters in the dual games and update the sufficient statistics of the dual games. The performance analysis has provided an upper bound on the performance difference between the optimal and suboptimal strategies. The main results are demonstrated in a security problem of underwater sensor networks.
Are stable instances easy?<|sep|>We introduce the notion of a stable instance for a discrete optimization problem, and argue that in many practical situations only sufficiently stable instances are of interest. The question then arises whether stable instances of NP--hard problems are easier to solve. In particular, whether there exist algorithms that solve correctly and in polynomial time all sufficiently stable instances of some NP--hard problem. The paper focuses on the Max--Cut problem, for which we show that this is indeed the case.
Inner structure of ZnO microspheres fabricated via laser ablation in superfluid helium<|sep|>ZnO microspheres fabricated via laser ablation in superfluid helium were found to have bubble-like voids. Even a microsphere demonstrating clear whispering gallery mode resonances in the luminescence had voids. Our analysis confirmed that the voids are located away from the surface and have negligible or little effect on the whispering gallery mode resonances since the electromagnetic energy localizes near the surface of these microspheres. The existence of the voids indicates that helium gas or any evaporated target material was present within the molten microparticles during the microsphere formation.
Exact Scalar Minimum Storage Coordinated Regenerating Codes<|sep|>We study the exact and optimal repair of multiple failures in codes for distributed storage. More particularly, we examine the use of interference alignment to build exact scalar minimum storage coordinated regenerating codes (MSCR). We show that it is possible to build codes for the case of k = 2 and d > k by aligning interferences independently but that this technique cannot be applied as soon as k > 2 and d > k. Our results also apply to adaptive regenerating codes.
Relativistic causality and clockless circuits<|sep|>Time plays a crucial role in the performance of computing systems. The accurate modelling of logical devices, and of their physical implementations, requires an appropriate representation of time and of all properties that depend on this notion. The need for a proper model, particularly acute in the design of clockless delay-insensitive (DI) circuits, leads one to reconsider the classical descriptions of time and of the resulting order and causal relations satisfied by logical operations. This questioning meets the criticisms of classical spacetime formulated by Einstein when founding relativity theory and is answered by relativistic conceptions of time and causality. Applying this approach to clockless circuits and considering the trace formalism, we rewrite Udding's rules which characterize communications between DI components. We exhibit their intrinsic relation with relativistic causality. For that purpose, we introduce relativistic generalizations of traces, called R-traces, which provide a pertinent description of communications and compositions of DI components.
Data linkage algebra, data linkage dynamics, and priority rewriting<|sep|>We introduce an algebra of data linkages. Data linkages are intended for modelling the states of computations in which dynamic data structures are involved. We present a simple model of computation in which states of computations are modelled as data linkages and state changes take place by means of certain actions. We describe the state changes and replies that result from performing those actions by means of a term rewriting system with rule priorities. The model in question is an upgrade of molecular dynamics. The upgrading is mainly concerned with the features to deal with values and the features to reclaim garbage.
Influence of structural disorder and Coulomb interactions in the superconductor-insulator transition applied to boron doped diamond<|sep|>The influence of disorder, both structural (non-diagonal) and on-site (diagonal), is studied through the inhomogeneous Bogoliubov-de Gennes (BdG) theory in narrow-band disordered superconductors with a view towards understanding superconductivity in boron doped diamond (BDD) and boron- doped nanocrystalline diamond (BNCD) films. We employ the attractive Hubbard model within the mean field approximation, including the Coulomb interaction between holes in the narrow acceptor band. We study substitutional boron incorporation in a triangular lattice, with disorder in the form of random potential fluctuations at the boron sites. The role of structural disorder was studied through non-uniform variation of the tight-binding coupling parameter where, following ex- perimental findings, we incorporate the concurrent increase in structural disorder with increasing boron concentration. We illustrate stark differences between the effects of structural and on-site disorder and show that structural disorder has a much greater effect on the density of states, mean pairing amplitude and superfluid density than on-site potential disorder. We show that structural disorder can increase the mean pairing amplitude while the spectral gap in the density of states decreases with states eventually appearing within the spectral gap for high levels of disorder. This study illustrates how the effects of structural disorder can explain some of the features found in superconducting BDD and BNCD films such as a tendency towards saturation of the T_{c} with boron doping and deviations from the expected BCS theory in the temperature dependence of the pairing amplitude and spectral gap.
Detection of Keplerian dynamics in a disk around the post-AGB star AC Her<|sep|>So far, only one rotating disk has been clearly identified and studied in AGB or post-AGB objects (in the Red Rectangle), by means of observations with high spectral and spatial resolution. However, disks are thought to play a key role in the late stellar evolution and are suspected to surround many evolved stars. We aim to extend our knowledge on these structures. We present interferometric observations of CO J=2-1 emission from the nebula surrounding the post-AGB star AC Her, a source belonging to a class of objects that share properties with the Red Rectangle and show hints of Keplerian disks. We clearly detect the Keplerian dynamics of a second disk orbiting an evolved star. Its main properties (size, temperature, central mass) are derived from direct interpretation of the data and model fitting. With this we confirm that there are disks orbiting the stars of this relatively wide class of post-AGB objects
Simulating Using Deep Learning The World Trade Forecasting of Export-Import Exchange Rate Convergence Factor During COVID-19<|sep|>By trade we usually mean the exchange of goods between states and countries. International trade acts as a barometer of the economic prosperity index and every country is overly dependent on resources, so international trade is essential. Trade is significant to the global health crisis, saving lives and livelihoods. By collecting the dataset called "Effects of COVID19 on trade" from the state website NZ Tatauranga Aotearoa, we have developed a sustainable prediction process on the effects of COVID-19 in world trade using a deep learning model. In the research, we have given a 180-day trade forecast where the ups and downs of daily imports and exports have been accurately predicted in the Covid-19 period. In order to fulfill this prediction, we have taken data from 1st January 2015 to 30th May 2021 for all countries, all commodities, and all transport systems and have recovered what the world trade situation will be in the next 180 days during the Covid-19 period. The deep learning method has received equal attention from both investors and researchers in the field of in-depth observation. This study predicts global trade using the Long-Short Term Memory. Time series analysis can be useful to see how a given asset, security, or economy changes over time. Time series analysis plays an important role in past analysis to get different predictions of the future and it can be observed that some factors affect a particular variable from period to period. Through the time series it is possible to observe how various economic changes or trade effects change over time. By reviewing these changes, one can be aware of the steps to be taken in the future and a country can be more careful in terms of imports and exports accordingly. From our time series analysis, it can be said that the LSTM model has given a very gracious thought of the future world import and export situation in terms of trade.
3-regular matchstick graphs with given girth<|sep|>We consider 3-regular planar matchstick graphs, i.e. those which have a planar embedding such that all edge lengths are equal, with given girth g. For girth 3 it is known that such graphs exist if and only if the number of vertices n is an even integer larger or equal to 8. Here we prove that such graphs exist for girth g=4 if and only if n is even and at least 20. We provide an example for girth g=5 consisting of 180 vertices.
Computation of extremes values of time averaged observables in climate models with large deviation techniques<|sep|>One of the goals of climate science is to characterize the statistics of extreme and potentially dangerous events in the present and future climate. Extreme events like heat waves, droughts, or floods due to persisting rains are characterized by large anomalies of the time average of an observable over a long time. The framework of Donsker-Varadhan large deviation theory could therefore be useful for their analysis. In this paper we discuss how concepts and numerical algorithms developed in relation with large deviation theory can be applied to study extreme, rare fluctuations of time averages of surface temperatures at regional scale with comprehensive numerical climate models. We study the convergence of large deviation functions for the time averaged European surface temperature obtained with direct numerical simulation of the climate model Plasim, and discuss their climate implications. We show how using a rare event algorithm can improve the efficiency of the computation of the large deviation rate functions. We discuss the relevance of the large deviation asymptotics for applications, and we show how rare event algorithms can be used also to improve the statistics of events on time scales shorter than the one needed for reaching the large deviation asymptotics.
Statistical Estimation of Mechanical Parameters of Clarinet Reeds Using Experimental and Numerical Approaches<|sep|>A set of 55 clarinet reeds is observed by holography, collecting 2 series of measurements made under 2 different moisture contents, from which the resonance frequencies of the 15 first modes are deduced. A statistical analysis of the results reveals good correlations, but also significant differences between both series. Within a given series, flexural modes are not strongly correlated. A Principal Component Analysis (PCA) shows that the measurements of each series can be described with 3 factors capturing more than $90\%$ of the variance: the first is linked with transverse modes, the second with flexural modes of high order and the third with the first flexural mode. A forth factor is necessary to take into account the individual sensitivity to moisture content. Numerical 3D simulations are conducted by Finite Element Method, based on a given reed shape and an orthotropic model. A sensitivity analysis revels that, besides the density, the theoretical frequencies depend mainly on 2 parameters: $E_L$ and $G_{LT}$. An approximate analytical formula is proposed to calculate the resonance frequencies as a function of these 2 parameters. The discrepancy between the observed frequencies and those calculated with the analytical formula suggests that the elastic moduli of the measured reeds are frequency dependent. A viscoelastic model is then developed, whose parameters are computed as a linear combination from 4 orthogonal components, using a standard least squares fitting procedure and leading to an objective characterization of the material properties of the cane \textit{Arundo donax}.
Robust Sequential Steady-State Analysis of Cascading Outages<|sep|>Simulating potential cascading failures can be useful for avoiding or mitigating such events. Currently, existing steady-state analysis tools are ill-suited for simulating cascading outages as they do not model frequency dependencies, they require good initial conditions to converge, and they are unable to distinguish between a collapsed grid state from a hard-to-solve test case. In this paper, we extend a circuit-theoretic approach for simulating the steady-state of a power grid to incorporate frequency deviations and implicit models for underfrequency and undervoltage load shedding. Using these models, we introduce a framework capable of robustly solving cascading outages of large-scale systems that can also locate infeasible regions. We demonstrate the efficacy of our approach by simulating entire cascading outages on more than 8000 nodes sample testcase.
Probabilistic Latent Tensor Factorization Model for Link Pattern Prediction in Multi-relational Networks<|sep|>This paper aims at the problem of link pattern prediction in collections of objects connected by multiple relation types, where each type may play a distinct role. While common link analysis models are limited to single-type link prediction, we attempt here to capture the correlations among different relation types and reveal the impact of various relation types on performance quality. For that, we define the overall relations between object pairs as a \textit{link pattern} which consists in interaction pattern and connection structure in the network, and then use tensor formalization to jointly model and predict the link patterns, which we refer to as \textit{Link Pattern Prediction} (LPP) problem. To address the issue, we propose a Probabilistic Latent Tensor Factorization (PLTF) model by introducing another latent factor for multiple relation types and furnish the Hierarchical Bayesian treatment of the proposed probabilistic model to avoid overfitting for solving the LPP problem. To learn the proposed model we develop an efficient Markov Chain Monte Carlo sampling method. Extensive experiments are conducted on several real world datasets and demonstrate significant improvements over several existing state-of-the-art methods.
2D velocity fields of simulated interacting disc galaxies<|sep|>We investigate distortions in the velocity fields of disc galaxies and their use to reveal the dynamical state of interacting galaxies at different redshift. For that purpose, we model disc galaxies in combined N-body/hydrodynamic simulations. 2D velocity fields of the gas are extracted from these simulations which we place at different redshifts from z=0 to z=1 to investigate resolution effects on the properties of the velocity field. To quantify the structure of the velocity field we also perform a kinemetry analysis. If the galaxy is undisturbed we find that the rotation curve extracted from the 2D field agrees well with long-slit rotation curves. This is not true for interacting systems, as the kinematic axis is not well defined and does in general not coincide with the photometric axis of the system. For large (Milky way type) galaxies we find that distortions are still visible at intermediate redshifts but partly smeared out. Thus a careful analysis of the velocity field is necessary before using it for a Tully-Fisher study. For small galaxies (disc scale length ~2 kpc) even strong distortions are not visible in the velocity field at z~0.5 with currently available angular resolution. Therefore we conclude that current distant Tully-Fisher studies cannot give reliable results for low-mass systems. Additionally to these studies we confirm the power of near-infrared integral field spectrometers in combination with adaptive optics (such as SINFONI) to study velocity fields of galaxies at high redshift (z~2).
Robust Distributed Accelerated Stochastic Gradient Methods for Multi-Agent Networks<|sep|>We study distributed stochastic gradient (D-SG) method and its accelerated variant (D-ASG) for solving decentralized strongly convex stochastic optimization problems where the objective function is distributed over several computational units, lying on a fixed but arbitrary connected communication graph, subject to local communication constraints where noisy estimates of the gradients are available. We develop a framework which allows to choose the stepsize and the momentum parameters of these algorithms in a way to optimize performance by systematically trading off the bias, variance, robustness to gradient noise and dependence to network effects. When gradients do not contain noise, we also prove that distributed accelerated methods can \emph{achieve acceleration}, requiring $\mathcal{O}(\kappa \log(1/\varepsilon))$ gradient evaluations and $\mathcal{O}(\kappa \log(1/\varepsilon))$ communications to converge to the same fixed point with the non-accelerated variant where $\kappa$ is the condition number and $\varepsilon$ is the target accuracy. To our knowledge, this is the first acceleration result where the iteration complexity scales with the square root of the condition number in the context of \emph{primal} distributed inexact first-order methods. For quadratic functions, we also provide finer performance bounds that are tight with respect to bias and variance terms. Finally, we study a multistage version of D-ASG with parameters carefully varied over stages to ensure exact $\mathcal{O}(-k/\sqrt{\kappa})$ linear decay in the bias term as well as optimal $\mathcal{O}(\sigma^2/k)$ in the variance term. We illustrate through numerical experiments that our approach results in practical algorithms that are robust to gradient noise and that can outperform existing methods.
Bayesian estimation of one-parameter qubit gates<|sep|>We address estimation of one-parameter unitary gates for qubit systems and seek for optimal probes and measurements. Single- and two-qubit probes are analyzed in details focusing on precision and stability of the estimation procedure. Bayesian inference is employed and compared with the ultimate quantum limits to precision, taking into account the biased nature of Bayes estimator in the non asymptotic regime. Besides, through the evaluation of the asymptotic a posteriori distribution for the gate parameter and the comparison with the results of Monte Carlo simulated experiments, we show that asymptotic optimality of Bayes estimator is actually achieved after a limited number of runs. The robustness of the estimation procedure against fluctuations of the measurement settings is investigated and the use of entanglement to improve the overall stability of the estimation scheme is also analyzed in some details.
Optical properties of coupled metal-semiconductor and metal-molecule nanocrystal complexes: the role of multipole effects<|sep|>We investigate theoretically the effects of interaction between an optical dipole (semiconductor quantum dot or molecule) and metal nanoparticles. The calculated absorption spectra of hybrid structures demonstrate strong effects of interference coming from the exciton-plasmon coupling. In particular, the absorption spectra acquire characteristic asymmetric lineshapes and strong anti-resonances. We present here an exact solution of the problem beyond the dipole approximation and find that the multipole treatment of the interaction is crucial for the understanding of strongly-interacting exciton-plasmon nano-systems. Interestingly, the visibility of the exciton resonance becomes greatly enhanced for small inter-particle distances due to the interference phenomenon, multipole effects, and electromagnetic enhancement. We find that the destructive interference is particularly strong. Using our exact theory, we show that the interference effects can be observed experimentally even in the exciting systems at room temperature.
Characterization of the Atmospheric Muon Flux in IceCube<|sep|>Muons produced in atmospheric cosmic ray showers account for the by far dominant part of the event yield in large-volume underground particle detectors. The IceCube detector, with an instrumented volume of about a cubic kilometer, has the potential to conduct unique investigations on atmospheric muons by exploiting the large collection area and the possibility to track particles over a long distance. Through detailed reconstruction of energy deposition along the tracks, the characteristics of muon bundles can be quantified, and individual particles of exceptionally high energy identified. The data can then be used to constrain the cosmic ray primary flux and the contribution to atmospheric lepton fluxes from prompt decays of short-lived hadrons. In this paper, techniques for the extraction of physical measurements from atmospheric muon events are described and first results are presented. The multiplicity spectrum of TeV muons in cosmic ray air showers for primaries in the energy range from the knee to the ankle is derived and found to be consistent with recent results from surface detectors. The single muon energy spectrum is determined up to PeV energies and shows a clear indication for the emergence of a distinct spectral component from prompt decays of short-lived hadrons. The magnitude of the prompt flux, which should include a substantial contribution from light vector meson di-muon decays, is consistent with current theoretical predictions.
Training Region-based Object Detectors with Online Hard Example Mining<|sep|>The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been -- detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.
Chern numbers of topological phonon band crossing determined with inelastic neutron scattering<|sep|>Topological invariants in the band structure, such as Chern numbers, are crucial for the classification of topological matters and dictate the occurrence of exotic properties, yet their direct spectroscopic determination has been largely limited to electronic bands. Here, we use inelastic neutron scattering in conjunction with ab initio calculations to identify a variety of topological phonon band crossings in MnSi and CoSi single crystals. We find a distinct relation between the Chern numbers of a band-crossing node and the scattering intensity modulation in momentum space around the node. Given sufficiently high resolution, our method can be used to determine arbitrarily large Chern numbers of topological phonon band-crossing nodes.
Statistical Estimation of Confounded Linear MDPs: An Instrumental Variable Approach<|sep|>In an Markov decision process (MDP), unobservable confounders may exist and have impacts on the data generating process, so that the classic off-policy evaluation (OPE) estimators may fail to identify the true value function of the target policy. In this paper, we study the statistical properties of OPE in confounded MDPs with observable instrumental variables. Specifically, we propose a two-stage estimator based on the instrumental variables and establish its statistical properties in the confounded MDPs with a linear structure. For non-asymptotic analysis, we prove a $\mathcal{O}(n^{-1/2})$-error bound where $n$ is the number of samples. For asymptotic analysis, we prove that the two-stage estimator is asymptotically normal with a typical rate of $n^{1/2}$. To the best of our knowledge, we are the first to show such statistical results of the two-stage estimator for confounded linear MDPs via instrumental variables.
Inferring Networks of Substitutable and Complementary Products<|sep|>In a modern recommender system, it is important to understand how products relate to each other. For example, while a user is looking for mobile phones, it might make sense to recommend other phones, but once they buy a phone, we might instead want to recommend batteries, cases, or chargers. These two types of recommendations are referred to as substitutes and complements: substitutes are products that can be purchased instead of each other, while complements are products that can be purchased in addition to each other. Here we develop a method to infer networks of substitutable and complementary products. We formulate this as a supervised link prediction task, where we learn the semantics of substitutes and complements from data associated with products. The primary source of data we use is the text of product reviews, though our method also makes use of features such as ratings, specifications, prices, and brands. Methodologically, we build topic models that are trained to automatically discover topics from text that are successful at predicting and explaining such relationships. Experimentally, we evaluate our system on the Amazon product catalog, a large dataset consisting of 9 million products, 237 million links, and 144 million reviews.
Anelastic Versus Fully Compressible Turbulent Rayleigh-B\'enard Convection<|sep|>Numerical simulations of turbulent Rayleigh-B\'enard convection in an ideal gas, using either the anelastic approximation or the fully compressible equations, are compared. Theoretically, the anelastic approximation is expected to hold in weakly superadiabatic systems with $\epsilon = \Delta T / T_r \ll 1$, where $\Delta T$ denotes the superadiabatic temperature drop over the convective layer and $T_r$ the bottom temperature. Using direct numerical simulations, a systematic comparison of anelastic and fully compressible convection is carried out. With decreasing superadiabaticity $\epsilon$, the fully compressible results are found to converge linearly to the anelastic solution with larger density contrasts generally improving the match. We conclude that in many solar and planetary applications, where the superadiabaticity is expected to be vanishingly small, results obtained with the anelastic approximation are in fact more accurate than fully compressible computations, which typically fail to reach small $\epsilon$ for numerical reasons. On the other hand, if the astrophysical system studied contains $\epsilon\sim O(1)$ regions, such as the solar photosphere, fully compressible simulations have the advantage of capturing the full physics. Interestingly, even in weakly superadiabatic regions, like the bulk of the solar convection zone, the errors introduced by using artificially large values for $\epsilon$ for efficiency reasons remain moderate. If quantitative errors of the order of $10\%$ are acceptable in such low $\epsilon$ regions, our work suggests that fully compressible simulations can indeed be computationally more efficient than their anelastic counterparts.
Multi-Step Bayesian Optimization for One-Dimensional Feasibility Determination<|sep|>Bayesian optimization methods allocate limited sampling budgets to maximize expensive-to-evaluate functions. One-step-lookahead policies are often used, but computing optimal multi-step-lookahead policies remains a challenge. We consider a specialized Bayesian optimization problem: finding the superlevel set of an expensive one-dimensional function, with a Markov process prior. We compute the Bayes-optimal sampling policy efficiently, and characterize the suboptimality of one-step lookahead. Our numerical experiments demonstrate that the one-step lookahead policy is close to optimal in this problem, performing within 98% of optimal in the experimental settings considered.
Multimodal Logical Inference System for Visual-Textual Entailment<|sep|>A large amount of research about multimodal inference across text and vision has been recently developed to obtain visually grounded word and sentence representations. In this paper, we use logic-based representations as unified meaning representations for texts and images and present an unsupervised multimodal logical inference system that can effectively prove entailment relations between them. We show that by combining semantic parsing and theorem proving, the system can handle semantically complex sentences for visual-textual inference.
Toward Few-step Adversarial Training from a Frequency Perspective<|sep|>We investigate adversarial-sample generation methods from a frequency domain perspective and extend standard $l_{\infty}$ Projected Gradient Descent (PGD) to the frequency domain. The resulting method, which we call Spectral Projected Gradient Descent (SPGD), has better success rate compared to PGD during early steps of the method. Adversarially training models using SPGD achieves greater adversarial accuracy compared to PGD when holding the number of attack steps constant. The use of SPGD can, therefore, reduce the overhead of adversarial training when utilizing adversarial generation with a smaller number of steps. However, we also prove that SPGD is equivalent to a variant of the PGD ordinarily used for the $l_{\infty}$ threat model. This PGD variant omits the sign function which is ordinarily applied to the gradient. SPGD can, therefore, be performed without explicitly transforming into the frequency domain. Finally, we visualize the perturbations SPGD generates and find they use both high and low-frequency components, which suggests that removing either high-frequency components or low-frequency components is not an effective defense.
Isospin violating dark matter in St\"uckelberg portal scenarios<|sep|>Hidden sector scenarios in which dark matter (DM) interacts with the Standard Model matter fields through the exchange of massive Z' bosons are well motivated by certain string theory constructions. In this work, we thoroughly study the phenomenological aspects of such scenarios and find that they present a clear and testable consequence for direct DM searches. We show that such string motivated St\"uckelberg portals naturally lead to isospin violating interactions of DM particles with nuclei. We find that the relations between the DM coupling to neutrons and protons for both, spin-independent (fn/fp) and spin-dependent (an/ap) interactions, are very flexible depending on the charges of the quarks under the extra U(1) gauge groups. We show that within this construction these ratios are generically different from plus and minus 1 (i.e. different couplings to protons and neutrons) leading to a potentially measurable distinction from other popular portals. Finally, we incorporate bounds from searches for dijet and dilepton resonances at the LHC as well as LUX bounds on the elastic scattering of DM off nucleons to determine the experimentally allowed values of fn/fp and an/ap.
Adversarial Attacks on Deep Learning Based mmWave Beam Prediction in 5G and Beyond<|sep|>Deep learning provides powerful means to learn from spectrum data and solve complex tasks in 5G and beyond such as beam selection for initial access (IA) in mmWave communications. To establish the IA between the base station (e.g., gNodeB) and user equipment (UE) for directional transmissions, a deep neural network (DNN) can predict the beam that is best slanted to each UE by using the received signal strengths (RSSs) from a subset of possible narrow beams. While improving the latency and reliability of beam selection compared to the conventional IA that sweeps all beams, the DNN itself is susceptible to adversarial attacks. We present an adversarial attack by generating adversarial perturbations to manipulate the over-the-air captured RSSs as the input to the DNN. This attack reduces the IA performance significantly and fools the DNN into choosing the beams with small RSSs compared to jamming attacks with Gaussian or uniform noise.
Bias-corrected methods for estimating the receiver operating characteristic surface of continuous diagnostic tests<|sep|>Verification bias is a well-known problem that may occur in the evaluation of predictive ability of diagnostic tests. When a binary disease status is considered, various solutions can be found in the literature to correct inference based on usual measures of test accuracy, such as the receiver operating characteristic (ROC) curve or the area underneath. Evaluation of the predictive ability of continuous diagnostic tests in the presence of verification bias for a three-class disease status is here discussed. In particular, several verification bias-corrected estimators of the ROC surface and of the volume underneath are proposed. Consistency and asymptotic normality of the proposed estimators are established and their finite sample behavior is investigated by means of Monte Carlo simulation studies. Two illustrations are also given.
Cavity quantum electrodynamics with mesoscopic topological superconductors<|sep|>We study one-dimensional $p$-wave superconductors capacitively coupled to a microwave stripline cavity. By probing the light exiting from the cavity, one can reveal the electronic susceptibility of the $p$-wave superconductor. We analyze two superconducting systems: the prototypical Kitaev chain, and a topological semiconducting wire. For both systems, we show that the photonic measurements, via the electronic susceptibility, allows us to determine the topological phase transition point, the emergence of the Majorana fermions, and the parity of their ground state. We show that all these effects, which are absent in effective theories that take into account the coupling of light to Majorana fermions only, are due to the interplay between the Majorana fermions and the bulk states of the superconductors.
The Completed SDSS-IV extended Baryon Oscillation Spectroscopic Survey: BAO and RSD measurements from the anisotropic power spectrum of the Quasar sample between redshift 0.8 and 2.2<|sep|>We measure the clustering of quasars of the final data release (DR16) of eBOSS. The sample contains $343\,708$ quasars between redshifts $0.8\leq z\leq2.2$ over $4699\,\mathrm{deg}^2$. We calculate the Legendre multipoles (0,2,4) of the anisotropic power spectrum and perform a BAO and a Full-Shape (FS) analysis at the effective redshift $z{\rm eff}=1.480$. The errors include systematic errors that amount to 1/3 of the statistical error. The systematic errors comprise a modelling part studied using a blind N-Body mock challenge and observational effects studied with approximate mocks to account for various types of redshift smearing and fibre collisions. For the BAO analysis, we measure the transverse comoving distance $D_{\rm M}(z_{\rm eff})/r_{\rm drag}=30.60\pm{0.90}$ and the Hubble distance $D_{\rm H}(z_{\rm eff})/r_{\rm drag}=13.34\pm{0.60}$. This agrees with the configuration space analysis, and the consensus yields: $D_{\rm M}(z_{\rm eff})/r_{\rm drag}=30.69\pm{0.80}$ and $D_{\rm H}(z_{\rm eff})/r_{\rm drag}=13.26\pm{0.55}$. In the FS analysis, we fit the power spectrum using a model based on Regularised Perturbation Theory, which includes Redshift Space Distortions and the Alcock-Paczynski effect. The results are $D_{\rm M}(z_{\rm eff})/r_{\rm drag}=30.68\pm{0.90}$ and $D_{\rm H}(z_{\rm eff})/r_{\rm drag}=13.52\pm{0.51}$ and we constrain the linear growth rate of structure $f(z_{\rm eff})\sigma_8(z_{\rm eff})=0.476\pm{0.047}$. Our results agree with the configuration space analysis. The consensus analysis of the eBOSS quasar sample yields: $D_{\rm M}(z_{\rm eff})/r_{\rm drag}=30.21\pm{0.79}$, $D_{\rm H}(z_{\rm eff})/r_{\rm drag}=3.23\pm{0.47}$ and $f(z_{\rm eff})\sigma_8(z_{\rm eff})=0.462\pm{0.045}$ and is consistent with a flat $\Lambda {\rm CDM}$ cosmological model using Planck results.
Data-Driven Control and Data-Poisoning attacks in Buildings: the KTH Live-In Lab case study<|sep|>This work investigates the feasibility of using input-output data-driven control techniques for building control and their susceptibility to data-poisoning techniques. The analysis is performed on a digital replica of the KTH Livein Lab, a non-linear validated model representing one of the KTH Live-in Lab building testbeds. This work is motivated by recent trends showing a surge of interest in using data-based techniques to control cyber-physical systems. We also analyze the susceptibility of these controllers to data-poisoning methods, a particular type of machine learning threat geared towards finding imperceptible attacks that can undermine the performance of the system under consideration. We consider the Virtual Reference Feedback Tuning (VRFT), a popular data-driven control technique, and show its performance on the KTH Live-In Lab digital replica. We then demonstrate how poisoning attacks can be crafted and illustrate the impact of such attacks. Numerical experiments reveal the feasibility of using data-driven control methods for finding efficient control laws. However, a subtle change in the datasets can significantly deteriorate the performance of VRFT.
Interface solitons in quadratically nonlinear photonic lattices<|sep|>We study the properties of two-color nonlinear localized modes which may exist at the interfaces separating two different periodic photonic lattices in quadratic media, focussing on the impact of phase mismatch of the photonic lattices on the properties, stability, and threshold power requirements for the generation of interface localized modes. We employ both an effective discrete model and continuum model with periodic potential and find good qualitative agreement between both models. Dynamics excitation of interface modes shows that, a two-color interface twisted mode splits into two beams with different escaping angles and carrying different energies when entering a uniform medium from the quadratic photonic lattice. The output position and energy contents of each two-color interface solitons can be controlled by judicious tuning of
Excitonic fine structure splitting in type-II quantum dots<|sep|>Excitonic fine structure splitting in quantum dots is closely related to the lateral shape of the wave functions. We have studied theoretically the fine structure splitting in InAs quantum dots with a type-II confinement imposed by a GaAsSb capping layer. We show that very small values of the fine structure splitting comparable with the natural linewidth of the excitonic transitions are achievable for realistic quantum dot morphologies despite the structural elongation and the piezoelectric field. For example, varying the capping layer thickness allows for a fine tuning of the splitting energy. The effect is explained by a strong sensitivity of the hole wave function to the morphology of the structure and a mutual compensation of the electron and hole anisotropies. The oscillator strength of the excitonic transitions in the studied quantum dots is comparable to those with a type-I confinement which makes the dots attractive for quantum communication technology as emitters of polarization-entangled photon pairs.
Designing ML-Resilient Locking at Register-Transfer Level<|sep|>Various logic-locking schemes have been proposed to protect hardware from intellectual property piracy and malicious design modifications. Since traditional locking techniques are applied on the gate-level netlist after logic synthesis, they have no semantic knowledge of the design function. Data-driven, machine-learning (ML) attacks can uncover the design flaws within gate-level locking. Recent proposals on register-transfer level (RTL) locking have access to semantic hardware information. We investigate the resilience of ASSURE, a state-of-the-art RTL locking method, against ML attacks. We used the lessons learned to derive two ML-resilient RTL locking schemes built to reinforce ASSURE locking. We developed ML-driven security metrics to evaluate the schemes against an RTL adaptation of the state-of-the-art, ML-based SnapShot attack.
A preprocessing perspective for quantum machine learning classification advantage using NISQ algorithms<|sep|>Quantum Machine Learning (QML) hasn't yet demonstrated extensively and clearly its advantages compared to the classical machine learning approach. So far, there are only specific cases where some quantum-inspired techniques have achieved small incremental advantages, and a few experimental cases in hybrid quantum computing are promising considering a mid-term future (not taking into account the achievements purely associated with optimization using quantum-classical algorithms). The current quantum computers are noisy and have few qubits to test, making it difficult to demonstrate the current and potential quantum advantage of QML methods. This study shows that we can achieve better classical encoding and performance of quantum classifiers by using Linear Discriminant Analysis (LDA) during the data preprocessing step. As a result, Variational Quantum Algorithm (VQA) shows a gain of performance in balanced accuracy with the LDA technique and outperforms baseline classical classifiers.
Electrohydrodynamic channeling effects in narrow fractures and pores<|sep|>In low-permeability rock, fluid and mineral transport occur in pores and fracture apertures at the scale of micrometers and below. At this scale, the presence of surface charge, and a resultant electrical double layer, may considerably alter transport properties. However, due to the inherent non-linearity of the governing equations, numerical and theoretical studies of the coupling between electric double layers and flow have mostly been limited to two-dimensional or axisymmetric geometries. Here, we present comprehensive three-dimensional simulations of electrohydrodynamic flow in an idealized fracture geometry consisting of a sinusoidally undulated bottom surface and a flat top surface. We investigate the effects of varying the amplitude and the Debye length (relative to the fracture aperture) and quantify their impact on flow channeling. The results indicate that channeling can be significantly increased in the plane of flow. Local flow in the narrow regions can be slowed down by up to $5 \%$ compared to the same geometry without charge, for the highest amplitude considered. This indicates that electrohydrodynamics may have consequences for transport phenomena and surface growth in geophysical systems.
Visualization of short-term heart period variability with network tools as a method for quantifying autonomic drive<|sep|>Signals from heart transplant recipients can be considered to be a natural source of information for a better understanding of the impact of the autonomic nervous system on the complexity of heart rate variability. Beat-to-beat heart rate variability can be represented as a network of increments between subsequent $RR$-intervals, which makes possible the visualization of short-term heart period fluctuations. A network is constructed of vertices representing increments between subsequent $RR$-intervals, and edges which connect adjacent $RR$-increments. Two modes of visualization of such a network are proposed. The method described is applied to nocturnal Holter signals recorded from healthy young people and from cardiac transplant recipients. Additionally, the analysis is performed on surrogate data: shuffled RR-intervals (to display short-range dependence), and shuffled phases of the Fourier Transform of RR-intervals (to filter out linear dependences). Important nonlinear properties of autonomic nocturnal regulation in short-term variability in healthy young persons are associated with $RR$-increments: accelerations and decelerations of a size greater than about 35 ms. They reveal that large accelerations are more likely antipersistent, while large decelerations are more likely persistent. Changes in $RR$-increments in a heart deprived of autonomic supervision are much lower than in a healthy individual, and appear to be maintained around a homeostatic state, but there are indications that this dynamics is nonlinear. The method is fruitful in the evaluation of the vagal activity - the quantity and quality of the vagal tone - during the nocturnal rest of healthy young people. The method also successfully extracts nonlinear effects related to intrinsic mechanisms of the heart regulation.
Distribution of the position of a driven tracer in a hardcore lattice gas<|sep|>We study the position of a biased tracer particle (TP) in a bath of hardcore particles moving on a lattice of arbitrary dimension and in contact with a reservoir. Starting from the master equation satisfied by the joint probability of the TP position and the bath configuration and resorting to a mean-field-type approximation, we presented a computation of the fluctuations of the TP position in a previous publication [Phys. Rev. E \textbf{87}, 032164 (2013)]. Counterintuitively, on a one-dimensional lattice, the diffusion coefficient of the TP was shown to be a non-monotonic function of the density of bath particles, and reaches a maximum for a nonzero value of the density. Here, we: (i) give the details of this computation and offer a physical insight into the understanding of the non-monotonicity of the diffusion coefficient; (ii) extend the mean-field-type approximation to decouple higher-order correlation functions, and obtain the evolution equation satisfied by the cumulant generating function of the position of the TP, valid in any space dimension. In the particular case of a one-dimensional lattice, we solve this equation and obtain the probability distribution of the TP position. We show that the position rescaled by its fluctuations is asymptotically distributed accordingly to a Gaussian distribution in the long-time limit.
Equitable Scheduling for the Total Completion Time Objective<|sep|>We investigate a novel scheduling problem where we have $n$ clients, each associated with a single job on each of a set of $m$ different days. On each day, a single machine is available to process the $n$ jobs non-preemptively. The goal is provide an equitable set of schedules for all $m$ days such that the sum of completion times of each client over all days is not greater than some specified equability parameter $k$. The $1\mid\mid\max_j\sum_i C_{i,j}$ problem, as we refer to it in this paper, fits nicely into a new model introduced by Heeger et al. [AAAI '21] that aims at capturing a generic notion of fairness in scheduling settings where the same set of clients repeatedly submit scheduling requests over a fixed period of time. We show that the $1\mid\mid\max_j\sum_i C_{i,j}$ problem is NP-hard even under quite severe restrictions. This leads us to investigating two natural special cases: One where we assume the number of days to be small and one where we consider the number of clients to be small. We present several tractability results for both cases.
Quantum harmonic oscillator with superoscillating initial datum<|sep|>In this paper we study the evolution of superoscillating initial data for the quantum driven harmonic oscillator. Our main result shows that superoscillations are amplified by the harmonic potential and that the analytic solution develops a singularity in finite time. We also show that for a large class of solutions of the Schr\"odinger equation, superoscillating behavior at any given time implies superoscillating behavior at any other time.
Supersymmetric Chern-Simons Theory in Presence of a Boundary<|sep|>In this paper we analyse super-Chern-Simons theory in $\mathcal{N} =1$ superspace formalism, in the presence of a boundary. We modify the Lagrangian for the Chern-Simons theory in such a way that it is supersymmetric even in the presence of a boundary. Also, even though the Chern-Simons theory is not gauge invariant in the presence of a boundary, if it is suitable coupled to a gauged Wess-Zumino-Witten model, then the resultant theory can be made gauge invariant. Thus, by suitably adding extra boundary degrees of freedom, the gauge and supersymmetry variations of the boundary theory exactly cancel the boundary terms generated by the variations of the bulk Chern-Simons theory. We also discuss how this can be applied to the ABJM model in $\mathcal{N} =1$ superspace, and we then describe the BRST and anti-BRST symmetries of the resultant gauge invariant supersymmetric theory.
Water evaporation from solute-containing aerosol droplets: effects of internal concentration and diffusivity profiles and onset of crust formation<|sep|>Saliva is primarily composed of water, but additionally includes a variety of organic and inorganic substances such as salt, proteins, peptides, mucins, virions, etc. The presence of such solutes affects the evaporation time of respiratory droplets that are sedimenting in air, and thereby the airborne transmission of infections. From solutions of the coupled heat-conduction and water-diffusion equations within the droplet and in the ambient vapor phase, we find that the solute-induced water vapor-pressure reduction considerably slows down the evaporation process and dominates the solute-concentration dependence of the droplet evaporation time. The evaporation-induced solute-concentration gradient near the droplet surface, which is accounted for using a two-stage evaporation model, is found to further intensify the slowing down of the drying process. On the other hand, the presence of solutes is found to reduce evaporation cooling of the droplet, which causes a slight decrease in the evaporation time. Overall, the first two effects are dominant, meaning that the droplet evaporation time increases in the presence of solutes. The solute-concentration dependence of the water diffusivity inside the droplet does not significantly change the evaporation time. Finally, crust formation on the droplet surface is found to increase the final equilibrium size of the droplet.
Knowledge Association with Hyperbolic Knowledge Graph Embeddings<|sep|>Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs. They also depend on high embedding dimensions to realize enough expressiveness. Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association. We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation. Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.
A Penalty Approach for Normalizing Feature Distributions to Build Confounder-Free Models<|sep|>Translating machine learning algorithms into clinical applications requires addressing challenges related to interpretability, such as accounting for the effect of confounding variables (or metadata). Confounding variables affect the relationship between input training data and target outputs. When we train a model on such data, confounding variables will bias the distribution of the learned features. A recent promising solution, MetaData Normalization (MDN), estimates the linear relationship between the metadata and each feature based on a non-trainable closed-form solution. However, this estimation is confined by the sample size of a mini-batch and thereby may cause the approach to be unstable during training. In this paper, we extend the MDN method by applying a Penalty approach (referred to as PDMN). We cast the problem into a bi-level nested optimization problem. We then approximate this optimization problem using a penalty method so that the linear parameters within the MDN layer are trainable and learned on all samples. This enables PMDN to be plugged into any architectures, even those unfit to run batch-level operations, such as transformers and recurrent models. We show improvement in model accuracy and greater independence from confounders using PMDN over MDN in a synthetic experiment and a multi-label, multi-site dataset of magnetic resonance images (MRIs).
Continuing EVN monitoring of HST-1 in the jet of M87<|sep|>The relativistic jet in M87 offers a unique opportunity for understanding the detailed jet structure and emission processes due to its proximity. In particular, the peculiar jet region HST-1 at ~1 arcsecond (or 80 pc, projected) from the nucleus has attracted a great deal of interest in the last decade because of its superluminal motion and broadband radio-to-X-ray outbursts, which may be further connected to the gamma-ray productions up to TeV energies. Over the last five years, we have been doing an intensive monitoring of HST-1 with EVN at 5GHz in order to examine the detailed structural evolution and its possible connection to high-energy activities. While this program already yielded interesting results in terms of the detailed mas-scale structure, proper motion measurements and structural variations, the recent HST-1 brightness is continuously decreasing at this frequency. To counter this, we have shifted our monitoring frequency to 1.7GHz from October 2013. This strategy successfully recovered the fainter emission that was missed in the last 5GHz session. Moreover, we again discovered the sudden emergence of a new component at the upstream edge of HST-1, demonstrating that the use of EVN 1.7GHz is indeed powerful to probe the current weak nature of HST-1. Here we report early results from the 1.7GHz monitoring as well as further progress on the long-term kinematic study.
Flavor Cosmology: Dynamical Yukawas in the Froggatt-Nielsen Mechanism<|sep|>Can the cosmological dynamics responsible for settling down the present values of the Cabibbo-Kobayashi-Maskawa matrix be related to electroweak symmetry breaking? If the Standard Model Yukawa couplings varied in the early universe and started with order one values before electroweak symmetry breaking, the CP violation associated with the CKM matrix could be the origin of the matter-antimatter asymmetry. The large effective Yukawa couplings which lead to the enhanced CP violation can also help in achieving a strong first-order electroweak phase transition. We study in detail the feasibility of this idea by implementing dynamical Yukawa couplings in the context of the Froggatt--Nielsen mechanism. We discuss two main realizations of such a mechanism, related phenomenology, cosmological and collider bounds, and provide an estimate of the baryonic yield. A generic prediction is that this scenario always features a new scalar field below the electroweak scale. We point out ways to get around this conclusion.
Utility of the Weak Temperature Gradient Approximation for Earth-Like Tidally Locked Exoplanets<|sep|>Planets in M dwarf stars' habitable zones are likely to be tidally locked with orbital periods of order tens of days. This means that the effects of rotation on atmospheric dynamics will be relatively weak, which requires small horizontal temperature gradients above the boundary layer of terrestrial atmospheres. An analytically solvable and dynamically consistent model for planetary climate with only three free parameters can be constructed by making the weak temperature gradient (WTG) approximation, which assumes temperatures are horizontally uniform aloft. The extreme numerical efficiency of a WTG model compared to a 3D general circulation model (GCM) makes it an optimal tool for Monte Carlo fits to observables over parameter space. Additionally, such low-order models are critical for developing physical intuition and coupling atmospheric dynamics to models of other components of planetary climate. The objective of this paper is to determine whether a WTG model provides an adequate approximation of the effect of atmospheric dynamics on quantities likely to be observed over the next decade. To do this we first tune a WTG model to GCM output for an Earth-like tidally locked planet with a dry, 1 bar atmosphere, then generate and compare the expected phase curves of both models. We find that differences between the two models would be extremely difficult to detect from phase curves using JWST. This result demonstrates the usefulness of the WTG approximation when used in conjunction with GCMs as part of a modeling hierarchy to understand the climate of remote planets.
Recurrent Dirichlet Belief Networks for Interpretable Dynamic Relational Data Modelling<|sep|>The Dirichlet Belief Network~(DirBN) has been recently proposed as a promising approach in learning interpretable deep latent representations for objects. In this work, we leverage its interpretable modelling architecture and propose a deep dynamic probabilistic framework -- the Recurrent Dirichlet Belief Network~(Recurrent-DBN) -- to study interpretable hidden structures from dynamic relational data. The proposed Recurrent-DBN has the following merits: (1) it infers interpretable and organised hierarchical latent structures for objects within and across time steps; (2) it enables recurrent long-term temporal dependence modelling, which outperforms the one-order Markov descriptions in most of the dynamic probabilistic frameworks. In addition, we develop a new inference strategy, which first upward-and-backward propagates latent counts and then downward-and-forward samples variables, to enable efficient Gibbs sampling for the Recurrent-DBN. We apply the Recurrent-DBN to dynamic relational data problems. The extensive experiment results on real-world data validate the advantages of the Recurrent-DBN over the state-of-the-art models in interpretable latent structure discovery and improved link prediction performance.
A Comprehensive Study of Bright Fermi-GBM Short Gamma-Ray Bursts: II. Very Short Burst and Its Implications<|sep|>A thermal component is suggested to be the physical composition of the ejecta of several bright gamma-ray bursts (GRBs). Such a thermal component is discovered in the time-integrated spectra of several short GRBs as well as long GRBs. In this work, we present a comprehensive analysis of ten very short GRBs detected by Fermi Gamma-Ray Burst Monitor to search for the thermal component. We found that both the resultant low-energy spectral index and the peak energy in each GRB imply a common hard spectral feature, which is in favor of the main classification of the short/hard versus long/soft dichotomy in the GRB duration. We also found moderate evidence for the detection of thermal component in eight GRBs. Although such a thermal component contributes a small proportion of the global prompt gamma-ray emission, the modified thermal-radiation mechanism could enhance the proportion significantly, such as in subphotospheric dissipation.
Detection of very high energy gamma-ray emission from the gravitationally-lensed blazar QSO B0218+357 with the MAGIC telescopes<|sep|>Context. QSO B0218+357 is a gravitationally lensed blazar located at a redshift of 0.944. The gravitational lensing splits the emitted radiation into two components, spatially indistinguishable by gamma-ray instruments, but separated by a 10-12 day delay. In July 2014, QSO B0218+357 experienced a violent flare observed by the Fermi-LAT and followed by the MAGIC telescopes. Aims. The spectral energy distribution of QSO B0218+357 can give information on the energetics of z ~ 1 very high energy gamma- ray sources. Moreover the gamma-ray emission can also be used as a probe of the extragalactic background light at z ~ 1. Methods. MAGIC performed observations of QSO B0218+357 during the expected arrival time of the delayed component of the emission. The MAGIC and Fermi-LAT observations were accompanied by quasi-simultaneous optical data from the KVA telescope and X-ray observations by Swift-XRT. We construct a multiwavelength spectral energy distribution of QSO B0218+357 and use it to model the source. The GeV and sub-TeV data, obtained by Fermi-LAT and MAGIC, are used to set constraints on the extragalactic background light. Results. Very high energy gamma-ray emission was detected from the direction of QSO B0218+357 by the MAGIC telescopes during the expected time of arrival of the trailing component of the flare, making it the farthest very high energy gamma-ray sources detected to date. The observed emission spans the energy range from 65 to 175 GeV. The combined MAGIC and Fermi-LAT spectral energy distribution of QSO B0218+357 is consistent with current extragalactic background light models. The broad band emission can be modeled in the framework of a two zone external Compton scenario, where the GeV emission comes from an emission region in the jet, located outside the broad line region.
Collective excitations of a quantized vortex in $^3P_2$ superfluids in neutron stars<|sep|>We discuss collective excitations (both fundamental and solitonic excitations) of quantized superfluid vortices in neutron $^3P_2$ superfluids, which likely exist in high density neutron matter such as neutron stars. Besides the well-known Kelvin modes (translational zero modes), we find a gapfull mode whose low-energy description takes the simple form of a double sine-Gordon model. The associated kink solution and its effects on spontaneous magnetization inside the vortex core are analyzed in detail.
A fast approximate skeleton with guarantees for any cloud of points in a Euclidean space<|sep|>The tree reconstruction problem is to find an embedded straight-line tree that approximates a given cloud of unorganized points in $\mathbb{R}^m$ up to a certain error. A practical solution to this problem will accelerate a discovery of new colloidal products with desired physical properties such as viscosity. We define the Approximate Skeleton of any finite point cloud $C$ in a Euclidean space with theoretical guarantees. The Approximate Skeleton ASk$(C)$ always belongs to a given offset of $C$, i.e. the maximum distance from $C$ to ASk$(C)$ can be a given maximum error. The number of vertices in the Approximate Skeleton is close to the minimum number in an optimal tree by factor 2. The new Approximate Skeleton of any unorganized point cloud $C$ is computed in a near linear time in the number of points in $C$. Finally, the Approximate Skeleton outperforms past skeletonization algorithms on the size and accuracy of reconstruction for a large dataset of real micelles and random clouds.
Radiative hydrodynamics simulations of red supergiant stars: I. interpretation of interferometric observations<|sep|>Context. It has been suggested that convection in Red Supergiant (RSG) stars gives rise to large-scale granules causing observable surface inhomogeneities. This convection is also extremely vigorous, and suspected to be one of the causes of mass-loss in RSGs. It must thus be understood in details. Evidence has been accumulated that there are asymmetries in the photospheres of RSGs, but detailedstudies of granulation are still lacking. Interferometric observations offer an exciting possibility to tackle this question, but they are still often interpreted using smooth symmetrical limb-darkened intensity distributions, or very simple spotted ad hoc models. Aims. We explore the impact of the granulation on visibility curves and closure phases using the radiative transfer code OPTIM3D. We simultaneously assess how 3D simulations of convection in RSG with CO5BOLD can be tested against these observations. Methods. We use 3D radiative-hydrodynamics (RHD) simulations of convection to compute intensity maps at various wavelengths and time, from which we derive interferometric visibility amplitudes and phases. We study their behaviour with time, position angle, and wavelength, and compare them to observations of the RSG alpha Ori Results. We provide average limb-darkening coefficients for RSGs. We detail the prospects for the detection and characterization of granulation (contrast, size) on RSGs. We demonstrate that our RHD simulations provide an excellent fit to existing interferometric observation of alpha Ori, contrary to limb darkened disks. This confirms the existence of large convective cells on the surface of Betelgeuse.
Numerical treatment of interfaces in Quantum Mechanics<|sep|>In this article we develop a numerical scheme to deal with interfaces between touching numerical grids when solving Schr\"o{}dinger equation. In order to pass the information among grids we use the values of the fields only at the contact point between them. Surprisingly we obtain a convergent methods which is third order accurate with respect to the spatial resolution. In test cases, at the minimal resolution needed to describe correctly the waves, the error of this approximation is similar to that of a homogeneous (centered differences everywhere) scheme with three points stencil, that is a sixth order finite difference operator. The semi-discrete approximation preserves the norm and uses standard finite difference operators satisfying summation by parts. For the time integrator we use a semi-implicit IMEX Runge Kutta method.
Spin alignment and violation of the OZI rule in exclusive $\omega$ and $\phi$ production in pp collisions<|sep|>Exclusive production of the isoscalar vector mesons $\omega$ and $\phi$ is measured with a 190 GeV$/c$ proton beam impinging on a liquid hydrogen target. Cross section ratios are determined in three intervals of the Feynman variable $x_{F}$ of the fast proton. A significant violation of the OZI rule is found, confirming earlier findings. Its kinematic dependence on $x_{F}$ and on the invariant mass $M_{p\mathrm{V}}$ of the system formed by fast proton $p_\mathrm{fast}$ and vector meson $V$ is discussed in terms of diffractive production of $p_\mathrm{fast}V$ resonances in competition with central production. The measurement of the spin density matrix element $\rho_{00}$ of the vector mesons in different selected reference frames provides another handle to distinguish the contributions of these two major reaction types. Again, dependences of the alignment on $x_{F}$ and on $M_{p\mathrm{V}}$ are found. Most of the observations can be traced back to the existence of several excited baryon states contributing to $\omega$ production which are absent in the case of the $\phi$ meson. Removing the low-mass $M_{p\mathrm{V}}$ resonant region, the OZI rule is found to be violated by a factor of eight, independently of $x_\mathrm{F}$.
Automatic morphological classification of galaxies: convolutional autoencoder and bagging-based multiclustering model<|sep|>In order to obtain morphological information of unlabeled galaxies, we present an unsupervised machine-learning (UML) method for morphological classification of galaxies, which can be summarized as two aspects: (1) the methodology of convolutional autoencoder (CAE) is used to reduce the dimensions and extract features from the imaging data; (2) the bagging-based multiclustering model is proposed to obtain the classifications with high confidence at the cost of rejecting the disputed sources that are inconsistently voted. We apply this method on the sample of galaxies with $H<24.5$ in CANDELS. Galaxies are clustered into 100 groups, each contains galaxies with analogous characteristics. To explore the robustness of the morphological classifications, we merge 100 groups into five categories by visual verification, including spheroid, early-type disk, late-type disk, irregular, and unclassifiable. After eliminating the unclassifiable category and the sources with inconsistent voting, the purity of the remaining four subclasses are significantly improved. Massive galaxies ($M_*>10^{10}M_\odot$) are selected to investigate the connection with other physical properties. The classification scheme separates galaxies well in the U-V and V-J color space and Gini-$M_{20}$ space. The gradual tendency of S\'{e}rsic indexes and effective radii is shown from the spheroid subclass to the irregular subclass. It suggests that the combination of CAE and multi-clustering strategy is an effective method to cluster galaxies with similar features and can yield high-quality morphological classifications. Our study demonstrates the feasibility of UML in morphological analysis that would develop and serve the future observations made with China Space Station telescope.
A Super-Jupiter Microlens Planet Characterized by High-Cadence KMTNet Microlensing Survey Observations of OGLE-2015-BLG-0954<|sep|>We report the characterization of a massive (m_p=3.9 +- 1.4 M_jup) microlensing planet (OGLE-2015-BLG-0954Lb) orbiting an M dwarf host (M=0.33 +- 0.12 M_sun) at a distance toward the Galactic bulge of 0.6 (+0.4,-0.2) kpc, which is extremely nearby by microlensing standards. The planet-host projected separation is a_perp ~ 1.2 AU. The characterization was made possible by the wide-field (4 sq. deg.) high cadence (Gamma = 6/hr) monitoring of the Korea Microlensing Telescope Network (KMTNet), which had two of its three telescopes in commissioning operations at the time of the planetary anomaly. The source crossing time t_* = 16 min is among the shortest ever published. The high-cadence, wide-field observations that are the hallmark of KMTNet are the only way to routinely capture such short crossings. High-cadence resolution of short caustic crossings will preferentially lead to mass and distance measurements for the lens. This is because the short crossing time typically implies a nearby lens, which enables the measurement of additional effects (bright lens and/or microlens parallax). When combined with the measured crossing time, these effects can yield planet/host masses and distance.
HeatER: An Efficient and Unified Network for Human Reconstruction via Heatmap-based TransformER<|sep|>Recently, vision transformers have shown great success in 2D human pose estimation (2D HPE), 3D human pose estimation (3D HPE), and human mesh reconstruction (HMR) tasks. In these tasks, heatmap representations of the human structural information are often extracted first from the image by a CNN, and then further processed with a transformer architecture to provide the final HPE or HMR estimation. However, existing transformer architectures are not able to process these heatmap inputs directly, forcing an unnatural flattening of the features prior to input. Furthermore, much of the performance benefit in recent HPE and HMR methods has come at the cost of ever-increasing computation and memory needs. Therefore, to simultaneously address these problems, we propose HeatER, a novel transformer design which preserves the inherent structure of heatmap representations when modeling attention while reducing the memory and computational costs. Taking advantage of HeatER, we build a unified and efficient network for 2D HPE, 3D HPE, and HMR tasks. A heatmap reconstruction module is applied to improve the robustness of the estimated human pose and mesh. Extensive experiments demonstrate the effectiveness of HeatER on various human pose and mesh datasets. For instance, HeatER outperforms the SOTA method MeshGraphormer by requiring 5% of Params and 16% of MACs on Human3.6M and 3DPW datasets. Code will be publicly available.
Activity-Based Search for Black-Box Contraint-Programming Solvers<|sep|>Robust search procedures are a central component in the design of black-box constraint-programming solvers. This paper proposes activity-based search, the idea of using the activity of variables during propagation to guide the search. Activity-based search was compared experimentally to impact-based search and the WDEG heuristics. Experimental results on a variety of benchmarks show that activity-based search is more robust than other heuristics and may produce significant improvements in performance.
Source properties of the lowest signal-to-noise-ratio binary black hole detections<|sep|>We perform a detailed parameter estimation study of binary black hole merger events reported in Zackay et al. and Venumadhav et al.. These are some of the faintest signals reported so far, and hence, relative to the loud events in the GWTC-1 catalog, the data should have lesser constraining power on their intrinsic parameters. Hence we examine the robustness of parameter inference to choices made in the analysis, as well as any potential systematics. We check the impact of different methods of estimating the noise power spectral density, different waveform models, and different priors for the compact object spins. For most of the events, the resulting differences in the inferred values of the parameters are much smaller than their statistical uncertainties. The estimation of the effective spin parameter $\chi_{\mathrm{eff}}$, i.e. the projection of the mass-weighted total spin along the angular momentum, can be sensitive to analysis choices for two of the sources with the largest effective spin magnitudes, GW151216 and GW170403. The primary differences arise from using a 3D isotropic spin prior: the tails of the posterior distributions should be interpreted with care and due consideration of the other data analysis choices.
Critical Fluctuations in Polymer Solutions: Crossover from Criticality to Tricriticality<|sep|>Critical fluctuations in fluids and fluid mixtures yield a nonanalytic asymptotic Ising-like critical thermodynamic behavior in terms of power laws with universal exponents. In polymer solutions, the amplitudes of these power laws depend on the degree of polymerization. Nonasymptotic behavior (upon the departure from the critical point) is particularly interesting in the case of polymer solutions, where it is governed by a competition between the correlation length of the critical fluctuations and the radius of gyration of the polymer molecules. If the correlation length is the dominant length scale, Ising-like critical behavior is observed. If, however, the radius of gyration exceeds the correlation length, tricritical behavior with mean-field critical exponents is observed. The Ising-like critical region shrinks with the increase of the polymer molecular weight. In the limit of an infinite degree of polymerization, the Ising-like critical region vanishes, yielding to theta-point tricriticality.
Mixture Proportion Estimation via Kernel Embedding of Distributions<|sep|>Mixture proportion estimation (MPE) is the problem of estimating the weight of a component distribution in a mixture, given samples from the mixture and component. This problem constitutes a key part in many "weakly supervised learning" problems like learning with positive and unlabelled samples, learning with label noise, anomaly detection and crowdsourcing. While there have been several methods proposed to solve this problem, to the best of our knowledge no efficient algorithm with a proven convergence rate towards the true proportion exists for this problem. We fill this gap by constructing a provably correct algorithm for MPE, and derive convergence rates under certain assumptions on the distribution. Our method is based on embedding distributions onto an RKHS, and implementing it only requires solving a simple convex quadratic programming problem a few times. We run our algorithm on several standard classification datasets, and demonstrate that it performs comparably to or better than other algorithms on most datasets.
Stability of generalized Einstein-Maxwell-scalar black holes<|sep|>We study the stability of static black holes in generalized Einstein-Maxwell-scalar theories. We derive the master equations for the odd and even parity perturbations. The sufficient and necessary conditions for the stability of black holes under odd-parity perturbations are derived. We show that these conditions are usually not similar to energy conditions even in the simplest case of a minimally coupled scalar field. We obtain the necessary conditions for the stability of even-parity perturbations. We also derived the speed of propagation of the five degrees of freedom and obtained the class of theories for which all degrees of freedom propagate at the speed of light. Finally, we have applied our results to various black holes in nonlinear electrodynamics, scalar-tensor theories and Einstein-Maxwell-dilaton theory. For the latter, we have also calculated the quasinormal modes.
LOFT as a discovery machine for jetted Tidal Disruption Events<|sep|>This is a White Paper in support of the mission concept of the Large Observatory for X-ray Timing (LOFT), proposed as a medium-sized ESA mission. We discuss the potential of LOFT for the study of jetted tidal disruption events. For a summary, we refer to the paper.
Extended Birkhoff's Theorem in the f(T) Gravity<|sep|>The f(T) theory, a generally modified teleparallel gravity, has been proposed as an alternative gravity model to account for the dark energy phenomena. Following our previous work [Xin-he Meng and Ying-bin Wang, EPJC(2011), arXiv:1107.0629v1], we prove that the Birkhoff's theorem holds in a more general context, specifically with the off diagonal tetrad case, in this communication letter. Then, we discuss respectively the results of the external vacuum and internal gravitational field in the f(T) gravity framework, as well as the extended meaning of this theorem. We also investigate the validity of the Birkhoff's theorem in the frame of f(T) gravity via conformal transformation by regarding the Brans-Dicke-like scalar as effective matter, and study the equivalence between both Einstein frame and Jordan frame.
Dual-Scale Single Image Dehazing Via Neural Augmentation<|sep|>Model-based single image dehazing algorithms restore haze-free images with sharp edges and rich details for real-world hazy images at the expense of low PSNR and SSIM values for synthetic hazy images. Data-driven ones restore haze-free images with high PSNR and SSIM values for synthetic hazy images but with low contrast, and even some remaining haze for real world hazy images. In this paper, a novel single image dehazing algorithm is introduced by combining model-based and data-driven approaches. Both transmission map and atmospheric light are first estimated by the model-based methods, and then refined by dual-scale generative adversarial networks (GANs) based approaches. The resultant algorithm forms a neural augmentation which converges very fast while the corresponding data-driven approach might not converge. Haze-free images are restored by using the estimated transmission map and atmospheric light as well as the Koschmiederlaw. Experimental results indicate that the proposed algorithm can remove haze well from real-world and synthetic hazy images.
Hydrogen bond analysis of confined water in mesoporous silica using the reactive force field<|sep|>The structural and dynamical properties of water confined in nanoporous silica with a pore diameter of 2.7 nm were investigated by performing large-scale molecular dynamics simulations using the reactive force field. The radial distribution function and diffusion coefficient of water were calculated, and the values at the center of the pore agreed well with experimental values for real water. In addition, the pore was divided into thin coaxial layers, and the average number of hydrogen bonds, hydrogen bond lifetime, and hydrogen bond strength were calculated as a function of the radial distance from the pore central axis. The analysis showed that hydrogen bonds involving silanol (Si-OH) have a longer lifetime, although the average number of hydrogen bonds per atom does not change from that at the pore center. The longer lifetime, as well as smaller diffusion coefficient, of these hydrogen bonds is attributed to their greater strength.
Statistical properties of magnetic structures and energy dissipation during turbulent reconnection in the Earth's magnetotail<|sep|>We present the first statistical study of magnetic structures and associated energy dissipation observed during a single period of turbulent magnetic reconnection, by using the \textit{in-situ} measurements of the Magnetospheric Multiscale mission in the Earth's magnetotail on July 26, 2017. The structures are selected by identifying a bipolar signature in the magnetic field and categorized as plasmoids or current sheets via an automated algorithm which examines current density and plasma flow. The size of the plasmoids forms a decaying exponential distribution ranging from sub-electron up to ion scales. The presence of substantial number of current sheets is consistent with a physical picture of dynamic production and merging of plasmoids during turbulent reconnection. The magnetic structures are locations of significant energy dissipation via electric field parallel to the local magnetic field, while dissipation via perpendicular electric field dominates outside of the structures. Significant energy also returns from particles to fields.
Heavy quark potential and jet quenching parameter in a D-instanton background<|sep|>Using the AdS/CFT correspondence, we study the heavy quark potential and the jet quenching parameter in the near horizon limit of D3-D(-1) background. The results are compared with those of conformal cases. It is shown that the presence of instantons tends to suppress the heavy quark potential and enhance the jet quenching parameter.
The weighted Tower of Hanoi<|sep|>The weighted Tower of Hanoi is a new generalization of the classical Tower of Hanoi problem, where a move of a disc between two pegs $i$ and $j$ is weighted by a positive real $w_{ij}\geq 0$. This new problem generalizes the concept of finding the minimum number of moves to solve the Tower of Hanoi, to find a sequence of moves with the minimum total cost. We present an optimal dynamic algorithm to solve the weighted Tower of Hanoi problem, we also establish some properties of this problem, as well as its relation with the Tower of Hanoi variants that are based on move restriction.
A matter of time: Using dynamics and theory to uncover mechanisms of transcriptional bursting<|sep|>Eukaryotic transcription generally occurs in bursts of activity lasting minutes to hours; however, state-of-the-art measurements have revealed that many of the molecular processes that underlie bursting, such as transcription factor binding to DNA, unfold on timescales of seconds. This temporal disconnect lies at the heart of a broader challenge in physical biology of predicting transcriptional outcomes and cellular decision-making from the dynamics of underlying molecular processes. Here, we review how new dynamical information about the processes underlying transcriptional control can be combined with theoretical models that predict not only averaged transcriptional dynamics, but also their variability, to formulate testable hypotheses about the molecular mechanisms underlying transcriptional bursting and control.
Focal-plane wavefront sensing with photonic lanterns I: theoretical framework<|sep|>The photonic lantern (PL) is a tapered waveguide that can efficiently couple light into multiple single-mode optical fibers. Such devices are currently being considered for a number of tasks, including the coupling of telescopes and high-resolution, fiber-fed spectrometers, coherent detection, nulling interferometry, and vortex-fiber nulling (VFN). In conjunction with these use cases, PLs can simultaneously perform low-order focal-plane wavefront sensing. In this work, we provide a mathematical framework for the analysis of the photonic lantern wavefront sensor (PLWFS), deriving linear and higher-order reconstruction models as well as metrics through which sensing performance -- both in the linear and nonlinear regimes -- can be quantified. This framework can be extended to account for additional optics such as beam-shaping optics and vortex masks, and is generalizable to other wavefront sensing architectures. Lastly, we provide initial numerical verification of our mathematical models, by simulating a 6-port PLWFS. In a companion paper, we provide a more comprehensive numerical characterization of few-port PLWFSs, and consider how the sensing properties of these devices can be controlled and optimized.
Soft vortex matter in a type-I/type-II superconducting bilayer<|sep|>Magnetic flux patterns are known to strongly differ in the intermediate state of type-I and type-II superconductors. Using a type-I/type-II bilayer we demonstrate hybridization of these flux phases into a plethora of unique new ones. Owing to a complicated multi-body interaction between individual fluxoids, many different intriguing patterns are possible under applied magnetic field, such as few-vortex clusters, vortex chains, mazes or labyrinthal structures resembling the phenomena readily encountered in soft matter physics. However, in our system the patterns are tunable by sample parameters, magnetic field, current and temperature, which reveals transitions from short-range clustering to long-range ordered phases such as parallel chains, gels, glasses and crystalline vortex lattices, or phases where lamellar type-I flux domains in one layer serve as a bedding potential for type-II vortices in the other - configurations clearly beyond the soft-matter analogy.
Quantum-Reduced Loop Gravity: Cosmology<|sep|>We introduce a new framework for loop quantum gravity: mimicking the spinfoam quantization procedure we propose to study the symmetric sectors of the theory imposing the reduction weakly on the full kinematical Hilbert space of the canonical theory. As a first application of Quantum-Reduced Loop Gravity we study the inhomogeneous Bianchi I model. The emerging quantum cosmological model represents a simplified arena on which the complete canonical quantization program can be tested. The achievements of this analysis could elucidate the relationship between Loop Quantum Cosmology and the full theory.
A Review of Verbal and Non-Verbal Human-Robot Interactive Communication<|sep|>In this paper, an overview of human-robot interactive communication is presented, covering verbal as well as non-verbal aspects of human-robot interaction. Following a historical introduction, and motivation towards fluid human-robot communication, ten desiderata are proposed, which provide an organizational axis both of recent as well as of future research on human-robot communication. Then, the ten desiderata are examined in detail, culminating to a unifying discussion, and a forward-looking conclusion.
An unbiased estimate for the mean of a {0,1} random variable with relative error distribution independent of the mean<|sep|>Say $X_1,X_2,\ldots$ are independent identically distributed Bernoulli random variables with mean $p$. This paper builds a new estimate $\hat p$ of $p$ that has the property that the relative error, $\hat p /p - 1$, of the estimate does not depend in any way on the value of $p$. This allows the construction of exact confidence intervals for $p$ of any desired level without needing any sort of limit or approximation. In addition, $\hat p$ is unbiased. For $\epsilon$ and $\delta$ in $(0,1)$, to obtain an estimate where $\mathbb{P}(|\hat p/p - 1| > \epsilon) \leq \delta$, the new algorithm takes on average at most $2\epsilon^{-2} p^{-1}\ln(2\delta^{-1})(1 - (14/3) \epsilon)^{-1}$ samples. It is also shown that any such algorithm that applies whenever $p \leq 1/2$ requires at least $0.2\epsilon^{-2} p^{-1}\ln((2-\delta)\delta^{-1})(1 + 2 \epsilon)$ samples. The same algorithm can also be applied to estimate the mean of any random variable that falls in $[0,1]$.
A Mixture of Expert Based Deep Neural Network for Improved ASR<|sep|>This paper presents a novel deep learning architecture for acoustic model in the context of Automatic Speech Recognition (ASR), termed as MixNet. Besides the conventional layers, such as fully connected layers in DNN-HMM and memory cells in LSTM-HMM, the model uses two additional layers based on Mixture of Experts (MoE). The first MoE layer operating at the input is based on pre-defined broad phonetic classes and the second layer operating at the penultimate layer is based on automatically learned acoustic classes. In natural speech, overlap in distribution across different acoustic classes is inevitable, which leads to inter-class mis-classification. The ASR accuracy is expected to improve if the conventional architecture of acoustic model is modified to make them more suitable to account for such overlaps. MixNet is developed keeping this in mind. Analysis conducted by means of scatter diagram verifies that MoE indeed improves the separation between classes that translates to better ASR accuracy. Experiments are conducted on a large vocabulary ASR task which show that the proposed architecture provides 13.6% and 10.0% relative reduction in word error rates compared to the conventional models, namely, DNN and LSTM respectively, trained using sMBR criteria. In comparison to an existing method developed for phone-classification (by Eigen et al), our proposed method yields a significant improvement.
Entanglement between two subsystems, the Wigner semicircle and extreme value statistics<|sep|>The entanglement between two arbitrary subsystems of random pure states is studied via properties of the density matrix's partial transpose, $\rho_{12}^{T_2}$. The density of states of $\rho_{12}^{T_2}$ is close to the semicircle law when both subsystems have dimensions which are not too small and are of the same order. A simple random matrix model for the partial transpose is found to capture the entanglement properties well, including a transition across a critical dimension. Log-negativity is used to quantify entanglement between subsystems and analytic formulas for this are derived based on the simple model. The skewness of the eigenvalue density of $\rho_{12}^{T_2}$ is derived analytically, using the average of the third moment over the ensemble of random pure states. The third moment after partial transpose is also shown to be related to a generalization of the Kempe invariant. The smallest eigenvalue after partial transpose is found to follow the extreme value statistics of random matrices, namely the Tracy-Widom distribution. This distribution, with relevant parameters obtained from the model, is found to be useful in calculating the fraction of entangled states at critical dimensions. These results are tested in a quantum dynamical system of three coupled standard maps, where one finds that if the parameters represent a strongly chaotic system, the results are close to those of random states, although there are some systematic deviations at critical dimensions.
On Exploiting Transaction Concurrency To Speed Up Blockchains<|sep|>Consensus protocols are currently the bottlenecks that prevent blockchain systems from scaling. However, we argue that transaction execution is also important to the performance and security of blockchains. In other words, there are ample opportunities to speed up and further secure blockchains by reducing the cost of transaction execution. Our goal is to understand how much we can speed up blockchains by exploiting transaction concurrency available in blockchain workloads. To this end, we first analyze historical data of seven major public blockchains, namely Bitcoin, Bitcoin Cash, Litecoin, Dogecoin, Ethereum, Ethereum Classic, and Zilliqa. We consider two metrics for concurrency, namely the single-transaction conflict rate per block, and the group conflict rate per block. We find that there is more concurrency in UTXO-based blockchains than in account-based ones, although the amount of concurrency in the former is lower than expected. Another interesting finding is that some blockchains with larger blocks have more concurrency than blockchains with smaller blocks. Next, we propose an analytical model for estimating the transaction execution speed-up given an amount of concurrency. Using results from our empirical analysis, the model estimates that 6x speed-ups in Ethereum can be achieved if all available concurrency is exploited.
Generating Gender Augmented Data for NLP<|sep|>Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of bias becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable rewriting approach for short conversational sentences. The rewriting method can be applied to sentences that, without extra-sentential context, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation (NMT) system trained to 'translate' from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results for automatic generation of gender alternatives for conversational sentences in Spanish.
Post-selection inference for e-value based confidence intervals<|sep|>Suppose that one can construct a valid $(1-\delta)$-confidence interval (CI) for each of $K$ parameters of potential interest. If a data analyst uses an arbitrary data-dependent criterion to select some subset $\mathcal{S}$ of parameters, then the aforementioned CIs for the selected parameters are no longer valid due to selection bias. We design a new method to adjust the intervals in order to control the false coverage rate (FCR). The main established method is the "BY procedure" by Benjamini and Yekutieli (JASA, 2005). Unfortunately, the BY guarantees require certain restrictions on the the selection criterion and on the dependence between the CIs. We propose a natural and much simpler method which is valid under any dependence structure between the original CIs, and any (unknown) selection criterion, but which only applies to a special, yet broad, class of CIs. Our procedure reports $(1-\delta|\mathcal{S}|/K)$-CIs for the selected parameters, and we prove that it controls the FCR at $\delta$ for confidence intervals that implicitly invert e-values; examples include those constructed via supermartingale methods, or via universal inference, or via Chernoff-style bounds on the moment generating function, among others. The e-BY procedure is admissible, and recovers the BY procedure as a special case via calibration. Our work also has implications for post-selection inference in sequential settings, since it applies at stopping times, to continuously-monitored confidence sequences, and under bandit sampling.
ODMTCNet: An Interpretable Multi-view Deep Neural Network Architecture for Image Feature Representation<|sep|>This work proposes an interpretable multi-view deep neural network architecture, namely optimal discriminant multi-view tensor convolutional network (ODMTCNet), by integrating statistical machine learning (SML) principles with the deep neural network (DNN) architecture.
Resonant Atom Traps for Electromagnetic Waves<|sep|>Exitation of atomic levels due to interaction with electromagnetic waves has been the subject of numerous works, both experimental and theoretical. This topic became of interest in accelerator physics in relation to high efficiency charge exchange injection into rings for high beam power applications. Taking equations of resonant atom-wave interaction equations as a basis, this paper shows that there exist some interesting phenomena which lead to the existence of trapped electomagnetic waves (photon traps) in a medium that consists of atoms with transition frequencies in proximity to the wave frequency. These traps may exist in random and periodic lattices, and may have very low loss rate. The atomic medium can serve as an excellent wavegiude or tool to form and transmit electromagnetic waves for applications to accelerators and to electromagnetic devices in general, where high pressure gas use is acceptable. In addition, such traps in gases may accumulate substantial energy for a long period of time, leading to the possibility of creating objects similar (or equivalent) to ball lightning.
Relaxation of electrons in quantum-confined states in Pb/Si(111) thin films from master equation with first-principles-derived rates<|sep|>Atomically thin films of Pb on Si(111) provide an experimentally tunable system comprising a highly structured electronic density of states. The lifetime of excited electrons in these states is limited by both electron-electron (e-e) and electron-phonon (e-ph) scattering. We employ the description by a master equation for the electronic occupation numbers to analyze the relative importance of both scattering mechanisms. The electronic and phononic band structures, as well as the matrix elements for electron-phonon coupling within deformation potential theory were obtained from density functional calculations, thus taking into account quantum confinement effects. For the relaxation dynamics, the contribution of impact ionization processes to the lifetime is estimated from the imaginary part of the electronic self-energy calculated in the GW approximation. By numerically solving rate equations for the occupations of the Pb-derived electronic states coupled to a phononic heat bath, we are able to follow the distribution of the electronic excitation energy to the various modes of Pb lattice vibrations. While e-e scattering is the dominant relaxation mechanism, we demonstrate that the e-ph scattering is highly phonon-mode-specific, with a large contribution from surface phonons. At electron energies of about 0.3 eV above the Fermi surface, a 'phonon bottleneck' characteristic of relaxation in nanostructures with well-separated electronic states is observed. The time scales extracted from the simulations are compared to data from pump-probe experiments using time-resolved two-photon photoemission.
Chemical abundance analysis of symbiotic giants. I. RW Hya and SY Mus<|sep|>The study of symbiotic systems is of considerable importance in our understanding of binary system stellar evolution in systems where mass loss or transfer takes place. Elemental abundances are of special significance since they can be used to track mass exchange. However, there are few symbiotic giants for which the abundances are fairly well determined. Here we present for the first time a detailed analysis of the chemical composition for the giants in the RW Hya and SY Mus systems. The analysis is based on high resolution (R 50000), high S/N, near-IR spectra. Spectrum synthesis employing standard LTE analysis and atmosphere models was used to obtain photospheric abundances of CNO and elements around the iron peak (Sc, Ti, Fe, and Ni). Our analysis reveals a significantly sub-solar metallicity, [Fe/H]-0.75}, for the RW Hya giant confirming its membership in the Galactic halo population and a near-solar metallicity for the SY Mus giant. The very low 12C/13C isotopic ratios, 6-10, derived for both objects indicate that the giants have experienced the first dredge-up
Investigation of the Spatially Dependent Charge Collection Probability in CuInS$_2$/ZnO Colloidal Nanocrystal Solar Cells<|sep|>Solar cells with a heterojunction between colloidal CuInS$_2$ and ZnO nanocrystals are an innovative concept in solution-processed photovoltaics, but the conversion efficiency cannot compete yet with devices employing lead chalcogenide quantum dots. Here, we present a detailed study on the charge collection in CuInS$_2$/ZnO solar cells. An inverted device architecture was utilized, in which the ZnO played an additional role as optical spacer layer. Variations of the ZnO thickness were exploited to create different charge generation profiles within the light-harvesting CuInS$_2$ layer, which strongly affected both the external and internal quantum efficiency. By the reconstruction of these experimental findings with the help of a purely optical model, we were able to draw conclusions on the spatial dependency of the charge collection probability. We provide evidence that only carriers generated within a narrow zone of circa 40 nm near the CuInS$_2$/ZnO interface contribute to the external photocurrent. The remaining part of the absorber can be considered as "dead zone" for charge collection, which reasonably explains the limited device performance and indicates a direction for future research. From the methodical point of view, the optical modeling approach developed in the present work has the advantage that no electrical input parameters are required and is believed to be easily transferable to other material systems.
A new Hamiltonian for the Topological BF phase with spinor networks<|sep|>We describe fundamental equations which define the topological ground states in the lattice realization of the SU(2) BF phase. We introduce a new scalar Hamiltonian, based on recent works in quantum gravity and topological models, which is different from the plaquette operator. Its gauge-theoretical content at the classical level is formulated in terms of spinors. The quantization is performed with Schwinger's bosonic operators on the links of the lattice. In the spin network basis, the quantum Hamiltonian yields a difference equation based on the spin 1/2. In the simplest case, it is identified as a recursion on Wigner 6j-symbols. We also study it in different coherent states representations, and compare with other equations which capture some aspects of this topological phase.
Measurement of the time-dependent CP asymmetry in B0 -> J/{\psi} KS0 decays<|sep|>This Letter reports a measurement of the CP violation observables S_J/{\psi}KS0 and C_J/{\psi}KS0 in the decay channel B0 -> J/{\psi} KS0 performed with 1.0/fb of pp collisions at sqrt(s) = 7 TeV collected by the LHCb experiment. The fit to the data yields S_J/{\psi}KS0 = 0.73 +- 0.07 (stat) +- 0.04 (syst) and C_J/{\psi}KS0 = 0.03 +- 0.09 (stat) +- 0.01 (syst). Both values are consistent with the current world averages and within expectations from the Standard Model.
GMRT Radio Halo Survey in galaxy clusters at z = 0.2 -- 0.4. II.The eBCS clusters and analysis of the complete sample<|sep|>We present the results of the GMRT cluster radio halo survey. The main purposes of our observational project are to measure which fraction of massive galaxy clusters in the redshift range z=0.2--0.4 hosts a radio halo, and to constrain the expectations of the particle re--acceleration model for the origin of the non--thermal radio emission. We selected a complete sample of 50 clusters in the X-ray band from the REFLEX (27) and the eBCS (23) catalogues. In this paper we present Giant Metrewave Radio Telescope (GMRT) observations at 610 MHz for all clusters still lacking high sensitivity radio information, i.e. 16 eBCS and 7 REFLEX clusters, thus completing the radio information for the whole sample. The typical sensitivity in our images is in the range 1$\sigma \sim 35-100 \mu$Jy b$^{-1}$. We found a radio halo in A697, a diffuse peripheral source of unclear nature in A781, a core--halo source in Z7160, a candidate radio halo in A1682 and ``suspect'' central emission in Z2661. Including the literature information, a total of 10 clusters in the sample host a radio halo. A very important result of our work is that 25 out of the 34 clusters observed with the GMRT do not host extended central emission at the sensitivity level of our observations, and for 20 of them firm upper limits to the radio power of a giant radio halo were derived. The GMRT Radio Halo Survey shows that radio halos are not common, and our findings on the fraction of giant radio halos in massive clusters are consistent with the statistical expectations based on the re--acceleration model. Our results favour primary to secondary electron models.
Maintainable Log Datasets for Evaluation of Intrusion Detection Systems<|sep|>Intrusion detection systems (IDS) monitor system logs and network traffic to recognize malicious activities in computer networks. Evaluating and comparing IDSs with respect to their detection accuracies is thereby essential for their selection in specific use-cases. Despite a great need, hardly any labeled intrusion detection datasets are publicly available. As a consequence, evaluations are often carried out on datasets from real infrastructures, where analysts cannot control system parameters or generate a reliable ground truth, or private datasets that prevent reproducibility of results. As a solution, we present a collection of maintainable log datasets collected in a testbed representing a small enterprise. Thereby, we employ extensive state machines to simulate normal user behavior and inject a multi-step attack. For scalable testbed deployment, we use concepts from model-driven engineering that enable automatic generation and labeling of an arbitrary number of datasets that comprise repetitions of attack executions with variations of parameters. In total, we provide 8 datasets containing 20 distinct types of log files, of which we label 8 files for 10 unique attack steps. We publish the labeled log datasets and code for testbed setup and simulation online as open-source to enable others to reproduce and extend our results.
On the Use of Policy Iteration as an Easy Way of Pricing American Options<|sep|>In this paper, we demonstrate that policy iteration, introduced in the context of HJB equations in [Forsyth & Labahn, 2007], is an extremely simple generic algorithm for solving linear complementarity problems resulting from the finite difference and finite element approximation of American options. We show that, in general, O(N) is an upper and lower bound on the number of iterations needed to solve a discrete LCP of size N. If embedded in a class of standard discretisations with M time steps, the overall complexity of American option pricing is indeed only O(N(M+N)), and, therefore, for M N, identical to the pricing of European options, which is O(MN). We also discuss the numerical properties and robustness with respect to model parameters in relation to penalty and projected relaxation methods.
Space Efficient Breadth-First and Level Traversals of Consistent Global States of Parallel Programs<|sep|>Enumerating consistent global states of a computation is a fundamental problem in parallel computing with applications to debug- ging, testing and runtime verification of parallel programs. Breadth-first search (BFS) enumeration is especially useful for these applications as it finds an erroneous consistent global state with the least number of events possible. The total number of executed events in a global state is called its rank. BFS also allows enumeration of all global states of a given rank or within a range of ranks. If a computation on n processes has m events per process on average, then the traditional BFS (Cooper-Marzullo and its variants) requires $\mathcal{O}(\frac{m^{n-1}}{n})$ space in the worst case, whereas ou r algorithm performs the BFS requires $\mathcal{O}(m^2n^2)$ space. Thus, we reduce the space complexity for BFS enumeration of consistent global states exponentially. and give the first polynomial space algorithm for this task. In our experimental evaluation of seven benchmarks, traditional BFS fails in many cases by exhausting the 2 GB heap space allowed to the JVM. In contrast, our implementation uses less than 60 MB memory and is also faster in many cases.
Weak Models of Distributed Computing, with Connections to Modal Logic<|sep|>This work presents a classification of weak models of distributed computing. We focus on deterministic distributed algorithms, and study models of computing that are weaker versions of the widely-studied port-numbering model. In the port-numbering model, a node of degree d receives messages through d input ports and sends messages through d output ports, both numbered with 1,2,...,d. In this work, VVc is the class of all graph problems that can be solved in the standard port-numbering model. We study the following subclasses of VVc: VV: Input port i and output port i are not necessarily connected to the same neighbour. MV: Input ports are not numbered; algorithms receive a multiset of messages. SV: Input ports are not numbered; algorithms receive a set of messages. VB: Output ports are not numbered; algorithms send the same message to all output ports. MB: Combination of MV and VB. SB: Combination of SV and VB. Now we have many trivial containment relations, such as SB \subseteq MB \subseteq VB \subseteq VV \subseteq VVc, but it is not obvious if, for example, either of VB \subseteq SV or SV \subseteq VB should hold. Nevertheless, it turns out that we can identify a linear order on these classes. We prove that SB \subsetneq MB = VB \subsetneq SV = MV = VV \subsetneq VVc. The same holds for the constant-time versions of these classes. We also show that the constant-time variants of these classes can be characterised by a corresponding modal logic. Hence the linear order identified in this work has direct implications in the study of the expressibility of modal logic. Conversely, one can use tools from modal logic to study these classes.
Dynamics of a planar thin shell at a Taub-FRW junction<|sep|>We address the problem of stitching together the vacuum, static, planar-symmetric Taub spacetime and the flat Friedmann-Robertson-Walker cosmology using the Israel thin-shell formalism. The joining of Taub and FRW spacetimes is reminiscent of the Oppenheimer-Snyder collapse used in modeling the formation of a singularity from a collapsing spherical ball of dust. A possible mechanism for the formation of a planar singularity is provided. It is hoped that tackling such example will improve our intuition on planar-symmetric systems in Einstein's general relativity.
Fuzzy Logic based Autonomous Parking Systems -- Part I: An Integrated Multi-functional System<|sep|>This paper presents an intelligent autonomous parking system with multi-functions. The integrated system consists of two sub-systems, namely the Fuzzy-Based Onboard System and control center. Unlike most current auto-parking experiments, this FBOS enables the car to perform both slot detection and parking under two different parking modes. Facilitated by the control center, real-time monitoring is also achieved to reduce chances of error.
Indirect Rate-Distortion Function of a Binary i.i.d Source<|sep|>The indirect source-coding problem in which a Bernoulli process is compressed in a lossy manner from its noisy observations is considered. These noisy observations are obtained by passing the source sequence through a The indirect source-coding problem in which a Bernoulli process is compressed in a lossy manner from its noisy observations is considered. These noisy observations are obtained by passing the source sequence through a binary symmetric channel so that the channel crossover probability controls the amount of information available about the source realization at the encoder. We use classic results in rate-distortion theory to compute an expression of the rate-distortion function for this model, where the Bernoulli source is not necessarily symmetric. The indirect rate-distortion function is given in terms of a solution to a simple equation. In addition, we derive an upper bound on the indirect rate-distortion function which is given in a closed. These expressions capture precisely the expected behavior that the noisier the observations, the smaller the return from increasing bit-rate to reduce distortion.
Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs<|sep|>In this paper, we explore the problem of iterative approximate Byzantine consensus in arbitrary directed graphs. In particular, we prove a necessary and sufficient condition for the existence of iterative byzantine consensus algorithms. Additionally, we use our sufficient condition to examine whether such algorithms exist for some specific graphs.
On the Theory of Transfer Learning: The Importance of Task Diversity<|sep|>We provide new statistical guarantees for transfer learning via representation learning--when transfer is achieved by learning a feature representation shared across different tasks. This enables learning on new tasks using far less data than is required to learn them in isolation. Formally, we consider $t+1$ tasks parameterized by functions of the form $f_j \circ h$ in a general function class $\mathcal{F} \circ \mathcal{H}$, where each $f_j$ is a task-specific function in $\mathcal{F}$ and $h$ is the shared representation in $\mathcal{H}$. Letting $C(\cdot)$ denote the complexity measure of the function class, we show that for diverse training tasks (1) the sample complexity needed to learn the shared representation across the first $t$ training tasks scales as $C(\mathcal{H}) + t C(\mathcal{F})$, despite no explicit access to a signal from the feature representation and (2) with an accurate estimate of the representation, the sample complexity needed to learn a new task scales only with $C(\mathcal{F})$. Our results depend upon a new general notion of task diversity--applicable to models with general tasks, features, and losses--as well as a novel chain rule for Gaussian complexities. Finally, we exhibit the utility of our general framework in several models of importance in the literature.
Metallicity of high stellar mass galaxies with signs of merger events<|sep|>We focus on an analysis of galaxies of high stellar mass and low metallicity. We cross-correlated the Millenium Galaxy Catalogue (MGC) and the Sloan Digital Sky Survey (SDSS) galaxy catalogue to provide a sample of MGC objects with high resolution imaging and both spectroscopic and photometric information available in the SDSS database. For each galaxy in our sample, we conducted a systematic morphological analysis by visual inspection of MGC images using their luminosity contours. The galaxies are classified as either disturbed or undisturbed objects. We divide the sample into three metallicity regions, within wich we compare the properties of disturbed and undisturbed objects. We find that the fraction of galaxies that are strongly disturbed, indicative of being merger remnants, is higher when lower metallicity objects are considered. The three bins analysed consist of approximatively 15%, 20%, and 50% disturbed galaxies (for high, medium, and low metallicity, respectively). Moreover, the ratio of the disturbed to undisturbed relative distributions of the population age indicator, Dn(4000), in the low metallicity bin, indicates that the disturbed objects have substantially younger stellar populations than their undisturbed counterparts. In addition, we find that an analysis of colour distributions provides similar results, showing that low metallicity galaxies with a disturbed morphology are bluer than those that are undisturbed. The bluer colours and younger populations of the low metallicity, morphologically disturbed objects suggest that they have experienced a recent merger with an associated enhanced star formation rate. [abridged]
Supervised Domain Adaptation using Graph Embedding<|sep|>Getting deep convolutional neural networks to perform well requires a large amount of training data. When the available labelled data is small, it is often beneficial to use transfer learning to leverage a related larger dataset (source) in order to improve the performance on the small dataset (target). Among the transfer learning approaches, domain adaptation methods assume that distributions between the two domains are shifted and attempt to realign them. In this paper, we consider the domain adaptation problem from the perspective of dimensionality reduction and propose a generic framework based on graph embedding. Instead of solving the generalised eigenvalue problem, we formulate the graph-preserving criterion as a loss in the neural network and learn a domain-invariant feature transformation in an end-to-end fashion. We show that the proposed approach leads to a powerful Domain Adaptation framework; a simple LDA-inspired instantiation of the framework leads to state-of-the-art performance on two of the most widely used Domain Adaptation benchmarks, Office31 and MNIST to USPS datasets.
Elliptical Insights: Understanding Statistical Methods through Elliptical Geometry<|sep|>Visual insights into a wide variety of statistical methods, for both didactic and data analytic purposes, can often be achieved through geometric diagrams and geometrically based statistical graphs. This paper extols and illustrates the virtues of the ellipse and her higher-dimensional cousins for both these purposes in a variety of contexts, including linear models, multivariate linear models and mixed-effect models. We emphasize the strong relationships among statistical methods, matrix-algebraic solutions and geometry that can often be easily understood in terms of ellipses.
Stabilizing Open Quantum Systems by Markovian Reservoir Engineering<|sep|>We study open quantum systems whose evolution is governed by a master equation of Kossakowski-Gorini-Sudarshan-Lindblad type and give a characterization of the convex set of steady states of such systems based on the generalized Bloch representation. It is shown that an isolated steady state of the Bloch equation cannot be a center, i.e., that the existence of a unique steady state implies attractivity and global asymptotic stability. Necessary and sufficient conditions for the existence of a unique steady state are derived and applied to different physical models including two- and four-level atoms, (truncated) harmonic oscillators, composite and decomposable systems. It is shown how these criteria could be exploited in principle for quantum reservoir engineeing via coherent control and direct feedback to stabilize the system to a desired steady state. We also discuss the question of limit points of the dynamics. Despite the non-existence of isolated centers, open quantum systems can have nontrivial invariant sets. These invariant sets are center manifolds that arise when the Bloch superoperator has purely imaginary eigenvalues and are closely related to decoherence-free subspaces.
Differential Cross Section of DP-Elastic Scattering at Intermediate Energies<|sep|>The deuteron-proton elastic scattering is studied in the multiple scattering expansion formalism. The contributions of the one-nucleon-exchange, single- and double scattering are taken into account. The Love and Franey parameterization of the nucleon-nucleon $t$-matrix is used, that gives an opportunity to include the off-energy-shell effects into calculations. Differential cross sections are considered at four energies, $T_d=390, 500, 880, 1200$ MeV. The obtained results are compared with the experimental data.
Sequential Decentralized Parameter Estimation under Randomly Observed Fisher Information<|sep|>We consider the problem of decentralized estimation using wireless sensor networks. Specifically, we propose a novel framework based on level-triggered sampling, a non-uniform sampling strategy, and sequential estimation. The proposed estimator can be used as an asymptotically optimal fixed-sample-size decentralized estimator under non-fading listening channels (through which sensors collect their observations), as an alternative to the one-shot estimators commonly found in the literature. It can also be used as an asymptotically optimal sequential decentralized estimator under fading listening channels. We show that the optimal centralized estimator under Gaussian noise is characterized by two processes, namely the observed Fisher information U_t, and the observed correlation V_t. It is noted that under non-fading listening channels only V_t is random, whereas under fading listening channels both U_t and V_t are random. In the proposed scheme, each sensor computes its local random process(es), and sends a single bit to the fusion center (FC) whenever the local random process(es) pass(es) certain predefined levels. The FC, upon receiving a bit from a sensor, updates its approximation to the corresponding global random process, and accordingly its estimate. The sequential estimation process terminates when the observed Fisher information (or the approximation to it) reaches a target value. We provide an asymptotic analysis for the proposed estimator and also the one based on conventional uniform-in-time sampling under both non-fading and fading channels; and determine the conditions under which they are asymptotically optimal, consistent, and asymptotically unbiased. Analytical results, together with simulation results, demonstrate the superiority of the proposed estimator based on level-triggered sampling over the traditional decentralized estimator based on uniform sampling.
Efficient Data Exchange in Unmanned Aerial Vehicle Networks Utilizing Unsupervised Learning-Based Clustering<|sep|>An unmanned aerial vehicle (UAV) network can serve as an aerial relay to periodically receive packets from macro base stations (BSs). Severe packet loss may happen especially when UAVs have bad wireless connections to a BS. In this paper, a data exchange scheme is proposed utilizing unsupervised learning to enable efficient lost packet retrieval through reliable wireless transmissions between UAVs instead of through retransmissions of macro BSs with a longer delay and higher overhead. With the proposed scheme, all UAVs are assigned to multiple clusters and a UAV can only request its lost packets to UAVs in the same cluster. By this way, UAVs in different clusters could carry out their lost packets retrieval processes simultaneously to expedite data exchange. The agglomerative hierarchical clustering, a type of unsupervised learning, is used to conduct clustering, guaranteeing that UAVs clustered together could supply and supplement each other's lost packets. To further enhance data exchange efficiency, a data exchange mechanism is designed, where the priority of UAVs performing data exchange is determined by the number of their lost packets or the number of requested packets that they can provide. The introduced data exchange mechanism can make each request-reply process maximally beneficial to other UAVs' lost packet retrieval in the same cluster. A new random backoff procedure based on the carrier sense multiple access with collision avoidance (CSMA/CA) is designed to support the data exchange mechanism. Finally, simulation studies are conducted to demonstrate the effectiveness and superiority of our proposed data exchange scheme.
Lagrange formalism of memory circuit elements: classical and quantum formulations<|sep|>The general Lagrange-Euler formalism for the three memory circuit elements, namely, memristive, memcapacitive, and meminductive systems, is introduced. In addition, {\it mutual meminductance}, i.e. mutual inductance with a state depending on the past evolution of the system, is defined. The Lagrange-Euler formalism for a general circuit network, the related work-energy theorem, and the generalized Joule's first law are also obtained. Examples of this formalism applied to specific circuits are provided, and the corresponding Hamiltonian and its quantization for the case of non-dissipative elements are discussed. The notion of {\it memory quanta}, the quantum excitations of the memory degrees of freedom, is presented. Specific examples are used to show that the coupling between these quanta and the well-known charge quanta can lead to a splitting of degenerate levels and to other experimentally observable quantum effects.
Asymptotic High Energy Total Cross Sections and Theories with Extra Dimensions<|sep|>The rate at which cross sections grow with energy is sensitive to the presence of extra dimensions in a rather model-independent fashion. We examine how rates would be expected to grow if there are more spatial dimensions than 3 which appear at some energy scale, making connections with black hole physics and string theory. We also review what is known about the corresponding generalization of the Froissart-Martin bound and the experimental status of high energy hadronic cross sections which appear to saturate it up to the experimentally accessible limit of 100 TeV. We discuss how extra dimensions can be searched for in high energy cross section data and find no room for large extra dimensions in present data. Any apparent signatures of extra dimensions at the LHC may have to be interpreted as due to some other form of new physics.
On fits to correlated and auto-correlated data<|sep|>Observables in particle physics and specifically in lattice QCD calculations are often extracted from fits. Standard $\chi^2$ tests require a reliable determination of the covariance matrix and its inverse from correlated and auto-correlated data, a challenging task often leading to close-to-singular estimates. These motivate modifications of the definition of $\chi^2$ such as uncorrelated fits. We show how the goodness-of-fit measured by their p-value can still be estimated robustly for a broad class of such fits.
Investigating signatures of cosmological time dilation in duration measures of prompt gamma-ray burst light curves<|sep|>We study the evolution with redshift of three measures of gamma-ray burst (GRB) duration ($T_{\rm 90}$, $T_{\rm 50}$ and $T_{\rm R45}$) in a fixed rest frame energy band for a sample of 232 Swift/BAT detected GRBs. Binning the data in redshift we demonstrate a trend of increasing duration with increasing redshift that can be modelled with a power-law for all three measures. Comparing redshift defined subsets of rest-frame duration reveals that the observed distributions of these durations are broadly consistent with cosmological time dilation. To ascertain if this is an instrumental effect, a similar analysis of Fermi/GBM data for the 57 bursts detected by both instruments is conducted, but inconclusive due to small number statistics. We then investigate under-populated regions of the duration redshift parameter space. We propose that the lack of low-redshift, long duration GRBs is a physical effect due to the sample being volume limited at such redshifts. However, we also find that the high-redshift, short duration region of parameter space suffers from censorship as any Swift GRB sample is fundamentally defined by trigger criteria determined in the observer frame energy band of Swift/BAT. As a result, we find that the significance of any evidence for cosmological time dilation in our sample of duration measures typically reduces to $<2\sigma$.
Alternating minimal energy methods for linear systems in higher dimensions. Part II: Faster algorithm and application to nonsymmetric systems<|sep|>In this paper we accomplish the development of the fast rank-adaptive solver for tensor-structured symmetric positive definite linear systems in higher dimensions. In [arXiv:1301.6068] this problem is approached by alternating minimization of the energy function, which we combine with steps of the basis expansion in accordance with the steepest descent algorithm. In this paper we combine the same steps in such a way that the resulted algorithm works with one or two neighboring cores at a time. The recurrent interpretation of the algorithm allows to prove the global convergence and to estimate the convergence rate. We also propose several strategies, both rigorous and heuristic, to compute new subspaces for the basis enrichment in a more efficient way. We test the algorithm on a number of high-dimensional problems, including the non-symmetrical Fokker-Planck and chemical master equations, for which the efficiency of the method is not fully supported by the theory. In all examples we observe a convincing fast convergence and high efficiency of the proposed method.
A Portable Diagnostic Device for Cardiac Magnetic Field Mapping<|sep|>In this paper we present a portable magnetocardiography device. The focus of this development was delivering a rapid assessment of chest pain in an emergency department. The aim was therefore to produce an inexpensive device that could be rapidly deployed in a noisy unshielded ward environment. We found that induction coil magnetometers with a coil design optimized for magnetic field mapping possess sufficient sensitivity ($104fT/\sqrt{Hz}$ noise floor at 10Hz) and response ($813fT/\mu V$ at 10Hz) for cycle averaged magnetocardiography and are able to measure depolarisation signals in an unshielded environment. We were unable to observe repolarisation signals to a reasonable fidelity. We present the design of the induction coil sensor array and signal processing routine along with data demonstrating performance in a hospital environment.
Linear-Response Dynamics from the Time-Dependent Gutzwiller Approximation<|sep|>Within a Lagrangian formalism we derive the time-dependent Gutzwiller approximation for general multi-band Hubbard models. Our approach explicitly incorporates the coupling between time-dependent variational parameters and a time-dependent density matrix from which we obtain dynamical correlation functions in the linear response regime. Our results are illustrated for the one-band model where we show that the interacting system can be mapped to an effective problem of fermionic quasiparticles coupled to "doublon" (double occupancy) bosonic fluctuations. The latter have an energy on the scale of the on-site Hubbard repulsion $U$ in the dilute limit but becomes soft at the Brinkman-Rice transition which is shown to be related to an emerging conservation law of doublon charge and the associated gauge invariance. Coupling with the boson mode produces structure in the charge response and we find that a similar structure appears in dynamical mean-field theory.
Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction<|sep|>A number of deep learning based algorithms have been proposed to recover high-quality videos from low-quality compressed ones. Among them, some restore the missing details of each frame via exploring the spatiotemporal information of neighboring frames. However, these methods usually suffer from a narrow temporal scope, thus may miss some useful details from some frames outside the neighboring ones. In this paper, to boost artifact removal, on the one hand, we propose a Recursive Fusion (RF) module to model the temporal dependency within a long temporal range. Specifically, RF utilizes both the current reference frames and the preceding hidden state to conduct better spatiotemporal compensation. On the other hand, we design an efficient and effective Deformable Spatiotemporal Attention (DSTA) module such that the model can pay more effort on restoring the artifact-rich areas like the boundary area of a moving object. Extensive experiments show that our method outperforms the existing ones on the MFQE 2.0 dataset in terms of both fidelity and perceptual effect. Code is available at https://github.com/zhaominyiz/RFDA-PyTorch.
Neutrino masses and cosmology with Lyman-alpha forest power spectrum<|sep|>We present constraints on neutrino masses, the primordial fluctuation spectrum from inflation, and other parameters of the $\Lambda$CDM model, using the one-dimensional Ly$\alpha$-forest power spectrum measured by Palanque-Delabrouille et al. (2013) from SDSS-III/BOSS, complemented by Planck 2015 cosmic microwave background (CMB) data and other cosmological probes. This paper improves on the previous analysis by Palanque-Delabrouille et al. (2015) by using a more powerful set of calibrating hydrodynamical simulations that reduces uncertainties associated with resolution and box size, by adopting a more flexible set of nuisance parameters for describing the evolution of the intergalactic medium, by including additional freedom to account for systematic uncertainties, and by using Planck 2015 constraints in place of Planck 2013. Fitting Ly$\alpha$ data alone leads to cosmological parameters in excellent agreement with the values derived independently from CMB data, except for a weak tension on the scalar index $n_s$. Combining BOSS Ly$\alpha$ with Planck CMB constrains the sum of neutrino masses to $\sum m_\nu < 0.12$ eV (95\% C.L.) including all identified systematic uncertainties, tighter than our previous limit (0.15 eV) and more robust. Adding Ly$\alpha$ data to CMB data reduces the uncertainties on the optical depth to reionization $\tau$, through the correlation of $\tau$ with $\sigma_8$. Similarly, correlations between cosmological parameters help in constraining the tensor-to-scalar ratio of primordial fluctuations $r$. The tension on $n_s$ can be accommodated by allowing for a running ${\mathrm d}n_s/{\mathrm d}\ln k$. Allowing running as a free parameter in the fits does not change the limit on $\sum m_\nu$. We discuss possible interpretations of these results in the context of slow-roll inflation.
Electronic Properties of Carbon Nanostructures<|sep|>The carbon nanostructures are perspective materials for the future applications. This has two reasons: first, the hexagonal atomic structure which enables a high molecular variability by placing different kinds of the defects and second, good electronic properties which can be modified for the purpose of the concrete applications with the help of the defects and of the chemical ingredients. A lot of kinds of the nanostructures was investigated. Here, the properties of less common forms will be examined - the graphitic nanocone and graphitic wormhole.
Orientational instability and spontaneous rotation of active nematic droplets<|sep|>In experiments, an individual chemically active liquid crystal (LC) droplet submerged in the bulk of a surfactant solution may self-propel along a straight, helical, or random trajectory. In this paper, we develop a minimal model capturing all three types of self-propulsion trajectories of a drop in the case of a nematic LC with homeotropic anchoring at LC-fluid interface. We emulate the director field within the drop by a single preferred polarization vector that is subject of two reorientation mechanisms, namely, the internal flow-induced displacement of the hedgehog defect and the droplet's rotation. Within this reduced-order model, the coupling between the nematic ordering of the drop and the surfactant transport is represented by variations of the droplet's interfacial properties with nematic polarization. Our analysis reveals that a novel mode of orientational instability emerges from the competition of the two reorientation mechanisms and is characterized by a spontaneous rotation of the self-propelling drop responsible for helical self-propulsion trajectories. In turn, we also show that random trajectories in isotropic and nematic drops alike stem from the advection-driven transition to chaos. The succession of the different propulsion modes is consistent with experimentally-reported transitions in the shape of droplet trajectories as the drop size is varied.
Stochastic Processes, Slaves and Supersymmetry<|sep|>We extend the work of Tanase-Nicola and Kurchan on the structure of diffusion processes and the associated supersymmetry algebra by examining the responses of a simple statistical system to external disturbances of various kinds. We consider both the stochastic differential equations (SDEs) for the process and the associated diffusion equation. The influence of the disturbances can be understood by augmenting the original SDE with an equation for {\it slave variables}. The evolution of the slave variables describes the behaviour of line elements carried along in the stochastic flow. These line elements together with the associated surface and volume elements constructed from them provide the basis of the supersymmetry properties of the theory. For ease of visualisation, and in order to emphasise a helpful electromagnetic analogy, we work in three dimensions. The results are all generalisable to higher dimensions and can be specialised to one and two dimensions. The electromagnetic analogy is a useful starting point for calculating asymptotic results at low temperature that can be compared with direct numerical evaluations. We also examine the problems that arise in a direct numerical simulation of the stochastic equation together with the slave equations. We pay special attention to the dependence of the slave variable statistics on temperature. We identify in specific models the critical temperature below which the slave variable distribution ceases to have a variance and consider the effect on estimates of susceptibilities.
The Herschel Virgo Cluster Survey: II. Truncated dust disks in HI-deficient spirals<|sep|>By combining Herschel-SPIRE observations obtained as part of the Herschel Virgo Cluster Survey with 21 cm HI data from the literature, we investigate the role of the cluster environment on the dust content of Virgo spiral galaxies.We show for the first time that the extent of the dust disk is significantly reduced in HI-deficient galaxies, following remarkably well the observed 'truncation' of the HI disk. The ratio of the submillimetre-to- optical diameter correlates with the HI-deficiency, suggesting that the cluster environment is able to strip dust as well as gas. These results provide important insights not only into the evolution of cluster galaxies but also into the metal enrichment of the intra-cluster medium.
On the origin of M81 group extended dust emission<|sep|>Galactic cirrus emission at far-infrared wavelengths affects many extragalactic observations. Separating this emission from that associated with extragalactic objects is both important and difficult. In this paper we discuss a particular case, the M81 group, and the identification of diffuse structures prominent in the infrared, but also detected at optical wavelengths. The origin of these structures has previously been controversial, ranging from them being the result of a past interaction between M81 and M82 or due to more local Galactic emission. We show that over of order a few arcminute scales the far-infrared (Herschel 250 &\mu&m) emission correlates spatially very well with a particular narrow velocity (2-3 km/s) component of the Galactic HI. We find no evidence that any of the far-infrared emission associated with these features actually originates in the M81 group. Thus we infer that the associated diffuse optical emission must be due to galactic light back scattered off dust in our galaxy. Ultra-violet observations pick out young stellar associations around M81, but no detectable far-infrared emission. We consider in detail one of the Galactic cirrus features, finding that the far-infrared HI relation breaks down below arc minute scales and that at smaller scales there can be quite large dust temperature variations.
Dark Photons from the Center of the Earth: Smoking-Gun Signals of Dark Matter<|sep|>Dark matter may be charged under dark electromagnetism with a dark photon that kinetically mixes with the Standard Model photon. In this framework, dark matter will collect at the center of the Earth and annihilate into dark photons, which may reach the surface of the Earth and decay into observable particles. We determine the resulting signal rates, including Sommerfeld enhancements, which play an important role in bringing the Earth's dark matter population to their maximal, equilibrium value. For dark matter masses $m_X \sim$ 100 GeV - 10 TeV, dark photon masses $m_{A'} \sim$ MeV - GeV, and kinetic mixing parameters $\varepsilon \sim 10^{-10} - 10^{-8}$, the resulting electrons, muons, photons, and hadrons that point back to the center of the Earth are a smoking-gun signal of dark matter that may be detected by a variety of experiments, including neutrino telescopes, such as IceCube, and space-based cosmic ray detectors, such as Fermi-LAT and AMS. We determine the signal rates and characteristics, and show that large and striking signals---such as parallel muon tracks---are possible in regions of the $(m_{A'}, \varepsilon)$ plane that are not probed by direct detection, accelerator experiments, or astrophysical observations.
Deep Cross-Modality and Resolution Graph Integration for Universal Brain Connectivity Mapping and Augmentation<|sep|>The connectional brain template (CBT) captures the shared traits across all individuals of a given population of brain connectomes, thereby acting as a fingerprint. Estimating a CBT from a population where brain graphs are derived from diverse neuroimaging modalities (e.g., functional and structural) and at different resolutions (i.e., number of nodes) remains a formidable challenge to solve. Such network integration task allows for learning a rich and universal representation of the brain connectivity across varying modalities and resolutions. The resulting CBT can be substantially used to generate entirely new multimodal brain connectomes, which can boost the learning of the downs-stream tasks such as brain state classification. Here, we propose the Multimodal Multiresolution Brain Graph Integrator Network (i.e., M2GraphIntegrator), the first multimodal multiresolution graph integration framework that maps a given connectomic population into a well centered CBT. M2GraphIntegrator first unifies brain graph resolutions by utilizing resolution-specific graph autoencoders. Next, it integrates the resulting fixed-size brain graphs into a universal CBT lying at the center of its population. To preserve the population diversity, we further design a novel clustering-based training sample selection strategy which leverages the most heterogeneous training samples. To ensure the biological soundness of the learned CBT, we propose a topological loss that minimizes the topological gap between the ground-truth brain graphs and the learned CBT. Our experiments show that from a single CBT, one can generate realistic connectomic datasets including brain graphs of varying resolutions and modalities. We further demonstrate that our framework significantly outperforms benchmarks in reconstruction quality, augmentation task, centeredness and topological soundness.
Multipartite entanglement in qubit systems<|sep|>We introduce a potential of multipartite entanglement for a system of n qubits, as the average over all balanced bipartitions of a bipartite entanglement measure, the purity. We study in detail its expression and look for its minimizers, the maximally multipartite entangled states. They have a bipartite entanglement that does not depend on the bipartition and is maximal for all possible bipartitions. We investigate their structure and consider several examples for small n.
SDSSJ143244.91+301435.3 at VLBI: a compact radio galaxy in a narrow-line Seyfert 1<|sep|>We present VLBI observations, carried out with the European Very Long Baseline Interferometry Network (EVN), of SDSSJ143244.91+301435.3, a radio-loud narrow-line Seyfert 1 (RLNLS1) characterized by a steep radio spectrum. The source, compact at Very Large Array (VLA) resolution, is resolved on the milliarcsec scale, showing a central region plus two extended structures. The relatively high brightness temperature of all components (5x10^6-1.3x10^8 K) supports the hypothesis that the radio emission is non-thermal and likely produced by a relativistic jet and/or small radio lobes. The observed radio morphology, the lack of a significant core and the presence of a low frequency (230 MHz) spectral turnover are reminiscent of the Compact Steep Spectrum sources (CSS). However, the linear size of the source (~0.5kpc) measured from the EVN map is lower than the value predicted using the turnover/size relation valid for CSS sources (~6kpc). This discrepancy can be explained by an additional component not detected in our observations, accounting for about a quarter of the total source flux density, combined to projection effects. The low core-dominance of the source (CD<0.29) confirms that SDSSJ143244.91+301435.3 is not a blazar, i.e. the relativistic jet is not pointing towards the observer. This supports the idea that SDSSJ143244.91+301435.3 may belong to the "parent population" of flat-spectrum RLNLS1 and favours the hypothesis of a direct link between RLNLS1 and compact, possibly young, radio galaxies.
Pose Guided Human Image Synthesis with Partially Decoupled GAN<|sep|>Pose Guided Human Image Synthesis (PGHIS) is a challenging task of transforming a human image from the reference pose to a target pose while preserving its style. Most existing methods encode the texture of the whole reference human image into a latent space, and then utilize a decoder to synthesize the image texture of the target pose. However, it is difficult to recover the detailed texture of the whole human image. To alleviate this problem, we propose a method by decoupling the human body into several parts (\eg, hair, face, hands, feet, \etc) and then using each of these parts to guide the synthesis of a realistic image of the person, which preserves the detailed information of the generated images. In addition, we design a multi-head attention-based module for PGHIS. Because most convolutional neural network-based methods have difficulty in modeling long-range dependency due to the convolutional operation, the long-range modeling capability of attention mechanism is more suitable than convolutional neural networks for pose transfer task, especially for sharp pose deformation. Extensive experiments on Market-1501 and DeepFashion datasets reveal that our method almost outperforms other existing state-of-the-art methods in terms of both qualitative and quantitative metrics.
Extracting Conflict-free Information from Multi-labeled Trees<|sep|>A multi-labeled tree, or MUL-tree, is a phylogenetic tree where two or more leaves share a label, e.g., a species name. A MUL-tree can imply multiple conflicting phylogenetic relationships for the same set of taxa, but can also contain conflict-free information that is of interest and yet is not obvious. We define the information content of a MUL-tree T as the set of all conflict-free quartet topologies implied by T, and define the maximal reduced form of T as the smallest tree that can be obtained from T by pruning leaves and contracting edges while retaining the same information content. We show that any two MUL-trees with the same information content exhibit the same reduced form. This introduces an equivalence relation in MUL-trees with potential applications to comparing MUL-trees. We present an efficient algorithm to reduce a MUL-tree to its maximally reduced form and evaluate its performance on empirical datasets in terms of both quality of the reduced tree and the degree of data reduction achieved.
The Smart Mask: Active Closed-Loop Protection against Airborne Pathogens<|sep|>Face masks provide effective, easy-to-use, and low-cost protection against airborne pathogens or infectious agents, including SARS-CoV-2. There is a wide variety of face masks available on the market for various applications, but they are all passive in nature, i.e., simply act as air filters for the nasal passage and/or mouth. In this paper, we present a new "active mask" paradigm, in which the wearable device is equipped with smart sensors and actuators to both detect the presence of airborne pathogens in real time and take appropriate action to mitigate the threat. The proposed approach is based on a closed-loop control system that senses airborne particles of different sizes close to the mask and then makes intelligent decisions to reduce their concentrations. This paper presents a specific implementation of this concept in which the on-board controller determines ambient air quality via a commercial particulate matter sensor, and if necessary activates a piezoelectric actuator that generates a mist spray to load these particles, thus causing them to fall to the ground. The proposed system communicates with the user via a smart phone application that provides various alerts, including notification of the need to recharge and/or decontaminate the mask prior to reuse. The application also enables a user to override the on-board control system and manually control the mist generator if necessary. Experimental results from a functional prototype demonstrate significant reduction in airborne particulate counts near the mask when the active protection system is enabled.
CHIP: CHannel Independence-based Pruning for Compact Neural Networks<|sep|>Filter pruning has been widely used for neural network compression because of its enabled practical acceleration. To date, most of the existing filter pruning works explore the importance of filters via using intra-channel information. In this paper, starting from an inter-channel perspective, we propose to perform efficient filter pruning using Channel Independence, a metric that measures the correlations among different feature maps. The less independent feature map is interpreted as containing less useful information$/$knowledge, and hence its corresponding filter can be pruned without affecting model capacity. We systematically investigate the quantification metric, measuring scheme and sensitiveness$/$reliability of channel independence in the context of filter pruning. Our evaluation results for different models on various datasets show the superior performance of our approach. Notably, on CIFAR-10 dataset our solution can bring $0.90\%$ and $0.94\%$ accuracy increase over baseline ResNet-56 and ResNet-110 models, respectively, and meanwhile the model size and FLOPs are reduced by $42.8\%$ and $47.4\%$ (for ResNet-56) and $48.3\%$ and $52.1\%$ (for ResNet-110), respectively. On ImageNet dataset, our approach can achieve $40.8\%$ and $44.8\%$ storage and computation reductions, respectively, with $0.15\%$ accuracy increase over the baseline ResNet-50 model. The code is available at https://github.com/Eclipsess/CHIP_NeurIPS2021.
The Cost of OSCORE and EDHOC for Constrained Devices<|sep|>Many modern IoT applications rely on the Constrained Application Protocol (CoAP) because of its efficiency and seamless integrability in the existing Internet infrastructure. One of the strategies that CoAP leverages to achieve these characteristics is the usage of proxies. Unfortunately, in order for a proxy to operate, it needs to terminate the (D)TLS channels between clients and servers. Therefore, end-to-end confidentiality, integrity and authenticity of the exchanged data cannot be achieved. In order to overcome this problem, an alternative to (D)TLS was recently proposed by the Internet Engineering Task Force (IETF). This alternative consists of two novel protocols: 1) Object Security for Constrained RESTful Environments (OSCORE) providing authenticated encryption for the payload data and 2) Ephemeral Diffie-Hellman Over COSE (EDHOC) providing the symmetric session keys required for OSCORE. In this paper, we present the design of four firmware libraries for these protocols especially targeted for constrained microcontrollers and their detailed evaluation. More precisely, we present the design of uOSCORE and uEDHOC libraries for regular microcontrollers and uOSCORE-TEE and uEDHOC-TEE libraries for microcontrollers with a Trusted Execution Environment (TEE), such as microcontrollers featuring ARM TrustZone-M. Our firmware design for the later class of devices concerns the fact that attackers may exploit common software vulnerabilities, e.g., buffer overflows in the protocol logic, OS or application to compromise the protocol security. uOSCORE-TEE and uEDHOC-TEE achieve separation of the cryptographic operations and keys from the remainder of the firmware, which could be vulnerable. We present an evaluation of our implementations in terms of RAM/FLASH requirements, execution speed and energy on a broad range of microcontrollers.
Human brain ferritin studied by muon Spin Rotation: a pilot study<|sep|>Muon Spin Rotation is employed to investigate the spin dynamics of ferritin proteins isolated from the brain of an Alzheimer's disease (AD) patient and of a healthy control, using a sample of horse-spleen ferritin as a reference. A model based on the N\'eel theory of superparamagnetism is developed in order to interpret the spin relaxation rate of the muons stopped by the core of the protein. Using this model, our preliminary observations show that ferritins from the healthy control are filled with a mineral compatible with ferrihydrite, while ferritins from the AD patient contain a crystalline phase with a larger magnetocrystalline anisotropy, possibly compatible with magnetite or maghemite.
Dynamical evolution of massive perturbers in realistic multi-component galaxy models I: implementation and validation<|sep|>Galaxies are self-gravitating structures composed by several components encompassing spherical, axial and triaxial symmetry. Although real systems feature heterogeneous components whose properties are intimately connected, semi-analytical approaches often exploit the linearity of the Poisson's equation to represent the potential and mass distribution of a multi-component galaxy as the sum of the individual components. In this work, we expand the semi-analytical framework developed in Bonetti et al. (2020) by including both a detailed implementation of the gravitational potential of exponential disc (modelled with a ${\rm sech}^2$ and an exponential vertical profile) and an accurate prescription for the dynamical friction experienced by massive perturbers in composite galaxy models featuring rotating disc structures. Such improvements allow us to evolve arbitrary orbits either within or outside the galactic disc plane. We validate the results obtained by our numerical model against public semi-analytical codes as well as full N-body simulations, finding that our model is in excellent agreement to the codes it is compared with. The ability to reproduce the relevant physical processes responsible for the evolution of massive perturber orbits and its computational efficiency make our framework perfectly suited for large parameter-space exploration studies.
Constraints on the rate of supernovae lasting for more than a year from Subaru/Hyper Suprime-Cam<|sep|>Some supernovae such as pair-instability supernovae are predicted to have the duration of more than a year in the observer frame. To constrain the rates of supernovae lasting for more than a year, we conducted a long-term deep transient survey using Hyper Suprime-Cam (HSC) on the 8.2m Subaru telescope. HSC is a wide-field (a 1.75 deg2 field-of-view) camera and it can efficiently conduct transient surveys. We observed the same 1.75 deg2 field repeatedly using the g, r, i, and z band filters with the typical depth of 26 mag for 4 seasons (from late 2016 to early 2020). Using these data, we searched for transients lasting for more than a year. Two supernovae were detected in 2 continuous seasons, one supernova was detected in 3 continuous seasons, but no transients lasted for all 4 seasons searched. The discovery rate of supernovae lasting for more than a year with the typical limiting magnitudes of 26 mag is constrained to be 1.4^{+1.3}_{-0.7}(stat.)^{+0.2}_{-0.3}(sys.) events deg-2 yr-1. All the long-lasting supernovae we found are likely Type IIn supernovae and our results indicate that about 40% of Type IIn supernovae have long-lasting light curves. No plausible pair-instability supernova candidates lasting for more than a year are discovered. By comparing the survey results and survey simulations, we constrain the luminous pair-instability supernova rate up to z ~ 3 should be of the order of 100 Gpc-3 yr-1 at most, which is 0.01 - 0.1 per cent of the core-collapse supernova rate.
Microphone array post-filter for separation of simultaneous non-stationary sources<|sep|>Microphone array post-filters have demonstrated their ability to greatly reduce noise at the output of a beamformer. However, current techniques only consider a single source of interest, most of the time assuming stationary background noise. We propose a microphone array post-filter that enhances the signals produced by the separation of simultaneous sources using common source separation algorithms. Our method is based on a loudness-domain optimal spectral estimator and on the assumption that the noise can be described as the sum of a stationary component and of a transient component that is due to leakage between the channels of the initial source separation algorithm. The system is evaluated in the context of mobile robotics and is shown to produce better results than current post-filtering techniques, greatly reducing interference while causing little distortion to the signal of interest, even at very low SNR.
Guided self-assembly of magnetic beads for biomedical applications<|sep|>Micromagnetic beads are widely used in biomedical applications for cell separation, drug delivery, and hypothermia cancer treatment. Here we propose to use self-organized magnetic bead structures which accumulate on fixed magnetic seeding points to isolate circulating tumor cells. The analysis of circulating tumor cells is an emerging tool for cancer biology research and clinical cancer management including the detection, diagnosis and monitoring of cancer. Microfluidic chips for isolating circulating tumor cells use either affinity, size or density capturing methods. We combine multiphysics simulation techniques to understand the microscopic behavior of magnetic beads interacting with Nickel accumulation points used in lab-on-chip technologies. Our proposed chip technology offers the possibility to combine affinity and size capturing with special antibody-coated bead arrangements using a magnetic gradient field created by Neodymium Iron Boron permanent magnets. The multiscale simulation environment combines magnetic field computation, fluid dynamics and discrete particle dynamics.
Domain growth and aging scaling in coarsening disordered systems<|sep|>Using extensive Monte Carlo simulations we study aging properties of two disordered systems quenched below their critical point, namely the two-dimensional random-bond Ising model and the three-dimensional Edwards-Anderson Ising spin glass with a bimodal distribution of the coupling constants. We study the two-times autocorrelation and space-time correlation functions and show that in both systems a simple aging scenario prevails in terms of the scaling variable $L(t)/L(s)$, where $L$ is the time-dependent correlation length, whereas $s$ is the waiting time and $t$ is the observation time. The investigation of the space-time correlation function for the random-bond Ising model allows us to address some issues related to superuniversality.
Thermodynamic properties of the new multiferroic material (NH$_4$)$_2$[FeCl$_5$(H$_2$O)]<|sep|>(NH$_4$)$_2$[FeCl$_5$(H$_2$O)], a member of the family of antiferromagnetic $A_2$[Fe$X_5$(H$_2$O)] compounds ($X$ = halide ion, $A$ = alkali metal or ammonium ion) is classified as a new multiferroic material. We report the onset of ferroelectricity below ~6.9 K within an antiferromagnetically ordered state ($T_N \sim 7.25 K$). The corresponding electric polarization can drastically be influenced by applying magnetic fields. Based on measurements of pyroelectric currents, dielectric constants and magnetization we characterize the magnetoelectric, dielectric and magnetic properties of (NH$_4$)$_2$[FeCl$_5$(H$_2$O)]. Combining these data with measurements of thermal expansion, magnetostriction and specific heat, we derive detailed magnetic field versus temperature phase diagrams. Depending on the direction of the magnetic field up to three different multiferroic phases are identified, which are separated by a magnetically ordered, but non-ferroelectric phase from the paramagnetic phase. Besides these low-temperature transitions, we observe an additional phase transition at ~79 K, which we suspect to be of structural origin.
Robustness Threats of Differential Privacy<|sep|>Differential privacy (DP) is a gold-standard concept of measuring and guaranteeing privacy in data analysis. It is well-known that the cost of adding DP to deep learning model is its accuracy. However, it remains unclear how it affects robustness of the model. Standard neural networks are not robust to different input perturbations: either adversarial attacks or common corruptions. In this paper, we empirically observe an interesting trade-off between privacy and robustness of neural networks. We experimentally demonstrate that networks, trained with DP, in some settings might be even more vulnerable in comparison to non-private versions. To explore this, we extensively study different robustness measurements, including FGSM and PGD adversaries, distance to linear decision boundaries, curvature profile, and performance on a corrupted dataset. Finally, we study how the main ingredients of differentially private neural networks training, such as gradient clipping and noise addition, affect (decrease and increase) the robustness of the model.
An Optimal Fully Distributed Algorithm to Minimize the Resource Consumption of Cloud Applications<|sep|>According to the pay-per-use model adopted in clouds, the more the resources consumed by an application running in a cloud computing environment, the greater the amount of money the owner of the corresponding application will be charged. Therefore, applying intelligent solutions to minimize the resource consumption is of great importance. Because centralized solutions are deemed unsuitable for large-distributed systems or large-scale applications, we propose a fully distributed algorithm (called DRA) to overcome the scalability issues. Specifically, DRA migrates the inter-communicating components of an application, such as processes or virtual machines, close to each other to minimize the total resource consumption. The migration decisions are made in a dynamic way and based only on local information. We prove that DRA achieves convergence and results always in the optimal solution.
Robust Factorization of Real-world Tensor Streams with Patterns, Missing Values, and Outliers<|sep|>Consider multiple seasonal time series being collected in real-time, in the form of a tensor stream. Real-world tensor streams often include missing entries (e.g., due to network disconnection) and at the same time unexpected outliers (e.g., due to system errors). Given such a real-world tensor stream, how can we estimate missing entries and predict future evolution accurately in real-time? In this work, we answer this question by introducing SOFIA, a robust factorization method for real-world tensor streams. In a nutshell, SOFIA smoothly and tightly integrates tensor factorization, outlier removal, and temporal-pattern detection, which naturally reinforce each other. Moreover, SOFIA integrates them in linear time, in an online manner, despite the presence of missing entries. We experimentally show that SOFIA is (a) robust and accurate: yielding up to 76% lower imputation error and 71% lower forecasting error; (b) fast: up to 935X faster than the second-most accurate competitor; and (c) scalable: scaling linearly with the number of new entries per time step.
On the readability of overlap digraphs<|sep|>We introduce the graph parameter readability and study it as a function of the number of vertices in a graph. Given a digraph D, an injective overlap labeling assigns a unique string to each vertex such that there is an arc from x to y if and only if x properly overlaps y. The readability of D is the minimum string length for which an injective overlap labeling exists. In applications that utilize overlap digraphs (e.g., in bioinformatics), readability reflects the length of the strings from which the overlap digraph is constructed. We study the asymptotic behaviour of readability by casting it in purely graph theoretic terms (without any reference to strings). We prove upper and lower bounds on readability for certain graph families and general graphs
Enhancing magic sets with an application to ontological reasoning<|sep|>Magic sets are a Datalog to Datalog rewriting technique to optimize query answering. The rewritten program focuses on a portion of the stable model(s) of the input program which is sufficient to answer the given query. However, the rewriting may introduce new recursive definitions, which can involve even negation and aggregations, and may slow down program evaluation. This paper enhances the magic set technique by preventing the creation of (new) recursive definitions in the rewritten program. It turns out that the new version of magic sets is closed for Datalog programs with stratified negation and aggregations, which is very convenient to obtain efficient computation of the stable model of the rewritten program. Moreover, the rewritten program is further optimized by the elimination of subsumed rules and by the efficient handling of the cases where binding propagation is lost. The research was stimulated by a challenge on the exploitation of Datalog/\textsc{dlv} for efficient reasoning on large ontologies. All proposed techniques have been hence implemented in the \textsc{dlv} system, and tested for ontological reasoning, confirming their effectiveness. Under consideration for publication in Theory and Practice of Logic Programming.
Three family unification in higher dimensional models<|sep|>In orbifold models, gauge, Higgs and the matter fields can be unified in one multiplet from the compactification of higher dimensional supersymmetric gauge theory. We study how three families of chiral fermions can be unified in the gauge multiplet. The bulk gauge interaction includes the Yukawa interactions to generate masses for quarks and leptons after the electroweak symmetry is broken. The bulk Yukawa interaction has global or gauged flavor symmetry originating from the R symmetry or bulk gauge symmetry, and the Yukawa structure is restricted. When the global and gauged flavor symmetries are broken by orbifold compactification, the remaining gauge symmetry which contains the standard model gauge symmetry is restricted. The restrictions from the bulk flavor symmetries can provide explanations of fermion mass hierarchy.
Optics in a nonlinear gravitational plane wave<|sep|>Gravitational waves can act like gravitational lenses, affecting the observed positions, brightnesses, and redshifts of distant objects. Exact expressions for such effects are derived here in general relativity, allowing for arbitrarily-moving sources and observers in the presence of plane-symmetric gravitational waves. At least for freely falling sources and observers, it is shown that the commonly-used predictions of linear perturbation theory can be generically overshadowed by nonlinear effects; even for very weak gravitational waves, higher-order perturbative corrections involve secularly-growing terms which cannot necessarily be neglected when considering observations of sufficiently distant sources. Even on more moderate scales where linear effects remain at least marginally dominant, nonlinear corrections are qualitatively different from their linear counterparts. There is a sense in which they can, for example, mimic the existence of a third type of gravitational wave polarization.
Analytical solution of the cylindrical torsion problem for the relaxed micromorphic continuum and other generalized continua (including full derivations)<|sep|>We solve the St.Venant torsion problem for an infinite cylindrical rod whose behaviour is described by a family of isotropic generalized continua, including the relaxed micromorphic and classical micromorphic model. The results can be used to determine the material parameters of these models. Special attention is given to the possible nonphysical stiffness singularity for a vanishing rod diameter, since slender specimens are in general described as stiffer.
Zero-shot Visual Commonsense Immorality Prediction<|sep|>Artificial intelligence is currently powering diverse real-world applications. These applications have shown promising performance, but raise complicated ethical issues, i.e. how to embed ethics to make AI applications behave morally. One way toward moral AI systems is by imitating human prosocial behavior and encouraging some form of good behavior in systems. However, learning such normative ethics (especially from images) is challenging mainly due to a lack of data and labeling complexity. Here, we propose a model that predicts visual commonsense immorality in a zero-shot manner. We train our model with an ETHICS dataset (a pair of text and morality annotation) via a CLIP-based image-text joint embedding. In a testing phase, the immorality of an unseen image is predicted. We evaluate our model with existing moral/immoral image datasets and show fair prediction performance consistent with human intuitions. Further, we create a visual commonsense immorality benchmark with more general and extensive immoral visual contents. Codes and dataset are available at https://github.com/ku-vai/Zero-shot-Visual-Commonsense-Immorality-Prediction. Note that this paper might contain images and descriptions that are offensive in nature.
X-Vector based voice activity detection for multi-genre broadcast speech-to-text<|sep|>Voice Activity Detection (VAD) is a fundamental preprocessing step in automatic speech recognition. This is especially true within the broadcast industry where a wide variety of audio materials and recording conditions are encountered. Based on previous studies which indicate that xvector embeddings can be applied to a diverse set of audio classification tasks, we investigate the suitability of x-vectors in discriminating speech from noise. We find that the proposed x-vector based VAD system achieves the best reported score in detecting clean speech on AVA-Speech, whilst retaining robust VAD performance in the presence of noise and music. Furthermore, we integrate the x-vector based VAD system into an existing STT pipeline and compare its performance on multiple broadcast datasets against a baseline system with WebRTC VAD. Crucially, our proposed x-vector based VAD improves the accuracy of STT transcription on real-world broadcast audio
Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems<|sep|>Random constraint satisfaction problems (CSP) have been studied extensively using statistical physics techniques. They provide a benchmark to study average case scenarios instead of the worst case one. The interplay between statistical physics of disordered systems and computer science has brought new light into the realm of computational complexity theory, by introducing the notion of clustering of solutions, related to replica symmetry breaking. However, the class of problems in which clustering has been studied often involve discrete degrees of freedom: standard random CSPs are random K-SAT (aka disordered Ising models) or random coloring problems (aka disordered Potts models). In this work we consider instead problems that involve continuous degrees of freedom. The simplest prototype of these problems is the perceptron. Here we discuss in detail the full phase diagram of the model. In the regions of parameter space where the problem is non-convex, leading to multiple disconnected clusters of solutions, the solution is critical at the SAT/UNSAT threshold and lies in the same universality class of the jamming transition of soft spheres. We show how the critical behavior at the satisfiability threshold emerges, and we compute the critical exponents associated to the approach to the transition from both the SAT and UNSAT phase. We conjecture that there is a large universality class of non-convex continuous CSPs whose SAT-UNSAT threshold is described by the same scaling solution.
Credulous Users and Fake News: a Real Case Study on the Propagation in Twitter<|sep|>Recent studies have confirmed a growing trend, especially among youngsters, of using Online Social Media as favourite information platform at the expense of traditional mass media. Indeed, they can easily reach a wide audience at a high speed; but exactly because of this they are the preferred medium for influencing public opinion via so-called fake news. Moreover, there is a general agreement that the main vehicle of fakes news are malicious software robots (bots) that automatically interact with human users. In previous work we have considered the problem of tagging human users in Online Social Networks as credulous users. Specifically, we have considered credulous those users with relatively high number of bot friends when compared to total number of their social friends. We consider this group of users worth of attention because they might have a higher exposure to malicious activities and they may contribute to the spreading of fake information by sharing dubious content. In this work, starting from a dataset of fake news, we investigate the behaviour and the degree of involvement of credulous users in fake news diffusion. The study aims to: (i) fight fake news by considering the content diffused by credulous users; (ii) highlight the relationship between credulous users and fake news spreading; (iii) target fake news detection by focusing on the analysis of specific accounts more exposed to malicious activities of bots. Our first results demonstrate a strong involvement of credulous users in fake news diffusion. This findings are calling for tools that, by performing data streaming on credulous' users actions, enables us to perform targeted fact-checking.
EvoGAN: An Evolutionary Computation Assisted GAN<|sep|>The image synthesis technique is relatively well established which can generate facial images that are indistinguishable even by human beings. However, all of these approaches uses gradients to condition the output, resulting in the outputting the same image with the same input. Also, they can only generate images with basic expression or mimic an expression instead of generating compound expression. In real life, however, human expressions are of great diversity and complexity. In this paper, we propose an evolutionary algorithm (EA) assisted GAN, named EvoGAN, to generate various compound expressions with any accurate target compound expression. EvoGAN uses an EA to search target results in the data distribution learned by GAN. Specifically, we use the Facial Action Coding System (FACS) as the encoding of an EA and use a pre-trained GAN to generate human facial images, and then use a pre-trained classifier to recognize the expression composition of the synthesized images as the fitness function to guide the search of the EA. Combined random searching algorithm, various images with the target expression can be easily sythesized. Quantitative and Qualitative results are presented on several compound expressions, and the experimental results demonstrate the feasibility and the potential of EvoGAN.
Detection of a pair of prominent X-ray cavities in Abell 3847<|sep|>We present results obtained from a detailed analysis of a deep Chandra observation of the bright FR II radio galaxy 3C~444 in Abell~3847 cluster. A pair of huge X-ray cavities are detected along North and South directions from the centre of 3C 444. X-ray and radio images of the cluster reveal peculiar positioning of the cavities and radio bubbles. The radio lobes and X-ray cavities are apparently not spatially coincident and exhibit offsets by ~61 kpc and ~77 kpc from each other along the North and South directions, respectively. Radial temperature and density profiles reveal the presence of a cool core in the cluster. Imaging and spectral studies showed the removal of substantial amount of matter from the core of the cluster by the radio jets. A detailed analysis of the temperature and density profiles showed the presence of a rarely detected elliptical shock in the cluster. Detection of inflating cavities at an average distance of ~55 kpc from the centre implies that the central engine feeds a remarkable amount of radio power (~6.3 X 10^44 erg/s) into the intra-cluster medium over ~10^8 yr, the estimated age of cavity. The cooling luminosity of the cluster was estimated to be ~8.30 X 10^43 erg/s, which confirms that the AGN power is sufficient to quench the cooling. Ratios of mass accretion rate to Eddington and Bondi rates were estimated to be ~0.08 and 3.5 X 10^4, respectively. This indicates that the black hole in the core of the cluster accretes matter through chaotic cold accretion.
Exact calculations of first-passage properties on the pseudofractal scale-free web<|sep|>In this paper, we consider discrete time random walks on the pseudofractal scale-free web (PSFW) and we study analytically the related first passage properties. First, we classify the nodes of the PSFW into different levels and propose a method to derive the generation function of the first passage probability from an arbitrary starting node to the absorbing domain, which is located at one or more nodes of low-level (i.e., nodes with large degree). Then, we calculate exactly the first passage probability, the survival probability, the mean and the variance of first passage time by using the generating functions as a tool. Finally, for some illustrative examples corresponding to given choices of starting node and absorbing domain, we derive exact and explicit results for such first passage properties. The method we propose can as well address the cases where the absorbing domain is located at one or more nodes of high-level on the PSFW, and it can also be used to calculate the first passage properties on other networks with self-similar structure, such as $(u, v)$ flowers and recursive scale-free trees.
Flux: FunctionaL Updates for XML (extended report)<|sep|>XML database query languages have been studied extensively, but XML database updates have received relatively little attention, and pose many challenges to language design. We are developing an XML update language called Flux, which stands for FunctionaL Updates for XML, drawing upon ideas from functional programming languages. In prior work, we have introduced a core language for Flux with a clear operational semantics and a sound, decidable static type system based on regular expression types. Our initial proposal had several limitations. First, it lacked support for recursive types or update procedures. Second, although a high-level source language can easily be translated to the core language, it is difficult to propagate meaningful type errors from the core language back to the source. Third, certain updates are well-formed yet contain path errors, or ``dead'' subexpressions which never do any useful work. It would be useful to detect path errors, since they often represent errors or optimization opportunities. In this paper, we address all three limitations. Specifically, we present an improved, sound type system that handles recursion. We also formalize a source update language and give a translation to the core language that preserves and reflects typability. We also develop a path-error analysis (a form of dead-code analysis) for updates.
Resonant cyclotron scattering in pulsar magnetospheres and its application to isolated neutron stars<|sep|>Resonant cyclotron scattering (RCS) in pulsar magnetospheres is considered. The photon diffusion equation (Kompaneets equation) for RCS is derived. The photon system is modeled three dimensionally. Numerical calculations show that there exist not only up scattering but also down scattering of RCS, depending on the parameter space. RCS's possible applications to the spectra energy distributions of magnetar candidates and radio quiet isolated neutron stars (INSs) are point out. The optical/UV excess of INSs may caused by the down scattering of RCS. The calculations for RX J1856.5-3754 and RX J0720.4-3125 are presented and compared with their observational data. In our model, the INSs are proposed to be normal neutron stars, although the quark star hypothesis is still possible. The low pulsation amplitude of INSs is a natural consequence in the RCS model.
Composite system in noncommutative space and the equivalence principle<|sep|>The motion of a composite system made of N particles is examined in a space with a canonical noncommutative algebra of coordinates. It is found that the coordinates of the center-of-mass position satisfy noncommutative algebra with effective parameter. Therefore, the upper bound of the parameter of noncommutativity is re-examined. We conclude that the weak equivalence principle is violated in the case of a non-uniform gravitational field and propose the condition for the recovery of this principle in noncommutative space. Furthermore, the same condition is derived from the independence of kinetic energy on the composition.
Ghost-vibrational resonance<|sep|>Ghost-stochastic resonance is a noise-induced resonance at a fundamental frequency missing in the input signal. We investigate the effect of a high-frequency, instead of a noise, in a single Duffing oscillator driven by a multi-frequency signal $F(t)= \sum^n_{i=1} f_i \cos(\omega_i + \Delta \omega_0)t$, $\omega_i=(k+i-1)\omega_0$, where $k$ is an integer greater than or equal to two. We show the occurrence of a high-frequency induced resonance at the missing fundamental frequency $\omega_0$. For the case of the two-frequency input signal, we obtain an analytical expression for the amplitude of the periodic component with the missing frequency. We present the influence of the number of forces $n$, the parameter $k$, the frequency $\omega_0$ and the frequency shift $\Delta \omega_0$ on the response amplitude at the frequency $\omega_0$. We also investigate the signal propagation in a network of unidirectionally coupled Duffing oscillators. Finally, we show the enhanced signal propagation in the coupled oscillators in absence of a high-frequency periodic force.
GRB 110530A: Peculiar Broad Bump and Delayed Plateau in Early Optical Afterglows<|sep|>We report our very early optical observations of GRB 110530A and investigate its jet properties together with its X-ray afterglow data. A peculiar broad onset bump followed by a plateau is observed in its early R band afterglow lightcurve. The optical data in the other bands and the X-ray data are well consistent with the temporal feature of the R band lightcurve. Our joint spectral fits of the optical and X-ray data show that they are in the same regime, with a photon index of $\sim 1.70$. The optical and X-ray afterglow lightcurves are well fitted with the standard external shock model by considering a delayed energy injection component. Based on our modeling results, we find that the radiative efficiency of the GRB jet is $\sim 1\%$ and the magnetization parameter of the afterglow jet is $<0.04$ with the derived extremely low $\epsilon_B$ (the fraction of shock energy to magnetic field) of $(1.64\pm 0.25)\times 10^{-6}$. These results indicate that the jet may be matter dominated. Discussion on delayed energy injection from accretion of late fall-back material of its pre-supernova star is also presented.
Deep Learning for Procedural Content Generation<|sep|>Procedural content generation in video games has a long history. Existing procedural content generation methods, such as search-based, solver-based, rule-based and grammar-based methods have been applied to various content types such as levels, maps, character models, and textures. A research field centered on content generation in games has existed for more than a decade. More recently, deep learning has powered a remarkable range of inventions in content production, which are applicable to games. While some cutting-edge deep learning methods are applied on their own, others are applied in combination with more traditional methods, or in an interactive setting. This article surveys the various deep learning methods that have been applied to generate game content directly or indirectly, discusses deep learning methods that could be used for content generation purposes but are rarely used today, and envisages some limitations and potential future directions of deep learning for procedural content generation.
Demonstrating Delay-based Reservoir Computing Using a Compact Photonic Integrated Chip<|sep|>Photonic delay-based reservoir computing (RC) has gained considerable attention lately, as it allows for simple technological implementations of the RC concept that can operate at high speed. In this paper, we discuss a practical, compact and robust implementation of photonic delay-based RC, by integrating a laser and a 5.4cm delay line on an InP photonic integrated circuit. We demonstrate the operation of this chip with 23 nodes at a speed of 0.87GSa/s, showing performances that are similar to previous non-integrated delay-based setups. We also investigate two other post-processing methods to obtain more nodes in the output layer. We show that these methods improve the performance drastically, without compromising the computation speed.
Imprints of feedback in young gasless clusters?<|sep|>We present the results of N-body simulations in which we take the masses, positions and velocities of sink particles from five pairs of hydrodynamical simulations of star formation by Dale et al. (2012, 2013) and evolve them for a further 10Myr. We compare the dynamical evolution of star clusters that formed under the influence of mass-loss driven by photoionization feedback, to the evolution of clusters that formed without feedback. We remove any remaining gas and follow the evolution of structure in the clusters (measured by the Q-parameter), half-mass radius, central density, surface density and the fraction of bound stars. There is little discernible difference in the evolution of clusters that formed with feedback compared to those that formed without. The only clear trend is that all clusters which form without feedback in the hydrodynamical simulations lose any initial structure over 10Myr, whereas some of the clusters which form with feedback retain structure for the duration of the subsequent N-body simulation. This is due to lower initial densities (and hence longer relaxation times) in the clusters from Dale et al. (2012, 2013) which formed with feedback, which prevents dynamical mixing from erasing substructure. However, several other conditions (such as supervirial initial velocities) also preserve substructure, so at a given epoch one would require knowledge of the initial density and virial state of the cluster in order to determine whether star formation in a cluster has been strongly influenced by feedback.
$(O,G)$-granular variable precision fuzzy rough sets based on overlap and grouping functions<|sep|>Since Bustince et al. introduced the concepts of overlap and grouping functions, these two types of aggregation functions have attracted a lot of interest in both theory and applications. In this paper, the depiction of $(O,G)$-granular variable precision fuzzy rough sets ($(O,G)$-GVPFRSs for short) is first given based on overlap and grouping functions. Meanwhile, to work out the approximation operators efficiently, we give another expression of upper and lower approximation operators by means of fuzzy implications and co-implications. Furthermore, starting from the perspective of construction methods, $(O,G)$-GVPFRSs are represented under diverse fuzzy relations. Finally, some conclusions on the granular variable precision fuzzy rough sets (GVPFRSs for short) are extended to $(O,G)$-GVPFRSs under some additional conditions.
Bridging the gap between paired and unpaired medical image translation<|sep|>Medical image translation has the potential to reduce the imaging workload, by removing the need to capture some sequences, and to reduce the annotation burden for developing machine learning methods. GANs have been used successfully to translate images from one domain to another, such as MR to CT. At present, paired data (registered MR and CT images) or extra supervision (e.g. segmentation masks) is needed to learn good translation models. Registering multiple modalities or annotating structures within each of them is a tedious and laborious task. Thus, there is a need to develop improved translation methods for unpaired data. Here, we introduce modified pix2pix models for tasks CT$\rightarrow$MR and MR$\rightarrow$CT, trained with unpaired CT and MR data, and MRCAT pairs generated from the MR scans. The proposed modifications utilize the paired MR and MRCAT images to ensure good alignment between input and translated images, and unpaired CT images ensure the MR$\rightarrow$CT model produces realistic-looking CT and CT$\rightarrow$MR model works well with real CT as input. The proposed pix2pix variants outperform baseline pix2pix, pix2pixHD and CycleGAN in terms of FID and KID, and generate more realistic looking CT and MR translations.
Enhancing the German Transmission Grid Through Dynamic Line Rating<|sep|>The German government recently announced that 80\% of the power supply should come from renewable energy by 2030. One key task lies in reorganizing the transmission system such that power can be transported from sites with good renewable potentials to the load centers. Dynamic Line Rating (DLR), which allows the dynamic calculation of transmission line capacities based on prevailing weather conditions rather than conservative invariant ratings, offers the potential to exploit existing grid capacities better. In this paper, we analyze the effect of DLR on behalf of a detailed power system model of Germany including all of today's extra high voltage transmission lines and substations. The evolving synergies between DLR and an increased wind power generation lead to savings of around 400 million euro per year in the short term and 900 million per year in a scenario for 2030.
Atmospheric gas dynamics in the Perseus cluster observed with Hitomi<|sep|>Extending the earlier measurements reported in Hitomi collaboration (2016, Nature, 535, 117), we examine the atmospheric gas motions within the central 100~kpc of the Perseus cluster using observations obtained with the Hitomi satellite. After correcting for the point spread function of the telescope and using optically thin emission lines, we find that the line-of-sight velocity dispersion of the hot gas is remarkably low and mostly uniform. The velocity dispersion reaches maxima of approximately 200~km~s$^{-1}$ toward the central active galactic nucleus (AGN) and toward the AGN inflated north-western `ghost' bubble. Elsewhere within the observed region, the velocity dispersion appears constant around 100~km~s$^{-1}$. We also detect a velocity gradient with a 100~km~s$^{-1}$ amplitude across the cluster core, consistent with large-scale sloshing of the core gas. If the observed gas motions are isotropic, the kinetic pressure support is less than 10\% of the thermal pressure support in the cluster core. The well-resolved optically thin emission lines have Gaussian shapes, indicating that the turbulent driving scale is likely below 100~kpc, which is consistent with the size of the AGN jet inflated bubbles. We also report the first measurement of the ion temperature in the intracluster medium, which we find to be consistent with the electron temperature. In addition, we present a new measurement of the redshift to the brightest cluster galaxy NGC~1275.
Bounded Cohomology of Groups acting on Cantor sets<|sep|>We study the bounded cohomology of certain groups acting on the Cantor set. More specifically, we consider the full group of homeomorphisms of the Cantor set as well as Thompson's group $V$. We prove that both of these groups are boundedly acyclic, that is the bounded cohomology with trivial real coefficients vanishes in positive degrees. Combining this result with the already established $\mathbb{Z}$-acyclicity of Thompson's group $V$, will make $V$ the first example of a finitely generated group, in fact the first example of a group of type $F_\infty$, which is universally boundedly acyclic. Before proving bounded acyclicity, we gather various properties of the groups under consideration and certain subgroups thereof. As a consequence the proofs of bounded acyclicity will be relatively short. It will turn out that the approaches to handle these groups are very similar. This suggests that there could be a unifying approach which would imply the bounded acyclicity of a larger class of groups acting on the Cantor set, including the discussed ones.
Protein Function Prediction Based on Kernel Logistic Regression with 2-order Graphic Neighbor Information<|sep|>To enhance the accuracy of protein-protein interaction function prediction, a 2-order graphic neighbor information feature extraction method based on undirected simple graph is proposed in this paper, which extends the 1-order graphic neighbor featureextraction method. And the chi-square test statistical method is also involved in feature combination. To demonstrate the effectiveness of our 2-order graphic neighbor feature, four logistic regression models (logistic regression (abbrev. LR), diffusion kernel logistic regression (abbrev. DKLR), polynomial kernel logistic regression (abbrev. PKLR), and radial basis function (RBF) based kernel logistic regression (abbrev. RBF KLR)) are investigated on the two feature sets. The experimental results of protein function prediction of Yeast Proteome Database (YPD) using the the protein-protein interaction data of Munich Information Center for Protein Sequences (MIPS) show that 2-order graphic neighbor information of proteins can significantly improve the average overall percentage of protein function prediction especially with RBF KLR. And, with a new 5-top chi-square feature combination method, RBF KLR can achieve 99.05% average overall percentage on 2-order neighbor feature combination set.
Learning Permutations with Sinkhorn Policy Gradient<|sep|>Many problems at the intersection of combinatorics and computer science require solving for a permutation that optimally matches, ranks, or sorts some data. These problems usually have a task-specific, often non-differentiable objective function that data-driven algorithms can use as a learning signal. In this paper, we propose the Sinkhorn Policy Gradient (SPG) algorithm for learning policies on permutation matrices. The actor-critic neural network architecture we introduce for SPG uniquely decouples representation learning of the state space from the highly-structured action space of permutations with a temperature-controlled Sinkhorn layer. The Sinkhorn layer produces continuous relaxations of permutation matrices so that the actor-critic architecture can be trained end-to-end. Our empirical results show that agents trained with SPG can perform competitively on sorting, the Euclidean TSP, and matching tasks. We also observe that SPG is significantly more data efficient at the matching task than the baseline methods, which indicates that SPG is conducive to learning representations that are useful for reasoning about permutations.
Testing the Manifold Hypothesis<|sep|>The hypothesis that high dimensional data tend to lie in the vicinity of a low dimensional manifold is the basis of manifold learning. The goal of this paper is to develop an algorithm (with accompanying complexity guarantees) for fitting a manifold to an unknown probability distribution supported in a separable Hilbert space, only using i.i.d samples from that distribution. More precisely, our setting is the following. Suppose that data are drawn independently at random from a probability distribution $P$ supported on the unit ball of a separable Hilbert space $H$. Let $G(d, V, \tau)$ be the set of submanifolds of the unit ball of $H$ whose volume is at most $V$ and reach (which is the supremum of all $r$ such that any point at a distance less than $r$ has a unique nearest point on the manifold) is at least $\tau$. Let $L(M, P)$ denote mean-squared distance of a random point from the probability distribution $P$ to $M$. We obtain an algorithm that tests the manifold hypothesis in the following sense. The algorithm takes i.i.d random samples from $P$ as input, and determines which of the following two is true (at least one must be): (a) There exists $M \in G(d, CV, \frac{\tau}{C})$ such that $L(M, P) \leq C \epsilon.$ (b) There exists no $M \in G(d, V/C, C\tau)$ such that $L(M, P) \leq \frac{\epsilon}{C}.$ The answer is correct with probability at least $1-\delta$.
Communication-Efficient Network-Distributed Optimization with Differential-Coded Compressors<|sep|>Network-distributed optimization has attracted significant attention in recent years due to its ever-increasing applications. However, the classic decentralized gradient descent (DGD) algorithm is communication-inefficient for large-scale and high-dimensional network-distributed optimization problems. To address this challenge, many compressed DGD-based algorithms have been proposed. However, most of the existing works have high complexity and assume compressors with bounded noise power. To overcome these limitations, in this paper, we propose a new differential-coded compressed DGD (DC-DGD) algorithm. The key features of DC-DGD include: i) DC-DGD works with general SNR-constrained compressors, relaxing the bounded noise power assumption; ii) The differential-coded design entails the same convergence rate as the original DGD algorithm; and iii) DC-DGD has the same low-complexity structure as the original DGD due to a {\em self-noise-reduction effect}. Moreover, the above features inspire us to develop a hybrid compression scheme that offers a systematic mechanism to minimize the communication cost. Finally, we conduct extensive experiments to verify the efficacy of the proposed DC-DGD and hybrid compressor.
Dialectics of Knowledge Representation in a Granular Rough Set Theory<|sep|>The concepts of rough and definite objects are relatively more determinate than those of granules and granulation in general rough set theory (RST) [1]. Representation of rough objects can however depend on the dialectical relation between granulation and definiteness. In this research, we make this exact in the context of RST over proto-transitive approximation spaces. This approach can be directly extended to many other types of RST. These are used for formulating an extended concept of knowledge interpretation (KI)(relative the situation for classical RST) and the problem of knowledge representation (KR) is solved. These will be of direct interest in granular KR in RST as developed by the present author [2] and of rough objects in general. In [3], these have already been used for five different semantics by the present author. This is an extended version of [4] with key examples and more results.
Characterizing (Un)moderated Textual Data in Social Systems<|sep|>Despite the valuable social interactions that online media promote, these systems provide space for speech that would be potentially detrimental to different groups of people. The moderation of content imposed by many social media has motivated the emergence of a new social system for free speech named Gab, which lacks moderation of content. This article characterizes and compares moderated textual data from Twitter with a set of unmoderated data from Gab. In particular, we analyze distinguishing characteristics of moderated and unmoderated content in terms of linguistic features, evaluate hate speech and its different forms in both environments. Our work shows that unmoderated content presents different psycholinguistic features, more negative sentiment and higher toxicity. Our findings support that unmoderated environments may have proportionally more online hate speech. We hope our analysis and findings contribute to the debate about hate speech and benefit systems aiming at deploying hate speech detection approaches.
Direct QR factorizations for tall-and-skinny matrices in MapReduce architectures<|sep|>The QR factorization and the SVD are two fundamental matrix decompositions with applications throughout scientific computing and data analysis. For matrices with many more rows than columns, so-called "tall-and-skinny matrices," there is a numerically stable, efficient, communication-avoiding algorithm for computing the QR factorization. It has been used in traditional high performance computing and grid computing environments. For MapReduce environments, existing methods to compute the QR decomposition use a numerically unstable approach that relies on indirectly computing the Q factor. In the best case, these methods require only two passes over the data. In this paper, we describe how to compute a stable tall-and-skinny QR factorization on a MapReduce architecture in only slightly more than 2 passes over the data. We can compute the SVD with only a small change and no difference in performance. We present a performance comparison between our new direct TSQR method, a standard unstable implementation for MapReduce (Cholesky QR), and the classic stable algorithm implemented for MapReduce (Householder QR). We find that our new stable method has a large performance advantage over the Householder QR method. This holds both in a theoretical performance model as well as in an actual implementation.
Gamma-ray Flaring Emission in Blazar OJ287 Located in the Jet >14 pc from the Black Hole<|sep|>We combine the Fermi-LAT light curve of the BL Lacertae type blazar OJ287 with time-dependent multi-waveband flux and linear polarization observations and submilliarcsecond-scale polarimetric images at lambda=7mm to locate the gamma-ray emission in prominent flares in the jet of the source >14pc from the central engine. We demonstrate a highly significant correlation between the strongest gamma-ray and millimeter-wave flares through Monte Carlo simulations. The two reported gamma-ray peaks occurred near the beginning of two major millimeter-wave outbursts, each of which is associated with a linear polarization maximum at millimeter wavelengths. Our very long baseline array observations indicate that the two millimeter-wave flares originated in the second of two features in the jet that are separated by >14pc. The simultaneity of the peak of the higher-amplitude gamma-ray flare and the maximum in polarization of the second jet feature implies that the gamma-ray and millimeter-wave flares are cospatial and occur >14pc from the central engine. We also associate two optical flares, accompanied by sharp polarization peaks, with the two gamma-ray events. The multi-waveband behavior is most easily explained if the gamma-rays arise from synchrotron self Compton scattering of optical photons from the flares. We propose that flares are triggered by interaction of moving plasma blobs with a standing shock.
Evidence for Warped Disks of Young Stars in the Galactic Center<|sep|>The central parsec around the super-massive black hole in the Galactic Center hosts more than 100 young and massive stars. Outside the central cusp (R~1") the majority of these O and Wolf-Rayet (WR) stars reside in a main clockwise system, plus a second, less prominent disk or streamer system at large angles with respect to the main system. Here we present the results from new observations of the Galactic Center with the AO-assisted near-infrared imager NACO and the integral field spectrograph SINFONI on the ESO/VLT. These include the detection of 27 new reliably measured WR/O stars in the central 12" and improved measurements of 63 previously detected stars, with proper motion uncertainties reduced by a factor of four compared to our earlier work. We develop a detailed statistical analysis of their orbital properties and orientations. Half of the WR/O stars are compatible with being members of a clockwise rotating system. The rotation axis of this system shows a strong transition as a function of the projected distance from SgrA*. The main clockwise system either is either a strongly warped single disk with a thickness of about 10 degrees, or consists of a series of streamers with significant radial variation in their orbital planes. 11 out of 61 clockwise moving stars have an angular separation of more than 30 degrees from the clockwise system. The mean eccentricity of the clockwise system is 0.36+/-0.06. The distribution of the counter-clockwise WR/O star is not isotropic at the 98% confidence level. It is compatible with a coherent structure such as stellar filaments, streams, small clusters or possibly a disk in a dissolving state. The observed disk warp and the steep surface density distribution favor in situ star formation in gaseous accretion disks as the origin of the young stars.
Quasinormal mode spectra for odd parity perturbations in spacetimes with smeared matter sources<|sep|>We have found the Quasi Normal Mode (QNM) frequencies of a class of static spherically symmetric spacetimes having a {\it {smeared}} matter distribution, parameterized by $\Theta$ - an inherent length scale. Here our main focus is on the QNMs for the odd parity perturbation in this background geometry. The results presented here for diffused mass distribution reveal significant changes in the QNM spectrum. This could be relevant for future generation (cosmological) observations, specifically to distinguish the signals of GW from a non-singular source in contrast to a singular geometry. We also provide numerical estimates for the $\Theta$-corrected QNM spectrum applicable to typical globular cluster like spherical galaxies having a Gaussian spread in their mass distribution. We find that the $\Theta$-correction to the GW signal due to smeared distribution is accessible to present day observational precision.
Magnetic properties and domain structure of ultrathin yttrium iron garnet/Pt bilayers<|sep|>We report on the structure, magnetization, magnetic anisotropy, and domain morphology of ultrathin yttrium iron garnet (YIG)/Pt films with thickness ranging from 3 to 90 nm. We find that the saturation magnetization is close to the bulk value in the thickest films and decreases towards low thickness with a strong reduction below 10 nm. We characterize the magnetic anisotropy by measuring the transverse spin Hall magnetoresistance as a function of applied field. Our results reveal strong easy plane anisotropy fields of the order of 50-100 mT, which add to the demagnetizing field, as well as weaker in-plane uniaxial anisotropy ranging from 10 to 100 $\mu$T. The in-plane easy axis direction changes with thickness, but presents also significant fluctuations among samples with the same thickness grown on the same substrate. X-ray photoelectron emission microscopy reveals the formation of zigzag magnetic domains in YIG films thicker than 10 nm, which have dimensions larger than several 100 $\mu$m and are separated by achiral N\'{e}el-type domain walls. Smaller domains characterized by interspersed elongated features are found in YIG films thinner than 10 nm.
An Edge-Cloud Integrated Framework for Flexible and Dynamic Stream Analytics<|sep|>With the popularity of Internet of Things (IoT), edge computing and cloud computing, more and more stream analytics applications are being developed including real-time trend prediction and object detection on top of IoT sensing data. One popular type of stream analytics is the recurrent neural network (RNN) deep learning model based time series or sequence data prediction and forecasting. Different from traditional analytics that assumes data are available ahead of time and will not change, stream analytics deals with data that are being generated continuously and data trend/distribution could change (a.k.a. concept drift), which will cause prediction/forecasting accuracy to drop over time. One other challenge is to find the best resource provisioning for stream analytics to achieve good overall latency. In this paper, we study how to best leverage edge and cloud resources to achieve better accuracy and latency for stream analytics using a type of RNN model called long short-term memory (LSTM). We propose a novel edge-cloud integrated framework for hybrid stream analytics that supports low latency inference on the edge and high capacity training on the cloud. To achieve flexible deployment, we study different approaches of deploying our hybrid learning framework including edge-centric, cloud-centric and edge-cloud integrated. Further, our hybrid learning framework can dynamically combine inference results from an LSTM model pre-trained based on historical data and another LSTM model re-trained periodically based on the most recent data. Using real-world and simulated stream datasets, our experiments show the proposed edge-cloud deployment is the best among all three deployment types in terms of latency. For accuracy, the experiments show our dynamic learning approach performs the best among all learning approaches for all three concept drift scenarios.
Arctic curves of the six-vertex model on generic domains: the Tangent Method<|sep|>We revisit the problem of determining the Arctic curve in the six-vertex model with domain wall boundary conditions. We describe an alternative method, by which we recover the previously conjectured analytic expression in the square domain. We adapt the method to work for a large class of domains, and for other models exhibiting limit shape phenomena. We study in detail some examples, and derive, in particular, the Arctic curve of the six-vertex model in a triangoloid domain at the ice-point.
How Does Metallicity Affect the Gas and Dust Properties of Galaxies?<|sep|>Comparison of the ISM properties of a wide range of metal-poor galaxies with normal metal-rich galaxies reveals striking differences. We find that the combination of the low dust abundance and the active star formation results in a very porous ISM filled with hard photons, heating the dust in dwarf galaxies to overall higher temperatures than their metal-rich counterparts. This results in photodissociation of molecular clouds to greater depths, leaving relatively large PDR envelopes and difficult-to-detect CO cores. From detailed modeling of the low-metallicity ISM, we find significant fractions of CO-dark H2 - a reservoir of molecular gas not traced by CO, but present in the [CII] and [CI]-emitting envelopes. Self-consistent analyses of the neutral and ionized gas diagnostics along with the dust SED is the necessary way forward in uncovering the multiphase structure of galaxies
segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection<|sep|>In this paper, we propose an approach that exploits object segmentation in order to improve the accuracy of object detection. We frame the problem as inference in a Markov Random Field, in which each detection hypothesis scores object appearance as well as contextual information using Convolutional Neural Networks, and allows the hypothesis to choose and score a segment out of a large pool of accurate object segmentation proposals. This enables the detector to incorporate additional evidence when it is available and thus results in more accurate detections. Our experiments show an improvement of 4.1% in mAP over the R-CNN baseline on PASCAL VOC 2010, and 3.4% over the current state-of-the-art, demonstrating the power of our approach.
Massive Black Hole Binaries from the TNG50-3 Simulation: II. Using Dual AGNs to Predict the Rate of Black Hole Mergers<|sep|>Dual active galaxy nuclei (dAGNs) trace the population of post-merger galaxies and are the precursors to massive black hole (MBH) mergers, an important source of gravitational waves that may be observed by LISA. In Paper I of this series, we used the population of nearly 2000 galaxy mergers predicted by the TNG50-3 simulation to seed semi-analytic models of the orbital evolution and coalescence of MBH pairs with initial separations of about 1 kpc. Here, we calculate the dAGN luminosities and separation of these pairs as they evolve in post-merger galaxies, and show how the coalescence fraction of dAGNs changes with redshift. We find that because of the several Gyr long dynamical friction timescale for orbital evolution, the fraction of dAGNs that eventually end in a MBH merger grows with redshift and does not pass 50% until a redshift of 1. However, dAGNs in galaxies with bulge masses >10^10 solar masses, or comprised of near-equal mass MBHs, evolve more quickly and have higher than average coalescence fractions. At any redshift, dAGNs observed with small separations (> 0.7 kpc) have a higher probability of merging in a Hubble time than more widely separated systems. As found in Paper I, radiation feedback effects can significantly reduce the number of MBH mergers, and this could be manifested as a larger than expected number of widely separated dAGNs. We present a method to estimate the MBH coalescence rate as well as the potential LISA detection rate given a survey of dAGNs. Comparing these rates to the eventual LISA measurements will help determine the efficiency of dynamical friction in post-merger galaxies.
The Earliest Phases of Star Formation (EPoS): A Herschel Key Program - The precursors to high-mass stars and clusters<|sep|>(Abridged) We present an overview of the sample of high-mass star and cluster forming regions observed as part of the Earliest Phases of Star Formation (EPoS) Herschel Guaranteed Time Key Program. A sample of 45 infrared-dark clouds (IRDCs) were mapped at PACS 70, 100, and 160 micron and SPIRE 250, 350, and 500 micron. In this paper, we characterize a population of cores which appear in the PACS bands and place them into context with their host cloud and investigate their evolutionary stage. We construct spectral energy distributions (SEDs) of 496 cores which appear in all PACS bands, 34% of which lack counterparts at 24 micron. From single-temperature modified blackbody fits of the SEDs, we derive the temperature, luminosity, and mass of each core. These properties predominantly reflect the conditions in the cold, outer regions. Taking into account optical depth effects and performing simple radiative transfer models, we explore the origin of emission at PACS wavelengths. The core population has a median temperature of 20K and has masses and luminosities that span four to five orders of magnitude. Cores with a counterpart at 24 micron are warmer and bluer on average than cores without a 24 micron counterpart. We conclude that cores bright at 24 micron are on average more advanced in their evolution, where a central protostar(s) have heated the outer bulk of the core, than 24 micron-dark cores. The 24 micron emission itself can arise in instances where our line of sight aligns with an exposed part of the warm inner core. About 10% of the total cloud mass is found in a given cloud's core population. We uncover over 300 further candidate cores which are dark until 100 micron. These are candidate starless objects, and further observations will help us determine the nature of these very cold cores.
Training Data Independent Image Registration With GANs Using Transfer Learning And Segmentation Information<|sep|>Registration is an important task in automated medical image analysis. Although deep learning (DL) based image registration methods out perform time consuming conventional approaches, they are heavily dependent on training data and do not generalize well for new images types. We present a DL based approach that can register an image pair which is different from the training images. This is achieved by training generative adversarial networks (GANs) in combination with segmentation information and transfer learning. Experiments on chest Xray and brain MR images show that our method gives better registration performance over conventional methods.
A Case for Practical Configuration Management Using Hardware-based Security Tokens<|sep|>Future industrial networks will consist of a complex mixture of new and legacy components, while new use cases and applications envisioned by Industry 4.0 will demand increased flexibility and dynamics from these networks. Industrial security gateways will become an important building block to tackle new security requirements demanded by these changes. Their introduction will further increase the already high complexity of these networks, demanding more efforts in properly and securely configuring them. Yet, past research showed, that most operators of industrial networks are already today unable to configure industrial networks in a secure fashion. Therefore, we propose a scheme that allows factory operators to configure security gateways in an easy and practical way that is also understandable for staff not trained in the security domain. We employ hardware security tokens that allow to reduce every day configuration to one physical interaction. Our results show the practical feasibility of our proposed scheme and that it does not reduce the security level of industrial security gateways in any way.
GYES, a multifibre spectrograph for the CFHT<|sep|>We have chosen the name of GYES, one of the mythological giants with one hundred arms, offspring of Gaia and Uranus, for our instrument study of a multifibre spectrograph for the prime focus of the Canada-France-Hawaii Telescope. Such an instrument could provide an excellent ground-based complement for the Gaia mission and a northern complement to the HERMES project on the AAT. The CFHT is well known for providing a stable prime focus environment, with a large field of view, which has hosted several imaging instruments, but has never hosted a multifibre spectrograph. Building upon the experience gained at GEPI with FLAMES-Giraffe and X-Shooter, we are investigating the feasibility of a high multiplex spectrograph (about 500 fibres) over a field of view 1 degree in diameter. We are investigating an instrument with resolution in the range 15000 to 30000, which should provide accurate chemical abundances for stars down to 16th magnitude and radial velocities, accurate to 1 km/s for fainter stars. The study is led by GEPI-Observatoire de Paris with a contribution from Oxford for the study of the positioner. The financing for the study comes from INSU CSAA and Observatoire de Paris. The conceptual study will be delivered to CFHT for review by October 1st 2010.
A comparative study of attention mechanism and generative adversarial network in facade damage segmentation<|sep|>Semantic segmentation profits from deep learning and has shown its possibilities in handling the graphical data from the on-site inspection. As a result, visual damage in the facade images should be detected. Attention mechanism and generative adversarial networks are two of the most popular strategies to improve the quality of semantic segmentation. With specific focuses on these two strategies, this paper adopts U-net, a representative convolutional neural network, as the primary network and presents a comparative study in two steps. First, cell images are utilized to respectively determine the most effective networks among the U-nets with attention mechanism or generative adversarial networks. Subsequently, selected networks from the first test and their combination are applied for facade damage segmentation to investigate the performances of these networks. Besides, the combined effect of the attention mechanism and the generative adversarial network is discovered and discussed.
Structure of Superheavy Nuclei Along Element 115 Decay Chains<|sep|>A recent high-resolution $\alpha$, $X$-ray, and $\gamma$-ray coincidence-spectroscopy experiment offered first glimpse of excitation schemes of isotopes along $\alpha$-decay chains of $Z=115$. To understand these observations and to make predictions about shell structure of superheavy nuclei below $^{288}115$, we employ two complementary mean-field models: self-consistent Skyrme Energy Density Functional approach and the macroscopic-microscopic Nilsson model. We discuss the spectroscopic information carried by the new data. In particular, candidates for the experimentally observed $E1$ transitions in $^{276}$Mt are proposed. We find that the presence and nature of low-energy $E1$ transitions in well-deformed nuclei around $Z=110, N=168$ strongly depends on the strength of the spin-orbit coupling; hence, it provides an excellent constraint on theoretical models of superheavy nuclei. To clarify competing theoretical scenarios, an experimental search for $E1$ transitions in odd-$A$ systems $^{275,277}$Mt, $^{275}$Hs, and $^{277}$Ds is strongly recommended.
Is the Policy Gradient a Gradient?<|sep|>The policy gradient theorem describes the gradient of the expected discounted return with respect to an agent's policy parameters. However, most policy gradient methods drop the discount factor from the state distribution and therefore do not optimize the discounted objective. What do they optimize instead? This has been an open question for several years, and this lack of theoretical clarity has lead to an abundance of misstatements in the literature. We answer this question by proving that the update direction approximated by most methods is not the gradient of any function. Further, we argue that algorithms that follow this direction are not guaranteed to converge to a "reasonable" fixed point by constructing a counterexample wherein the fixed point is globally pessimal with respect to both the discounted and undiscounted objectives. We motivate this work by surveying the literature and showing that there remains a widespread misunderstanding regarding discounted policy gradient methods, with errors present even in highly-cited papers published at top conferences.
Spark Parameter Tuning via Trial-and-Error<|sep|>Spark has been established as an attractive platform for big data analysis, since it manages to hide most of the complexities related to parallelism, fault tolerance and cluster setting from developers. However, this comes at the expense of having over 150 configurable parameters, the impact of which cannot be exhaustively examined due to the exponential amount of their combinations. The default values allow developers to quickly deploy their applications but leave the question as to whether performance can be improved open. In this work, we investigate the impact of the most important of the tunable Spark parameters on the application performance and guide developers on how to proceed to changes to the default values. We conduct a series of experiments with known benchmarks on the MareNostrum petascale supercomputer to test the performance sensitivity. More importantly, we offer a trial-and-error methodology for tuning parameters in arbitrary applications based on evidence from a very small number of experimental runs. We test our methodology in three case studies, where we manage to achieve speedups of more than 10 times.
Random Tight Frames<|sep|>We introduce probabilistic frames to study finite frames whose elements are chosen at random. While finite tight frames generalize orthonormal bases by allowing redundancy, independent, uniformly distributed points on the sphere approximately form a finite unit norm tight frame (FUNTF). In the present paper, we develop probabilistic versions of tight frames and FUNTFs to significantly weaken the requirements on the random choice of points to obtain an approximate finite tight frame. Namely, points can be chosen from any probabilistic tight frame, they do not have to be identically distributed, nor have unit norm. We also observe that classes of random matrices used in compressed sensing are induced by probabilistic tight frames.
Yangian Symmetry for the Action of Planar N=4 Super Yang-Mills and N=6 Super Chern-Simons Theories<|sep|>In this article we establish the notion of classical Yangian symmetry for planar N=4 supersymmetric Yang-Mills theory and for related planar gauge theories. After revisiting Yangian invariance for the equations of motion, we describe how the bi-local generators act on the action of the model such that the latter becomes exactly invariant. In particular, we elaborate on the relevance of the planar limit and how to act non-linearly with bi-local generators on the cyclic action.
New parallel programming language design: a bridge between brain models and multi-core/many-core computers?<|sep|>The recurrent theme of this paper is that sequences of long temporal patterns as opposed to sequences of simple statements are to be fed into computation devices, being them (new proposed) models for brain activity or multi-core/many-core computers. In such models, parts of these long temporal patterns are already committed while other are predicted. This combination of matching patterns and making predictions appears as a key element in producing intelligent processing in brain models and getting efficient speculative execution on multi-core/many-core computers. A bridge between these far-apart models of computation could be provided by appropriate design of massively parallel, interactive programming languages. Agapia is a recently proposed language of this kind, where user controlled long high-level temporal structures occur at the interaction interfaces of processes. In this paper Agapia is used to link HTMs brain models with TRIPS multi-core/many-core architectures.
Magnetic Transformations in the Organic Conductor kappa-(BETS)2Mn[N(CN)2]3 at the Metal-Insulator Transition<|sep|>A complex study of magnetic properties including dc magnetization, 1H NMR and magnetic torque measurements has been performed for the organic conductor kappa-(BETS)2Mn[N(CN)2]3 which undergoes a metal-insulator transition at T_MI~25K. NMR and the magnetization data indicate a transition in the manganese subsystem from paramagnetic to a frozen state at T_MI, which is, however, not a simple Neel type order. Further, a magnetic field induced transition resembling a spin flop has been detected in the torque measurements at temperatures below T_MI. This transition is most likely related to the spins of pi-electrons localized on the organic molecules BETS and coupled with the manganese 3d spins via exchange interaction.
What ignites on the neutron star of 4U 0614+091?<|sep|>[abridged] The LMXB 4U 0614+091 is a source of sporadic thermonuclear (type I) X-ray bursts. We find bursts with a wide variety of characteristics in serendipitous wide-field X-ray observations by EURECA/WATCH, RXTE/ASM, BeppoSAX/WFC, HETE-2/FREGATE, INTEGRAL/IBIS/ISGRI, and Swift/BAT, as well as pointed observations by RXTE/PCA and HEXTE. Most of them reach a peak flux of ~15 Crab, but a few only reach a peak flux below a Crab. One of the bursts shows a very strong photospheric radius-expansion phase. This allows us to evaluate the distance to the source: 3.2 kpc. The burst durations are between 10 sec to 5 min. However, after one of the intermediate-duration bursts, a faint tail is seen to at least ~2.4 hours after the start of the burst. One very long burst lasted for several hours. This superburst candidate was followed by a normal type-I burst only 19 days later. This is, to our knowledge, the shortest burst-quench time among the superbursters. A superburst in this system is difficult to reconcile if it accretes at ~1% L_Edd. The intermediate-duration bursts occurred when 4U 0614+091's persistent emission was lowest and calm, and when bursts were infrequent (on average one every month to 3 months). The average burst rate increased significantly after this period. The maximum average burst recurrence rate is once every week to 2 weeks. The burst behaviour may be partly understood if there is at least an appreciable amount of helium present in the accreted material from the donor star. If the system is an ultra-compact X-ray binary with a CO white-dwarf donor, as has been suggested, this is unexpected. If the bursts are powered by helium, we find that the energy production per accumulated mass is about 2.5 times less than expected for pure helium matter.
Control of Optical Dynamic Memory Capacity of an Atomic Bose-Einstein Condensate<|sep|>Light storage in an atomic Bose-Einstein condensate is one of the most practical usage of these coherent atom-optical systems. In order to make them even more practical, it is necessary to enhance our ability to inject multiple pulses into the condensate. In this paper, we report that dispersion of pulses injected into the condensate can be compensated by optical nonlinearity. In addition, we will present a brief review of our earlier results in which enhancement of light storage capacity is accomplished by utilizing multi-mode light propagation or choosing an optimal set of experimental parameters.
General risk measures for robust machine learning<|sep|>A wide array of machine learning problems are formulated as the minimization of the expectation of a convex loss function on some parameter space. Since the probability distribution of the data of interest is usually unknown, it is is often estimated from training sets, which may lead to poor out-of-sample performance. In this work, we bring new insights in this problem by using the framework which has been developed in quantitative finance for risk measures. We show that the original min-max problem can be recast as a convex minimization problem under suitable assumptions. We discuss several important examples of robust formulations, in particular by defining ambiguity sets based on $\varphi$-divergences and the Wasserstein metric.We also propose an efficient algorithm for solving the corresponding convex optimization problems involving complex convex constraints. Through simulation examples, we demonstrate that this algorithm scales well on real data sets.
Lorentz factor distribution of blazars from the optical Fundamental plane of black hole activity<|sep|>Blazar radiation is dominated by a relativistic jet which can be modeled at first approximation using just two intrinsic parameters - the Lorentz factor $\Gamma$ and the viewing angle $\theta$. Blazar jet observations are often beamed due to relativistic effects, complicating the understanding of these intrinsic properties. The most common way to estimate blazar Lorentz factors needs the estimation of apparent jet speeds and Doppler beaming factors. We present a new and independent method of constructing the blazar Lorentz factor distribution, using the optical fundamental plane of black hole activity. The optical fundamental plane is a plane stretched out by both the supermassive black holes and the X-ray binaries, in the 3D space provided by their [OIII] line luminosity, radio luminosity and black hole mass. We use the intrinsic radio luminosity obtained from the optical fundamental plane to constrain the boosting parameters of the VLBA Imaging and Polarimetry Survey (VIPS) blazar sample. We find a blazar bulk Lorentz factor distribution in the form of a power law as $N(\Gamma) \propto \Gamma^{-2.1 \pm 0.4}$ for the $\Gamma$ range of 1 to 40. We also discuss the viewing angle distribution of the blazars and the dependence of our results on the input parameters.
A Type System for the Vectorial Aspect of the Linear-Algebraic Lambda-Calculus<|sep|>We describe a type system for the linear-algebraic lambda-calculus. The type system accounts for the part of the language emulating linear operators and vectors, i.e. it is able to statically describe the linear combinations of terms resulting from the reduction of programs. This gives rise to an original type theory where types, in the same way as terms, can be superposed into linear combinations. We show that the resulting typed lambda-calculus is strongly normalizing and features a weak subject-reduction.
Self-Rule to Multi-Adapt: Generalized Multi-source Feature Learning Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Detection<|sep|>Supervised learning is constrained by the availability of labeled data, which are especially expensive to acquire in the field of digital pathology. Making use of open-source data for pre-training or using domain adaptation can be a way to overcome this issue. However, pre-trained networks often fail to generalize to new test domains that are not distributed identically due to tissue stainings, types, and textures variations. Additionally, current domain adaptation methods mainly rely on fully-labeled source datasets. In this work, we propose Self-Rule to Multi-Adapt (SRMA), which takes advantage of self-supervised learning to perform domain adaptation, and removes the necessity of fully-labeled source datasets. SRMA can effectively transfer the discriminative knowledge obtained from a few labeled source domain's data to a new target domain without requiring additional tissue annotations. Our method harnesses both domains' structures by capturing visual similarity with intra-domain and cross-domain self-supervision. Moreover, we present a generalized formulation of our approach that allows the framework to learn from multiple source domains. We show that our proposed method outperforms baselines for domain adaptation of colorectal tissue type classification \new{in single and multi-source settings}, and further validate our approach on an in-house clinical cohort. The code and trained models are available open-source: https://github.com/christianabbet/SRA.
A Converse for Fault-tolerant Quantum Computation<|sep|>With improvements in achievable redundancy for fault-tolerant quantum computing, it is natural to ask: what is the minimum required redundancy? In this paper, we obtain a lower bound on the minimum redundancy required for $\epsilon$-accurate implementation of a large class of operations, which includes unitary operators. For the practically relevant case of sub-exponential (in input size) depth and sub-linear gate size, our bound on redundancy is tighter than the best known lower bound in \cite{FawziMS2022}. We obtain this bound by connecting fault-tolerant computation with a set of finite blocklength quantum communication problems whose accuracy requirements satisfy a joint constraint. This bound gives a strictly lower noise threshold for non-degradable noise and captures its dependence on gate size. This bound directly extends to the case where noise at the outputs of a gate are correlated but noise across gates are independent.
Strong Baselines for Complex Word Identification across Multiple Languages<|sep|>Complex Word Identification (CWI) is the task of identifying which words or phrases in a sentence are difficult to understand by a target audience. The latest CWI Shared Task released data for two settings: monolingual (i.e. train and test in the same language) and cross-lingual (i.e. test in a language not seen during training). The best monolingual models relied on language-dependent features, which do not generalise in the cross-lingual setting, while the best cross-lingual model used neural networks with multi-task learning. In this paper, we present monolingual and cross-lingual CWI models that perform as well as (or better than) most models submitted to the latest CWI Shared Task. We show that carefully selected features and simple learning models can achieve state-of-the-art performance, and result in strong baselines for future development in this area. Finally, we discuss how inconsistencies in the annotation of the data can explain some of the results obtained.
Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks<|sep|>This work addresses the problem of vehicle identification through non-overlapping cameras. As our main contribution, we introduce a novel dataset for vehicle identification, called Vehicle-Rear, that contains more than three hours of high-resolution videos, with accurate information about the make, model, color and year of nearly 3,000 vehicles, in addition to the position and identification of their license plates. To explore our dataset we design a two-stream CNN that simultaneously uses two of the most distinctive and persistent features available: the vehicle's appearance and its license plate. This is an attempt to tackle a major problem: false alarms caused by vehicles with similar designs or by very close license plate identifiers. In the first network stream, shape similarities are identified by a Siamese CNN that uses a pair of low-resolution vehicle patches recorded by two different cameras. In the second stream, we use a CNN for OCR to extract textual information, confidence scores, and string similarities from a pair of high-resolution license plate patches. Then, features from both streams are merged by a sequence of fully connected layers for decision. In our experiments, we compared the two-stream network against several well-known CNN architectures using single or multiple vehicle features. The architectures, trained models, and dataset are publicly available at https://github.com/icarofua/vehicle-rear.
Narrow-band anisotropic electronic structure of ReS$_2$<|sep|>We have used angle resolved photoemission spectroscopy to investigate the band structure of ReS$_2$, a transition-metal dichalcogenide semiconductor with a distorted 1T crystal structure. We find a large number of narrow valence bands, which we attribute to the combined influence of the structural distortion and spin-orbit coupling. We further image how this leads to a strong in-plane anisotropy of the electronic structure, with quasi-one-dimensional bands reflecting predominant hopping along zig-zag Re chains. We find that this does not persist up to the top of the valence band, where a more three-dimensional character is recovered with the fundamental band gap located away from the Brillouin zone centre along $k_z$. These experiments are in good agreement with our density-functional theory calculations, shedding new light on the bulk electronic structure of ReS$_2$, and how it can be expected to evolve when thinned to a single layer.
Deep Learning Based MAC via Joint Channel Access and Rate Adaptation<|sep|>The existing medium access control (MAC) protocol of Wi-Fi networks (i.e., carrier-sense multiple access with collision avoidance (CSMA/CA)) suffers from poor performance in dense deployments due to the increasing number of collisions and long average backoff time in such scenarios. To tackle this issue, we propose an intelligent wireless MAC protocol based on deep learning (DL), referred to as DL-MAC, which significantly improves the spectrum efficiency of Wi-Fi networks. The goal of DL-MAC is to enable not only intelligent channel access but also intelligent rate adaptation. To achieve this goal, we design a deep neural network (DNN) that takes the historical received signal strength indications (RSSIs) as inputs and outputs joint channel access and rate adaptation decision. Notably, the proposed DL-MAC takes the constraints of practical applications into account and the DL-MAC is evaluated using the real wireless data sampled from the actual environments on the 2.4GHz frequency band. The experimental results show that our DL-MAC can achieve around 86\% performance of the global optimal MAC, and around the double performance of the traditional Wi-Fi MAC in the environments of our lab and the Shenzhen Baoan International Airport departure hall.
Sound-Triggered Collapse of Stably Oscillating Low-Mass Cores in a Two-Phase Interstellar Medium<|sep|>Inspired by Barnard 68, a Bok globule, that undergoes stable oscillations, we perform multi-phase hydrodynamic simulations to analyze the stability of Bok globules. We show that a high-density soft molecular core, with an adiabatic index $\gamma$ = 0.7 embedded in a warm isothermal diffuse gas, must have a small density gradient to retain the stability. Despite being stable, the molecular core can still collapse spontaneously as it will relax to develop a sufficiently large density gradient after tens of oscillations, or a few $10^7$ years. However, during its relaxation, the core may abruptly collapse triggered by the impingement of small-amplitude, long-wavelength ($\sim$ 6 $-$ 36 pc) sound waves in the warm gas. This triggered collapse mechanism is similar to a sonoluminescence phenomenon, where underwater ultrasounds can drive air bubble coalescence. The collapse configuration is found to be different from both inside-out and outside-in models of low-mass star formation; nonetheless the mass flux is close to the prediction of the inside-out model. The condition and the efficiency for this core collapse mechanism are identified. Generally speaking, a broad-band resonance condition must be met, where the core oscillation frequency and the wave frequency should match each other within a factor of several. A consequence of our findings predicts the possibility of propagating low-mass star formation, for which collapse of cores, within a mass range short of one order of magnitude, takes place sequentially tracing the wave front across a region of few tens of pc over $10^7$ years.
PAC-Bayesian Bounds for Deep Gaussian Processes<|sep|>Variational approximation techniques and inference for stochastic models in machine learning has gained much attention the last years. Especially in the case of Gaussian Processes (GP) and their deep versions, Deep Gaussian Processes (DGPs), these viewpoints improved state of the art work. In this paper we introduce Probably Approximately Correct (PAC)-Bayesian risk bounds for DGPs making use of variational approximations. We show that the minimization of PAC-Bayesian generalization risk bounds maximizes the variational lower bounds belonging to the specific DGP model. We generalize the loss function property of the log likelihood loss function in the context of PAC-Bayesian risk bounds to the quadratic-form-Gaussian case. Consistency results are given and an oracle-type inequality gives insights in the convergence between the raw model (predictor without variational approximation) and our variational models (predictor for the variational approximation). Furthermore, we give extensions of our main theorems for specific assumptions and parameter cases. Moreover, we show experimentally the evolution of the consistency results for two Deep Recurrent Gaussian Processes (DRGP) modeling time-series, namely the recurrent Gaussian Process (RGP) and the DRGP with Variational Sparse Spectrum approximation, namely DRGP-(V)SS.
Correlated density-dependent chiral forces for infinite matter calculations within the Green's function approach<|sep|>The properties of symmetric nuclear and pure neutron matter are investigated within an extended self-consistent Green's function method that includes the effects of three-body forces. We use the ladder approximation for the study of infinite nuclear matter and incorporate the three-body interaction by means of a density-dependent two-body force. This force is obtained via a correlated average over the third particle, with an in-medium propagator consistent with the many-body calculation we perform. We analyze different prescriptions in the construction of the average and conclude that correlations provide small modifications at the level of the density-dependent force. Microscopic as well as bulk properties are studied, focusing on the changes introduced by the density dependent two-body force. The total energy of the system is obtained by means of a modified Galitskii-Migdal-Koltun sum rule. Our results validate previously used uncorrelated averages and extend the availability of chirally motivated forces to a larger density regime.
High-precision molecular dynamics simulation of UO2-PuO2: pair potentials comparison in UO2<|sep|>Our series of articles is devoted to high-precision molecular dynamics simulation of mixed actinide-oxide (MOX) fuel in the approximation of rigid ions and pair interactions (RIPI) using high-performance graphics processors (GPU). In this first article 10 most recent and widely used interatomic sets of pair potentials (SPP) are assessed by reproduction of solid phase properties of uranium dioxide (UO2) - temperature dependences of the lattice constant, bulk modulus, enthalpy and heat capacity. Measurements were performed with 1K accuracy in a wide temperature range from 300K up to melting points. The best results are demonstrated by two recent SPPs MOX-07 and Yakub-09, which both had been fitted to the recommended thermal expansion in the range of temperatures 300-3100K. They reproduce the experimental data better than the widely used SPPs Basak-03 and Morelon-03 at temperatures above 2500K.
Politics of Adversarial Machine Learning<|sep|>In addition to their security properties, adversarial machine-learning attacks and defenses have political dimensions. They enable or foreclose certain options for both the subjects of the machine learning systems and for those who deploy them, creating risks for civil liberties and human rights. In this paper, we draw on insights from science and technology studies, anthropology, and human rights literature, to inform how defenses against adversarial attacks can be used to suppress dissent and limit attempts to investigate machine learning systems. To make this concrete, we use real-world examples of how attacks such as perturbation, model inversion, or membership inference can be used for socially desirable ends. Although the predictions of this analysis may seem dire, there is hope. Efforts to address human rights concerns in the commercial spyware industry provide guidance for similar measures to ensure ML systems serve democratic, not authoritarian ends
Transposing Noninvertible Polynomials<|sep|>Landau-Ginzburg mirror symmetry predicts isomorphisms between graded Frobenius algebras (denoted $\mathcal{A}$ and $\mathcal{B}$) that are constructed from a nondegenerate quasihomogeneous polynomial $W$ and a related group of symmetries $G$. Duality between $\mathcal{A}$ and $\mathcal{B}$ models has been conjectured for particular choices of $W$ and $G$. These conjectures have been proven in many instances where $W$ is restricted to having the same number of monomials as variables (called $\textit{invertible}$). Some conjectures have been made regarding isomorphisms between $\mathcal{A}$ and $\mathcal{B}$ models when $W$ is allowed to have more monomials than variables. In this paper we show these conjectures are false; that is, the conjectured isomorphisms do not exist. Insight into this problem will not only generate new results for Landau-Ginzburg mirror symmetry, but will also be interesting from a purely algebraic standpoint as a result about groups acting on graded algebras.
A proposed atom interferometry determination of $G$ at $10^{-5}$ using a cold atomic fountain<|sep|>In precision metrology the determination of the Newtonian gravity constant $G$ represents a real problem, since its history is plagued by huge unknown discrepancies between a large number of independent experiments. In this paper we propose a novel experimental setup for measuring $G$ with a relative accuracy of $10^{-5}$ using a standard cold atomic fountain and matter wave interferometry. We discuss in details the major sources of systematic errors, providing also the expected statistical uncertainty. Feasibility of determining $G$ at a level of $10^{-6}$ level is also discussed.
One-Dimensional Traps, Two-Body Interactions, Few-Body Symmetries: I. One, Two, and Three Particles<|sep|>This is the first in a pair of articles that classify the configuration space and kinematic symmetry groups for $N$ identical particles in one-dimensional traps experiencing Galilean-invariant two-body interactions. These symmetries explain degeneracies in the few-body spectrum and demonstrate how tuning the trap shape and the particle interactions can manipulate these degeneracies. The additional symmetries that emerge in the non-interacting limit and in the unitary limit of an infinitely strong contact interaction are sufficient to algebraically solve for the spectrum and degeneracy in terms of the one-particle observables. Symmetry also determines the degree to which the algebraic expressions for energy level shifts by weak interactions or nearly-unitary interactions are universal, i.e.\ independent of trap shape and details of the interaction. Identical fermions and bosons with and without spin are considered. This article sequentially analyzes the symmetries of one, two and three particles in asymmetric, symmetric, and harmonic traps; the sequel article treats the $N$ particle case.
The singularity theorems of General Relativity and their low regularity extensions<|sep|>On the occasion of Sir Roger Penrose's 2020 Nobel Prize in Physics, we review the singularity theorems of General Relativity, as well as their recent extension to Lorentzian metrics of low regularity. The latter is motivated by the quest to explore the nature of the singularities predicted by the classical theorems. Aiming at the more mathematically minded reader, we give a pedagogical introduction to the classical theorems with an emphasis on the analytical side of the arguments. We especially concentrate on focusing results for causal geodesics under appropriate geometric and initial conditions, in the smooth and in the low regularity case. The latter comprise the main technical advance that leads to the proofs of $C^1$-singularity theorems via a regularisation approach that allows to deal with the distributional curvature. We close with an overview on related lines of research and a future outlook.
Global hot-star wind models for stars from Magellanic Clouds<|sep|>We provide mass-loss rate predictions for O stars from Large and Small Magellanic Clouds. We calculate global (unified, hydrodynamic) model atmospheres of main sequence, giant, and supergiant stars for chemical composition corresponding to Magellanic Clouds. The models solve radiative transfer equation in comoving frame, kinetic equilibrium equations (also known as NLTE equations), and hydrodynamical equations from (quasi-)hydrostatic atmosphere to expanding stellar wind. The models allow us to predict wind density, velocity, and temperature (consequently also the terminal wind velocity and the mass-loss rate) just from basic global stellar parameters. As a result of their lower metallicity, the line radiative driving is weaker leading to lower wind mass-loss rates with respect to the Galactic stars. We provide a formula that fits the mass-loss rate predicted by our models as a function of stellar luminosity and metallicity. On average, the mass-loss rate scales with metallicity as $ \dot M\sim Z^{0.59}$. The predicted mass-loss rates are lower than mass-loss rates derived from H$\alpha$ diagnostics and can be reconciled with observational results assuming clumping factor $C_\text{c}=9$. On the other hand, the predicted mass-loss rates either agree or are slightly higher than the mass-loss rates derived from ultraviolet wind line profiles. The calculated \ion{P}{v} ionization fractions also agree with values derived from observations for LMC stars with $T_\text{eff}\leq40\,000\,$K. Taken together, our theoretical predictions provide reasonable models with consistent mass-loss rate determination, which can be used for quantitative study of stars from Magellanic Clouds.
State selective detection of hyperfine qubits<|sep|>In order to faithfully detect the state of an individual two-state quantum system (qubit) realized using, for example, a trapped ion or atom, state selective scattering of resonance fluorescence is well established. The simplest way to read out this measurement and assign a state is the threshold method. The detection error can be decreased by using more advanced detection methods like the time-resolved method or the $\pi$-pulse detection method. These methods were introduced to qubits with a single possible state change during the measurement process. However, there exist many qubits like the hyperfine qubit of $^{171}Yb^+$ where several state change are possible. To decrease the detection error for such qubits, we develope generalizations of the time-resolved method and the $\pi$-pulse detection method for such qubits. We show the advantages of these generalized detection methods in numerical simulations and experiments using the hyperfine qubit of $^{171}Yb^+$. The generalized detection methods developed here can be implemented in an efficient way such that experimental real time state discrimination with improved fidelity is possible.
Bipartite Mixed Membership Distribution-Free Model. A novel model for community detection in overlapping bipartite weighted networks<|sep|>Modeling and estimating mixed memberships for un-directed un-weighted networks in which nodes can belong to multiple communities has been well studied in recent years. However, for a more general case, the bipartite weighted networks in which nodes can belong to multiple communities, row nodes can be different from column nodes, and all elements of adjacency matrices can be any finite real values, to our knowledge, there is no model for such bipartite weighted networks. To close this gap, this paper introduces a novel model, the Bipartite Mixed Membership Distribution-Free (BiMMDF) model. As a special case, bipartite signed networks with mixed memberships can also be generated from BiMMDF. Our model enjoys its advantage by allowing all elements of an adjacency matrix to be generated from any distribution as long as the expectation adjacency matrix has a block structure related to node memberships under BiMMDF. The proposed model can be viewed as an extension of many previous models, including the popular mixed membership stochastic blcokmodels. An efficient algorithm with a theoretical guarantee of consistent estimation is applied to fit BiMMDF. In particular, for a standard bipartite weighted network with two row (and column) communities, to make the algorithm's error rates small with high probability, separation conditions are obtained when adjacency matrices are generated from different distributions under BiMMDF. The behavior differences of different distributions on separation conditions are verified by extensive synthetic bipartite weighted networks generated under BiMMDF. Experiments on real-world directed weighted networks illustrate the advantage of the algorithm in studying highly mixed nodes and asymmetry between row and column communities.
Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models<|sep|>Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.
Secret Key Agreement Using Conferencing in State- Dependent Multiple Access Channels with An Eavesdropper<|sep|>In this paper, the problem of secret key agreement in state-dependent multiple access channels with an eavesdropper is studied. For this model, the channel state information is non-causally available at the transmitters; furthermore, a legitimate receiver observes a degraded version of the channel state information. The transmitters can partially cooperate with each other using a conferencing link with a limited rate. In addition, a backward public channel is assumed between the terminals. The problem of secret key sharing consists of two rounds. In the first round, the transmitters wish to share a common key with the legitimate receiver. Lower and upper bounds on the common key capacity are established. In a special case, the capacity of the common key is obtained. In the second round, the legitimate receiver agrees on two independent private keys with the corresponding transmitters using the public channel. Inner and outer bounds on the private key capacity region are characterized. In a special case, the inner bound coincides with the outer bound. We provide some examples to illustrate our results.
Background and Imaging Simulations for the Hard X-Ray Camera of the MIRAX Mission<|sep|>We report the results of detailed Monte Carlo simulations of the performance expected both at balloon altitudes and at the probable satellite orbit of a hard X-ray coded-aperture camera being developed for the MIRAX mission. Based on a thorough mass model of the instrument and detailed specifications of the spectra and angular dependence of the various relevant radiation fields at both the stratospheric and orbital environments, we have used the well-known package GEANT4 to simulate the instrumental background of the camera. We also show simulated images of source fields to be observed and calculated the detailed sensitivity of the instrument in both situations. The results reported here are especially important to researchers in this field considering that we provide important information, not easily found in the literature, on how to prepare input files and calculate crucial instrumental parameters to perform GEANT4 simulations for high-energy astrophysics space experiments.
Dynamical localization of chaotic eigenstates in the mixed-type systems: spectral statistics in a billiard system after separation of regular and chaotic eigenstates<|sep|>We study the quantum mechanics of a billiard (Robnik 1983) in the regime of mixed-type classical phase space (the shape parameter \lambda=0.15) at very high-lying eigenstates, starting at about 1.000.000th eigenstate and including the consecutive 587654 eigenstates. By calculating the normalized Poincar\'e Husimi functions of the eigenstates and comparing them with the classical phase space structure, we introduce the overlap criterion which enables us to separate with great accuracy and reliability the regular and chaotic eigenstates, and the corresponding energies. The chaotic eigenstates appear all to be dynamically localized, meaning that they do not occupy unformly the entire available chaotic classical phase space component, but are localized on a proper subset. We find with unprecedented precision and statistical significance that the level spacing distribution of the regular levels obeys the Poisson statistics, and the chaotic ones obey the Brody statistics, as anticipated in a recent paper by Batisti\'c and Robnik (2010), where the entire spectrum was found to obey the BRB statistics. There are no effects of dynamical tunneling in this regime, due to the high energies, as they decay exponentially with the inverse effective Planck constant which is proportional to the square root of the energy.
Code Properties of the Holographic Sierpinski Triangle<|sep|>We study the holographic quantum error correcting code properties of a Sierpinski Triangle-shaped boundary subregion in $AdS_4/CFT_3$. Due to existing no-go theorems in topological quantum error correction regarding fractal noise, this gives holographic codes a specific advantage over topological codes. We then further argue that a boundary subregion in the shape of the Sierpinski gasket in $AdS_5/CFT_4$ does not possess these holographic quantum error correction properties.
Multilinear Subspace Clustering<|sep|>In this paper we present a new model and an algorithm for unsupervised clustering of 2-D data such as images. We assume that the data comes from a union of multilinear subspaces (UOMS) model, which is a specific structured case of the much studied union of subspaces (UOS) model. For segmentation under this model, we develop Multilinear Subspace Clustering (MSC) algorithm and evaluate its performance on the YaleB and Olivietti image data sets. We show that MSC is highly competitive with existing algorithms employing the UOS model in terms of clustering performance while enjoying improvement in computational complexity.
Type 1 low z AGN. I. Emission properties<|sep|>We analyze the emission properties of a new sample of 3,579 type 1 AGN, selected from the SDSS DR7 based on the detection of broad H-alpha emission. The sample extends over a broad H-alpha luminosity L_bHa of 10^40 - 10^44 erg s^-1 and a broad H-alpha FWHM of 1,000 - 25,000 km s^-1, which covers the range of black hole mass 10^6<M_BH/M_Sun<10^9.5 and luminosity in Eddington units 10^-3 < L/L_Edd < 1. We combine ROSAT, GALEX and 2MASS observations to form the SED from 2.2 mic to 2 keV. We find the following: 1. The distribution of the H-alpha FWHM values is independent of luminosity. 2. The observed mean optical-UV SED is well matched by a fixed shape SED of luminous quasars, which scales linearly with L_bHa, and a host galaxy contribution. 3. The host galaxy r-band (fibre) luminosity function follows well the luminosity function of inactive non-emission line galaxies (NEG), consistent with a fixed fraction of ~3% of NEG hosting an AGN, regardless of the host luminosity. 4. The hosts of lower luminosity AGN have a mean z band luminosity and u-z colour which are identical to NEG with the same redshift distribution. With increasing L_bHa the AGN hosts become bluer and less luminous than NEG. The implied increasing star formation rate with L_bHa is consistent with the relation for SDSS type 2 AGN of similar bolometric luminosity. 5. The optical-UV SED of the more luminous AGN shows a small dispersion, consistent with dust reddening of a blue SED, as expected for thermal thin accretion disc emission. 6. There is a rather tight relation of nuL_nu(2 keV) and L_bHa, which provides a useful probe for unobscured (true) type 2 AGN. 7. The primary parameter which drives the X-ray to UV emission ratio is the luminosity, rather than M_BH or L/L_Edd.
A new zero-knowledge code based identification scheme with reduced communication<|sep|>In this paper we present a new 5-pass identification scheme with asymptotic cheating probability 1/2 based on the syndrome decoding problem. Our protocol is related to the Stern identification scheme but has a reduced communication cost compared to previous code-based zero-knowledge schemes, moreover our scheme permits to obtain a very low size of public key and secret key. The contribution of this paper is twofold, first we propose a variation on the Stern authentication scheme which permits to decrease asymptotically the cheating probability to 1/2 rather than 2/3 (and very close to 1/2 in practice) but with less communication. Our solution is based on deriving new challenges from the secret key through cyclic shifts of the initial public key syndrome; a new proof of soundness for this case is given Secondly we propose a new way to deal with hashed commitments in zero-knowledge schemes based on Stern's scheme, so that in terms of communication, on the average, only one hash value is sent rather than two or three. Overall our new scheme has the good features of having a zero-knowledge security proof based on well known hard problem of coding theory, a small size of secret and public key (a few hundred bits), a small calculation complexity, for an overall communication cost of 19kb for authentication (for a $2^{16}$ security) and a signature of size of 93kb (11.5kB) (for security $2^{80}$), an improvement of 40% compared to previous schemes based on coding theory.
Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image Segmentation using Partially Labeled Data<|sep|>Computer-assisted quantitative analysis on Giga-pixel pathology images has provided a new avenue in histology examination. The innovations have been largely focused on cancer pathology (i.e., tumor segmentation and characterization). In non-cancer pathology, the learning algorithms can be asked to examine more comprehensive tissue types simultaneously, as a multi-label setting. The prior arts typically needed to train multiple segmentation networks in order to match the domain-specific knowledge for heterogeneous tissue types (e.g., glomerular tuft, glomerular unit, proximal tubular, distal tubular, peritubular capillaries, and arteries). In this paper, we propose a dynamic single segmentation network (Omni-Seg) that learns to segment multiple tissue types using partially labeled images (i.e., only one tissue type is labeled for each training image) for renal pathology. By learning from ~150,000 patch-wise pathological images from six tissue types, the proposed Omni-Seg network achieved superior segmentation accuracy and less resource consumption when compared to the previous the multiple-network and multi-head design. In the testing stage, the proposed method obtains "completely labeled" tissue segmentation results using only "partially labeled" training images. The source code is available at https://github.com/ddrrnn123/Omni-Seg
Self-gravitating systems of ideal gases in the 1PN approximation<|sep|>We obtain the Maxwell-J\"uttner distribution function at first order in the post-Newtonian approximation within the framework of general relativity. Taking into account the aforesaid distribution function, we compute the particle four-flow and energy-momentum tensor. We focus on the search of static solutions for the gravitational potentials with spherical symmetry. In doing so, we obtain the density, pressure and gravitational potential energy profiles in terms of dimensionless radial coordinate by solving the aforesaid equations numerically. In particular, we find the parametric profile for the equation of state $p/\rho$ in terms of the dimensionless radial coordinate. Due to its physical relevance, we also find the galaxy rotation curves using the post-Newtonian approximation. We join two different kinds of static solutions in order to account for the linear regime near the center and the typical flatten behavior at large radii as well.
Vacuum stability from vector dark matter<|sep|>We study a model of vector dark matter with the complex scalar Higgs portal. Renormalisation group equations at the 2-loop level are used to analyse perturbativity and stability of the vacuum. We impose experimental and theoretical constraints on the model and find regions in the parameter space consistent with the dark matter relic abundance inferred from the Planck data and bounds on DM-nucleon scattering cross-section from XENON and LUX experiments.
A classification of transitive links and periodic links<|sep|>We generalized the periodic links to \emph{transitive} links in a $3$-manifold $M$. We find a complete classification theorem of transitive links in a $3$-dimensional sphere $\mathbb{R}^3$. We study these links from several different aspects including polynomial invariants using the relation between link polynomials of a transitive link and its factor links.
Detecting Majorana Bound States by Nanomechanics<|sep|>We propose a nanomechanical detection scheme for Majorana bound states, which have been predicted to exist at the edges of a one-dimensional topological superconductor, implemented, for instance, using a semiconducting wire placed on top of an s-wave superconductor. The detector makes use of an oscillating electrode, which can be realized using a doubly clamped metallic beam, tunnel coupled to one edge of the topological superconductor. We find that a measurement of the nonlinear differential conductance provides the necessary information to uniquely identify Majorana bound states.
Indian Language Wordnets and their Linkages with Princeton WordNet<|sep|>Wordnets are rich lexico-semantic resources. Linked wordnets are extensions of wordnets, which link similar concepts in wordnets of different languages. Such resources are extremely useful in many Natural Language Processing (NLP) applications, primarily those based on knowledge-based approaches. In such approaches, these resources are considered as gold standard/oracle. Thus, it is crucial that these resources hold correct information. Thereby, they are created by human experts. However, human experts in multiple languages are hard to come by. Thus, the community would benefit from sharing of such manually created resources. In this paper, we release mappings of 18 Indian language wordnets linked with Princeton WordNet. We believe that availability of such resources will have a direct impact on the progress in NLP for these languages.
CAIXA: a Catalogue of AGN In the XMM-Newton Archive I. Spectral analysis<|sep|>We present CAIXA, a Catalogue of AGN In the XMM-Newton Archive. It consists of all the radio-quiet X-ray unobscured ($\mathrm{N_H}<2\times10^{22}$ cm$^{-2}$) Active Galactic Nuclei (AGN) observed by XMM-Newton in targeted observations, whose data are public as of March 2007. With its 156 sources, this is the largest catalogue of high signal-to-noise X-ray spectra of AGN. All the EPIC pn spectra of the sources in CAIXA were extracted homogeneously and a baseline model was applied in order to derive their basic X-ray properties. These data are complemented by multiwavelength data found in the literature: Black Hole masses, Full Width Half Maximum (FWHM) of H$\beta$, radio and optical fluxes. Here we describe our homogeneous spectral analysis of the X-ray data in CAIXA and present all the results on the parameters adopted in our best-fit models.
Simultaneous Partial Inverses and Decoding Interleaved Reed-Solomon Codes<|sep|>The paper introduces the simultaneous partial-inverse problem (SPI) for polynomials and develops its application to decoding interleaved Reed--Solomon codes beyond half the minimum distance. While closely related both to standard key equations and to well-known Pad\'e approximation problems, the SPI problem stands out in several respects. First, the SPI problem has a unique solution (up to a scale factor), which satisfies a natural degree bound. Second, the SPI problem can be transformed (monomialized) into an equivalent SPI problem where all moduli are monomials. Third, the SPI problem can be solved by an efficient algorithm of the Berlekamp--Massey type. Fourth, decoding interleaved Reed--Solomon codes (or subfield-evaluation codes) beyond half the minimum distance can be analyzed in terms of a partial-inverse condition for the error pattern: if that condition is satisfied, then the (true) error locator polynomial is the unique solution of a standard key equation and can be computed in many different ways, including the well-known multi-sequence Berlekamp--Massey algorithm and the SPI algorithm of this paper. Two of the best performance bounds from the literature (the Schmidt--Sidorenko--Bossert bound and the Roth--Vontobel bound) are generalized to hold for the partial-inverse condition and thus to apply to several different decoding algorithms.
Composition Properties of Bayesian Differential Privacy<|sep|>Differential privacy is a rigorous privacy standard that has been applied to a range of data analysis tasks. To broaden the application scenarios of differential privacy when data records have dependencies, the notion of Bayesian differential privacy has been recently proposed. However, it is unknown whether Bayesian differential privacy preserves three nice properties of differential privacy: sequential composability, parallel composability, and post-processing. In this paper, we provide an affirmative answer to this question; i.e., Bayesian differential privacy still have these properties. The idea behind sequential composability is that if we have $m$ algorithms $Y_1, Y_2, \ldots, Y_m$, where $Y_{\ell}$ is independently $\epsilon_{\ell}$-Bayesian differential private for ${\ell}=1,2,\ldots,m$, then by feeding the result of $Y_1$ into $Y_2$, the result of $Y_2$ into $Y_3$, and so on, we will finally have an $\sum_{\ell=1}^m \epsilon_{\ell}$-Bayesian differential private algorithm. For parallel composability, we consider the situation where a database is partitioned into $m$ disjoint subsets. The $\ell$-th subset is input to a Bayesian differential private algorithm $Y_{\ell}$, for ${\ell}=1,2,\ldots,m$. Then the parallel composition of $Y_1$, $Y_2$, $\ldots$, $Y_m$ will be $\max_{\ell=1}^m \epsilon_{\ell}$-Bayesian differential private. The post-processing property means that a data analyst, without additional knowledge about the private database, cannot compute a function of the output of a Bayesian differential private algorithm and reduce its privacy guarantee.
Working Principles of Binary Differential Evolution<|sep|>We conduct a first fundamental analysis of the working principles of binary differential evolution (BDE), an optimization heuristic for binary decision variables that was derived by Gong and Tuson (2007) from the very successful classic differential evolution (DE) for continuous optimization. We show that unlike most other optimization paradigms, it is stable in the sense that neutral bit values are sampled with probability close to $1/2$ for a long time. This is generally a desirable property, however, it makes it harder to find the optima for decision variables with small influence on the objective function. This can result in an optimization time exponential in the dimension when optimizing simple symmetric functions like OneMax. On the positive side, BDE quickly detects and optimizes the most important decision variables. For example, dominant bits converge to the optimal value in time logarithmic in the population size. This enables BDE to optimize the most important bits very fast. Overall, our results indicate that BDE is an interesting optimization paradigm having characteristics significantly different from classic evolutionary algorithms or estimation-of-distribution algorithms (EDAs). On the technical side, we observe that the strong stochastic dependencies in the random experiment describing a run of BDE prevent us from proving all desired results with the mathematical rigor that was successfully used in the analysis of other evolutionary algorithms. Inspired by mean-field approaches in statistical physics we propose a more independent variant of BDE, show experimentally its similarity to BDE, and prove some statements rigorously only for the independent variant. Such a semi-rigorous approach might be interesting for other problems in evolutionary computation where purely mathematical methods failed so far.
Synthetically Trained Neural Networks for Learning Human-Readable Plans from Real-World Demonstrations<|sep|>We present a system to infer and execute a human-readable program from a real-world demonstration. The system consists of a series of neural networks to perform perception, program generation, and program execution. Leveraging convolutional pose machines, the perception network reliably detects the bounding cuboids of objects in real images even when severely occluded, after training only on synthetic images using domain randomization. To increase the applicability of the perception network to new scenarios, the network is formulated to predict in image space rather than in world space. Additional networks detect relationships between objects, generate plans, and determine actions to reproduce a real-world demonstration. The networks are trained entirely in simulation, and the system is tested in the real world on the pick-and-place problem of stacking colored cubes using a Baxter robot.
From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification<|sep|>We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.
Mesh-TensorFlow: Deep Learning for Supercomputers<|sep|>Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the "batch" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at https://github.com/tensorflow/mesh .
The bow-shock and high-speed jet in the faint, 40 arcmin diameter, outer halo of the evolved Helix planetary nebula (NGC 7293)<|sep|>In previous, very deep, optical images of NGC 7293 both a feature that has the morphology of a bow-shock and one with that of a jet were discovered in the faint 40 arcmin diameter halo of the nebula. Spatially resolved longslit profiles of the Halpha and [N II] 6548, 6584 A nebular emission lines from both features have now been obtained. The bow-shaped feature has been found to have Halpha radial velocities close to the systemic heliocentric radial velocity, -27 km/s, of NGC 7293 and is faint in the [N II] 6548, 6584 A emission lines. Furthermore, the full width of these profiles matches the relative motion of NGC 7293 with its ambient interstellar medium consequently it is deduced that the feature is a real bow-shock caused by the motion of NGC 7293 as it ploughs through this medium. The proper motion of the central star also points towards this halo feature which substantiates this interpretation of its origin. Similarly [N II] 6584 A line profiles reveal that the jet-like filament is indeed a collimated outflow, as suggested by its morphology, at around 300 km/s with turbulent widths of around 50 km/s. It's low Halpha/[N II] 6548, 6584 A brightness ratio suggests collisional ionization as expected in a high-speed jet.
Design and Analysis of Dynamic Auto Scaling Algorithm (DASA) for 5G Mobile Networks<|sep|>Network Function Virtualization (NFV) enables mobile operators to virtualize their network entities as Virtualized Network Functions (VNFs), offering fine-grained on-demand network capabilities. VNFs can be dynamically scale-in/out to meet the performance requirements for future 5G networks. However, designing an auto-scaling algorithm with low operation cost and low latency while considering the capacity of legacy network equipment is a challenge. In this paper, we propose a VNF Dynamic Auto Scaling Algorithm (DASA) considering the tradeoff between performance and operation cost. We also develop an analytical model to quantify the tradeoff and validate the analysis through extensive simulations. The system is modeled as a queueing model while legacy network equipment is considered as a reserved block of servers. The VNF instances are powered on and off according to the number of job requests. The results show that the proposed DASA can significantly reduce operation cost given the latency upper-bound. Moreover, the models provide a quick way to evaluate the cost-performance tradeoff without wide deployment, which can save cost and time.
Silicon carbide absorption features: dust formation in the outflows of extreme carbon stars<|sep|>Infrared carbon stars without visible counterparts are generally known as extreme carbon stars. We have selected a subset of these stars with absorption features in the 10-13 $\mu$m range, which has been tentatively attributed to silicon carbide (SiC). We add three new objects meeting these criterion to the seven previously known, bringing our total sample to ten sources. We also present the result of radiative transfer modeling for these stars, comparing these results to those of previous studies. In order to constrain model parameters, we use published mass-loss rates, expansion velocities and theoretical dust condensation models to determine the dust condensation temperature. These show that the inner dust temperatures of the dust shells for these sources are significantly higher than previously assumed. This also implies that the dominant dust species should be graphite instead of amorphous carbon. In combination with the higher condensation temperature we show that this results in a much higher acceleration of the dust grains than would be expected from previous work. Our model results suggest that the very optically thick stage of evolution does not coincide with the timescales for the superwind, but rather, that this is a very short-lived phase. Additionally, we compare model and observational parameters in an attempt to find any correlations. Finally, we show that the spectrum of one source, IRAS 17534$-$3030, strongly implies that the 10-13 $\mu$m feature is due to a solid state rather than a molecular species.
Neural Template: Topology-aware Reconstruction and Disentangled Generation of 3D Meshes<|sep|>This paper introduces a novel framework called DTNet for 3D mesh reconstruction and generation via Disentangled Topology. Beyond previous works, we learn a topology-aware neural template specific to each input then deform the template to reconstruct a detailed mesh while preserving the learned topology. One key insight is to decouple the complex mesh reconstruction into two sub-tasks: topology formulation and shape deformation. Thanks to the decoupling, DT-Net implicitly learns a disentangled representation for the topology and shape in the latent space. Hence, it can enable novel disentangled controls for supporting various shape generation applications, e.g., remix the topologies of 3D objects, that are not achievable by previous reconstruction works. Extensive experimental results demonstrate that our method is able to produce high-quality meshes, particularly with diverse topologies, as compared with the state-of-the-art methods.
Two Higgs Doublet Model with Scalar Mediation via Yukawa Interactions<|sep|>Scalar sector offers a valuable avenue to explore its implications in the particle physics phenomenology for new physics and several model dependent phenomena. Two Higgs doublet models are among the promising models for such explorations. In the presence of a standard model Higgs, a two Higgs doublet model is studied in which the two Higgs fields are connected with each other via a scalar singlet field. The study is conducted using Dyson Schwinger equations under the Yukawa vertices set at their tree level form up to certain constants. Among the field propagators, the two Higgs propagators are found to be least effected in the parameter space which also reflects in the insignificant beyond the bare mass contributions to the renormalized Higgs masses. Evidence of universality in terms of identical coupling for both Higgs is found in the model. The model is found to be non-trivial in the explored parameter space. There are cutoff effects in the model. Stability in the calculated correlation functions and parameters generally ensues beyond 100 TeV cutoff.
Inferring the star formation histories of the most massive and passive early-type galaxies at z<0.3<|sep|>Massive galaxies are key probes to understand how the baryonic matter evolves within the dark matter halos. We use the "archaeological" approach to infer the stellar population properties and star formation histories of the most massive (M > 10^10.75 Msun) and passive early-type galaxies (ETGs) at 0 < z < 0.3, based on stacked, high signal to noise ratio (SNR), Sloan Digital Sky Survey spectra. We exploit the information present in the full-spectrum by means of the STARLIGHT public code to retrieve the ETGs evolutionary properties, such as age, metallicity and star formation history. We find that the stellar metallicities are slightly supersolar (Z ~ 0.027 +/- 0.002) and do not depend on redshift. Dust extinction is very low, with a mean of Av ~ 0.08 +/- 0.03 mag. The ETGs show an anti-hierarchical evolution (downsizing) where more massive galaxies are older. The SFHs can be approximated by a parametric function of the form SFR(t) \propto \tau^-(c+1) t^(c) exp(-t/\tau), with typical short e-folding times of \tau ~ 0.6 - 0.8 Gyr (and a dispersion of +/- 0.1 Gyr) and c ~ 0.1 (and a dispersion of +/- 0.05). The inferred SFHs are also used to place constraints on the properties and evolution of the ETG progenitors. In particular, the ETGs of our samples should have formed most stars through a phase of vigorous star formation (SFRs > 350-400 Msun yr^-1) at z ~ 4 - 5, and are quiescent by z ~ 1.5 -2. Our results represent an attempt to demonstrate quantitatively the evolutionary link between the most massive ETGs at z < 0.3 and the properties of suitable progenitors at high redshifts, also showing that the full-spectrum fitting is a powerful approach to reconstruct the star formation histories of massive quiescent galaxies.
Lie-B\"acklund symmetry and non-invariant solutions of nonlinear evolution equations<|sep|>We study the symmetry reduction of nonlinear partial differential equations which are used for describing diffusion processes in nonhomogeneous medium. We find ansatzes reducing partial differential equations to systems of ordinary differential equations. The ansatzes are constructed by using operators of Lie-B\"acklund symmetry of third order ordinary differential equation. The method gives the possibility to find solutions which cannot be obtained by virtue of classical Lie method. Such solutions have been constructed for nonlinear diffusion equations which are invariant with respect to one-parameter, two-parameter and three-parameter Lie groups of point transformations.
Infrared scaling for a graviton condensate<|sep|>The coupling between gravity and matter provides an intriguing length scale in the infrared for theories of gravity within Einstein-Hilbert action and beyond. In particular, we will show that such an infrared length scale is determined by the number of gravitons $N_{g}\gg1$ associated to a given mass in the non-relativistic limit. After tracing out the matter degrees of freedom, the graviton vacuum is found to be in a displaced vacuum with an occupation number of gravitons $N_{g}\gg1$. In the infrared, the length scale appears to be $L=\sqrt{N_{g}}\ell_{p}$, where $L$ is the new infrared length scale, and $\ell_{p}$ is the Planck length. In a specific example, we have found that the infrared length scale is greater than the Schwarzschild radius for a slowly moving in-falling thin shell of matter. We will argue that the appearance of such an infrared length scale in higher curvature theories of gravity, such as in quadratic and cubic curvature theories of gravity, is also expected. Furthermore, we will show that gravity is fundamentally different from the electromagnetic interaction where the number of photons, $N_{p}$, is the fine structure constant after tracing out an electron wave function.
Anomaly Detection via Self-organizing Map<|sep|>Anomaly detection plays a key role in industrial manufacturing for product quality control. Traditional methods for anomaly detection are rule-based with limited generalization ability. Recent methods based on supervised deep learning are more powerful but require large-scale annotated datasets for training. In practice, abnormal products are rare thus it is very difficult to train a deep model in a fully supervised way. In this paper, we propose a novel unsupervised anomaly detection approach based on Self-organizing Map (SOM). Our method, Self-organizing Map for Anomaly Detection (SOMAD) maintains normal characteristics by using topological memory based on multi-scale features. SOMAD achieves state-of the-art performance on unsupervised anomaly detection and localization on the MVTec dataset.
Gravitational field of one uniformly moving extended body and N arbitrarily moving pointlike bodies in post-Minkowskian approximation<|sep|>High precision astrometry, space missions and certain tests of General Relativity, require the knowledge of the metric tensor of the solar system, or more generally, of a gravitational system of N extended bodies. Presently, the metric of arbitrarily shaped, rotating, oscillating and arbitrarily moving N bodies of finite extension is only known for the case of slowly moving bodies in the post-Newtonian approximation, while the post-Minkowskian metric for arbitrarily moving celestial objects is known only for pointlike bodies with mass-monopoles and spin-dipoles. As one more step towards the aim of a global metric for a system of N arbitrarily shaped and arbitrarily moving massive bodies in post-Minkowskian approximation, two central issues are on the scope of our investigation: (i) We first consider one extended body with full multipole structure in uniform motion in some suitably chosen global reference system. For this problem a co-moving inertial system of coordinates can be introduced where the metric, outside the body, admits an expansion in terms of Damour-Iyer moments. A Poincare transformation then yields the corresponding metric tensor in the global system in post-Minkowskian approximation. (ii) It will be argued why the global metric, exact to post-Minkowskian order, can be obtained by means of an instantaneous Poincare transformation for the case of pointlike mass-monopoles and spin-dipoles in arbitrary motion.
Analyzing the Flux Anomalies of the Large-Separation Lensed Quasar SDSS J1029+2623<|sep|>Using a high resolution radio image, we successfully resolve the two fold image components B and C of the quasar lens system SDSS J1029+2623. The flux anomalies associated with these two components in the optical regime persist, albeit less strongly, in our radio observations, suggesting that the cluster must be modeled by something more than a single central potential. We argue that placing substructure close to one of the components can account for a flux anomaly with negligible changes in the component positions. Our best fit model has a substructure mass of ~10^8 solar masses up to the mass-sheet degeneracy, located roughly 0.1 arcsecs West and 0.1 arcsecs North of component B. We demonstrate that a positional offset between the centers of the source components can explain the differences between the optical and radio flux ratios.
Self-Organizing Maps as a Storage and Transfer Mechanism in Reinforcement Learning<|sep|>The idea of reusing information from previously learned tasks (source tasks) for the learning of new tasks (target tasks) has the potential to significantly improve the sample efficiency reinforcement learning agents. In this work, we describe an approach to concisely store and represent learned task knowledge, and reuse it by allowing it to guide the exploration of an agent while it learns new tasks. In order to do so, we use a measure of similarity that is defined directly in the space of parameterized representations of the value functions. This similarity measure is also used as a basis for a variant of the growing self-organizing map algorithm, which is simultaneously used to enable the storage of previously acquired task knowledge in an adaptive and scalable manner.We empirically validate our approach in a simulated navigation environment and discuss possible extensions to this approach along with potential applications where it could be particularly useful.
Warm molecular gas and kinematics in the disc around HD 100546<|sep|>The disc around the Herbig Ae/Be star HD 100546 is one of the most extensively studied discs in the southern sky. Although there is a wealth of information about its dust content and composition, not much is known about its gas and large scale kinematics. We detect and study the molecular gas in the disc at spatial resolution from 7.7" to 18.9" using the APEX telescope. The lines 12CO J=7-6, J=6-5, J=3-2, 13CO J=3-2 and [C I] 3P2-3P1 are observed, diagnostic of disc temperature, size, chemistry, and kinematics. We use parametric disc models that reproduce the low-J 12CO emission from Herbig~Ae stars and vary the basic disc parameters - temperature, mass and size. Using the molecular excitation and radiative transfer code RATRAN we fit the observed spectral line profiles. Our observations are consistent with more than 0.001 Msun of molecular gas in a disc of approximately 400 AU radius in Keplerian rotation around a 2.5 Msun star, seen at an inclination of 50 degrees. The detected 12CO lines are dominated by gas at 30-70~K. The non-detection of the [C I] line indicates excess ultraviolet emission above that of a B9 type model stellar atmosphere. Asymmetry in the 12CO line emission suggests that one side of the outer disc is colder by 10-20~K than the other, possibly due to a shadow by a warped geometry of the inner disc. Pointing offsets, foreground cloud absorption and asymmetry in the disc extent are excluded scenarios. Efficient heating of the outer disc ensures that low- and high-J 12CO lines are dominated by the outermost disc regions, indicating a 400 AU radius. The 12CO J=6--5 line arises from a disc layer higher above disc midplane, and warmer by 15-20~K than the layer emitting the J=3--2 line. The existing models of discs around Herbig Ae stars, assuming a B9.5 type model stellar atmosphere overproduce the [CI] 3P2--3P1 line intensity from HD 100546 by an order of magnitude.
ProposalCLIP: Unsupervised Open-Category Object Proposal Generation via Exploiting CLIP Cues<|sep|>Object proposal generation is an important and fundamental task in computer vision. In this paper, we propose ProposalCLIP, a method towards unsupervised open-category object proposal generation. Unlike previous works which require a large number of bounding box annotations and/or can only generate proposals for limited object categories, our ProposalCLIP is able to predict proposals for a large variety of object categories without annotations, by exploiting CLIP (contrastive language-image pre-training) cues. Firstly, we analyze CLIP for unsupervised open-category proposal generation and design an objectness score based on our empirical analysis on proposal selection. Secondly, a graph-based merging module is proposed to solve the limitations of CLIP cues and merge fragmented proposals. Finally, we present a proposal regression module that extracts pseudo labels based on CLIP cues and trains a lightweight network to further refine proposals. Extensive experiments on PASCAL VOC, COCO and Visual Genome datasets show that our ProposalCLIP can better generate proposals than previous state-of-the-art methods. Our ProposalCLIP also shows benefits for downstream tasks, such as unsupervised object detection.
3D Numerical Simulations of f-Mode Propagation Through Magnetic Flux Tubes<|sep|>Three-dimensional numerical simulations have been used to study the scattering of a surface-gravity wave packet by vertical magnetic flux tubes, with radii from 200 km to 3 Mm, embedded in stratified polytropic atmosphere. The scattered wave was found to consist primarily of m=0 (axisymmetric) and m=1 modes. It was found that the ratio of the amplitude of these two modes is strongly dependant on the radius of the flux tube: The kink mode is the dominant mode excited in tubes with a small radius while the sausage mode is dominant for large tubes. Simulations of this type provide a simple, efficient and robust way to start understanding the seismic signature of flux tubes, which have recently began to be observed.
You Only Need One Model for Open-domain Question Answering<|sep|>Recent approaches to Open-domain Question Answering refer to an external knowledge base using a retriever model, optionally rerank passages with a separate reranker model and generate an answer using another reader model. Despite performing related tasks, the models have separate parameters and are weakly-coupled during training. We propose casting the retriever and the reranker as internal passage-wise attention mechanisms applied sequentially within the transformer architecture and feeding computed representations to the reader, with the hidden representations progressively refined at each stage. This allows us to use a single question answering model trained end-to-end, which is a more efficient use of model capacity and also leads to better gradient flow. We present a pre-training method to effectively train this architecture and evaluate our model on the Natural Questions and TriviaQA open datasets. For a fixed parameter budget, our model outperforms the previous state-of-the-art model by 1.0 and 0.7 exact match scores.
Efficient Cavity Searching for Gene Network of Influenza A Virus<|sep|>High order structures (cavities and cliques) of the gene network of influenza A virus reveal tight associations among viruses during evolution and are key signals that indicate viral cross-species infection and cause pandemics. As indicators for sensing the dynamic changes of viral genes, these higher order structures have been the focus of attention in the field of virology. However, the size of the viral gene network is usually huge, and searching these structures in the networks introduces unacceptable delay. To mitigate this issue, in this paper, we propose a simple-yet-effective model named HyperSearch based on deep learning to search cavities in a computable complex network for influenza virus genetics. Extensive experiments conducted on a public influenza virus dataset demonstrate the effectiveness of HyperSearch over other advanced deep-learning methods without any elaborated model crafting. Moreover, HyperSearch can finish the search works in minutes while 0-1 programming takes days. Since the proposed method is simple and easy to be transferred to other complex networks, HyperSearch has the potential to facilitate the monitoring of dynamic changes in viral genes and help humans keep up with the pace of virus mutations.
Strategies for spectroscopy on Extremely Large Telescopes. I - Image Slicing<|sep|>One of the problems of producing spectrographs for Extremely Large Telescopes (ELTs) is that the beam size is required to scale with telescope aperture if all other parameters are held constant, leading to enormous size and implied cost. This is a particular problem for image sizes much larger than the diffraction limit, as is likely to be the case if Adaptive Optics systems are not initially able to deliver highly corrected images over the full field of the instrument or if signal/noise considerations require large spatial samples. In this case, there is a potential advantage in image slicing to reduce the effective slitwidth and hence the beam size. However, this implies larger detectors and oversizing of the optics which may cancel out the advantage. By the means of a toy model of a spectrograph whose dimensions are calibrated using existing instruments, the size and relative cost of spectrographs for ELTs have been estimated. Using a range of scaling laws derived from the reference instruments, it is possible to estimate the uncertainties in the predictions and to explore the consequences of different design strategies. The model predicts major cost savings (2 - 100x) by slicing with factors of 5-20 depending on the type of spectrograph. The predictions suggest that it is better to accommodate the multiplicity of slices within a single spectrograph rather than distribute them among smaller, cheaper replicas in a parallel architecture, but the replication option provides an attractive upgrade path to integral field spectroscopy (IFS) as the input image quality is improved... [Full abstract in text]
Capacity Region of Vector Gaussian Interference Channels with Generally Strong Interference<|sep|>An interference channel is said to have strong interference if for all input distributions, the receivers can fully decode the interference. This definition of strong interference applies to discrete memoryless, scalar and vector Gaussian interference channels. However, there exist vector Gaussian interference channels that may not satisfy the strong interference condition but for which the capacity can still be achieved by jointly decoding the signal and the interference. This kind of interference is called generally strong interference. Sufficient conditions for a vector Gaussian interference channel to have generally strong interference are derived. The sum-rate capacity and the boundary points of the capacity region are also determined.
Towards time-dependent, non-equilibrium charge-transfer force fields: Contact electrification and history-dependent dissociation limits<|sep|>Force fields uniquely assign interatomic forces for a given set of atomic coordinates. The underlying assumption is that electrons are in their quantum-mechanical ground state or in thermal equilibrium. However, there is an abundance of cases where this is unjustified because the system is only locally in equilibrium. In particular, the fractional charges of atoms, clusters, or solids tend to not only depend on atomic positions but also on how the system reached its state. For example, the charge of an isolated solid -- and thus the forces between atoms in that solid -- usually depends on the counterbody with which it has last formed contact. Similarly, the charge of an atom, resulting from the dissociation of a molecule, can differ for different solvents in which the dissociation took place. In this paper we demonstrate that such charge-transfer history effects can be accounted for by assigning discrete oxidation states to atoms. With our method, an atom can donate an integer charge to another, nearby atom to change its oxidation state as in a redox reaction. In addition to integer charges, atoms can exchange "partial charges" which are determined with the split charge equilibration method.
Expolring Architectures for CNN-Based Word Spotting<|sep|>The goal in word spotting is to retrieve parts of document images which are relevant with respect to a certain user-defined query. The recent past has seen attribute-based Convolutional Neural Networks take over this field of research. As is common for other fields of computer vision, the CNNs used for this task are already considerably deep. The question that arises, however, is: How complex does a CNN have to be for word spotting? Are increasingly deeper models giving increasingly bet- ter results or does performance behave asymptotically for these architectures? On the other hand, can similar results be obtained with a much smaller CNN? The goal of this paper is to give an answer to these questions. Therefore, the recently successful TPP- PHOCNet will be compared to a Residual Network, a Densely Connected Convolutional Network and a LeNet architecture empirically. As will be seen in the evaluation, a complex model can be beneficial for word spotting on harder tasks such as the IAM Offline Database but gives no advantage for easier benchmarks such as the George Washington Database.
Exact (1+1)-dimensional flows of a perfect fluid<|sep|>We present a general solution of relativistic (1+1)-dimensional hydrodynamics for a perfect fluid flowing along the longitudinal direction as a function of time, uniformly in transverse space. The Khalatnikov potential is expressed as a linear combination of two generating functions with polynomial coefficients of 2 variables. The polynomials, whose algebraic equations are solved, define an infinite-dimensional basis of solutions. The kinematics of the (1+1)-dimensional flow are reconstructed from the potential.
Welsch Based Multiview Disparity Estimation<|sep|>In this work, we explore disparity estimation from a high number of views. We experimentally identify occlusions as a key challenge for disparity estimation for applications with high numbers of views. In particular, occlusions can actually result in a degradation in accuracy as more views are added to a dataset. We propose the use of a Welsch loss function for the data term in a global variational framework for disparity estimation. We also propose a disciplined warping strategy and a progressive inclusion of views strategy that can reduce the need for coarse to fine strategies that discard high spatial frequency components from the early iterations. Experimental results demonstrate that the proposed approach produces superior and/or more robust estimates than other conventional variational approaches.
Learning to generate one-sentence biographies from Wikidata<|sep|>We investigate the generation of one-sentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.
The Consistency of Fermi-LAT Observations of the Galactic Center with a Millisecond Pulsar Population in the Central Stellar Cluster<|sep|>I show that the spectrum and morphology of a recent Fermi-LAT observation of the Galaxy center are consistent with a millisecond pulsar population in the nuclear Central stellar cluster of the Milky Way. The Galaxy Center gamma-ray spectrum is consistent with the spectrum of four of eight globular clusters that have been detected in the gamma-ray. A dark matter annihilation interpretation cannot be ruled out, though no unique features exist that would require this conclusion.
A slow gravity compensated Atom Laser<|sep|>We report on a slow guided atom laser beam outcoupled from a Bose-Einstein condensate of 87Rb atoms in a hybrid trap. The acceleration of the atom laser beam can be controlled by compensating the gravitational acceleration and we reach residual accelerations as low as 0.0027 g. The outcoupling mechanism allows for the production of a constant flux of 4.5x10^6 atoms per second and due to transverse guiding we obtain an upper limit for the mean beam width of 4.6 \mu\m. The transverse velocity spread is only 0.2 mm/s and thus an upper limit for the beam quality parameter is M^2=2.5. We demonstrate the potential of the long interrogation times available with this atom laser beam by measuring the trap frequency in a single measurement. The small beam width together with the long evolution and interrogation time makes this atom laser beam a promising tool for continuous interferometric measurements.
Analytic solutions for marginal deformations in open superstring field theory<|sep|>We extend the calculable analytic approach to marginal deformations recently developed in open bosonic string field theory to open superstring field theory formulated by Berkovits. We construct analytic solutions to all orders in the deformation parameter when operator products made of the marginal operator and the associated superconformal primary field are regular.
Breakdown of light transport models in photonic scattering slabs with strong absorption and anisotropy<|sep|>The radiative transfer equation (RTE) models the transport of light inside photonic scattering samples such as paint, foam and tissue. Analytic approximations to solve the RTE fail for samples with strong absorption and dominant anisotropic scattering and predict unphysical negative energy densities and the diffuse flux in the wrong direction. Here we fully characterize the unphysical regions of three popular approximations to the RTE for a slab, namely the $P_1$ approximation (or diffusion approximation), the $P_3$ approximation, and a popular modification to $P_3$ that corrects the forward scattering in the approximation. We find that the delta function correction to $P_3$ eliminates the unphysical range in the forward scattering. In addition, we compare the predictions of these analytical methods to exact Monte Carlo simulations for the physical and unphysical regions. We present maps of relative errors for the albedo and the anisotropy of the scatterers for a realistic index contrast typical of a polymer slab in air and optical thickness. The relative error maps provide a guideline for the accuracy of the analytical methods to interpret experiments on light transport in photonic scattering slabs. Our results show that the $P_1$ approximation is significantly inaccurate to extract transport parameters unless the sample scatters purely isotropic and elastic. The $P_3$ approximation exceeds $P_1$ in terms of accuracy in its physical range for moderate absorption, and the $P_3$ with the delta function correction is the most accurate approximation considered here for the forward direction.
BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification<|sep|>Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the ever increasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERTMLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work.
Enhancing or suppressing spin Hall effect of light in layered nanostructures<|sep|>The spin Hall effect (SHE) of light in layered nanostructures is investigated theoretically in this paper. A general propagation model describing the spin-dependent transverse splitting in the SHE of light is established from the viewpoint of classical electrodynamics. We show that the transverse displacement of wave-packet centroid can be tuned to either a negative or a positive value, or even zero, by just adjusting the structure parameters, suggesting that the SHE of light in layered nanostructures can be enhanced or suppressed in a desired way. The inherent secret behind this interesting phenomenon is the optical Fabry-Perot resonance in the layered nanostructure. We believe that these findings will open the possibility for developing new nano-photonic devices.
Reflexive spatial behaviour does not guarantee evolution advantage in prey--predator communities<|sep|>We consider the model of spatially distributed population consisting of two species with "\textsl{predator\,--\,prey}" interaction; each of the species occupies two stations. Transfer of individuals between the stations (migration) is not random and yields the maximization of a net reproduction of each species. Besides, each species implements reflexive behavior strategy to determine the optimal migration flow.
Variant-based Equational Unification under Constructor Symbols<|sep|>Equational unification of two terms consists of finding a substitution that, when applied to both terms, makes them equal modulo some equational properties. A narrowing-based equational unification algorithm relying on the concept of the variants of a term is available in the most recent version of Maude, version 3.0, which provides quite sophisticated unification features. A variant of a term t is a pair consisting of a substitution sigma and the canonical form of tsigma. Variant-based unification is decidable when the equational theory satisfies the finite variant property. However, this unification procedure does not take into account constructor symbols and, thus, may compute many more unifiers than the necessary or may not be able to stop immediately. In this paper, we integrate the notion of constructor symbol into the variant-based unification algorithm. Our experiments on positive and negative unification problems show an impressive speedup.
Contrast stability and "stripe" formation in Scanning Tunnelling Microscopy imaging of highly oriented pyrolytic graphite: The role of STM-tip orientations<|sep|>Highly oriented pyrolytic graphite (HOPG) is an important substrate in many technological applications and is routinely used as a standard in Scanning Tunnelling Microscopy (STM) calibration, which makes the accurate interpretation of the HOPG STM contrast of great fundamental and applicative importance. We demonstrate by STM simulations based on electronic structure obtained from first principles that the relative local orientation of the STM-tip apex with respect to the HOPG substrate has a considerable effect on the HOPG STM contrast. Importantly for experimental STM analysis of HOPG, the simulations indicate that local tip-rotations maintaining a major contribution of the $d_{3z^2-r^2}$ tip-apex state to the STM current affect only the secondary features of the HOPG STM contrast resulting in "stripe" formation and leaving the primary contrast unaltered. Conversely, tip-rotations leading to enhanced contributions from $m\ne 0$ tip-apex electronic states can cause a triangular-hexagonal change in the primary contrast. We also report a comparison of two STM simulation models with experiments in terms of bias-voltage-dependent STM topography brightness correlations, and discuss our findings for the HOPG(0001) surface in combination with tungsten tip models of different sharpnesses and terminations.
Determining topological order from a local ground state correlation function<|sep|>Topological insulators are physically distinguishable from normal insulators only near edges and defects, while in the bulk there is no clear signature to their topological order. In this work we show that the Z index of topological insulators and the Z index of the integer quantum Hall effect manifest themselves locally. We do so by providing an algorithm for determining these indices from a local equal time ground-state correlation function at any convenient boundary conditions. Our procedure is unaffected by the presence of disorder and can be naturally generalized to include weak interactions. The locality of these topological indices implies bulk-edge correspondence theorem.
Novel Collider and Dark Matter Phenomenology of a Top-philic Z'<|sep|>We consider extending the Standard Model by including an additional Abelian gauge group broken at low energies under which the right-handed top quark is the only effectively charged Standard Model fermion. The associated gauge boson $(Z')$ is then naturally top-philic and couples only to the rest of the SM particle content at loop-level or via kinetic mixing with the hypercharge gauge boson which is assumed to be small. Working at the effective theory level, we demonstrate that such a minimal extension allows for an improved fitting of the $\sim 2\sigma$ excess observed in $t\bar{t}h$ searches at the LHC in a region of parameter space that satisfies existing collider constraints. We also present the reach of the LHC at 13 TeV in constraining the relevant region of parameter space. Additionally we show that within the same framework a suitably chosen fermion charged only under the exotic Abelian group can, in the region of parameter space preferred by the $\bar{t}th$ measurements, simultaneously explain the dark matter relic density and the $\gamma$-ray excess at the galactic center observed by the Fermi-LAT experiment.
Quantitative probing: Validating causal models using quantitative domain knowledge<|sep|>We present quantitative probing as a model-agnostic framework for validating causal models in the presence of quantitative domain knowledge. The method is constructed as an analogue of the train/test split in correlation-based machine learning and as an enhancement of current causal validation strategies that are consistent with the logic of scientific discovery. The effectiveness of the method is illustrated using Pearl's sprinkler example, before a thorough simulation-based investigation is conducted. Limits of the technique are identified by studying exemplary failing scenarios, which are furthermore used to propose a list of topics for future research and improvements of the presented version of quantitative probing. The code for integrating quantitative probing into causal analysis, as well as the code for the presented simulation-based studies of the effectiveness of quantitative probing is provided in two separate open-source Python packages.
Learning to Selectively Transfer: Reinforced Transfer Learning for Deep Text Matching<|sep|>Deep text matching approaches have been widely studied for many applications including question answering and information retrieval systems. To deal with a domain that has insufficient labeled data, these approaches can be used in a Transfer Learning (TL) setting to leverage labeled data from a resource-rich source domain. To achieve better performance, source domain data selection is essential in this process to prevent the "negative transfer" problem. However, the emerging deep transfer models do not fit well with most existing data selection methods, because the data selection policy and the transfer learning model are not jointly trained, leading to sub-optimal training efficiency. In this paper, we propose a novel reinforced data selector to select high-quality source domain data to help the TL model. Specifically, the data selector "acts" on the source domain data to find a subset for optimization of the TL model, and the performance of the TL model can provide "rewards" in turn to update the selector. We build the reinforced data selector based on the actor-critic framework and integrate it to a DNN based transfer learning model, resulting in a Reinforced Transfer Learning (RTL) method. We perform a thorough experimental evaluation on two major tasks for text matching, namely, paraphrase identification and natural language inference. Experimental results show the proposed RTL can significantly improve the performance of the TL model. We further investigate different settings of states, rewards, and policy optimization methods to examine the robustness of our method. Last, we conduct a case study on the selected data and find our method is able to select source domain data whose Wasserstein distance is close to the target domain data. This is reasonable and intuitive as such source domain data can provide more transferability power to the model.
Dynamics of a Reaction-Diffusion Benthic-Drift Model with Strong Allee Effect Growth<|sep|>The dynamics of a reaction-diffusion-advection benthic-drift population model that links changes in the flow regime and habitat availability with population dynamics is studied. In the model, the stream is divided into drift zone and benthic zone, and the population is divided into two interacting compartments, individuals residing in the benthic zone and individuals dispersing in the drift zone. The benthic population growth is assumed to be of strong Allee effect type. The influence of flow speed and individual transfer rates between zones on the population persistence and extinction is considered, and the criteria of population persistence or extinction are formulated and proved.
Stream-orbit misalignment II: A new algorithm to constrain the Galactic potential<|sep|>In the first of these two papers we demonstrated that assuming streams delineate orbits can lead to order one errors in potential parameters for realistic Galactic potentials. Motivated by the need for an improvement on orbit-fitting, we now present an algorithm for constraining the Galactic potential using tidal streams without assuming that streams delineate orbits. This approach is independent of the progenitor mass so is valid for all observed tidal streams. The method makes heavy use of angle-action variables and seeks the potential which recovers the expected correlations in angle space. We demonstrate that the method can correctly recover the parameters of a simple two-parameter logarithmic potential by analysing an N-body simulation of a stream. We investigate the magnitude of the errors in observational data for which the method can still recover the correct potential and compare this to current and future errors in data. The errors in the observables of individual stars for current and near future data are shown to be too large for the direct use of this method, but when the data are averaged in bins on the sky, the resulting averaged data are accurate enough to constrain correctly the potential parameters for achievable observational errors. From pseudo-data with errors comparable to those that will be furnished in the era of Gaia (20 per cent distance errors, 1.2 mas/yr proper motion errors, and 10 km/s line-of-sight velocity errors) we recover the circular velocity, V_c=220 km/s, and the flattening of the potential, q=0.9, to be V_c=223+/-10km/s and q=0.91+/-0.09.
First-order Optimization for Superquantile-based Supervised Learning<|sep|>Classical supervised learning via empirical risk (or negative log-likelihood) minimization hinges upon the assumption that the testing distribution coincides with the training distribution. This assumption can be challenged in modern applications of machine learning in which learning machines may operate at prediction time with testing data whose distribution departs from the one of the training data. We revisit the superquantile regression method by proposing a first-order optimization algorithm to minimize a superquantile-based learning objective. The proposed algorithm is based on smoothing the superquantile function by infimal convolution. Promising numerical results illustrate the interest of the approach towards safer supervised learning.
Quiescent X-Ray/Optical Counterparts of the Black Hole Transient H 1705-250<|sep|>We report the result of a new Chandra observation of the black hole X-ray transient H 1705-250 in quiescence. H 1705-250 was barely detected in the new 50 ks Chandra observation. With 5 detected counts, we estimate the source quiescent luminosity to be Lx~9.1e30 erg/s in the 0.5-10 keV band (adopting a distance of 8.6 kpc). This value is in line with the quiescent luminosities found among other black hole X-ray binaries with similar orbital periods. By using images taken with the Faulkes Telescope North, we derive a refined position of H 1705-250. We also present the long-term lightcurve of the optical counterpart from 2006 to 2012, and show evidence for variability in quiescence.
DeepFaceEditing: Deep Face Generation and Editing with Disentangled Geometry and Appearance Control<|sep|>Recent facial image synthesis methods have been mainly based on conditional generative models. Sketch-based conditions can effectively describe the geometry of faces, including the contours of facial components, hair structures, as well as salient edges (e.g., wrinkles) on face surfaces but lack effective control of appearance, which is influenced by color, material, lighting condition, etc. To have more control of generated results, one possible approach is to apply existing disentangling works to disentangle face images into geometry and appearance representations. However, existing disentangling methods are not optimized for human face editing, and cannot achieve fine control of facial details such as wrinkles. To address this issue, we propose DeepFaceEditing, a structured disentanglement framework specifically designed for face images to support face generation and editing with disentangled control of geometry and appearance. We adopt a local-to-global approach to incorporate the face domain knowledge: local component images are decomposed into geometry and appearance representations, which are fused consistently using a global fusion module to improve generation quality. We exploit sketches to assist in extracting a better geometry representation, which also supports intuitive geometry editing via sketching. The resulting method can either extract the geometry and appearance representations from face images, or directly extract the geometry representation from face sketches. Such representations allow users to easily edit and synthesize face images, with decoupled control of their geometry and appearance. Both qualitative and quantitative evaluations show the superior detail and appearance control abilities of our method compared to state-of-the-art methods.
Quantum Communication and Quantum Multivariate Polynomial Interpolation<|sep|>The paper is devoted to the problem of multivariate polynomial interpolation and its application to quantum secret sharing. We show that using quantum Fourier transform one can produce the protocol for quantum secret sharing distribution.
Initiation of the detonation in the gravitationally confined detonation model of Type Ia supernovae<|sep|>We study the initiation of the detonation in the gravitationally confined detonation (GCD) model of Type Ia supernovae (SNe Ia). Initiation of the detonation occurs spontaneously in a region where the length scale of the temperature gradient extending from a flow (in which carbon burning is already occurring) into unburned fuel is commensurate to the range of critical length scales which have been derived from 1D simulations that resolve the initiation of a detonation. By increasing the maximum resolution in a truncated cone that encompasses this region, beginning somewhat before initiation of the detonation occurs, we successfully simulate in situ the first gradient-initiated detonation in a whole-star simulation. The detonation emerges when a compression wave overruns a pocket of fuel situated in a Kelvin-Helmholtz cusp at the leading edge of the inwardly directed jet of burning carbon. The compression wave pre-conditions the temperature in the fuel in such a way that the Zel'dovich gradient mechanism can operate and a detonation ensues. We explore the dependence of the length scale of the temperature gradient on spatial resolution and discuss the implications for the robustness of this detonation mechanism. We find that the time and the location at which initiation of the detonation occurs varies with resolution. In particular, initiation of a detonation had not yet occurred in our highest resolution simulation by the time we ended the simulation because of the computational demand it required. We suggest that the turbulent shear layer surrounding the inwardly directed jet provides the most favorable physical conditions, and therefore the most likely location, for initiation of a detonation in the GCD model.
A Model of the IEEE 802.11 DCF in Presence of Non Ideal Transmission Channel and Capture Effects<|sep|>In this paper, we provide a throughput analysis of the IEEE 802.11 protocol at the data link layer in non-saturated traffic conditions taking into account the impact of both transmission channel and capture effects in Rayleigh fading environment. Impacts of both non-ideal channel and capture become important in terms of the actual observed throughput in typical network conditions whereby traffic is mainly unsaturated, specially in an environment of high interference. We extend the multi-dimensional Markovian state transition model characterizing the behavior at the MAC layer by including transmission states that account for packet transmission failures due to errors caused by propagation through the channel, along with a state characterizing the system when there are no packets to be transmitted in the buffer of a station.
Distribution of spectral linear statistics on random matrices beyond the large deviation function -- Wigner time delay in multichannel disordered wires<|sep|>An invariant ensemble of $N\times N$ random matrices can be characterised by a joint distribution for eigenvalues $P(\lambda_1,\cdots,\lambda_N)$. The study of the distribution of linear statistics, i.e. of quantities of the form $L=(1/N)\sum_if(\lambda_i)$ where $f(x)$ is a given function, appears in many physical problems. In the $N\to\infty$ limit, $L$ scales as $L\sim N^\eta$, where the scaling exponent $\eta$ depends on the ensemble and the function $f$. Its distribution can be written under the form $P_N(s=N^{-\eta}\,L)\simeq A_{\beta,N}(s)\,\exp\big\{-(\beta N^2/2)\,\Phi(s)\big\}$, where $\beta\in\{1,\,2,\,4\}$ is the Dyson index. The Coulomb gas technique naturally provides the large deviation function $\Phi(s)$, which can be efficiently obtained thanks to a "thermodynamic identity" introduced earlier. We conjecture the pre-exponential function $A_{\beta,N}(s)$. We check our conjecture on several well controlled cases within the Laguerre and the Jacobi ensembles. Then we apply our main result to a situation where the large deviation function has no minimum (and $L$ has infinite moments)~: this arises in the statistical analysis of the Wigner time delay for semi-infinite multichannel disordered wires (Laguerre ensemble). The statistical analysis of the Wigner time delay then crucially depends on the pre-exponential function $A_{\beta,N}(s)$, which ensures the decay of the distribution for large argument.
A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin<|sep|>Bitcoin, with its ever-growing popularity, has demonstrated extreme price volatility since its origin. This volatility, together with its decentralised nature, make Bitcoin highly subjective to speculative trading as compared to more traditional assets. In this paper, we propose a multimodal model for predicting extreme price fluctuations. This model takes as input a variety of correlated assets, technical indicators, as well as Twitter content. In an in-depth study, we explore whether social media discussions from the general public on Bitcoin have predictive power for extreme price movements. A dataset of 5,000 tweets per day containing the keyword `Bitcoin' was collected from 2015 to 2021. This dataset, called PreBit, is made available online. In our hybrid model, we use sentence-level FinBERT embeddings, pretrained on financial lexicons, so as to capture the full contents of the tweets and feed it to the model in an understandable way. By combining these embeddings with a Convolutional Neural Network, we built a predictive model for significant market movements. The final multimodal ensemble model includes this NLP model together with a model based on candlestick data, technical indicators and correlated asset prices. In an ablation study, we explore the contribution of the individual modalities. Finally, we propose and backtest a trading strategy based on the predictions of our models with varying prediction threshold and show that it can used to build a profitable trading strategy with a reduced risk over a `hold' or moving average strategy.
Magneto-transport of graphene and quantum phase transitions in the quantum Hall regime<|sep|>We studied the magneto-transport in SiO2 substrate-supported monolayer graphene and the quantum phase transitions that characterize the quantum Hall regime, using magnetic fields up to 28T and temperatures down to 4K. The analysis of the temperature dependence of the Hall and longitudinal resistivity reveals new non-universalities of the critical exponents of the plateau-insulator transition. These exponent depends on the type of disorder that governs the electrical transport, which knowledge is important for the design and fabrication of new graphene nano-devices.
Measurement of the solar system acceleration using the Earth scale factor<|sep|>We propose an alternative method to detect the secular aberration drift induced by the solar system acceleration due to the attraction to the Galaxy centre. This method is free of the individual radio source proper motion caused by intrinsic structure variation. We developed a procedure to estimate the scale factor directly from very long baseline interferometry (VLBI) data analysis in a source-wise mode within a global solution. The scale factor is estimated for each reference radio source individually as a function of astrometric coordinates (right ascension and declination). This approach splits the systematic dipole effect and uncorrelated motions on the level of observational parameters. We processed VLBI observations from 1979.7 to 2016.5 to obtain the scale factor estimates for more than 4,000 reference radio sources. We show that the estimates highlight a dipole systematics aligned with the direction to the centre of the Galaxy. With this method we obtained a Galactocentric acceleration vector with an amplitude of 5.2 $\pm$ 0.2 \mu as/yr and direction $\alpha_G = 281\deg \pm 3\deg$ and $\delta_G = -35\deg \pm 3\deg$.
Structural correlations and phase separation in binary mixtures of charged and uncharged colloids<|sep|>Structural correlations between colloids in a binary mixture of charged and uncharged spheres are calculated using computer simulations of the primitive model with explicit microions. For aqueous suspensions in a solvent of large dielectric constant, the traditional Derjaguin-Landau-Vervey-Overbeek (DLVO) theory of linear screening, supplemented with hard core interactions, reproduces the structural correlations obtained in the full primitive model quantitatively. However for lower dielectric contrast, the increasing Coulomb coupling between the micro- and macroions results in strong deviations. We find a fluid-fluid phase separation into two regions either rich in charged or rich in uncharged particles which is not reproduced by DLVO theory. Our results are verifiable in scattering or real-space experiments on charged-uncharged mixtures of colloids or nanoparticles.
Particle generation through restrictive planes in GEANT4 simulations for potential applications of cosmic ray muon tomography<|sep|>In this study, by attempting to resolve the angular complication during the particle generation for the muon tomography applications in the GEANT4 simulations, we exhibit an unconventional methodology that is hinged on the direction limitation via the vectorial construction from the generation location to the restriction area rather than using a certain angular distribution or interval. In other words, we favor a momentum direction that is determined by a vector constructed between an initial point randomly chosen on a generative point/plane and a latter point arbitrarily selected on a restrictive plane of the same dimensions with the basal cross section of the volume-of-interest (VOI). On account of setting out such a generation scheme, we optimize the particle loss by keeping an angular disparity that is directly dependent on the VOI geometry as well as the vertical position of the restrictive plane for a tomographic system of a finite size. We demonstrate our strategy for a set of target materials including aluminum, copper, iron, lead, and uranium with a dimension of 40$\times$10$\times$40 $\rm cm^{3}$ over three restrictive planes of different positions by using a discrete energy spectrum between 0.1 and 8 GeV and we compute the scattering angle, the number of absorption, and the particle loss. Upon our simulation outcomes, we show that the particle generation by means of restrictive planes is an effective strategy that is flexible towards a variety of computational objectives in the GEANT4 simulations.
Robust Tracking Control for Constrained Robots<|sep|>In this paper, a novel robust tracking control law is proposed for constrained robots under unknown stiffness environment. The stability and the robustness of the controller are proved using a Lyapunov-based approach where the relationship between the error dynamics of the robotic system and its energy is investigated. Finally, a 3DOF constrained robotic arm is used to prove the stability, the robustness and the safety of the proposed approach.
Lowest-ID with Adaptive ID Reassignment: A Novel Mobile Ad-Hoc Networks Clustering Algorithm<|sep|>Clustering is a promising approach for building hierarchies and simplifying the routing process in mobile ad-hoc network environments. The main objective of clustering is to identify suitable node representatives, i.e. cluster heads (CHs), to store routing and topology information and maximize clusters stability. Traditional clustering algorithms suggest CH election exclusively based on node IDs or location information and involve frequent broadcasting of control packets, even when network topology remains unchanged. More recent works take into account additional metrics (such as energy and mobility) and optimize initial clustering. However, in many situations (e.g. in relatively static topologies) re-clustering procedure is hardly ever invoked; hence initially elected CHs soon reach battery exhaustion. Herein, we introduce an efficient distributed clustering algorithm that uses both mobility and energy metrics to provide stable cluster formations. CHs are initially elected based on the time and cost-efficient lowest-ID method. During clustering maintenance phase though, node IDs are re-assigned according to nodes mobility and energy status, ensuring that nodes with low-mobility and sufficient energy supply are assigned low IDs and, hence, are elected as CHs. Our algorithm also reduces control traffic volume since broadcast period is adjusted according to nodes mobility pattern: we employ infrequent broadcasting for relative static network topologies, and increase broadcast frequency for highly mobile network configurations. Simulation results verify that energy consumption is uniformly distributed among network nodes and that signaling overhead is significantly decreased.
Interferometric probe of paired states<|sep|>We propose a new method for detecting paired states in either bosonic or fermionic systems using interference experiments with independent or weakly coupled low dimensional systems. We demonstrate that our method can be used to detect both the FFLO and the d-wave paired states of fermions, as well as quasicondensates of singlet pairs for polar F=1 atoms in two dimensional systems. We discuss how this method can be used to perform phase-sensitive determination of the symmetry of the pairing amplitude.
Radio-Optical Galaxy Shape Correlations in the COSMOS Field<|sep|>We investigate the correlations in galaxy shapes between optical and radio wavelengths using archival observations of the COSMOS field. Cross-correlation studies between different wavebands will become increasingly important for precision cosmology as future large surveys may be dominated by systematic rather than statistical errors. In the case of weak lensing, galaxy shapes must be measured to extraordinary accuracy (shear systematics of $< 0.01\%$) in order to achieve good constraints on dark energy parameters. By using shape information from overlapping surveys in optical and radio bands, robustness to systematics may be significantly improved without loss of constraining power. Here we use HST-ACS optical data, VLA radio data, and extensive simulations to investigate both our ability to make precision measurements of source shapes from realistic radio data, and to constrain the intrinsic astrophysical scatter between the shapes of galaxies as measured in the optical and radio wavebands. By producing a new image from the VLA-COSMOS L-band radio visibility data that is well suited to galaxy shape measurements, we are able to extract precise measurements of galaxy position angles. Comparing to corresponding measurements from the HST optical image, we set a lower limit on the intrinsic astrophysical scatter in position angles, between the optical and radio bands, of $\sigma_\alpha > 0.212\pi$ radians (or $38.2^{\circ}$) at a $95\%$ confidence level.
Automated One-Loop Calculations with GoSam<|sep|>We present the program package GoSam which is designed for the automated calculation of one-loop amplitudes for multi-particle processes in renormalisable quantum field theories. The amplitudes, which are generated in terms of Feynman diagrams, can be reduced using either D-dimensional integrand-level decomposition or tensor reduction. GoSam can be used to calculate one-loop QCD and/or electroweak corrections to Standard Model processes and offers the flexibility to link model files for theories Beyond the Standard Model. A standard interface to programs calculating real radiation is also implemented. We demonstrate the flexibility of the program by presenting examples of processes with up to six external legs attached to the loop.
Conductivity of two-dimensional narrow gap semiconductors subjected to strong Coulomb disorder<|sep|>In the ideal disorder-free situation, a two-dimensional band gap insulator has an activation energy for conductivity equal to half the band gap $\Delta$. But transport experiments usually exhibit a much smaller activation energy at low temperature, and the relation between this activation energy and $\Delta$ is unclear. Here we consider the temperature-dependent conductivity of a two-dimensional insulator on a substrate containing Coulomb impurities, with random potential amplitude $\Gamma \gg \Delta$. We show that the conductivity generically exhibits three regimes of conductivity, and only the highest temperature regime exhibits an activation energy that reflects the band gap. At lower temperatures, the conduction proceeds through activated hopping or Efros-Shklovskii variable-range hopping between electron and hole puddles created by the disorder. We show that the activation energy and characteristic temperature associated with these processes steeply collapse near a critical impurity concentration. Larger concentrations lead to an exponentially small activation energy and exponentially long localization length, which in mesoscopic samples can appear as a disorder-induced insulator-to-metal transition. We also arrive at a similar steep disorder driven insulator-metal transition in thin films of three-dimensional topological insulators with large dielectric constant, for which Coulomb impurities inside the film create a large disorder potential due to confinement of their electric field inside the film.
Reflection and Rotation Symmetry Detection via Equivariant Learning<|sep|>The inherent challenge of detecting symmetries stems from arbitrary orientations of symmetry patterns; a reflection symmetry mirrors itself against an axis with a specific orientation while a rotation symmetry matches its rotated copy with a specific orientation. Discovering such symmetry patterns from an image thus benefits from an equivariant feature representation, which varies consistently with reflection and rotation of the image. In this work, we introduce a group-equivariant convolutional network for symmetry detection, dubbed EquiSym, which leverages equivariant feature maps with respect to a dihedral group of reflection and rotation. The proposed network is built end-to-end with dihedrally-equivariant layers and trained to output a spatial map for reflection axes or rotation centers. We also present a new dataset, DENse and DIverse symmetry (DENDI), which mitigates limitations of existing benchmarks for reflection and rotation symmetry detection. Experiments show that our method achieves the state of the arts in symmetry detection on LDRS and DENDI datasets.
Feature Guided Search for Creative Problem Solving Through Tool Construction<|sep|>Robots in the real world should be able to adapt to unforeseen circumstances. Particularly in the context of tool use, robots may not have access to the tools they need for completing a task. In this paper, we focus on the problem of tool construction in the context of task planning. We seek to enable robots to construct replacements for missing tools using available objects, in order to complete the given task. We introduce the Feature Guided Search (FGS) algorithm that enables the application of existing heuristic search approaches in the context of task planning, to perform tool construction efficiently. FGS accounts for physical attributes of objects (e.g., shape, material) during the search for a valid task plan. Our results demonstrate that FGS significantly reduces the search effort over standard heuristic search approaches by approximately 93% for tool construction.
A new approach for numerical simulation of the time-dependent Ginzburg-Landau equations<|sep|>We introduce a new approach for finite element simulations of the time-dependent Ginzburg-Landau equations (TDGL) in a general curved polygon, possibly with reentrant corners. Specifically, we reformulate the TDGL into an equivalent system of equations by decomposing the magnetic potential to the sum of its divergence-free and curl-free parts, respectively. Numerical simulations of vortex dynamics show that, in a domain with reentrant corners, the new approach is much more stable and accurate than the old approaches of solving the TDGL directly (under either the temporal gauge or the Lorentz gauge); in a convex domain, the new approach gives comparably accurate solutions as the old approaches.
An Improved Traffic Matrix Decomposition Method with Frequency-Domain Regularization<|sep|>We propose a novel network traffic matrix decomposition method named Stable Principal Component Pursuit with Frequency-Domain Regularization (SPCP-FDR), which improves the Stable Principal Component Pursuit (SPCP) method by using a frequency-domain noise regularization function. An experiment demonstrates the feasibility of this new decomposition method.
Penalized Push-Sum Algorithm for Constrained Distributed Optimization with Application to Energy Management in Smart Grid<|sep|>We study distributed convex constrained optimization on a time-varying multi-agent network. Each agent has access to its own local cost function, its local constraints, and its instant number of out-neighbors. The collective goal is to minimize the sum of the cost functions over the set of all constraints. We utilize the push-sum protocol to be able to solve this distributed optimization problem. We adapt the push-sum optimization algorithm, which has been studied in context of unconstrained optimization so far, to convex constrained optimization by introducing an appropriate choice of penalty functions and penalty parameters. Under some additional technical assumptions on the gradients we prove convergence of the distributed penalty-based push-sum algorithm to the optimal value of the global objective function. We apply the proposed penalty-based push-sum algorithm to the problem of distributed energy management in smart grid and discuss the advantages of this novel procedure in comparison with existing ones.
Tracking without bells and whistles<|sep|>The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.
SenTion: A framework for Sensing Facial Expressions<|sep|>Facial expressions are an integral part of human cognition and communication, and can be applied in various real life applications. A vital precursor to accurate expression recognition is feature extraction. In this paper, we propose SenTion: A framework for sensing facial expressions. We propose a novel person independent and scale invariant method of extracting Inter Vector Angles (IVA) as geometric features, which proves to be robust and reliable across databases. SenTion employs a novel framework of combining geometric (IVA's) and appearance based features (Histogram of Gradients) to create a hybrid model, that achieves state of the art recognition accuracy. We evaluate the performance of SenTion on two famous face expression data set, namely: CK+ and JAFFE; and subsequently evaluate the viability of facial expression systems by a user study. Extensive experiments showed that SenTion framework yielded dramatic improvements in facial expression recognition and could be employed in real-world applications with low resolution imaging and minimal computational resources in real-time, achieving 15-18 fps on a 2.4 GHz CPU with no GPU.
Sharp recovery bounds for convex demixing, with applications<|sep|>Demixing refers to the challenge of identifying two structured signals given only the sum of the two signals and prior information about their structures. Examples include the problem of separating a signal that is sparse with respect to one basis from a signal that is sparse with respect to a second basis, and the problem of decomposing an observed matrix into a low-rank matrix plus a sparse matrix. This paper describes and analyzes a framework, based on convex optimization, for solving these demixing problems, and many others. This work introduces a randomized signal model which ensures that the two structures are incoherent, i.e., generically oriented. For an observation from this model, this approach identifies a summary statistic that reflects the complexity of a particular signal. The difficulty of separating two structured, incoherent signals depends only on the total complexity of the two structures. Some applications include (i) demixing two signals that are sparse in mutually incoherent bases; (ii) decoding spread-spectrum transmissions in the presence of impulsive errors; and (iii) removing sparse corruptions from a low-rank matrix. In each case, the theoretical analysis of the convex demixing method closely matches its empirical behavior.
Full $\mathcal{O}(\alpha)$ electroweak radiative corrections to $e^+e^- \rightarrow t \bar{t} \gamma$ with GRACE-Loop<|sep|>We present the full $\mathcal{O}(\alpha)$ electroweak radiative corrections to the process $e^+e^- \rightarrow t \bar{t} \gamma$ at the International Linear Collider (ILC). The computation is performed with the help of the GRACE-Loop system. We present the total cross-section and the top quark forward-backward asymmetry ($A_{FB}$) as a function of the center-of-mass energy and compare them with the process $e^+e^- \rightarrow t \bar{t}$. We find that the value of $A_{FB}$ in $t \bar{t} \gamma$ production is larger than $A_{FB}$ in $t\bar{t}$ production. It is an important result for the measurement of the top quark forward-backward asymmetry at the ILC. Applying a structure function method, we also subtract the QED correction to gain the genuine weak correction in both the $\alpha$ scheme and the $G_{\mu}$ scheme ($\delta_{W}^{G_{\mu}}$). We obtain numerical values for $\delta_{W}^{G_{\mu}}$ which are changing from 2% to -24% when we vary the center-of-mass energy from 360 GeV to 1 TeV.
Variance Based Algorithm for Grouped-Subcarrier Allocation in OFDMA Wireless Systems<|sep|>In this paper, a reduced complexity algorithm is proposed for grouped-subcarriers and power allocation in the downlink of OFDMA packet access wireless systems. The available subcarriers for data communication are grouped into partitions (groups) where each group is defined as a subchannel. The scheduler located at the base station allocates subchannels to users based on the variance of subchannel gains. The proposed algorithm for group allocation is a two-step algorithm that allocates groups to users based on the descending order of their variances to resolve the conflicting selection problem, followed by a step of fairness proportionality enhancement. To reduce the feedback burden and the complexity of the power allocation algorithm, each user feeds back the CSI on each group if the variance of gains of subcarriers inside it is less than a predefined threshold. To Show the performance of the proposed scheme, a selection of simulation results is presented.
Y(9.46 GeV) and the gluon discovery (a critical recollection of PLUTO results)<|sep|>The hadronic decays of Y(9.46GeV) were first studied by PLUTO experiment at DORIS e+e- storage ring (DESY). To determine the contribution of PLUTO to the discovery of the gluon, as members of the collaboration, we have reconsidered all the material produced by it in 1978 and the first half of 1979. It results clearly that the experiment demonstrated the main decay of the Y resonance to be mediated by 3 gluons hadronizing into 3 jets. Jettiness resulted evident by the <P_T> with respect to the thrust axis, which was as observed by PLUTO itself at nearby continuum c.m.s. energies for 2-quark jet events. Instead, the average sphericity <S>, more topological variables and the momentum distribution showed a net difference with the same data, results compatible with jettiness only in case of more than 2 jets. Flatness as consequence of a 3-body decay (therefore 3 jets) was indicated by the low <P_out>, altogether a result independent of models. The charged multiplicity was observed to be larger than in the continuum and in case of MC 3 gluon jets fragmenting like quarks, as expected for gluon jets. In June 1979 PLUTO measured the matrix element of the 3-gluon decay to be quantitatively according QCD (even after hadronization, which does not obscure the perturbative predictions) and demonstrated the spin 1 nature of the gluon by excluding spin 0 and spin 1/2. The gluon hadronization like a quark jet, as in 3-gluon jet MC, was compatible with topological data and multiplicity; this was the first experimental study of (identified) gluon jets. The PLUTO results were confirmed both by other experiments at DORIS and later by more sophisticated detectors. At higher energies at PETRA the existence of gluons of spin 1 was confirmed by PLUTO and by 3 more experiments by measuring the gluon radiation, soft gluons by jet broadening, hard gluons by the emission of (now clearly visible) gluon jets by quarks.
Temperature dependence of low-energy phonons in magnetic nonsuperconducting TbNi2B2C<|sep|>We report temperature dependence of low-energy phonons in magnetic nonsuperconducting TbNi2B2C single crystals measured by inelastic neutron scattering. We observed a low-temperature softening and broadening of two phonon branches, qualitatively similar to that previously reported for superconducting RNi2B2C (R= rare earth, Y) compounds. This result suggests that the superconductivity in TbNi2B2C compounds is absent not because of a weak electron-phonon coupling but as a result of pairbreaking due to magnetism.
Semantic Hierarchy Preserving Deep Hashing for Large-scale Image Retrieval<|sep|>Deep hashing models have been proposed as an efficient method for large-scale similarity search. However, most existing deep hashing methods only utilize fine-level labels for training while ignoring the natural semantic hierarchy structure. This paper presents an effective method that preserves the classwise similarity of full-level semantic hierarchy for large-scale image retrieval. Experiments on two benchmark datasets show that our method helps improve the fine-level retrieval performance. Moreover, with the help of the semantic hierarchy, it can produce significantly better binary codes for hierarchical retrieval, which indicates its potential of providing more user-desired retrieval results.
Sector coupling via hydrogen to lower the cost of energy system decarbonization<|sep|>There is growing interest in hydrogen (H$_2$) use for long-duration energy storage in a future electric grid dominated by variable renewable energy (VRE) resources. Modelling the role of H$_2$ as grid-scale energy storage, often referred as "power-to-gas-to-power (P2G2P)" overlooks the cost-sharing and emission benefits from using the deployed H$_2$ production and storage assets to also supply H$_2$ for decarbonizing other end-use sectors where direct electrification may be challenged. Here, we develop a generalized modelling framework for co-optimizing energy infrastructure investment and operation across power and transportation sectors and the supply chains of electricity and H$_2$, while accounting for spatio-temporal variations in energy demand and supply. Applying this sector-coupling framework to the U.S. Northeast under a range of technology cost and carbon price scenarios, we find a greater value of power-to-H$_2$ (P2G) versus P2G2P routes. P2G provides flexible demand response, while the extra cost and efficiency penalties of P2G2P routes make the solution less attractive for grid balancing. The effects of sector-coupling are significant, boosting VRE generation by 12-55% with both increased capacities and reduced curtailments and reducing the total system cost (or levelized costs of energy) by 6-14% under 96% decarbonization scenarios. Both the cost savings and emission reductions from sector coupling increase with H$_2$ demand for other end-uses, more than doubling for a 96% decarbonization scenario as H$_2$ demand quadraples. Moreover, we found that the deployment of carbon capture and storage is more cost-effective in the H$_2$ sector because of the lower cost and higher utilization rate. These findings highlight the importance of using an integrated multi-sector energy system framework with multiple energy vectors in planning energy system decarbonization pathways.
Demand forecasting techniques for build-to-order lean manufacturing supply chains<|sep|>Build-to-order (BTO) supply chains have become common-place in industries such as electronics, automotive and fashion. They enable building products based on individual requirements with a short lead time and minimum inventory and production costs. Due to their nature, they differ significantly from traditional supply chains. However, there have not been studies dedicated to demand forecasting methods for this type of setting. This work makes two contributions. First, it presents a new and unique data set from a manufacturer in the BTO sector. Second, it proposes a novel data transformation technique for demand forecasting of BTO products. Results from thirteen forecasting methods show that the approach compares well to the state-of-the-art while being easy to implement and to explain to decision-makers.
Deeply Exploit Depth Information for Object Detection<|sep|>This paper addresses the issue on how to more effectively coordinate the depth with RGB aiming at boosting the performance of RGB-D object detection. Particularly, we investigate two primary ideas under the CNN model: property derivation and property fusion. Firstly, we propose that the depth can be utilized not only as a type of extra information besides RGB but also to derive more visual properties for comprehensively describing the objects of interest. So a two-stage learning framework consisting of property derivation and fusion is constructed. Here the properties can be derived either from the provided color/depth or their pairs (e.g. the geometry contour adopted in this paper). Secondly, we explore the fusion method of different properties in feature learning, which is boiled down to, under the CNN model, from which layer the properties should be fused together. The analysis shows that different semantic properties should be learned separately and combined before passing into the final classifier. Actually, such a detection way is in accordance with the mechanism of the primary neural cortex (V1) in brain. We experimentally evaluate the proposed method on the challenging dataset, and have achieved state-of-the-art performance.
Fast B-spline Curve Fitting by L-BFGS<|sep|>We propose a novel method for fitting planar B-spline curves to unorganized data points. In traditional methods, optimization of control points and foot points are performed in two very time-consuming steps in each iteration: 1) control points are updated by setting up and solving a linear system of equations; and 2) foot points are computed by projecting each data point onto a B-spline curve. Our method uses the L-BFGS optimization method to optimize control points and foot points simultaneously and therefore it does not need to perform either matrix computation or foot point projection in every iteration. As a result, our method is much faster than existing methods.
ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data<|sep|>Causal inference from observational data is a subject of active research and development in statistics and computer science. Many toolkits have been developed for this purpose that depends on statistical software. However, these toolkits do not scale to large datasets. In this paper we describe a suite of techniques for expressing causal inference tasks from observational data in SQL. This suite supports the state-of-the-art methods for causal inference and run at scale within a database engine. In addition, we introduce several optimization techniques that significantly speedup causal inference, both in the online and offline setting. We evaluate the quality and performance of our techniques by experiments of real datasets.
PGPE theory of finite temperature collective modes for a trapped Bose gas<|sep|>We develop formalism based on the projected Gross Pitaevskii equation to simulate the finite temperature collective mode experiments of Jin et al. [PRL 78, 764 (1997)]. We examine the $m=0$ and $m=2$ quadrupolar modes on the temperature range $0.51T_c-0.83T_c$ and calculate the frequencies of, and phase between, the condensate and noncondensate modes, and the condensate mode damping rate. This study is the first quantitative comparison of the projected Gross-Pitaevskii equation to experimental results in a dynamical regime.
Tale of tails using rule augmented sequence labeling for event extraction<|sep|>The problem of event extraction is a relatively difficult task for low resource languages due to the non-availability of sufficient annotated data. Moreover, the task becomes complex for tail (rarely occurring) labels wherein extremely less data is available. In this paper, we present a new dataset (InDEE-2019) in the disaster domain for multiple Indic languages, collected from news websites. Using this dataset, we evaluate several rule-based mechanisms to augment deep learning based models. We formulate our problem of event extraction as a sequence labeling task and perform extensive experiments to study and understand the effectiveness of different approaches. We further show that tail labels can be easily incorporated by creating new rules without the requirement of large annotated data.
Nonequilibrium Green's function theory for predicting device-to-device variability<|sep|>Due to random dopant fluctuations, the device-to-device variability is a serious challenge to emerging nanoelectronics. In this work we present theoretical formalisms and numerical simulations of quantum transport variability, based on the nonequilibrium Green's functions and the multiple scattering theory. We have developed a general formalism using the diagrammatic technique within the coherent potential approximation (CPA) that can be applied to a wide range of disorder concentrations. In addition, we have developed a method by using a perturbative expansion within the low concentration approximation (LCA) that is extremely useful for typical nanoelectronic devices having low dopant concentration. Applying both formalisms, transport fluctuations due to random impurities can be predicted without lengthy brute force computation of ensemble of devices structures. Numerical implementations of the formalisms are demonstrated using both tight-binding models and first principles models.
Behaviour of light transmission channels in random media with inhomogeneous disorder<|sep|>We present a numerical study on the light transport properties and statistics of transmission channels in random media with inhomogeneous disorder. For the case of longitudinal inhomogeneity of disorder we find that the statistics of the transmission channels is independent of the inhomogeneity and the system can be equivalent to a counterpart with homogeneous disorder strength, both of which have the same statistical distribution of the transmission channels. However, for the case of transverse inhomogeneity of disorder, such equivalence does not exist, moreover, the transmission eigenvalues are pushed to the two ends of the distribution and the distribution of the total transmission is broadened since the spatial structure gives rise to larger and smaller transmitted incident channels.
Determining the outcome of cosmic bubble collisions in full General Relativity<|sep|>Cosmic bubble collisions provide an important possible observational window on the dynamics of eternal inflation. In eternal inflation, our observable universe is contained in one of many bubbles formed from an inflating metastable vacuum. The collision between bubbles can leave a detectable imprint on the cosmic microwave background radiation. Although phenomenological models of the observational signature have been proposed, to make the theory fully predictive one must determine the bubble collision spacetime, and thus the cosmological observables, from a scalar field theory giving rise to eternal inflation. Because of the intrinsically non-linear nature of the bubbles and their collision, this requires a numerical treatment incorporating General Relativity. In this paper, we present results from numerical simulations of bubble collisions in full General Relativity. These simulations allow us to accurately determine the outcome of bubble collisions, and examine their effect on the cosmology inside a bubble universe. We confirm the validity of a number of approximations used in previous analytic work, and identify qualitatively new features of bubble collision spacetimes. Both vacuum bubbles and bubbles containing a realistic inflationary cosmology are studied. We identify the constraints on the scalar field potential that must be satisfied in order to obtain collisions that are consistent with our observed cosmology, yet leave detectable signatures.
Multi-granularity Item-based Contrastive Recommendation<|sep|>Contrastive learning (CL) has shown its power in recommendation. However, most CL-based recommendation models build their CL tasks merely focusing on the user's aspects, ignoring the rich diverse information in items. In this work, we propose a novel Multi-granularity item-based contrastive learning (MicRec) framework for the matching stage (i.e., candidate generation) in recommendation, which systematically introduces multi-aspect item-related information to representation learning with CL. Specifically, we build three item-based CL tasks as a set of plug-and-play auxiliary objectives to capture item correlations in feature, semantic and session levels. The feature-level item CL aims to learn the fine-grained feature-level item correlations via items and their augmentations. The semantic-level item CL focuses on the coarse-grained semantic correlations between semantically related items. The session-level item CL highlights the global behavioral correlations of items from users' sequential behaviors in all sessions. In experiments, we conduct both offline and online evaluations on real-world datasets, verifying the effectiveness and universality of three proposed CL tasks. Currently, MicRec has been deployed on a real-world recommender system, affecting millions of users. The source code will be released in the future.
Polaritonic Rabi and Josephson Oscillations<|sep|>The dynamics of coupled condensates is a wide-encompassing problem with relevance to superconductors, BECs in traps, superfluids, etc. Here, we provide a unified picture of this fundamental problem that includes i) detuning of the free energies, ii) different self-interaction strengths and iii) finite lifetime of the modes. At such, this is particularly relevant for the dynamics of polaritons, both for their internal dynamics between their light and matter constituents, as well as for the more conventional dynamics of two spatially separated condensates. Polaritons are short-lived, interact only through their material fraction and are easily detuned. At such, they bring several variations to their atomic counterpart. We show that the combination of these parameters results in important twists to the phenomenology of the Josephson effect, such as the behaviour of the relative phase (running or oscillating) or the occurrence of self-trapping. We undertake a comprehensive stability analysis of the fixed points on a normalized Bloch sphere, that allows us to provide a generalized criterion to identify the Rabi and Josephson regimes in presence of detuning and decay.
Multivariate Chebyshev Inequality with Estimated Mean and Variance<|sep|>A variant of the well-known Chebyshev inequality for scalar random variables can be formulated in the case where the mean and variance are estimated from samples. In this paper we present a generalization of this result to multiple dimensions where the only requirement is that the samples are independent and identically distributed. Furthermore, we show that as the number of samples tends to infinity our inequality converges to the theoretical multi-dimensional Chebyshev bound.
Iteration Complexity of Variational Quantum Algorithms<|sep|>There has been much recent interest in near-term applications of quantum computers. Variational quantum algorithms (VQA), wherein an optimization algorithm implemented on a classical computer evaluates a parametrized quantum circuit as an objective function, are a leading framework in this space. In this paper, we analyze the iteration complexity of VQA, that is, the number of steps VQA required until the iterates satisfy a surrogate measure of optimality. We argue that although VQA procedures incorporate algorithms that can, in the idealized case, be modeled as classic procedures in the optimization literature, the particular nature of noise in near-term devices invalidates the claim of applicability of off-the-shelf analyses of these algorithms. Specifically, the form of the noise makes the evaluations of the objective function via circuits biased, necessitating the perspective of convergence analysis of variants of these classical optimization procedures, wherein the evaluations exhibit systematic bias. We apply our reasoning to the most often used procedures, including SPSA the parameter shift rule, which can be seen as zeroth-order, or derivative-free, optimization algorithms with biased function evaluations. We show that the asymptotic rate of convergence is unaffected by the bias, but the level of bias contributes unfavorably to both the constant therein, and the asymptotic distance to stationarity.
Searching for Intermediate Mass Black Holes in the Milky Way's galactic halo<|sep|>Intermediate Mass Black Holes (IMBHs) are a class of black holes with masses in the range $10^2 \div 10^5$ $M_\odot$, which can not directly derive from stellar evolution. Looking for these objects and estimating their abundance is important not only for a deeper understanding of their origin but also for unveiling the nature and distribution of the dark matter in the galactic halo. Since February 2018 to January 2020, the Large and Small Magellanic Cloud have been intensively monitored by the DECAM instrument, installed on the 4m V. Blanco Telescope (CTIO, Chile) with the main objective to discover microlensing events possibly due to IMBHs. Here we outline the developed data analysis pipeline. We have tested it versus known variable sources finding many not previously known variables objects. A few sources show a light curve similar to that expected for a microlensing event, but further analysis is required to confirm the microlensing nature of these events. For these sources, and in particular for the uncatalogued variable stars, we try to determine if they are periodic or not via a periodogram analysis.
Interaction between magnetic vortex cores in a pair of nonidentical nanodisks<|sep|>Interacting magnetic nanoobjects constitute one key component for many proposed spintronic devices, from microwave nano-oscillators to magnetic memory elements. For this reason, the mechanism of this interaction and its dependence with distance $d$ between these nanoobjects has been the subject of several recent studies. In the present work, the problem of the interaction between magnetic nanodisks with magnetic vortex structures is treated both analytically and through micromagnetic simulation. The coupling of two magnetic nanodisks is obtained, in the case of nanodisks with different gyrotropic frequencies. The motion of the two vortex cores, and consequently the time dependence of the total magnetization $M(t)$ is derived using Thiele's equation and by simulation. The magnetic coupling strength as a function of the distance between the disks of each pair is computed with a recently developed method, using the magnetic vortex echoes, analogous to the NMR spin echoes. The results of the two approaches differ by approximately $10\%$ and can be fitted with a multipolar expansion of odd negative powers of $d$; using one single term, a dependence with distance found is broadly in agreement with studies employing other techniques. The coupling frequencies as a function of distance for the pair of nonidentical disks are also obtained.
On travelling wave solutions of a model of a liquid film flowing down a fibre<|sep|>Existence of non-negative weak solutions is shown for a full curvature thin-film model of a liquid thin film flowing down a vertical fibre. The proof is based on the application of a priori estimates derived for energy-entropy functionals. Long-time behaviour of these weak solutions is analysed and, under some additional constraints for the model parameters and initial values, convergence towards a travelling wave solution is obtained. Numerical studies of energy minimizers and travelling waves are presented to illustrate analytical results.
Analyzing Whole-Body Pose Transitions in Multi-Contact Motions<|sep|>When executing whole-body motions, humans are able to use a large variety of support poses which not only utilize the feet, but also hands, knees and elbows to enhance stability. While there are many works analyzing the transitions involved in walking, very few works analyze human motion where more complex supports occur. In this work, we analyze complex support pose transitions in human motion involving locomotion and manipulation tasks (loco-manipulation). We have applied a method for the detection of human support contacts from motion capture data to a large-scale dataset of loco-manipulation motions involving multi-contact supports, providing a semantic representation of them. Our results provide a statistical analysis of the used support poses, their transitions and the time spent in each of them. In addition, our data partially validates our taxonomy of whole-body support poses presented in our previous work. We believe that this work extends our understanding of human motion for humanoids, with a long-term objective of developing methods for autonomous multi-contact motion planning.
Thick smectic shells<|sep|>The known ground state of ultrathin smectic films confined to the surface of a sphere is described by four +1/2 defects assembled on a great circle and a director which follows geodesic lines. Using a simple perturbative approach we show that for thick smectic films on a sphere with planar anchoring this solution breaks down, distorting the smectic layers. The instability happens when the bend elastic constant exceeds the anchoring strength times the radius of the inner sphere. Above this threshold, the formation of a periodic chevron-like structure, observed experimentally as well, relieves geometric frustration. We quantify the effect of thickness and curvature of smectic shells and provide insight into the wavelength of the observed texture.
Contour Detection Using Contrast Formulas in the Framework of Logarithmic Models<|sep|>In this paper we use a new logarithmic model of image representation, developed in [1,2], for edge detection. In fact, in the framework of the new model we obtain the formulas for computing the "contrast of a pixel" and the "contrast" image is just the "contour" or edge image. In our setting the range of values is preserved and the quality of the contour is good for high as well as for low luminosity regions. We present the comparison of our results with the results using classical edge detection operators.
Assessing the Performance of Leja and Clenshaw-Curtis Collocation for Computational Electromagnetics with Random Input Data<|sep|>We consider the problem of quantifying uncertainty regarding the output of an electromagnetic field problem in the presence of a large number of uncertain input parameters. In order to reduce the growth in complexity with the number of dimensions, we employ a dimension-adaptive stochastic collocation method based on nested univariate nodes. We examine the accuracy and performance of collocation schemes based on Clenshaw-Curtis and Leja rules, for the cases of uniform and bounded, non-uniform random inputs, respectively. Based on numerical experiments with an academic electromagnetic field model, we compare the two rules in both the univariate and multivariate case and for both quadrature and interpolation purposes. Results for a real-world electromagnetic field application featuring high-dimensional input uncertainty are also presented.
Asteroseismology of solar-type stars<|sep|>Until the last few decades, investigations of stellar interiors had been restricted to theoretical studies only constrained by observations of their global properties and external characteristics. However, in the last thirty years the field has been revolutionized by the ability to perform seismic investigations of stellar interiors. This revolution begun with the Sun, where helioseismology has been yielding information competing with what can be inferred about the Earth's interior from geoseismology. The last two decades have witnessed the advent of asteroseismology of solar-like stars, thanks to a dramatic development of new observing facilities providing the first reliable results on the interiors of distant stars. The coming years will see a huge development in this field. In this review we focus on solar-type stars, i.e., cool main-sequence stars where oscillations are stochastically excited by surface convection. After a short introduction and a historical overview of the discipline, we review the observational techniques generally used, and we describe the theory behind stellar oscillations in cool main-sequence stars. We continue with a complete description of the normal mode analyses through which it is possible to extract the physical information about the structure and dynamics of the stars. We then summarize the lessons that we have learned and discuss unsolved issues and questions that are still unanswered.
Strong Optimistic Solving for Dynamic Symbolic Execution<|sep|>Dynamic symbolic execution (DSE) is an effective method for automated program testing and bug detection. It is increasing the code coverage by the complex branches exploration during hybrid fuzzing. DSE tools invert the branches along some execution path and help fuzzer examine previously unavailable program parts. DSE often faces over- and underconstraint problems. The first one leads to significant analysis complication while the second one causes inaccurate symbolic execution. We propose strong optimistic solving method that eliminates irrelevant path predicate constraints for target branch inversion. We eliminate such symbolic constraints that the target branch is not control dependent on. Moreover, we separately handle symbolic branches that have nested control transfer instructions that pass control beyond the parent branch scope, e.g. return, goto, break, etc. We implement the proposed method in our dynamic symbolic execution tool Sydr. We evaluate the strong optimistic strategy, the optimistic strategy that contains only the last constraint negation, and their combination. The results show that the strategies combination helps increase either the code coverage or the average number of correctly inverted branches per one minute. It is optimal to apply both strategies together in contrast with other configurations.
Voronoi Density Estimator for High-Dimensional Data: Computation, Compactification and Convergence<|sep|>The Voronoi Density Estimator (VDE) is an established density estimation technique that adapts to the local geometry of data. However, its applicability has been so far limited to problems in two and three dimensions. This is because Voronoi cells rapidly increase in complexity as dimensions grow, making the necessary explicit computations infeasible. We define a variant of the VDE deemed Compactified Voronoi Density Estimator (CVDE), suitable for higher dimensions. We propose computationally efficient algorithms for numerical approximation of the CVDE and formally prove convergence of the estimated density to the original one. We implement and empirically validate the CVDE through a comparison with the Kernel Density Estimator (KDE). Our results indicate that the CVDE outperforms the KDE on sound and image data.
Testing the isotropy of the Universe by using the JLA compilation of type-Ia supernovae<|sep|>We probe the possible anisotropy of the Universe by using the JLA compilation of type-Ia supernovae. We apply the Markov Chain Monte Carlo (MCMC) method to constrain the amplitude and direction of anisotropy in three cosmological models. For the dipole-modulated $\Lambda$CDM model, the anisotropic amplitude is consistent with zero at $68\%$ C.L., and has an upper bound $A_D<1.98\times10^{-3}$ at $95\%$ C.L. Regardless of much larger uncertainty, we find the dipole direction of JLA is amazingly opposite to that of Union2. Similar results are found for the dipole-modulated $w$CDM and CPL models. Thus, the Universe is still well consistent with the isotropy according to the JLA compilation.
OwlEyes-Online: A Fully Automated Platform for Detecting and Localizing UI Display Issues<|sep|>Graphical User Interface (GUI) provides visual bridges between software apps and end users. However, due to the compatibility of software or hardware, UI display issues such as text overlap, blurred screen, image missing always occur during GUI rendering on different devices. Because these UI display issues can be found directly by human eyes, in this paper, we implement an online UI display issue detection tool OwlEyes-Online, which provides a simple and easy-to-use platform for users to realize the automatic detection and localization of UI display issues. The OwlEyes-Online can automatically run the app and get its screenshots and XML files, and then detect the existence of issues by analyzing the screenshots. In addition, OwlEyes-Online can also find the detailed area of the issue in the given screenshots to further remind developers. Finally, OwlEyes-Online will automatically generate test reports with UI display issues detected in app screenshots and send them to users. The OwlEyes-Online was evaluated and proved to be able to accurately detect UI display issues. Tool Link: http://www.owleyes.online:7476 Github Link: https://github.com/franklinbill/owleyes Demo Video Link: https://youtu.be/002nHZBxtCY
Trading Strategies Generated by Lyapunov Functions<|sep|>Functional portfolio generation, initiated by E.R. Fernholz almost twenty years ago, is a methodology for constructing trading strategies with controlled behavior. It is based on very weak and descriptive assumptions on the covariation structure of the underlying market model, and needs no estimation of model parameters. In this paper, the corresponding generating functions $G$ are interpreted as Lyapunov functions for the vector process $\mu(\cdot)$ of market weights; that is, via the property that $G(\mu(\cdot))$ is a supermartingale under an appropriate change of measure. This point of view unifies, generalizes, and simplifies several existing results, and allows the formulation of conditions under which it is possible to outperform the market portfolio over appropriate time-horizons. From a probabilistic point of view, the present paper yields results concerning the interplay of stochastic discount factors and concave transformations of semimartingales on compact domains.
A Probabilistic Framework for Moving-Horizon Estimation: Stability and Privacy Guarantees<|sep|>This work proposes a unifying probabilistic framework for the design of robustly asymptotically stable moving-horizon estimators (MHE) for discrete-time nonlinear systems, and a mechanism to incorporate differential privacy in moving-horizon estimation. We begin with an investigation of the classical notion of strong local observability of nonlinear systems and its relationship to optimization-based state estimation. We then present a general moving-horizon estimation framework for strongly locally observable systems, as an iterative minimization scheme in the space of probability measures. This framework allows for the minimization of the estimation cost with respect to different metrics. In particular, we consider two variants, which we name $W_2$-MHE and KL-MHE, where the minimization scheme uses the 2-Wasserstein distance and the KL-divergence, respectively. The $W_2$-MHE yields a gradient-based estimator whereas the KL-MHE yields a particle filter, for which we investigate asymptotic stability and robustness properties. Stability results for these moving-horizon estimators are derived in the probabilistic setting, against the backdrop of the classical notion of strong local observability which, to the best of our knowledge, differentiates it from other previous works. We then propose a mechanism to encode differential privacy of the measurements used by the estimator via an entropy regularization of the MHE objective functional. In particular, we find sufficient bounds on the regularization parameter to achieve the desired level of differential privacy. Numerical simulations demonstrate the performance of these estimators.
Modelling Concurrent Behaviors in the Process Specification Language<|sep|>In this paper, we propose a first-order ontology for generalized stratified order structure. We then classify the models of the theory using model-theoretic techniques. An ontology mapping from this ontology to the core theory of Process Specification Language is also discussed.
Robust Benchmarking for Machine Learning of Clinical Entity Extraction<|sep|>Clinical studies often require understanding elements of a patient's narrative that exist only in free text clinical notes. To transform notes into structured data for downstream use, these elements are commonly extracted and normalized to medical vocabularies. In this work, we audit the performance of and indicate areas of improvement for state-of-the-art systems. We find that high task accuracies for clinical entity normalization systems on the 2019 n2c2 Shared Task are misleading, and underlying performance is still brittle. Normalization accuracy is high for common concepts (95.3%), but much lower for concepts unseen in training data (69.3%). We demonstrate that current approaches are hindered in part by inconsistencies in medical vocabularies, limitations of existing labeling schemas, and narrow evaluation techniques. We reformulate the annotation framework for clinical entity extraction to factor in these issues to allow for robust end-to-end system benchmarking. We evaluate concordance of annotations from our new framework between two annotators and achieve a Jaccard similarity of 0.73 for entity recognition and an agreement of 0.83 for entity normalization. We propose a path forward to address the demonstrated need for the creation of a reference standard to spur method development in entity recognition and normalization.
Large-N behavior of three-dimensional lattice CP(N-1) models<|sep|>We investigate the phase diagram and critical behavior of a three-dimensional lattice CP(N-1) model in the large-N limit. Numerical evidence of first-order transitions is always observed for sufficiently large values of N, i.e. N>2 up to N=100. The transition becomes stronger---both the latent heat and the surface tension increase---as N increases. Moreover, on the high-temperature side, gauge fields decorrelate on distances of the order of one lattice spacing for all values of N considered. Our results are consistent with a simple scenario, in which the transition is of first order for any N, including N=\infty. We critically discuss the analytic large-N calculations that predicted a large-N continuous transition, showing that one crucial assumption made in these computations fails for the model we consider.
Closed-loop robots driven by short-term synaptic plasticity: Emergent explorative vs. limit-cycle locomotion<|sep|>We examine the hypothesis, that short-term synaptic plasticity (STSP) may generate self-organized motor patterns. We simulated sphere-shaped autonomous robots, within the LPZRobots simulation package, containing three weights moving along orthogonal internal rods. The position of a weight is controlled by a single neuron receiving excitatory input from the sensor, measuring its actual position, and inhibitory inputs from the other two neurons. The inhibitory connections are transiently plastic, following physiologically inspired STSP-rules. We find that a wide palette of motion patterns are generated through the interaction of STSP, robot, and environment (closed-loop configuration), including various forward meandering and circular motions, together with chaotic trajectories. The observed locomotion is robust with respect to additional interactions with obstacles. In the chaotic phase the robot is seemingly engaged in actively exploring its environment. We believe that our results constitute a concept of proof that transient synaptic plasticity, as described by STSP, may potentially be important for the generation of motor commands and for the emergence of complex locomotion patterns, adapting seamlessly also to unexpected environmental feedback. We observe spontaneous and collision induced mode switchings, finding in addition, that locomotion may follow transiently limit cycles which are otherwise unstable. Regular locomotion corresponds to stable limit cycles in the sensorimotor loop, which may be characterized in turn by arbitrary angles of propagation. This degeneracy is, in our analysis, one of the drivings for the chaotic wandering observed for selected parameter settings, which is induced by the smooth diffusion of the angle of propagation.
QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding<|sep|>Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compression heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal. In this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks. In particular, experiments show that gradient quantization applied to training of deep neural networks for image classification and automated speech recognition can lead to significant reductions in communication cost, and end-to-end training time. For instance, on 16 GPUs, we are able to train a ResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show that there exist generic parameter settings under which all known network architectures preserve or slightly improve their full accuracy when using quantization.
A Bayesian Foundation for Physical Theories<|sep|>Bayesian probability theory is used as a framework to develop a formalism for the scientific method based on principles of inductive reasoning. The formalism allows for precise definitions of the key concepts in theories of physics and also leads to a well-defined procedure to select one or more theories among a family of (well-defined) candidates by ranking them according to their posterior probability distributions, which result from Bayes's theorem by incorporating to an initial prior the information extracted from a dataset, ultimately defined by experimental evidence. Examples with different levels of complexity are given and three main applications to basic cosmological questions are analysed: (i) typicality of human observers, (ii) the multiverse hypothesis and, extremely briefly, some few observations about (iii) the anthropic principle. Finally, it is demonstrated that this formulation can address problems that were out of the scope of scientific research until now by presenting the isolated worlds problem and its resolution via the presented framework.
Probabilistic Handshake in All-to-all Broadcast Coded Slotted ALOHA<|sep|>We propose a probabilistic handshake mechanism for all-to-all broadcast coded slotted ALOHA. We consider a fully connected network where each user acts as both transmitter and receiver in a half-duplex mode. Users attempt to exchange messages with each other and to establish one-to-one handshakes, in the sense that each user decides whether its packet was successfully received by the other users: After performing decoding, each user estimates in which slots the resolved users transmitted their packets and, based on that, decides if these users successfully received its packet. The simulation results show that the proposed handshake algorithm allows the users to reliably perform the handshake. The paper also provides some analytical bounds on the performance of the proposed algorithm which are in good agreement with the simulation results.
A New Automatic Method to Identify Galaxy Mergers I. Description and Application to the STAGES Survey<|sep|>We present an automatic method to identify galaxy mergers using the morphological information contained in the residual images of galaxies after the subtraction of a Sersic model. The removal of the bulk signal from the host galaxy light is done with the aim of detecting the fainter minor mergers. The specific morphological parameters that are used in the merger diagnostic suggested here are the Residual Flux Fraction and the asymmetry of the residuals. The new diagnostic has been calibrated and optimized so that the resulting merger sample is very complete. However, the contamination by non-mergers is also high. If the same optimization method is adopted for combinations of other structural parameters such as the CAS system, the merger indicator we introduce yields merger samples of equal or higher statistical quality than the samples obtained through the use of other structural parameters. We explore the ability of the method presented here to select minor mergers by identifying a sample of visually classified mergers that would not have been picked up by the use of the CAS system, when using its usual limits. Given the low prevalence of mergers among the general population of galaxies and the optimization used here, we find that the merger diagnostic introduced in this work is best used as a negative merger test, i.e., it is very effective at selecting non-merging galaxies. As with all the currently available automatic methods, the sample of merger candidates selected is contaminated by non-mergers, and further steps are needed to produce a clean sample. This merger diagnostic has been developed using the HST/ACS F606W images of the A901/02 cluster (z=0.165) obtained by the STAGES team. In particular, we have focused on a mass and magnitude limited sample (log M/M_{O}>9.0, R_{Vega}<23.5mag)) which includes 905 cluster galaxies and 655 field galaxies of all morphological types.
Josephson effect in topological superconducting rings coupled to a microwave cavity<|sep|>We theoretically study a one dimensional p-wave superconducting mesoscopic ring interrupted by a weak link and coupled inductively to a microwave cavity. We establish an input-output description for the cavity field in the presence of the ring, and identify the electronic contributions to the cavity response and their dependence on various parameters, such as the magnetic flux, chemical potential, and cavity frequency. We show that the cavity response is $4\pi$ periodic as a function of the magnetic flux in the topological region, stemming from the so called fractional Josephson current carried by the Majorana fermions, while it is $2\pi$ periodic in the non-topological phase, consistent with the normal Josephson effect. We find a strong dependence of the signal on the cavity frequency, as well as on the parity of the ground state. Our model takes into account fully the interplay between the low-energy Majorana modes and the gaped bulks states, which we show is crucial for visualizing the evolution of the Josephson effect during the transition from the topological to the trivial phase.
A CRC-aided Hybrid Decoding for Turbo Codes<|sep|>Turbo codes and CRC codes are usually decoded separately according to the serially concatenated inner codes and outer codes respectively. In this letter, we propose a hybrid decoding algorithm of turbo-CRC codes, where the outer codes, CRC codes, are not used for error detection but as an assistance to improve the error correction performance. Two independent iterative decoding and reliability based decoding are carried out in a hybrid schedule, which can efficiently decode the two different codes as an entire codeword. By introducing an efficient error detecting method based on normalized Euclidean distance without CRC check, significant gain can be obtained by using the hybrid decoding method without loss of the error detection ability.
Shock Dynamics In Relativistic Jets<|sep|>We present a formalism of the dynamics of internal shocks in relativistic jets where the source has a time-dependent injection velocity and mass-loss rate. The variation of the injection velocity produces a two-shock wave structure, the working surface, that moves along the jet. This new formalism takes into account the fact that momentum conservation is not valid for relativistic flows where the relativistic mass lost by radiation must be taken into account, in contrast to the classic regime. We find analytic solutions for the working surface velocity and radiated energy for the particular case of a step function variability of the injection parameters. We model two cases: a pulse of fast material and a pulse of slow material (with respect to the mean flow). Applying these models to gamma ray burst light curves, one can determine the ratio of the Lorentz factors gamma_2 / gamma_1 and the ratio of the mass-loss rates dot{m_2} / dot{m_1} of the upstream and downstream flows. As an example, we apply this model to the sources GRB 080413B and GRB 070318 and find the values of these ratios. Assuming a Lorentz factor gamma_1=100, we further estimate jet mass-loss rates between dot{m_1} ~ 10^{-5}-1 Msun.yr^{-1}. We also calculate the fraction of the injected mass lost by radiation. For GRB 070318 this fraction is ~7%. In contrast, for GRB 080413B this fraction is larger than 50%; in this case radiation losses clearly affect the dynamics of the internal shocks.
A Probabilistic Characterization of Random Proximity Catch Digraphs and the Associated Tools<|sep|>Proximity catch digraphs (PCDs) are based on proximity maps which yield proximity regions and are special types of proximity graphs. PCDs are based on the relative allocation of points from two or more classes in a region of interest and have applications in various fields. In this article, we provide auxiliary tools for and various characterizations of PCDs based on their probabilistic behavior. We consider the cases in which the vertices of the PCDs come from uniform and non-uniform distributions in the region of interest. We also provide some of the newly defined proximity maps as illustrative examples.
Filtering Procedures for Sensor Data in Basketball<|sep|>Big Data Analytics help team sports' managers in their decisions by processing a number of different kind of data. With the advent of Information Technologies, collecting, processing and storing big amounts of sport data in different form became possible. A problem that often arises when using sport data regards the need for automatic data cleaning procedures. In this paper we develop a data cleaning procedure for basketball which is based on players' trajectories. Starting from a data matrix that tracks the movements of the players on the court at different moments in the game, we propose an algorithm to automatically drop inactive moments making use of available sensor data. The algorithm also divides the game into sorted actions and labels them as offensive or defensive. The algorithm's parameters are validated using proper robustness checks.
Incorporating Monitors in Reactive Synthesis without Paying the Price<|sep|>Temporal synthesis attempts to construct reactive programs that satisfy a given declarative (LTL) formula. Practitioners have found it challenging to work exclusively with declarative specifications, and have found languages that combine modelling with declarative specifications more useful. Synthesised controllers may also need to work with pre-existing or manually constructed programs. In this paper we explore an approach that combines synthesis of declarative specifications in the presence of an existing behaviour model as a monitor, with the benefit of not having to reason about the state space of the monitor. We suggest a formal language with automata monitors as non-repeating and repeating triggers for LTL formulas. We use symbolic automata with memory as triggers, resulting in a strictly more expressive and succinct language than existing regular expression triggers. We give a compositional synthesis procedure for this language, where reasoning about the monitor state space is minimal. To show the advantages of our approach we apply it to specifications requiring counting and constraints over arbitrarily long sequence of events, where we can also see the power of parametrisation, easily handled in our approach. We provide a tool to construct controllers (in the form of symbolic automata) for our language.
Dark Matter Signals from Cascade Annihilations<|sep|>A leading interpretation of the electron/positron excesses seen by PAMELA and ATIC is dark matter annihilation in the galactic halo. Depending on the annihilation channel, the electron/positron signal could be accompanied by a galactic gamma ray or neutrino flux, and the non-detection of such fluxes constrains the couplings and halo properties of dark matter. In this paper, we study the interplay of electron data with gamma ray and neutrino constraints in the context of cascade annihilation models, where dark matter annihilates into light degrees of freedom which in turn decay into leptons in one or more steps. Electron and muon cascades give a reasonable fit to the PAMELA and ATIC data. Compared to direct annihilation, cascade annihilations can soften gamma ray constraints from final state radiation by an order of magnitude. However, if dark matter annihilates primarily into muons, the neutrino constraints are robust regardless of the number of cascade decay steps. We also examine the electron data and gamma ray/neutrino constraints on the recently proposed "axion portal" scenario.
Optical counterparts of undetermined type $\gamma$-ray Active Galactic Nuclei with blazar-like Spectral Energy Distributions<|sep|>During its first four years of scientific observations, the Fermi Large Area Telescope (Fermi-LAT) detected 3033 $\gamma$-ray sources above a 4$\sigma$ significance level. Although most of the extra-Galactic sources are active galactic nuclei (AGN) of the blazar class, other families of AGNs are observed too, while a still high fraction of detections ($\sim 30\%$) remains with uncertain association or classification. According to the currently accepted interpretation, the AGN $\gamma$-ray emission arises from inverse Compton (IC) scattering of low energy photons by relativistic particles confined in a jet that, in the case of blazars, is oriented very close to our line of sight. Taking advantage of data from radio and X-ray wavelengths, which we expect to be produced together with $\gamma$-rays, providing a much better source localization potential, we focused our attention on a sample of $\gamma$-ray Blazar Candidates of Undetermined Type (BCUs), starting a campaign of optical spectroscopic observations. The main aims of our investigation include a census of the AGN families that contribute to $\gamma$-ray emission and a study of their redshift distribution, with the subsequent implications on the intrinsic source power. We furthermore analyze which $\gamma$-ray properties can better constrain the nature of the source, thus helping in the study of objects not yet associated with a reliable low frequency counterpart. In this communication we report on the instruments and techniques used to identify the optical counterparts of $\gamma$-ray sources, we give an overview on the status of our work, and we discuss the implications of a large scale study of $\gamma$-ray emitting AGNs.
Stability of concentrated suspensions under Couette and Poiseuille flow<|sep|>The stability of two-dimensional Poiseuille flow and plane Couette flow for concentrated suspensions is investigated. Linear stability analysis of the two-phase flow model for both flow geometries shows the existence of a convectively driven instability with increasing growth rates of the unstable modes as the particle volume fraction of the suspension increases. In addition it is shown that there exists a bound for the particle phase viscosity below which the two-phase flow model may become ill-posed as the particle phase approaches its maximum packing fraction. The case of two-dimensional Poiseuille flow gives rise to base state solutions that exhibit a jammed and unyielded region, due to shear-induced migration, as the maximum packing fraction is approached. The stability characteristics of the resulting Bingham-type flow is investigated and connections to the stability problem for the related classical Bingham-flow problem are discussed.
A Ku-Band Novel Micromachined Bandpass Filter with Two Transmission Zeros<|sep|>This paper presents a micromachined bandpass filter with miniature size that has relatively outstanding performance. A silicon-based eight-order microstrip bandpass filter is fabricated and measured. A novel design method of the interdigital filter that can create two transmission zeros is described. The location of the transmission zeros can be shifted arbitrarily in the stopband. By adjusting the zero location properly, the filter provides much better skirt rejection and lower insertion loss than a conventional microstrip interdigital filter. To reduce the chip size, through-silicon-substrate-via-hole is used. Good experimental results are obtained.
Quantum Model-Discovery<|sep|>Quantum computing promises to speed up some of the most challenging problems in science and engineering. Quantum algorithms have been proposed showing theoretical advantages in applications ranging from chemistry to logistics optimization. Many problems appearing in science and engineering can be rewritten as a set of differential equations. Quantum algorithms for solving differential equations have shown a provable advantage in the fault-tolerant quantum computing regime, where deep and wide quantum circuits can be used to solve large linear systems like partial differential equations (PDEs) efficiently. Recently, variational approaches to solving non-linear PDEs also with near-term quantum devices were proposed. One of the most promising general approaches is based on recent developments in the field of scientific machine learning for solving PDEs. We extend the applicability of near-term quantum computers to more general scientific machine learning tasks, including the discovery of differential equations from a dataset of measurements. We use differentiable quantum circuits (DQCs) to solve equations parameterized by a library of operators, and perform regression on a combination of data and equations. Our results show a promising path to Quantum Model Discovery (QMoD), on the interface between classical and quantum machine learning approaches. We demonstrate successful parameter inference and equation discovery using QMoD on different systems including a second-order, ordinary differential equation and a non-linear, partial differential equation.
Reduced Physics Model of the Tokamak Scrape-off-Layer for Pulse Design<|sep|>The dynamic interplay between the core and the edge plasma has important consequences in the confinement and heating of fusion plasma. The transport of the Scrape-Off-Layer (SOL) plasma imposes boundary conditions on the core plasma, and neutral transport through the SOL influences the core plasma sourcing. In order to better study these effects in a self-consistent, time-dependent fashion with reasonable turn-around time, a reduced model is needed. In this paper we introduce the SOL Box Model, a reduced SOL model that calculates the plasma temperature and density in the SOL given the core-to-edge particle and power fluxes and recycling coefficients. The analytic nature of the Box Model allows one to readily incorporate SOL physics in time-dependent transport solvers for pulse design applications in the control room. Here we demonstrate such a coupling with the core transport solver TRANSP and compare the results with density and temperature measurements, obtained through Thomson scattering and Langmuir probes, of an NSTX discharge. Implications for future interpretive and predictive simulations are discussed.
A New Representation of Successor Features for Transfer across Dissimilar Environments<|sep|>Transfer in reinforcement learning is usually achieved through generalisation across tasks. Whilst many studies have investigated transferring knowledge when the reward function changes, they have assumed that the dynamics of the environments remain consistent. Many real-world RL problems require transfer among environments with different dynamics. To address this problem, we propose an approach based on successor features in which we model successor feature functions with Gaussian Processes permitting the source successor features to be treated as noisy measurements of the target successor feature function. Our theoretical analysis proves the convergence of this approach as well as the bounded error on modelling successor feature functions with Gaussian Processes in environments with both different dynamics and rewards. We demonstrate our method on benchmark datasets and show that it outperforms current baselines.
Approximation of Search Times for On-street Parking Based on Supply and Demand<|sep|>We propose a method for approximating the probability p({\tau}, n) of searching for on-street parking longer than time {\tau} from the start of a parking search near a given destination n, based on high-resolution maps of parking demand and supply in a city. We verify the method by comparing its outcomes to the estimates obtained with an agent-based model of on-street parking search. As a practical example, we construct maps of cruising time for the Israeli city of Bat Yam, and demonstrate that despite the low overall demand-to-supply ratio of 0.65, excessive demand in the city center results in parking searches of longer than 10 minutes. We discuss the application of the proposed approach for urban planning.
Grain Surface Classification via Machine Learning Methods<|sep|>In this study, radar signals were analyzed to classify grain surface types by using machine learning methods. Radar backscatter signals were recorded using a vector network analyzer between 18-40 GHz. A total of 5681 measurements of A scan signals were collected. The proposed method framework consists of two parts. First Order Statistical features are obtained by applying Fast Fourier Transform (FFT), Discrete Cosine Transform (DCT), Discrete Wavelet Transform (DWT) on backscatter signals in the first part of the framework. Classification process of these features was carried out with Support Vector Machine (SVM). In the second part of the proposed framework, two dimensional matrices in complex form were obtained by applying Short Time Fourier Transform (STFT) on the signals. Gray-Level Co-Occurrence Matrix (GLCM) and Gray-Level Run-Length Matrix (GLRLM) were obtained and feature extraction process was completed. Classification process was carried out with DVM. 10-k cross validation was applied. The highest performance was achieved with STFT+GLCM+SVM.
CrossFill: Foam Structures with Graded Density for Continuous Material Extrusion<|sep|>The fabrication flexibility of 3D printing has sparked a lot of interest in designing structures with spatially graded material properties. In this paper, we propose a new type of density graded structure that is particularly designed for 3D printing systems based on filament extrusion. In order to ensure high-quality fabrication results, extrusion-based 3D printing requires not only that the structures are self-supporting, but also that extrusion toolpaths are continuous and free of self-overlap. The structure proposed in this paper, called CrossFill, complies with these requirements. In particular, CrossFill is a self-supporting foam structure, for which each layer is fabricated by a single, continuous and overlap-free path of material extrusion. Our method for generating CrossFill is based on a space-filling surface that employs spatially varying subdivision levels. Dithering of the subdivision levels is performed to accurately reproduce a prescribed density distribution. We demonstrate the effectiveness of CrossFill on a number of experimental tests and applications.
Diffractive mechanisms in $pp \to pp \pi^{0}$ reaction at high energies<|sep|>We present a study of exclusive production of $\pi^{0}$ meson in proton-proton collisions at high energies. Both diffractive bremsstrahlung (Drell-Hiida-Deck type model), photon-photon, photon-omega and photon-odderon exchange mechanisms are included in the calculation. The $\pi^{0}$-bremsstrahlung contribution dominates at large (forward, backward) pion rapidities and contributes at small $\pi^0 p$ invariant mass and could be therefore misinterpreted as the Roper resonance $N^{*}(1440)$. Large cross sections of the order of mb are predicted. We predict strong dependence of the slope in $t$ (squared four-momentum transfer between ingoing and outgoing proton) on the mass of the supplementary excited $\pi^{0} p$ system. At high energy and midrapidity, the photon-photon contribution dominates over the diffractive components, however, the corresponding cross section is rather small. The photon-odderon and odderon-photon contributions are included in addition and first estimates (upper limits) of their contributions are presented. We suggest a search for the odderon contribution at midrapidity and at $p_{\perp,\pi^{0}} \sim$ 0.5 GeV. Our predictions are ready for verification at LHC. The bremsstrahlung mechanisms discussed here contribute also to the $pp \to p(n \pi^{+})$ reaction. Both channels give a sizable contribution to the low-mass single diffractive cross section and must be included in extrapolating the measured experimental single diffractive cross section.
Flatland Competition 2020: MAPF and MARL for Efficient Train Coordination on a Grid World<|sep|>The Flatland competition aimed at finding novel approaches to solve the vehicle re-scheduling problem (VRSP). The VRSP is concerned with scheduling trips in traffic networks and the re-scheduling of vehicles when disruptions occur, for example the breakdown of a vehicle. While solving the VRSP in various settings has been an active area in operations research (OR) for decades, the ever-growing complexity of modern railway networks makes dynamic real-time scheduling of traffic virtually impossible. Recently, multi-agent reinforcement learning (MARL) has successfully tackled challenging tasks where many agents need to be coordinated, such as multiplayer video games. However, the coordination of hundreds of agents in a real-life setting like a railway network remains challenging and the Flatland environment used for the competition models these real-world properties in a simplified manner. Submissions had to bring as many trains (agents) to their target stations in as little time as possible. While the best submissions were in the OR category, participants found many promising MARL approaches. Using both centralized and decentralized learning based approaches, top submissions used graph representations of the environment to construct tree-based observations. Further, different coordination mechanisms were implemented, such as communication and prioritization between agents. This paper presents the competition setup, four outstanding solutions to the competition, and a cross-comparison between them.
Stateless multicast switching in software defined networks<|sep|>Multicast data delivery can significantly reduce traffic in operators' networks, but has been limited in deployment due to concerns such as the scalability of state management. This paper shows how multicast can be implemented in contemporary software defined networking (SDN) switches, with less state than existing unicast switching strategies, by utilising a Bloom Filter (BF) based switching technique. Furthermore, the proposed mechanism uses only proactive rule insertion, and thus, is not limited by congestion or delay incurred by reactive controller-aided rule insertion. We compare our solution against common switching mechanisms such as layer-2 switching and MPLS in realistic network topologies by modelling the TCAM state sizes in SDN switches. The results demonstrate that our approach has significantly smaller state size compared to existing mechanisms and thus is a multicast switching solution for next generation networks.
High-Performance Neural Networks for Visual Object Classification<|sep|>We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.
Bidirectional deep learning of polarization transfer in liquid crystals with application to quantum state preparation<|sep|>Accurate control of light polarization represents a core building block in polarization metrology, imaging, and optical and quantum communications. Voltage-controlled liquid crystals offer an efficient way of polarization transformation. However, common twisted nematic liquid crystals are notorious for lacking an accurate theoretical model linking control voltages and output polarization. An inverse model, which would predict control voltages required to prepare a target polarization, is even more challenging. Here we report both the direct and inverse models based on deep neural networks, radial basis functions, and linear interpolation. We present an inverse-direct compound model solving the problem of control voltages ambiguity. We demonstrate one order of magnitude improvement in accuracy using deep learning compared to the radial basis function method and two orders of magnitude improvement compared to the linear interpolation. Errors of the deep neural network model also decrease faster than the other methods with an increasing number of training data. The best direct and inverse models reach the average infidelities of $4 \times 10^{-4}$ and $2 \times 10^{-4}$, respectively, which is the accuracy level not reported yet. Furthermore, we demonstrate local and remote preparation of an arbitrary single-photon polarization state using the deep learning models. The results will impact the application of twisted-nematic liquid crystals, increasing their control accuracy across the board. The presented bidirectional learning can be used for optimal classical control of complex photonic devices and quantum circuits beyond interpolation.
Reconfiguration Dynamics in folded and intrinsically disordered protein with internal friction: Effect of solvent quality and denaturant<|sep|>We consider a phantom chain model of polymer with internal friction in a harmonic confinement and extend it to take care of effects of solvent quality following a mean field approach where an exponent $\nu$ is introduced. The model termed as "Solvent Dependent Compacted Rouse with Internal Friction (SDCRIF)" is then used to calculate the reconfiguration time of a chain that relates to recent F\"{o}rster resonance energy transfer (FRET) studies on folded and intrinsically disordered proteins (IDPs) and can account for the effects of solvent quality as well as the denaturant concentration on the reconfiguration dynamics. Following an ansatz that relates the strength of the harmonic confinement ($k_c$) with the internal friction of the chain ($\xi_{int}$), SDCRIF can convincingly reproduce the experimental data and explain how the denaturant can change the time scale for the internal friction. It can also predict near zero internal friction in case of IDPs. In addition, our calculations show that the looping time as well as the reconfiguration time scales with the chain length $N$ as $\sim N^\alpha$, where $\alpha$ depends weakly on the internal friction but has rather stronger dependence on the solvent quality. In absence of any internal friction, $\alpha=2\nu+1$ and it goes down in presence of internal friction, but looping slows down in general. On the contrary, poorer the solvent, faster the chain reconfigures and forms loop, even though one expects high internal friction in the collapsed state. However, if the internal friction is too high then the looping and reconfiguration dynamics become slow even in poor solvent.
Informational Substitutes<|sep|>We propose definitions of substitutes and complements for pieces of information ("signals") in the context of a decision or optimization problem, with game-theoretic and algorithmic applications. In a game-theoretic context, substitutes capture diminishing marginal value of information to a rational decision maker. We use the definitions to address the question of how and when information is aggregated in prediction markets. Substitutes characterize "best-possible" equilibria with immediate information aggregation, while complements characterize "worst-possible", delayed aggregation. Game-theoretic applications also include settings such as crowdsourcing contests and Q\&A forums. In an algorithmic context, where substitutes capture diminishing marginal improvement of information to an optimization problem, substitutes imply efficient approximation algorithms for a very general class of (adaptive) information acquisition problems. In tandem with these broad applications, we examine the structure and design of informational substitutes and complements. They have equivalent, intuitive definitions from disparate perspectives: submodularity, geometry, and information theory. We also consider the design of scoring rules or optimization problems so as to encourage substitutability or complementarity, with positive and negative results. Taken as a whole, the results give some evidence that, in parallel with substitutable items, informational substitutes play a natural conceptual and formal role in game theory and algorithms.
End-to-End Memristive HTM System for Pattern Recognition and Sequence Prediction<|sep|>Neuromorphic systems that learn and predict from streaming inputs hold significant promise in pervasive edge computing and its applications. In this paper, a neuromorphic system that processes spatio-temporal information on the edge is proposed. Algorithmically, the system is based on hierarchical temporal memory that inherently offers online learning, resiliency, and fault tolerance. Architecturally, it is a full custom mixed-signal design with an underlying digital communication scheme and analog computational modules. Therefore, the proposed system features reconfigurability, real-time processing, low power consumption, and low-latency processing. The proposed architecture is benchmarked to predict on real-world streaming data. The network's mean absolute percentage error on the mixed-signal system is 1.129X lower compared to its baseline algorithm model. This reduction can be attributed to device non-idealities and probabilistic formation of synaptic connections. We demonstrate that the combined effect of Hebbian learning and network sparsity also plays a major role in extending the overall network lifespan. We also illustrate that the system offers 3.46X reduction in latency and 77.02X reduction in power consumption when compared to a custom CMOS digital design implemented at the same technology node. By employing specific low power techniques, such as clock gating, we observe 161.37X reduction in power consumption.
Universal network deployment model for universal connectivity<|sep|>There is interest in the deployment of cable and other networking infrastructure for private use in public land, but the lack of clear guidelines to regulate deployment in public land can block authorization decisions, which can be controversial due to the consequences of the private ownership and use of a private infrastructure in public space. The guifi.net Foundation proposed a universal deployment model for municipalities, where new deployments by a private requester are allowed as long it provides paths that simultaneously allow for three uses: self-service for the city council, private for the requester, and shared or common use for everyone else. The principle can be extended to apply to any other regional or even international infrastructure deployed in non-private land, although the proportion of resources for each uses can be adjusted. The effect of this model is that the deployment of private infrastructures generate a direct return as infrastructure for shared use by everyone can contribute to deliver universal connectivity.
Control Barrier Functions With Unmodeled Dynamics Using Integral Quadratic Constraints<|sep|>This paper presents a control design method that achieves safety for systems with unmodeled dynamics at the plant input. The proposed method combines control barrier functions (CBFs) and integral quadratic constraints (IQCs). Simplified, low-order models are often used in the design of the controller. Parasitic, unmodeled dynamics (e.g. actuator dynamics, time delays, etc) can lead to safety violations. The proposed method bounds the input-output behavior of these unmodeled dynamics in the time-domain using an alpha-IQC. The alpha-IQC is then incorporated into the CBF constraint to ensure safety. The approach is demonstrated with a simple example.
Isoscaling in statistical fragment emission in an extended compound nucleus model<|sep|>Based on an extended compound nucleus model, isospin effects in statistical fragment emission from excited nuclear systems are investigated. An experimentally observed scaling behavior of the ratio of isotope yields $Y_i(N,Z)$ from two similar emitting sources with different neutron-to-proton ratios is predicted theoretically, i.e., the relationship of $Y_2/Y_1 \propto exp(\alpha N + \beta Z)$ is demonstrated. The symmetry energy coefficient $C_{sym}$ extracted from the simulation results is $\sim$ 27 MeV which is consistent with realistic theoretical estimates and recent experimental data. The influence of the surface entropy on the isoscaling behavior is discussed in detail. It is found that although the surface entropy increases the numercial values of isoscaling parameters $\alpha$ and $\beta$, it does not affect the isoscaling behavior qualitatively and has only a minor effect on the extracted symmetry energy coefficient.
Cosmic microwave background anomalies viewed via Gumbel Statistics<|sep|>We describe and discuss the application of Gumbel statistics, which model extreme events, to WMAP 5-year measurements of the cosmic microwave background. We find that temperature extrema of the CMB are well modelled by the Gumbel formalism and describe tests for Gaussianity that the approach can provide. Comparison to simulations reveals Gumbel statistics to have only weak discriminatory power for the conventional statistic: $f_{NL}<1000$, though it may probe other regimes of non-Gaussianity. Tests based on hemispheric cuts reveal interesting alignment with other reported CMB anomalies. The approach has the advantage of model independence and may find further utility with smaller scale data.
Template based Graph Neural Network with Optimal Transport Distances<|sep|>Current Graph Neural Networks (GNN) architectures generally rely on two important components: node features embedding through message passing, and aggregation with a specialized form of pooling. The structural (or topological) information is implicitly taken into account in these two steps. We propose in this work a novel point of view, which places distances to some learnable graph templates at the core of the graph representation. This distance embedding is constructed thanks to an optimal transport distance: the Fused Gromov-Wasserstein (FGW) distance, which encodes simultaneously feature and structure dissimilarities by solving a soft graph-matching problem. We postulate that the vector of FGW distances to a set of template graphs has a strong discriminative power, which is then fed to a non-linear classifier for final predictions. Distance embedding can be seen as a new layer, and can leverage on existing message passing techniques to promote sensible feature representations. Interestingly enough, in our work the optimal set of template graphs is also learnt in an end-to-end fashion by differentiating through this layer. After describing the corresponding learning procedure, we empirically validate our claim on several synthetic and real life graph classification datasets, where our method is competitive or surpasses kernel and GNN state-of-the-art approaches. We complete our experiments by an ablation study and a sensitivity analysis to parameters.
Effects of vertex corrections on diagrammatic approximations applied to the study of transport through a quantum dot<|sep|>In the present work, we calculate the conductance through a single quantum dot weakly coupled to metallic contacts. We use the spin-1/2 Anderson model to describe the quantum dot, while considering a finite Coulomb repulsion. We solve the interacting system using the non-crossing-approximation (NCA) and the one-crossing approximation (OCA). We obtain the linear response conductance as a function of temperature and energy position of the localized level. From the comparison of both approximations we extract the role of the vertex corrections, which are introduced in the OCA calculations and neglected in the NCA scheme. As a function of the energy position, we observe that the diagrams omitted within NCA are really important for appropriately describing transport phenomena in Kondo systems as well as in the mixed valence regime. On the other hand, as a function of temperature, the corrections introduced by OCA partly recover the universal scaling properties known from numerical approaches such as the Numerical Renormalization Group(NRG).
Dark Energy Coupled with Dark Matter in Viscous Fluid Cosmology<|sep|>We investigate cosmological models with two interacting fluids: dark energy and dark matter in flat Friedmann-Robertson-Walker universe. The interaction between dark energy and dark matter is described in terms of the parameters present in the inhomogeneous equation of state when allowance is made for bulk viscosity, for the Little Rip, the Pseudo Rip, and the bounce universes. We obtain analytic representation for characteristic properties in these cosmological models, in particular the bulk viscosity $\zeta=\zeta(H,t)$ as function of Hubble parameter and time. We discuss the corrections of thermodynamical parameters in the equations of state due coupling between the viscous fluid and dark matter. Some common properties of these corrections are elucidated.
A frequency quintupled laser at 308 nm for spectroscopy of intercombination lines in zinc<|sep|>Many experiments in atomic physics and quantum optics, among them optical atomic clocks, require laser sources in the ultra-violet wavelength range with very low intensity noise and phase noise. The development of such lasers is a challenge, especially when a robust and transportable system is required. Here, we report on the development of a novel continuous wave (cw) frequency-quintupled laser at 308 nm with an output power of 0.5 mW, based on a fiber laser operating in the telecom band. Three consecutive frequency conversion stages in nonlinear crystals are employed. The performance of the laser system is demonstrated by linear absorption spectroscopy of a narrow intercombination line in zinc.
On Reliability-Aware Server Consolidation in Cloud Datacenters<|sep|>In the past few years, datacenter (DC) energy consumption has become an important issue in technology world. Server consolidation using virtualization and virtual machine (VM) live migration allows cloud DCs to improve resource utilization and hence energy efficiency. In order to save energy, consolidation techniques try to turn off the idle servers, while because of workload fluctuations, these offline servers should be turned on to support the increased resource demands. These repeated on-off cycles could affect the hardware reliability and wear-and-tear of servers and as a result, increase the maintenance and replacement costs. In this paper we propose a holistic mathematical model for reliability-aware server consolidation with the objective of minimizing total DC costs including energy and reliability costs. In fact, we try to minimize the number of active PMs and racks, in a reliability-aware manner. We formulate the problem as a Mixed Integer Linear Programming (MILP) model which is in form of NP-complete. Finally, we evaluate the performance of our approach in different scenarios using extensive numerical MATLAB simulations.
Efficiency of Exponentiality Tests Based on a Special Property of Exponential Distribution<|sep|>New goodness-of-fit tests for exponentiality based on a particular property of exponential law are constructed. Test statistics are functionals of U-empirical processes. The first of these statistics is of integral type, the second one is a Kolmogorov type statistic. We show that the kernels corresponding to our statistics are nondegenerate. The limiting distributions and large deviations of new statistics under the null hypothesis are described. Their local Bahadur efficiency for various parametric alternatives is calculated and is compared with simulated powers of new tests. Conditions of local optimality of new statistics in Bahadur sense are discussed and examples of "most favorable" alternatives are given. New tests are applied to reject the hypothesis of exponentiality for the length of reigns of Roman emperors which was intensively discussed in recent years.
Free Lunch for Few-shot Learning: Distribution Calibration<|sep|>Learning from a limited number of samples is challenging since the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples, then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on two datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation.
Applicability of Large Corporate Credit Models to Small Business Risk Assessment<|sep|>There is a massive underserved market for small business lending in the US with the Federal Reserve estimating over \$650B in unmet annual financing needs. Assessing the credit risk of a small business is key to making good decisions whether to lend and at what terms. Large corporations have a well-established credit assessment ecosystem, but small businesses suffer from limited publicly available data and few (if any) credit analysts who cover them closely. We explore the applicability of (DL-based) large corporate credit risk models to small business credit rating.
FormuLog: Datalog for static analysis involving logical formulae<|sep|>Datalog has become a popular language for writing static analyses. Because Datalog is very limited, some implementations of Datalog for static analysis have extended it with new language features. However, even with these features it is hard or impossible to express a large class of analyses because they use logical formulae to represent program state. FormuLog fills this gap by extending Datalog to represent, manipulate, and reason about logical formulae. We have used FormuLog to implement declarative versions of symbolic execution and abstract model checking, analyses previously out of the scope of Datalog-based languages. While this paper focuses on the design of FormuLog and one of the analyses we have implemented in it, it also touches on a prototype implementation of the language and identifies performance optimizations that we believe will be necessary to scale FormuLog to real-world static analysis problems.
Contrastive Entropy: A new evaluation metric for unnormalized language models<|sep|>Perplexity (per word) is the most widely used metric for evaluating language models. Despite this, there has been no dearth of criticism for this metric. Most of these criticisms center around lack of correlation with extrinsic metrics like word error rate (WER), dependence upon shared vocabulary for model comparison and unsuitability for unnormalized language model evaluation. In this paper, we address the last problem and propose a new discriminative entropy based intrinsic metric that works for both traditional word level models and unnormalized language models like sentence level models. We also propose a discriminatively trained sentence level interpretation of recurrent neural network based language model (RNN) as an example of unnormalized sentence level model. We demonstrate that for word level models, contrastive entropy shows a strong correlation with perplexity. We also observe that when trained at lower distortion levels, sentence level RNN considerably outperforms traditional RNNs on this new metric.
Variational Depth from Focus Reconstruction<|sep|>This paper deals with the problem of reconstructing a depth map from a sequence of differently focused images, also known as depth from focus or shape from focus. We propose to state the depth from focus problem as a variational problem including a smooth but nonconvex data fidelity term, and a convex nonsmooth regularization, which makes the method robust to noise and leads to more realistic depth maps. Additionally, we propose to solve the nonconvex minimization problem with a linearized alternating directions method of multipliers (ADMM), allowing to minimize the energy very efficiently. A numerical comparison to classical methods on simulated as well as on real data is presented.
Distortion of Magnetic Fields in a Starless Core IV: Magnetic Field Scaling on Density and Mass-to-flux Ratio Distribution in FeSt 1-457<|sep|>In the present study, the magnetic field scaling on density, $|B| \propto \rho^{\kappa}$, was revealed in a single starless core for the first time. The $\kappa$ index of $0.78 \pm 0.10$ was obtained toward the starless dense core FeSt 1-457 based on the analysis of the radial distribution of the polarization angle dispersion of background stars measured at the near-infrared wavelengths. The result prefers $\kappa = 2/3$ for the case of isotropic contraction, and the difference of the observed value from $\kappa = 1/2$ is 2.8 sigma. The distribution of the ratio of mass to magnetic flux was evaluated. FeSt 1-457 was found to be magnetically supercritical near the center ($\lambda \approx 2$), whereas nearly critical or slightly subcritical at the core boundary ($\lambda \approx 0.98$). Ambipolar-diffusion-regulated star formation models for the case of moderate magnetic field strength may explain the physical status of FeSt 1-457. The mass-to-flux ratio distribution for typical dense cores (critical Bonnor--Ebert sphere with central $\lambda=2$ and $\kappa=1/2$--$2/3$) was calculated and found to be magnetically critical/subcritical at the core edge, which indicates that typical dense cores are embedded in and evolve from magnetically critical/subcritical diffuse surrounding medium.
Shape and Angular Distribution of the 4.438-MeV Line from Proton Inelastic Scattering off 12C<|sep|>The emission of the 4.438-MeV gamma-ray line in proton inelastic scattering off 12C has been investigated in detail. The correlated scattering and emission process is described independently for the direct reaction mechanism and for the compound-nucleus (CN) component. The inelastic scattering process for direct reactions is treated with a coupled-channels nuclear reaction code, while the CN component is described as a superposition of separate resonances with definite spin and parity, treated with the angular momentum coupling theory. The calculations are compared to a comprehensive data set on measured line shapes and angular distributions in the proton energy range E_p = 5.44 - 25.0 MeV. In the range E_p ~ 12 - 25 MeV a good agreement is obtained in calculations assuming direct reactions with only a negligible part of CN reactions. At lower energy, the data are reproduced by incoherent sums of the direct component with typically one CN resonance. Based on these results, the prospectives for line shape calculations applied to solar flares and gamma-ray emission in proton radiotherapy are discussed.
Application of Coherent State Approach for the cancellation of Infrared divergences to all orders in LFQED<|sep|>We sketch an all order proof of cancellation of infrared (IR) divergences in Light Front Quantum Electrodynamics (LFQED) using a coherent state formalism. In this talk, it has been shown that the true IR divergences in fermion self energy are eliminated to all orders in a light-front time-ordered perturbative calculation if one uses coherent state basis instead of the usual Fock basis to calculate the Hamiltonian matrix elements.
Constraining the formation of black-holes in short-period Black-Hole Low-Mass X-ray Binaries<|sep|>The formation of stellar mass black holes is still very uncertain. Two main uncertainties are the amount of mass ejected in the supernova event (if any) and the magnitude of the natal kick the black hole receives at birth (if any). Repetto et al. (2012), studying the position of Galactic X-ray binaries containing black holes, found evidence for black holes receiving high natal kicks at birth. In this Paper we extend that study, taking into account the previous binary evolution of the sources as well. The seven short-period black-hole X-ray binaries that we use, are compact binaries consisting of a low-mass star orbiting a black hole in a period less than $1$ day. We trace their binary evolution backwards in time, from the current observed state of mass-transfer, to the moment the black hole was formed, and we add the extra information on the kinematics of the binaries. We find that several systems could be explained by no natal kick, just mass ejection, while for two systems (and possibly more) a high kick is required. So unless the latter have an alternative formation, such as within a globular cluster, we conclude that at least some black holes get high kicks. This challenges the standard picture that black hole kicks would be scaled down from neutron star kicks. Furthermore, we find that five systems could have formed with a non-zero natal kick but zero mass ejected (i.e. no supernova) at formation, as predicted by neutrino-driven natal kicks.
Gravitational instability of filamentary molecular clouds, including ambipolar diffusion<|sep|>The gravitational instability of a filamentary molecular cloud in non-ideal magnetohydrodynamics is investigated. The filament is assumed to be in hydrostatic equilibrium. We add the effect of ambipolar diffusion to the filament which is threaded by an initial uniform axial magnetic field along its axis. We write down the fluid equations in cylindrical coordinates and perform linear perturbation analysis. We integrate the resultant differential equations and then derive the numerical dispersion relation. We find that, a more efficient ambipolar diffusion leads to an enhancement of the growth of the most unstable mode, and to increase of the fragmentation scale of the filament.
Tetrahedron in F-theory Compactification<|sep|>Complex tetrahedral surface $\mathcal{T}$ is a non planar projective surface that is generated by four intersecting complex projective planes $CP^{2}$. In this paper, we study the family $\{\mathcal{T}_{m}\} $ of blow ups of $\mathcal{T}$ and exhibit the link of these $\mathcal{T}_{m}$s with the set of del Pezzo surfaces $dP_{n}$ obtained by blowing up n isolated points in the $CP^{2}$. The $\mathcal{T}_{m}$s are toric surfaces exhibiting a $U(1) \times U(1) $ symmetry that may be used to engineer gauge symmetry enhancements in the Beasley-Heckman-Vafa theory. The blown ups of the tetrahedron have toric graphs with faces, edges and vertices where may localize respectively fields in adjoint representations, chiral matter and Yukawa tri-fields couplings needed for the engineering of F- theory GUT models building.
Knowledge-Augmented Language Models for Cause-Effect Relation Classification<|sep|>Previous studies have shown the efficacy of knowledge augmentation methods in pretrained language models. However, these methods behave differently across domains and downstream tasks. In this work, we investigate the augmentation of pretrained language models with commonsense knowledge in the cause-effect relation classification and commonsense causal reasoning tasks. After automatically verbalizing ATOMIC2020, a wide coverage commonsense reasoning knowledge graph, and GLUCOSE, a dataset of implicit commonsense causal knowledge, we continually pretrain BERT and RoBERTa with the verbalized data. Then we evaluate the resulting models on cause-effect pair classification and answering commonsense causal reasoning questions. Our results show that continually pretrained language models augmented with commonsense knowledge outperform our baselines on two commonsense causal reasoning benchmarks, COPA and BCOPA-CE, and the Temporal and Causal Reasoning (TCR) dataset, without additional improvement in model architecture or using quality-enhanced data for fine-tuning.
Query Language for Complex Similarity Queries<|sep|>For complex data types such as multimedia, traditional data management methods are not suitable. Instead of attribute matching approaches, access methods based on object similarity are becoming popular. Recently, this resulted in an intensive research of indexing and searching methods for the similarity-based retrieval. Nowadays, many efficient methods are already available, but using them to build an actual search system still requires specialists that tune the methods and build the system manually. Several attempts have already been made to provide a more convenient high-level interface in a form of query languages for such systems, but these are limited to support only basic similarity queries. In this paper, we propose a new language that allows to formulate content-based queries in a flexible way, taking into account the functionality offered by a particular search engine in use. To ensure this, the language is based on a general data model with an abstract set of operations. Consequently, the language supports various advanced query operations such as similarity joins, reverse nearest neighbor queries, or distinct kNN queries, as well as multi-object and multi-modal queries. The language is primarily designed to be used with the MESSIF framework for content-based searching but can be employed by other retrieval systems as well.
Ergodic and localized regions in quantum spin glasses on the Bethe lattice<|sep|>By considering the quantum dynamics of a transverse field Ising spin glass on the Bethe lattice we find the existence of a many body localized region at small transverse field and low temperature. The region is located within the thermodynamic spin glass phase. Accordingly, we conjecture that quantum dynamics inside the glassy region is split in a small MBL and a large delocalized (but not necessarily ergodic) region. This has implications for the analysis of the performance of quantum adiabatic algorithms.
Asynchronous Snapshots of Actor Systems for Latency-Sensitive Applications<|sep|>The actor model is popular for many types of server applications. Efficient snapshotting of applications is crucial in the deployment of pre-initialized applications or moving running applications to different machines, e.g for debugging purposes. A key issue is that snapshotting blocks all other operations. In modern latency-sensitive applications, stopping the application to persist its state needs to be avoided, because users may not tolerate the increased request latency. In order to minimize the impact of snapshotting on request latency, our approach persists the application's state asynchronously by capturing partial heaps, completing snapshots step by step. Additionally, our solution is transparent and supports arbitrary object graphs. We prototyped our snapshotting approach on top of the Truffle/Graal platform and evaluated it with the Savina benchmarks and the Acme Air microservice application. When performing a snapshot every thousand Acme Air requests, the number of slow requests ( 0.007% of all requests) with latency above 100ms increases by 5.43%. Our Savina microbenchmark results detail how different utilization patterns impact snapshotting cost. To the best of our knowledge, this is the first system that enables asynchronous snapshotting of actor applications, i.e. without stop-the-world synchronization, and thereby minimizes the impact on latency. We thus believe it enables new deployment and debugging options for actor systems.
Technology Ethics in Action: Critical and Interdisciplinary Perspectives<|sep|>This special issue interrogates the meaning and impacts of "tech ethics": the embedding of ethics into digital technology research, development, use, and governance. In response to concerns about the social harms associated with digital technologies, many individuals and institutions have articulated the need for a greater emphasis on ethics in digital technology. Yet as more groups embrace the concept of ethics, critical discourses have emerged questioning whose ethics are being centered, whether "ethics" is the appropriate frame for improving technology, and what it means to develop "ethical" technology in practice. This interdisciplinary issue takes up these questions, interrogating the relationships among ethics, technology, and society in action. This special issue engages with the normative and contested notions of ethics itself, how ethics has been integrated with technology across domains, and potential paths forward to support more just and egalitarian technology. Rather than starting from philosophical theories, the authors in this issue orient their articles around the real-world discourses and impacts of tech ethics--i.e., tech ethics in action.
On the Exponentially Weighted Aggregate with the Laplace Prior<|sep|>In this paper, we study the statistical behaviour of the Exponentially Weighted Aggregate (EWA) in the problem of high-dimensional regression with fixed design. Under the assumption that the underlying regression vector is sparse, it is reasonable to use the Laplace distribution as a prior. The resulting estimator and, specifically, a particular instance of it referred to as the Bayesian lasso, was already used in the statistical literature because of its computational convenience, even though no thorough mathematical analysis of its statistical properties was carried out. The present work fills this gap by establishing sharp oracle inequalities for the EWA with the Laplace prior. These inequalities show that if the temperature parameter is small, the EWA with the Laplace prior satisfies the same type of oracle inequality as the lasso estimator does, as long as the quality of estimation is measured by the prediction loss. Extensions of the proposed methodology to the problem of prediction with low-rank matrices are considered.
Global Synchronization of Pulse-Coupled Oscillator Networks Under Byzantine Attacks<|sep|>Synchronization of pulse-coupled oscillators (PCOs) has gained significant attention recently due to their increased applications in sensor networks and wireless communications. Given the distributed and unattended nature of wireless sensor networks, it is imperative to enhance the resilience of PCO synchronization against malicious attacks. However, most existing results on attack-resilient pulse-based synchronization are obtained under assumptions of all-to-all coupling topologies or restricted initial phase distributions. In this paper, we propose a new pulse-based synchronization mechanism to improve the attack resilience of PCO synchronization that is applicable to non-all-to-all networks. Under the proposed synchronization mechanism, we prove that perfect synchronization of legitimate oscillators can be guaranteed in the presence of multiple Byzantine attackers who can emit attack pulses arbitrarily without any constraint except that practical bit rate constraint renders the number of pulses from an attacker to be finite. The new mechanism can guarantee synchronization even when the initial phases of all legitimate oscillators are arbitrarily distributed in the entire oscillation period, which is in distinct difference from most existing attack-resilient synchronization approaches (including the seminal paper from Lamport and Melliar-Smith [1]) that require a priori (almost) synchronization among legitimate oscillators. Numerical simulation results are given to confirm the theoretical results.
Technical Report: A Receding Horizon Algorithm for Informative Path Planning with Temporal Logic Constraints<|sep|>This technical report is an extended version of the paper 'A Receding Horizon Algorithm for Informative Path Planning with Temporal Logic Constraints' accepted to the 2013 IEEE International Conference on Robotics and Automation (ICRA). This paper considers the problem of finding the most informative path for a sensing robot under temporal logic constraints, a richer set of constraints than have previously been considered in information gathering. An algorithm for informative path planning is presented that leverages tools from information theory and formal control synthesis, and is proven to give a path that satisfies the given temporal logic constraints. The algorithm uses a receding horizon approach in order to provide a reactive, on-line solution while mitigating computational complexity. Statistics compiled from multiple simulation studies indicate that this algorithm performs better than a baseline exhaustive search approach.
Early and Late-time Cosmic Acceleration in Non-minimal Yang-Mills-$f(G)$ Gravity<|sep|>In this paper we show that power-law inflation can be realized in non-minimal gravitational coupling of Yang-Mills field with a general function of the Gauss-Bonnet invariant in the framework of Einstein gravity. Such a non-minimal coupling may appear due to quantum corrections. We also discuss the non-minimal Yang-Mills-$f(G)$ gravity in the framework of modified Gauss-Bonnet action which is widely studied recently. It is shown that both inflation and late-time cosmic acceleration are possible in such a theory.
Effect of Cd2+ on the Growth and Thermal Properties of K2SO4 crystal<|sep|>Single crystals of pure and Cd2+ doped potassium sulfate were grown from aqueous solutions by the slow evaporation technique. From nutrient solutions with a CdSO4 concentration of 4wt.% crystals containing 0.014wt.% dopant concentration could be obtained. The X-ray diffraction patterns of powdered crystals confirmed their crystal structures for both cases. Thermal analysis of pure crystals shows that the alpha-beta phase transformation peak around 580 deg C is superimposed with spurious effects, while for Cd2+ doped crystals this is not the case. The thermal hysteresis of the phase transition is 8 K for undoped K2SO4 and is reduced to 3.5 K for K2SO4:Cd2+. Compared to undoped crystals, the optical transmittance of Cd2+ doped crystals is higher.
Stochastic Bound Majorization<|sep|>Recently a majorization method for optimizing partition functions of log-linear models was proposed alongside a novel quadratic variational upper-bound. In the batch setting, it outperformed state-of-the-art first- and second-order optimization methods on various learning tasks. We propose a stochastic version of this bound majorization method as well as a low-rank modification for high-dimensional data-sets. The resulting stochastic second-order method outperforms stochastic gradient descent (across variations and various tunings) both in terms of the number of iterations and computation time till convergence while finding a better quality parameter setting. The proposed method bridges first- and second-order stochastic optimization methods by maintaining a computational complexity that is linear in the data dimension and while exploiting second order information about the pseudo-global curvature of the objective function (as opposed to the local curvature in the Hessian).
Intriguing microstructures of five-dimensional neutral Gauss-Bonnet AdS black hole<|sep|>In this paper, we analytically study the phase structure and construct the Ruppeiner geometry in the extended phase space for the five-dimensional neutral Gauss-Bonnet AdS black hole. Through calculating the scalar curvature of the Ruppeiner geometry and combining the phase transition, we show that the attractive interaction is dominant in the microstructure of the black hole system. More significantly, there is an intriguing property that the normalized scalar curvature has the same expression for the saturated small and large black hole curves. This implies that although the microstructure is different before and after the small-large black hole phase transition, the interaction between the microscopic constituents keeps unchanged. These results are quite valuable on further understanding the microstructure of the AdS black hole in modified gravity.
Towards efficient representation identification in supervised learning<|sep|>Humans have a remarkable ability to disentangle complex sensory inputs (e.g., image, text) into simple factors of variation (e.g., shape, color) without much supervision. This ability has inspired many works that attempt to solve the following question: how do we invert the data generation process to extract those factors with minimal or no supervision? Several works in the literature on non-linear independent component analysis have established this negative result; without some knowledge of the data generation process or appropriate inductive biases, it is impossible to perform this inversion. In recent years, a lot of progress has been made on disentanglement under structural assumptions, e.g., when we have access to auxiliary information that makes the factors of variation conditionally independent. However, existing work requires a lot of auxiliary information, e.g., in supervised classification, it prescribes that the number of label classes should be at least equal to the total dimension of all factors of variation. In this work, we depart from these assumptions and ask: a) How can we get disentanglement when the auxiliary information does not provide conditional independence over the factors of variation? b) Can we reduce the amount of auxiliary information required for disentanglement? For a class of models where auxiliary information does not ensure conditional independence, we show theoretically and experimentally that disentanglement (to a large extent) is possible even when the auxiliary information dimension is much less than the dimension of the true latent representation.
Cosmological constraints for the Cosmic Defect theory<|sep|>The Cosmic Defect theory has been confronted with four observational constraints: primordial nuclear species abundances emerging from the big bang nucleosynthesis; large scale structure formation in the universe; cosmic microwave background acoustic scale; luminosity distances of type Ia supernovae. The test has been based on a statistical analysis of the a posteriori probabilities for three parameters of the theory. The result has been quite satisfactory and such that the performance of the theory is not distinguishable from the one of the Lambda-CDM theory. The use of the optimal values of the parameters for the calculation of the Hubble constant and the age of the universe confirms the compatibility of the Cosmic Defect approach with observations.
An Efficient Optimal Planning and Control Framework For Quadrupedal Locomotion<|sep|>In this paper, we present an efficient Dynamic Programing framework for optimal planning and control of legged robots. First we formulate this problem as an optimal control problem for switched systems. Then we propose a multi--level optimization approach to find the optimal switching times and the optimal continuous control inputs. Through this scheme, the decomposed optimization can potentially be done more efficiently than the combined approach. Finally, we present a continuous-time constrained LQR algorithm which simultaneously optimizes the feedforward and feedback controller with $O(n)$ time-complexity. In order to validate our approach, we show the performance of our framework on a quadrupedal robot. We choose the Center of Mass dynamics and the full kinematic formulation as the switched system model where the switching times as well as the contact forces and the joint velocities are optimized for different locomotion tasks such as gap crossing, walking and trotting.
An Efficient Schmidt-EKF for 3D Visual-Inertial SLAM<|sep|>It holds great implications for practical applications to enable centimeter-accuracy positioning for mobile and wearable sensor systems. In this paper, we propose a novel, high-precision, efficient visual-inertial (VI)-SLAM algorithm, termed Schmidt-EKF VI-SLAM (SEVIS), which optimally fuses IMU measurements and monocular images in a tightly-coupled manner to provide 3D motion tracking with bounded error. In particular, we adapt the Schmidt Kalman filter formulation to selectively include informative features in the state vector while treating them as nuisance parameters (or Schmidt states) once they become matured. This change in modeling allows for significant computational savings by no longer needing to constantly update the Schmidt states (or their covariance), while still allowing the EKF to correctly account for their cross-correlations with the active states. As a result, we achieve linear computational complexity in terms of map size, instead of quadratic as in the standard SLAM systems. In order to fully exploit the map information to bound navigation drifts, we advocate efficient keyframe-aided 2D-to-2D feature matching to find reliable correspondences between current 2D visual measurements and 3D map features. The proposed SEVIS is extensively validated in both simulations and experiments.
Enhanced Transfer Learning Through Medical Imaging and Patient Demographic Data Fusion<|sep|>In this work we examine the performance enhancement in classification of medical imaging data when image features are combined with associated non-image data. We compare the performance of eight state-of-the-art deep neural networks in classification tasks when using only image features, compared to when these are combined with patient metadata. We utilise transfer learning with networks pretrained on ImageNet used directly as feature extractors and fine tuned on the target domain. Our experiments show that performance can be significantly enhanced with the inclusion of metadata and use interpretability methods to identify which features lead to these enhancements. Furthermore, our results indicate that the performance enhancement for natural medical imaging (e.g. optical images) benefit most from direct use of pre-trained models, whereas non natural images (e.g. representations of non imaging data) benefit most from fine tuning pre-trained networks. These enhancements come at a negligible additional cost in computation time, and therefore is a practical method for other applications.
Deep Ordinal Regression for Pledge Specificity Prediction<|sep|>Many pledges are made in the course of an election campaign, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual analysis. In this paper we collate a novel dataset of manifestos from eleven Australian federal election cycles, with over 12,000 sentences annotated with specificity (e.g., rhetorical vs.\ detailed pledge) on a fine-grained scale. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.
A model of discrete Kolmogorov-type competitive interaction in a two-species ecosystem<|sep|>An ecosystem is a nonlinear dynamical system, its orbits giving rise to the observed complexity in the system. The diverse components of the ecosystem interact in discrete time to give rise to emergent features that determine the trajectory of system's time evolution. The paper studies the evolutionary dynamics of a toy two species ecosystem modelled as a discrete time Kolmogorov system. It is assumed that only the two species comprise the ecosystem and compete with each other for obtaining growth resources, mediated through inter as well as intraspecific coupling constants to obtain resources for growth. Numerical simulations reveal the transition from regular to irregular dynamics and emergence of chaos during the process of evolution of these populations. We find that the presence or absence of chaotic dynamics is being determined by the interspecific interaction coefficients. For values of the interspecific interaction constants widely apart, the system emerges to regular dynamics, implying coexistence of the competing populations on a long term evolutionary scenario.
Sampling Strategy for Fine-Tuning Segmentation Models to Crisis Area under Scarcity of Data<|sep|>The use of remote sensing in humanitarian crisis response missions is well-established and has proven relevant repeatedly. One of the problems is obtaining gold annotations as it is costly and time consuming which makes it almost impossible to fine-tune models to new regions affected by the crisis. Where time is critical, resources are limited and environment is constantly changing, models has to evolve and provide flexible ways to adapt to a new situation. The question that we want to answer is if prioritization of samples provide better results in fine-tuning vs other classical sampling methods under annotated data scarcity? We propose a method to guide data collection during fine-tuning, based on estimated model and sample properties, like predicted IOU score. We propose two formulas for calculating sample priority. Our approach blends techniques from interpretability, representation learning and active learning. We have applied our method to a deep learning model for semantic segmentation, U-Net, in a remote sensing application of building detection - one of the core use cases of remote sensing in humanitarian applications. Preliminary results shows utility in prioritization of samples for tuning semantic segmentation models under scarcity of data condition.
Service-Based Drone Delivery<|sep|>Service delivery is set to experience a major paradigm shift with fast advances in drone technologies coupled with higher expectations from customers and increased competition. We propose a novel service-oriented approach to enable the ubiquitous delivery of packages in a drone-operated skyway network. We discuss the benefits, framework and architecture, contemporary approaches, open challenges and future visioned directions of service-based drone deliveries.
Strongly Universal Reversible Gate Sets<|sep|>It is well-known that the Toffoli gate and the negation gate together yield a universal gate set, in the sense that every permutation of $\{0,1\}^n$ can be implemented as a composition of these gates. Since every bit operation that does not use all of the bits performs an even permutation, we need to use at least one auxiliary bit to perform every permutation, and it is known that one bit is indeed enough. Without auxiliary bits, all even permutations can be implemented. We generalize these results to non-binary logic: If $A$ is a finite set of odd cardinality then a finite gate set can generate all permutations of $A^n$ for all $n$, without any auxiliary symbols. If the cardinality of $A$ is even then, by the same argument as above, only even permutations of $A^n$ can be implemented for large $n$, and we show that indeed all even permutations can be obtained from a finite universal gate set. We also consider the conservative case, that is, those permutations of $A^n$ that preserve the weight of the input word. The weight is the vector that records how many times each symbol occurs in the word. It turns out that no finite conservative gate set can, for all $n$, implement all conservative even permutations of $A^n$ without auxiliary bits. But we provide a finite gate set that can implement all those conservative permutations that are even within each weight class of $A^n$.
Shortcuts to adiabaticity applied to nonequilibrium entropy production: An information geometry viewpoint<|sep|>We apply the method of shortcuts to adiabaticity to nonequilibrium systems. For unitary dynamics, the system Hamiltonian is separated into two parts. One of them defines the adiabatic states for the state to follow and the nonadiabatic transitions are prevented by the other part. This property is implemented to the nonequilibrium entropy production and we find that the entropy is separated into two parts. The separation represents the Pythagorean theorem for the Kullback-Leibler divergence and an information-geometric interpretation is obtained. We also study a lower bound of the entropy, which is applied to derive a trade-off relation between time, entropy and state distance.
Density of warm ionized gas near the Galactic Center: Low radio frequency observations<|sep|>We have observed the Galactic Center (GC) region at 0.154 and 0.255 GHz with the GMRT. A total of 62 compact likely extragalactic sources are detected. Their scattering sizes go down linearly with increasing angular distance from the GC up to about 1 deg. The apparent scattering sizes of sources are more than an order of magnitude down than predicted earlier by the NE2001 model of Galactic electron distribution within 359.5 deg < l < 0.5 deg and -0.5 deg <b <0.5 deg (Hyperstrong scattering region) of the Galaxy. High free-free optical depths are observed towards most of the extended nonthermal sources within 0.6 deg from the GC. Significant variation of optical depth indicate the absorbing medium is patchy at an angular scale of 10' and electron density is ~10 per cc that matches with the NE2001 model. This model predicts the extragalactic (EG) sources to be resolved out from 1.4 GHz interferometric surveys. However, 8 likely EG sources out of 10 expected in the region are present in 1.4 GHz catalog. Ionized interfaces of dense molecular clouds to the ambient medium are most likely responsible for strong scattering and low radio frequency absorption. However, dense GC clouds traced by CS $J=1-0$ emission are found to have a narrow distribution of ~0.2 deg across the Galactic plane. Angular distribution of most of the EG sources seen through the so called Hyperstrong scattering region are random in $b$, and typically ~7 out of 10 sources will not be seen through to the dense molecular clouds, and it explains why most of them are not scatter broadened at 1.4 GHz.
Response Aware Model-Based Collaborative Filtering<|sep|>Previous work on recommender systems mainly focus on fitting the ratings provided by users. However, the response patterns, i.e., some items are rated while others not, are generally ignored. We argue that failing to observe such response patterns can lead to biased parameter estimation and sub-optimal model performance. Although several pieces of work have tried to model users' response patterns, they miss the effectiveness and interpretability of the successful matrix factorization collaborative filtering approaches. To bridge the gap, in this paper, we unify explicit response models and PMF to establish the Response Aware Probabilistic Matrix Factorization (RAPMF) framework. We show that RAPMF subsumes PMF as a special case. Empirically we demonstrate the merits of RAPMF from various aspects.
Investigation of activation cross-section data of proton induced nuclear reactions on rhenium<|sep|>In the frame of systematic investigations of activation cross-section data for different applications the excitation functions of natRe(p,x)185Os, 183mOs, 183gOs, 182Os, 181mOs, 186gRe, 184mRe, 184gRe, 183Re, 182mRe, 182gRe and 181gRe reactions were measured up to 70 MeV. The data for the natRe(p,x)183mOs, 183gOs, 182Os, 181gOs,186gRe, 184mRe,182mRe, 182gRe, 181Re reactions are reported for the first time. Activation method, stacked foil irradiation technique and gamma-spectroscopy for activity detection were used. The experimental data were compared with predictions of three theoretical codes. From the measured cross-section thick target integral yields were also calculated and presented.
An organ deformation model using Bayesian inference to combine population and patient-specific data<|sep|>Objective: Organ deformation models have the potential to improve delivery and reduce toxicity of radiotherapy, but existing data-driven motion models are based on either patient-specific or population data. We propose to combine population and patient-specific data using a Bayesian framework. Our goal is to accurately predict individual motion patterns while using fewer scans than previous models. Approach: We have derived and evaluated two Bayesian deformation models. The models were applied retrospectively to the rectal wall from a cohort of prostate cancer patients. These patients had repeat CT scans evenly acquired throughout radiotherapy. Each model was used to create coverage probability matrices (CPMs). The spatial correlations between these CPMs and ``true'' CPMs, derived from independent scans of the same patient, were calculated. Main results: Spatial correlation with ground truth were significantly higher for the Bayesian deformation models than both patient-specific and population-derived models with 1, 2 or 3 patient-specific scans as input. Statistical motion simulations indicate that this result will also hold for more than 3 scans. Significance: The improvement over known models means that fewer scans per patient are needed to achieve accurate deformation predictions. The models have applications in robust radiotherapy planning and evaluation, among others.
Inducing the Lovelock action<|sep|>We re-analyze a possible ambiguity in the application of dimensional regularization to Einstein-Gauss-Bonnet gravity, arising from the way one treats the Gauss-Bonnet term. It is demonstrated that the addition of such a term to the action gives rise to a non-minimal graviton wave operator, but does not produce new on shell divergences at one loop order in d=4. However, from a d-dimensional perspective the Gauss-Bonnet term is shown to generate new divergences in the form of the six-dimensional Euler density. The conjecture that one would next produce the eight-dimensional Euler term is shown to be false.
Entanglement transfer via Chiral Quantum Walk on a Triangular Chain<|sep|>We investigate the chiral quantum walk (CQW) as a mechanism for an entanglement transfer on a triangular chain structure. We specifically consider two-site spatially entangled cases in short-time and long-time regimes. Using the concurrence as an entanglement measure; fidelity, and the Bures distance as the measure of the quality of the state transfer, we evaluate the success of the entanglement transfer. We compare the entangled state transfer time and quality in CQW against a continuous-time quantum random walk. We also observe the effect of mixed states on the entanglement transfer quality.
Infrared-suppressed QCD coupling and the hadronic contribution to muon g-2<|sep|>A variant of QCD with the coupling suppressed in the infrared (IR) regime, as suggested by large-volume lattice calculations of the Landau-gauge gluon and ghost dressing functions, is considered. The coupling is further restricted by the condition of approximate coincidence with perturbative QCD in the high momentum regime, and by the $\tau$-lepton semihadronic decay rate in the intermediate momentum regime, the rate which is evaluated by a renormalon-motivated resummation method. The obtained coupling turns out to be free of Landau singularities. The $D=4,6$ condensate values of the Adler function are then extracted by application of the Borel sum rules to the OPAL and ALEPH (V+A)-channel data of $\tau$-decay, and the corresponding V-channel condensate values are deduced as well. We then show that the correct value of the hadronic vacuum polarization contribution to the muon anomalous magnetic moment, $a_{\mu}^{\rm had(1)}$, is reproduced by regularizing the $D=4,6$ OPE terms in the V-channel Adler function with IR-regularization masses ${\cal M}_{D/2} \lesssim 1 \ {\rm GeV}$, suggesting the internal consistency of the presented QCD framework.
Automatic Active-Region Identification and Azimuth Disambiguation of the SOLIS/VSM Full-Disk Vector Magnetograms<|sep|>The Vector Spectromagnetograph (VSM) of the NSO's Synoptic Optical Long-Term Investigations of the Sun (SOLIS) facility is now operational and obtains the first-ever vector magnetic field measurements of the entire visible solar hemisphere. To fully exploit the unprecedented SOLIS/VSM data, however, one must first address two critical problems: first, the study of solar active regions requires an automatic, physically intuitive, technique for active-region identification in the solar disk. Second, use of active-region vector magnetograms requires removal of the azimuthal $180^o$-ambiguity in the orientation of the transverse magnetic field component. Here we report on an effort to address both problems simultaneously and efficiently. To identify solar active regions we apply an algorithm designed to locate complex, flux-balanced, magnetic structures with a dominant E-W orientation on the disk. Each of the disk portions corresponding to active regions is thereafter extracted and subjected to the Nonpotential Magnetic Field Calculation (NPFC) method that provides a physically-intuitive solution of the 180-degree ambiguity. Both algorithms have been integrated into the VSM data pipeline and operate in real time, without human intervention. We conclude that this combined approach can contribute meaningfully to our emerging capability for full-disk vector magnetography as pioneered by SOLIS today and will be carried out by ground-based and space-borne magnetographs in the future.
Inferring population history with DIYABC: a user-friendly approach to Approximate Bayesian Computation<|sep|>Genetic data obtained on population samples convey information about their evolutionary history. Inference methods can extract this information (at least partially) but they require sophisticated statistical techniques that have been made available to the biologist community (through computer programs) only for simple and standard situations typically involving a small number of samples. We propose here a computer program (DIYABC) for inference based on Approximate Bayesian Computation (ABC), in which scenarios can be customized by the user to fit many complex situations involving any number of populations and samples. Such scenarios involve any combination of population divergences, admixtures and stepwise population size changes. DIYABC can be used to compare competing scenarios, estimate parameters for one or more scenarios, and compute bias and precision measures for a given scenario and known values of parameters (the current version applies to unlinked microsatellite data). This article describes key methods used in the program and provides its main features. The analysis of one simulated and one real data set, both with complex evolutionary scenarios, illustrates the main possibilities of DIYABC
IRAC Photometric Analysis and the Mid-IR Photometric Properties of Lyman Break Galaxies<|sep|>We present photometric analysis of deep mid-infrared observations obtained by Spitzer/IRAC covering the fields Q1422+2309, Q2233+1341, DSF2237a,b, HDFN, SSA22a,b and B20902+34, giving the number counts and the depths for each field. In a sample of 751 LBGs lying in those fields, 443, 448, 137 and 152 are identified at 3.6microns, 4.5microns, 5.8microns, 8.0microns IRAC bands respectively, expanding their spectral energy distribution to rest-near-infrared and revealing that LBGs display a variety of colours. Their rest-near-infrared properties are rather inhomogeneous, ranging from those that are bright in IRAC bands and exhibit [R]-[3.6] > 1.5 colours to those that are faint or not detected at all in IRAC bands with [R]-[3.6] < 1.5 colours and these two groups of LBGs are investigated. We compare the mid-IR colours of the LBGs with the colours of star-forming galaxies and we find that LBGs have colours consistent with star-foming galaxies at z~3. The properties of the LBGs detected in the 8microns IRAC band (rest frame K-band) are examined separately, showing that they exhibit redder [R]-[3.6] colours than the rest of the population and that IRAC 8microns band can be used as a diagnostic tool, to separate AGN dominated objects from normal star-forming galaxies at z~3
Valence quark and meson cloud contributions for the gamma* Lambda -> Lambda* and gamma* Sigma0 -> Lambda* reactions<|sep|>We estimate the valence quark contributions for the gamma* Y -> Lambda* (Y=Lambda, Sigma0) electromagnetic transition form factors. We focus particularly on the case Lambda*=Lambda(1670) as an analog reaction with gamma* N -> N(1535). The results are compared with those obtained from chiral unitary model, where the Lambda* resonance is dynamically generated and thus the electromagnetic structure comes directly from the meson cloud excitation of the baryon ground states. The form factors for the case Y=Sigma0 in particular, depend crucially on the two real phase (sign) combination, a phase between the Lambda and Lambda* states, and the other, the phase between the Lambda and Sigma0 radial wave functions. Depending on the combination of these two phases, the form factors for the gamma* Sigma0 -> Lambda* reaction can be enhanced or suppressed. Therefore, there is a possibility to determine the phase combination by experiments.
Spectra of absolute instruments from the WKB approximation<|sep|>We calculate frequency spectra of absolute optical instruments using the WKB approximation. The resulting eigenfrequencies approximate the actual values very accurately, in some cases they even give the exact values. Our calculations confirm results obtained previously by a completely different method. In particular, the eigenfrequencies of absolute instruments form tight groups that are almost equidistantly spaced. We demonstrate our method and its results on several examples.
Network as a computer: ranking paths to find flows<|sep|>We explore a simple mathematical model of network computation, based on Markov chains. Similar models apply to a broad range of computational phenomena, arising in networks of computers, as well as in genetic, and neural nets, in social networks, and so on. The main problem of interaction with such spontaneously evolving computational systems is that the data are not uniformly structured. An interesting approach is to try to extract the semantical content of the data from their distribution among the nodes. A concept is then identified by finding the community of nodes that share it. The task of data structuring is thus reduced to the task of finding the network communities, as groups of nodes that together perform some non-local data processing. Towards this goal, we extend the ranking methods from nodes to paths. This allows us to extract some information about the likely flow biases from the available static information about the network.
Artificial Intelligence Enabled Traffic Monitoring System<|sep|>Manual traffic surveillance can be a daunting task as Traffic Management Centers operate a myriad of cameras installed over a network. Injecting some level of automation could help lighten the workload of human operators performing manual surveillance and facilitate making proactive decisions which would reduce the impact of incidents and recurring congestion on roadways. This article presents a novel approach to automatically monitor real time traffic footage using deep convolutional neural networks and a stand-alone graphical user interface. The authors describe the results of research received in the process of developing models that serve as an integrated framework for an artificial intelligence enabled traffic monitoring system. The proposed system deploys several state-of-the-art deep learning algorithms to automate different traffic monitoring needs. Taking advantage of a large database of annotated video surveillance data, deep learning-based models are trained to detect queues, track stationary vehicles, and tabulate vehicle counts. A pixel-level segmentation approach is applied to detect traffic queues and predict severity. Real-time object detection algorithms coupled with different tracking systems are deployed to automatically detect stranded vehicles as well as perform vehicular counts. At each stages of development, interesting experimental results are presented to demonstrate the effectiveness of the proposed system. Overall, the results demonstrate that the proposed framework performs satisfactorily under varied conditions without being immensely impacted by environmental hazards such as blurry camera views, low illumination, rain, or snow.
PatchPerPix for Instance Segmentation<|sep|>We present a novel method for proposal free instance segmentation that can handle sophisticated object shapes which span large parts of an image and form dense object clusters with crossovers. Our method is based on predicting dense local shape descriptors, which we assemble to form instances. All instances are assembled simultaneously in one go. To our knowledge, our method is the first non-iterative method that yields instances that are composed of learnt shape patches. We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy data of cell nuclei. We show furthermore that our method also applies to 3d light microscopy data of Drosophila neurons, which exhibit extreme cases of complex shape clusters
Intrinsic bottom and its impact on heavy new physics at the LHC<|sep|>Heavy quark parton distribution functions (PDFs) play an important role in several Standard Model and New Physics processes. Most analyses rely on the assumption that the charm and bottom PDFs are generated perturbatively by gluon splitting and do not involve any non-perturbative degrees of freedom. On the other hand, non- perturbative, intrinsic heavy quark parton distributions have been predicted in the literature. We demonstrate that to a very good approximation the scale-evolution of the intrinsic heavy quark content of the nucleon is governed by non-singlet evolution equations. This allows to analyze the intrinsic heavy quark distributions without having to resort to a full-fledged global analysis of parton distribution functions. We exploit this freedom to model intrinsic bottom distributions which are so far missing in the literature. We estimate the impact of the non-perturbative contribution to the charm and bottom-quark PDFs and on several important parton-parton luminosities at the LHC.
Towards the construction of a model to describe the inter-ELM evolution of the pedestal on MAST<|sep|>Pedestal profiles that span the ELM cycle have been obtained and used to test the idea that the pedestal pressure gradient in MAST is limited by the onset of Kinetic Ballooning Modes (KBMs). During the inter-ELM period of a regularly type I ELM-ing discharge on MAST, the pressure pedestal height and width increase together while the pressure gradient increases by only 15 % during the ELM cycle. Stability analyses show that the pedestal region over which infinite-n ballooning modes are unstable also broadens during the ELM cycle. To test the relationship between the width of the region that is unstable to n = \infty ideal magnetohydrodynamic ballooning modes and KBMs the gyrokinetic code, GS2, has been used for microstability analysis of the edge plasma region in MAST. The gyrokinetic simulations find that KBM modes with twisting parity are the dominant microinstabilities in the steep pedestal region, with a transition to more slowly growing tearing parity modes in the shallower pressure gradient core region immediately inside the pedestal top. The region over which KBMs are unstable increases during the ELM cycle, and a good correlation is found between the regions unstable to KBMs and infinite-n ideal ballooning modes.
The Consequences of Gamma-ray Burst Jet Opening Angle Evolution on the Inferred Star Formation Rate<|sep|>Gamma-ray burst (GRB) data suggest that the jets from GRBs in the high redshift universe are more narrowly collimated than those at lower redshifts. This implies that we detect relatively fewer long GRB progenitor systems (i.e. massive stars) at high redshifts, because a greater fraction of GRBs have their jets pointed away from us. As a result, estimates of the star formation rate (from the GRB rate) at high redshifts may be diminished if this effect is not taken into account. In this paper, we estimate the star formation rate (SFR) using the observed GRB rate, accounting for an evolving jet opening angle. We find that the SFR in the early universe (z > 3) can be up to an order of magnitude higher than the canonical estimates, depending on the severity of beaming angle evolution and the fraction of stars that make long gamma-ray bursts. Additionally, we find an excess in the SFR at low redshifts, although this lessens when accounting for evolution of the beaming angle. Finally, under the assumption that GRBs do in fact trace canonical forms of the cosmic SFR, we constrain the resulting fraction of stars that must produce GRBs, again accounting for jet beaming-angle evolution. We find this assumption suggests a high fraction of stars in the early universe producing GRBs - a result that may, in fact, support our initial assertion that GRBs do not trace canonical estimates of the SFR.
Context-Aware Neural Machine Translation Learns Anaphora Resolution<|sep|>Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).
Extending the baseline: Spitzer Mid-Infrared Photometry of Globular Cluster Systems in the Centaurus A and Sombrero Galaxies<|sep|>Spitzer IRAC mid-infrared photometry is presented for the globular cluster (GC) systems of the NGC 5128 ("Centaurus A") and NGC 4594 ("Sombrero") galaxies. Existing optical photometric and spectroscopic are combined with this new data in a comprehensive optical to mid-IR colour catalogue of 260 GCs. Empirical colour-metallicity relationships are derived for all optical to mid-IR colour combinations. These colours prove to be very effective quantities to test the photometric predictions of simple stellar population (SSP) models. In general, four SSP models show larger discrepancies between each other and the data at bluer wavelengths, especially at high metallicities. Such differences become very important when attempting to use colour-colour model predictions to constrain the ages of stellar populations. Furthermore, the age-substructure determined from colour-colour diagrams and 91 NGC 5128 GCs with spectroscopic ages from Beasley et al. (2008) are inconsistent, suggesting any apparent GC system age-substructure implied by a colour-colour analysis must be verified independently. Unlike blue wavebands, certain optical to mid-IR colours are insensitive to the flux from hot horizontal branch stars and thus provide an excellent metallicity proxy. The NGC 5128 GC system shows strong bimodality in the optical R-band to mid-IR colour distributions, hence proving it is bimodal in metallicity. In this new colour space, a colour-magnitude trend, a "blue tilt", is found in the NGC 5128 metal-poor GC data. The NGC 5128 young GCs do not contribute to this trend. [abridged]
A Survey on Federated Learning and its Applications for Accelerating Industrial Internet of Things<|sep|>Federated learning (FL) brings collaborative intelligence into industries without centralized training data to accelerate the process of Industry 4.0 on the edge computing level. FL solves the dilemma in which enterprises wish to make the use of data intelligence with security concerns. To accelerate industrial Internet of things with the further leverage of FL, existing achievements on FL are developed from three aspects: 1) define terminologies and elaborate a general framework of FL for accommodating various scenarios; 2) discuss the state-of-the-art of FL on fundamental researches including data partitioning, privacy preservation, model optimization, local model transportation, personalization, motivation mechanism, platform & tools, and benchmark; 3) discuss the impacts of FL from the economic perspective. To attract more attention from industrial academia and practice, a FL-transformed manufacturing paradigm is presented, and future research directions of FL are given and possible immediate applications in Industry 4.0 domain are also proposed.
RankGAN: A Maximum Margin Ranking GAN for Generating Faces<|sep|>We present a new stage-wise learning paradigm for training generative adversarial networks (GANs). The goal of our work is to progressively strengthen the discriminator and thus, the generators, with each subsequent stage without changing the network architecture. We call this proposed method the RankGAN. We first propose a margin-based loss for the GAN discriminator. We then extend it to a margin-based ranking loss to train the multiple stages of RankGAN. We focus on face images from the CelebA dataset in our work and show visual as well as quantitative improvements in face generation and completion tasks over other GAN approaches, including WGAN and LSGAN.
Lepton Flavour Violation in minimal grand-unified type II seesaw models<|sep|>We revisit minimal non-supersymmetric models of SU(5) Grand Unification with the type II seesaw mechanism as the origin of neutrino masses. Imposing the requirement of gauge coupling unification and the proton lifetime bounds, we perform a Bayesian fit and obtain robust quantitative information on the mass scales of the beyond the Standard Model particles. We then study lepton-flavour-violating (LFV) processes induced by the type II scalar triplet and its SU(5) partners, showing that the interplay of upcoming searches for different LFV observables can provide additional information on the masses of the new particles, as well as non-trivial constraints on neutrino parameters.
Electronic band crossing in sliding bilayer graphene: Tight-binding calculations and symmetry group representation analysis<|sep|>Dirac points are found to emerge due to the crossing of bands in the electronic structure of bilayer graphene for configurations in which the alignment between two hexagonal lattices preserves the parallelism of the armchair/zigzag lines between two layers. On the base of electronic calculations using a tight-binding model for the $\pi$ bands it is shown that the crossing of the energy-band dispersion curves occurs in the vicinity of the corner points of the hexagonal Brillouin zone. Group representation theory analysis confirms the emergence of such Dirac points. It is demonstrated that the band crossings at generic $\mathbf{k}$ points are guaranteed by the compatibility relations between the symmetries of eigenstates at the high-symmetry $\mathbf{k}$ points in the Brillouin zone. The presence of Dirac points governs the geometrical properties of the energy surfaces, and thus the topological structure of the Fermi energy surface and the energy spectrum.
Chemical abundance analysis of symbiotic giants - III. Metallicity and CNO abundance patterns in 24 southern systems<|sep|>The elemental abundances of symbiotic giants are essential to address the role of chemical composition in the evolution of symbiotic binaries, to map their parent population, and to trace their mass transfer history. However, the number of symbiotic giants with fairly well determined photospheric composition is still insufficient for statistical analyses. This is the third in a series of papers on the chemical composition of symbiotic giants determined from high resolution (R ~ 50000), near-IR spectra. Here we present results for 24 S-type systems. Spectrum synthesis methods employing standard local thermal equilibrium analysis and atmosphere models were used to obtain photospheric abundances of CNO and elements around the iron peak (Fe, Ti, Ni, and Sc). Our analysis reveals metallicities distributed in a wide range from slightly supersolar ([Fe/H] ~ +0.35 dex) to significantly subsolar ([Fe/H] ~ -0.8 dex) but principally with near-solar and slightly subsolar metallicity ([Fe/H] ~ -0.4 to -0.3 dex). The enrichment in 14N isotope, found in all these objects, indicates that the giants have experienced the first dredge-up. This was confirmed in a number of objects by the low 12C/13C ratio (5-23). We found that the relative abundance of [Ti/Fe] is generally large in red symbiotic systems.
Spatial-Temporal Mitosis Detection in Phase-Contrast Microscopy via Likelihood Map Estimation by 3DCNN<|sep|>Automated mitotic detection in time-lapse phasecontrast microscopy provides us much information for cell behavior analysis, and thus several mitosis detection methods have been proposed. However, these methods still have two problems; 1) they cannot detect multiple mitosis events when there are closely placed. 2) they do not consider the annotation gaps, which may occur since the appearances of mitosis cells are very similar before and after the annotated frame. In this paper, we propose a novel mitosis detection method that can detect multiple mitosis events in a candidate sequence and mitigate the human annotation gap via estimating a spatiotemporal likelihood map by 3DCNN. In this training, the loss gradually decreases with the gap size between ground truth and estimation. This mitigates the annotation gaps. Our method outperformed the compared methods in terms of F1- score using a challenging dataset that contains the data under four different conditions.
Universal properties of 3d O(4) symmetric models: The scaling function of the free energy density and its derivatives<|sep|>We present direct representations of the scaling functions of the 3d O(4) model which are relevant for comparisons to other models, in particular QCD. This is done in terms of expansions in the scaling variable z=t/h^{1/\beta\delta}. The expansions around z=0 and the corresponding asymptotic ones for z --> +/- infty, overlap such that no interpolation is needed. We explicitly present the expansion coefficients which have been determined numerically from data of a previous high statistics simulation of the O(4) model on a three-dimensional lattice of linear extension L=120. This allows to derive smooth representations of the first three derivatives of the scaling function of the free energy density, which determine universal properties of up to sixth order cumulants of net charge fluctuations in QCD.
State Identification for Labeled Transition Systems with Inputs and Outputs<|sep|>For Finite State Machines (FSMs) a rich testing theory has been developed to discover aspects of their behavior and ensure their correct functioning. Although this theory is widely used, e.g., to check conformance of protocol implementations, its applicability is limited by restrictions of the FSM framework: the fact that inputs and outputs alternate in an FSM, and outputs are fully determined by the previous input and state. Labeled Transition Systems with inputs and outputs (LTSs), as studied in ioco testing theory, provide a richer framework for testing component oriented systems, but lack the algorithms for test generation from FSM theory. In this article, we propose an algorithm for the fundamental problem of state identification during testing of LTSs. Our algorithm is a direct generalization of the well-known algorithm for computing adaptive distinguishing sequences for FSMs proposed by Lee & Yannakakis. Our algorithm has to deal with so-called compatible states, states that cannot be distinguished in case of an adversarial system-under-test. Analogous to the result of Lee & Yannakakis, we prove that if an (adaptive) test exists that distinguishes all pairs of incompatible states of an LTS, our algorithm will find one. In practice, such adaptive tests typically do not exist. However, in experiments with an implementation of our algorithm on an industrial benchmark, we find that tests produced by our algorithm still distinguish more than 99% of the incompatible state pairs.
Franck-Condon Effect in Central Spin System<|sep|>We study the quantum transitions of a central spin surrounded by a collective-spin environment. It is found that the influence of the environmental spins on the absorption spectrum of the central spin can be explained with the analog of the Franck-Condon (FC) effect in conventional electron-phonon interaction system. Here, the collective spins of the environment behave as the vibrational mode, which makes the electron to be transitioned mainly with the so-called "vertical transitions" in the conventional FC effect. The "vertical transition" for the central spin in the spin environment manifests as, the certain collective spin states of the environment is favored, which corresponds to the minimal change in the average of the total spin angular momentum.
mGNN: Generalizing the Graph Neural Networks to the Multilayer Case<|sep|>Networks are a powerful tool to model complex systems, and the definition of many Graph Neural Networks (GNN), Deep Learning algorithms that can handle networks, has opened a new way to approach many real-world problems that would be hardly or even untractable. In this paper, we propose mGNN, a framework meant to generalize GNNs to the case of multi-layer networks, i.e., networks that can model multiple kinds of interactions and relations between nodes. Our approach is general (i.e., not task specific) and has the advantage of extending any type of GNN without any computational overhead. We test the framework into three different tasks (node and network classification, link prediction) to validate it.
Microscopic pairing theory of a binary Bose mixture with interspecies attractions: bosonic BEC-BCS crossover and ultradilute low-dimensional quantum droplets<|sep|>Ultradilute quantum droplets are intriguing new state of matter, in which the attractive mean-field force can be balanced by the repulsive force from quantum fluctuations to avoid collapse. Here, we present a microscopic theory of ultradilute quantum droplets in three-, one- and two-dimensional two-component Bose-Bose mixtures, by generalizing the conventional Bogoliubov theory to include the bosonic pairing arising from the interspecies attraction. Our pairing theory is fully equivalent to a variational approach and hence gives an upper bound for the energy of quantum droplets. In three dimensions, we predict the existence of a strongly interacting Bose droplet at the crossover from Bose-Einstein condensates (BEC) to Bardeen--Cooper--Schrieffer (BCS) superfluids and map out the bosonic BEC-BCS crossover phase diagram. In one dimension, we find that the energy of the one-dimensional Bose droplet calculated by the pairing theory is in an excellent agreement with the latest diffusion Monte Carlo simulation {[}Phys. Rev. Lett. \textbf{122}, 105302 (2019){]}, for nearly all the interaction strengths at which quantum droplets exist. In two dimensions, we show that Bose droplets disappear and may turn into a soliton-like many-body bound state, when the interspecies attraction exceeds a critical value. Below the threshold, the pairing theory predicts more or less the same results as the Bogoliubov theory derived by Petrov and Astrakharchik {[}Phys. Rev. Lett. \textbf{117}, 100401 (2016){]}. The predicted energies from both theories are higher than the diffusion Monte Carlo results, due to the weak interspecies attraction and the increasingly important role played by the beyond-Bogoliubov-approximation effect in two dimensions.
Constraining the Bulk Composition of Disintegrating Exoplanets Using Combined Transmission Spectra from JWST and SPICA<|sep|>Disintegrating planets are ultra-short-period exoplanets that appear to have a comet-like dust tail. They are commonly interpreted as low-mass planets whose solid surface is evaporating and whose tail is made of recondensing minerals. Transmission spectroscopy of the dust tails could thus allow us to directly probe the elementary compositions of the planets. Previous work already investigated the feasibility of such observations using the JWST mid-infrared instrument. In this study, we explore if one can obtain a strong constrain on the tail composition by adding spectroscopy at longer wavelengths using SPICA mid-infrared instrument. We use a simple model for the spatial distribution of the dust tails and produce their synthetic transmission spectra assuming various dust compositions. We find that combined infrared spectra from JWST and SPICA will allow us to diagnose various components of the dust tails. JWST will be able to detect silicate and carbide absorption features with a feature-to-noise ratio of $\gtrsim$ 3 in the tail transmission spectrum of a disintegrating planet located within 100 pc from the Earth with a transit depth deeper than 0.5%. SPICA can distinguish between Fe- and Mg-bearing crystalline silicates for planets at $\lesssim$ 100 pc with a transit depth of $\gtrsim$ 2%. Transit searches with current and future space telescopes (e.g., $TESS$ and PLATO) will provide ideal targets for such spectroscopic observations.
The 2006 November outburst of EG Aquarii: the SU UMa nature revealed<|sep|>We report time-resolved CCD photometry of the cataclysmic variable EG Aquarii during the 2006 November outburst During the outburst, superhumps were unambiguously detected with a mean period of 0.078828(6) days, firstly classifying the object as an SU UMa-type dwarf nova. It also turned out that the outburst contained a precursor. At the end of the precursor, immature profiles of humps were observed. By a phase analysis of these humps, we interpreted the features as superhumps. This is the second example that the superhumps were shown during a precursor. Near the maximum stage of the outburst, we discovered an abrupt shift of the superhump period by ${\sim}$ 0.002 days. After the supermaximum, the superhump period decreased at the rate of $\dot{P}/P$=$-8.2{\times}10^{-5}$, which is typical for SU UMa-type dwarf novae. Although the outburst light curve was characteristic of SU UMa-type dwarf novae, long-term monitoring of the variable shows no outbursts over the past decade. We note on the basic properties of long period and inactive SU UMa-type dwarf novae.
Multi-probe study of excited states in $\mathrm{^{12}C}$: disentangling the sources of monopole strength between the Hoyle state and $E_{x} = 13$ MeV<|sep|>Knowledge of the low-lying monopole strength in $\mathrm{^{12}C}$, the Hoyle state in particular, is crucial for our understanding of both the astrophysically important $3\alpha$ reaction and of $\alpha$-particle clustering. The $\mathrm{^{12}C}(\alpha, \alpha^{\prime})\mathrm{^{12}C}$ and $\mathrm{^{14}C}(p, t)\mathrm{^{12}C}$ reactions were employed to populate states in $^{12}$C. A self-consistent, simultaneous analysis of the inclusive spectra with lineshapes was performed, which accounted for distortion due to nuclear dynamics and experimental effects. Clear evidence was found for excess monopole strength at $E_{x} \sim 9$ MeV, particularly in the $\mathrm{^{12}C}(\alpha, \alpha^{\prime})\mathrm{^{12}C}$ reaction at $0^{\circ}$. This additional strength cannot be reproduced by the previously established monopole states between $E_{x} = 7$ and 13 MeV. An additional $0^{+}$ state at $E_{x} \sim 9$ MeV yielded a significantly improved fit of the data and is the leading candidate for the predicted breathing-mode excitation of the Hoyle state. Alternatively, the results may suggest that a more sophisticated, physically motivated parameterization of the astrophysically important monopole strengths in $\mathrm{^{12}C}$ is required.
Pure Gauge Configurations and Solutions to Fermionic Superstring Field Theories Equations of Motion<|sep|>Recent results on solutions to the equation of motion of the cubic fermionic string field theory and an equivalence of non-polynomial and cubic string field theories are discussed. To have a possibility to deal with both GSO(+) and GSO(-) sectors in the uniform way a matrix formulation for the NS fermionic SFT is used. In constructions of analytical solutions to open string field theories truncated pure gauge configurations parameterized by wedge states play an essential role. The matrix form of this parametrization for the NS fermionic SFT is presented. Using the cubic open superstring field theory as an example we demonstrate explicitly that for the large parameter of the perturbation expansion these truncated pure gauge configurations give divergent contributions to the equation of motion on the subspace of the wedge states. The perturbation expansion is cured by adding extra terms that are nothing but the terms necessary for the equation of motion contracted with the solution itself to be satisfied.
A Graph Model for Imperative Computation<|sep|>Scott's graph model is a lambda-algebra based on the observation that continuous endofunctions on the lattice of sets of natural numbers can be represented via their graphs. A graph is a relation mapping finite sets of input values to output values. We consider a similar model based on relations whose input values are finite sequences rather than sets. This alteration means that we are taking into account the order in which observations are made. This new notion of graph gives rise to a model of affine lambda-calculus that admits an interpretation of imperative constructs including variable assignment, dereferencing and allocation. Extending this untyped model, we construct a category that provides a model of typed higher-order imperative computation with an affine type system. An appropriate language of this kind is Reynolds's Syntactic Control of Interference. Our model turns out to be fully abstract for this language. At a concrete level, it is the same as Reddy's object spaces model, which was the first "state-free" model of a higher-order imperative programming language and an important precursor of games models. The graph model can therefore be seen as a universal domain for Reddy's model.
Alpenglow - A Signature for Chameleons in Axion-Like Particle Search Experiments<|sep|>We point out that chameleon field theories might reveal themselves as an 'afterglow' effect in axion-like particle search experiments due to chameleon-photon conversion in a magnetic field. We estimate the parameter space which is accessible by currently available technology and find that afterglow experiments could constrain this parameter space in a way complementary to gravitational and Casimir force experiments.In addition, one could reach photon-chameleon couplings which are beyond the sensitivity of common laser polarization experiments. We also sketch the idea of a Fabry-Perot cavity with chameleons which could increase the experimental sensitivity significantly.
Linear quantum systems: a tutorial<|sep|>The purpose of this tutorial is to give a brief introduction to linear quantum control systems. The mathematical model of linear quantum control systems is presented first, then some fundamental control-theoretic notions such as stability, controllability and observability are given, which are closely related to several important concepts in quantum information science such as decoherence-free subsystems, quantum non-demolition variables, and back-action evasion measurements. After that, quantum Gaussian states are introduced, in particular, an information-theoretic uncertainty relation is presented which often gives a better bound for mixed Gaussian states than the well-known Heisenberg uncertainty relation. The quantum Kalman filter is presented for quantum linear systems, which is the quantum analogy of the Kalman filter for classical (namely, non-quantum-mechanical) linear systems. The quantum Kalman canonical decomposition for quantum linear systems is recorded, and its application is illustrated by means of a recent experiment. As single- and multi-photon states are useful resources in quantum information technology, the response of quantum linear systems to these types of input is presented. Finally, coherent feedback control of quantum linear systems is briefly introduced, and a recent experiment is used to demonstrate the effectiveness of quantum linear systems and networks theory.dback control of quantum linear systems is briefly introduced, and a recent experiment is used to demonstrate the effectiveness of quantum linear systems and networks theory.
Constraining extra dimensions on cosmological scales with LISA future gravitational wave siren data<|sep|>We investigate the idea that current cosmic acceleration could be the consequence of gravitational leakage into extra dimensions on cosmological scales rather than the result of a non-zero cosmological constant, and consider the ability of future gravitational-wave siren observations to probe this phenomenon and constrain the parameters of phenomenological models of this gravitational leakage. In theories that include additional non-compact spacetime dimensions, the gravitational leakage intro extra dimensions leads to a reduction in the amplitude of observed gravitational waves and thereby a systematic discrepancy between the distance inferred to such sources from GW and EM observations. We investigate the capability of a gravitational space interferometer such as LISA to probe this modified gravity on large scales. We find that the extent to which LISA will be able to place limits on the number of spacetime dimensions and other cosmological parameters characterising modified gravity will strongly depend on the actual number and redshift distribution of sources, together with the uncertainty on the GW measurements. A relatively small number of sources ($\sim 1$) and high measurement uncertainties would strongly restrict the ability of LISA to place meaningful constraints on the parameters in cosmological scenarios where gravity is only five-dimensional and modified at scales larger than about $\sim 4$ times the Hubble radius. Conversely, if the number of sources observed amounts to a four-year average of $\sim 27$, then in the most favourable cosmological scenarios LISA has the potential to place meaningful constraints on the cosmological parameters with a precision of $\sim 1\%$ on the number of dimensions and $\sim 7.5\%$ on the scale beyond which gravity is modified, thereby probing the late expansion of the universe up to a redshift of $\sim 8$.
Attention-Based Models for Speech Recognition<|sep|>Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.
Fast and Accurate Optical Fiber Channel Modeling Using Generative Adversarial Network<|sep|>In this work, a new data-driven fiber channel modeling method, generative adversarial network (GAN) is investigated to learn the distribution of fiber channel transfer function. Our investigation focuses on joint channel effects of attenuation, chromic dispersion, self-phase modulation (SPM), and amplified spontaneous emission (ASE) noise. To achieve the success of GAN for channel modeling, we modify the loss function, design the condition vector of input and address the mode collapse for the long-haul transmission. The effective architecture, parameters, and training skills of GAN are also displayed in the paper. The results show that the proposed method can learn the accurate transfer function of the fiber channel. The transmission distance of modeling can be up to 1000 km and can be extended to arbitrary distance theoretically. Moreover, GAN shows robust generalization abilities under different optical launch powers, modulation formats, and input signal distributions. Comparing the complexity of GAN with the split-step Fourier method (SSFM), the total multiplication number is only 2% of SSFM and the running time is less than 0.1 seconds for 1000-km transmission, versus 400 seconds using the SSFM under the same hardware and software conditions, which highlights the remarkable reduction in complexity of the fiber channel modeling.
Limits of the Stokes and Navier-Stokes equations in a punctured periodic domain<|sep|>In this paper we treat three problems on a two-dimensional `punctured periodic domain': we take $\Omega_r=(-L,L)^2\setminus D_r$, where $D_r=B(0,r)$ is the disc of radius $r$ centred at the origin. We impose periodic boundary conditions on the boundary of the box $\Omega=(-L,L)^2$, and Dirichlet boundary conditions on the circumference of the disc. In this setting we consider the Poisson equation, the Stokes equations, and the time-dependent Navier-Stokes equations, all with a fixed forcing function $f$ (which must satisfy $\int_\Omega f=0$ for the stationary problems), and examine the behaviour of solutions as $r\to0$. In all three cases we show convergence of the solutions to those of the limiting problem, i.e.\ the problem posed on all of $\Omega$ with periodic boundary conditions.
Robust Estimators in Generalized Pareto Models<|sep|>This paper deals with optimally-robust parameter estimation in generalized Pareto distributions (GPDs). These arise naturally in many situations where one is interested in the behavior of extreme events as motivated by the Pickands-Balkema-de Haan extreme value theorem (PBHT). The application we have in mind is calculation of the regulatory capital required by Basel II for a bank to cover operational risk. In this context the tail behavior of the underlying distribution is crucial. This is where extreme value theory enters, suggesting to estimate these high quantiles parameterically using, e.g. GPDs. Robust statistics in this context offers procedures bounding the influence of single observations, so provides reliable inference in the presence of moderate deviations from the distributional model assumptions, respectively from the mechanisms underlying the PBHT.
On the Geometry of Supersymmetric Quantum Mechanical Systems<|sep|>We consider some simple examples of supersymmetric quantum mechanical systems and explore their possible geometric interpretation with the help of geometric aspects of real Clifford algebras. This leads to natural extensions of the considered systems to higher dimensions and more complicated potentials.
Structure and Magnetic Fields in the Precessing Jet System SS433 III. Evolution of the Intrinsic Brightness of the Jets from a Deep Multi-Epoch VLA Campaign<|sep|>We present a sequence of five deep observations of SS433 made over the summer of 2007 using the VLA in the A configuration at 5 and 8 GHz. In this paper we study the brightness profiles of the jets and their time evolution. We also examine the spectral index distribution in the source. We find (as previously reported from the analysis of a single earlier image) that the profiles of the east and west jets are remarkably similar if projection and Doppler beaming are taken into account. The sequence of five images allows us to disentangle the evolution of brightness of individual pieces of jet from the variations of jet power originating at the core. We find that the brightness of each piece of the jet fades as an exponential function of age (or distance from the core), exp(-tau/tau'), where tau is the age at emission and tau' = 55.9 +- 1.7 days. This evolutionary model describes both the east and west jets equally well. There is also significant variation (by a factor of at least five) in jet power with birth epoch, with the east and west jets varying in synchrony. The lack of deceleration between the scale of the optical Balmer line emission (10^15 cm) and that of the radio emission (10^17 cm) requires that the jet material is much denser than its surroundings. We find that the density ratio must exceed 300:1.
K\"ahler-driven Tribrid Inflation<|sep|>We discuss a new class of tribrid inflation models in supergravity, where the shape of the inflaton potential is dominated by effects from the K\"ahler potential. Tribrid inflation is a variant of hybrid inflation which is particularly suited for connecting inflation with particle physics, since the inflaton can be a D-flat combination of charged fields from the matter sector. In models of tribrid inflation studied so far, the inflaton potential was dominated by either loop corrections or by mixing effects with the waterfall field (as in "pseudosmooth" tribrid inflation). Here we investigate the third possibility, namely that tribrid inflation is dominantly driven by effects from higher-dimensional operators of the K\"ahler potential. We specify for which superpotential parameters the new regime is realized and show how it can be experimentally distinguished from the other two (loop-driven and "pseudosmooth") regimes.
NIR jets from a clustered region of massive star formation: Morphology and composition in the IRAS 18264-1152 region<|sep|>Massive stars form deeply embedded in their parental clouds, making it challenging to directly observe these stars and their immediate environments. It is known that accretion and ejection processes are intrinsically related, thus observing massive protostellar outflows can provide crucial information about the processes governing massive star formation close to the central engine. We aim to probe the IRAS 18264-1152 (G19.88-0.53) high-mass star-forming complex in the near infrared (NIR) through its molecular hydrogen (H2) jets to analyse the morphology and composition of the line emitting regions and to compare with other outflow tracers. We observed the H2 NIR jets via K-band (1.9-2.5um) observations obtained with the integral field units VLT/SINFONI and VLT/KMOS. SINFONI provides the highest NIR angular resolution achieved so far for the central region (~0.2''). We compared the geometry of the NIR outflows with that of the associated molecular outflow probed by CO (2-1) emission mapped with SMA. We identify nine point sources. Four of these display a rising continuum in the K-band and are BrG emitters, revealing that they are young, potentially jet-driving sources. The spectro-imaging analysis focusses on the H2 jets, for which we derived visual extinction, temperature, column density, area, and mass. The intensity, velocity, and excitation maps based on H2 emission strongly support the existence of a protostellar cluster, with at least two (and up to four) different large-scale outflows. The literature is in agreement with the outflow morphology found here. We derived a stellar density of ~4000 stars pc^-3. Our study reveals the presence of several outflows driven by young sources from a forming cluster of young, massive stars. The derived stellar number density together with the geometry of the outflows suggest that stars can form in a relatively ordered manner in this cluster.
Feedback-Controlled Sequential Lasso Screening<|sep|>One way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features. Prior research has shown that sequential screening methods offer the greatest promise in this endeavor. Most existing work on sequential screening targets the context of tuning parameter selection, where one screens and solves a sequence of $N$ lasso problems with a fixed grid of geometrically spaced regularization parameters. In contrast, we focus on the scenario where a target regularization parameter has already been chosen via cross-validated model selection, and we then need to solve many lasso instances using this fixed value. In this context, we propose and explore a feedback controlled sequential screening scheme. Feedback is used at each iteration to select the next problem to be solved. This allows the sequence of problems to be adapted to the instance presented and the number of intermediate problems to be automatically selected. We demonstrate our feedback scheme using several datasets including a dictionary of approximate size 100,000 by 300,000.
Back to Futures<|sep|>Common approaches to concurrent programming begin with languages whose semantics are naturally sequential and add new constructs that provide limited access to concurrency, as exemplified by futures. This approach has been quite successful, but often does not provide a satisfactory theoretical backing for the concurrency constructs, and it can be difficult to give a good semantics that allows a programmer to use more than one of these constructs at a time. We take a different approach, starting with a concurrent language based on a Curry-Howard interpretation of adjoint logic, to which we add three atomic primitives that allow us to encode sequential composition and various forms of synchronization. The resulting language is highly expressive, allowing us to encode futures, fork/join parallelism, and monadic concurrency in the same framework. Notably, since our language is based on adjoint logic, we are able to give a formal account of linear futures, which have been used in complexity analysis by Blelloch and Reid-Miller. The uniformity of this approach means that we can similarly work with many of the other concurrency primitives in a linear fashion, and that we can mix several of these forms of concurrency in the same program to serve different purposes.
Ensemble Learning Applied to Classify GPS Trajectories of Birds into Male or Female<|sep|>We describe our first-place solution to the Animal Behavior Challenge (ABC 2018) on predicting gender of bird from its GPS trajectory. The task consisted in predicting the gender of shearwater based on how they navigate themselves across a big ocean. The trajectories are collected from GPS loggers attached on shearwaters' body, and represented as a variable-length sequence of GPS points (latitude and longitude), and associated meta-information, such as the sun azimuth, the sun elevation, the daytime, the elapsed time on each GPS location after starting the trip, the local time (date is trimmed), and the indicator of the day starting the from the trip. We used ensemble of several variants of Gradient Boosting Classifier along with Gaussian Process Classifier and Support Vector Classifier after extensive feature engineering and we ranked first out of 74 registered teams. The variants of Gradient Boosting Classifier we tried are CatBoost (Developed by Yandex), LightGBM (Developed by Microsoft), XGBoost (Developed by Distributed Machine Learning Community). Our approach could easily be adapted to other applications in which the goal is to predict a classification output from a variable-length sequence.
Simplifying Multiloop Integrands and Ultraviolet Divergences of Gauge Theory and Gravity Amplitudes<|sep|>We use the duality between color and kinematics to simplify the construction of the complete four-loop four-point amplitude of N=4 super-Yang-Mills theory, including the nonplanar contributions. The duality completely determines the amplitude's integrand in terms of just two planar graphs. The existence of a manifestly dual gauge-theory amplitude trivializes the construction of the corresponding N=8 supergravity integrand, whose graph numerators are double copies (squares) of the N=4 super-Yang-Mills numerators. The success of this procedure provides further nontrivial evidence that the duality and double-copy properties hold at loop level. The new form of the four-loop four-point supergravity amplitude makes manifest the same ultraviolet power counting as the corresponding N=4 super-Yang-Mills amplitude. We determine the amplitude's ultraviolet pole in the critical dimension of D=11/2, the same dimension as for N=4 super-Yang-Mills theory. Strikingly, exactly the same combination of vacuum integrals (after simplification) describes the ultraviolet divergence of N=8 supergravity as the subleading-in-1/N_c^2 single-trace divergence in N=4 super-Yang-Mills theory.
The High-$z$ Universe Confronts Warm Dark Matter: Galaxy Counts, Reionization and the Nature of Dark Matter<|sep|>We use $N$-body simulations to show that high-redshift galaxy counts provide an interesting constraint on the nature of dark matter, specifically Warm Dark Matter (WDM), owing to the lack of early structure formation these models. Our simulations include three WDM models with thermal-production masses of 0.8 keV, 1.3 keV, and 2.6 keV, as well as CDM. Assuming a relationship between dark halo mass and galaxy luminosity that is set by the observed luminosity function at bright magnitudes, we find that 0.8 keV WDM is disfavored by direct galaxy counts in the Hubble Ultra Deep Field at $>\!\!10\sigma$. Similarly, 1.3 keV WDM is statistically inconsistent at $2.2\sigma$. Future observations with JWST (and possibly HST via the Frontier Fields) could rule out $1.3$ keV WDM at high significance, and may be sensitive to WDM masses greater than 2.6 keV. We also examine the ability of galaxies in these WDM models to reionize the universe, and find that 0.8 keV and 1.3 keV WDM produce optical depths to the Cosmic Microwave Background (CMB) that are inconsistent at 68% C.L. with current Planck results, even with extremely high ionizing radiation escape fractions, and 2.6 keV WDM requires an optimistic escape fraction to yield an optical depth consistent with Planck data. Although CMB optical depth calculations are model dependent, we find a strong challenge for stellar processes alone to reionize the universe in a 0.8 keV and 1.3 keV WDM cosmology.
Scaling Properties of Ge-SixGe1-x Core-Shell Nanowire Field Effect Transistors<|sep|>We demonstrate the fabrication of high-performance Ge-SixGe1-x core-shell nanowire field-effect transistors with highly doped source and drain, and systematically investigate their scaling properties. Highly doped source and drain regions are realized by low energy boron implantation, which enables efficient carrier injection with a contact resistance much lower than the nanowire resistance. We extract key device parameters, such as intrinsic channel resistance, carrier mobility, effective channel length, and external contact resistance, as well as benchmark the device switching speed and ON/OFF current ratio.
Synergies between Vera C. Rubin Observatory, Nancy Grace Roman Space Telescope, and Euclid Mission: Constraining Dark Energy with Type Ia Supernovae<|sep|>We review the needs of the supernova community for improvements in survey coordination and data sharing that would significantly boost the constraints on dark energy using samples of Type Ia supernovae from the Vera C. Rubin Observatories, the \textit{Nancy Grace Roman Space Telescope}, and the \textit{Euclid} Mission. We discuss improvements to both statistical and systematic precision that the combination of observations from these experiments will enable. For example, coordination will result in improved photometric calibration, redshift measurements, as well as supernova distances. We also discuss what teams and plans should be put in place now to start preparing for these combined data sets. Specifically, we request coordinated efforts in field selection and survey operations, photometric calibration, spectroscopic follow-up, pixel-level processing, and computing. These efforts will benefit not only experiments with Type Ia supernovae, but all time-domain studies, and cosmology with multi-messenger astrophysics.
Investigating light neutralinos at neutrino telescopes<|sep|>We present a detailed analysis of the neutrino-induced muon signals coming from neutralino pair-annihilations inside the Sun and the Earth with particular emphasis for light neutralinos. The theoretical model considered is an effective MSSM without gaugino-mass unification, which allows neutralinos of light masses (below 50 GeV). The muon events are divided in through-going and stopping muons, using the geometry of the Super-Kamiokande detector. In the evaluation of the signals, we take into account the relevant hadronic and astrophysics uncertainties and include neutrino oscillation and propagation properties in a consistent way. We derive the ranges of neutralino masses which could be explored at neutrino telescopes with a low muon-energy threshold (around 1 GeV) depending on the category of events and on the values of the various astrophysics and particle-physics parameters. A final analysis is focussed to the upward muon fluxes which could be generated by those neutralino configurations which are able to explain the annual modulation data of the DAMA/LIBRA experiment. We show how combining these data with measurements at neutrino telescopes could help in pinning down the features of the DM particle and in restraining the ranges of the many quantities (of astrophysics and particle-physics origins) which enter in the evaluations and still suffer from large uncertainties.
Multi-Region Neural Representation: A novel model for decoding visual stimuli in human brains<|sep|>Multivariate Pattern (MVP) classification holds enormous potential for decoding visual stimuli in the human brain by employing task-based fMRI data sets. There is a wide range of challenges in the MVP techniques, i.e. decreasing noise and sparsity, defining effective regions of interest (ROIs), visualizing results, and the cost of brain studies. In overcoming these challenges, this paper proposes a novel model of neural representation, which can automatically detect the active regions for each visual stimulus and then utilize these anatomical regions for visualizing and analyzing the functional activities. Therefore, this model provides an opportunity for neuroscientists to ask this question: what is the effect of a stimulus on each of the detected regions instead of just study the fluctuation of voxels in the manually selected ROIs. Moreover, our method introduces analyzing snapshots of brain image for decreasing sparsity rather than using the whole of fMRI time series. Further, a new Gaussian smoothing method is proposed for removing noise of voxels in the level of ROIs. The proposed method enables us to combine different fMRI data sets for reducing the cost of brain studies. Experimental studies on 4 visual categories (words, consonants, objects and nonsense photos) confirm that the proposed method achieves superior performance to state-of-the-art methods.
Power-balancing dual-port grid-forming power converter control for renewable integration and hybrid AC/DC power systems<|sep|>In this work, we investigate grid-forming (GFM) control for dc/ac power converters in emerging power systems that contain ac and dc networks, renewable generation, and conventional generation. We propose a novel power-balancing GFM control strategy that simultaneously forms the converter ac and dc voltage (i.e., dual-port GFM), unifies standard grid-following (GFL) and GFM functions, and is backwards compatible with conventional machine-based generation. Notably, in contrast to state-of-the-art control architectures that use a mix of grid-forming and grid-following control, dual-port GFM control can be used independently of the converter power source or network configuration. Our main contribution are stability conditions that cover emerging hybrid ac/dc networks as well as machines and converters with and without controlled power source, that only require knowledge of the system topology. Finally, a detailed case study is used to illustrate and validate the results.
Private Information Retrieval from Non-Replicated Databases<|sep|>We consider the problem of private information retrieval (PIR) of a single message out of $K$ messages from $N$ non-colluding and non-replicated databases. Different from the majority of the existing literature, which considers the case of replicated databases where all databases store the same content in the form of all $K$ messages, here, we consider the case of non-replicated databases under a special non-replication structure where each database stores $M$ out of $K$ messages and each message is stored across $R$ different databases. This generates an $R$-regular graph structure for the storage system where the vertices of the graph are the messages and the edges are the databases. We derive a general upper bound for $M=2$ that depends on the graph structure. We then specialize the problem to storage systems described by two special types of graph structures: cyclic graphs and \emph{fully-connected graphs}. We prove that the PIR capacity for the case of cyclic graphs is $\frac{2}{K+1}$, and the PIR capacity for the case of fully-connected graphs is $\min\{\frac{2}{K},\frac{1}{2}\}$. To that end, we propose novel achievable schemes for both graph structures that are capacity-achieving. The central insight in both schemes is to introduce dependency in the queries submitted to databases that do not contain the desired message, such that the requests can be compressed. In both cases, the results show severe degradation in PIR capacity due to non-replication.
Extragalactic Point Source Search in WMAP 61 and 94 GHz Data<|sep|>We report the results of an extragalactic point source search using the 61 and 94 GHz (V- and W-band) temperature maps from the Wilkinson Microwave Anisotropy Probe (WMAP). Applying a method that cancels the ``noise'' due to the CMB anisotropy signal, we find in the $|b| > 10\degr$ region 31 sources in the first-year maps and 64 sources in the three-year co-added maps, at a $5\sigma$ level. The 1$\sigma$ position uncertainties are 1.6' and 1.4' each. The increased detections and improved positional accuracy are expected from the higher signal-to-noise ratio of WMAP three-year data. All sources detected in the first-year maps are repeatedly detected in the three-year maps, which is a strong proof of the consistency and reliability of this method. Among all the detections, 21 are new, i.e. not in the WMAP three-year point source catalog. We associate all but two of them with known objects. The two unidentified sources are likely to be variable or extended as observations through VLA, CARMA and ATCA all show non-detection at the nominal locations. We derive the source count distribution at WMAP V-band by combining our verified detections with sources from the WMAP three-year catalog. Assuming the effect of source clustering is negligible, the contribution to the power spectrum from faint sources below 0.75 Jy is estimated to be $(2.4\pm0.8) \times 10^{-3} \mu K^2$ sr for V-band, which implies a source correction amplitude $A = 0.012\pm0.004 \mu K^2$ sr.
Reactive Liquid: Optimized Liquid Architecture for Elastic and Resilient Distributed Data Processing<|sep|>Today's most prominent IT companies are built on the extraction of insight from data, and data processing has become crucial in data-intensive businesses. Nevertheless, the size of data which should be processed is growing significantly fast. The pace of the data growing has changed the nature of data processing. Today, data-intensive industries demand highly scalable and fault tolerant data processing architectures which can handle the massive amount of data. In this paper, we presented a distributed architecture for elastic and resilient data processing based on the Liquid which is a nearline and offline big data architecture. We used the Reactive Manifesto to design the architecture highly reactive to workload changes and failures. We evaluate our architecture by drawing some numerical comparisons between our architecture prototype and the Liquid prototype. The performed evaluation shows that our architecture can be more scalable against workload and more resilient against failures than the Liquid architecture is.
$f(R)$ Dual Theories of Quintessence : Expansion-Collapse Duality<|sep|>The accelerated expansion of the universe demands presence of an exotic matter, namely the dark energy. Though the cosmological constant fits this role very well, a scalar field minimally coupled to gravity, or quintessence, can also be considered as a viable alternative for the cosmological constant. We study $f(R)$ gravity models which can lead to an effective description of dark energy implemented by quintessence fields in Einstein gravity, using the Einstein frame-Jordan frame duality. For a family of viable quintessence models, the reconstruction of the $f(R)$ function in the Jordan frame consists of two parts. We first obtain a perturbative solution of $f(R)$ in the Jordan frame, applicable near the present epoch. Second, we obtain an asymptotic solution for $f(R)$, consistent with the late time limit of the Einstein frame if the quintessence field drives the universe. We show that for certain class of viable quintessence models, the Jordan frame universe grows to a maximum finite size, after which it begins to collapse back. Thus, there is a possibility that in the late time limit where the Einstein frame universe continues to expand, the Jordan frame universe collapses. The condition for this expansion-collapse duality is then generalized to time varying equations of state models, taking into account the presence of non-relativistic matter or any other component in the Einstein frame universe. This mapping between an expanding geometry and a collapsing geometry at the field equation level may have interesting potential implications on the growth of perturbations therein at late times.
Nuclear effects in neutral current quasi-elastic neutrino interactions<|sep|>The interpretation of the charged (CCQE) and neutral (NCE) current quasi elastic events collected by the MiniBooNE collaboration involves a number of unresolved issues. While it has been suggested that the data can be explained in terms of an effective nucleon axial mass, $M_A$, the results of our theoretical calculations suggest that the CCQE and NCE samples cannot be described by the same value of $M_A$. We argue that the disagreement between theory and data may arise from the uncertainties associated with the flux average procedure. We also analyze the role of the strange quark in NCE interactions and find that, due to a cancellation between proton and neutron contributions, it turns out to be negligible.
Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic Experiences for Robot Action Execution<|sep|>We present a novel deep neural network architecture for representing robot experiences in an episodic-like memory which facilitates encoding, recalling, and predicting action experiences. Our proposed unsupervised deep episodic memory model 1) encodes observed actions in a latent vector space and, based on this latent encoding, 2) infers most similar episodes previously experienced, 3) reconstructs original episodes, and 4) predicts future frames in an end-to-end fashion. Results show that conceptually similar actions are mapped into the same region of the latent vector space. Based on these results, we introduce an action matching and retrieval mechanism, benchmark its performance on two large-scale action datasets, 20BN-something-something and ActivityNet and evaluate its generalization capability in a real-world scenario on a humanoid robot.
The Generalised Colouring Numbers on Classes of Bounded Expansion<|sep|>The generalised colouring numbers $\mathrm{adm}_r(G)$, $\mathrm{col}_r(G)$, and $\mathrm{wcol}_r(G)$ were introduced by Kierstead and Yang as generalisations of the usual colouring number, also known as the degeneracy of a graph, and have since then found important applications in the theory of bounded expansion and nowhere dense classes of graphs, introduced by Ne\v{s}et\v{r}il and Ossona de Mendez. In this paper, we study the relation of the colouring numbers with two other measures that characterise nowhere dense classes of graphs, namely with uniform quasi-wideness, studied first by Dawar et al. in the context of preservation theorems for first-order logic, and with the splitter game, introduced by Grohe et al. We show that every graph excluding a fixed topological minor admits a universal order, that is, one order witnessing that the colouring numbers are small for every value of $r$. Finally, we use our construction of such orders to give a new proof of a result of Eickmeyer and Kawarabayashi, showing that the model-checking problem for successor-invariant first-order formulas is fixed-parameter tractable on classes of graphs with excluded topological minors.
Automated and Explainable Ontology Extension Based on Deep Learning: A Case Study in the Chemical Domain<|sep|>Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction enables them to maintain a high quality, allowing them to be widely accepted across their community. However, the manual development process does not scale for large domains. We present a new methodology for automatic ontology extension and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We trained a Transformer-based deep learning model on the leaf node structures from the ChEBI ontology and the classes to which they belong. The model is then capable of automatically classifying previously unseen chemical structures. The proposed model achieved an overall F1 score of 0.80, an improvement of 6 percentage points over our previous results on the same dataset. Additionally, we demonstrate how visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions.
Beam Designs for Millimeter-Wave Backhaul with Dual-Polarized Uniform Planar Arrays<|sep|>This paper proposes hybrid beamforming designs for millimeter-wave (mmWave) multiple-input multiple-output (MIMO) backhaul systems equipped with uniform planar arrays (UPAs) of dual-polarization antennas at both the transmit and receive base stations. The proposed beamforming designs are to near-optimally solve optimization problems taking the dual-polarization UPA structure into account. Based on the solutions of optimization problems, this paper shows it is possible to generate the optimal dual-polarization beamformer from the optimal single-polarization beamformer sharing the same optimality. As specific examples, squared error and magnitude of inner product are considered respectively for optimization criteria. To optimize proposed beamformers, partial channel information is needed, and the use of low overhead pilot sequences is also proposed to figure out the required information. Simulation results verify that the resulting beamformers have the most uniform gain (with the squared error criterion) or the highest average gain (with the magnitude of inner product criterion) in the covering region with the UPA of dual-polarization antennas.
Modeling Transitivity in Complex Networks<|sep|>An important source of high clustering coefficient in real-world networks is transitivity. However, existing approaches for modeling transitivity suffer from at least one of the following problems: i) they produce graphs from a specific class like bipartite graphs, ii) they do not give an analytical argument for the high clustering coefficient of the model, and iii) their clustering coefficient is still significantly lower than real-world networks. In this paper, we propose a new model for complex networks which is based on adding transitivity to scale-free models. We theoretically analyze the model and provide analytical arguments for its different properties. In particular, we calculate a lower bound on the clustering coefficient of the model which is independent of the network size, as seen in real-world networks. More than theoretical analysis, the main properties of the model are evaluated empirically and it is shown that the model can precisely simulate real-world networks from different domains with and different specifications.
Using Duality in Circuit Complexity<|sep|>We investigate in a method for proving separation results for abstract classes of languages. A well established method to characterize varieties of regular languages are identities. We use a recently established generalization of these identities to non-regular languages by Gehrke, Grigorieff, and Pin: so called equations, which are capable of describing arbitrary Boolean algebras of languages. While the main concern of their result is the existence of these equations, we investigate in a general method that could allow to find equations for language classes in an inductive manner. Thereto we extend an important tool -- the block product or substitution principle -- known from logic and algebra, to non-regular language classes. Furthermore, we abstract this concept by defining it directly as an operation on (non-regular) language classes. We show that this principle can be used to obtain equations for certain circuit classes, given equations for the gate types. Concretely, we demonstrate the applicability of this method by obtaining a description via equations for all languages recognized by circuit families that contain a constant number of (inner) gates, given a description of the gate types via equations.
The large deviations of the whitening process in random constraint satisfaction problems<|sep|>Random constraint satisfaction problems undergo several phase transitions as the ratio between the number of constraints and the number of variables is varied. When this ratio exceeds the satisfiability threshold no more solutions exist; the satisfiable phase, for less constrained problems, is itself divided in an unclustered regime and a clustered one. In the latter solutions are grouped in clusters of nearby solutions separated in configuration space from solutions of other clusters. In addition the rigidity transition signals the appearance of so-called frozen variables in typical solutions: beyond this threshold most solutions belong to clusters with an extensive number of variables taking the same values in all solutions of the cluster. In this paper we refine the description of this phenomenon by estimating the location of the freezing transition, corresponding to the disappearance of all unfrozen solutions (not only typical ones). We also unveil phase transitions for the existence and uniqueness of locked solutions, in which all variables are frozen. From a technical point of view we characterize atypical solutions with a number of frozen variables different from the typical value via a large deviation study of the dynamics of a stripping process (whitening) that unveils the frozen variables of a solution, building upon recent works on atypical trajectories of the bootstrap percolation dynamics. Our results also bear some relevance from an algorithmic perspective, previous numerical studies having shown that heuristic algorithms of various kinds usually output unfrozen solutions.
A linear-time algorithm for semitotal domination in strongly chordal graphs<|sep|>In a graph $G=(V,E)$ with no isolated vertex, a dominating set $D \subseteq V$, is called a semitotal dominating set if for every vertex $u \in D$ there is another vertex $v \in D$, such that distance between $u$ and $v$ is at most two in $G$. Given a graph $G=(V,E)$ without isolated vertices, the Minimum Semitotal Domination problem is to find a minimum cardinality semitotal dominating set of $G$. The semitotal domination number, denoted by $\gamma_{t2}(G)$, is the minimum cardinality of a semitotal dominating set of $G$. The decision version of the problem remains NP-complete even when restricted to chordal graphs, chordal bipartite graphs, and planar graphs. Galby et al. in [6] proved that the problem can be solved in polynomial time for bounded MIM-width graphs which includes many well known graph classes, but left the complexity of the problem in strongly chordal graphs unresolved. Henning and Pandey in [20] also asked to resolve the complexity status of the problem in strongly chordal graphs. In this paper, we resolve the complexity of the problem in strongly chordal graphs by designing a linear-time algorithm for the problem.
Elastic Coulomb breakup of $^{34}$Na<|sep|>Purpose : The aim of this paper is to study the elastic Coulomb breakup of $^{34}$Na on $^{208}$Pb to give us a core of $^{33}$Na with a neutron and in the process we try and investigate the one neutron separation energy and the ground state configuration of $^{34}$Na. Method : A fully quantum mechanical Coulomb breakup theory within the architecture of post-form finite range distorted wave Born approximation extended to include the effects of deformation is used to research the elastic Coulomb breakup of $^{34}$Na on $^{208}$Pb at 100 MeV/u. The triple differential cross-section calculated for the breakup is integrated over the desired components to find the total cross-section, momentum and angular distributions as well as the average momenta, along with the energy-angular distributions. Results : The total one neutron removal cross-section is calculated to test the possible ground state configurations of $^{34}$Na. The average momentum results along with energy-angular calculations indicate $^{34}$Na to have a halo structure. The parallel momentum distributions with narrow full widths at half maxima signify the same. Conclusion : We have attempted to analyse the possible ground state configurations of $^{34}$Na and in congruity with the patterns in the `island of inversion' conclude that even without deformation, $^{34}$Na should be a neutron halo with a predominant contribution to its ground state most probably coming from $^{33}$Na($3/2^{+}$) $\otimes$ $2p_{3/2}\nu$ configuration. We also surmise that it would certainly be useful and rewarding to test our predictions with an experiment to put stricter limits on its ground state configuration and binding energy.
Resource-efficient adaptive Bayesian tracking of magnetic fields with a quantum sensor<|sep|>Single-spin quantum sensors, for example based on nitrogen-vacancy centres in diamond, provide nanoscale mapping of magnetic fields. In applications where the magnetic field may be changing rapidly, total sensing time is crucial and must be minimised. Bayesian estimation and adaptive experiment optimisation can speed up the sensing process by reducing the number of measurements required. These protocols consist of computing and updating the probability distribution of the magnetic field based on measurement outcomes and of determining optimized acquisition settings for the next measurement. However, the computational steps feeding into the measurement settings of the next iteration must be performed quickly enough to allow for real-time updates. This article addresses the issue of computational speed by implementing an approximate Bayesian estimation technique, where probability distributions are approximated by a finite sum of Gaussian functions. Given that only three parameters are required to fully describe a Gaussian density, we find that in many cases, the magnetic field probability distribution can be described by fewer than ten parameters, achieving a reduction in computation time by factor 10 compared to existing approaches. For T2* = 1 micro second, only a small decrease in computation time is achieved. However, in these regimes, the proposed Gaussian protocol outperforms the existing one in tracking accuracy.
Structural and excited-state properties of oligoacene crystals from first principles<|sep|>Molecular crystals are a prototypical class of van der Waals (vdW) bound organic materials with excited state properties relevant for optoelectronics applications. Predicting the structure and excited state properties of molecular crystals presents a challenge for electronic structure theory, as standard approximations to density functional theory (DFT) do not capture long range vdW dispersion interactions and do not yield excited state properties. In this work, we use a combination of DFT including vdW forces) using both non local correlation functionals and pair wise correction methods (together with many body perturbation theory (MBPT) to study the geometry and excited states, respectively, of the entire series of oligoacene crystals, from benzene to hexacene. We find that vdW methods can predict lattice constants within 1 percent of the experimental measurements, on par with the previously reported accuracy of pairwise approximations for the same systems. We further find that excitation energies are sensitive to geometry, but if optimized geometries are used MBPT can yield excited state properties within a few tenths of an eV from experiment. We elucidate trends in MBPT computed charged and neutral excitation energies across the acene series and discuss the role of common approximations used in MBPT.
Interface-resolved direct numerical simulations of sediment transport in a turbulent oscillatory boundary layer<|sep|>The flow within an oscillatory boundary layer, which approximates the flow generated by propagating sea waves of small amplitude close to the bottom, is simulated numerically by integrating Navier-Stokes and continuity equations. The bottom is made up of spherical particles, free to move, which mimic sediment grains. The approach allows to fully-resolve the flow around the particles and to evaluate the forces and torques that the fluid exerts on their surface. Then, the dynamics of sediments is explicitly computed by means of Newton-Euler equations. For the smallest value of the flow Reynolds number presently simulated, the flow regime turns out to fall in the intermittently turbulent regime such that turbulence appears when the free stream velocity is close to its largest values but the flow recovers a laminar like behaviour during the remaining phases of the cycle. For the largest value of the Reynolds number turbulence is significant almost during the whole flow cycle. The evaluation of the sediment transport rate allows to estimate the reliability of the empirical predictors commonly used to estimate the amount of sediments transported by the sea waves. For large values of the Shields parameter, the sediment flow rate during the accelerating phases does not differ from that observed during the decelerating phases. However, for relatively small values of the Shields parameter, the amount of moving particles depends not only on the bottom shear stress but also on flow acceleration. Moreover, the numerical results provide information on the role that turbulent eddies have on sediment dynamics.
Constraints on decaying dark matter from weak lensing and cluster counts<|sep|>We revisit a cosmological constraint on dark matter decaying into dark radiation at late times. In Enqvist et al. (2015), we mainly focused on the effects of decaying dark matter (DDM) on the cosmic microwave background (CMB) and nonlinear matter power spectrum. Extending our previous analysis, here we use N-body simulation to investigate how DDM affects the halo mass function. This allows us to incorporate the cluster counts observed by the Sunyaev-Zel'dovich effect to study a bound on the lifetime of DDM. We also update the data of CMB and cosmic shear power spectrum with the Planck 2015 results and KiDS450 observations, respectively. From these cosmological observations, we obtain an lower bound on the lifetime $\Gamma^{-1}\ge 175\,$Gyr from the Planck2015 results (CMB+SZ cluster count) combined with the KiDS450 and the recent measurements of the baryon acoustic scale.
The effect of Galactic foreground subtraction on redshifted 21-cm observations of quasar HII regions<|sep|>We assess the impact of Galactic synchrotron foreground removal on the observation of high-redshift quasar HII regions in redshifted 21-cm emission. We consider the case where a quasar is observed in an intergalactic medium (IGM) whose ionisation structure evolves slowly relative to the light crossing time of the HII region, as well as the case where the evolution is rapid. The latter case is expected towards the end of the reionisation era where the highest redshift luminous quasars will be observed. In the absence of foregrounds the fraction of neutral hydrogen in the IGM could be measured directly from the contrast between the HII region and surrounding IGM. However, we find that foreground removal lowers the observed contrast between the HII region and the IGM. This indicates that measurement of the neutral fraction would require modelling to correct for this systematic effect. On the other hand, foreground removal does not modify the most prominent features of the 21-cm maps. Using a simple algorithm we demonstrate that measurements of the size and shape of observed HII regions will not be affected by continuum foreground removal. Moreover, measurements of these quantities will not be adversely affected by the presence of a rapidly evolving IGM.
Boosting Deep Neural Networks with Geometrical Prior Knowledge: A Survey<|sep|>While Deep Neural Networks (DNNs) achieve state-of-the-art results in many different problem settings, they are affected by some crucial weaknesses. On the one hand, DNNs depend on exploiting a vast amount of training data, whose labeling process is time-consuming and expensive. On the other hand, DNNs are often treated as black box systems, which complicates their evaluation and validation. Both problems can be mitigated by incorporating prior knowledge into the DNN. One promising field, inspired by the success of convolutional neural networks (CNNs) in computer vision tasks, is to incorporate knowledge about symmetric geometrical transformations of the problem to solve. This promises an increased data-efficiency and filter responses that are interpretable more easily. In this survey, we try to give a concise overview about different approaches to incorporate geometrical prior knowledge into DNNs. Additionally, we try to connect those methods to the field of 3D object detection for autonomous driving, where we expect promising results applying those methods.
Random phase approximation with exchange for an accurate description of crystalline polymorphism<|sep|>We determine the correlation energy of BN, SiO$_2$ and ice polymorphs employing a recently developed RPAx (random phase approximation with exchange) approach. The RPAx provides larger and more accurate polarizabilities as compared to the RPA, and captures effects of anisotropy. In turn, the correlation energy, defined as an integral over the density-density response function, gives improved binding energies without the need for error cancellation. Here, we demonstrate that these features are crucial for predicting the relative energies between low- and high-pressure polymorphs of different coordination number as, e.g., between $\alpha$-quartz and stishovite in SiO$_2$, or layered and cubic BN. Furthermore, a reliable (H$_2$O)$_2$ potential energy surface is obtained, necessary for describing the various phases of ice. The RPAx gives results comparable to other high-level methods such as coupled cluster and quantum Monte Carlo, also in cases where the RPA breaks down. Although higher computational cost than RPA we observe a faster convergence with respect to the number of eigenvalues in the response function.
Criticality of Large Delay Tolerant Networks via Directed Continuum Percolation in Space-Time<|sep|>We study delay tolerant networking (DTN) and in particular, its capacity to store, carry and forward messages so that the messages eventually reach their final destinations. We approach this broad question in the framework of percolation theory. To this end, we assume an elementary mobility model, where nodes arrive to an infinite plane according to a Poisson point process, move a certain distance L, and then depart. In this setting, we characterize the mean density of nodes required to support DTN style networking. In particular, under the given assumptions, we show that DTN is feasible when the mean node degree is greater than 4 e(g), where parameter g=L/d is the ratio of the distance L to the transmission range d, and e(g) is the critical reduced number density of tilted cylinders in a directed continuum percolation model. By means of Monte Carlo simulations, we give numerical values for e(g). The asymptotic behavior of e(g) when g tends to infinity is also derived from a fluid flow analysis.
Spatial Interactions of Peers and Performance of File Sharing Systems<|sep|>We propose a new model for peer-to-peer networking which takes the network bottlenecks into account beyond the access. This model allows one to cope with key features of P2P networking like degree or locality constraints or the fact that distant peers often have a smaller rate than nearby peers. We show that the spatial point process describing peers in their steady state then exhibits an interesting repulsion phenomenon. We analyze two asymptotic regimes of the peer-to-peer network: the fluid regime and the hard--core regime. We get closed form expressions for the mean (and in some cases the law) of the peer latency and the download rate obtained by a peer as well as for the spatial density of peers in the steady state of each regime, as well as an accurate approximation that holds for all regimes. The analytical results are based on a mix of mathematical analysis and dimensional analysis and have important design implications. The first of them is the existence of a setting where the equilibrium mean latency is a decreasing function of the load, a phenomenon that we call super-scalability.
MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning<|sep|>The increased availability and usage of modern medical imaging induced a strong need for automatic medical image segmentation. Still, current image segmentation platforms do not provide the required functionalities for plain setup of medical image segmentation pipelines. Already implemented pipelines are commonly standalone software, optimized on a specific public data set. Therefore, this paper introduces the open-source Python library MIScnn. The aim of MIScnn is to provide an intuitive API allowing fast building of medical image segmentation pipelines including data I/O, preprocessing, data augmentation, patch-wise analysis, metrics, a library with state-of-the-art deep learning models and model utilization like training, prediction, as well as fully automatic evaluation (e.g. cross-validation). Similarly, high configurability and multiple open interfaces allow full pipeline customization. Running a cross-validation with MIScnn on the Kidney Tumor Segmentation Challenge 2019 data set (multi-class semantic segmentation with 300 CT scans) resulted into a powerful predictor based on the standard 3D U-Net model. With this experiment, we could show that the MIScnn framework enables researchers to rapidly set up a complete medical image segmentation pipeline by using just a few lines of code. The source code for MIScnn is available in the Git repository: https://github.com/frankkramer-lab/MIScnn.
Chance-Constrained Two-Stage Unit Commitment under Uncertain Load and Wind Power Output Using Bilinear Benders Decomposition<|sep|>In this paper, we study unit commitment (UC) problems considering the uncertainty of load and wind power generation. UC problem is formulated as a chance-constrained two-stage stochastic programming problem where the chance constraint is used to restrict the probability of load imbalance. In addition to the conventional mixed integer linear programming formulation using Big-M, we present the bilinear mixed integer formulation of chance constraint, and then derive its linear counterpart using McCormick linearization method. Then, we develop a bilinear variant of Benders decomposition method, which is an easy-to-implement algorithm, to solve the resulting large-scale linear counterpart. Our results on typical IEEE systems demonstrate that (i) the bilinear mixed integer programming formula-tion is stronger than the conventional one; (ii) the proposed Benders decomposition algorithm is generally an order of magnitude faster than using a professional solver to directly compute both linear and bilinear chance-constrained UC models.
Optimally fuzzy temporal memory<|sep|>Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register---a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system.
The unusually large population of Blazhko variables in the globular cluster NGC 5024 (M53)<|sep|>We report the discovery of amplitude and phase modulations typical of the Blazhko effect in 22 RRc and 9 RRab type RR Lyrae stars in NGC 5024 (M53). This brings the confirmed Blazhko variables in this cluster to 23 RRc and 11 RRab, that represent 66% and 37% of the total population of RRc and RRab stars in the cluster respectively, making NGC 5024 the globular cluster with the largest presently known population of Blazhko RRc stars. We place a lower limit on the overall incidence rate of the Blazhko effect among the RR Lyrae population in this cluster of 52%. New data have allowed us to refine the pulsation periods. The limitations imposed by the time span and sampling of our data prevents reliable estimations of the modulation periods. The amplitudes of the modulations range between 0.02 and 0.39 mag. The RRab and RRc are neatly separated in the CMD, and the RRc Blazhko variables are on averge redder than their stable couterparts; these two facts may support the hypothesis that the HB evolution in this cluster is towards the red and that the Blazhko modulations in the RRc stars are connected with the pulsation mode switch.
Probabilistic aspects of ZM-groups<|sep|>In this paper we study probabilistic aspects such as (cyclic) subgroup commutativity degree and (cyclic) factorization number of ZM-groups. We show that these quantities can be computed using the sizes of the conjugacy classes of these groups. In the end, we point out a class of groups whose (cyclic) subgroup commutativity degree vanishes asymptotically.
The Dynamical State of The Serpens South Filamentary Infrared Dark Cloud<|sep|>We present the results of N$_2$H$^+$ ($J=1-0$) observations toward Serpens South, the nearest cluster-forming, infrared dark cloud. The physical quantities are derived by fitting the hyperfine structure of N$_2$H$^+$. The Herschel and 1.1-mm continuum maps show that a pc-scale filament fragments into three clumps with radii of $0.1-0.2$ pc and masses of $40-230M_\odot$. We find that the clumps contain smaller-scale ($\sim 0.04$ pc) structures, i.e., dense cores. We identify 70 cores by applying CLUMPFIND to the N$_2$H$^+$ data cube. In the central cluster-forming clump, the excitation temperature and line-width tend to be large, presumably due to protostellar outflow feedback and stellar radiation. However, for all the clumps, the virial ratios are evaluated to be $0.1-0.3$, indicating that the internal motions play only a minor role in the clump support. The clumps exhibit no free-fall, but low-velocity infall, and thus the clumps should be supported by additional forces. The most promising force is the globally-ordered magnetic field observed toward this region. We propose that the Serpens South filament was close to magnetically-critical and ambipolar diffusion triggered the cluster formation. We find that the northern clump, which shows no active star formation, has a mass and radius comparable to the central cluster-forming clump, and therefore, it is a likely candidate of a {\it pre-protocluster clump}. The initial condition for cluster formation is likely to be a magnetically-supported clump of cold, quiescent gas. This appears to contradict the accretion-driven turbulence scenario, for which the turbulence in the clumps is maintained by the accretion flow.
Van der Waals-like phase transition from holographic entanglement entropy in Lorentz breaking massive gravity<|sep|>In this paper, phase transition of AdS black holes in lorentz breaking massive gravity has been studied in the framework of holography. We find that there is a first order phase transition(FPT) and second order phase transition(SPT) both in Bekenstein-Hawking entropy(BHE)-temperature plane and holographic entanglement entropy(HEE)-temperature plane. Furthermore, for the FPT, the equal area law is checked and for the SPT, the critical exponent of the heat capacity is also computed. Our results confirm that the phase structure of HEE is similar to that of BHE in lorentz breaking massive gravity, which implies that HEE and BHE have some potential underlying relationship.
On the frequency correlations of low-frequency QPOs with kilohertz QPOs in accreting millisecond X-ray pulsars<|sep|>We investigate frequency correlations of low frequency (LF, <80 Hz) and kHz quasi-periodic oscillations (QPOs) using the complete RXTE data sets on 6 accreting millisecond X-ray pulsars (AMXPs) and compare them to those of non-pulsating neutron star low mass X-ray binaries with known spin. For the AMXPs SAX J1808.4-3658 and XTE J1807-294, we find frequency-correlation power law indices that, surprisingly, are significantly lower than in the non-pulsars, and consistent with the relativistic precession model (RPM) prediction of 2.0 appropriate to test-particle orbital and Lense-Thirring precession frequencies. As previously reported, power law normalizations are significantly higher in these AMXPs than in the non-pulsating sources, leading to requirements on the neutron star specific moment of inertia in this model that cannot be satisfied with realistic equations of state. At least two other AMXPs show frequency correlations inconsistent with those of SAX J1808.4-3658 and XTE J1807-294, and possibly similar to those of the non-pulsating sources; for two AMXPs no conclusions could be drawn. We discuss these results in the context of a model that has had success in black hole (BH) systems involving a torus-like hot inner flow precessing due to (prograde) frame dragging, and a scenario in which additional (retrograde) magnetic and classical precession torques not present in BH systems are also considered. We show that a combination of these interpretations may accommodate our results.
Exploration of dynamical regimes of irradiated small protonated water clusters<|sep|>We explore from a theoretical perspective the dynamical response of small water clusters, (H$_2$O)$_n$H$_3$O$^+$ with $n=1,2,3$, to a short laser pulse for various frequencies, from infrared (IR) to ultra-violet (UV) and intensities (from $6\times10^{13}$ W/cm$^2$ to $5\times10^{14}$ W/cm$^2$). To that end, we use time-dependent local-density approximation for the electrons, coupled to molecular dynamics for the atomic cores (TDLDA-MD). The local-density approximation is augmented by a self-interaction correction (SIC) to allow for a correct description of electron emission. For IR frequencies, we see a direct coupling of the laser field to the very light H$^+$ ions in the clusters. Resonant coupling (in the UV) and/or higher intensities lead to fast ionization with subsequent Coulomb explosion. The stability against Coulomb pressure increases with system size. Excitation to lower ionization stages induced strong ionic vibrations. These maintain rather harmonic pattern in spite of the sizeable amplitudes (often 10% of the bond length).
Sensitive survey for 13CO, CN, H2CO, and SO in the disks of T Tauri and Herbig Ae stars II: Stars in $\rho$ Oph and upper Scorpius<|sep|>We attempt to determine the molecular composition of disks around young low-mass stars in the $\rho$ Oph region and to compare our results with a similar study performed in the Taurus-Auriga region. We used the IRAM 30 m telescope to perform a sensitive search for CN N=2-1 in 29 T Tauri stars located in the $\rho$ Oph and upper Scorpius regions. $^{13}$CO J=2-1 is observed simultaneously to provide an indication of the level of confusion with the surrounding molecular cloud. The bandpass also contains two transitions of ortho-H$_2$CO, one of SO, and the C$^{17}$O J=2-1 line, which provides complementary information on the nature of the emission. Contamination by molecular cloud in $^{13}$CO and even C$^{17}$O is ubiquitous. The CN detection rate appears to be lower than for the Taurus region, with only four sources being detected (three are attributable to disks). H$_2$CO emission is found more frequently, but appears in general to be due to the surrounding cloud. The weaker emission than in Taurus may suggest that the average disk size in the $\rho$ Oph region is smaller than in the Taurus cloud. Chemical modeling shows that the somewhat higher expected disk temperatures in $\rho$ Oph play a direct role in decreasing the CN abundance. Warmer dust temperatures contribute to convert CN into less volatile forms. In such a young region, CN is no longer a simple, sensitive tracer of disks, and observations with other tracers and at high enough resolution with ALMA are required to probe the gas disk population.
Quantum error correction with only two extra qubits<|sep|>Noise rates in quantum computing experiments have dropped dramatically, but reliable qubits remain precious. Fault-tolerance schemes with minimal qubit overhead are therefore essential. We introduce fault-tolerant error-correction procedures that use only two ancilla qubits. The procedures are based on adding "flags" to catch the faults that can lead to correlated errors on the data. They work for various distance-three codes. In particular, our scheme allows one to test the [[5,1,3]] code, the smallest error-correcting code, using only seven qubits total. Our techniques also apply to the [[7,1,3]] and [[15,7,3]] Hamming codes, thus allowing to protect seven encoded qubits on a device with only 17 physical qubits.
Trusted Multi-View Classification<|sep|>Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability and robustness by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.
A detailed view of the gas shell around R Sculptoris with ALMA<|sep|>Thermal pulses are fundamental to the chemical evolution of AGB stars and their circumstellar envelopes. A further consequence of thermal pulses is the formation of detached shells of gas and dust around the star. We aim to determine the physical properties of the detached gas shell around R Sculptoris, in particular the shell mass and temperature, and to constrain the evolution of the mass-loss rate during and after a thermal pulse. We analyse CO(1-0), CO(2-1), and CO(3-2) emission, observed by. The spatial resolution of the ALMA data allows us to separate the detached shell emission from the extended emission inside the shell. We perform radiative transfer modelling of both components to determine the shell properties and the post-pulse mass-loss properties. The ALMA data show a gas shell with a radius of 19.5" expanding at 14.3km/s. The different scales probed by the ALMA Cycle 0 array show that the shell must be entirely filled with gas, contrary to the idea of a detached shell. The comparison to single-dish spectra and radiative transfer modelling confirms this. We derive a shell mass of 4.5e-3 Msun with a temperature of 50K. Typical timescales for thermal pulses imply a pulse mass-loss rate of 2.3e-5 Msun/yr. For the post-pulse mass-loss rate, we find evidence for a gradual decline of the mass-loss rate, with an average value of 1.6e-5 Msun/yr. The total amount of mass lost since the last thermal pulse is 0.03 Msun, a factor four higher compared to classical models, with a sharp decline in mass-loss rate immediately after the pulse. We find that the mass-loss rate after a thermal pulse has to decline more slowly than generally expected from models of thermal pulses. This may cause the star to lose significantly more mass during a thermal pulse cycle, which affects the chemical evolution of the star and the interstellar medium.
The Hagedorn temperature Revisited<|sep|>The Hagedorn temperature, T_H is determined from the number of hadronic resonances including all mesons and baryons. This leads to a stable result T_H = 174 MeV consistent with the critical and the chemical freeze-out temperatures at zero chemical potential. We use this result to calculate the speed of sound and other thermodynamic quantities in the resonance hadron gas model for a wide range of baryon chemical potentials following the chemical freeze-out curve. We compare some of our results to those obtained previously in other papers.
Deciphering a novel image cipher based on mixed transformed Logistic maps<|sep|>Since John von Neumann suggested utilizing Logistic map as a random number generator in 1947, a great number of encryption schemes based on Logistic map and/or its variants have been proposed. This paper re-evaluates the security of an image cipher based on transformed logistic maps and proves that the image cipher can be deciphered efficiently under two different conditions: 1) two pairs of known plain-images and the corresponding cipher-images with computational complexity of $O(2^{18}+L)$; 2) two pairs of chosen plain-images and the corresponding cipher-images with computational complexity of $O(L)$, where $L$ is the number of pixels in the plain-image. In contrast, the required condition in the previous deciphering method is eighty-seven pairs of chosen plain-images and the corresponding cipher-images with computational complexity of $O(2^{7}+L)$. In addition, three other security flaws existing in most Logistic-map-based ciphers are also reported.
Comparative study of the discrete velocity and lattice Boltzmann methods for rarefied gas flows through irregular channels<|sep|>Rooted from the gas kinetics, the lattice Boltzmann method is a powerful tool in modeling hydrodynamics. In the past decade, it has been extended to simulate the rarefied gas flow beyond the Navier-Stokes level, either by using the high-order Gauss-Hermite quadrature, or by introducing the relaxation time that is a function of the gas-wall distance. While the former method, with a limited number of discrete velocities (i.e. D2Q36), is accurate up to the early transition flow regime, the latter method, with the same discrete velocities as that used in simulating hydrodynamics (i.e. D2Q9), is accurate up to the free-molecular flow regime in the Poiseuille flow between two parallel plates. This is quite astonishing in the sense that more discrete velocities are less accurate. In this paper, by solving the Bhatnagar-Gross-Krook kinetic equation accurately via the discrete velocity method, we find that the accuracy of the lattice Boltzmann method is reduced significantly in the simulation of rarefied gas flows through the rough surface and porous media. Our simulation results could serve as benchmarking cases for future development of the lattice Boltzmann method for modeling and simulation of rarefied gas flows in complex geometries.
Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models<|sep|>Eye tracking data during reading is a useful source of information to understand the cognitive processes that take place during language comprehension processes. Different languages account for different brain triggers , however there seems to be some uniform indicators. In this paper, we describe our submission to the CMCL 2022 shared task on predicting human reading patterns for multi-lingual dataset. Our model uses text representations from transformers and some hand engineered features with a regression layer on top to predict statistical measures of mean and standard deviation for 2 main eye-tracking features. We train an end to end model to extract meaningful information from different languages and test our model on two seperate datasets. We compare different transformer models and show ablation studies affecting model performance. Our final submission ranked 4th place for SubTask-1 and 1st place for SubTask-2 for the shared task.
Alternating Heisenberg Spin-1/2 Chains in a Transverse Magnetic Field<|sep|>The ground state phase diagram of the alternating spin-1/2 chains with anisotropic ferromagnetic coupling under the influence of a symmetry breaking transverse magnetic field is studied. We have used the exact diagonalization technique. In the limit where the antiferromagnetic coupling is dominant, we have identified two Ising-type quantum phase transitions. We have calculated two critical fields $h_{c_{1}}$ and $h_{c_{2}}$, corresponding to the transition between different magnetic phases of the system. It is found that the intermediate state ($h_{c_{1}}<h<h_{c_{2}}$) is gapful, describing the stripe-antiferromagnetic phase.
Transitivity of codimension one Anosov actions of R^k on closed manifolds<|sep|>In this paper, we define codimension one Anosov actions of $\RR^k, k\geq 2,$ on a closed connected orientable manifold $M$. We prove that if the ambient manifold has dimension greater than $k+2$, then the action is topologically transitive. This generalizes a result of Verjovsky for codimension one Anosov flows.
Outlier-Robust Filtering For Nonlinear Systems With Selective Observations Rejection<|sep|>Considering a common case where measurements are obtained from independent sensors, we present a novel outlier-robust filter for nonlinear dynamical systems in this work. The proposed method is devised by modifying the measurement model and subsequently using the theory of Variational Bayes and general Gaussian filtering. We treat the measurement outliers independently for independent observations leading to selective rejection of the corrupted data during inference. By carrying out simulations for variable number of sensors we verify that an implementation of the proposed filter is computationally more efficient as compared to the proposed modifications of similar baseline methods still yielding similar estimation quality. In addition, experimentation results for various real-time indoor localization scenarios using Ultra-wide Band (UWB) sensors demonstrate the practical utility of the proposed method.
Upper critical field of high quality single crystals of KFe$_2$As$_2$<|sep|>Measurements of temperature-dependent in-plane resistivity, $\rho(T)$, were used to determine the upper critical field and its anisotropy in high quality single crystals of stoichiometric iron arsenide superconductor KFe$_2$As$_2$. The crystals were characterized by residual resistivity ratio, $\rho(300K)/\rho(0)$ up to 3000 and resistive transition midpoint temperature, $T_c$=3.8 K, significantly higher than in previous studies on the same material. We find increased $H_{c2}(T)$ for both directions of the magnetic field, which scale with the increased $T_c$. This unusual linear $H_{c2}(T_c)$ scaling is not expected for orbital limiting mechanism of the upper critical field in clean materials.
Online Dynamic Motion Planning and Control for Wheeled Biped Robots<|sep|>Wheeled-legged robots combine the efficiency of wheeled robots when driving on suitably flat surfaces and versatility of legged robots when stepping over or around obstacles. This paper introduces a planning and control framework to realise dynamic locomotion for wheeled biped robots. We propose the Cart-Linear Inverted Pendulum Model (Cart-LIPM) as a template model for the rolling motion and the under-actuated LIPM for contact changes while walking. The generated motion is then tracked by an inverse dynamic whole-body controller which coordinates all joints, including the wheels. The framework has a hierarchical structure and is implemented in a model predictive control (MPC) fashion. To validate the proposed approach for hybrid motion generation, two scenarios involving different types of obstacles are designed in simulation. To the best of our knowledge, this is the first time that such online dynamic hybrid locomotion has been demonstrated on wheeled biped robots.
Unconventional quantum Hall effects in two-dimensional massive spin-1 fermion systems<|sep|>Unconventional fermions with high degeneracies in three dimensions beyond Weyl and Dirac fermions have sparked tremendous interest in condensed matter physics. Here, we study quantum Hall effects (QHEs) in a two-dimensional (2D) unconventional fermion system with a pair of gapped spin-1 fermions. We find that the original unlimited number of zero energy Landau levels (LLs) in the gapless case develop into a series of bands, leading to a novel QHE phenomenon that the Hall conductance first decreases (or increases) to zero and then revives as an infinite ladder of fine staircase when the Fermi surface is moved toward zero energy, and it suddenly reverses with its sign being flipped due to a Van Hove singularity when the Fermi surface is moved across zero. We further investigate the peculiar QHEs in a dice model with a pair of spin-1 fermions, which agree well with the results of the continuous model.
On the Null Space Constant for $l_p$ Minimization<|sep|>The literature on sparse recovery often adopts the $l_p$ "norm" $(p\in[0,1])$ as the penalty to induce sparsity of the signal satisfying an underdetermined linear system. The performance of the corresponding $l_p$ minimization problem can be characterized by its null space constant. In spite of the NP-hardness of computing the constant, its properties can still help in illustrating the performance of $l_p$ minimization. In this letter, we show the strict increase of the null space constant in the sparsity level $k$ and its continuity in the exponent $p$. We also indicate that the constant is strictly increasing in $p$ with probability $1$ when the sensing matrix ${\bf A}$ is randomly generated. Finally, we show how these properties can help in demonstrating the performance of $l_p$ minimization, mainly in the relationship between the the exponent $p$ and the sparsity level $k$.
Automatic morphological classification of galaxy images<|sep|>We describe an image analysis supervised learning algorithm that can automatically classify galaxy images. The algorithm is first trained using a manually classified images of elliptical, spiral, and edge-on galaxies. A large set of image features is extracted from each image, and the most informative features are selected using Fisher scores. Test images can then be classified using a simple Weighted Nearest Neighbor rule such that the Fisher scores are used as the feature weights. Experimental results show that galaxy images from Galaxy Zoo can be classified automatically to spiral, elliptical and edge-on galaxies with accuracy of ~90% compared to classifications carried out by the author. Full compilable source code of the algorithm is available for free download, and its general-purpose nature makes it suitable for other uses that involve automatic image analysis of celestial objects.
Decaying Dark Atom constituents and cosmic positron excess<|sep|>We present a scenario where dark matter is in the form of dark atoms that can accomodate the experimentally observed excess of positrons in PAMELA and AMS-02 while being compatible with the constraints imposed on the gamma-ray flux from Fermi/LAT. This scenario assumes that the dominant component of dark matter is in the form of a bound state between a helium nucleus and a $-2$ particle and a small component is in the form of a WIMP-like dark atom compatible with direct searches in underground detectors. One of the constituents of this WIMP-like state is a $+2$ metastable particle with a mass of 1 TeV or slightly below that by decaying to $e^+e^+$, $\mu^+ \mu^+$ and $\tau^+ \tau^+$ produces the observed positron excess. These decays can naturally take place via GUT interactions. If it exists, such a metastable particle can be found in the next run of LHC. The model predicts also the ratio of leptons over baryons in the Universe to be close to -3.
Correlation effects in superconducting quantum dot systems<|sep|>We study the effect of electron correlations on a system consisting of a single-level quantum dot with local Coulomb interaction attached to two superconducting leads. We use the single-impurity Anderson model with BCS superconducting baths to study the interplay between the proximity induced electron pairing and the local Coulomb interaction. We show how to solve the model using the continuous-time hybridization-expansion quantum Monte Carlo method. The results obtained for experimentally relevant parameters are compared with results of self-consistent second order perturbation theory as well as with the numerical renormalization group method.
Achieving Large Sum Rate and Good Fairness in MISO Broadcast Communication<|sep|>A tradeoff between sum rate and fairness for MISO broadcast communication employing dirty paper coding or zero-forcing dirty paper coding at physical layer is investigated in this paper. The tradeoff is based on a new design objective termed "tri-stage" approach as well as a new L1-based fairness measure that is much more robust than the well-known Jain's index for comparing fairness levels achieved by various design objectives at a much finer resolution in high SNR regime. The newly proposed tri-stage design also introduces a new concept of statistical power allocation that randomly allocates powers to users based on an optimal probability distribution derived from the tradeoff between sum rate and fairness. Simulation results show that the proposed approach can simultaneously achieve a larger sum rate and better fairness than the reputable proportional fairness criterion. A performance upper bound is also given in the paper to show that the excellent performance of the proposed approach at moderate and high SNR regimes as well as some potential for further improvement in low SNR regime.
Design and Analysis of E2RC Codes<|sep|>We consider the design and analysis of the efficiently-encodable rate-compatible ($E^2RC$) irregular LDPC codes proposed in previous work. In this work we introduce semi-structured $E^2RC$-like codes and protograph $E^2RC$ codes. EXIT chart based methods are developed for the design of semi-structured $E^2RC$-like codes that allow us to determine near-optimal degree distributions for the systematic part of the code while taking into account the structure of the deterministic parity part, thus resolving one of the open issues in the original construction. We develop a fast EXIT function computation method that does not rely on Monte-Carlo simulations and can be used in other scenarios as well. Our approach allows us to jointly optimize code performance across the range of rates under puncturing. We then consider protograph $E^2RC$ codes (that have a protograph representation) and propose rules for designing a family of rate-compatible punctured protographs with low thresholds. For both the semi-structured and protograph $E^2RC$ families we obtain codes whose gap to capacity is at most 0.3 dB across the range of rates when the maximum variable node degree is twenty.
Non-degenerate Bound State Solitons in Multi-component Bose-Einstein Condensates<|sep|>We investigate non-degenerate bound state solitons systematically in multi-component Bose-Einstein condensates, through developing Darboux transformation method to derive exact soliton solutions analytically. In particular, we show that bright solitons with nodes correspond to the excited bound eigen-states in the self-induced effective quantum wells, in sharp contrast to the bright soliton and dark soliton reported before (which usually correspond to ground state and free eigen-state respectively). We further demonstrate that the bound state solitons with nodes are induced by incoherent interactions between solitons in different components. Moreover, we reveal that the interactions between these bound state solitons are usually inelastic, caused by the incoherent interactions between solitons in different components and the coherent interactions between solitons in same component. The bound state solitons can be used to discuss many different physical problems, such as beating dynamics, spin-orbital coupling effects, quantum fluctuations, and even quantum entanglement states.
Weighted Matching in the Semi-Streaming Model<|sep|>We reduce the best known approximation ratio for finding a weighted matching of a graph using a one-pass semi-streaming algorithm from 5.828 to 5.585. The semi-streaming model forbids random access to the input and restricts the memory to O(n*polylog(n)) bits. It was introduced by Muthukrishnan in 2003 and is appropriate when dealing with massive graphs.
Just DIAL: DomaIn Alignment Layers for Unsupervised Domain Adaptation<|sep|>The empirical fact that classifiers, trained on given data collections, perform poorly when tested on data acquired in different settings is theoretically explained in domain adaptation through a shift among distributions of the source and target domains. Alleviating the domain shift problem, especially in the challenging setting where no labeled data are available for the target domain, is paramount for having visual recognition systems working in the wild. As the problem stems from a shift among distributions, intuitively one should try to align them. In the literature, this has resulted in a stream of works attempting to align the feature representations learned from the source and target domains. Here we take a different route. Rather than introducing regularization terms aiming to promote the alignment of the two representations, we act at the distribution level through the introduction of \emph{DomaIn Alignment Layers} (\DIAL), able to match the observed source and target data distributions to a reference one. Thorough experiments on three different public benchmarks we confirm the power of our approach.
A sharp-front moving boundary model for malignant invasion<|sep|>We analyse a novel mathematical model of malignant invasion which takes the form of a two-phase moving boundary problem describing the invasion of a population of malignant cells into a population of background tissue, such as skin. Cells in both populations undergo diffusive migration and logistic proliferation. The interface between the two populations moves according to a two-phase Stefan condition. Unlike many reaction-diffusion models of malignant invasion, the moving boundary model explicitly describes the motion of the sharp front between the cancer and surrounding tissues without needing to introduce degenerate nonlinear diffusion. Numerical simulations suggest the model gives rise to very interesting travelling wave solutions that move with speed $c$, and the model supports both malignant invasion and malignant retreat, where the travelling wave can move in either the positive or negative $x$-directions. Unlike the well-studied Fisher-Kolmogorov and Porous-Fisher models where travelling waves move with a minimum wave speed $c \ge c^* > 0$, the moving boundary model leads to travelling wave solutions with $|c| < c^{**}$. We interpret these travelling wave solutions in the phase plane and show that they are associated with several features of the classical Fisher-Kolmogorov phase plane that are often disregarded as being nonphysical. We show, numerically, that the phase plane analysis compares well with long time solutions from the full partial differential equation model as well as providing accurate perturbation approximations for the shape of the travelling waves.
Cosmological Constraints from the double source plane lens SDSSJ0946+1006<|sep|>We present constraints on the equation of state of dark energy, $w$, and the total matter density, $\Omega_{\mathrm{M}}$, derived from the double-source-plane strong lens SDSSJ0946+1006, the first cosmological measurement with a galaxy-scale double-source-plane lens. By modelling the primary lens with an elliptical power-law mass distribution, and including perturbative lensing by the first source, we are able to constrain the cosmological scaling factor in this system to be $\beta^{-1}=1.404 \pm 0.016$, which implies $\Omega_{\mathrm{M}}= 0.33_{-0.26}^{+0.33}$ for a flat $\Lambda$ cold dark matter ($\Lambda$CDM) cosmology. Combining with a cosmic microwave background prior from Planck, we find $w$ = $-1.17^{+0.20}_{-0.21}$ assuming a flat $w$CDM cosmology. This inference shifts the posterior by 1${\sigma}$ and improves the precision by 30 per cent with respect to Planck alone, and demonstrates the utility of combining simple, galaxy-scale multiple-source-plane lenses with other cosmological probes to improve precision and test for residual systematic biases.
The Hyades Cluster: Identification of a Planetary System and Escaping White Dwarfs<|sep|>Recently, some hot DA-type white dwarfs have been proposed to plausibly be escaping members of the Hyades. We used hydrogen Balmer lines to measure the radial velocities of seven such stars and confirm that three, and perhaps two others, are/were indeed cluster members and one is not. The other candidate Hyad is strongly magnetic and its membership status remains uncertain. The photospheres of at least one quarter of field white dwarf stars are "polluted" by elements heavier than helium that have been accreted. These stars are orbited by extended planetary systems that contain both debris belts and major planets. We surveyed the seven classical single Hyades white dwarfs and the newly identified (escaping) Hyades white dwarfs and found calcium in the photosphere of LP 475-242 of type DBA (now DBAZ), thus implying the presence of an orbiting planetary system. The spectrum of white dwarf GD 31, which may be, but probably is not, an escaping member of the Hyades, displays calcium absorption lines; these originate either from the interstellar medium or, less likely, from a gaseous circumstellar disk. If GD 31 was once a Hyades member, then it would be the first identified white dwarf Hyad with a cooling age >340 Myr.
Flux tubes at finite temperature<|sep|>The chromoelectric field generated by a static quark-antiquark pair, with its peculiar tube-like shape, can be nicely described, at zero temperature, within the dual superconductor scenario for the QCD confining vacuum. In this work we investigate, by lattice Monte Carlo simulations of the SU(3) pure gauge theory, the fate of chromoelectric flux tubes across the deconfinement transition. We find that, as the temperature is increased towards and above the deconfinement temperature $T_c$, the amplitude of the field inside the flux tube gets smaller, while the shape of the flux tube does not vary appreciably across deconfinement. This scenario with flux-tube "evaporation" above $T_c$ has no correspondence in ordinary (type-II) superconductivity, where instead the transition to the phase with normal conductivity is characterized by a divergent fattening of flux tubes as the transition temperature is approached from below. We present also some evidence about the existence of flux-tube structures in the magnetic sector of the theory in the deconfined phase.
Water in Comet 2/2003 K4 (LINEAR) with Spitzer<|sep|>We present sensitive 5.5 to 7.6 micron spectra of comet C/2003 K4 (LINEAR) obtained on 16 July 2004 (r_{h} = 1.760 AU, Delta_{Spitzer} = 1.409 AU, phase angle 35.4 degrees) with the Spitzer Space Telescope. The nu_{2} vibrational band of water is detected with a high signal-to-noise ratio (> 50). Model fitting to the best spectrum yields a water ortho-to-para ratio of 2.47 +/- 0.27, which corresponds to a spin temperature of 28.5^{+6.5}_{-3.5} K. Spectra acquired at different offset positions show that the rotational temperature decreases with increasing distance from the nucleus, which is consistent with evolution from thermal to fluorescence equilibrium. The inferred water production rate is (2.43 +/- 0.25) \times 10^{29} molec. s^{-1}. The spectra do not show any evidence for emission from PAHs and carbonate minerals, in contrast to results reported for comets 9P/Tempel 1 and C/1995 O1 (Hale-Bopp). However, residual emission is observed near 7.3 micron the origin of which remains unidentified.
Deconfinement transition in two-flavour lattice QCD with dynamical overlap fermions<|sep|>We study the deconfinement transition in two-flavour lattice QCD with dynamical overlap fermions. Our simulations have been carried out on a $16^3 \times 6$ lattice at a pion mass around 500 MeV with a special HMC algorithm without any approximation such as fixed topology. We consider several temperatures from 220 MeV which is close to the deconfinement to 280 MeV which is above it. The dependence of the Polyakov loop, the chiral condensate, the Dirac spectra and the connected part of chiral susceptibility on the inverse gauge coupling has been studied. Our data indicates that the transition point lies between $\beta = 7.6$ and $\beta = 8.1$, but a more precise determination is not possible with our present statistics.
Diffusion of active chiral particles<|sep|>The diffusion of chiral active Brownian particles in three-dimensional space is studied analytically, by consideration of the corresponding Fokker-Planck equation for the probability density of finding a particle at position $\boldsymbol{x}$ and moving along the direction $\hat{\boldsymbol{v}}$ at time $t$, and numerically, by the use of Langevin dynamics simulations. The analysis is focused on the marginal probability density of finding a particle at a given location and at a given time (independently of its direction of motion), which is found from an infinite hierarchy of differential-recurrence relations for the coefficients that appear in the multipole expansion of the probability distribution which contains the whole kinematic information. This approach allows the explicit calculation of the time dependence of the mean squared displacement and the time dependence of the kurtosis of the marginal probability distribution, quantities from which the effective diffusion coefficient and the "shape" of the positions distribution are examined. Oscillations between two characteristic values were found in the time evolution of the kurtosis, namely, between the value that corresponds to a Gaussian and the one that corresponds to a distribution of spherical shell shape. In the case of an ensemble of particles, each one rotating around an uniformly-distributed random axis, it is found evidence of the so called effect "anomalous, yet Brownian, diffusion", for which particles follow a non-Gaussian distribution for the positions yet the mean squared displacement is a linear function of time.
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?<|sep|>There have been a lot of interest in the scaling properties of Transformer models. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour? How does this influence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model architectures such as Transformers, Switch Transformers, Universal Transformers, Dynamic convolutions, Performers, and recently proposed MLP-Mixers. Via extensive experiments, we show that (1) architecture is an indeed an important consideration when performing scaling and (2) the best performing model can fluctuate at different scales. We believe that the findings outlined in this work has significant implications to how model architectures are currently evaluated in the community.
Federated Learning on Adaptively Weighted Nodes by Bilevel Optimization<|sep|>We propose a federated learning method with weighted nodes in which the weights can be modified to optimize the model's performance on a separate validation set. The problem is formulated as a bilevel optimization where the inner problem is a federated learning problem with weighted nodes and the outer problem focuses on optimizing the weights based on the validation performance of the model returned from the inner problem. A communication-efficient federated optimization algorithm is designed to solve this bilevel optimization problem. Under an error-bound assumption, we analyze the generalization performance of the output model and identify scenarios when our method is in theory superior to training a model only locally and to federated learning with static and evenly distributed weights.
Unified Picture of Electromagnetic and Gravitational Forces in Two-Spinor Language<|sep|>The well known Geodesic Equation of General Relativity is newly formulated in Weyl two-spinor language in a convenient way susceptible of being combined with a set of two-spinor equations, equivalent to the Lorentz Force of Electrodynamics, obtained in previous studies. General spinor equations of motion for a test charged spin 1/2 particle in arbitrary electromagnetic and gravitational fields fields are obtained describing not only spacetime trajectories but also spin precession. For the case of the Schwarzschild metric and radial trajectories, spinor equations unifying gravitational and electric forces are obtained and solved for a weak gravitational field (or long distance from the source).
Topological Insulators and Superconductors from String Theory<|sep|>Topological insulators and superconductors in different spatial dimensions and with different discrete symmetries have been fully classified recently, revealing a periodic structure for the pattern of possible types of topological insulators and supercondutors, both in terms of spatial dimensions and in terms of symmetry classes. It was proposed that K-theory is behind the periodicity. On the other hand, D-branes, a solitonic object in string theory, are also known to be classified by K-theory. In this paper, by inspecting low-energy effective field theories realized by two parallel D-branes, we establish a one-to-one correspondence between the K-theory classification of topological insulators/superconductors and D-brane charges. In addition, the string theory realization of topological insulators and superconductors comes naturally with gauge interactions, and the Wess-Zumino term of the D-branes gives rise to a gauge field theory of topological nature, such as ones with the Chern-Simons term or the $\theta$-term in various dimensions. This sheds light on topological insulators and superconductors beyond non-interacting systems, and the underlying topological field theory description thereof. In particular, our string theory realization includes the honeycomb lattice Kitaev model in two spatial dimensions, and its higher-dimensional extensions. Increasing the number of D-branes naturally leads to a realization of topological insulators and superconductors in terms of holography (AdS/CFT).
Dissipation and spontaneous emission in quantum electrodynamical density functional theory based on optimized effective potential: A proof of concept study<|sep|>We generalize the optimized effective potential (OEP) formalism in the quantum electrodynamical density functional theory (QEDFT) to the case of continuous distribution of photon modes, and study its applicability to dissipative dynamics of electron systems interacting with photons of lossy cavities. Specifically, we test whether this technique is capable of capturing the quantum features of electron-photon interaction related to spontaneous emission and the corresponding energy transfer from the electrons to cavity photons. For this purpose, we analyze a discrete three-site system with one electron coupled to photons of the cavity, which, in fact, is a minimal model allowing to eliminate classical radiation and the corresponding energy loss, but still have nontrivial density dynamics. By considering two typical spectral densities of photon modes, modeling (i) lossy cavity with Lorentzian broadening of photon peaks, and (ii) the Ohmic bath, and several representative dynamical regimes, we find that OEP-QEDFT demonstrates a good qualitative and quantitative performance, especially in the case when the disspation is dominated by one-photon processes.
Non-Abelian two-form gauge transformation and gauge theories in six dimensions<|sep|>A new non-Abelian gauge transformation for two-forms is introduced. Construction is based on a fixed map from the spacetime to the loop space which attachs a closed loop to each point of the spacetime. It is argued that this set-up is consistent with the surface ordering ambiguity which is the main problem to construct the Wilson surface operator for non-Abelian groups. With the aim of the Wilson surface operator, we achieve a non-Abelian gauge transformation for two-forms. We interpret the Dirac operator as a vector field and define a covariant derivative and rederive the gauge transformation of the two-form. At the end, we construct an Abelian interacting gauge theory in six dimensions.
Zero-bias anomaly in nano-scale hole-doped Mott insulators on a triangular silicon surface<|sep|>Adsorption of 1/3 monolayer of Sn on a heavily-doped p-type Si(111) substrate results in the formation of a hole-doped Mott insulator, with electronic properties that are remarkably similar to those of the high-Tc copper oxide compounds. In this work, we show that the maximum hole-density of this system increases with decreasing domain size as the area of the Mott insulating domains approaches the nanoscale regime. Concomitantly, scanning tunneling spectroscopy data at 4.4 K reveal an increasingly prominent zero bias anomaly (ZBA). We consider two different scenarios as potential mechanisms for this ZBA: chiral d_(x^2 -y^2 )+ id_xy wave superconductivity and a dynamical Coulomb blockade (DCB) effect. The latter arises due to the formation of a resistive depletion layer between the nano-domains and the substrate. Both models fit the tunneling spectra with weaker ZBAs, while the DCB model clearly fits better to spectra recorded at higher temperatures or from the smallest domains with the strongest ZBA. Consistently, STS spectra from the lightly-doped substrates display oscillatory behavior that can be attributed to conventional Coulomb staircase behavior, which becomes stronger for smaller sized domains. We conclude that the ZBA is predominantly due to a DCB effect, while a superconducting instability is absent or a minor contributing factor.
Analyzing transverse momentum spectra by a new method in high-energy collisions<|sep|>We analyzed the transverse momentum spectra of positively and negatively charged pions ($\pi^+$ and $\pi^-$), positively and negatively charged kaons ($K^+$ and $K^-$), protons and antiprotons ($p$ and $\bar p$), as well as $\phi$ produced in mid-(pseudo)rapidity region in central nucleus--nucleus (AA) collisions over a center-of-mass energy range from 2.16 to 2760 GeV per nucleon pair. The transverse momentum of the considered particle is regarded as the joint contribution of two participant partons which obey the modified Tsallis-like transverse momentum distribution and have random azimuths in superposition. The calculation of transverse momentum distribution of particles is performed by the Monte Carlo method and compared with the experimental data measured by international collaborations. The excitation functions of effective temperature and other parameters are obtained in the considered energy range. With the increase of collision energy, the effective temperature parameter increases quickly and then slowly. The boundary appears at around 5 GeV, which means the change of reaction mechanism and/or generated matter.
Adjoint approach to calculating shape gradients for 3D magnetic confinement equilibria<|sep|>The shape gradient quantifies the change in some figure of merit resulting from differential perturbations to a shape. Shape gradients can be applied to gradient-based optimization, sensitivity analysis, and tolerance calculation. An efficient method for computing the shape gradient for toroidal 3D MHD equilibria is presented. The method is based on the self-adjoint property of the equations for driven perturbations of MHD equilibria and is similar to the Onsager symmetry of transport coefficients. Two versions of the shape gradient are considered. One describes the change in a figure of merit due to an arbitrary displacement of the outer flux surface; the other describes the change in the figure of merit due to the displacement of a coil. The method is implemented for several example figures of merit and compared with direct calculation of the shape gradient. In these examples the adjoint method reduces the number of equilibrium computations by factors of $\mathcal{O}(N)$, where $N$ is the number of parameters used to describe the outer flux surface or coil shapes.
Invariant Jet differentials and Asymptotic Serre duality<|sep|>We generalize the main result of Demailly \cite{D2} for the bundles $E_{k,m}^{GG}(V^*)$ of jet differentials of order $k$ and weighted degree $m$ to the bundles $E_{k,m}(V^*)$ of the invariant jet differentials of order $k$ and weighted degree $m$. Namely, Theorem 0.5 from \cite{D2} and Theorem 9.3 from \cite{D1} provide a lower bound $\frac{c^k}{k}m^{n+kr-1}$ on the number of the linearly independent holomorphic global sections of $E_{k,m}^{GG} V^* \bigotimes \mathcal{O}(-m \delta A)$ for some ample divisor $A$. The group $G_k$ of local reparametrizations of $(\mathbb{C},0)$ acts on the $k$-jets by orbits of dimension $k$, so that there is an automatic lower bound $\frac{c^k}{k} m^{n+kr-1}$ on the number of the linearly independent holomorphic global sections of $E_{k,m}V^* \bigotimes \mathcal{O}(-m \delta A)$. We formulate and prove the existence of an asymptotic duality along the fibers of the Green-Griffiths jet bundles over projective manifolds. We also prove a Serre duality for asymptotic sections of jet bundles. An application is also given for partial application to the Green-Griffiths conjecture.
Random lasing in an Anderson localizing optical fiber<|sep|>A directional random laser mediated by transverse Anderson localization in a disordered glass optical fiber is reported. Previous demonstrations of random lasers have found limited applications because of their multi-directionality and chaotic fluctuations in the laser emission. The random laser presented in this paper operates in the Anderson localization regime. The disorder induced localized states form isolated local channels that make the output laser beam highly directional and stabilize its spectrum. The strong transverse disorder and longitudinal invariance result in isolated lasing modes with negligible interaction with their surroundings, traveling back and forth in a Fabry-Perot cavity formed by the air-fiber interfaces. It is shown that if a localized input pump is scanned across the disordered fiber input facet, the output laser signal follows the transverse position of the pump. Moreover, a uniformly distributed pump across the input facet of the disordered fiber generates a laser signal with very low spatial coherence that can be of practical importance in many optical platforms including image transport with fiber bundles.
Hadronic Light by Light Contributions to the Muon Anomalous Magnetic Moment With Physical Pions<|sep|>The current measurement of muonic $g - 2$ disagrees with the theoretical calculation by about 3 standard deviations. Hadronic vacuum polarization (HVP) and hadronic light by light (HLbL) are the two types of processes that contribute most to the theoretical uncertainty. The current value for HLbL is still given by models. I will describe results from a first-principles lattice calculation with a 139 MeV pion in a box of 5.5 fm extent. Our current numerical strategies, including noise reduction techniques, evaluating the HLbL amplitude at zero external momentum transfer, and important remaining challenges, in particular those associated with finite volume effects, will be discussed.
How Scale Affects Structure in Java Programs<|sep|>Many internal software metrics and external quality attributes of Java programs correlate strongly with program size. This knowledge has been used pervasively in quantitative studies of software through practices such as normalization on size metrics. This paper reports size-related super- and sublinear effects that have not been known before. Findings obtained on a very large collection of Java programs -- 30,911 projects hosted at Google Code as of Summer 2011 -- unveils how certain characteristics of programs vary disproportionately with program size, sometimes even non-monotonically. Many of the specific parameters of nonlinear relations are reported. This result gives further insights for the differences of "programming in the small" vs. "programming in the large." The reported findings carry important consequences for OO software metrics, and software research in general: metrics that have been known to correlate with size can now be properly normalized so that all the information that is left in them is size-independent.
Dynamical symmetrization of the state of identical particles<|sep|>We propose a dynamical model for state symmetrization of two identical particles produced in spacelike-separated events by independent sources. We adopt the hypothesis that the pair of non-interacting particles can initially be described by a tensor product state since they are in principle distinguishable due to their spacelike separation. As the particles approach each other, a quantum jump takes place upon particle collision, which erases their distinguishability and projects the two-particle state onto an appropriately (anti)symmetrized state. The probability density of the collision times can be estimated quasi-classically using the Wigner functions of the particles' wavepackets, or derived from fully quantum mechanical considerations using an appropriately adapted time-of-arrival operator. Moreover, the state symmetrization can be formally regarded as a consequence of the spontaneous measurement of the collision time. We show that symmetric measurements performed on identical particles can in principle discriminate between the product and symmetrized states. Our model and its conclusions can be tested experimentally.
Modeling IoT-aware Business Processes - A State of the Art Report<|sep|>This research report presents an analysis of the state of the art of modeling Internet of Things (IoT)-aware business processes. IOT links the physical world to the digital world. Traditionally, we would find information about events and processes in the physical world in the digital world entered by humans and humans using this information to control the physical world. In the IoT paradigm, the physical world is equipped with sensors and actuators to create a direct link with the digital world. Business processes are used to coordinate a complex environment including multiple actors for a common goal, typically in the context of administrative work. In the past few years, we have seen research efforts on the possibilities to model IoT- aware business processes, extending process coordination to real world entities directly. This set of research efforts is relatively small when compared to the overall research effort into the IoT and much of the work is still in the early research stage. To create a basis for a bridge between IoT and BPM, the goal of this report is to collect and analyze the state of the art of existing frameworks for modeling IoT-aware business processes.
Red and Blueshifts in Multi-stranded Coronal Loops: A New Temperature Diagnostic<|sep|>Based on observations from the EUV Imaging Spectrometer (EIS) on board Hinode, the existence of a broad distribution of blue and red Dopplershift in active region loops has been revealed; the distribution of Dopplershifts depends on the peak temperature of formation of the observed spectral lines. To reproduce those observations, we use a nanoflare heating model for multi-stranded coronal loops (Sarkar and Walsh 2008, 2009) and a set of spectral lines covering a broad range of temperature (from 0.25 MK to 5.6 MK). We first show that red- and blueshifts are ubiquitous in all wavelength ranges; redshifts/downflows dominating cool spectral lines (from O V to Si VII) and blueshifts/upflows dominating the hot lines (from Fe XV to Ca XVII). These Dopplershifts are indicative of plasma condensation and evaporation. By computing the average Dopplershift, we derive a new temperature diagnostic for coronal loops: the temperature at which the average Dopplershift vanishes estimates the mean temperature of the plasma along the coronal loop and at the footpoints. To compare closely with observations, we model dense and sparse Hinode/EIS rasters at the instrument resolution. The temperature diagnostic provides the same temperature estimates as the model whatever the type of raster or the viewing angle. To conclude, we have developed a robust temperature diagnostic to measure the plasma temperature of a coronal loop using a broad range of spectral lines.
On profitability of selfish mining<|sep|>We review the so called selfish mining strategy in the Bitcoin network and compare its profitability to honest mining.We build a rigorous profitability model for repetition games. The time analysis of the attack has been ignored in the previous literature based on a Markov model,but is critical. Using martingale's techniques and Doob Stopping Time Theorem we compute the expected duration of attack cycles. We discover a remarkable property of the bitcoin network: no strategy is more profitable than the honest strategy before a difficulty adjustment. So selfish mining can only become profitable afterwards, thus it is an attack on the difficulty adjustment algorithm. We propose an improvement of Bitcoin protocol making it immune to selfish mining attacks. We also study miner's attraction to selfish mining pools. We calculate the expected duration time before profit for the selfish miner, a computation that is out of reach by the previous Markov models.
Spectral classification of the mass donors in the high-mass X-ray binaries EXO 1722-363 and OAO 1657-415<|sep|>We report near-infrared observations of the mass donors of the eclipsing high-mass X-ray binary (HMXB) systems EXO 1722-363 and OAO 1657-415 in order to derive their accurate spectral classifications. We determined that EXO 1722-363 was of spectral type B0 - B1 Ia, positioned at a distance 8.0 +2.5/-2.0 kpc with a progenitor mass in the range 30 - 40 M_Sun. Luminosity calculations imply that L_X ~ 10^35 - 10^37 erg s^-1 for this distance range. We conclude that EXO 1722-363 shares many of the properties associated with other X-ray binary B-type supergiant donors. We found that OAO 1657-415 correlates closely with the spectra of a class of transitional objects, the Ofpe/WNL, an intermediate evolutionary stage between massive O type stars leaving the main sequence and evolving into Wolf-Rayets. Due to the wide range range in Luminosity displayed by Ofpe/WNL stars, (log L/L_Sun ~ 5.3 - 6.2) distance determinations are problematic. For OAO 1657-415 we report a distance of 4.4 <= d <= 12 kpc, implying an X-ray luminosity of 1.5 x 10^36 <= L_X <= 10^37 erg s^-1. We have used our new classification of OAO 1657-415 to explain the physical processes responsible for its unique position within the Corbet diagram. Ofpe/WNL stars demonstrate a high rate of mass-loss through a dense stellar wind combined with a low terminal velocity. This combination of wind properties leads to a high accretion rate and transfer of angular momentum to the neutron star in this system. We believe this in turn leads to a smaller instantaneous equilibrium spin period with respect to normal OB supergiants.
Decay Replay Mining to Predict Next Process Events<|sep|>In complex processes, various events can happen in different sequences. The prediction of the next event given an a-priori process state is of importance in such processes. Recent methods have proposed deep learning techniques such as recurrent neural networks, developed on raw event logs, to predict the next event from a process state. However, such deep learning models by themselves lack a clear representation of the process states. At the same time, recent methods have neglected the time feature of event instances. In this paper, we take advantage of Petri nets as a powerful tool in modeling complex process behaviors considering time as an elemental variable. We propose an approach which starts from a Petri net process model constructed by a process mining algorithm. We enhance the Petri net model with time decay functions to create continuous process state samples. Finally, we use these samples in combination with discrete token movement counters and Petri net markings to train a deep learning model that predicts the next event. We demonstrate significant performance improvements and outperform the state-of-the-art methods on nine real-world benchmark event logs.
Investigating the Origins of Spiral Structure in Disk Galaxies through a Multiwavelength Study<|sep|>The density-wave theory of spiral structure proposes that star formation occurs in or near a spiral-shaped region of higher density that rotates rigidly within the galactic disk at a fixed pattern speed. In most interpretations of this theory, newborn stars move downstream of this position as they come into view, forming a downstream spiral which is tighter, with a smaller pitch angle than that of the density wave itself. Rival theories, including theories which see spiral arms as essentially transient structures, may demand that pitch angle should not depend on wavelength. We measure the pitch angle of a large sample of galaxies at several wavelengths associated with star formation or very young stars (8.0 {\mu}m, H-{\alpha} line and 151 nm in the far-UV) and show that they all have the same pitch angle, which is larger than the pitch angle measured for the same galaxies at optical and near-infrared wavelengths. Our measurements in the B band and at 3.6 {\mu}m have unambiguously tighter spirals than the starforming wavelengths. In addition, we have measured in the u-band, which seems to fall midway between these two extremes. Thus, our results are consistent with a region of enhanced stellar light situated downstream of a starforming region.
A Simple Yao-Yao-Based Spanner of Bounded Degree<|sep|>It is a standing open question to decide whether the Yao-Yao structure for unit disk graphs (UDGs) is a length spanner of not. This question is highly relevant to the topology control problem for wireless ad hoc networks. In this paper we make progress towards resolving this question by showing that the Yao-Yao structure is a length spanner for UDGs of bounded aspect ratio. We also propose a new local algorithm, called Yao-Sparse-Sink, based on the Yao-Sink method introduced by Li, Wan, Wang and Frieder, that computes a (1+e)-spanner of bounded degree for a given UDG and for given e > 0. The Yao-Sparse-Sink method enables an efficient local computation of sparse sink trees. Finally, we show that all these structures for UDGs -- Yao, Yao-Yao, Yao-Sink and Yao-Sparse-Sink -- have arbitrarily large weight.
A Unified, Hardware-Fitted, Cross-GPU Performance Model<|sep|>We present a mechanism to symbolically gather performance-relevant operation counts from numerically-oriented subprograms (`kernels') expressed in the Loopy programming system, and apply these counts in a simple, linear model of kernel run time. We use a series of `performance-instructive' kernels to fit the parameters of a unified model to the performance characteristics of GPU hardware from multiple hardware generations and vendors. We evaluate the predictive power of the model on a broad array of computational kernels relevant to scientific computing. In terms of the geometric mean, our simple, vendor- and GPU-type-independent model achieves relative accuracy comparable to that of previously published work using hardware specific models.
Electronic structure of GdN, and the influence of exact exchange<|sep|>GdN bulk is studied with the local density approximation, on the Hartree-Fock level, and on the level of the hybrid functional B3LYP. A local basis set formalism is used, as implemented in the present CRYSTAL06 release. It is demonstrated that the code is technically capable of treating this system with its 4f electrons explicitly, i.e. out of the core. The band structure at the level of the local density approximation is in good agreement with earlier calculations and is found to be half-metallic. The Hartree-Fock band structure is insulating with a large gap. Interestingly, three solutions were found at the B3LYP level. The lowest of them is insulating for majority spin, and the Fermi surface for minority spin consists only of points, resulting in a very low density of states around the Fermi level.
Alkali vapor pressure modulation on the 100ms scale in a single-cell vacuum system for cold atom experiments<|sep|>We describe and characterize a device for alkali vapor pressure modulation on the 100ms timescale in a single-cell cold atom experiment. Its mechanism is based on optimized heat conduction between a current-modulated alkali dispenser and a heat sink at room temperature. We have studied both the short-term behavior during individual pulses and the long-term pressure evolution in the cell. The device combines fast trap loading and relatively long trap lifetime, enabling high repetition rates in a very simple setup. These features make it particularly suitable for portable atomic sensors.
The Design and Fabrication of Platform Device for Dna Amplification<|sep|>Thermalcycler were extensively used machine for amplify DNA sample. One of the major problems in the working time was that it spent most of time for cooling and heating. In order to improve the efficient, this study presented a novel method for amplify DNA sample. For this concept, the DNA sample in the silicon chamber which was pushed by a beam through three temperature regions around a center and then the DNA segments could be amplified rapidly after 30 cycles. The polymerase chain reaction platform was composed of thin-film heaters, copper plates, DC powers, and temperature controllers. The photolithography and bulk etching technologies were utilized to construct the thin-film heater and DNA reaction chambers. Finally, 1 pound gL 100bp DNA segment of E. coli K12 was amplified successfully within 36 minutes on this PCR platform.
On Neural Network Equivalence Checking using SMT Solvers<|sep|>Two pretrained neural networks are deemed equivalent if they yield similar outputs for the same inputs. Equivalence checking of neural networks is of great importance, due to its utility in replacing learning-enabled components with equivalent ones, when there is need to fulfill additional requirements or to address security threats, as is the case for example when using knowledge distillation, adversarial training etc. SMT solvers can potentially provide solutions to the problem of neural network equivalence checking that will be sound and complete, but as it is expected any such solution is associated with significant limitations with respect to the size of neural networks to be checked. This work presents a first SMT-based encoding of the equivalence checking problem, explores its utility and limitations and proposes avenues for future research and improvements towards more scalable and practically applicable solutions. We present experimental results that shed light to the aforementioned issues, for diverse types of neural network models (classifiers and regression networks) and equivalence criteria, towards a general and application-independent equivalence checking approach.
Chromoelectric Dipole Moments of Quarks in MSSM Extensions<|sep|>An analysis is given of the chromoelectric dipole moment of quarks and of the neutron in an MSSM extension where the matter sector contains an extra vectorlike generation of quarks and mirror quarks. The analysis includes contributions to the CEDM from the exchange of the $W$ and the $Z$ bosons, from the exchange of charginos and neutralinos and the gluino. Their contribution to the EDM of quarks is investigated. The interference between the MSSM sector and the new sector with vectorlike quarks is investigated. It is shown that inclusion of the vectorlike quarks can modify the quark EDMs in a significant way. Further, this interference also provides a probe of the vectorlike quark sector. These results are of interest as in the future measurements on the neutron EDM could see an improvement up to two orders of magnitude over the current experimental limits and provide a window to new physics beyond the standard model.
Measurement of light charged particles in the decay channels of medium-mass excited compound nuclei<|sep|>The 48Ti on 40Ca reactions have been studied at 300 and 600 MeV focusing on the fusion-evaporation (FE) and fusion-fission (FF) exit channels. Energy spectra and multiplicities of the emitted light charged particles have been compared to Monte Carlo simulations based on the statistical model. Indeed, in this mass region (A about 100) models predict that shape transitions can occur at high spin values and relatively scarce data exist in the literature about coincidence measurements between evaporation residues and light charged particles. Signals of shape transitions can be found in the variations of the lineshape of high energy gamma rays emitted from the de-excitation of GDR states gated on different region of angular momenta. For this purpose it is important to keep under control the FE and FF processes, to regulate the statistical model parameters and to control the onset of possible preequilibrium emissions from 300 to 600 MeV bombarding energy.
The MAGIC project. III. Radial and azimuthal Galactic abundance gradients using classical Cepheids<|sep|>Radial abundance gradients provide sound constraints for chemo-dynamical models of galaxies. Azimuthal variations of abundance ratios are solid diagnostics to understand their chemical enrichment. In this paper we investigate azimuthal variations of abundances in the Milky Way using Cepheids. We provide the detailed chemical composition (25 elements) of 105 Classical Cepheids from high-resolution SALT spectra observed by the MAGIC project. Negative abundance gradients, with abundances decreasing from the inner to the outer disc, have been reported both in the Milky Way and in external galaxies, and our results are in full agreement with literature results. We find azimuthal variations of the oxygen abundance [O/H]. While a large number of external spirals show negligible azimuthal variations, the Milky Way seems to be one of the few galaxies with noticeable [O/H] azimuthal asymmetries. They reach ~0.2 dex in the inner Galaxy and in the outer disc, where they are the largest, thus supporting similar findings for nearby spiral galaxies as well as recent 2D chemo-dynamical models.
Spectrum of the Anomalous Microwave Emission in the North Celestial Pole with WMAP 7-Year data<|sep|>We estimate the frequency spectrum of the diffuse anomalous microwave emission (AME) on the North Celestial Pole (NCP) region of the sky with the Correlated Component Analysis (CCA) component separation method applied to WMAP 7-yr data. The NCP is a suitable region for this analysis because the AME is weakly contaminated by synchrotron and free-free emission. By modeling the AME component as a peaked spectrum we estimate the peak frequency to be $21.7\pm0.8$\,GHz, in agreement with previous analyses which favored $\nu_{\rm p}<23$\,GHz. The ability of our method to correctly recover the position of the peak is verified through simulations. We compare the estimated AME spectrum with theoretical spinning dust models to constrain the hydrogen density $n_{\rm H}$. The best results are obtained with densities around 0.2--0.3\,cm$^{-3}$, typical of warm ionised medium (WIM) to warm neutral medium (WNM) conditions. The degeneracy with the gas temperature prevents an accurate determination of $n_{\rm H}$, especially for low hydrogen ionization fractions, where densities of a few cm$^{-3}$ are also allowed.
DeepType: Multilingual Entity Linking by Neural Type System Evolution<|sep|>The wealth of structured (e.g. Wikidata) and unstructured data about the world available today presents an incredible opportunity for tomorrow's Artificial Intelligence. So far, integration of these two different modalities is a difficult process, involving many decisions concerning how best to represent the information so that it will be captured or useful, and hand-labeling large amounts of data. DeepType overcomes this challenge by explicitly integrating symbolic information into the reasoning process of a neural network with a type system. First we construct a type system, and second, we use it to constrain the outputs of a neural network to respect the symbolic structure. We achieve this by reformulating the design problem into a mixed integer problem: create a type system and subsequently train a neural network with it. In this reformulation discrete variables select which parent-child relations from an ontology are types within the type system, while continuous variables control a classifier fit to the type system. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1) heuristic search or stochastic optimization over discrete variables that define a type system informed by an Oracle and a Learnability heuristic, 2) gradient descent to fit classifier parameters. We apply DeepType to the problem of Entity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) and find that it outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system or recent deep learning-based entity embeddings, while explicitly using symbolic information lets it integrate new entities without retraining.
Chemical tracers of high-metallicity environments<|sep|>We present for the first time a detailed study of the properties of molecular gas in metal-rich environments such as early-type galaxies (ETGs). We have explored Photon-Dominated Region (PDR) chemistry for a wide range of physical conditions likely to be appropriate for these sources. We derive fractional abundances of the 20 most chemically reactive species as a function of the metallicity, as a function of the optical depth and for various volume number gas densities, Far-Ultra Violet (FUV) radiation fields and cosmic ray ionisation rates. We also investigate the response of the chemistry to the changes in $\alpha-$element enhancement as seen in ETGs. We find that the fractional abundances of CS, H$_{2}$S, H$_{2}$CS, H$_{2}$O, H$_{3}$O$^{+}$, HCO$^{+}$ and H$_{2}$CN seem invariant to an increase of metallicity whereas C$^{+}$, CO, C$_{2}$H, CN, HCN, HNC and OCS appear to be the species most sensitive to this change. The most sensitive species to the change in the fractional abundance of $\alpha-$elements are C$^{+}$, C, CN, HCN, HNC, SO, SO$_{2}$, H$_{2}$O and CS. Finally, we provide line brightness ratios for the most abundant species, especially in the range observable with ALMA. Discussion of favorable line ratios to use for the estimation of super-solar metallicities and $\alpha$-elements are also provided.
Opinions within Media, Power and Gossip<|sep|>Despite the increasing diffusion of the Internet technology, TV remains the principal medium of communication. People's perceptions, knowledge, beliefs and opinions about matter of facts get (in)formed through the information reported on by the mass-media. However, a single source of information (and consensus) could be a potential cause of anomalies in the structure and evolution of a society. Hence, as the information available (and the way it is reported) is fundamental for our perceptions and opinions, the definition of conditions allowing for a good information to be disseminated is a pressing challenge. In this paper starting from a report on the last Italian political campaign in 2008, we derive a socio-cognitive computational model of opinion dynamics where agents get informed by different sources of information. Then, a what-if analysis, performed trough simulations on the model's parameters space, is shown. In particular, the scenario implemented includes three main streams of information acquisition, differing in both the contents and the perceived reliability of the messages spread. Agents' internal opinion is updated either by accessing one of the information sources, namely media and experts, or by exchanging information with one another. They are also endowed with cognitive mechanisms to accept, reject or partially consider the acquired information.
Weyl locally integrable conformal gravity, rotation curves and cosmic filaments<|sep|>Weyl's conformal theory of gravity is an extension of Einstein's theory of general relativity which associates metrics with 1-forms . In the case of locally integrable (closed non-exact) 1-forms the spacetime manifolds are no more simply connected. The Weil connections yield curvature tensors which satisfy the basic properties of Riemann curvature tensors. The Ricci tensors are symmetric, conformally invariant, and the Einstein tensors computed with the Weyl connections implicate a cosmological term replacing the cosmological constant by a function of spacetime, and a shear stress tensor. A toy model based on the Schwarzschild metric is presented where the associated 1-form is proportional to $d\varphi$ in Schwarzschild coordinates. This implies a singularity on the whole z-axis and it generates a torque effect on geodesics. According to initial conditions planar geodesics show almost constant velocities independently of r. In the free case spin effects occur in the neighbourhood of the singularity which are comparable to recent observations concerning cosmic filaments.
Nonlocal Exchange Interactions in Strongly Correlated Electron Systems<|sep|>We study the influence of ferromagnetic nonlocal exchange on correlated electrons in terms of a $SU(2)$-Hubbard-Heisenberg model and address the interplay of on-site interaction induced local moment formation and the competition of ferromagnetic direct and antiferromagnetic kinetic exchange interactions. In order to simulate thermodynamic properties of the system in a way that largely accounts for the on-site interaction driven correlations in the system, we advance the correlated variational scheme introduced in [M. Sch\"uler et al., Phys. Rev. Lett. 111, 036601 (2013)] to account for explicitily symmetry broken electronic phases by introducing an auxiliary magnetic field. After benchmarking the method against exact solutions of a finite system, we study the $SU(2)$-Hubbard-Heisenberg model on a square lattice. We obtain the $U$-$J$ finite temperature phase diagram of a $SU(2)$-Hubbard-Heisenberg model within the correlated variational approach and compare to static mean field theory. While the generalized variational principle and static mean field theory yield transitions from dominant ferromagnetic to antiferromagnetic correlations in similar regions of the phase diagram, we find that the nature of the associated phase tranistions differs between the two approaches. The fluctuations accounted for in the generalized variational approach render the transitions continuous, while static mean field theory predicts discontinuous transitions between ferro- and antiferromagnetically ordered states.
Multiple stellar systems under photometric and astrometric analysis<|sep|>The light-time effect method, its limitations and applications were studied. A powerful combined method of simultaneous analysis of the O-C diagrams and astrometric orbit in triple eclipsing-astrometric binaries was presented. Eleven eclipsing systems were studied in detail according to their O-C diagrams (RY Aqr, BF CMi, RW Cap, TY Cap, SS Cet, RR Dra, TY Del, TZ Eri, RV Per, UZ Sge, and BO Vul). The introduced method for studying the astrometric-eclipsing binaries was applied to QS Aql, VW Cep, Zeta Phe, V505 Sgr, HT Vir, and V2388 Oph. The algorithm for such an analysis was introduced and the its limitations were discussed. The catalogue of another systems, which contain eclipsing binaries in astrometric binaries, was presented. Such systems could be useful for prospective analysis. The method itself could be easily modified for estimation of the parallax of the individual systems.
Robust Domain Decomposition Preconditioners for Abstract Symmetric Positive Definite Bilinear Forms<|sep|>An abstract framework for constructing stable decompositions of the spaces corresponding to general symmetric positive definite problems into "local" subspaces and a global "coarse" space is developed. Particular applications of this abstract framework include practically important problems in porous media applications such as: the scalar elliptic (pressure) equation and the stream function formulation of its mixed form, Stokes' and Brinkman's equations. The constant in the corresponding abstract energy estimate is shown to be robust with respect to mesh parameters as well as the contrast, which is defined as the ratio of high and low values of the conductivity (or permeability). The derived stable decomposition allows to construct additive overlapping Schwarz iterative methods with condition numbers uniformly bounded with respect to the contrast and mesh parameters. The coarse spaces are obtained by patching together the eigenfunctions corresponding to the smallest eigenvalues of certain local problems. A detailed analysis of the abstract setting is provided. The proposed decomposition builds on a method of Efendiev and Galvis, Multiscale Model. Simul., 8 (2010), pp. 1461--1483, developed for second order scalar elliptic problems with high contrast. Applications to the finite element discretizations of the second order elliptic problem in Galerkin and mixed formulation, the Stokes equations, and Brinkman's problem are presented. A number of numerical experiments for these problems in two spatial dimensions are provided.
Does Bankruptcy Protection Affect Asset Prices? Evidence from changes in Homestead Exemptions<|sep|>Does the ability to protect an asset from unsecured creditors affect its price? This paper identifies the impact of bankruptcy protection on house prices using 139 changes in homestead exemptions. Large increases in the homestead exemption raised house prices 3% before 2005. Smaller exemption increases, to adjust for inflation, did not affect house prices. The effect disappeared after BAPCPA, a 2005 federal law designed to prevent bankruptcy abuse. The effect was bigger in inelastic locations.
Improving Contextual Coherence in Variational Personalized and Empathetic Dialogue Agents<|sep|>In recent years, latent variable models, such as the Conditional Variational Auto Encoder (CVAE), have been applied to both personalized and empathetic dialogue generation. Prior work have largely focused on generating diverse dialogue responses that exhibit persona consistency and empathy. However, when it comes to the contextual coherence of the generated responses, there is still room for improvement. Hence, to improve the contextual coherence, we propose a novel Uncertainty Aware CVAE (UA-CVAE) framework. The UA-CVAE framework involves approximating and incorporating the aleatoric uncertainty during response generation. We apply our framework to both personalized and empathetic dialogue generation. Empirical results show that our framework significantly improves the contextual coherence of the generated response. Additionally, we introduce a novel automatic metric for measuring contextual coherence, which was found to correlate positively with human judgement.
Suppressing Lepton Flavour Violation in a Soft-Wall Extra Dimension<|sep|>A soft-wall warped extra dimension allows one to relax the tight constraints imposed by electroweak data in conventional Randall-Sundrum models. We investigate a setup, where the lepton flavour structure of the Standard Model is realised by split fermion locations. Bulk fermions with general locations are not analytically tractable in a soft-wall background, so we follow a numerical approach to perform the Kaluza-Klein reduction. Lepton flavour violation is induced by the exchange of Kaluza-Klein gauge bosons. We find that rates for processes such as muon-electron conversion are significantly reduced compared to hard-wall models, allowing for a Kaluza-Klein scale as low as 2 TeV. Accommodating small neutrino masses forces one to introduce a large hierarchy of scales into the model, making pressing the question of a suitable stabilisation mechanism.
Non-linear quantum critical dynamics and fluctuation-dissipation ratios far from equilibrium<|sep|>Non-thermal correlations of strongly correlated electron systems and the far-from-equilibrium properties of phases of condensed matter have become a topical research area. Here, an overview of the non-linear dynamics found near continuous zero-temperature phase transitions within the context of effective temperatures is presented. In particular, we focus on models of critical Kondo destruction. Such a quantum critical state, where Kondo screening is destroyed in a critical fashion, is realized in a number of rare earth intermetallics. This raises the possibility of experimentally testing for the existence of fluctuation-dissipation relations far from equilibrium in terms of effective temperatures near quantum criticality. Finally, we present an analysis of a non-interacting, critical reference system, the pseudogap resonant level model, in terms of effective temperatures and contrast these results with those obtained near interacting quantum critical points.
Acquisition-invariant brain MRI segmentation with informative uncertainties<|sep|>Combining multi-site data can strengthen and uncover trends, but is a task that is marred by the influence of site-specific covariates that can bias the data and therefore any downstream analyses. Post-hoc multi-site correction methods exist but have strong assumptions that often do not hold in real-world scenarios. Algorithms should be designed in a way that can account for site-specific effects, such as those that arise from sequence parameter choices, and in instances where generalisation fails, should be able to identify such a failure by means of explicit uncertainty modelling. This body of work showcases such an algorithm, that can become robust to the physics of acquisition in the context of segmentation tasks, while simultaneously modelling uncertainty. We demonstrate that our method not only generalises to complete holdout datasets, preserving segmentation quality, but does so while also accounting for site-specific sequence choices, which also allows it to perform as a harmonisation tool.
Equilibrium-charge diagram of single quantum dot in an axial magnetic field<|sep|>The chemical potential of a quantum dot parabolically confining one or two electrons is studied in an axial magnetic field. The number of electrons in the dot is given by combination between the chemical potentials of the dots and leads. The equilibrium-charge diagram of the dot is obtained in the phase space determined by the magnetic field and the lead voltage. The influence of the magnetic field on the electron number and the electron transport is discussed. The dependence of the electron number on the magnetic fields and the lead voltage is clear at a glance in the equilibrium-charge diagram. Moreover, the diagram is a compact tool to show the condition of the electron transport.
Studying stellar halos with future facilities<|sep|>Stellar halos around galaxies retain fundamental evidence of the processes which lead to their build up. Sophisticated models of galaxy formation in a cosmological context yield quantitative predictions about various observable characteristics, including the amount of substructure, the slope of radial mass profiles and three dimensional shapes, and the properties of the stellar populations in the halos. The comparison of such models with the observations provides constraints on the general picture of galaxy formation in the hierarchical Universe, as well as on the physical processes taking place in the halos formation. With the current observing facilities, stellar halos can be effectively probed only for a limited number of nearby galaxies. In this paper we illustrate the progress that we expect in this field with the future ground based large aperture telescopes (E-ELT) and with space based facilities as JWST.
Sharing pattern submodels for prediction with missing values<|sep|>Missing values are unavoidable in many applications of machine learning and present a challenge both during training and at test time. When variables are missing in recurring patterns, fitting separate pattern submodels have been proposed as a solution. However, independent models do not make efficient use of all available data. Conversely, fitting a shared model to the full data set typically relies on imputation which may be suboptimal when missingness depends on unobserved factors. We propose an alternative approach, called sharing pattern submodels, which make predictions that are a) robust to missing values at test time, b) maintains or improves the predictive power of pattern submodels, and c) has a short description enabling improved interpretability. We identify cases where sharing is provably optimal, even when missingness itself is predictive and when the prediction target depends on unobserved variables. Classification and regression experiments on synthetic data and two healthcare data sets demonstrate that our models achieve a favorable trade-off between pattern specialization and information sharing.
Slipping magnetic reconnections with multiple flare ribbons during an X-class solar flare<|sep|>With the observations of the Solar Dynamics Observatory, we present the slipping magnetic reconnections with multiple flare ribbons (FRs) during an X1.2 eruptive flare on 2014 January 7. A center negative polarity was surrounded by several positive ones, and there appeared three FRs. The three FRs showed apparent slipping motions, and hook structures formed at their ends. Due to the moving footpoints of the erupting structures, one tight semi-circular hook disappeared after the slippage along its inner and outer edge, and coronal dimmings formed within the hook. The east hook also faded as a result of the magnetic reconnection between the arcades of a remote filament and a hot loop that was impulsively heated by the under flare loops. Our results are accordant with the slipping magnetic reconnection regime in 3D standard model for eruptive flares. We suggest that complex structures of the flare is likely a consequence of the more complex flux distribution in the photosphere, and the eruption involves at least two magnetic reconnections.
Robust Unit Commitment Considering Strategic Wind Generation Curtailment<|sep|>Wind generation is traditionally treated as a non-dispatchable resource and is fully absorbed unless there are security issues. To tackle the operational reliability issues caused by the volatile and non-dispatchable wind generation, many dispatch frameworks have been proposed, including robust unit commitment (RUC) considering wind variation. One of the drawbacks that commonly exist in those dispatch frameworks is increased demand on flexibility resources and associated costs. To improve wind dispatchability and reduce flexibility resource costs, in this paper, we propose a novel RUC model considering strategic wind generation curtailment (WGC). Strategic WGC can reduce wind uncertainty and variability and increase the visibility of wind generation capacity. As a result, the ramping requirement for wind generation will be reduced and ramp-up capability of wind generation can be increased, leading to reduced day-ahead operational cost with guaranteed operational reliability requirement of power systems. The economic benefits also include profits gained by wind farm by providing ramping-up capacities other auxiliary services. We also propose a solution algorithm based on the column and constraint generation (C&CG). Simulations on the IEEE 39-bus system and two larger test systems demonstrate the effectiveness of the proposed RUC model and efficiency of the proposed computational methodology.
Insight into bias in time-stratified case-crossover studies<|sep|>The use of case-crossover designs has become widespread in epidemiological and medical investigations of transient associations. However, the most popular reference-select strategy, the time-stratified schema, is not a suitable solution for controlling bias in case-crossover studies. To prove this, we conducted a time series decomposition for daily ozone (O3) records; scrutinized the ability of the time-stratified schema on controlling the yearly, monthly and weekly time trends; and found it failed on controlling the weekly time trend. Based on this finding, we proposed a new logistic regression approach in which we did adjustment for the weekly time trend. A comparison between the traditional model and the proposed method was done by simulation. An empirical study was conducted to explore potential associations between air pollutants and AMI hospitalizations. In summary, time-stratified schema provide effective control on yearly and monthly time trends but not on weekly time trend. Therefore, the estimation from the traditional logistical regression basically reveals the effect of weekly time trend, instead of the transient effect. In contrast, the proposed logistic regression with adjustment for weekly time trend can effectively eliminate system bias in case-crossover studies.
Formulating an $n$-person noncooperative game as a tensor complementarity problem<|sep|>In this paper, we consider a class of $n$-person noncooperative games, where the utility function of every player is given by a homogeneous polynomial defined by the payoff tensor of that player, which is a natural extension of the bimatrix game where the utility function of every player is given by a quadratic form defined by the payoff matrix of that player. We will call such a problem the multilinear game. We reformulate the multilinear game as a tensor complementarity problem, a generalization of the linear complementarity problem; and show that finding a Nash equilibrium point of the multilinear game is equivalent to finding a solution of the resulted tensor complementarity problem. Especially, we present an explicit relationship between the solutions of the multilinear game and the tensor complementarity problem, which builds a bridge between these two classes of problems. We also apply a smoothing-type algorithm to solve the resulted tensor complementarity problem and give some preliminary numerical results for solving the multilinear games.
V1460 Her: A fast spinning white dwarf accreting from an evolved donor star<|sep|>We present time-resolved optical and ultraviolet spectroscopy and photometry of V1460~Her, an eclipsing cataclysmic variable with a 4.99\,h orbital period and an overluminous K5-type donor star. The optical spectra show emission lines from an accretion disc along with absorption lines from the donor. We use these to measure radial velocities, which, together with constraints upon the orbital inclination from photometry, imply masses of $M_1=0.869\pm0.006\,\mathrm{M}_\odot$ and $M_2=0.295\pm0.004\,\mathrm{M}_\odot$ for the white dwarf and the donor. The radius of the donor, $R_2=0.43\pm0.002\,\mathrm{R}_\odot$, is $\approx 50$ per cent larger than expected given its mass, while its spectral type is much earlier than the M3.5 type that would be expected from a main sequence star with a similar mass. HST spectra show strong $\mathrm{N{\small V}}$ 1240 A emission but no $\mathrm{C{\small IV}}$ 1550 A emission, evidence for CNO-processed material. The donor is therefore a bloated, over-luminous remnant of a thermal-timescale stage of high mass transfer and has yet to re-establish thermal equilibrium. Remarkably, the HST ultraviolet data also show a strong 30 per cent peak-to-peak, $38.9\,$s pulsation that we explain as being due to the spin of the white dwarf, potentially putting V1460 Her in a similar category to the propeller system AE Aqr in terms of its spin frequency and evolutionary path. AE Aqr also features a post-thermal timescale mass donor, and V1460 Her may therefore be its weak magnetic field analogue since the accretion disc is still present, with the white dwarf spin-up a result of a recent high accretion rate.
Future-Aware Diverse Trends Framework for Recommendation<|sep|>In recommender systems, modeling user-item behaviors is essential for user representation learning. Existing sequential recommenders consider the sequential correlations between historically interacted items for capturing users' historical preferences. However, since users' preferences are by nature time-evolving and diversified, solely modeling the historical preference (without being aware of the time-evolving trends of preferences) can be inferior for recommending complementary or fresh items and thus hurt the effectiveness of recommender systems. In this paper, we bridge the gap between the past preference and potential future preference by proposing the future-aware diverse trends (FAT) framework. By future-aware, for each inspected user, we construct the future sequences from other similar users, which comprise of behaviors that happen after the last behavior of the inspected user, based on a proposed neighbor behavior extractor. By diverse trends, supposing the future preferences can be diversified, we propose the diverse trends extractor and the time-aware mechanism to represent the possible trends of preferences for a given user with multiple vectors. We leverage both the representations of historical preference and possible future trends to obtain the final recommendation. The quantitative and qualitative results from relatively extensive experiments on real-world datasets demonstrate the proposed framework not only outperforms the state-of-the-art sequential recommendation methods across various metrics, but also makes complementary and fresh recommendations.
Learning for Detecting Norm Violation in Online Communities<|sep|>In this paper, we focus on normative systems for online communities. The paper addresses the issue that arises when different community members interpret these norms in different ways, possibly leading to unexpected behavior in interactions, usually with norm violations that affect the individual and community experiences. To address this issue, we propose a framework capable of detecting norm violations and providing the violator with information about the features of their action that makes this action violate a norm. We build our framework using Machine Learning, with Logistic Model Trees as the classification algorithm. Since norm violations can be highly contextual, we train our model using data from the Wikipedia online community, namely data on Wikipedia edits. Our work is then evaluated with the Wikipedia use case where we focus on the norm that prohibits vandalism in Wikipedia edits.
Few-shot Generation of Personalized Neural Surrogates for Cardiac Simulation via Bayesian Meta-Learning<|sep|>Clinical adoption of personalized virtual heart simulations faces challenges in model personalization and expensive computation. While an ideal solution is an efficient neural surrogate that at the same time is personalized to an individual subject, the state-of-the-art is either concerned with personalizing an expensive simulation model, or learning an efficient yet generic surrogate. This paper presents a completely new concept to achieve personalized neural surrogates in a single coherent framework of meta-learning (metaPNS). Instead of learning a single neural surrogate, we pursue the process of learning a personalized neural surrogate using a small amount of context data from a subject, in a novel formulation of few-shot generative modeling underpinned by: 1) a set-conditioned neural surrogate for cardiac simulation that, conditioned on subject-specific context data, learns to generate query simulations not included in the context set, and 2) a meta-model of amortized variational inference that learns to condition the neural surrogate via simple feed-forward embedding of context data. As test time, metaPNS delivers a personalized neural surrogate by fast feed-forward embedding of a small and flexible number of data available from an individual, achieving -- for the first time -- personalization and surrogate construction for expensive simulations in one end-to-end learning framework. Synthetic and real-data experiments demonstrated that metaPNS was able to improve personalization and predictive accuracy in comparison to conventionally-optimized cardiac simulation models, at a fraction of computation.
Fragmentation of star-forming filaments in the X-shape Nebula of the California molecular cloud<|sep|>Dense molecular filaments are central to the star formation process, but the detailed manner in which they fragment into prestellar cores is not yet well understood. Here, we investigate the fragmentation properties and dynamical state of several star-forming filaments in the X-shape Nebula region of the California MC, in an effort to shed some light on this issue. We used multi-wavelength far-infrared images from Herschel and the getsources and getfilaments extraction methods to identify dense cores and filaments and derive their basic properties. We also used a map of $\rm ^{13}CO (2-1)$ emission from SMT 10m submillimeter telescope to constrain the dynamical state of the filaments. We identified 10 filaments, as well as 57 dense cores. Two star-forming filaments (# 8 and # 10) stand out in that they harbor quasi-periodic chains of dense cores with a typical projected core spacing of $\sim$0.15 pc. These two filaments have thermally supercritical line masses and are not static. Filament~8 exhibits a prominent transverse velocity gradient, suggesting that it is accreting gas from the parent cloud gas reservoir. In both cases, the observed (projected) core spacing is similar to the filament width and significantly shorter than the canonical separation of $\sim \,$4 times the filament width predicted by classical cylinder fragmentation theory. We suggest that continuous accretion of gas onto the two star-forming filaments, as well as geometrical bending of the filaments, may account for the observed core spacing. Our findings suggest that the characteristic fragmentation lengthscale of molecular filaments is quite sensitive to external perturbations from the parent cloud, such as gravitational accretion of ambient material.
Unit Disk Cover Problem<|sep|>Given a set ${\cal D}$ of unit disks in the Euclidean plane, we consider (i) the {\it discrete unit disk cover} (DUDC) problem and (ii) the {\it rectangular region cover} (RRC) problem. In the DUDC problem, for a given set ${\cal P}$ of points the objective is to select minimum cardinality subset ${\cal D}^* \subseteq {\cal D}$ such that each point in ${\cal P}$ is covered by at least one disk in ${\cal D}^*$. On the other hand, in the RRC problem the objective is to select minimum cardinality subset ${\cal D}^{**} \subseteq {\cal D}$ such that each point of a given rectangular region ${\cal R}$ is covered by a disk in ${\cal D}^{**}$. For the DUDC problem, we propose an $(9+\epsilon)$-factor ($0 < \epsilon \leq 6$) approximation algorithm. The previous best known approximation factor was 15 \cite{FL12}. For the RRC problem, we propose (i) an $(9 + \epsilon)$-factor ($0 < \epsilon \leq 6$) approximation algorithm, (ii) an 2.25-factor approximation algorithm in reduce radius setup, improving previous 4-factor approximation result in the same setup \cite{FKKLS07}. The solution of DUDC problem is based on a PTAS for the subproblem LSDUDC, where all the points in ${\cal P}$ are on one side of a line and covered by the disks centered on the other side of that line.
Cavity approach to sphere packing in Hamming space<|sep|>In this paper we study the hard sphere packing problem in the Hamming space by the cavity method. We show that both the replica symmetric and the replica symmetry breaking approximations give maximum rates of packing that are asymptotically the same as the lower bound of Gilbert and Varshamov. Consistently with known numerical results, the replica symmetric equations also suggest a crystalline solution, where for even diameters the spheres are more likely to be found in one of the subspaces (even or odd) of the Hamming space. These crystalline packings can be generated by a recursive algorithm which finds maximum packings in an ultra-metric space. Finally, we design a message passing algorithm based on the cavity equations to find dense packings of hard spheres. Known maximum packings are reproduced efficiently in non trivial ranges of dimensions and number of spheres.
Agent Based Negotiation using Cloud - an Approach in E-Commerce<|sep|>Cloud computing allows subscription based access to computing. It also allows storage services over Internet. Automated Negotiation is becoming an emerging, and important area in the field of Multi Agent Systems in ECommerce. Multi Agent based negotiation system is necessary to increase the efficiency of E-negotiation process. Cloud computing provides security and privacy to the user data and low maintenance costs. We propose a Negotiation system using cloud. In this system, all product information and multiple agent details are stored on cloud. Both parties select their agents through cloud for negotiation. Agent acts as a negotiator. Agents have users details and their requirements for a particular product. Using users requirement, agents negotiate on some issues such as price, volume, duration, quality and so on. After completing negotiation process, agents give feedback to the user about whether negotiation is successful or not. This negotiation system is dynamic in nature and increases the agents with the increase in participating user.
Generalized conditional symmetries of evolution equations<|sep|>We analyze the relationship of generalized conditional symmetries of evolution equations to the formal compatibility and passivity of systems of differential equations as well as to systems of vector fields in involution. Earlier results on the connection between generalized conditional invariance and generalized reduction of evolution equations are revisited. This leads to a no-go theorem on determining equations for operators of generalized conditional symmetry. It is also shown that up to certain equivalences there exists a one-to-one correspondence between generalized conditional symmetries of an evolution equation and parametric families of its solutions.
A surface moving mesh method based on equidistribution and alignment<|sep|>A surface moving mesh method is presented for general surfaces with or without explicit parameterization. The method can be viewed as a nontrivial extension of the moving mesh partial differential equation method that has been developed for bulk meshes and demonstrated to work well for various applications. The main challenges in the development of surface mesh movement come from the fact that the Jacobian matrix of the affine mapping between the reference element and any simplicial surface element is not square. The development starts with revealing the relation between the area of a surface element in the Euclidean or Riemannian metric and the Jacobian matrix of the corresponding affine mapping, formulating the equidistribution and alignment conditions for surface meshes, and establishing a meshing energy function based on the conditions. The moving mesh equation is then defined as the gradient system of the energy function, with the nodal mesh velocities being projected onto the underlying surface. The analytical expression for the mesh velocities is obtained in a compact, matrix form, which makes the implementation of the new method on a computer relatively easy and robust. Moreover, it is analytically shown that any mesh trajectory generated by the method remains nonsingular if it is so initially. It is emphasized that the method is developed directly on surface meshes, making no use of any information on surface parameterization. It utilizes surface normal vectors to ensure that the mesh vertices remain on the surface while moving, and also assumes that the initial surface mesh is given. The new method can apply to general surfaces with or without explicit parameterization since the surface normal vectors can be computed even when the surface only has a numerical representation. A selection of two- and three-dimensional examples are presented.
The ALICE experiment -- A journey through QCD<|sep|>The ALICE experiment was proposed in 1993, to study strongly interacting matter at extreme energy densities via a comprehensive investigation of nuclear collisions at the LHC. Its physics programme initially focused on the determination of the properties of the Quark-Gluon Plasma (QGP), a deconfined state of quarks and gluons and was extended along the years, covering a diverse ensemble of observables related to Quantum Chromodynamics (QCD), the theory of strong interactions. The experiment has studied Pb-Pb, Xe-Xe, p-Pb and pp collisions in the multi-TeV energy range, during the Run 1 and Run 2 data taking periods at the LHC (2009-2018). The aim of this review article is to gather and summarise a selection of ALICE physics results and to discuss their implications on the current understanding of the macroscopic and microscopic properties of strongly interacting matter at the highest temperature reached in the laboratory. It will be shown that it is possible to have a quantitative description of the properties of the QGP produced in Pb--Pb collisions. We also show that various features, commonly ascribed to QGP formation, are detected for a wide range of interacting system sizes. Precision measurements of QCD-related observables not directly connected to the study of the QGP will also be discussed. Prospects for future measurements with the ALICE detector and its foreseen upgrades will also be briefly described.
Dependent Inductive and Coinductive Types are Fibrational Dialgebras<|sep|>In this paper, I establish the categorical structure necessary to interpret dependent inductive and coinductive types. It is well-known that dependent type theories \`a la Martin-L\"of can be interpreted using fibrations. Modern theorem provers, however, are based on more sophisticated type systems that allow the definition of powerful inductive dependent types (known as inductive families) and, somewhat limited, coinductive dependent types. I define a class of functors on fibrations and show how data type definitions correspond to initial and final dialgebras for these functors. This description is also a proposal of how coinductive types should be treated in type theories, as they appear here simply as dual of inductive types. Finally, I show how dependent data types correspond to algebras and coalgebras, and give the correspondence to dependent polynomial functors.
Separation delay via hydro-acoustic control of a NACA4412 airfoil in pre-stalled conditions<|sep|>We have performed large-eddy simulations of turbulent separation control via impedance boundary conditions (IBCs) on a \nacafft airfoil in near-stalled conditions. The uncontrolled baseline flow is obtained for freestream Mach numbers of $M_\infty=0.3$, chord-Reynolds numbers $Re_c = 1.5\times10^6$ and angle of attack, $\alpha=14^{\circ{}}$. Flow control is applied via imposition of complex IBCs using the time-domain implementation developed by Scalo, Bodart, and Lele, $\textit{Phys. Fluids} $(2015). Separation is delayed due to the enhanced mixing associated with convectively amplified spanwise-oriented Kelvin-Helmholtz (KH) rollers, generated via hydro-acoustic instabilities. The latter are the result of the interaction of the wall-normal transpiration through the impedance panel and the overlying mean background shear. The result is an alteration of the coupled instability between the separating shear layer and the vortex shedding in the wake (already present in the uncontrolled baseline flow) yielding unique wake topologies associated with different intensities for the passively generated KH vortical structures. Specifically, enhancements up to +13\% in the lift coefficients have been obtained. Results show that tuning of the resonant cavities below the natural shedding frequency is required to generate KH rollers structures with a sufficiently large entrainment diameter to encompass the full extent of the separated region, thereby enhancing mixing and promoting reattachment. Overall, the results presented in this work show that the adoption of hydro-acoustically tuned resonant panels is a promising passive control technique for boundary layer separation control.
