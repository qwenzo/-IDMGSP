Climate & BCG: Effects on COVID-19 Death Growth Rates<|sep|>Contrary to previous claims our analysis of growth rates for deaths from countries worldwide are consistent with no eﬀect from climate, pollution or BCG vaccination. The only signiﬁcant correlation detected is with the positive rate of tests: a country that intrinsically had a high R0 (due to high population density etc...), would naturally tend to be more overwhelmed and hence run low on testing kits earlier, leading to increased fraction of positive tests. We did ﬁnd some weak suggestive evidence, at < 3σ, that temperature and relative humidity correlate with death growth rates. A separate analysis of blood type data shows that A+ type is the most important blood type, though the signiﬁcance is marginal, both because the data quality is low and the statistical signiﬁcance is weak. Our combined statistical and machine learning analysis ﬁnds no evidence for PM2.5 pollution, other blood types or UV Index as drivers of COVID-19. More data could be obtained by dropping the requirement that all countries in the sample have data for all the potential factors, which could potentially allow for some of the eﬀects to be detected at higher statistical signiﬁcance but at the cost of making model comparison signiﬁcantly more diﬃcult.
Provable Self-Representation Based Outlier Detection in a Union of Subspaces<|sep|>We presented an outlier detection method that combined data self-representation and random walks on a representation graph. Unlike many prior methods for robust PCA, our method is able to deal with multiple subspaces and does not require the number of subspaces or their dimensions to be known. Our analysis showed that the method is guaranteed to identify outliers when certain geometric conditions are satisﬁed and two connectivity assumptions hold. In our experiments on face image and object image databases, our method achieves the state-of-the-art performance. This work was supported by NSF BIGDATA grant 1447822. The authors also thank Manolis Tsakiris, Conner Lane and Chun-Guang Li for helpful comments. The appendix is organized as follows. In Section A we discuss subspace-preserving representations and give an outline of the proof for Theorem 4.1. Section B contains relevant background on Markov chain theory, which is then used in Section C for proving Lemma 4.1 and Lemma 4.2, as well as providing an in-depth discussion of the Ces`aro mean used for outlier detection. In Section D we provide some additional results for experiments on the Extended Yale B database that provide additional insight into the behavior of the methods. The idea of a subspace-preserving representation has been extensively studied in the literature of subspace clustering to guarantee the correctness of clustering [12, 43, 44, 31, 27, 10, 38, 59, 17, 58, 56, 50, 52, 24]. Concretely, the data in a subspace clustering task are assumed to lie in a union of low dimensional subspaces, without any outliers that lie outside of the subspaces. A data self-representation matrix is called subspace-preserving if each point uses only points that are from its own subspace in its representation. Theoretical results in subspace clustering can be adapted to study subspace-preserving representations in the presence of outliers. Here, we use the analysis and result from [56], which studied the elastic net representation (4) for subspace clustering, to prove a subspace-preserving representation result in the presence of outliers, i.e. Theorem 4.1. We also present a corollary of Theorem 4.1 which allows us to compare our result with other subspace clustering results. The proof of Theorem 4.1 follows mostly from the work [56]. We provide an outline of the proof for completeness. Consider the vector rℓ j, which is the solution of the problem in the statement of Theorem 4.1. Notice that the entries of rℓ j correspond to columns of the data matrix Xℓ −j. One can subsequently construct a representation vector by padding additional zeros to rℓ j at entries corresponding to points in X that are not in Xℓ −j. Note that this vector is trivially subspace-preserving by construction. The idea of the proof is to show that this constructed vector, which is subspace-preserving by construction, is a solution to the optimization problem (4) (and no other vector is). A sufﬁcient condition for this to hold is that δj, which is computed from rℓ j, needs to have low correlation with all points xk /∈ Sℓ. More precisely, we have the following lemma. Lemma A.1 can be proved by using the optimality condition of the optimization problem in (4). Equivalently, it suggests that rj is subspace-preserving if Lemma A.2 ([57, Lemma C.2]). If κj be the maximum coherence between the oracle point δj and columns of Xℓ −j, i.e. κj = maxk̸=j:xk∈Sℓ |⟨xk, ¯δj⟩|, then Another commonly used geometric quantity for characterizing when representations will be subspace-preserving is the inradius of sets of points [43, 44, 59, 58, 53, 52, 50]. In order to understand the relationship to the results found in these works, we present a corollary of Theorem 4.1. Deﬁnition A.1 (inradius). The (relative) inradius of a convex body P, denoted as ρ(P), is the radius of the largest ℓ2 ball in the span of P that can be inscribed in P. where ¯δj is deﬁned in Theorem 4.1, and ρj is the inradius of the convex hull of the symmetrized points in Xℓ j, i.e. The inradius captures the distribution of the columns of Xℓ −j, i.e. it is large if points are well spread out in Sℓ. Thus, the condition in (A.5) is easier to be satisﬁed if the set of points in Sℓ is dense and well covers the entire subspace. Note that this requirement is stronger than that in Theorem 4.1, which only requires points in Sℓ to be dense around the oracle point δj (i.e. it requires maxk̸=j:xk∈Sℓ |⟨xk, ¯δj⟩| to be large). In fact, it is established in [56] that maxk̸=j:xk∈Sℓ |⟨xk, ¯δj⟩| ≥ ρj, so that the condition in (A.5) is a stronger requirement than that of (7) in Theorem 4.1. We present background material on Markov chain theory that will help us understand the Ces`aro mean (6) used for outlier detection in our method. The following material is organized from textbooks [41, 14, 45, 23] and the website http://www.math.uah.edu/stat. We consider a Markov chain (X0, X1, · · · ) on a ﬁnite state space Ω with transition probabilities pij for i, j ∈ Ω. The t-step transition probabilities are deﬁned to be p(t) ij := P{Xt = j|X0 = i}. Deﬁnition B.1. State j is accessible from state i, denoted as i → j, if p(t) ij > 0 for some t > 0. We say that the states i and j communicate with each other, denoted by i ↔ j, if i → j and j → i. Since it can be shown that ↔ is an equivalence relation, it induces a partition of the state space Ω into disjoint equivalence classes known as communicating classes. We are interested in each of the closed communicating classes. Deﬁnition B.2. A non-empty set C ⊆ Ω is called a closed set if pij = 0 for i ∈ C and j /∈ C. Theorem B.1 ([41]). The state space Ω has the unique decomposition Ω = I ∪ E1 ∪ . . . En, where I is the set of inessential states, and E1, . . . , En are closed communicating classes containing essential states. By Theorem B.1, the state space of any Markov chain is composed of the essential states and inessential states, and the essential states can be further decomposed into a union of communicating classes. Therefore, the probability Theorem B.2 ([23, Proposition 1.14, Corollary 1.17]). A Markov chain consisting of one closed communicating class has a unique stationary distribution. Moreover, each entry of the stationary distribution is positive. By Theorem B.2, each component Eℓ for ℓ = 1, · · · , n in the decomposition of the Markov chain in Theorem B.1 has a unique positive stationary distribution πEℓ, i.e. Let f (t) ij := P{Xt = j, Xt′ ̸= j for 1 ≤ t′ < t|X0 = i} be the probability that the chain starting at i enters j for the ﬁrst time at the t-th step. The hitting probability fij = P{Xt = j for some t > 0|X0 = i} is the probability that the random walk ever makes a transition to state j when started at i, i.e. The mean return time µj := �∞ t=1 tf (t) jj is the expected time for a random walk starting from state j will return to state j. A general convergence result is stated as follows. Lemma B.1. If i, j ∈ Ω are in the same closed communicating class, then fij = fji = 1. Also, if i ∈ Ω is an inessential state and Eℓ ⊆ Ω is a closed communicating class, then fij = fi→Eℓ for all j ∈ Eℓ, where fi→Eℓ is the hitting probability from state i to class Eℓ. Lemma B.2. For every closed communicating class Eℓ ⊆ Ω, it holds that µEℓ = 1/πEℓ (entry-wise division), where µEℓ is the vector of mean return times of states in Eℓ. If i ∈ Ω is an inessential state, then µi = ∞. By combining Theorem B.3 with Lemma B.1 and Lemma B.2, the Ces`aro limit of a probability transition matrix of the form in (B.1) can be written as 1 · πE1 0 0 ... ... 0 1 · πEn 0 f I→E1 · πE1 · · · f I→E1 · πEn 0 in which f I→Eℓ is a column vector of hitting probability from each state in I to class Eℓ. We note that while the Ces`aro mean converges, the tstep transition probability P t does not necessarily converge. Consider, for example, the probability transition matrix P = [ 0 1 1 0 ]. In this case, p(t) 12 = 1 when t is odd and p(t) 12 = 0 when t is even, i.e. p(t) 12 is oscillating and never converges. In general, P t converges if and only if each of the closed communicating classes Eℓ for ℓ = 1, . . . , n is aperiodic.
Full Diversity Space-Time Block Codes with Low-Complexity Partial Interference Cancellation Group Decoding<|sep|>In this paper, a design of full-diversity STBC with reducedcomplexity PIC group decoding was proposed. The proposed code design can obtain full diversity under both ML decoding and PIC group decoding. Moreover, the decoding complexity of the full diversity STBC is equivalent to a joint ML decoding of M/2 symbols for M transmit antennas while the rate can be up to 2 symbols per channel use. For example, in a
Skeleton-based Gait Index Estimation with LSTMs<|sep|>This paper proposes an approach for gait index estimation that requires a sequence of skeletons. Unlike related studies, the feature extraction is implicitly performed using auto-encoders. By employing a LSTM for the encoder and another one for the decoder, our system has the ability to work with temporal inputs. A weak gait index is estimated by joint coordinates belonging to each 3D axis. A weighted combination of the three weak indices signiﬁcantly improves the efﬁciency of our index estimation. By adding a dropout layer right after the input, our system is slightly enhanced and outperformed related studies that work with skeletons as well as silhouettes. The authors would like to thank the NSERC (Natural Sciences and Engineering Research Council of Canada) for having supported this work (Discovery Grant RGPIN-2015-05671). This work was also supported by The Ministry of Education and Training, Vietnam, Grant KYTH-59.
Aggregates of clusters in the Gaia data<|sep|>Based on our analysis of the aggregates (derived from the stars which were assigned to multiple clusters), we conclude that the automatic procedure used in CG18 and CG20 failed to correctly determine parallaxes of some of the clusters listed in Table 1: 1. Alessi 5 and BH 99: The central parallaxes from the source work were correctly reproduced by analysing the distribution of parallaxes. We propose that the match results from the fact that the clusters are only partially overlapping in the phase space. Another important contributing factor is that only one of the stars is shared among them. Assuming that their relative distance is about 50 pc, it is interesting to note that the metallicity and the extinction (taken from the literature)
3C273 variability at 7 mm: Evidences of shocks and precession in the jet<|sep|>We presented the results of four years monitoring of 3C273 at 7 mm. During this period we detected a ﬂare in 2010 March, that we interpreted as the radio counterpart of the extremely intense γ-ray ﬂare observed by Fermi/LAT in
Material Dependence of the Wire-Particle Casimir Interaction<|sep|>In summary, we have studied the Casimir energy between a cylindrical wire and a spherical particle. For large separations, we have derived the asymptotic energies for the Drude, plasma and perfect metal models. In addition, we have calculated the Casimir interaction between a metallic wire and an isotropic atom. Our results for the wire–atom system is in complete agreement with previous results obtained through a diﬀerent method [20– 23] . Furthermore, we have computed the Casimir interaction between a spherical particle and a wire. Such computations are quite demanding due to lack of spherical symmetry. Our numerical results perfectly match the asymptotic energies. For short separations, we obtained the energy using the Proximity Force Approximation (PFA) and compared it with our numerical data. This comparison indicates that as the distance between the wire and particle decreases, the numerical results for the Casimir energy becomes closer to the PFA one. It is noteworthy that depending on the separation, the material properties of the metallic wire may not play a role in the interaction energy, similar to the parallel wires and wire–plate systems [11, 12]. In a cylinder–sphere system, we do not observe a universal behavior as we have previously obtained for parallel wires and a wire–plate geometries because of the physical properties of the spherical particle. However, one can still have “universal” regimes in which the interaction does not depend on the material properties of the metallic wire. In case of the plasma wire with the plasma wavelength λp and radius Rc, at suﬃciently large separations, d/Rc ≫ exp(λ2 p/R2 c), the material properties of the wire does not play any role in the asymptotic interactions between the wire and a particle or an atom. In contrast for the Drude wire with conductivity σ and the characteristic length λσ = 2πc/σ, at large separations d2/R2 c ≫ d/λσ, the asymptotic energy depends on the the material properties of the wire. Quite interestingly, in the opposite limit, d2/R2 c ≪ d/λσ and d ≫ λ2 p/λσ, the asymptotic interaction becomes independent of the material properties of the Drude wire. The speciﬁc behavior of the Drude wires has been explained in Refs. [11, 12] in terms of large scale charge ﬂuctuations. At the end we emphasize that since simple generic geometries appear in many nano- and micrometer–sized systems, the knowledge of the interaction between a metallic sphere and cylinder could be important for an eﬃcient design of low–dimensional structures. We thank M. Kardar and U. Mohideen for useful discussions. This work was supported by the NSF through grants DMR-06-45668 (RZ), DARPA contract The T-matrix of a sphere in spherical vector wave basis is diagonal in the quantum numbers l, m and the electromagnetic polarizations E and M ϵ(iκ)/µ(iκ). The Tmatrix elements for E-multipoles, T e s,ℓm, are obtained from Eq. (A1) by interchanging ϵ and µ. The T-matrix elements of a cylinder with dielectric response ϵ(iκ) and magnetic permeability µ(iκ) are given by Note that ∆2 can be obtained from Eq. (A4) by interchanging ϵ with µ, and ∆3 can be found by replacing K′ n with I′ n and Kn with In in Eq. (A4). Moreover, T MM kzn can be obtained by replacing ϵ with µ and considering T ME kzn = −T EM kzn . The electromagnetic ﬁeld far enough outside a sphere can be written in terms of the regular wave function with the electromagnetic polarization P, |Ereg kzmP (icκ)⟩, the free electromagnetic Green’s function, G0(icκ), and the scattering operator of the sphere, Ts(icκ) [7], where icκ arguments are dropped for brevity. The expansion of the free Green’s function in terms of the regular and outgoing wave functions is given by where CE = −CM = 1/(2πL) is the normalization coefﬁcient with L the overall length of the cylinder. Inserting Eq. (B2) into Eq. (B1) yields the T-matrix of a sphere in cylindrical basis. Now we expand the cylindrical basis wave functions in terms of the spherical basis waves, where Q is the electromagnetic polarization, ℓ is the quantum number related to the spherical wave functions and the coeﬃcients DℓmQ,kzmP are the elements of the conversion matrix from the cylindrical to spherical basis, see Appendix C for the detailed description. Since the azimuthal dependence of the wave functions in both sides of Eq. (B5) are the same, the sum runs on the quantum number ℓ and polarization Q. Inserting Eq. (B5) into Eq. (B4), we obtain the T-matrix of the sphere in the spherical basis, see Appendix A, and C′ M(κ) = −C′ E(κ) = κ the normalization coeﬃcients of the Green’s function expansion in spherical basis. The ratio of the normalization coeﬃcients in Eq. (B6) is (1 − 2δP,Q)/(2πκL). Since the T-matrix of the sphere in spherical basis is diagonal in l, m and polarization, Eq. (B6) is simpliﬁed to The coeﬃcients of the expansion of the cylindrical vector waves in terms of the spherical ones determine the elements of the conversion matrix Dℓm,kzm. These coefﬁcients are known and have already been calculated [27– 29]. In this appendix we make the previously derived coeﬃcients consistent with the Wick-rotated vector wave bases introduced in Ref. [7]. The expansion of cylindrical vector waves (mmλ, nmλ) in terms of spherical vector waves (mℓm, nℓm) is given by [28] with cos(α) = h/k. Note that m and n are the vector wave functions in Euclidean space. Taking into account h ≡ kz, k ≡ iκ and λ ≡ i � κ2 + k2z, the vector wave bases (Mreg, Nreg) deﬁned in Ref. [7] are related to the bases (m, n) deﬁned in Ref. [28] by the relations
The luminosities of cool supergiants in the Magellanic Clouds, and the Humphreys-Davidson limit revisited<|sep|>We have combined various surveys of cool supergiants and used multi-wavelength survey photometry from the U-band to the midinfrared to redetermine the luminosity distributions of cool massive stars in the Large and Small Magellanic Clouds. Our main ﬁndings are as follows: • The most luminous cool stars in the LMC and SMC have log(L/L⊙)=5.77 and 5.55 respectively, though the brightest of these is highly variable. The next most luminous stars have log(L/L⊙)=5.50 and 5.36 respectively. If these stars represent the upper luminosity limit Lmax (otherwise known as the HumphreysDavidson limit), this is a downward revision of the previously quoted limit of log(L/L⊙)≃5.7 in the literature. • We ﬁnd no evidence to support the commonly-held view that Lmax is higher at lower metallicity. Indeed our results indicate that it is unlikely that Lmax in the SMC is higher than in the LMC, even after accounting for low number statistics. This argues against metallicity-dependent mass-loss being the cause of Lmax. • A population synthesis analysis of the two luminosity distributions reveals that the Geneva evolutionary models predict too many luminous cool stars, particularly in the SMC. Speciﬁcally, models predict >19 cool supergiants in the SMC with luminosities log(L/L⊙)>5.36, whereas we see only one. • The luminosity distributions of cool supergiants splice together with those of apparently-single Wolf-Rayet stars in each of the MCs, suggesting a changing evolutionary sequence of massive stars with increasing initial mass. • The spectral types of cool supergiants are earlier in the SMC than in the LMC, a well-known result. However, the shift extends beyond that of M to K, with a substantial number of G supergiants in the SMC. This implies that the average temperatures of cool supergiants are hotter at lower metallicities. The authors would like to thank the referee Jacco van Loon for suggestions and comments which helped us improve the paper, Fabian Schneider for enquiring about the upper luminosity of cool supergiants in the Large Magellanic Cloud, and Nathan Smith for useful comments and discussion. This publication makes use of data products from the Wide-ﬁeld Infrared Survey Explorer, which is a joint project of the University of California, Los Angeles, and the Jet Propulsion Laboratory/California Institute of Technology, funded by the National Aeronautics and Space Administration. This publication makes use of data products from the Two Micron All Sky Survey, which is a joint project of the University of Massachusetts and the Infrared Processing and Analysis Center/California Institute of Technology, funded by the National Aeronautics and Space Administration and the National Science Foundation. This work made use of the IDL astronomy library, available at https://idlastro.gsfc.nasa.gov, and the Coyote IDL graphics library.
Physical Parameters of Asteroids Estimated from the WISE 3 Band Data and NEOWISE Post-Cryogenic Survey<|sep|>The NEOWISE Post-Cryogenic Survey has resulted in the collection of 3.4 and 4.6 µm observations for nearly 100 NEOs to date. Comparison with well-known calibrator objects shows that for objects with low amplitude lightcurves, good signal-to-noise measurements, and thermally-dominated ﬂuxes in band W2, diameters can be computed to within ±20% and albedos to within ±40% of their values. While the accuracy of the diameters and albedos computed from bands W1 and W2 only is lower when compared to the fully cryogenic mission data, it is nevertheless possible to learn useful physical information about these objects. We have computed preliminary thermal models for the NEOs detected during the 3-Band Cryogenic and Post-Cryogenic Survey phases, including a number of objects that are co-orbital with the Earth and Mars. Future work will include extrapolation of these detections to place constraints on the populations of these objects as well as strength and eﬀect of non-gravitational forces on their orbital evolution.
Projection-based reduced order models for a cut finite element method in parametrized domains<|sep|>In this work a POD-Galerkin ROM based on CutFEM high ﬁdelity simulations was presented for linear PDE problems (elliptic and Stokes), characterized by a geometrical parametrization with (possibly) large variations. A CutFEM discretization naturally allows to use a level set description of the parametrized geometry. In our opinion, this results in a simpler and more versatile high ﬁdelity method when compared to a FE formulation with pull back to a reference domain.
Accelerating PageRank using Partition-Centric Processing<|sep|>In this paper, we formulated a Partition-Centric Processing Methodology (PCPM) that perceives a graph as a set of links between nodes and partitions instead of nodes and their individual neighbors. We presented several features of this abstraction and developed system level optimizations to exploit them. We developed a novel PNG data layout for efﬁcient processing using PCPM. The idea for this layout originated when we were trying to relax the Vertex-centric programming constraint that all outgoing edges of a node should be traversed consecutively. The additional freedom arising from treating edges individually allowed us to group edges in a way that eliminates random memory accesses to sustain > 75% of peak memory bandwidth. We conducted extensive analytical and experimental evaluation of our approach. Using a simple index based partitioning, we observed an average 2.7× speedup in execution time and 1.7× reduction in DRAM communication volume over state-of-the-art. In the future, we will explore edge partitioning models [21, 8] to further reduce communication and improve load balancing for PCPM. Although we demonstrate the advantages of PCPM on PageRank, we show that it can be easily extended to generic SpMV computation. We believe that PCPM can be an efﬁcient programming model for other graph algorithms or graph analytics frameworks. In this context, there are many promising directions for further exploration. For instance, the streaming memory access patterns of PNG enabled PCPM are highly suitable for High Bandwidth Memory (HBM) and disk-based systems. Exploring PCPM as a programming model for heterogenous memory or processor architectures is an interesting avenue for future work. PCPM accesses nodes from only one graph partition at a time. Hence, G-Store’s smallest number of bits representation [18] can be used to reduce the memory footprint and DRAM communication even further. Devising novel methods for enhanced compression can also make PCPM amenable to be used for large-scale graph processing on commodity PCs. Acknowledgements: This material is based on work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract Number FA8750-17-C-0086, National Science Foundation (NSF) under Contract Numbers CNS-1643351 and ACI-1339756 and Air Force Research Laboratory under Grant Number FA8750-15 1-0185. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of DARPA, NSF or AFRL. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.
A two-stage formalism for common-envelope phases of massive stars<|sep|>We propose a new formalism to predict the outcomes of CE phases with massive star donors. In our new formalism, we split the whole CE process into two stages: (I) dynamical inspiral through the outer convective part of the envelope, and (II) thermal timescale mass transfer of the inner radiative part of the envelope. We treat stage I as an adiabatic process and apply the commonly used α-formalism (or energy formalism). Stage II is treated as a stable mass transfer, focusing on the angular momentum evolution of the system. The combined outcome gives the ﬁnal post-CE separation. We demonstrated the predictions for the ﬁnal separations with our formalism with some ﬁducial choices of parameters (αCE = 1 for stage I and isotropic reemission for stage II). Overall, the predicted post-CE orbital separations seem to be rather wide compared to what would be predicted from classical energy formalisms. There is a qualitative diﬀerence depending on whether the companion mass is larger or smaller than the donor core mass. When the companion is lighter than the core, the orbit can shrink substantially down to values similar to those predicted by traditional formalisms. On the other hand when the companion is heavier than the core, the orbit can widen after the initial plunge-in, ending up at large orbital separations. The generally wide post-CE separations can have broad implications. One example is that we may predict a high fraction of stripped-envelope supernova progenitors to have low-mass companions, which previously would have been considered unable to survive CEs. In a supernova called SN2006jc, we estimated from the current companion photometry that the progenitor system experienced a CE phase with a Md ∼ 12 M⊙ donor and M2 ∼ 3–4 M⊙ companion, ending up with a a ∼ 40 R⊙ separation (Sun et al. 2020; Ogata et al. 2021). Our new formalism predicts a ∼ 40–100 R⊙ for M2 = 3 M⊙, which is roughly in agreement with this observational constraint. Another example may be iPTF13bvn (Cao et al. 2013). Based on its classiﬁcation as a type Ib su pernova and on the progenitor radius inferred from presupernova photometry, we argued that it should have experienced a CE phase with αCE ≳ 20 (Hirai 2017). Such high eﬃciencies are easily achievable in our new formalism, implying that iPTF13bvn may have a lowmass companion star. Many other stripped-envelope supernova progenitors may also have dim low-mass companions, which may explain the lack of companion detections. The increase in the post-CE separation predicted by our two-stage formalism may explain how binaries with relatively extreme mass ratios may survive mass transfer, addressing a puzzle in the formation of low-mass X-ray binaries (Podsiadlowski & Rappaport 2000; Podsiadlowski et al. 2002). The wide separations predicted for massive companions also indicate that CEs are unlikely to contribute signiﬁcantly to the formation of binary black holes as gravitational-wave sources, since extreme mass ratios are unlikely when transferring mass onto a black hole formed from the primary and 30M⊙ black hole binaries with separations above ∼ 50R⊙ will not merge in the age of Universe. On the other hand, our formalism can reproduce very tight Galactic double neutron star systems observed as radio pulsars (e.g. Hulse & Taylor 1975), provided a CE stage initiated by an early Hertzsprung-gap donor is survivable. One of the main features of our formalism is that the CE eﬃciency has a strong dependence on the system parameters such as the evolutionary stage of the donor and the companion mass. When translated to the traditional α-formalism, our ﬁnal separations correspond to an extremely wide range of equivalent αCE values from ∼ 0.1 to > 104. This should have a signiﬁcant impact on the results of population synthesis studies, which currently typically employ a universal value of αCE. Frequently, the uncertainties are explored by varying the value of the universal αCE, but that cannot reproduce the qualitative sensitivity to the donor’s evolutionary stage and companion mass. We leave it for future work to explore how the predictions of the populations of various objects such as binary black hole and binary neutron star mergers, X-ray binaries, double pulsars and strippedenvelope supernovae will be impacted with our new formalism. We stress that we are not trying to establish a complete model, but rather a physically-motivated framework as a basis for further research. Many of the assumptions going into this formalism should be examined in future work. Some other intricacies such as preCE evolution, onset of CE phases, envelope fallback, etc have been ignored to maintain simplicity for our formalism. However, we believe that our model captures the key features that distinguish massive star donors from low-mass donors, and provides a useful guideline for how to treat CEs in population synthesis studies. The authors thank Orsola De Marco, Mike Lau, and Daniel Price for discussions, and Lewis Picker for comments on the manuscript. This work was performed on the OzSTAR national facility at Swinburne University of Technology. The OzSTAR program receives funding in part from the Astronomy National Collaborative Research Infrastructure Strategy (NCRIS) allocation provided by the Australian Government. IM is a recipient of the Australian Research Council Future Fellowship FT190100574.
Multiclass Common Spatial Pattern for EEG based Brain Computer Interface with Adaptive Learning Classifier<|sep|>In this paper we have presented common spatial pattern algorithm for multiclass EEG classiﬁcation with a pre-processing step which can improve the generalization of CSP covariance matrices by removing the trials which are noisy/aﬀected with artefact. SRIT2NFIS is used as a classiﬁer to handle the non stationarity present in the signal. The results were presented on publically available data set BCI competition IV data 2a. Results are compared with the currently state of the art algorithms for multiclass classiﬁcation. It clearly shows that proposed method outperforms the in four class classiﬁcation by improving the mean accuracy 8-13%. The increase is signiﬁcant in subjects which earlier had very low accuracies. Authors would like to thank Prof. N. Sundararajan, Prof. S. Suresh and Mr. Ankit Das for their immense support in this research work.
"I have no idea what they're trying to accomplish:" Enthusiastic and Casual Signal Users' Understanding of Signal PINs<|sep|>We conducted an online study (n = 235) of Signal users recruited from Reddit, Signal Community Forum, snowballing, and Proliﬁc about their understanding and choice of Signal PINs. In total, 86 % of participants set a PIN, with 57 % able to technically describe what Signal PINs are used for (enthusiasts) and 43 % unable to accurately describe how Signal PINs are used (casuals). We also ﬁnd that PIN composition followed similar lines: enthusiasts use signiﬁcantly longer PINs with more complex compositions, and casual participants used more traditional, numeric PINs despite the fact that Signal allows PINs to be alphanumeric. This suggests that communication about the Signal PIN has been effective for part of the Signal population only and that new strategies will be needed to reach the remainder. As an example of in-app authentication — an authentication mechanism that occurs within a mobile app setting — our investigation shows that in the case of Signal, in-app usage of PINs can be confusing for users who have grown accustomed to screen lock and website login. These authentication metaphors are used often enough that users can be reasonably expected to handle them without much explanation. Where some authentication machinery (a PIN, for example) is repurposed for symmetric-key derivation, only enthusiasts can be expected to read the blogs, documents, tweets, and online help text to gain a full understanding. Thus, we conclude that communication needs to meet the understanding of the (possibly multiple) user communities. Outside of a core constituency, even something as simple as the name matters. Signal’s choice of the term “PIN” can be seen as correct and well-understood by the developers and enthusiasts. However, Signal may be well served in renaming their PIN, e.g., to “Account Recovery Password,” and other uses of in-app authentication will need to carefully choose names and messaging to match user expectations. Though our study does not measure the effect of this intervention, we believe there is strong evidence that suggests renaming Signal PIN to better reﬂect its usage could be helpful. First, a number of participants described it as an authentication mechanism or message privacy mechanism or simply indicated they do not know. A more precise name, like “Account Recovery,” would help users place the Signal PIN in context with other credentials they manage. Second, reusing the term “PIN” suggests to users that only digits are valid. Using the word “Password” or “Passcode” could elicit broader classes beyond digits and encourage more diverse composition. This material is based upon work supported by the National Science Foundation under Grant No. 184530. Further support was received through the research training group “Human Centered Systems Security” sponsored by the state of North Rhine-Westphalia, Germany, and the German Research Foundation (DFG) within the framework of the Excellence Strategy of the Federal Government and the States – EXC 2092 CASA – 390781972.
Open quantum systems with delayed coherent feedback<|sep|>We have derived a generalisation of a technique, derived previously by one of us, for simulating feedback in open quantum systems. Our derivation uses only elementary methods and is based on decomposing the time evolution of a general open quantum system into intervals represented as separate system copies—that is to say, this decomposition is not limited to systems exhibiting delayed coherent feedback. The resulting simulation method admits multiple subsystems with multiple delays in cases where those delays are commensurable. We used our generalised method to simulate systems with multiple delays, including cascaded systems with delayed backscatter. In addition, we presented a generalisation of the quantum regression formula that applies to systems with delayed feedback, and demonstrated how to use this formula to compute two-time correlation functions of the system and output ﬁeld properties. Finally, we showed that delayed coherent feedback can be simulated through either an exotic quantum teleportation protocol requiring time travel, or through a probabilistic teleportation protocol. We conclude with some general remarks on the relation between the techniques presented above and non-Markovian open quantum systems in general. Consider, for example, a single open quantum system that interacts with a feedback reservoir with a generic memory kernel f(t). This memory kernel may be approximated by requiring that integrals over it become left Riemann sums: for some chosen h. Note that Eq. (59) takes the same form as Eq. (35) provided f(t) is Hermitian. The error in this approximation is O(h2), and the exact memory kernel is of course recovered in the limit h → 0. As such, provided we have the computational resources to consider suﬃciently small h, any memory kernel – even a continuous one – may be approximated as a series of discrete delayed feedback loops. Because of this, continuous coherent feedback can be viewed as an inﬁnite chain of cascaded system copies, subject once again to the inter-system mapping formula (2). The method owes its conceptual generality to the ability of this mapping rule to insert (or teleport, as shown in Sec. 6) a history into the system’s evolution after the fact. We would like to thank the anonymous referee for pointing out an error in an earlier version of the manuscript. This work was supported by the Marsden Fund of the Royal Society of New Zealand. We brieﬂy discuss here the reason why Eq. (26) is true whenever Φ(m) is divisible, for all m. We can deﬁne an auxiliary map Φt′ that satisﬁes A sum of divisible maps, with positive real coeﬃcients, is also divisible [17, 28–30]. Because the indicator functions 1··· take the non-negative values 0 or 1, if both Φ(n) and Φ(n−1) are divisible – as we have assumed – then Φt′ must be divisible as well. As such, we may write Φt′(ξ, t′)Φt′(t′, 0)χ = Φt′(ξ, 0)χ = trE{ ˜Ut′(ξ, t′) ˜Ut′(t′, 0)[ρ ˜E ⊗ χ] ˜U † t′(t′, 0) ˜U † t′(ξ, t′)} . Our aim is to show that the environment operator ˜Bα;m(t′) appearing in Eq. (22) may be chosen such that Eq. (32) agrees with Eq. (21). The unitary (17) that appears in Eq. (21) may be re-written as eS;j1···jn(ξ; t′) = trE � ˜U ′(t′ + (n − 1)ξ, 0) [ρ ˜E ⊗ eS;j1···jn] ˜U ′†(t′ + (n − 1)ξ, 0) � , (B.3) Observe that the unitaries (B.1) and (B.4) diﬀer only in terms of the environment operators (B.2) and (B.5). If we restrict attention to vacuum reservoirs (as we do in Sec. 2.3), the right-hand sides of Eqs. (21) and (B.3) are equal if the two interactions have equivalent dissipation kernels. To be more speciﬁc, we need
The nature of the SDSS galaxies in various classes based on morphology, colour and spectral features - II. Multi-wavelength properties<|sep|>The multi-wavelength properties of galaxies in the ﬁne classes provide evidence supporting the main results in Paper I, and add several new ﬁndings on them. Since most main results are digested in the Discussion section (§10), we simply list the important ﬁndings in this paper here. (1) From the UV – optical – NIR colours, non-passive REGs seem to have larger metallicity and younger age than pREGs. This implies that non-passive REGs may have suffered recent SF events, which have produced young and metal-rich stellar populations in them. (2) REGs show conspicuous dependence of their radio luminosity on their optical absolute magnitude, which indicates that the major radio source in REGs is radio-loud AGNs. Some radio-loud REGs are optically passive, corresponding to previously known radio-loud AGNs without emission line. (3) hBEGs have the most negative slopes among all classes in the optical – NIR CMR. Both age and metallicity may affect the rapid CMR slope of hBEGs, but the eﬀect of metallicity seems to be larger than that of age. (4) The UV – optical – NIR colours of BEGs are well explained using the SSP + recent SF model, if it is supposed that there is no signiﬁcant diﬀerence in their formation epoch between diﬀerent spectral classes. (5) The IRAS detection fractions of pBEGs and lBEGs are small, compared to those of BLGs, but the IRAS detection fractions of hBEGs and sBEGs are as large as those of BLGs. This shows that hBEGs and sBEGs have very young or currently-forming stars, probably accompanied by rich dust reservoirs. (6) RLGs have intermediate star formation between REGs and blue galaxies. The UV – optical – NIR colours indicate that RLGs have not only a considerable amount of old stars but also more young stars than REGs. (7) The UV – optical colours and the radio detection trend of pRLGs show that pRLGs have properties similar to REGs rather than non-passive RLGs. (9) Not only the CMR slope variation between the optical and NIR bands but also the tight and linear UV CMR of hBLGs shows that faint BLGs are younger than bright BLGs. From this, the previously known mass – age relation (or galaxy downsizing) is conﬁrmed using multi-wavelength data. (10) The UV – optical – NIR colours indicate that the mean stellar ages of pBLGs are quite young. Therefore, if pBLGs are intrinsically passive, their SF activity may have stopped recently. Otherwise, there may be current SF in the outskirts of pBLGs. (11) The SFR[OII] of Seyferts is always larger than that of LINERs in a given morphology-colour class. Considering the AGN feedback, it is a possible scenario that LINER galaxies may be at the end of the AGN feedback process, the SF of which may have been already suﬃciently suppressed by AGN activity at the Seyfert phase. (12) Among AGN host galaxies, some ﬁne classes sometimes show diﬀerent properties, such as their IRAS colours or UV – optical CMRs, even though they belong to the same spectral class. The physical origin of such diﬀerences is an open question. This paper is the second in the series of comprehensive studies on the nature of the SDSS galaxies in ﬁnely-divided classes. In the following paper (Lee et al. 2009), we will inspect the environments of the SDSS galaxies divided into the ﬁne classes. The authors thank the anonymous referee for very useful comments that improved signiﬁcantly the original manuscript. JHL appreciates the support and advice of Dr. Eon-Chang Sung. This work was supported in part by a grant (R01-2007-000-20336-0) from the Basic Research Program of the Korea Science and Engineering Foundation (KOSEF). CBP and YYC acknowledge the support of the KOSEF through the Astrophysical Research Centre for the Structure and Evolution of the Cosmos (ARCSEC). Funding for the SDSS and SDSS-II has been provided by the Alfred P. Sloan Foundation, the Participating Institutions, the National Science Foundation, the US Department of Energy, the National Aeronautics and Space Administration, the Japanese Monbukagakusho, the Max Planck Society, and the Higher Education Funding Council for England. The SDSS Web site is http://www.sdss.org/. The SDSS is managed by the Astrophysical Research Consortium for the Participating Institutions. The Participating Institutions are the American Museum of Natural History, Astrophysical Institute Potsdam, the University of Basel, the University of Cambridge, Case Western Reserve University, the University of Chicago, Drexel University, Fermilab, the Institute for Advanced Study, the Japan Participation Group, Johns Hopkins University, the Joint Institute for Nuclear Astrophysics, the Kavli Institute for Particle Astrophysics and Cosmology, the Korean Scientist Group, the Chinese Acadeour of Sciences (LAMOST), Los Alamos National Laboratory, the Max-Planck-Institute for Astronomy (MPIA), the Max Planck Institute for Astrophysics (MPA), New Mexico State University, Ohio State University, the University of Pittsburgh, the University of Portsmouth, Princeton University, the US Naval Observatory, and the University of Washington. The FIRST Survey is supported in part under the auspices of the Department of Energy by Lawrence Livermore National Laboratory under contract no. W-7405-ENG-48 and the Institute for Geophysics and Planetary Physics. This publication makes use of data products from the 2MASS, which is a joint project of the University of Massachusetts and the Infrared Processing and Analysis Centre/California Institute of Technology, funded by the National Aeronautics and Space Administration and the National Science Foundation. The galaxy Evolution Explorer (GALEX) is a NASA Small Explorer. The mission was developed in cooperation with the Centre National d’Etudes Spatiales of France and the Korean Ministry of Science and Technology.
A $Gaia$ Data Release 2 catalogue of white dwarfs and a comparison with SDSS<|sep|>We retrieved the available Gaia DR2 data for ≃ 24 000 spectroscopically conﬁrmed white dwarfs from SDSS, and analysed the properties and distribution of these objects in the Hertzsprung-Russell diagram to deﬁne a reliable method to select high-conﬁdence white dwarf candidates from Gaia DR2. After deﬁning several quality cuts to remove objects with poor Gaia measurements, we ﬁnd that no simple selection relying solely on Gaia colour and absolute magnitude can separate white dwarfs from contaminant objects without excluding a signiﬁcant number of known white dwarfs. We therefore make use of the distribution in GBP − GRP colour and Gabs of a sample of spectroscopically conﬁrmed white dwarfs and contaminants from SDSS to calculate probabilities of being a white dwarf (PWD) for all Gaia objects in our sample. This results in a total of 486 641 objects with calculated PWD from which it is possible to select a sample of ≃ 260 000 high-conﬁdence white dwarf candidates. The PWD values, coupled with Gaia quality ﬂags, can be used to ﬂexibly select samples of white dwarfs with varying degree of completeness and contamination according to one’s speciﬁc goals. For general purpose we recommend a cut at PWD > 0.75, which we estimate includes 95 per cent of all the white dwarfs in the total sample, with minimal level of contamination (≃ 4 per cent). We also provide stellar parameters (Teﬀ, log g and mass) for a subsample of 225 370 candidates that have Gaia parallax and photometric measurements precise enough to achieve a reliable ﬁt to our adopted
A search for ultrahigh-energy neutrinos associated with astrophysical sources using the third flight of ANITA<|sep|>We ﬁnd that there is no signiﬁcant evidence for any source-associated neutrinos with ANITAIII, although the potential SN association of the ANITA-III diﬀuse analysis event is somewhat intriguing, especially in combination with previous results. The methodology developed is shown to be capable of achieving higher eﬃciencies at lower backgrounds compared to a diﬀuse search. However, because the analysis eﬃciency of the ANITA-III diﬀuse search was already high (>80%), there is relatively little room for neutrinos that could have triggered ANITA-III but not have passed analysis cuts employed in the diﬀuse search. As such, a null result is not surprising. Still, using this methodology we are able to set limits on individual sources with ANITA, which can not be done coherently in the diﬀuse search. A similar search is in progress for the more recent and more sensitive ANITA-IV payload, although that also had a high diﬀuse analysis eﬃciency. For similar experiments where the achieved diﬀuse analysis eﬃciency at an acceptable background level is not as high, the methodology described here can be more impactful, as there is additional phase space for discovery currently not accessible to diﬀuse searches. For example, a signiﬁcant gap currently exists between diﬀuse analysis and trigger eﬃciency in the Askaryan Radio Array (ARA) experiment [55]. Employing an adaptation of the method described here therefore has the potential to discover new candidates within the ARA dataset Similarly, it follows that it is advantageous for future UHE radio neutrino experiments to reduce trigger thresholds below expected achievable analysis thresholds for diﬀuse searches. A reduced trigger threshold could not be achieved in ANITA-III or ANITA-IV as the acquisition system could not handle the corresponding increased data rate. Future UHE neutrino detectors using the Askaryan method such as PUEO [56], RNO-G [57], or the radio extension of IceCube Gen2 [58] will use improved trigger techniques capable of substantially reducing the achievable trigger thresholds while maintaining lower rates. If the threshold is chosen to be low enough, then the techniques developed here can potentially unveil candidate events not discoverable in a diﬀuse search. The method outlined here suﬀers from an inability to reduce background below some level, due to the use of a sideband for background estimation. Finding additional handles on background could potentially further reduce background and help improve sensitivity. This may be accomplished through improved modeling of anthropogenic backgrounds (easier in the case of ﬁxed-position detectors than balloon payloads) or through the use of sidebands with relatively more phase space (for example, by improving angular reconstruction or introducing additional variables). With the minimal background estimate achievable in this analysis, four events are necessary for > 3σ evidence and eight events would be required for > 5σ discovery, before accounting for trials factors. Doubling the relative size of the sideband, for example, would reduce the number of events required to exceed each signiﬁcance threshold by one. We
A jigsaw puzzle framework for homogenization of high porosity foams<|sep|>We presented a modeling strategy for Alporas R ⃝ foam having at heart the synthesis of stochastic microstructure realizations based on the concept of Wang tiling. Besides the standard upper bounds on the eﬀective stiﬀness given by Kinematic Uniform Boundary Conditions, we obtained the guaranteed lower bounds by means of minimal kinematic boundary conditions. With this formulation, shown to equal the common Static Uniform Boundary Conditions in [29], we can avoid the question of an appropriate beam loading that would yield macroscopically uniform stress ﬁeld. Hence, both bounds were rendered prescribing relevant boundary displacement. Based on data in Fig. 6, we conclude that RVE for the two-dimensional model of highly porous materials should be about hundreds of the characteristic pore diameter length in dimensions. This ﬁnding corresponds with statements made in [17] regarding the minimal size of RVE in the case of inﬁnite contrast of phase properties. On the other hand, it contradicts the recommendation of Ashby et al. [1] who propose RVE size of approximately seven times the mean pore diameter. However, this recommendation is given for three-dimensional samples and does not need to be valid for planar analyses. Comparison of the homogenized stiﬀness coeﬃcients with the reference values reported by Ashby et al. [1], Tabs. 1 and 3, leads us to unambiguous conclusions. Despite the several times reported aptness of the spatial wired model [1, 5, 7], it can be conjectured that the adopted planar beam representation has a limited capability in predicting the complex behavior of Alporas R ⃝ foam. Possibly, it lacks the stiﬀness contribution from the out-of-plane beams and membranes as well as the cell walls parallel to the investigated plane, though, the membrane contribution of cell walls was reported negligible in the case of high-porosity foams [7]. The qualitative analysis of the impact of the geometry representation clearly shows that the Voronoi mesh adopted in [8] leads to the overestimated value of bulk modulus. Assuming only the volumetric deformation, the axial stiﬀness of straight beams dominates the behavior of the Voronoi model resulting in nearly incompressible behavior, whilst in the case of real-geometry and tiling-based meshes the axial and bending stiﬀness contribute equally. This explains the results reported in [8] where the authors considered only the volumetric excitation. On top of that, based on their experimental observations, they assumed
Geometrical approach to SU(2) navigation with Fibonacci anyons<|sep|>Topological Quantum Computation with Fibonacci anyons strongly relies on the possibility of closely approaching any unitary matrix upon braiding the anyons. In this paper, we have shown how to fulﬁll this task for the ”three anyons-one qubit” case, by generalizing the standard geodesic dome covering of the sphere S2 to the ”SU(2) − S3” hyperdome case. The eﬃciency of this construction is due to the close, and yet unexplored, relation between Fibonacci braid generators and the binary icosahedral group generators. As a consequence, iterative ﬁner and ﬁner SU(2) meshes can be generated, in a controlled way, with braid words of limited length. Generalization to many qubits (with more Fibonacci anyons) is not easy. The ﬁrst step would consist in selecting the high order discrete subgroup of SU(N) and try to approach their generating set by braiding the anyons. As usual, one should ﬁrst focus on one and two-qubit gates, since it is known that generic SU(N) can be generated by their suitable concatenation. So the natural next step will be to analyse the ”two-qubits SU(4)” case. One way, presently still under study, is to ﬁrst analyse nice discrete sets of two-qubits related by symmetry, and simply associated to successive shells of the eight-dimensional dense lattice E8 [13]. The ﬁrst shell, with 240 points, corresponds (upon modding out a global phase factor) to 60 two-qubits ”physical” states : 36 product states, and 24 maximally entangled (EPR) states. The product states are easily generated by separately braiding two sets of three braids (one needs only to use elements from the binary tetrahedral group, a Y subgroup). The entangled states will require more subtle braiding operations, such that they keep the system inside the two-qubit Hilbert space. Taking advantage of known properties about E8 shellings [14] (together with the entanglement sensitive S7 Hopf ﬁbration [15]), larger sets of two-qubit states with intermediate entanglement could also be generated. Acknowledgement It is a pleasure to acknowledge discussions with Pedro Ribeiro, Raphael Voituriez and David Bessis. This work (except the alternative method described in paragraph 4) was presented at the Dublin workshop on topological quantum computing in september 2007, where I beneﬁtted from remarks by D. Bonesteels and M. Freedman. with i2 = j2 = k2 = ijk = −1, the latter “Hamilton” relations deﬁning the non-commutative quaternion multiplication rule. The conjugate of a quaternion q is Quaternions can also be deﬁned equivalently, using the complex numbers c1 = x0 + x1i and c2 = x2 + x3i, in the form q = c1 + c2j, or equivalently as an ordered pair of complex numbers satisfying (c1, c2) + (d1, d2) = (c1 + d1, c2 + d2) (A.2) (c1, c2) (d1, d2) = � c1d1 − c2 allows to write M as the unit quaternion M = a + bi + cj + dk, (A.6) with the identiﬁcation A ﬁbred space E is deﬁned by a (many-to-one) map from E to the so-called “base space”, all points of a given ﬁbre F being mapped onto a single base point. A ﬁbration is said ”trivial” if the base B can be embedded in the ﬁbred space E, the latter being faithfully described as the direct product of the base and the ﬁbre (think for instance of ﬁbrations of R3 by parallel lines R and base R2 or by parallel planes R2 and base R). The simplest, and most famous, example of a non trivial ﬁbration is the Hopf ﬁbration [16] of S3 by great circles S1 and base space S2. One standard notation for a ﬁbred space is that of a mapE F→ B, which reads here S3 S1 → S2. Its non trivial character implies S3 ̸= S2 × S1. To describe this ﬁbration in an analytical form, we deﬁne elements of S3 as pairs of complex numbers (α, β) which satisfy |α|2 + |β|2 = 1. The Hopf map is deﬁned as the composition of a map h1 from S3 to R2 (+∞), followed by an inverse stereographic map h2 from R2 to S2 : The ﬁrst map h1 clearly shows that the full S3 great circle, parametrized by (α exp iω, β exp iω), is mapped onto the same single point with complex coordinate C. The Hopf map is therefore a mean to represent SU(2) matrices, either on the complex plane or on the sphere S2, but with identical images for matrices diﬀering only upon multiplication by the matrix � exp iω 0 0 exp −iω Let us ﬁrst recall the {p, q} and {p, q, r} Schl¨aﬄi notations. {p, q} denotes a regular two-dimensional tiling (either spherical, euclidean or hyperbolic), such that each site belongs to q regular p-gones : {4, 3} is a cube, {6, 3} is a honeycomb tiling. {p, q, r} is a regular three-dimensional tiling, such that each edge belongs to r polyhedra of the type {p, q} : {4, 3, 4} is the standard cubic tiling in R3. So, {3, 3, 5} denotes a tiling of regular tetrahedra {3, 3}, with exactly ﬁve such tetrahedra sharing an edge. The regular tetrahedron dihedral angle being slightly less than 2π/5, this leads to a polytope structure on the 3 dimensional curved space S3 where V, E, F, C are (respectively) the number of vertices, edges, faces and cells. With one vertex on the pole, the successive ”horizontal” sections are (i) an icosahedral shell,(ii) a dodecahedral shell, (iii) a new icosahedral shell, (iv) an equatorial icosidodecahedral shell. The next shells then symmetrically reproduce the same pattern down to the S3 south pole. The dual polytope {5, 3, 3} has 600 vertices and 120 dodecahedral cells. The {3, 3, 5} symmetry group plays an important role in the present study. S3 orientation preserving point symmetries form the group SO(4), while the full group is O(4). Symmetry elements are easily written in terms of unit quaternions. For the SO(4) action, a given point on S3, labelled by the quaternion q, is sent to lqr, with l, r ∈ Q (with an additional quotient by Z2, see below). The remaining indirect symmetries in O(4) are such that q is sent to l¯qr. The (properly oriented) {3, 3, 5} 120 vertices (on a unit radius S3) are in one-toone correspondance with the 120 elements of the binary icosahedral group Y . Due to the group structure, multiplying on the left or on the right by Y elements sends the polytope onto itself. Recalling that the group center is just {1, −1}, one ﬁnds as a whole the 7200 elements of the orientation preserving group G′ (discrete subgroup of SO(4)). This order can also be computed directly from the number of fundamental regions; for a regular polytope, this amounts to generate the tetrahedral orthoscheme O associated with the full symmetry group, such that the latter is generated by reﬂections about the orthoscheme faces. One orthoscheme is simply build from a regular cell {p, q} of the {p, q, r}, by selecting a cell vertex V , a middle edge point E (for an edge through the selected vertex), a middle face point F (for a face sharing the cell vertex and the selected edge) and ﬁnally the cell centre C (see Figure D2-left) Polytope {3, 3, 5} has 600 tetrahedral cells. Each cell being decomposed into 24 orthoschemes, one recovers the 14400 fundamental regions and therefore the G group order. If one let the G generators freely act onto a point M in one orthoscheme O, one eventually gets a set P of N regularly spaced points on S3, with N depending on the location of M : • If M is a generic point on a {3, 3, 5} edge, N = 1440 , while N = 720 if M at a mid-edge position Finally, as discussed in the text (and in the next appendix), one also considers sets P which are the image under G of several points M, forming a seed set S Figure D1. Left : geodesic dome based on icosahedral symmetry; center : an icosahedron triangular cell, with a (fundamental region) orthoscheme decorated with three vertices (the so-called seed set S) : a triangular cell vertex V (black circle) , a face center F (white circle), a vertex D at one third on an edge(grey circle); right : triangle cell decoration for obtained as the local images of the three points in the orthoscheme Geodesic domes are triangulations of the sphere S2, usually built with icoshaedral symmetry. There are several diﬀerent families of such discrete sets, the simplest being obtained by a decomposition of an icosahedron triangular faces into smaller triangles. Fig D1-left shows an example with 92 vertices, where edges are scaled by a factor 3 (this factor is only approximate if the dome vertices and edges are centrally mapped onto the sphere S2). The geodesic dome shares the same symmetry group as the original icosahedron. Its vertices can therefore be generated from a seed set S located in one of the group orthoscheme. Figure D1-centre displays such an orthoscheme, inside a triangular face, with S made of three points, a face vertex V , face centre F and a point D at one third along an edge. The seed set is then propagated under the group action, here a reﬂection in the orthoscheme edges, leading to the geodesic dome 92 vertices in the following way : V has 12 images (forming the original icosahedron), F has 20 images (forming the dual dodecahedron), and D has 60 images (forming a ”buckyball” polyhedron). Figure D1-right shows the image of S, propagated inside one triangle of the original icosahedron. The generalization to S3 proceeds along similar lines. Take a {3, 3, 5} tetrahedral cell (Figure D2-left), with one orhoscheme, and the three seed vertices described in paragraph 3-2. And then propagate the seed set under the G group action. Figure D2right shows the image of the propagated seed set, restricted to a {3, 3, 5} tetrahedral cell. Figure D2. Left: a tetrahedral {3, 3, 5} cell, with one fundamental orthoscheme whose four vertices are a cell vertex V , a mid-edge point E, a face center F , and the cell center C. The ﬁgure also shows the decoration of the orthoscheme by the seed set SP 1 , with V (black circle), C (white circle), and a third point located at one third on an edge (grey circle). Right : The cell decoration for P1, obtained as the local images of SP 1
Star Formation and Gas Phase History of the Cosmic Web<|sep|>We have introduced a new approach to studying the environment in which galaxies form and evolve. By coupling tailored simulations with a structure ﬁnding algorithm that self-consistently tracks gas in clusters, ﬁlaments and voids, we have begun to examine the redshift evolution of the properties of the different environments of galaxies and their circumgalactic medium. We have have deﬁned and studied poor clusters, ﬁlaments and voids focusing on 1) the temperature and density evolution 2) the phase population of the gas in these structures, and 3) the star formation rates and efﬁciencies in these structures. We ﬁnd that during the bulk of the cosmic evolution most of the stars and gas inhabit ﬁlamentary structures. At a redshift z = 0 (see Fig. 10), 79.1% of the gas mass is found in ﬁlaments. The gas phase mass fractions are 43.1%, 30.0%, 24.7% and 2.2% for the diffuse, WHIM, hot halo and condensed phases, respectively. Although most of the WHIM is found in ﬁlaments, we caution against equating the ﬁlamentary environment with the WHIM since the ﬁlamentary gas is in fact multiphase, consisting of almost equal parts hot halo, WHIM and diffuse gas. The condensed gas in these ﬁlaments dominates the star forming regions in the universe through all epochs. Our deﬁnition of structure allows us to probe the inner and outer regions of clusters and ﬁlaments. At high redshifts, the ﬁlaments are have low structure measure values, corresponding to a relatively low contrast with the background. At redshift z = 0, the ﬁlamentary material has transitioned into higher contrast regions as we would expect from the growth of cosmic structure. At redshift z = 0, 75.4% of the star mass is found in the ﬁlamentary neighborhood. The unassigned material does house signiﬁcant star formation at higher redshifts when we use a structure threshold = 0.1. However, since this star formation is reassigned to ﬁlamentary regions when we relax this criterion, we suspect that this star formation is occurring in low contrast regions, or the outskirts, of the ﬁlaments. The star mass in clusters and voids, is correspondingly less affected by the change in structure threshold, leading us to conclude that the unassigned material is indeed in the low contrast regions in ﬁlaments. There is some indication of star formation occurring in voids, but at low redshifts the gas vacates the voids and ﬂow into ﬁlaments and clusters. This leads to a peak of the total stellar mass in voids at a redshift z = 4, which then decreases at lower redshifts. Our ability to map the temperature-density evolution of the ﬁlaments, clusters, and voids allows us to study the different phases of gas in these structures. We have used this to track the star formation in different phases of the gas. When star formation peaks at a redshift z = 3.2, 8.6% of the stars form in the diffuse phase, 0.36% form in the WHIM phase, 5.2% form in the hot halo phase, and 86% form in the condensed phase (see Fig. 12a). At that redshift 65% of the gas is in ﬁlaments and it is broken down into 53%, 2.9%, 1.8% and 7.4% of the total gas is in the diffuse, WHIM, hot halo and condensed phases. Over half of all star formation therefore occurs in 7.4% of the gas. At a redshift z = 0, 0.16% of all the stars form in the diffuse phase, 0.16% form in the WHIM phase, 30.7% form in the hot halo phase and 68.9% form in the condensed phase (see Fig. 13a). Figs. 12a and 13a demonstrate a transition from a trimodal gas phase (diffuse, hot halo and condensed) distribution for star formation at high redshift to a bimodal gas phase star formation distribution in the present era (hot halo and condensed). The transition from the hot halo and diffuse phase containing similar amounts of star formation at z = 3 to the hot halo having over 150 times the star formation rate at z = 0 cannot be completely attributed the decrease in the diffuse phase. At z = 0, the diffuse phase still has more gas than the hot halo phase (see Fig. 11a). This indicates that hot halos are simply more efﬁcient at converting gas into stars (see Fig. 15a). We also notice in Fig. 15 that the poor clusters and groups are overall the most efﬁcient at converting gas into stars. The higher densities (and higher metallicities) enable gas to cool and form stars. The condensed phase plays the most signiﬁcant role in converting structure gas into stars followed by the hot halo at low redshift and the diffuse phase at high redshifts. However, in the inner regions of ﬁlaments that correspond to a high ﬁlament measure, star formation efﬁciency is comparable to that in poor clusters. We have shown that ﬁlaments play a signiﬁcant role in the history of star formation. The majority of star formation occurs within cold, condensed gas in ﬁlaments at intermediate redshifts (z ∼ 3). We also show that much of the star formation above a redshift z = 3 occurs in low contrast regions of ﬁlaments, but as the density contrast increases at lower redshift the star formation switches to high contrast regions, or the inner parts of ﬁlaments. Since ﬁlaments bridge the void regions to the cluster regions, it suggests that the majority of star formation occurs in galaxies at intermediate redshifts in ﬁlamentary regions prior to the accretion onto clusters.
Learn to Forget: Machine Unlearning via Neuron Masking<|sep|>In this paper, we emphatically discussed the machine unlearning problem, and inspired by the “active forgetting” mechanism of the human nervous system, proposed our “learn to forget” method, Forsaken, to achieve machine unlearning for neural networks. In the method, the user could stimulate the neurons of a machine learning model to unlearn speciﬁc memorization by training a dummy gradient generator. In particular, the mask gradient could be treated in the same way as the common procedure of machine learning, which was more practical and efﬁcient than the prior works. For better evaluating the performance of machine unlearning, we also presented the ﬁrst indicator, called forgetting rate, to measure the transformation rate of the eliminated data from “memorized” to “unknown”. Reconsidering the design of Forsaken or other machine unlearning methods, we failed to provide provable guarantee for hiding the indirect footprint of unlearned data points. With our work as the stepping stone, we hoped more future works could further dive into the machine unlearning ﬁeld and provide more extensive options to protect both private user data security and data unlearning privacy.
Mean-square convergence of a semi-discrete scheme for stochastic nonlinear Maxwell equations<|sep|>In this paper, we consider a semi-implicit discretization in temporal direction for stochastic nonlinear Maxwell equations. First, we establish the regularity properties of the continuous and discrete problems. Then based on these regularity properties and utilizing the energy estimate technique, the mean-square convergence order 1/2 is derived. Future work will include the study for the full discretization of the stochastic Maxwell equations, in which the error estimates in spatial direction depend on the enough smoothness of the noise covariance and the initial data. Besides, due to the high dimensions and stochasticity of stochastic Maxwell equations, the computational implement is an important and technical issue. In order to approximate this problem efﬁciently and effectively, some techniques such as splitting approach may be employed, and thus the analysis of the effect on the convergence order induced by these techniques also constitutes future work.
Efficient Bayesian phase estimation using mixed priors<|sep|>In this work we have analyzed the performance of Bayesian phase estimation for diﬀerent representations of the prior distributions. As a ﬁrst representation we use a normal distribution, which was used earlier in [19]. We show that updates to the distributions can be evaluated analytically in noiseless settings, as well as in settings with several types of commonly encountered noise. The normal-based approach is fast, but performs poorly when multiple eigenphases are present, certainly in experiments with only a single measurement round. In these cases the distributions tend to collapse into a single distribution, which is then impossible to untangle since the updates will essentially be identical. As a second representation, we consider the truncated Fourier series representation used in [13]. For noise Table 2: Number of successful phase estimates for 100 problem instances with n phases, obtained using Bayesian phase estimation using the given number of phase parameters and post processing with bundle size τ (in degrees). less problems, as well as for problems with many diﬀerent types of noise, the Fourier series is ideal in that it captures exactly the desired distributions. However, the number of coeﬃcients required to maintain exact representations keeps growing with each additional round of measurement. In order for the method to remain computationally attractive, it is therefore necessary to truncate the Fourier series. We show that this truncation eventually causes the distribution to become unstable, thereby limiting the accuracy that can be achieved using a ﬁxed number of terms. To combine the advantages of both approaches, we propose a mixed approach in which the distributions are initially represented as truncated Fourier series. When successive updates cause the standard deviation of a distribution to fall below a suitably chosen threshold, we change the representation to a normal distribution. We show that the proposed mixed approach performs well with both static and adaptively chosen values of k, and that the performance remains stable when decoherence or read-out errors are present. Finally, we show how the Bayesian approach can be combined with the quantum Fourier transformation, which is traditionally used for phase estimation. Measurement errors can be dealt with successfully in this setting but the current method requires the evaluation of the probability distributions for all possible states. Reduction of this complexity is an interesting topic for future work. In terms of scalability of the method we conclude that Bayesian phase estimation is especially well suited for problems where the state is a superposition of a small number of dominant eigenstates and possibly many spurious eigenstates with a much smaller weight. The author would like to thank Antonio C´orcoles and Maika Takita for useful discussions, and the referees for feedback that helped improve the paper.
The new semianalytic code GalICS 2.0 - Reproducing the galaxy stellar mass function and the Tully-Fisher relation simultaneously<|sep|>GalICS 2.0 is a new semianalytic code that we run on merger trees from an N-body simulation in a Planck cosmology. The simulation is used to follow the evolution of DM haloes in mass, position, angular momentum and concentration. In the version used for this article, GalICS 2.0 assumes a one-to-one correspondence between galaxies and haloes/subhaloes. A beta version that includes the possibility of a delay between halo and galaxy mergers shows no diﬀerence with respect to our conclusions. The masses of luminous galaxies within haloes are determined by: i) the rate at which gas ﬂows to the centre and accretes onto galaxies, ii) the rate at which the accreted gas is converted into stars, and iii) the rate at which gas is blown out of galaxies. In GalICS 2.0, galaxies grow through accretion of cold ﬂows (Kereˇs et al. 2005; Dekel & Birnboim 2006;
Towards Ultra-Reliable Low-Latency Communications: Typical Scenarios, Possible Solutions, and Open Issues<|sep|>In this paper, we elaborated the delay components and packet loss probabilities in three typical communication scenarios for URLLC. Then, We summarize possible solutions and techniques in the physical layer, the link layer, and the network architecture design aspects for URLLC. The solutions from each of these three layers are important for enabling URLLC. However, without cross-layer optimization, the separated optimization in the three aspects cannot obtain the global optimal solution, and may lead to incorrect conclusions. Motivated by this fact, we presented some optimization results in cross-layer resource management. Finally, we outlined the basic idea in prediction and communication co-design for wide area large scale networks and discussed some open issues. This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 61701317, Young Elite Scientists Sponsorship Program by CAST under Grant 2018QNRC001, Guangdong Natural Science Foundation under Grant 2017A030310371, Shenzhen Basic Research Program under JCYJ20170302150006125, The Start-up Fund of Shenzhen University under 2017076, Tencent Rhinoceros Birds - Scientiﬁc Research Foundation for Young Teachers of Shenzhen University, The Start-up Fund of Peacock Project, the SUTD-ZJU Research Collaboration under Grant SUTDZJU/RES/01/2016 and SUTD-ZJU/RES/05/2016, ARC under Grant DP150104019 and DP190101988, and the University of Sydney.
Alterations And Rearrangements Of A Non-Autonomous Dynamical System<|sep|>In this work, we investigated the dynamics arising from various possible alterations and rearrangements arising from a given nonautonomous dynamical system (X, F). We prove that if (X, G) is obtained by inserting/deleting ﬁnitely many maps from the family F, under certain conditions, the modiﬁed system exhibits behavior similar to (X, F) and hence the dynamics is preserved under such modiﬁcations. We prove that while minimality and equicontinuity are preserved unconditionally, proximality is preserved when the family F is commutative and injective. We prove that various notions of mixing and sensitivities are equivalent for the two systems when the family F is feeble open. We prove that the results established do not hold good when the conditions imposed are relaxed and hence the conditions imposed are indeed necessary for the results to hold good. We generalize our results to the case when the family G is a ﬁnite rearrangement of F. We prove that the results obtained hold good strictly for ﬁnite rearrangements and fail to hold true when the rearrangement is inﬁnite.
Green-aware Mobile Edge Computing for IoT: Challenges, Solutions and Future Directions<|sep|>In this work, we present a discussion on green-aware mobile edge computing for  IoT. Specially, we discuss the related challenges about how to apply MEC for IoT  to achieve the energy efficiency objective. Moreover, we propose a general  framework including the necessary entities to support the green-aware resource  scheduling in the MEC scenario. Thereafter, we present a green-aware model for  offloading tasks from IoT devices to edge servers to achieve the efficient  management of energy and latency. Then we investigate several state-of-the-art  approaches in the related area and compare them from comprehensive perspectives.  Finally, we provide a set of future research directions, where we hope to attract  researchers' attention to establish more validated research in the green-aware MEC  enabled IoT area, e.g. collaborating the MEC with MCC together to take advantage  of the heterogeneity of them for task offloading.    Acknowledgements: This work is supported by Key-Area Research and  Development Program of Guangdong Province (NO. 2020B010164003), and SIAT  Innovation Program for Excellent Young Researchers.
The Fun is Finite: Douglas-Rachford and Sudoku Puzzle -- Finite Termination and Local Linear Convergence<|sep|>In this paper, we studied local convergence properties of Douglas–Rachford splitting method when applied to solve non-convex feasibility problems. Under a proper non-degeneracy condition, both ﬁnite convergence and local linear convergence are proved for the standard Douglas–Rachford splitting and a damped version of the method. Understanding when the methods fail, especially for the damped Douglas–Rachford splitting, require further study on the property of the methods. Acknowledgement We would like to thank Guoyin Li for helpful discussions on the convergence of Douglas–Rachford splitting for non-convex optimization. J.L. was partly supported by Leverhulme trust, Newton trust and the EPSRC centre “EP/N014588/1”. R.T. acknowledges funding from EPSRC Grant
Radiative heat transfer in nonlinear Kerr media<|sep|>We conclude by proposing a practical system where above mentioned effects can potentially be observed. In order to reach the strongly nonliner regime, it is desirable to have |ζ| = |α|kBT γe γ2 ∼ 1. Given a choice of operating temperature, the goal is therefore to design a cavity with a large Purcell factor α/γ ∼ Q/V . If the goal is to observe large enhancements from TPA, it is also desirable to operate with materials and wavelengths where the nonlinear FOM n2 λβTPA ≲ 1, corresponding to large TPA.10 All of these conditions can be achieved in a number of material systems and geometries. For illustration, we consider the Ge nanobeam cavity shown on the inset of Fig. 2 and based on the family of nanobeam cavities described in Ref. 31, which supports a mode at λ = 2.09µm. At this wavelength, Ge has an index of refraction n ≈ 4 and Kerr coefﬁcient χ(3) ≈ (1.2 − 11i) × 10−17(m/V)210,48, corresponding to a FOM ≈ 0.008. This yields a mode with α ≈ 0.001(χ(3)/ε0λ3), Q ≈ 108, and modal volume V ≈ 0.8(λ/n)3, leading to |ζ| ≈ 1 for operation at T = 1000K. (Large Purcell factors such as these were recently predicted in a similar, albeit silicon platform31.) We note that there are other possible cavity designs, wavelength and material choices, including GaP and ZnSe, and that it is also possible to operate with larger bandwidths at the expense of larger temperatures and/or smaller mode volumes. Because these thermal effects scale linearly with Purcell factor, we believe that nanophotonic cavities with ultra-small modal volumes and bandwidths are the most promising candidates for experimental realization. This is in contrast to the situation encountered in traditional nonlinear devices involving incident (non-thermal) light, where the threshold power for observing strong nonlinear effects ∼ V/Q2 and therefore favors designs that sacriﬁce modal volume in favor of smaller bandwidths.12 Finally, we note that the predictions above offer only a glimpse of the potentially interesting radiative phenomena that can arise in passive nonlinear media at and out of equilibrium. In future work, it may be interesting to consider the impact of other nonlinear phenomena on thermal radiation, including free-carrier absorption and third harmonic generation, as well as applications of the Kerr effect to thermal rectiﬁcation46,47. In a related context of optomechanics, the coupling of photonic and mechanical resonances leads to novel nonlinear effects that are also manifested in the radiation spectrum of photonic cavities, often studied in the presence of incident, non-thermal radiation pressure.18,49 We believe that electronic nonlinearities such as the Kerr effect in semiconductors offer an alternative approach to exploring similar ideas involving nonlinear ﬂuctuations. We are grateful to Mark Dykman for very helpful comments and suggestions. This work was supported in part by the Army Research Ofﬁce through the Institute for Soldier Nanotechnologies under Contract No. W911NF-13-D0001, and by the National Science Foundation under Grant No. DMR-145483. In this appendix, we review the procedure for deriving the FP equation [Eq. (6)] from the corresponding nonlinear Langevin equation [Eq. (2)], which can be written in the following simpliﬁed form: where D is the diffusion coefﬁcient, and ξ and s+ are deltacorrelated white-noise sources obtained by taking the derivative of standard Wiener processes40, ξ = ˙ Wξ and s+ = ˙ Ws. For a ﬁnite discretization time ∆t, the coupled-mode equations can be written as follows37: where the choice of 0 ≤ λ ≤ 1 determines the corresponding Stochastic interpretation rule. Taylor expanding each term and deﬁning ∆a ≡ a(t)− a(t− ∆t) and ∆Wi = Wi(t)− Wi(t− ∆t), with ∆Wi denoting a standard Brownian increment with zero mean and variance ⟨∆W ∗ i ∆Wi⟩ = kBT ∆t, one ﬁnds the following expression to O(∆t), Transforming the Langevin equation into a FP PDE involves a standard procedure 37 and leads to an equation of the form ∂P In this appendix, we derive perturbative expressions for the energy spectrum ⟨|a(ω)|2⟩ and transfer function Φ(ω) of the nonlinear cavity. For convenience, we deﬁne α = α1 − α2i, with α2 = − Im α > 0 as required by any passive nonlinear system. We begin by deﬁning a perturbed cavity ﬁeld a(t) = a0(t) + δa(t), where a0 is the linear cavity ﬁeld and δa is a correction of linear order in α. Plugging in the perturbed ﬁeld into the coupled-mode equations and ignoring terms O(α2) and higher, one obtains the coupled equations:
Redshift Space Distortion of 21cm line at 1<z<5 with Cosmological Hydrodynamic Simulations<|sep|>Future 21 cm surveys will reveal the three dimensional distribution of neutral hydrogen gas over cosmological scales, which will potentially be a new probe of the large-scale structure of the Universe. In this paper, we explore the properties of Hi clustering using two diﬀerent set of cosmological hydrodynamic simulations, the Illustris and the Osaka simulations that include nonlinear baryonic effects of star formation and feedback. We ﬁrst measure the scale and redshift dependences of Hi bias in real space by taking the ratio of power spectra of Hi–dark matter cross correlation and dark matter auto correlation. Fitting with the constant plus linearly-scaled bias with k, we ﬁnd that the Hi bias monotonically increases with redshift for both simulations. This result is consistent with the Illustris-TNG simulation (VillaescusaNavarro et al. 2018), but the redshift evolution is stronger in the Illustris-1 & 3 than in TNG100-1. We also ﬁnd that the Hi bias shows a signiﬁcant scale dependence at z > 4 up to the scales where the perturbation theory holds, but it is consistent with being constant at z ⩽ 3. If we limit our analysis to the large scales of k < 0.25 hMpc−1, we ﬁnd no evidence of scale dependence at 1 < z < 5. In both cases, the best-ﬁtting bias parameters are fairly consistent between Illustris and Osaka simulations, which implies that the scale dependence of Hi bias on large scales is not sensitive to the details of the small-scale astrophysics. This means that, as far as we use the large scale modes, the cosmological analysis such as the determination of BAO scale is unlikely to be aﬀected by the astrophysical uncertainties of feedback on small scales. However, at the same time, if one use the data more aggressively up to higher k, we certainly need accurate knowledge on the astrophysical eﬀects such as supernova or AGN feedback. We leave more detailed and thorough investigation of the astrophysical impact of feedback on the Hi power spectrum as a future work. Further discussion on the evolution of ΩHI is given in the appendix. We then measure the redshift space distortion using the anisotropic two dimensional power spectrum. We jointly ﬁt the monopole and quadrupole of the Legendre expanded power spectra including the peculiar velocity eﬀect to the models widely applied for galaxy redshift surveys, with the free parameters of bias bHi and velocity dispersion σv. We note that, since we only have two simulations, the cosmic variance largely aﬀects the amplitude of large scale ﬂuctuations. Therefore, we ﬁt the data only in the range of 0.18 < k < kmax where kmax is given by Eq. (10). We ﬁnd that the measured bias parameter in redshift space is consistent with the one directly measured in real space from the ratio of power spectra. We also ﬁnd that the velocity dispersion of Hi gas is systematically below the prediction from linear perturbation theory but marginally consistent with the prediction. Compared with the previous work by Sarkar & Bharadwaj (2018), we ﬁnd signiﬁcant disagreement on the values of Hi bias for the entire redshift range, which may mainly arise from the prescription of the Hi gas assignment to the dark matter halos in an N-body simulation by Sarkar & Bharadwaj (2018). On the other hand, the best-ﬁtting values of our velocity dispersion are fairly consistent with the previous work within the statistical error of the single box simulation. Although the simulations used in this paper solve baryonic distribution and hydrogen ionization process in
Stellar models in Brane Worlds<|sep|>In this paper we studied the equilibrium conﬁgurations of stars with gravitational corrections in braneworld models, and provided numerical solutions when necessary. For that we considered the high and low energy limits of the equations of motion to show the threshold between GR and braneworlds, and explored the appropriate boundary conditions to obtain general conclusions about the physical properties of the diﬀerent stellar conﬁgurations. Our analysis took into account the corresponding Weyl functions which provide non-local terms in the pressure and density, and which can have noticeable eﬀects in diverse features of a star. This study allows us to relinquish the nonlocal anisotropic stress under the conditions of a Schwarzschild exterior and non-constant density, which
Two-parton Light-cone Distribution Amplitudes of Tensor Mesons<|sep|>We have systematically studied the two-parton light-cone distribution amplitudes for 1 3P2 nonet tensor mesons. The light-cone distribution amplitudes can be presented by using QCD conformal partial wave expansion. We have obtained the asymptotic twoparton distribution amplitudes of twist-2 and twist-3. The relevant decay constants have been estimated using the QCD sum rule techniques. We have also studied the decay constants for f2(1270) and f ′ 2(1525) based on the hypothesis of tensor meson dominance together with the data of Γ(f2 → ππ) and Γ(f ′ 2 → K ¯K). The results are in accordance with the sum rule predictions. One of us (H.Y.C.) wishes to thank C.N. Yang Institute for Theoretical Physics at SUNY Stony Brook for its hospitality. This work was supported in part by the National Center for Theoretical Sciences and the National Science Council of R.O.C. under Grant Nos. NSC96-2112-M-033-004-MY3, NSC97-2112-M-008-002-MY3 and NSC99-2112-M003-005-MY3.
Towards precision distances and 3D dust maps using broadband Period--Magnitude relations of RR Lyrae stars<|sep|>We have applied a simultaneous Bayesian linear regression methodology to 637 mean-ﬂux magnitude measurements of a calibration sample of 134 RR Lyrae stars to derive new,
UV Direct-Writing of Metals on Polyimide<|sep|>A novel direct-writing fabrication process for micropatterning of metals on polyimide is presented.  This process  can be carried out in air, on contoured surfaces, and requires  only low fluence UV light sources and low cost source  materials.  The line widths of the tracks fabricated by UVphotomask method were in good agreement with that of the  mask.  Using laser direct-writing, smaller track width can be  achieved with accuracy.  Implementation of this method for  fabrication of MEMS packaging and other devices requires  reliable control over the resistivity, adhesion, feature size  and thickness of the deposit. from the UK Engineering Physical Sciences Research  Council (EPSRC) through its Basic Technology Program.   The work was conducted under the project entitled “A  thousand Micro-emitters per square millimeters” referenced  GR/S85764. The authors also acknowledge the support of  EPSRC-funded project 3D-Mintegration (www.3dmintegration.com).
Tunable scattering cancellation of light using anisotropic cylindrical cavities<|sep|>In summary, we investigated the scattering eﬃciency of cylindrical cavities formed by a radially anisotropic nanostructure, when the illumination is carried out by a TEz plane wave. A radially anisotropic medium was established by the form birefringence of a metamaterial composed of periodic distribution of subwavelength concentric multilayers. The Lorenz-Mie scattering coefﬁcients are determined by means of the proper boundary conditions, which we set in terms of a transfer matrix formalism. For subwavelength cavities, we observe two resonances corresponding to modal symmetric and antisymmetric surface polaritons. By the side of each resonance peak, a minimum in scattering at a slightly higher frequency is found, which are attributed to Fano shapes. For scatterers immersed in air, the fundamental frequency where the scattering cancellation is shown at the lowest energy makes the metamaterial permittivity ǫ⊥ takes values near zero. Thus, by balancing the composition of the metamaterial, we might tune the invisibility frequency up to the plasma frequency of the constituting Drude medium. Unfortunately, such approach is not valid for high ﬁlling factors close to unity, where the tubular particle behaves like a low-birefringent plasmonic cavity. In these cases, the secondary dip in the scattering spectrum might eﬃciently be used to obtain an invisibility eﬀect. The onset of additional resonances at increasing cavity sizes, in the order of the current wavelength, governs the contour of the scattering spectrum thus vanishing the invisibility eﬀect. The main invisibility frequency remains practically unaltered of various values of the aspect ratio T/R. In particular, examining the case which suggests a cylinder with a very small concentric hole, we obtained the occurrence of multiple peaks and their associated minima in eﬃciency, indicating an accumulation of excited Fano resonances. Finally, severe discrepancies to the rule of ǫ⊥ near zero, enabling to ﬁnd the principal Fano resonance with associated scattering cancellation, were found by modifying the dielectric constant in the core and in the environment medium, even at low and moderate ﬁlling factors. For instance, for a high dielectric constant of the core and environment medium, the fundamental frequency of invisibility is set practically ﬁxed for the whole range of ﬁlling factors. These results demonstrate that the invisibility regime dramatically depends on the permittivities of the core and environment medium.
The shortest period detached white dwarf + main-sequence binary<|sep|>We have used a combination of ULTRACAM and SOFI photometry and X-shooter spectroscopy to constrain the system parameters of the 94 minute orbital period eclipsing PCEB SDSS J085746.18+034255.3. We measure a temperature for the white dwarf of 35, 300 ± 400K and a distance of 968 ± 55pc. By detecting the secondary eclipse in our ULTRACAM photometry we were able to determine an inclination of i = 85.5◦ ± 0.2◦. We were also able to measure the radial velocity amplitude of the white dwarf as 64±6 km s−1. No absorption features from the secondary star were visible in the X-shooter spectra, however we were able to determine the range of possible Ksec values by applying a correction factor to the measured radial velocity amplitude of several emission lines originating from its heated face. By combining the radial velocity measurements with the light curve ﬁt we measured the mass and radius of the white dwarf to be 0.51 ± 0.05M⊙ and 0.0247 ± 0.0008R⊙ respectively. The measured temperature, mass and radius are inconsistent with a carbon-oxygen core white dwarf but are consistent with a helium core white dwarf at the upper mass range for helium core white dwarfs. The secondary star has a mass of 0.09 ± 0.01M⊙ and a radius of 0.110 ± 0.004 consistent with evolutionary models of low-mass stars. Its mass places it at the hydrogen burning limit. We also ﬁnd that SDSS J0857+0342 has only recently emerged from the common envelope phase and will reach a semi-detached conﬁguration in ∼ 4 × 108 years when it will become a cataclysmic variable with a 66 minute orbital period, at which point its orbital period will increase. Due to the detection of the secondary eclipse, SDSS J0857+0342 is ideally set up to measure precise masses and radii for both of the stars. Currently the limiting factor is the poorly constrained Ksec. With higher S/N spectra a more reliable K-correction can be made, or potentially a direct measurement via absorption lines in the infrared, which will vastly improve the precision of the masses and allow us to test the mass radius relation for massive helium core white dwarfs and very low-mass stars. ULTRACAM, TRM, BTG, CMC, VSD and SPL are supported by the Science and Technology Facilities Council (STFC). SPL also acknowledges the support of an RCUK Fellowship. ARM acknowledges ﬁnancial support from FONDECYT in the form of grant number 3110049. MRS thanks for support from FONDECYT (1100782). The results presented in this paper are based on observations collected at the European Southern Observatory under programme IDs 086.D-0265, 286.D-5030 and 087.D-0046.
High-z dusty star-forming galaxies: a top-heavy initial mass function?<|sep|>We have exploited the physical model by Cai et al. (2013) to investigate the impact of a top-heavy IMF on sub-mm counts of high-z (z > 4 and z > 6) dusty galaxies. The ﬁrst estimates of sub-mm counts at z ∼> 4 galaxies (Dowell et al. 2014; Asboth et al. 2016; Ivison et al. 2016), taken at face value, favor the top-heavier IMFs suggested by Zhang et al. (2018). On the contrary, the extreme (ﬂat) IMF used by Baugh et al. (2005) overpredicts, in the framework of the Cai et al. (2013) approach, counts for z > 4. If the “raw” counts are corrected for ﬂux boosting due to instrumental noise and confusion following some recent studies (B´ethermin et al. 2017; Donevski et al. 2018; Duivenvoorden et al. 2018), approximate consistency with predictions based on a universal “Chabrier” IMF is recovered. On the other hand, with the exception of the counts by Donevski et al. (2018), the corrected data points are also consistent, within the uncertainties, with a top-heavy IMF. A deﬁnite conclusion on the abundance of z > 4 sub-mm galaxies must await for higher resolution data and a much higher fraction of spectroscopic redshifts. From the theoretical point of view an increase of the Jeans mass is expected in the case of high SFRs due to the large cosmic ray energy densities which raise the gas temperatures
Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels<|sep|>In this paper, we presented the Multichannel Generative Language Model (MGLM). MGLM is a generative joint distribution model that marginalizes over all possible factorizations within and across channels. MGLM endows ﬂexible inference, including unconditional, conditional, and partially observed generation. We experimented with those inference modes using the Multi30K dataset containing English, French, Czech, and German. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity tradeoffs and ﬁnd MGLM outperform traditional bilingual discriminative models. Our work focused on a speciﬁc instantiation of channels as languages. However, MGLM is not limited to only languages and can generalize to other notions of channels. In future work, we will consider other textual channels, such as paraphrases, premises and hypotheses, questions and answers, and multimodal channels, such as images. Another direction can investigate scaling MGLM to dozens/hundreds of channels. Fully generative models still often lag behind purely discriminative counterparts in performance, but we hope our work motivates future research on building generative joint distribution models of the world. We give thanks to Mohammad Norouzi, Lala Li, Sara Sabour, Geoffrey Hinton, Silviu Pitis, and the Google Brain team for useful discussions and feedbacks.
Asymmetry and structural information in preferential attachment graphs<|sep|>In this paper, we just proved that a version of the standard preferential attachment graph is asymmetric if every node adds more than two edges. It is easy to extend this statement to the case when the attachment is uniform and a mixture of uniform and preferential: e.g., for a ﬁxed β ∈ [0, 1], the probability that a connection choice goes to node w at time n + 1 is Another, possibly more practical, model was introduced by Cooper and Frieze [7] in which essentially the number of edges added follows a given distribution. We believe our methodology can handle this case, too. However, consider a model in which the weight of a vertex when m new edges are generated is proportional to the degree raised to some power α. In this paper we considered α = 1. We are conﬁdent our approach could be adopted to work for all α > 0 to ﬁnd the threshold mα for the asymmetry which, clearly, will grow with α. However, in the case α ̸= 1 the problem becomes much harder since, for instance, the probability that t chooses vertex s as its neighbor depends not only on the degree degt(s) but on the whole degree sequence at the time t (though there has been some work on the asymptotic degree distribution and other structural properties in the case of α > 1 [21, 18]). Nonetheless, these diﬃculties could be overcome by modern combinatorial methods and we plan to deal with this model in the nearest future.
Disease Normalization with Graph Embeddings<|sep|>concepts (canonical identiﬁers) by adapting state-of-the-art neural graph embeddings (GCN and node2vec) that exploit both MeSH®’s hierarchical structure and the description of diseases. Our graph-based disease node encoding is the ﬁrst of its kind, to best of our knowledge. We also apply multi-tasking to transfer information between disease detection (NER) and resolution (EL) components of the task, leveraging their common signals to improve on the single models. We observe that bioELMo embeddings lead to substantial improvement in NER performance. We demonstrate that node lexicalization does improve over either
Photometric observation of HAT-P-16b in the near-UV<|sep|>We have investigated the primary transit of HAT-P-16b observed on December 29th, 2012 in the near-UV ﬁlter. In this study we derived a new set of planetary system parameters Mb = 4.193 ± 0.092 MJup, Rb = 1.274 ± 0.057 RJup;qb ¼ 2:52 ± 0.34 (g/cm3), log gb = 3.81 ± 0.06, H = 0.223 ± 0.019. We ﬁnd that HAT-P-16b’s near-UV planetary radius of Rp = 1.274 ± 0.057 RJup is consistent within error of its near-IR radius of Rp ¼ 1:289 � 0:066 RJup (Buchhave et al., 2010). Comparing our results with the discovery paper values, HAT-P-16b appears to have a constant planetary radius in the near-UV and near-IR wavelengths. We did not detect an early ingress as proposed by VJH11a despite having a detectable timing difference in the range of 19–38 min (B⁄ = 100 G and Bp ranging from 8 to 40 G). Even though we did not detect an early ingress we are still able to ﬁnd an upper limit to the magnetic ﬁeld ratio of HAT-P-16b. Since we could not distinguish a timing difference below the Nyquist frequency, we used a timing difference of twice the maximum near-UV light curve cadence (120 s) to derive a range on the upper limit of HAT-P-16b’s magnetic ﬁeld ratio equal to 0.0082B�. Our value is consistent with the non-detection of the magnetic ﬁeld of TrES-3b using the same method (Turner et al., 2013). Based on this result, we advocate for follow-up studies on the magnetic ﬁeld of HAT-P-16b using other detection methods such as magnetic star-planet interactions and radio emission, as well as observations with telescopes capable of achieving a better near-UV cadence to verify our ﬁndings, the techniques of VJH11a, and to search for a temporal bow shock variation predicted by Vidotto et al. (2011c). Our ﬁndings imply that the magnetic ﬁeld of HAT-P-16b is abnormally low, or that the effect proposed by VJH11a can only be observed with near-UV wavelengths not accessible from the ground. To further constrain these possibilities, we encourage follow up observations of other exoplanets predicted by VJH11a to exhibit an early near-UV ingress. Lastly, an in-depth radiative transfer analysis is needed to determine whether VJH11a’s techniques can only be used in narrow band spectroscopy and not broad-band photometry. We sincerely thank the University of Arizona Astronomy Club, the Steward Observatory TAC, the Steward Observatory telescope day crew, Dr. Phillip Pinto, Dr. Karin Ostrom, Charles Ostrom, the Associated Students of the University of Arizona, Lauren Biddle, Michael Berube and Robert Thompson for supporting this research. We would also like to thank the anonymous referee for their insightful comments during the publication process. This manuscript is much improved thanks to their comments.
Dust dynamics and evolution in expanding HII regions. I. Radiative drift of neutral and charged grains<|sep|>In this paper we investigated dust dynamics in a pressuredriven expanding H ii region to understand the appearance of IR bubbles. As drag force and radiation pressure force strongly depend on dust size, their combined action leads to dust diﬀerentiation within an H ii region. We particularly focused on the eﬀect of radiation pressure on charged grain motion and do not consider stellar wind and dust destruction. The grain charge is consistently evaluated to the compute aerodynamic drag force. Our conclusions can be summarized as follows: • Grain charge is of utter importance for dust velocity and the resultant density proﬁles (see also Gail & Sedlmayr 1979)). If all the grains are assumed to be neutral, the inner parts of the H ii region are almost devoid of dust. The grain charge increases dust friction with the gas and suppresses dust removal from the H ii region. • The grain ensemble is characterized by the mean charge and charge dispersion. It is crucial to consider both parameters in the dynamics of small grains (a < 100 ˚A). Coulomb drag for larger grains (a > 100 ˚A) can be computed using their mean charge. • The interplay between radiation pressure and drag generally leads to double-peak dust density proﬁles. The outer peak corresponds to the gas density maximum on the ionization and dissociation fronts. The inner peak arises due to the negative gradient of grain charge within the ionized region. Grains in the outer parts of the region have smaller charges and are less coupled to the gas. Sweeping of these grains leads to a density dip between the inner ionized region and the outer density maximum. The details of this process strongly depend on grain size. • PAHs, having the largest surface-to-mass ratio, are well coupled to the gas whenever they have a non-zero charge. PAHs with zero charge are easily blown away from the star’s vicinity. Near-zero charge ﬂuctuations during the expansion lead to removal of PAHs from the inner part of the H ii region. So, in principle, the lack of 8 µm emission inside the H ii region can be explained not only by the destruction of PAHs but also by the radiative removal of PAHs if their charge ﬂuctuates around zero. • Very small grains acquire zero charge less frequently than PAHs and, consequently, are better coupled to the gas and do not escape ionized region easily. The density distribution of VSGs is the most uniform relative to other grain types. Stochastically heated VSGs may partially (along with ISGs) explain the double-peak morphology of IR bubbles at 24 µm. • The intriguing feature of intermediate-sized grains (a ≈ 200 ˚A) is that their emission could contribute to both midand far-IR bands. The inner peak on the ISG density distribution is several orders of magnitude smaller than the outer peak, but the negative dust temperature gradient may lead to an inverse ratio of corresponding intensities at 24 µm. At the same time, the outer peak in the ISG distribution may be observable at 70–100 µm. • Big grains are subject to the strongest sweeping by the radiation pressure. Being able to contribute to far-IR, they are worthy candidates for the outer ring for emission at 70– 100 µm. The real dust size distribution in H ii regions is more complicated than described by the four dust types listed above. However, consideration of PAHs, VSGs, ISGs and BGs is necessary to understand the contribution of diﬀerent subsamples of the grain size distribution in the dust emission spectrum. While such problems are generally ill deﬁned, additional information can be retrieved from the H ii region morphology accompanied by grain dynamics modelling. We thank an anonymous referee for her/his stimulating report. This work is supported by the President of the Russian Federation grants (MK-2570.2014.2 and NSh-3620.2014.2), the Dynasty foundation and by RFBR grant 13-02-00642. We are grateful to L. Deharveng, A. Zavagno, W. Henney, and M. Murga for useful discussions and comments. The grain charge is primarily determined by photoelectric emission and accretion of ions and electrons. The probability f(Zgr) of ﬁnding a grain having charge Zgre can be determined from the equation of detailed balance (Tielens 2005): where Jpe [ s−1] is a photoelectric emission rate, Je and Jion [ s−1] are the electron and ion accretion rates, respectively. Writing down the above equation for every Zgr, one ﬁnds an equation for f(Zgr): for Zgr < Z0. Here f0 ≡ f(Z0). From the computational point of view, it is important that Z0 should correspond to the maximum of f(Zgr), which can be estimated from the equation: Photon attenuation depth and electron mean free path are assumed to be equal to 100˚Aand 10˚A, respectively. The accretion rate of particles with number density n and mass m was adopted from Draine & Sutin (1987): Here ˜J accounts for Coulomb focusing. As sticking probabilities are not well known, we follow Weingartner & Draine (2001) in assuming sticking probabilities for the electrons and ions to be equal to 0.5 and 1, respectively. The photoemission rate depends on the mean intensity Jν [ erg/(cm2 s Hz sr) ] and photo-ionization yield Y :
Dissecting Energy Consumption of NB-IoT Devices Empirically<|sep|>We conducted a comprehensive measurement study of the energy consumption of two popular NB-IoT boards that connect to two commercial deployments in a European country. Our ﬁndings indicate that NB-IoT is far from being plug and play and requires careful setting for improving energy efﬁciency. Since we focus on conﬁguration parameters and their impact on the energy consumption, our recommendations can be generalized to any NB-IoT deployment. We observe that the main factors determining energy consumption and thus battery life are: 1) module, 2) operator, 3) signal quality, 4) use of energy saving enhancements such as RAI and eDRX and, 5) in a limited number of scenarios, packet size. Furthermore, our analysis has helped the measured networks identifying and ﬁxing a couple anomalous conﬁgurations, and we could ﬁnally track the effectiveness of this adjustments. Finally, we have indicated strategies for improving energy efﬁciency, pointing out the elements that could bring to energy waste without improving the reliability, such as too aggressive ECL thresholds or not using the RAI ﬂag. We also identiﬁed the key parameters needed for estimating the battery lifetime, and which of the metadata reported by the device are more meaningful. Possible future research directions include the energy impact of application layer protocols such as MQTT and CoAP, as well as recommendations for parameter tuning of these protocols.
Measurement of the muon-induced neutron seasonal modulation with LVD<|sep|>The seasonal variation of muon-induced neutrons per muon was found on the basis of data for 15 years. The measured characteristics of the neutron variation indicate seasonal variation in the average energy of muons at the LVD depth with amplitude of 10%, i.e. Eµ = 280 ± 28 GeV. Previously it was assumed that the ﬂux of cosmogenic neutrons is proportional to the amplitude of muon intensity variation 1.5%. We have shown that the neutron ﬂux has an amplitude of seasonal variation in 6 times more, because the average muon energy also varies with the amplitude of ∼ 10%. This work was supported by the Russian Foundation for Basic Research (project No. 15-02-01056 a) and the program of investigations of the Presidium of Russian Academy of Sciences High Energy Physics and Neutrino Astrophysics. [1] P.M. Blacket, Phys.Rev. 54, 973 (1938); Paul H. Barrett, Lowell M. Bollinger, G. Cocconi, Y. Eisenberg, and K. Greisen, Reviews of Modern Physics, V. 24, N 3, (1952). [2] M. Ambrosio et al., MACRO Collaboration, Astroparticle Physics, 7 (1997) 109-124. [3] E.W. Grashorn for MINOS collaboration, Proc. 30ICRC, arXiv:0710.1616 (2007).
Analytical properties of Einasto dark matter haloes<|sep|>We studied the spatial and lensing properties of the Einasto proﬁle by analytical means. For the spatial properties we applied the method used by Ciotti & Bertin (1999) to derive an analytical expansion for the dimensionless parameter dn of the Einasto model. We also derived analytical expressions for the cumulative mass proﬁle M (r) and the gravitational potential Ψ (r). For the lensing properties, we used the Mellin integral transform formalism to derive closed, analytical expressions for the surface mass density Σ (x), cumulative surface mass M (x), deﬂection angle α (x), deﬂection potential ψ (x), magniﬁcation µ (x) and shear γ (x) . For general values of the Einasto index n, these are expressed in terms of the Fox H function. Using the properties of the Fox H function, we calculated explicit power and logarithmic-power expansions for these lensing quantities, and we obtained simpliﬁed expressions for integer and half-integer values of n. These series expansions allow to perform arbitraryprecision calculations of the surface mass density and lensing properties of Einasto dark matter haloes. We also studied the asymptotic behaviour of the surface mass density, cumulative surface mass and deﬂection potential at small and large radii using the series expansions. Furthermore, we compared the S´ersic and Einasto surface mass densities using the equivalent values for the S´ersic m and Einasto n indices for ﬁxed values of ΥIe, Re, ρ0rs and rs, showing that both proﬁles have a similar behaviour. However, we noted that for the Einasto proﬁle the external wings are more spread out and it seems to be less sensitive than the S´ersic proﬁle to the value of the surface mass density for a given Einasto index and radius in the inner region. Also, we ﬁnd that the Einasto proﬁle is more ‘cuspy’ than the S´ersic proﬁle: the former has higher values of the central surface mass density. These features are of key importance, because it is in this region that the lensing eﬀect is more important, and these dissimilarities in the surface mass densities imply a diﬀerence in the lensing properties of both proﬁles. This result agrees with previous work of Cardone et al. (2005) and Dhar & Williams (2010). Our results are the ﬁrst step in studying the properties of the Einasto proﬁle using analytical means. The constant increase of computational power opens the possibility of using more realistic and sophisticated proﬁles like the Einasto proﬁle in cosmological studies, where our results may apply. For example, they can be used in strong- and weak-lensing studies of galaxies and clusters, where dark matter is believed to be the main mass component and the mass distribution can be assumed to be given by an Einasto proﬁle. The better performance in cosmological N-body simulations of the Einasto proﬁle (Navarro et al. 2004; Merritt et al. 2006; Gao et al. 2008; Hayashi & White 2008; Stadel et al. 2009; Navarro et al. 2010) makes its inclusion in strong and weak lensing studies very promising. Recently, Chemin et al. (2011) analysed the rotation curves (RC) of spiral galaxies from THINGS (The HI Nearby Galaxy Survey, (de Blok et al. 2008)) and found that the Einasto proﬁle provides a better match to the observed RC than the NFW proﬁle (Navarro, Frenk, & White 1996, 1997) and the cored pseudo-isothermal proﬁle. Also, Dhar & Williams (2011) modelled the surface brightness proﬁles of a sample of elliptical galaxies of the Virgo cluster using a multi-component Einasto proﬁle based on an analytical approximation, and obtained a good ﬁt for shallow-cusp and steep-cusp galaxies with ﬁt residual errors lower in comparison to measurement errors in a wide dynamical radial range. Additionally, in their models of the most massive galaxies, the outer components are characterised by being in the range 5 ≲ n ≲ 8, and a comparable range is obtained from N-body simulations for the Einasto proﬁle. Our exact analytical results for the spatial and lensing properties of the Einasto may be used to constrain the value of the Einasto index and determine if the galaxy or cluster studied is dark matter dominated or not. More studies like Chemin et al. (2011) and Dhar & Williams (2011) could help to strengthen the position of the Einasto proﬁle as a new standard model for dark matter haloes. Also, increasing the use of the Einasto proﬁle in new cosmological studies could provide progress towards a solution to the cusp-core problem. This paper continues the eﬀort initiated by Baes & Gentile (2011) and Baes & van Hese (2011) to advocate the use of the Fox H and Meijer G functions in theoretical astrophysics, in particular for studying the analytical properties of density models like the S´ersic model, where no additional analytical progress could be made until the Mellin integral transform formalism was applied. We hope that our work has again demonstrated the usefulness of the Fox H and Meijer G functions as tools for analytical work. Acknowledgements. ERM and FFA wish to thank H. Morales and R. Carboni for critical reading. This research has made use of NASA’s Astrophysics Data System Bibliographic Services. GG is a postdoctoral researcher of the FWOVlaanderen (Belgium). Moreover, we would like to thank the referee for valuable suggestions on the manuscript.
The cavity method: from exact solutions to algorithms<|sep|>The Cavity method is a powerful and versatile approach to the description of disordered systems, that has been shown so far to provide the exact asymptotic solution for many models. For given (ﬁnite) system instances, its algorithmic counterpart has many practical applications, ranging from a statistical description of the Boltzman-Gibbs distribution to the individuation of single solutions of a CSP. Moreover, at variance with more traditional methods for inference such as MCMC sampling, it can provide an analytical description, given implicitly by the solution(s) of the cavity equations. This fact enables many possibilities, such as its recursive application (SP), and a functional expression of statistical features as a function of the disorder parameters (see for instance chapter 21 for a discussion of inverse problems). Abou-Chacra, R., Thouless, D., and Anderson, P. (1973). A selfconsistent theory of localization, J. Phys. C 6, p. 1734. Altarelli, F., Braunstein, A., Dall’Asta, L., Lage-Castellanos, A., and Zecchina, R. (2014). Bayesian inference of epidemics on networks via belief propagation, Physical Review Letters 112, 11. Altarelli, F., Braunstein, A., Dall’Asta, L., and Zecchina, R. (2013). Optimizing spread dynamics on graphs by message passing, Journal of Statistical Mechanics: Theory and Experiment 2013, 9. Altarelli, F., Braunstein, A., Ramezanpour, A., and Zecchina, R. (2011). Stochastic matching problem, Physical review letters 106, 19, p. 190601. Aurell, E. and Mahmoudi, H. (2012). Dynamic mean-ﬁeld and cavity methods for diluted Ising systems, Phys. Rev. E 85, 3, p. 031119, doi:10.1103/PhysRevE.85.031119, https://link.aps.org/doi/10.1103/ PhysRevE.85.031119. Baldassi, C., Borgs, C., Chayes, J. T., Ingrosso, A., Lucibello, C., Saglietti, L., and Zecchina, R. (2016). Unreasonable eﬀectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes, Proceedings of the National Academy of Sciences 113, 48, pp. E7655–E7662, doi:10.1073/pnas.1608103113, https://www.pnas.org/doi/ abs/10.1073/pnas.1608103113. Bapst, V., Foini, L., Krzakala, F., Semerjian, G., and Zamponi, F. (2012). The quantum adiabatic algorithm applied to random optimization problems: The quantum spin glass perspective, Physics Reports , 0, pp. –, doi: 10.1016/j.physrep.2012.10.002, http://www.sciencedirect.com/science/ article/pii/S037015731200347X?v=s5. Bayati, M., Borgs, C., Braunstein, A., Chayes, J., Ramezanpour, A., and Zecchina, R. (2008). Statistical mechanics of steiner trees, Physical Review Letters 101, 3. Bayati, M. and Nair, C. (2006). A rigorous proof of the cavity method for counting matchings, arXiv:cond-mat/0607290 http://arxiv.org/abs/ cond-mat/0607290, arXiv: cond-mat/0607290. Bayati, M., Shah, D., and Sharma, M. (2005). Maximum weight matching via
Complex-valued information entropy measure for networks with directed links (digraphs). Application to citations by community agents with opposite opinions<|sep|>Directed networks are very common. Citation networks belong to a huge subclass of those. It is common knowledge also that opinion formation demands information exchanges which are thereby necessarily ”directed” between agents. When representing such networks through adjacency matrices, it is apparent that such matrices are necessarily asymmetric. This paper, on one hand, introduces a technique in order to obtain some insight into directed networks. It appears that one can consider a network information entropy through the link distribution on which information is exchanged. Through an analogy with Boltzmann entropy in usual statistical mechanics, one can observe that in order to get more insight on the network entropy, one can calculate the whole set of eigenvalues of the adjacency matrix of the network. This is similar to consider the set of discrete values of a transfer matrix in quantum or statistical mechanics. However, the resulting information entropy turns out to be a complex mathematical feature. It needs some interpretation1. The latter can be based on the free energy concept. Starting form the notion of equilibrium free energy, Zweger [58] attempted a dynamical interpretation of a classical complex free energy, in 1985. He pointed out that the problem is to determine a characteristic ”relaxation” time for some process in which the dynamical (Langevin or Fokker-Planck) equation is connected to some Hamiltonian or some corresponding transfer matrix. A probability current can be written, in fact, in terms of some unstable mode times an equilibrium factor which is the imaginary part of the free energy [59]. Usually [58, 60], the imaginary part of the free energy (or largest eigenvalues) give some information about the ”nucleation stage” of the dynamics. The real part, of course, determines the equilibrium energy state. We propose that another, though related approach, can be considered. Instead of some ”relaxation time”, one may consider the ”spatial aspect of the phenomenon”, e.g. through some correlation range length ξ. This approach makes some sense,in particular for networks, analytically described through 1A reviewer suggested that ”it would be fair, to indicate which branch is better or worse in possible applications/interpretations”. This interesting point, however, sends back the reader to wonder what Riemann sheet is used in numerical algorithms, - ... diﬀerent ones, as we have alas observed reconciling various calculations, whence inducing the extensive reports in Sect. 2.2.1 and 2.2.2, surely serving as warnings some Hamiltonian or transfer matrix. Moreover, such a spatial scale introduction may remind of some analogy with Discrete Scale Invariance (DSI) (or lack of DSI) feature [19, 20]. In fact, this DSI leads to complex dimensions and complex critical exponents. The illustrating example implies two distinct communities, with markedly diﬀerent opinions : the Neocreationist and Intelligent Design Proponents (IDP) on one hand, and the Darwinian Evolution Defenders (DED) on the other hand. These are communities for which an opinion consensus can be hardly expected. It appears that for the whole set of agents, two agents (”states”) are markedly dominating. They seem to belong to the DED community. In contrast, the IDP community has only one main ”state”. Interestingly, the same is true for the inter-community ” information exchange phenomenon” for which there is only one main dominating state. In summary, we have presented an original work extension of IE, connecting the CIE method, to sound statistical mechanics. By examining, diﬀerent eigenvalues of asymmetric matrices, - sometimes complex eigenvalues, yet starting with the largest ones, one can describe an IE, - like if in thermodynamics, one describes a free energy in terms of eigenvalues of some Hamiltonian. Thus, one not only obtains the ”basic” free energy, but also corrections due to some underlying scale structure. Moreover considerations on the mathematical form of the IE, i.e. its real and imaginary part, when they exist, allow to emphasize characteristics, which we attribute to the leadership range. This has been exempliﬁed by considering a network with two speciﬁc communities having diﬀerent opinions, exchanged through citations. Note added at the completion of this report: Another argument in favor of studying asymmetric matrices can be mentioned. Indeed, during the process of ﬁnishing up the present work, for submission, a paper, submitted to to EPJB, occurred on arXives [61] entitled Asymmetric correlation matrices: an analysis of ﬁnancial data. Due to the asymmetry in time delayed correlations between ﬁnancial time series, it is indeed also of interest to extend the spectral analysis to the realm of complex eigenvalues, - as ﬁrst attempted in [62]. Though diﬀerent in essence, such works and the present one indicate that one should not be stacked to studying only systems within real algebra. Acknowledgements GR thanks the COST Action MP0801 for the STSM 4475 grant, allowing her stay at the University of Liege in Feb. 09. MA thanks the COST Action MP0801 for the STSM 6698 grant, allowing his stay at the University of Viterbo, in Sept. 2010. has been adapted from its original writing, with real numbers, to one involving complex numbers, but the more so better appropriate for the IE, i.e., Tsallis [63, 64, 65] proposed that a large category of systems may be treated by a similar formalism, but where a more general entropy measure is deﬁned by which depends on the real parameter q and which reduces to the Shannon entropy for q →1. Along the lines in the main text, it is tempting to deﬁne a qIE as thereby accepting a complex-valued IE in Tsallis sense. Tsallis theory is sometimes referred to in the literature as no-extensive statistical mechanics, in contrast with the extensivity of the Shannon entropy. For general q, a proper extremisation of Eq.(17) leads to generalized canonical distributions, often called Tsallis distributions, where x denotes the energy of the system, Zs(q) ≡ ΣNs ν=1e−H(s,ν)q, and ex q is the q-exponential function deﬁned by One should also note that Tsallis formalism draws a direct parallelism with the equilibrium theory, where β ′ plays the role of the inverse of a temperature, and Z that of a partition function. However Tsallis and others [63, 64, 65] have often insisted ion the connexion between q- and non-equilibrium eﬀects. One might consider connecting the above IE to Tsallis considerations, in further work; see already [66]. Recall that a q−Theil index has been already introduced [67, 68]. To remain within a ”no self-citation” scheme, let all the diagonal elements be equal to zero, i.e. H11 = H22 = H33 = 0, and call this ”new” matrix H0. Moreover, let all non diagonal elements be equal to either 1 or 0. These ”reductions” are made in the spirit of tying the present subsection to the main text, involving a (”large”) citation network for which the (adjacency) matrix has elements taking only a 1 or 0 value. Recall that the EVs of any 3x3 matrix are solutions of the cubic equation By ”construction”, tr(H0) = tr2(H0) = 0. Moreover, one easily obtains that tr(H0)2/2 = H12H21 + H13H31 + H23H32, and det(H0) = H13H32H21 + H31H12H23. Therefore, only 7 types of cubic equations, as Eq.(22), have to be considered It is somewhat easily deduced that only type III and type VI lead to complex eigenvalues. Both types have one positive real root. Note that the type I cubic equation has a real negative (of course evenly) degenerate root = -1, requesting special attention when calculating H; see end of Sect. 2.2. The networks made of three nodes corresponding to such cubic equations are illustrated in Fig. 5, - one network is displayed for each case only; the others are easily and readily deduced by permutation of bonds [69, 70, 71]. On one hand, this illustrates well why type I has degenerate eigenvalues. On the other hand, the complex eigenvalues (type III and type VI) are now understood as arising from the transitivity relationship, corresponding to 14-120C and 9-030T triads, in Pajek Manual notations [72]. Even though it might look surprising to describe 3x3 matrices in a modern scientiﬁc paper, the present illustration has been found necessary as the most simple one leading to some appreciation of the EV behavior of larger random matrices.
Evidence of Particle Acceleration in the Superbubble 30 Doradus C with NuSTAR<|sep|>We have presented evidence of particle acceleration in the superbubble 30 Dor C using hard X-ray images and spectra from targeted and serendipitous NuSTAR and XMM-Newton observations. The complete shell of the SB is detected up to ∼20 keV, and the young SNR MCSNR J0536−6913 is detected up to 8 keV. Additionally, hard X-ray emission is evident at locations of six point sources previously identiﬁed with Chandra and XMMNewton, and we discussed the possible associations of these objects with massive star clusters, AGN, and stel
Logistic map with memory from economic model<|sep|>realized in [46, 47, 48, 49, 50, 51, 52, 53, 54, 55]. New types of chaotic behavior and new kinds of [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 15, 16, 17, 18, 19] in the continuous time
Molecular line mapping of the giant molecular cloud associated with RCW 106 - II. Column density and dynamical state of the clumps<|sep|>In this paper we have presented C18O mapping of the central part of the GMC associated with RCW 106, which had been previously mapped at similar resolution in 13CO (Paper I). The 13CO and C18O data have been used to estimate the optical depth and excitation temperature as a function of position and velocity, and thereby obtain estimates of the column density assuming LTE. In a separate analysis, we have decomposed both 13CO and C18O datacubes into clumps using an automated decomposition algorithm and estimated the clump masses using the LTE column density cube. The principal results of this study are as follows: (i) Correcting for opacity does not have a dramatic effect on the overall cloud mass, since regions of high opacity are restricted to compact regions. Depending on the adopted excitation temperature, assuming negligible opacity can even lead to an overestimate of the cloud mass. (ii) Correcting the 13CO emission for opacity does broaden the distribution of column densities, which otherwise exhibits a relatively rapid falloff at the high end. The resulting column density PDF is well-approximated as a log-normal function, consistent with what is commonly found in numerical simulations of turbulence. (iii) The regions of highest opacity are found in the northern part of the cloud, in a ring-like structure surrounding the luminous H II region G333.6−0.2. We speculate that this ring is a limb-brightened
An estimation of the Moon radius by counting craters: a generalization of Monte-Carlo calculation of $\pi $ to spherical geometry<|sep|>In this work the Moon radius can be obtained experimentally by counting craters in an spherical square and a quarter of circle inscribed in it. This procedure is the generalization of the Monte-Carlo method to obtain π in Euclidean geometry to spherical geometry. By deviating from ﬂat geometry, the ratio of random points in a square and a circle inscribed in it gives a deviation from π. This deviation can be related to the radius of inscribed circle and the radius of the sphere. In particular, the Moon contain zones with random craters that can be used as random points over a surface sphere. By applying the Monte-Carlo method, that is, by counting craters inside the square and circle deﬁned in the surface of the Moon, a deviation from π is obtained and this result is used to compute the Moon radius. The obtained value is RM = 1820 ± 146 km and the real value of the Moon radius is inside the error. Although the method introduced in this work is not very precise, shows how the randomness can be useful to obtain information about the underlying space in which this random phenomena occurs. In turn, this method can give better approximations by considering several squares and circles inscribed in the Moon.
Utilizing UNet for the future traffic map prediction task Traffic4cast challenge 2020<|sep|>year’s experiments and experience, we could further try new ideas with regard to the  UNet encoder and decoder’s structural compositions. Also we tried to combine base  model outputs using simple yet effective methods to improve performance further.  Our methods showed effective performance on the real world data collected from  three large cities worldwide.
Interference Effects in Quantum Belief Networks<|sep|>This work was motivated by the preliminary experiments of [71] about violations of the classical probability theory on the sure thing principle. This principle states that if one chooses action A over B in a state of the world X, and if one also chooses action A over B under the complementary state of the world X, then one should always choose action A over B, even when the state of the world in unspeciﬁed. When humans need to make decisions under risk, several heuristics are used, since humans cannot process large amounts of data. These decisions coupled with heuristics lead to violations on the law of total probability. Recent work in cognitive psychology revealed that quantum probability theory provides another method of computing probabilities without falling into the restrictions that classical probability have in modelling cognitive systems of decision making. Quantum probability theory can also be seen as a generalisation of classical probability theory, since it also includes the classical probabilities as a special case (when the interference term is zero). The main difference between quantum and classical probability lies in the fact that on quantum probability we are constantly updating some beliefs when making a decision, while in classical probability all beliefs are assumed to have a deﬁnite value before a decision is made, and this value is the outcome of the decision [3]. The main research question for this work was how could these quantum probabilities affect probabilistic graphical models, such as Bayesian Networks, since many of nowadays decision making systems are based on such structures (medical diagnosis, spam ﬁltering, image segmentation, etc). In this work, we proposed a novel Bayesian Network for the Computer Science community based on quantum probabilities. Our method can accommodate puzzling observations that the classical probability failed to explain (for instance, the two-step gambling game). When the nodes of the proposed Bayesian Network are represented as a superposition state, then one can look at this state as many waves moving across in different directions. These waves can crash into each other causing waves to be bigger or to cancel each other. This is the interference phenomena that the proposed Bayesian Network offers and that has direct implications when making inferences. Therefore, the proposed network represents and simulates quantum aspects motivated by Feynman’s path integrals. Experimental results revealed that the proposed quantum Bayesian Network enables many degrees of freedom in choosing the ﬁnal outcome of the probabilities. If we had a real scenario, with real observations, one could use the present model to ﬁt it to the observed data, by simply tuning the parameter θ. This parameter can open a door into machine learning approaches. Learning algorithms using the proposed method might produce better prediction models, since the quantum probability amplitudes are able to fully represent real word data. For future work, we intend to explore machine learning algorithms under quantum probabilistic graphical model formalisms. The overall results also suggested that when the classical probability of some variable is already high, then the quantum probability tends to increase it even more. When the classical probability is very low, then the proposed model tends to lower it. When there are many unobserved nodes in the network then the levels of uncertainty are very high. But, in the opposite scenario, when there are very few unobserved nodes, then the proposed quantum model tends to collapse into its classical counterpart, since the uncertainty levels are very low. The proposed Bayesian Network can integrate human thoughts by representing a person’s beliefs in an N-dimensional unit length quantum state vector. In the same way, the proposed quantum structure is general enough to represent any other context in which there is a need to formalise uncertainty, including prediction problems in data fusion. In the context of Bayesian Networks, data fusion is introduced in the work of [58]. The author argues that, just like people, Bayesian Networks are structures that integrate data from multiple sources of evidence and enable the generation of a coherent interpretation of that data through a reasoning process. The fusion of all these multiple data sources can be done using Bayes theorem. When a data source is unknown, then the Bayes rule is extended in order to sum out all possible values of the probability distribution representing the unknown data source. The proposed Quantum Bayesian Network takes advantage of these uncertainties by representing them in a superposition state, which enables the fusion of the data sources through quantum interference effects. These effects produce changes in the ﬁnal likelihoods of the outcomes and provide a promising way to perform predictions more accurately according to reality. So, the Quantum Bayesian Network that is proposed in this work is potentially relevant and applicable in any behavioural situation in which uncertainty is involved.
A Lagrangian model of copepod dynamics: Clustering by escape jumps in turbulence<|sep|>In this study we have considered a Lagrangian model for active particles. The model is trimmed in a way to reproduce some dynamical features experimentally observed in the motion of copepods in still water. Its main characteristics is the possibility to locally acquire an extra-velocity (jump) in response to a variation of the ﬂuid ﬂow conditions surrounding the particle. The direction of the jump is ruled by the hydrodynamics of small neutrally-buoyant particles. The Lagrangian model has been coupled to a turbulent developed ﬂow described by the incompressible Navier-Stokes equations. We have shown that jump escape reaction from spatiotemporal events characterised by high shear-rate leads
Mitigation of Civilian-to-Military Interference in DSRC for Urban Operations<|sep|>This paper addressed the channel congestion for a DSRC system operating in a mixture of military and civilian vehicles. A novel protocol was proposed that increases the chance of a successful packet delivery for a military vehicle. The protocol is designed to prioritize the military vehicles in competition for a channel. It prohibits a transmission from a civilian vehicle with the distance to the closest military vehicle being smaller than a threshold. The simulations showed the proposed protocol’s performance based on the three widely accepted metrics–PDR, average delay, and throughput. According to the simulation results, the protocol effectively increased the performance of communications at a military vehicle with respect to all the three metrics. Multiple key design insights for the military communications were drawn from this paper’s results. Speciﬁcally, they revealed the required separation distances between a civilian and a military vehicle in terms of three different metrics. Furthermore, they showed the impact of parameters on the performance of DSRC for military’s urban operations–i.e., the threshold distance between a military and a civilian, and the trafﬁc density of civilian vehicles in relation to the military. As such, this work has many possible extensions. For instance, in this paper, we considered only the DSRC-based communications. One possible extension is to incorporate discussions on coexistence of military and civilian in a cellular vehicle-to-everything (C-V2X) system as well. An even more promising avenue of future work is to develop a comprehensive algorithm to enable coexistence under a mixture of DSRC and C-V2X in the 5.9 GHz band.
Triple collinear emissions in parton showers<|sep|>We have presented a new scheme to include triple collinear splitting functions into parton showers. As a proof of principle we have recomputed the timelike and spacelike ﬂavor-changing NLO DGLAP kernels Pqq′ and matched each component of the integrand to the relevant parton-shower expression. The implementation into two entirely independent Monte-Carlo simulations, based on the general-purpose event generation frameworks PYTHIA and SHERPA has been cross-checked to very high numerical accuracy. The impact of the ﬂavor changing triple-collinear kernels Pqq′ and Pq¯q has been studied in timelike and spacelike parton evolution as a ﬁrst application. We ﬁnd that the numerical impact of the kernels investigated here is marginal, with eﬀects of up to ∼ 1% on diﬀerential jet rates in e+e− →hadrons at √s=91.2 GeV (LEP I), neutral-current DIS with Q2 > 100 GeV2 at √s=300 GeV (HERA II), and pp → e+νe at 8 TeV (LHC I). We thank Lance Dixon, Falko Dulat, Thomas Gehrmann, Frank Krauss, Silvan Kuttimalai and Leif L¨onnblad for numerous fruitful discussions. This work was supported by the US Department of Energy under contracts DE–AC02– 76SF00515 and DE–AC02–07CH11359. In this section we give the phase-space parametrizations employed in our implementation of 1 → 3 parton branchings. We construct kinematic mappings that allow us to relate the splitting and evolution variables to manifestly Lorentz invariant quantities. In order to cover all possible applications, we list formulae for arbitrary external particle masses. While this is not strictly needed in the course of this work, it may be useful to include higher-order eﬀects involving heavy quark splitting functions in the future. The main results are Eqs. (A10), (A30), (A43) and (A58), as well as the corresponding D-dimensional phase-space factors, Eqs. (A21) and (A48).
Anisotropic CR diffusion and gamma-ray production close to supernova remnants, with an application to W28<|sep|>The details of the transport of CRs in the Galaxy are still little understood. Studies of the composition of CRs provide us with an estimate of the average conﬁnement time of CRs within the Galaxy, which can be translated into a spatially averaged diﬀusion coeﬃcient for CRs (e.g. Strong et al. 2007; Castellina & Donato 2012). Whether the CR diﬀusion coeﬃcient has large spatial variations or it is rather uniform throughout the Galaxy is not known, thought a suppression of diﬀusion close to CR sources might be expected due to CR streaming instability (Ptuskin et al. 2008; Malkov et al. 2012). To this purpose, the detection of gamma–ray emission from the vicinity of CR accelerators might be used to constrain the CR diﬀusion coeﬃcient, and thus assess the importance of such suppression (e.g. Aharonian & Atoyan 1996; Gabici et al. 2009). This is because CRs escaping the accelerators would produce gamma rays via proton–proton interactions with the ambient medium. Both the morphology of the resulting emission and its spectrum would depend on the functional form (i.e. energy dependence, level of anisotropy) of the diﬀusion coeﬃcient. An object that has been extensively investigated in this context is the SNR W28. Three massive molecular clouds, with total mass in the ≈ 105M⊙ range, are located in the vicinity of the SNR shell and emit gamma rays. This has been interpreted as the result of the illumination of the clouds by the CRs that escaped the SNR. Several models have been proposed to ﬁt these observations, and all of them are based on the assumption that the diﬀusion of CRs proceeds isotropically (e.g. Giuliani et al. 2010; Gabici et al. 2010, and see Sec. 4 for a complete list of references). There is a general consensus on the fact that the (isotropic) diﬀusion coeﬃcient has to be suppressed by a factor of ≈ 10...100 with respect to the average Galactic one in order to explain the observations. This implies coeﬃcients in the range �D ≈ 1026...1027 cm2/s. In this paper, the assumption of isotropy of diﬀusion has been relaxed, and a more physically motivated situation have been investigated, in which CRs propagate mainly along the magnetic ﬁeld lines. We considered here the limiting scenario in which the diﬀusion of CRs across ﬁeld lines is very small and thus can be neglected. In such a situation, the transverse displacement of CRs is uniquely due to the wandering of the ﬁeld lines (Jokipii & Parker 1969). Spectra and morphology of the spatial distribution of CRs around SNRs have been computed and described. The main feature is the elongated, ﬁlamentary distribution of CRs, as opposed to the spherical distribution found in the case of isotropic diﬀusion. In order to ﬁt the gamma–ray data from the W28 region within this scenario, one has to assume that the molecular clouds in its vicinity are magnetically connected to the SNR through a magnetic ﬁeld ﬂux tube. If this is the case, an accurate ﬁt to data can be obtained. Under this assumption, particles are bound to the ﬂux tube and thus forced to propagate along a speciﬁc direction. For plausible values of the diﬀusion coeﬃcient of magnetic ﬁeld lines, in order to obtain the correct CR over–density at the location of the molecular clouds a large (parallel) diﬀusion coeﬃcient of the order of �D∥ ≈ 1028 cm2/s has to be adopted. The fact that a very good agreement has been found with data in the two radically diﬀerent scenarios characterized by isotropic and anisotropic diﬀusion tells us that more data needs to be collected from more SNRs in order to infer with reasonable conﬁdence the properties of the diﬀusion of particles escaping their accelerators. The diﬀuse emission that these runaway particles would produce in their interaction with the ambient gas is, even in the absence of very massive clouds, within the capabilities of the Cherenkov Telescope Array (Acero et al. 2012; Casanova et al. 2010). These observations will provide us with precious informations about the properties of the transport of CRs in the We thank F. Casse, A. Marcowith, F. Piazza, V. Ptuskin, R. Schlickeiser, and L. Sironi for helpful discussions. The work of LN and SG has been supported by ANR under the JCJC Programme.
SelectVisAR: Selective Visualisation of Virtual Environments in Augmented Reality<|sep|>In two studies we investigated how a selective visualisation system of VEs can influence an AR user’s perception of a co-located VR user. We looked at two variables: the level of visual information and the effect of scale. Regarding level of visual information, we observed that filtering specific selections of the VE did not significantly affect the competence of how well people could identify events in the VE. These selections were based on the interactive range and possibilities of the VR user. Regarding scale, users generally agreed that smaller visualisations provide a better overview of the VE, but had the chance of decoupling the user from the task at hand. In terms of user preference, our qualitative data showed that participants tended to prefer static visualisations over dynamic visualisations, disliking the lack of control they could exercise for visualising the VE. In future work we would like to improve these visualisations to apply to a greater variety of VE contexts. Techniques such as Proximity are generalisable, but techniques such as Context are very specific to the context of the VE as they use a predetermined selection of virtual objects. This selection of visualised objects can be made in different ways, instead of a predetermined selection future work can investigate the creation of an interface for the AR, or VR, user to make this selection themselves. Moreover, we would like to apply these visualisations into an interactive implementation of this visualisation system, as currently the AR user only assumes a passive spectator role in the task. We could test our visualisations in a collaborative task that requires both the AR user and VR user to interact with elements from the VE.
Almost-Orthogonal Layers for Efficient General-Purpose Lipschitz Networks<|sep|>In this work, we proposed AOL, a method for constructing deep networks that have Lipschitz constant of at most 1 and therefore are robust against small changes in the input data. Our main contribution is a rescaling technique for network layers that ensures them to be 1-Lipschitz. It can be computed and trained efﬁciently, and is applicable to fully-connected and various types of convolutional layers. Training with the rescaled layers leads to weight matrices that are almost orthogonal without the need for a special parametrization and computationally costly orthogonalization schemes. We present experiments and ablation studies in the context of image classiﬁcation with certiﬁed robustness. They show that AOL-networks achieve results comparable with methods that explicitly enforce orthogonalization, while offering the simplicity and ﬂexibility of earlier bound-based approaches.
Clustering of Galaxies with Dynamical Dark Energy<|sep|>In this paper, we have analyzed the eﬀect of dynamical dark energy on the cluster of galaxies by using the standard techniques of statistical mechanics, and found that two-point correlation function in presence of time-dependent dark energy is very close to Peebles’s power law. It indicates that our
Min-Max Regret Problems with Ellipsoidal Uncertainty Sets<|sep|>Minmax regret problems are a cornerstone in robust optimization. Despite their popularity, research has been focusing on only very simple uncertainty sets, which might not reﬂect actual requirements in real-world problems. In this work, we considered minmax regret problems with ellipsoidal uncertainty sets. We gave a thorough discussion of arising problem complexities for the unconstrained combinatorial problem, and the shortest path problem. To solve these problems, two types of cuts that can be used in a scenario relaxation procedure were derived, as well as two linearizations to solve the subproblem of generating new cuts. We compared the performance of these methods in two computational experiments, using unconstrained and shortest path problems as a testbed. We found that the increased complexity of master problems with type 2 cuts are worth the eﬀort, as less iterations are required to solve the minmax regret problem to optimality. The advantage is particularly strong for the unconstrained problem if the values of the deviation matrix C are dense and large. In future research, heuristic solution algorithms should be developed and tested, due to the high computational eﬀort when solving these problems.
Multi-Tubal Rank of Third Order Tensor and Related Low Rank Tensor Completion Problem<|sep|>In this paper, we extended tubal rank to multi-tubal rank and then established a relationship between multi-tubal rank and Tucker rank. The tubal rank focuses on one mode of the tensor, while multi-tubal rank considers all three modes of the tensor together. Based on multi-tubal rank, we established a new tensor completion model and applied a tensor factorization based method for solving the established problem. In addition, we applied spatio-temporal characteristics to the video inpainting and internet traﬃc simulation to modify the established model as a novel one. A modiﬁed tensor factorization based method was presented to solve such data completion problem, which got better performance without increasing the computational cost. Experimental results showed that the performance of our proposed methods were signiﬁcantly better than existing methods in the literature.
A parsec-scale outflow from the luminous YSO IRAS 17527-2439<|sep|>1. IRAS 17527 appears to be a luminous YSO taking birth through disk accretion. A well-collimated parsec-scale outﬂow is discovered in IRAS 17527 in H2 line emission. 2. The outﬂow is seen to be bent in an ‘S’-shaped fashion, suggesting a precession of the jet. There is a tentative detection of a second outﬂow in the H2 image. The possibility of more than one YSO in IRAS 17527 needs to be explored. 3. The H2 image exhibits a strong continuum component in the emission along the outﬂow, which may be caused by radiation escaping through the outﬂow cavity. This, in turn, reduces the radiation pressure on the accreted matter and aids growth of the central source through accretion. 4. The bulk of the emission in the direction of the outﬂow, observed in K and the Spitzer bands, is rotated counterclockwise with respect to the direction of the outﬂow traced by the H2 line emission knots. It is probably a result of the precession of the jet. Spectroscopy in the the near-IR through Spitzer-IRAC bands is required for a proper understanding of this. 5. The model ﬁt to the SED shows that the central source is probably a Class-I protostar; this is supported by its location in the Spitzer-IRAC colour-colour diagram. The YSO has a mass of ∼12.23 M⊙ and a total luminosity of ∼6.17×103 L⊙. 6. The disk parameters and the disk accretion ratio are poorly determined by the SED ﬁtting. This may be caused by the contribution to the source magnitudes from emission from the outﬂow and from a possible companion. Observations at high angular resolution are required in the 2-20 µm wavelength range for a more accurate determination of the source magnitudes at these wavelengths. Acknowledgements. The UKIRT is operated by the Joint Astronomy Centre on behalf of the Science and Technology Facilities Council (STFC) of the UK. The UKIRT data presented in this paper are obtained during the UKIDSS back up time. I thank the Cambridge Astronomical Survey Unit (CASU) for processing the WFCAM data, and the WFCAM Science Archive (WSA) for making the data available. This work makes use of data obtained with AKARI, a JAXA project with the participation of ESA, and IRAS data downloaded from the SIMBAD database operated by CDS, Strasbourg, France. The Spitzer archival data are downloaded from NASA/ IPAC Infrared Science Archive, which is operated by the Jet Propulsion Laboratory, Caltec, under contract with NASA. I also thank the anonymous referee and the editor Malcolm Walmsley for their comments and suggestions which have improved the paper.
Countering Misinformation on Social Networks Using Graph Alterations<|sep|>We demonstrated that it is possible to counter misinformation without explicit identiﬁcation of misinformation content by altering the content propagation dynamics on social network. A major advantage of our approach is that it does not require the system to be able to identify if a particular news item contains misinformation content or not. Furthermore, our approach can be used in conjunction with these detection algorithms to improve the effectiveness of misinformation control while maintaining some of the advantages offered by the network-design-based approach. In our future studies, we will investigate this possibility further. Throughout this work we have assumed that content is either true and false. In reality this clear of a separation between true and false content types is rarely possible. It is possible to extend our methods and algorithms to admit more content types, which can increase the performance of Algorithm 1 since more content types can achieve a more nuanced description of real misinformation dynamics. However, this extension requires modiﬁcations on the problem statement and the formulation of the problem.
Maximum Number of Modes of Gaussian Mixtures<|sep|>One of the motivations to study critical points of Gaussian mixtures comes from the mean shift algorithm. It converges if there are only ﬁnitely many critical points. This was the main goal sought in [17] for Gaussian kernels. As our bound (12) only bounds the number of non-degenerate critical points of Gaussian mixtures, a ﬁnal answer is still open. However, since non-degenerate critical points are isolated, we see that the set of critical points of a Gaussian mixture is ﬁnite if and only if it consists of isolated points, since this set is closed and bounded (compare [8]). Quantitatively, we do not expect our upper bound to be tight. Rather, proving the lower bound �d+k−1 d � for all d, k will be the main focus of a forthcoming paper, extending the technique used to prove Theorem 6. We observe that the construction strategies used in our lower bound can be extended to elliptical distributions, not only Gaussians, and this could be pursued further. For example, an extension of Ray and Lindsay’s concept of ridgeline for mixtures of elliptical distributions is done in [1], including a study of modes for mixtures of two t-distributions. The authors would like to thank Bernd Sturmfels for bringing the problem to their attention and for his supportive comments. Thanks to Peter Green for pointing to the discussion thread in the ANZstat mailing list. Carlos Am´endola was supported by the Einstein Foundation Berlin. Alexander Engstr¨om would like to thank G¨unter Ziegler and Freie Universit¨at Berlin for their hospitality during his sabbatical. The authors also express their gratitude to anonymous referees for their helpful suggestions.
Trail of the Higgs in the primordial spectrum<|sep|>The Higgs eﬀective potential becomes beyond a energy scale of Λ ∼ 1010 GeV, which raises critical doubt for the stability of the Higgs during inﬂation. This instability can be cured by introducing a simple direct coupling, such as a quadratic Higgs-inﬂaton coupling ξφ2h2. The coupling to the inﬂaton plays the role of a heavy eﬀective mass of the Higgs during inﬂation, which can suppress the amplitude of its quantum ﬂuctuation as well as the Hawking-Moss decay rate. We have derived some conditions for which this suppression is eﬀective, which gives a range 8.64 × 10−13 ≲ ξ ≲ 9.05 × 10−6. This coupling is so small that it is very diﬃcult to be observed from collider searches in the near future. However, it can leave distinctive features in the primordial power spectrum of the curvature perturbation during inﬂation. We have calculated carefully the corrections to the power spectrum due to this Higgs-inﬂaton cross term, and found that the corrections give rise to typical logarithmic oscillations with a suppression factor sensitive to the initial condition of the beginning of inﬂation. These corrections can be observed on the largest scales on the CMB, as low-ℓ power modulations for instance, if inﬂation happens only a few e-folds before the largest scale we observe today has left the horizon. We would like to thank Qing-Guo Huang and Ying-li Zhang for useful discussions. JG and SP are grateful to the Kavli Institute for Theoretical Physics China for hospitality during the workshop “Early Universe, Cosmology and Fundamental Physics”, where this work was under progress. We acknowledge the Max-Planck-Gesellschaft, the Korea Ministry of Education, Science and Technology, Gyeongsangbuk-Do and Pohang City for the support of the Independent Junior Research Group at the Asia Paciﬁc Center for Theoretical Physics. This work is also supported in part by a Starting Grant through the Basic Science Research Program of the National Research Foundation of Korea (2013R1A1A1006701). CH is supported by World Premier International Research Center Initiative, MEXT, Japan. In this appendix, we deal with the integrals we have met in (54). One typical integral is � xa±2iµe−2ix, where a is a constant of O(1). It has an analytical result, but what we need is its asymptotic behavior for µ ≫ 1. To see this, we use the Taylor series of the exponent, take the limit µ ≫ 1, and then resum the leading order after the integration. We ﬁnd � xa±2iµe−2ixdx = xa+1±2iµ Usually the range of the integral is taken from 0 to ∞. For the upper limit, add an imaginary negative part iε to x, and the strongly oscillating part in x → ∞(1 − iε) converges to 0 [21]. The lower limit is also zero for a > −1, therefore � ∞
Local Explanation of Dialogue Response Generation<|sep|>Beyond the recent advances on interpreting classiﬁcation models, we explore the possibility to understand sequence generation models in depth. We focus on dialogue response generation and ﬁnd that its challenges lead to complex and less transparent models. We propose local explanation of response generation (LERG), which aims at explaining dialogue response generation models through the mutual interactions between input and output features. LERG views the dialogue generation models as a certainty estimation of a human response so that it avoids dealing with the diverse output space. To facilitate future research, we further propose a uniﬁcation and three properties of explanations for text generation. The experiments demonstrate that LERG can ﬁnd explanations that can both recover a model’s prediction and be interpreted by humans. Next steps can be taking models’ explainability as evaluation metrics, integrating concept-level explanations, and proposing new methods for text generation models while still adhering to the properties.
Population I Cepheids and star formation history of the Large Magellanic Cloud<|sep|>It is believed that close encounters between the Magellanic Clouds and the Galaxy, and/or between the two segments of the Magellanic Clouds has induced episodic star formation in these galaxies. Star formation in the LMC occurs on a large scale, both in terms of total number of stars formed and in the spatial extent. Pietrzynski & Udalski (2000) found three periods of enhanced cluster formation in the LMC at about 7 Myr, 125 Myr, and 800 Myr, in agreement with an earlier ﬁnding of Girardi et al (1995) from an age calibration of integrated colours of star clusters. From a detailed analysis of 20 million stars in the MCPS survey, Harris & Zaritsky (2009) showed that enhanced star formation activities took place in the LMC at roughly 2 Gyr, 500 Myr, 100 Myr, and 12 Myr. Glatt et al. (2010) studied 1193 star clusters in the LMC and found enhanced star formation at 125 Myr and 800 Myr. All these studies are broadly in agreement. Since it is well known that the pulsation period of a Cepheid is a good indicator of its age, Cepheids can therefore be used to reconstruct the star formation history in the LMC. However, as the maximum age of Pop I Cepheids is not expected to be more than 500 – 600 Myr, our study of star formation using these variables provides no information about the details of the star formation episodes in the LMC outside this limited time period. The OGLE catalogue has allowed us to statistically analyse the star formation scenario in the LMC from very accurate period determinations of 1849 FU and 1238 FO Cepheids detected in their third phase of observations. Taking advantage of a large and homogeneous sample of CCs, we have studied the period distribution of CCs and found a bimodal period distribution. Our study shows that these two peaks actually correspond to FU and FO mode Cepheids showing maxima in the distribution at log P = 0.49±0.01 and 0.28±0.01, respectively. When we combined these two class of pulsating stars, after converting periods of FO Cephieds to that of the corresponding FU Cepheids, and employing a period-age relation derived through the LMC cluster Cepehids, we found an age distribution comprising of only a single age maxima with a pronounced peak at the log(Age)=8.2±0.1. Our study shows that the Cepheids are not homogeneously distributed throughout the LMC bar but lie in the clumpy structures. The clumps are more prominent in the southeast region, far-oﬀ from the optical center of the LMC. The enhanced population of Cepheids suggests that the a major star formation episode has taken place at around 125-200 Myr ago in the LMC. However, a combined analysis of CCs and star clusters put this value at around 125 Myr. It is believed that star formation episode at this time scale was triggered due to close encounter between the SMC and the LMC. On a comparison of spatial distribution of CCs and star clusters, a mutual avoiding of clumps of the CCs and star clusters was noticed in the LMC. We are thankful to the referee for constructive comments which has improved the paper. We are grateful to Dr. D. V. Phani Kumar for his assistance that helps to improve the presentation of the results. Thanks to Drs. Luis Balona and Chris Engelbrecht for their valuable inputs. This publication makes use of the catalog by the OGLE microlensing survey.
Maximum of N Independent Brownian Walkers till the First Exit From the Half Space<|sep|>To summarize, we have presented an exact solution for the probability distribution of the maximum m of a set of N independent Brownian motions starting at the initial positions ⃗x ≡ {x1, x2, . . . , xN} on the positive half-axis and the process terminating when any one of the walkers crosses the origin. We have shown that for large m, the pdf of m decays as a power law, PN(m|⃗x) ≃ BN(x1x2 . . . xN)/mN+1 where the prefactor BN has an interesting N dependence. For a ﬁxed N > 1, integer moments of m up to order (N − 1) are ﬁnite, while all higher integer moments are inﬁnite. The cumulative distribution of this maximum also provides an exact solution to the ﬁrst-exit probability
M2-branes Coupled to Antisymmetric Fluxes<|sep|>wrote down the gauge invariance conditions and solved them in some concrete examples. One example is the gauge theory based on A4 algebra. The other class of examples are non-Abelian gauge theories by lifting SU(N) or U(N) to 3-algebras with a Lorentzian signature. The reduction of M2-branes to D2-branes puts addtional In the case of A4, we found all of the components can be ﬁxed up to an overall normalization. For 3-algebras with a Lorentzian signature, only some components can be ﬁxed. We also oﬀered some tentative thoughts on cubic matrices as the representations of 3-algebras. In spite of notorious diﬃculty, this is a direction deserving to follow up in the future. We plan to study some physical consequences of the terms introduced in this paper, and their supersymmetric generalization in a future paper. Acknowledgement: We would like to thank Yushu Song for substantial discussions and Qin-Yan Tan for kind help in programme. This work was supported by
Magnetic complexity as an explanation for bimodal rotation populations among young stars<|sep|>Using detailed 3D MHD models of a solar-like wind, we ﬁnd that mass loss and spin down rates get rapidly suppressed with increasing complexity of the stellar magnetic ﬁeld. Higher order magnetic moments generate a magnetosphere with a larger number of closed magnetic ﬁeld lines that suppress mass loss. For higher order magnetic moments, the spin-down rate is no longer a simple function of the mass loss rate. Both closed and open ﬁeld line regions become more homogeneously distributed over latitude for increasing magnetic complexity. As a consequence, mass loss is no longer equatorially dominated as for the dipolar case, and angular momentum loss is less eﬃcient. Furthermore, the steeper radial decrease of the magnetic ﬁeld magnitude for higher order multipoles results in a smaller Alfv´en surface and a shorter lever arm for magnetic braking. Echoing the MDM of Brown (2014), we propose a new interpretation for the bimodal rotation distribution observed in young clusters: Young solar-like stars have complex magnetic morphologies and lose angular momentum in an ineﬃcient way. During this slow spin-down, magnetic complexity is eroded, precipitating a rapid transition from weak to strong wind coupling. The very rapid and drastic transition to the eﬃcient spin down regime is caused by the very steep dependence of angular momentum loss on magnetic complexity found in this study. CG and OC were supported by SI Grand Challenges grant “Lessons from Mars: Are Habitable Atmospheres on Planets around M Dwarfs Viable?”. OC was also supported by SI CGPS grant “Can Exoplanets Around Red Dwarfs Maintain Habitable Atmospheres?”. JJD was supported by NASA contract NAS8-03060 to the Chandra X-ray Center. Numerical simulations were performed on the NASA HEC Pleiades system under award SMD-13-4526.
Physics-based Shadow Image Decomposition for Shadow Removal<|sep|>In this work, we have presented a novel approach for shadow removal in single images. Our main contribution is to use deep networks as the parameters estimators for an illumination model. Our approach has advantages over previous approaches. Compared to early methods using an illumination model for removing shadows, our deep networks can estimate the parameters for the model from a single image accurately and automatically. Compared to deep learning methods that perform shadow removal via an end-toend mapping, our shadow removal framework outputs images with high quality and much fewer artifacts since we do not use the deep network to output the per-pixel values. Our model clearly achieves state-of-the-art shadow removal results on the ISTD dataset and the proposed video dataset, for both fully-supervised [60] and weakly-supervised [61] settings. Our current approach can be extended in a number of ways. A more physically sophisticated illumination model would help the framework to output more realistic images. It would also be useful to develop a deep-learning based framework for shadow editing via a physical illumination model. This work was partially supported by the NSF EarthCube program (Award 1740595), the National Neographic/Microsoft AI for Earth program, the Partner University Fund, the SUNY2020 Infrastructure Transportation Security Center, and a gift from Adobe. Computational support provided by the Institute for Advanced Computational Science and a GPU donation from NVIDIA. We thank Tomas Vicente, Kumara Kahatapitiya, and Cristina Mata for assistance with the prior ICCV 2019 and ECCV 2020 manuscripts.
Removing Qualified Names in Modular Languages<|sep|>In this paper, we proposed an extension to functional languages with DI/MI/MQ expressions. These expressions are particularly useful for avoiding qualiﬁed names in functional languages. The MQ expressions can be implemented by treating them like exceptions. That is, when an MQ expression is encountered, suspend the current execution, switch to another execution ( That is, evaluating a query with respect to another module) and then resume the suspended execution. Our ultimate interest is to design a module system for Computability Logic [2]–[6].
Bounding Box Disparity: 3D Metrics for Object Detection With Full Degree of Freedom<|sep|>With this paper we publish an open source library1 for multiple 3D metrics for object detection, including the ﬁrst analytic solution and its implementation of volumetric Intersection over Union and volume-to-volume distance for two bounding boxes with full degree of freedom. Further, we introduce the combined metric Bounding Box Disparity. In future work this could be extended such that for non overlapping bounding boxes also the rotation and size differences are considered.
High order semi-Lagrangian methods for the BGK equation<|sep|>This paper presents high order shock capturing semilagrangian methods for the numerical solutions of BGK-type equations. The methods are based on L-stable schemes for solution of the BGK equations along the characteristics, and are asymptotic preserving, in the sense that are able to solve the equations also in the ﬂuid dynamic limit. Two families of schemes are presented, which diﬀer for the choice of the time integrator: Runge-Kutta or BDF. A further distinction concerns space discretization: some schemes are based on high order reconstruction, while other are constructed on the lattice in phase space, thus requiring no space interpolation. Numerical experiments show that schemes without interpolation can be costeﬀective, especially for problems that do not require a ﬁne mesh in velocity. In particular, BDF3 without interpolation appears to have the best performance in most tests. Future plans includes to extend such schemes to problem in several space dimension and treat more general boundary condition.
Stochastically excited oscillations on the upper main sequence<|sep|>The potential to detect stochastically excited oscillations in stars signiﬁcantly more massive than the Sun became possible only with the space missions CoRoT and Kepler. It is not only the quality of the data which is outstanding, but also the uninterrupted and long observing seasons which demonstrated that we need to revisit our knowledge on pulsation mechanisms (Fig. 1). Convection is one of the most complex processes in astrophysics and therefore often neglected with the argument that convection zones in stars hotter than early F are negligible. However, the latest results reviewed in this article demonstrate that the impact of convection needs to be taken into account.
Planar wiggler as a tool for generating hard twisted photons<|sep|>Let us brieﬂy recapitulate the results. Starting from the general formulas for the radiation probability of twisted photons with quantum recoil taken into account that were derived in [26], we obtained simpler formulas under the assumptions that the radiated photons are paraxial and the energy of a charged particle as it is given by the Lorentz equations can be regarded as a constant. The radiation of both scalar and Dirac charged particles was investigated. As an application of these general formulas, the explicit formulas for the radiation probability of twisted photons by charged particles in planar undulators were deduced. It was proved that the selection rule established in [24] for the radiation of twisted photons by charged particles in planar undulators when the quantum recoil is discarded holds also in the case when this recoil is included into the theory. The numerical simulations for the radiation of twisted photons in the planar wiggler and the crystalline undulator showed a good agreement with the general formulas derived.
Can we measure the neutrino mass hierarchy in the sky?<|sep|>The shape of the matter power spectrum contains information, in order of decreasing sensitivity, about the sum of neutrino masses, the amplitude of the mass splitting and the hierarchy (i.e., the mass splitting order). We have introduced a novel parameterization of the neutrino mass hierarchy, ∆, that has the advantage of changing continuously between normal, degenerate and inverted hierarchies and whose sign changes between normal and inverted. The absolute value of ∆ describes the maximum mass diﬀerence between the eigenstates. We stress that, current constraints from neutrino oscillations have ruled out large part of the parameter space given by the sum of the masses and the ∆ parameter, leaving two narrow regions: for a ﬁxed value of the total mass, the value of ∆ for the normal hierarchy is related to that of the inverted one and ∆NH ≃ −∆IH (but, in detail, ∆NH ̸≡ |∆IH|). It is the allowed region that cosmology should explore. We found that the information about ∆ accessible from the power spectrum shape yields a degeneracy: parameters values ∆ and −∆ yield nearly identical power spectra and therefore that the likelihood surface in ∆ is bimodal. This was not noted in the literature before and not taking this into account when using the Fisher matrixapproach to forecast future surveys performance may lead to spurious indications of a surveys ability to determine the hierarchy. Detecting the signature of the hierarchy in the sky is therefore extremely challenging, and therefore we asked: “can cosmology in the cosmic-variance limit, Figure 5. Role of cosmology in determining the nature of neutrino mass. Future neutrinoless double beta decay (0νβbeta) experiments and future cosmological surveys will be highly complementary in addressing the question of whether neutrinos are Dirac or Majorana particles. Next generation means near future experiments whose goal is to reach a sensitivity to the neutrinoless double beta decay eﬀective mass of 0.01 eV. We can still ﬁnd two small windows where this combination of experiments will not be able to give a deﬁnite answer, but this region is much reduced by combining 0νβbeta and cosmological observations. and for an ideal experiment, distinguish the neutrino heirarchy?” or in other words, “is there enough information in the sky to measure the neutrino hierarchy?” To address these questions we have considered ideal, full-sky, cosmic variance-limited surveys and found that substantial Bayesian evidence (ln B ≥ 1) can be achieved. Are such a surveys feasible in the next 5-10 years? There are two candidates for such surveys : a full extragalactic survey in the optical/infrared like Euclid∥ [32] and a full 21cm survey by the SKA¶. Each of these surveys is scheduled to start operations by 2018. Euclid will make an all sky Hubble-quality map for weak lensing and will directly trace the dark matter using this technique; whilst the cosmic variance limited survey we consider here is ambitous with respect to this survey these result serve as a qualitative measure of this surveys expected performance (costraints should be only a factor ≤ 1.5 larger at worst). Euclid will also target emission line galaxies up to z ∼ 3 (therefore these galaxies will have bias parameter close to 1) however nP, quantifying the the ratio of the signal to shot noise, will be only slightly above 1. The 21cm surveys provide the most un-biased indirect tracer of the dark matter distribution in the Universe and have negligible shot noise. For the degenerate and inverted mass spectra, the next generation neutrinoless double beta decay experiments can determine if neutrinos are their own antiparticle. For the normal hierarchy, the eﬀective electron-neutrino mass may even vanish. However, if the large-scale structure cosmological data, improved data on the tritium beta decay, or the long-baseline neutrino oscillation experiments establish the degenerate or inverted mass spectrum, the null result from such double-beta decay experiments will lead to a deﬁnitive result pointing to the Dirac nature of the neutrino mass. This is summarized in ﬁgure 5. If the small mixing in the neutrino mixing matrix is negligible, cosmology might be the most promising arena to help in this puzzle. Our work shows that depending on the total neutrino mass, there might be substantial evidence by cosmological data to infer the neutrino hierarchy. CPG is supported by the Spanish MICINN grant FPA-2007-60323 and the Generalitat Valenciana grant PROMETEO/2009/116. LV acknowledges support from FP7PEOPLE-2007-4-3-IRG n. 202182 and FP7- IDEAS Phys.LSS 240117. LV and RJ are supported by MICINN grant AYA2008-03531. LV & RJ acknowledge support from World Premier International Research Center Initiative (WPI initiative), MEXT, Japan. TDK is supported by STFC Rolling Grant RA0888. TDK thanks ICCUBIEEC, University of Barcelona and Instituto de F´ısica Corpuscular (CSIC-UVEG), Val`encia, for hospitality during part of this work. We thank Alan F. Heavens for useful discussions.
The feature of shadow images and observed luminosity of the Bardeen black hole surrounded by different accretions<|sep|>It is commonly known that the black hole is surrounded by the accretion matter, and the distribution of accretion plays a key role in the imaging results of black holes. In the context of Bardeen spacetime, we have investigated the black hole shadow and observation luminosity under the diﬀerent accretion models. In particular, we pay close attention to the diﬀerent observation characteristics of the distant observer when the related physical quantities is the diﬀerent values. When the black hole is surrounded by the spherically symmetric accretion ﬂow, we consider two behaviors of the accretion, i.e., the static and infalling accretion model. For the case of static spherically symmetric accretions, one can ﬁnd that the observed luminosity increases with the increase of the monopole charge g, while the photon sphere and radius of black hole decrease. In addition, the maximum observation luminosity always appears in the region of photon sphere under the diﬀerent values of monopole charge g. From the observed intensity density map, one can see that the central region of the shadow is not completely dark, and the observation brightness of the central region increases slightly with the increase of g. In the infalling accretion model, the maximum observation luminosity is obviously lower than that of the static model, which is the most prominent diﬀerence between these two models. The Doppler eﬀect of infalling matter leads to the diﬀerence between the two kinds of spherical accretion. As a result, the observed image in the infalling accretion model is darker than the static one under the same state parameter. On the other hand, the change of observation intensity is more obvious when the monopole charge g takes more larger value. The maximum observation luminosity with g = 0.75 is about twice that of g = 0 (Schwarzschild black hole). It must also be mentioned that in both spherical accretion models, there is a great diﬀerence in the observed luminosity, but the position of photon sphere and the shadow radii unchanged under the same parameter, which means that the shadow is an inherent property of spacetime and not aﬀected by accretion ﬂow behavior. By considering the Bardeen black hole surrounded by a geometrically and optically thin disk accretion, we have distinguished the trajectory of light ray near black hole. Then, we take three diﬀerent emission models for the thin disk accretion ﬂow. For the observer at inﬁnity (r0 → ∞), there is not only a dark central area, but also the photon rings and lensing rings outside of black hole shadow. In the diﬀerent emission models, the maximum observation luminosity and the size of central dark area are diﬀerent. The thin disk serves as the only background light source, the light ray in the region of photon rings can accretes through the thin disk more than three times, and gains enough extra brightness. However, the photon rings cannot be observed directly for the observer, due to the image of the photon ring region is extremely demagnetized and the photon ring region is very narrow. In addition, the image of lensing ring area with the high demagnetization, leads to the lensing rings can only provide a very small part of the total observed ﬂux. Therefore, the observation intensity are dominated by direct emission in these three diﬀerent emission models. Although these results are obtained with the ideal models, it is very important to push forward the theoretical study of black hole shadow as far as possible and lay the foundation for the observation results. This work is supported by the National Natural Science Foundation of China (Grant Nos. 11875095, 11903025), the Sichuan Youth Science and Technology Innovation Research Team(21CXTD0038), and Basic Research Project of Science and Technology Committee of Chongqing (Grant No. cstc2018jcyjA2480).
Mild solutions to the dynamic programming equation for stochastic optimal control problems<|sep|>In this paper it is shown, via nonlinear semigroup theory in L1, both the existence and the uniqueness of a mild solution for the dynamic programming equation for stochastic optimal control problem with control in the volatility term. Latter problem is related to the analysis of controlled stochastic volatility models, within the ﬁnancial frameworks, whose related computational study is the subject of our ongoing research.
Weak associativity and deformation quantization<|sep|>In this manuscript we have shown that the requirement of the associativity of star products can be relaxed in such a way that new star product will satisfy an important for physical applications properties, like the Moufang identities, alternative identities, etc. We substitute the condition of the associativity by the requirement (15) meaning that the star multiplication should be alternative. The condition (15) is gauge invariant from the point of view of deformation quantization. We show that in this case an integrated associator vanishes, which can be used to show that the non-associativity will not manifest itself on the physical observables. At the same time, the condition (15) implies the restrictions on the classical bracket {f, g} = P ij(x)∂if∂jg, it should be a Malcev-Poisson bracket (24). We discuss physical meaning of this restriction and show that on the Malcev-Poisson manifold there is a weaker form of the classical Poisson theorem, which can be used for the construction of integrals of motion. Finally we note that there are some diﬀerent approaches in treating the non-associative systems within the framework of deformation quantization. For exemple, one may require the associativity of the star product only for the physical observables [33], or introduce new elements to the original algebra which will make it associative [13,34]. The parts of this work were presented on the conference Non-commutative ﬁeld theory and quantum gravity, Corfu, Greece, 2015, and submitted to the proceedings of Corfu Summer Institute [35]. I appreciate the valuable remarks of Ivan Shestakov and Richard Szabo, I am also grateful to Jim Stasheﬀ for correspondence. This work was supported in part by FAPESP, projects 2014/03578-6 and 2016/04341-5, and CNPq, project 443436/2014-2.
Scintillation properties of pure and Ce$^{3+}$-doped SrF$_2$ crystals<|sep|>The SrF2 crystal are well suited for use as gamma radiation detector. It has a higher (4.18 g/cm3) than NaI-Tl (3.67 g/cm3) density, comparable light output, and it is no hygroscopic. Taking into account that crystals SrF2 - 0.3 mol.% Ce3+ and SrF2 1 mol.% Ce3+ have a high temperature stability of light output in the temperature interval from -50 ◦C to 200 ◦C, these materials can be applied in well-logging scintillation detectors. Summarizing the experimental results we conclude that strontium ﬂuoride crystals would be useful as newly perspective scintillator. This work was partially supported by Federal Target Program ”Scientiﬁc and scientiﬁc-pedagogical personnel of innovative Russia” in 2009-2013 and Russian Foundation for Basic Research (RFBR). Authors are thankful to V. Kozlovskii for growing the crystals investigated in this work.
Interplay between destructive quantum interference and symmetry-breaking phenomena in graphene quantum junctions<|sep|>We investigated the interplay between destructive QI and symmetry-breaking phenomena involving the spin and valley degrees of freedom in graphene nanoﬂakes. Speciﬁcally, by establishing a relation between the analytic structure of the real-space Green’s function and the symmetries of the Hamiltonian, we provide a clear understanding of the origin of the QI antiresonances and of their eﬀects on ballistic transport. Interestingly, our analysis works both in the symmetric and in the symmetry-broken cases, and we show that breaking a symmetry shifts the position of the antiresonance, without spoiling the destructive QI eﬀects. This demonstrates the generality and the robustness of the phenomenon within a generic theoretical framework and also in the presence of electron-electron interactions. Ultimately, it allows us to predict the occurrence of QI antiresonances in complex nanostructures interacting with the environ ment. In our original proposal in Ref. 5, we showed that destructive QI can be used to generate nearly completely spin-polarized currents in the absence of magnetic ﬁelds or spin-orbit coupling, simply exploiting the spontaneous breaking of the SU(2) spin-rotational symmetry induced by electronic correlation in the presence of the edges. The present work extends the scope of our previous study to multi-component systems where other degrees of freedom (e.g., valley, orbital, layer) can be manipulated via an external handle lifting the symmetry. In the speciﬁc case considered here, the mechanisms involved are the onset of a magnetically ordered state (associated with the electron spin) and the breaking of the inversion symmetry due to the interaction with a speciﬁc substrate (associated with the valley). We show that a tuning the coupling between the nanoﬂake and the substrate can turn a spin ﬁlter into a valley ﬁlter. The approach developed in the present work follows a general scheme according to which it is possible to manipulate the transport properties of a quantum junctions exploiting destructive QI, provided we have identiﬁed a symmetry and the corresponding symmetry-breaking control parameter. Other mechanisms suitable to this purpose are: (i) the switchable magnetic bistability of metal-organic complexes,62 (ii) the Jahn-Teller distortions in charged fullerenes,63 and (iii) the formation of a moir´e pattern in twisted bilayer graphene junctions.64 In this respect, we believe that our work can drive the community towards a promising and -so far- only sporadically explored direction. We thank R. Stadler and C. Lambert for valuable discussions. We acknowledges ﬁnancial support from MIUR PRIN 2015 (Prot. 2015C5SEJJ001) and SISSA/CNR project ”Superconductivity, Ferroelectricity and Magnetism in bad metals” (Prot. 232/2015) and from the H2020 Framework Programme, under ERC Advanced Grant No. 692670 ”FIRSTORM”. V.B. acknowledges support from Regione Lazio (L.R. 13/08) through project SIMAP. A.V. acknowledges ﬁnancial support from the Austrian Science Fund (FWF) through the Erwin Schr¨odinger fellowship J3890-N36.
Darboux Transformation and Exact Solutions of the Myrzakulov-Lakshmanan-II Equation<|sep|>In this paper, we have considered the integrable ML-II equation which is one of (2+1)-dimensional generalizations of Heisenberg ferromagnetic equation with self-consistent potentials. We have derived the DT for the ML-II equation. Using DT and seed solutions, we have constructed the 1-soliton solution and 2-soliton solution. Similarly, using the higher order DT, one can also generate multi-soliton solutions as well as other types exact solutions like positon, rogue wave, breather etc. There are still some questions which are still open. One of main questions is the physical interpretations and observation of the above constructed soliton solutions in magnetic physics. We hope at least some of these open questions will closed in future investigations. Another interesting question is the dynamical interactions of solitons which also can be analysed in the DT.
Radio polarimetry of compact steep spectrum sources at sub-arcsecond resolution<|sep|>We have presented multi-frequency VLA polarisation observations of CSSs and estimated their percentage polarisations and RM values. About half of the sources are point-like, even at the ∼ 0.1×0.1 arcseconds resolution achieved by the present VLA A-array 23.2-GHz observations. The remaining sources have double or triple structures. One source, 1210+134 (4C13.46) shows a complex structure. Low values for the polarisation percentage in CSSs is conﬁrmed by the present observations. On the average, quasars are more highly polarised than galaxies. We have compiled the available RM estimates for CSS sources as seen on a sub-arcsecond scale. These show a wide range of values, with indications of very large RMs, although some of the values need to be conﬁrmed via observations at a larger number of frequencies. Values of RMrf as high as ≈36 000 rad m−2 have been found. CSS galaxies are characterized by RM values that are larger than those found for CSS quasars. A majority of the objects show very large values of RMrf . Acknowledgements. The VLA is operated by the U.S. National Radio Astronomy Observatory, which is a facility of the National Science Foundation operated under a cooperative agreement by Associated Universities, Inc. This research has made use of data from the MOJAVE database that is maintained by the MOJAVE team (Lister et al., 2009). It has also used the NASA/IPAC Extragalactic Database (NED) which is operated by the Jet Propulsion Laboratory, California Institute of Technology, under contract with the National Aeronautics and Space Administration, and NASA’s Astrophysics Data System. We regret that it took so long to make these data public. We are very grateful to the referee, Prof. Ralf Spencer, for the very helpful
Rapid estimation of drifting parameters in continuously measured quantum systems<|sep|>a method that improves the computational overhead to carry out the estimation analysis. There is a trade oﬀ between duration of the data window and of estimation accuracy. Given a target precision of the estimate, we have shown that this sets the temporal resolution on the drifting parameter, and illustrated that it can work by simulating measurement results. If the parameter is changing in an irregular fashion, a major conclusion is that weak continuous monitoring can faithfully track the parameter, and that the method we have described here is statistically optimal for that measurement strategy. We also showed that the method is computationally eﬃcient for such tracking applications, scaling only linearly with the number of time points used in the estimation once an initial frequency range has been identiﬁed. This capability opens the door for real-time parameter tracking combined with adaptive feedback [4] for parameter stabilization. The question of how to extend these methods to multiple quantum systems, and estimate parameters such as an interaction energy is an important open question that will be pursued in subsequent work. Note added: After the public presentation of these results at the APS March meeting [28], but before the posting of our preprint, an independent work covering some of the same physics was posted on the arXiv, authored by Kiilerich and Mølmer [29]. We thank Irfan Siddiqi, Alexander Korotkov and Shengshi Pang for helpful discussions. This work was supported by US Army Research Oﬃce Grants No. W911NF-15-1-0496, No. W911NF-13-1-0402, by National Science Foundation grant DMR- 1506081, by John Templeton Foundation grant ID 58558, by Development and Promotion of Science and Technology Talents Project Thailand, and by the National Council of Science and Technology (CONACyT) Mexico. We also acknowledge partial support by Perimeter Institute for Theoretical Physics. Research at Perimeter Institute is supported by the Government of Canada through Industry Canada and by the Province of Ontario through the Ministry of Economic Development & Innovation. ∗ L.C. and A.C. contributed equally to this work.
Ionised gas abundances in barred spiral galaxies<|sep|>We have carried out a detailed analysis of the nebular abundances along the bar and in the bulge of a sample of 20 early– type galaxies to compare them with the stellar abundance distributions obtained in previous works using line-strength indices. We focus on several relations, in particular, the distribution of abundances and star formation and the properties of the galaxies; the interpretation of the ionizing mechanisms and the relative inﬂuence of the AGN; the relation between abundances and rotational velocities and velocity dispersion in the bulge. We have found that most of the emission in our sample of galaxies are concentrated in the bulge region. We have used several methods to estimate nebular abundances. Considering different methods for star-forming galaxies, does not change our results about gaseous metallicity gradients nor change the comparison with the stellar metalicities. In three galaxies the estimated nebular abundances lie clearly below the stellar metallicities. Furthermore, we see that the galaxies with the lowest abundances are those with the largest rotational velocities, and although there is a large dispersion in the values, this eﬀect might be the result of a central rejuvenation mechanism in the most massive and late-type galaxies. The comparison between gaseous and stellar component shows that in most galaxies the slope for the nebular abundance distribution in the bulge region is shallower than that of the stellar metallicity. We can conclude that the combination of observations of both gas and stellar metallicities is crucial to determine the star formation history in galaxies. The study of both phases is very useful to learn about the origin of the gas involved in the star formation; for instance, we clearly see galaxies for which external accreted gas is the fuel source for the present star formation. Due to the small parameter range covered by our sample we can not conclude anything about the role of bars in the mixing, but we observe a variety of gas and stellar metallicities distributions that indicate very diﬀerent mixing and inﬂow histories from galaxy to galaxy. Therefore, it would be very interesting to complete this study with a wider sample, including later-type and unbarred galaxies. Acknowledgements. We are grateful to the referee for the useful comments and suggestions that have improved the manuscript. We acknowledge the usage of the HyperLeda database (http://leda.univ-lyon1.fr). This research has been supported by the Spanish Ministry of Science and Innovation (MICINN) under grants AYA2011-24728, AYA2010-21322-C03-02, AYA2010-21322-C0303, AYA2007-67625-C02-02 and Consolider-Ingenio CSD2010-00064, and by the Junta de Andaluc´ıa (FQM-108). PSB acknowledge support from the Ramon y Cajal Program nanced by the Spanish Ministry of Science and Innovation. PSB also acknowledges an ERC grant within the 6th European Community Framework Programme.
Hiding neutrino mass in modified gravity cosmologies<|sep|>In this work, we have explored the nature of the degeneracy between neutrino masses and modiﬁed gravity. We focussed on widely applicable results by using the Horndeski generalized scalar-tensor theories of gravity. These generic models contain, in the linear regime, few redshift dependent functions that here are tuned to maximize the impact on the neutrino mass determination. We have studied the parameter space of the number of modiﬁed gravity parameters (6) in the model for several neutrino mass schemes and considered mock state
Particle Acceleration in Collapsing Magnetic Traps with a Braking Plasma Jet<|sep|>We have identiﬁed new types of trapping in CMTs with the addition of a braking jet, which (crucially) cannot be obtained using earlier models. The braking jet allows trapping of test particle orbits in the central jet region, in the loop legs and a combination of the two. The regions accessible to orbits are determined by the initial conditions (such as initial position with respect to the front, initial pitch angle and the trap parameters). Particle orbit energy gains also depend on the region which traps them, with orbits which are trapped in the loop legs Table 5. Maximum and average kinetic energies (in keV) obtained for each type of particle orbit for parameter values and initial conditions shown in Table 3. Average energies are calculated by averaging the maximum energies achieved by each particle orbit for each orbit type. 1 50.9 18.1 73.1 17.5 2a 65.4 45.4 95.2 74.5 2b 49.4 29.4 86.2 34.6 2c 93.3 49.1 3 17.2 8.9 21.7 11.1
An experimental data-driven mass-spring model of flexible Calliphora wings<|sep|>A ﬂexible blowﬂy wing model has been developed based on the experimental data. The sophisticated structures of the wings were taken into account by distinguishing the vein and the membrane during the meshing procedure. The membrane was modeled as a 2D planar sheet whose tensile strength was much larger than its bending stiﬀness and the veins were modeled as rods whose bending stiﬀness values were calculated based on their ﬂexural rigidity EI. While the second moment of area I can be estimated using the vein diameters, the Young’s modulus E remains somewhat uncertain due to the vast range of known cuticle’s property [29]. As mechanical properties of insect wings are essential for insect ﬂight aerodynamics, we here presented a numerical method to evaluate the Young’s modulus of veins and the joint stiﬀness of blowﬂy wings. The mathematical optimization tool CMAES [46] was employed for determining the right elastic properties by comparing the wing model with static experimental measurements. The method allowed us to ﬁnd appropriate stiﬀness values for approximating the static deformation behavior of real insect wings under external point forces. We obtained here nine sets of stiﬀness parameters for the Calliphora wing model. The high-resolution numerical simulations of a Calliphora wing model with the optimized stiﬀness, ﬂapping in a moving airﬂow, allowed us to gain insight into the dynamic behavior of insect wings, as well as the inﬂuence of wing ﬂexibility on the aerodynamic performance of insects. Firstly, we performed numerical experiments with the full set of stiﬀness parameters optimized based on the measurements conducted on nine diﬀerent individuals. We found that even though wing stiﬀness can vary among individuals, their aerodynamic properties are very similar by comparing dimensionless parameters at the same Reynolds number. With this conclusion, it is necessary to point out that our ﬁndings are restricted to a simple selected wing kinematics pattern. A ﬂy might adapt the wing kinematics according to its wing stiﬀness. This hypothesis can be tested out by optimizing the wing kinematics based on wing stiﬀness. However, such studies are computationally expensive or even prohibitive and are left for future work.
Risk Measurement, Risk Entropy, and Autonomous Driving Risk Modeling<|sep|>In self-driving era, automotive engineers and actuaries, just like statistical physicists, can observe and study the macro-and micro-state risks of autonomous vehicles with new generation sensing and data collection technology. The risk modeling method proposed in this paper is derived from the research results of vehicle dynamics and traﬃc engineering. Based on the information theory, the criteria for the optimization model are established. This new approach will signiﬁcantly contribute for actuarial modeling in autonomous driving scenarios. This research has achieved, (a) To measure and estimate the crash risk from a microscopic state. A uniﬁed risk index, risk entropy, is proposed, and Discrete Markov Chain is used to describe the dynamic mechanism of the stochastic process of accidents; (b) A good causal interpretation capability. The new model can demonstrate how mistakes of autonomous driving system and road traﬃc conditions aﬀect the occurrence of accidents, which is very suitable for generating simulation data to price emerging risks; and (c) Can be extended to model human driver risk. Adding aggressive driving decision-making tactics can model human driving risks [18]. In addition to this research, this new generation risk models will not only serve insurance industry, but also become a safety algorithm software of autonomous vehicles, because safety-ﬁrst criteria applies to both automotive industry and insurance industry. Acknowledgements This study was supported by the selfdriving simulation datasets of Chainlink Co. Ltd. The author thanks Dr. Long Tai Chen for his advice and support.
Multiple-isotope pellet cycles captured by turbulent transport modelling in the JET tokamak<|sep|>The JINTRAC integrated modelling framework with the turbulent transport model QuaLiKiz as the turbulent transport model and HPI2 as the pellet deposition model successfully reproduced observations over multiple pellet cycles in JET mixed-isotope experiments. Good agreement on the density proﬁle evolution and on the neutron rate timescales was achieved. The compensation between R/Ln stabilization and R/LT destabilization was shown to lead to maintained ITG drive and allow prompt deuterium penetration on energy conﬁnement timescales following each pellet injection throughout the pellet train. The key QuaLiKiz prediction of ITG instability in post-pellet negative R/Ln regimes was veriﬁed by linear and nonlinear GENE simulations. The same core modelling approach presented in this Paper can be used to predict the timescale for the fuel penetration in ITER and future reactors, with optimistic preliminary results with regard to fuelling capability and burn control.
Automatic Segmentation, Localization, and Identification of Vertebrae in 3D CT Images Using Cascaded Convolutional Neural Networks<|sep|>In this paper, we propose a multi-stage framework for segmentation, localization and identiﬁcation of vertebrae in 3D CT images. A novelty of this framework is to divide the three tasks into two stages. The ﬁrst stage is multi-class segmentation of cervical, thoracic, and lumbar vertebrae. The second stage is iterative instance segmentation of individual vertebrae. By doing this, the method successfully works without a priori knowledge of which part of the anatomy is visible in the 3D CT images. This means that the method can be applied to a wide range of 3D CT images and applications. In the experiments using two public datasets, the method achieved the best Dice score for volume segmentation, and achieved the best mean localization error and identiﬁcation rate. As far as we know, this is the ﬁrst uniﬁed framework that tackles the three tasks simultaneously with the state of the art performance. We hope that the proposed method will help doctors in clinical practice.
Space-compatible cavity-enhanced single-photon generation with hexagonal boron nitride<|sep|>We have demonstrated coupling of a quantum emitter hosted by multilayer hBN to a confocal microcavity. The hemispherical geometries have been fabricated using FIB milling with sub-nm precision. The cavity mode volume is of the order of λ3. The cavity improves the spectral purity of the emitter substantially, with the FWHM decreasing from 5.76 to 0.224 nm. Moreover, the cavity suppresses oﬀ-resonant noise, which allows us to improve its single-photon purity. The excited state lifetime of the emitter is also shortened by the Purcell eﬀect by a factor of 2.3. The emission of the cavity is linearly polarized and stable over long timeframes, with no signs of photobleaching or blinking. The cavity also features a linearly tunable PDMS spacer between both mirrors, which allows in-situ tuning of the single-photon line over the full free-space ZPL of the quantum emitter. This would allow us to fabricate multiple identical single-photon sources, by locking all to the same emission wavelength, making this approach fully scalable. Furthermore, the complete SPS is portable and fully self-contained within 10 × 10 × 10 cm3, the size of a 1U CubeSat. This makes the single-photon source a promising candidate for low cost satellite-based long-distance QKD, especially as the quantum emitters in hBN are space-certiﬁed. Despite the source’s performance being not yet suﬃcient for oneway quantum computing, using the single-photon source for QKD even now enhances the quantum key generation rate on useful distances. The microcavity platform can also be easily adapted to other quantum emitters in 2D materials and oﬀers a promising path towards scalable quantum information processing. This work was funded by the Australian Research Council (CE110001027, FL150100019, DE140100805, DP180103238). We thank the ACT Node of the Australian National Fabrication Facility for access to their nano- and microfabrication facilities. We also thank Hark Hoe Tan for access to the TRPL system. R.L. acknowledges support by an Australian Government Research Training Program (RTP) Scholarship. Borosilicate glass substrates with a size of 18 × 18 mm2 × 160 µm have been coated with 100 nm gold using electron-beam thermal evaporation to prevent substrate charging eﬀects. The ion accelerating voltage in the FIB (FEI Helios 600 NanoLab) is 30 kV with currents ≤ 0.28 nA. The dose rate is encoded in the RGB color of a hemispherical pixel map. The dose rate to RGB value was carefully calibrated using AFM measurements. During the milling process we add I2-gas, which ensures a smooth surface. Finally, the gold ﬁlm is chemically etched using a custom-made potassium iodide (KI:I2:H2O with ratio 4:1:40 by weight) solution. Surface characterizations before and after the KI-etching show no diﬀerence in radius or roughness. We also tried hydroﬂuoric acid to etch the hemispheres, but for the feature sizes required for the cavity we could not achieve a smooth surface. We calibrated the deposition rate of the sputter coater (AJA) using variable angle spectroscopic ellipsometry (JA Woollam M-2000D), which measures ﬁlm thickness and refractive index. At 565 nm we found nSiO2 = 1.521, nTiO2 = 2.135 and nMgF2 = 1.390. The deposition was done at room temperature. For the highly reﬂective coating we deposit alternating layers of SiO2/TiO2 with thickness of λ/4n and the SiO2 terminating the mirror. Due to the refractive index of MgF2 being roughly in the middle between glass and air, the backsides of the substrates are coated with one quarter wave layer of MgF2, serving as an anti-reﬂective coating. To maximize the escape eﬃciency into one particular direction it is common to make one of the stacks thicker (e.g. 10:9), so the photons couple primarily into a single direction. For simplicity, we used 9:9 stacks, which thus introduces 50% loss.
Randomness for Free<|sep|>In this work we have presented a precise characterization for classes of games where randomization can be obtained for free in transition functions and in strategies. As a consequence of our characterization we obtain new undecidability results. The other impact of our characterization is as follows: for the class of games where randomization is free in transition function, future algorithmic and complexity analysis can focus on the simpler class of deterministic games; and for the class of games where randomization is free in strategies, future analysis of such games can focus on the simpler class of pure strategies. Thus our results will be useful tools for simpler analysis techniques in the study of games, as already demonstrated in [6, 7, 8, 9, 16, 17]. Finally, note that it can be expected that randomness would not be for free in both the transition function and the strategies, and the results of this paper show that the classes of games in which randomness is for free in the transition function (Table 2) are those in which randomized strategies are more powerful than pure strategies (Table 3), i.e. randomness is not for free in strategies when randomness is for free in the transition function.
Deep high-resolution X-ray spectra from cool-core clusters<|sep|>The deep XMM-Newton RGS spectra of these objects show Fe XVII line emission, indicating material around 0.5 keV in temperature. The width of the emission lines and the spatial extent of the cold gas seen by Chandra show that the regions they are emitted from are resolved. Assuming pressure equilibrium, we can see that this cool component is not volume ﬁlling in Abell 262 and Abell 3581 and likely consists of cool blobs such as those seen in 2A 0335+096 or ﬁlaments as in Perseus or Centaurus. The strength of these Fe XVII emission lines and lack of O VII emission lines show that there is substantially less material seen at lower temperatures than expected in the cooling ﬂow picture, as seen in previous studies. The multiphase nature of this cold material presents problems for heating mechanisms which are preventing the bulk of the material from cooling. The coolest material requires four times more heat than its immediate surroundings. Any heating mechanism stopping this material from cooling needs to target it in some
A thermodynamically consistent quasi-particle model without density-dependent infinity of the vacuum zero point energy<|sep|>To summarize, in this paper the improved quasiparticle model proposed in Ref. [34] is generalized from ﬁnite temperature and zero chemical potential to the case of zero temperature and ﬁnite chemical potential in order to calculate the EOS of (2+1) ﬂavor QCD at ﬁnite chemical potential and zero temperature. We ﬁrst calculate the partition function at ﬁnite temperature and chemical potential, then go to the limit T = 0 and obtain the EOS for cold and dense QCD. Furthermore, we use this EOS to calculate the quark-number density, the energy density, the quark-number susceptibility and the speed of sound. We can see that as the chemical potential increases, the behaviors of the pressure, the energy density, the quark-number susceptibility and the speed of sound tend to the free quark gas result. A comparison between our EOS and one prominent example of the EOS of QCD, the cold, perturbative EOS of QCD proposed by Fraga, Pisarski, and Schaﬀner-Bielich in Ref. [40] is made. The quark-number density diﬀers signiﬁcantly from the Fermi distribution of free quark theory. Physically this is a consequence of dynamical chiral symmetry breaking in the low energy region. It is found that when µ is below a critical value µ0, the quark-number density vanishes. This feature agrees with the general conclusion in Ref. [41]. The value µ0 obtained here is almost of the same order of magnitude as the estimate made in Ref. [41]. From the comparison between our results of energy density and the speed of sound and the corresponding ones of Fraga et al., we can see that when µ is smaller than 500 MeV, our model is more valid. Finally, we would like to stress that the lack of MC data on dense strongly interacting matter at zero temperature leads to the uncertainty of phenomenological parameters of quasi-particle model at zero temperature and ﬁnite chemical potential. In our future work, we hope to use the most recent astronomical observations [16] to constrain the parameters of the quasi-particle model at ﬁnite chemical potential.
Effective carrying capacity and analytical solution of a particular case of the Richards-like two-species population dynamics model<|sep|>We have proposed a generalized two-species population model and have found its analytical solution for the commensalism and amensalism regimes. From this result, we are able to show that the solution of the two-species model is equivalent when one considers one-species model with a time dependent extrinsic growth factor. Also, the extrinsic growth factor can be incorporated into the carrying capacity and the full equivalence, i.e. steady state and transient solution, is retrieved for the Gompertz model. Otherwise, the equivalence persists only for the steady state solutions. This feature reveals an important aspect of sigmoid growth curves. Considering two well known growth models, Verhulst and Gompertz , besides presenting a similar sigmoid be havior, they are diﬀerently aﬀected by an extrinsic growth factor. For the Gompertz model, the extrinsic growth factor can be fully incorporated into the carrying capacity; i.e. removing individuals from the population has the same eﬀect if one considers that this population grows in a more limited environment. Any kind of external inﬂuence can be seen as a modiﬁcation in the environment limitations. Although the extrinsic factor can also be incorporated in the Verhulst model, it correctly describe only the steady state solution, the equivalence is lost when one considers the transient solution. B. C. T. C. acknowledges support from CAPES. F. R. acknowledgessupport from CNPq (151057/20095). A. S. M. acknowledges the Brazilian agency CNPq (305738/2010-0 and 476722/2010-1) for support.
The Atlas3D Project -- XI. Dense molecular gas properties of CO-luminous early-type galaxies<|sep|>Using the IRAM 30m telescope, we have surveyed the 13CO, HCN and HCO+ emission of the 18 galaxies with the strongest 12CO emission in the ATLAS3D sample of nearby early-type galaxies. We detect all 18 galaxies in both 13CO(1-0) and 13CO(2-1) transitions, 12/18 in HCN(1-0) and 10/18 in HCO+(1-0). Fitting the 12CO(1-0) lines with a double-peak or Gaussian proﬁle and evaluating the reduced χ2 value, we ﬁnd that one third of the galaxies are best ﬁt by a Gaussian, one third with a clear doublepeak proﬁle and the remaining one-third with a very ﬂat-topped ‘double-peak’ proﬁle. Two galaxies (NGC 1266 and NGC 5866) have clearly different proﬁles in the various molecular lines, indicating spatial variations of the molecular gas conditions within the galaxies. Discrepant proﬁles in a few other galaxies are consistent with pointing offsets. We hypothesize that at higher signal-to-noise ratios, the majority of the line proﬁles would be found to be intrinsically inconsistent due to changing molecular gas conditions. Higher signal-to-noise single-dish data or interferometric maps in the various lines will allow us to determine if this is indeed the case. The molecular gas line ratios of early-type galaxies are generally consistent with the ranges observed in spiral galaxies, although we also identify some outliers. In terms of the HCN/12CO(1-0) ratio, often used as a dense gas tracer, we extend the work of Gao & Solomon (2004b) to lower FIR luminosities and ﬁnd an increased scatter in the ratio, compared to late-type galaxies of similar FIR-luminosities. Because of the important effects of the optical depth of 12CO(1-0) seen in the R10 ratio, we suggest that optical depth may be undermining the use of HCN/12CO(1-0) as a dense gas tracer and instead recommend the use of HCN/13CO(1-0). Unfortunately, few spirals have literature HCN/13CO(1-0) ratios. Determining whether the early-types have signiﬁcantly different dense gas fractions thus requires a larger sample of spirals to be observed in both HCN(1-0) and 13CO(1-0). We compare the molecular gas ratios R10, 12CO(1-0)/HCN and 12CO(1-0)/HCO+ to several ISM and stellar galaxy properties. R10 is found to correlate with the molecular-to-atomic gas ratio, dust temperature, dust morphology, absolute K-band magnitude and SSP age. 12CO(1-0)/HCN similarly correlates with the molecular-to-atomic gas ratio, dust temperature, absolute K-band magnitude and SSP age. The persistence of most R10 correlations with 12CO(1-0)/HCN rules out abundance variations of 13CO as the driving factor of the relations. No correlation with any of these properties is seen in the 13CO(1-0)/HCN ratio, further ruling out a dominant contribution from ion-exchange fractionation towards 13CO at low temperatures. With these options removed, a change in optical depth of the 12CO(1-0) line is the best explanation for the correlations we see. Based on the lower molecular-to-atomic gas ratios, unsettled dust and occasionally high dust temperatures, we ﬁnd that high R10 and 12CO(1-0)/HCN ratios in early-type galaxies are linked to recent or ongoing interactions and/or starbursts. Both of these processes can naturally account for optically thinner 12CO(1-0) line emission, as interactions are known to be more turbulent and star formation feedback can also produce more turbulence or heat up the molecular gas. Early-type galaxies with settled gas and dust discs tend to have less optically thin molecular gas, along with lower dust temperatures and higher molecular to atomic gas mass ratios. The correlations with K-band luminosity and SSP age can plausibly be explained by sample selection effects, further work including less massive early-type galaxies with settled gas and dust discs is required before conclusions are reached. A set of outliers with low R10 and R21, high HCN/12CO(10) and only upper limits to their HCN/HCO+ ratios are identiﬁed. These galaxies may have particularly stable molecular gas which remains very optically thick in 12CO or have their molecular gas conﬁned to a thin layer by hot gas pressure. NGC 1266, a galaxy with a molecular outﬂow, also stands apart from both spirals and the rest of the early-type sample. Its HCN/13CO(1-0) and 13CO(10)/HCO+ ratios suggest that it has a very high dense gas fraction, but its high R10 and R21 values simultaneously suggest that much of its 12CO emission is optically thin. This combination of ratios is seen in some other extremely active starburst or Seyfert galaxies. Based on observations carried out with the IRAM Plateau de Bure Interferometer. IRAM is supported by INSU/CNRS (France), MPG (Germany) and IGN (Spain). AC would like to thank Suzanne Aalto and Ron Snell for useful discussions during the preparation of this work as well as Leslie Sage and Gary Welch for providing 12CO(1-0) and 12CO(2-1) data for two sample galaxies. This work was supported by the rolling grants ‘Astrophysics at Oxford’ PP/E001114/1 and ST/H002456/1 and visitors grants PPA/V/S/2002/00553, PP/E001564/1 and ST/H504862/1 from the UK Research Councils. EB thanks John Fell OUP Research Fund, ref 092/267. MC acknowledges support from a Royal Society University Research Fellowship. RLD acknowledges travel and computer grants from Christ Church, Oxford and support from the Royal Society in the form of a Wolfson Merit Award 502011.K502/jd. RLD also acknowledges the support of the ESO Visitor Programme which funded a 3 month stay in 2010. FB acknowledges support from the European Research Council through grant ERC-StG-257720. SK acknowledges support from the the Royal Society Joint Projects Grant JP0869822. RMcD is supported by the Gemini Observatory, which is operated by the Association of Universities for Research in Astronomy, Inc., on behalf of the international Gemini partnership of Argentina, Australia, Brazil, Canada, Chile, the United Kingdom, and the United States of America. TN and MBois acknowledge support from the DFG Cluster of Excellence ‘Origin and Structure of the Universe’. MS acknowledges support from a STFC Advanced Fellowship ST/F009186/1. NS and TD acknowledge support from an STFC studentship. MBois has received, during this research, funding from the European Research Council under the Advanced Grant Program Num 267399-Momentum. The authors acknowledge ﬁnancial support from ESO. The research leading to these results has received funding from the European Community’s Seventh Framework Programme (/FP7/20072013/) under grant agreement No 229517.
Mechanical energy fluxes associated with saturated coronal heating in M dwarfs: comparison with predictions of a turbulent dynamo<|sep|>We have obtained quantitative estimates of mechanical energy fluxes Fmech which emerge in the  form of Alfven waves from models of low mass stars in the vicinity of the transition to complete  convection. In our model, it is assumed that magnetic fields are generated in equipartition with  convective kinetic energy. Our models are found to generate values of Fmech which range upto (25) x 107 erg cm-2 s-1. These fluxes overlap well with the upper limits which have been reported  for the fluxes required to explain the empirical X-ray emission from the coronae of M dwarfs ,  (1.4-3) x 107 erg cm-2 s-1). As a result, we would like to suggest that a turbulent dynamo, in  which magnetic fields are generated in equipartition with convective energy, can generate  enough mechanical flux to account quantitatively for the upper limits on coronal heating in M  dwarfs. In an earlier paper (MHM), we presented numerical evidence that rotational energy could  account for dynamo activity among stars where an interface exists between an outer convective  envelope and an inner radiative core. Such an interface is expected to exist only in stars with  masses larger than a critical value (0.32-0.34 M⊙). However, in contrast, turbulent dynamos can exist in all stars where convection is operating near the stellar surface. In some stars, both types  of dynamos may contribute significantly to the overall mechanical flux. We suggest that in order  to distinguish between interface dynamos and turbulent dynamos, the slope of the rotationactivity correlation might be a helpful parameter if it can be derived for a sampling of slowly  rotating (“unsaturated”) M dwarfs with closely spaced spectral sub-types. Unfortunately, in  practice, the situation may be too complicated to allow a clean distinction.
Using Partial Monotonicity in Submodular Maximization<|sep|>In this paper we have deﬁned the monotonicity ratio, analyzed how the approximation ratios of standard submodular maximization algorithms depend on this ratio, and then demonstrated that this leads to improved approximation guarantees for the applications of movie recommendation, image summarization and quadratic programming. We believe that the monotonicity ratio is a natural parameter of submodular maximization problems, reﬁning the binary distinction between monotone and non-monotone objective functions and improving the power of submodular maximization tools in machine learning applications. Thus, we hope to see future work towards understanding the optimal dependence on m of the approximation ratios of various submodular maximization problems. An important take-home message from our work is that, at least in the unconstrained submodular maximization case, the optimal algorithm has an approximation ratio whose dependence on m is non-linear. Such algorithms are rarely obtained using current techniques, which might be one of the reasons why these techniques have so far failed to obtain tight approximation guarantees for constrained non-monotone submodular maximization. In this section, we show how the proof of the symmetry gap technique due to Vondr´ak [51] can be adapted to prove Theorem 3.3. Let us begin the section by restating the theorem itself. Theorem 3.3. Consider a non-negative m-monotone submodular function f and a collection F ⊆ 2N of feasible sets such that the problem max{f(S) | S ∈ F} is strongly symmetric with respect to some group G of permutations over N and has a symmetry gap γ. Let C be the class of problems max{ ˜f(S) | S ∈ ˜F} in which ˜f is a non-negative m-monotone submodular function, and ˜F is a reﬁnement of F. Then, for every ε > 0, any (even randomized) (1 + ε)γ-approximation algorithm for the class C would require exponentially many value queries to ˜f. The crux of the symmetry gap technique is two lemmata due to [51] that we restate below. Lemma A.1 shows that given a non-negative set function f, one can obtain from it two continuous versions: a continuous version ˆF that resembles f itself, and a continuous version ˆG that resembles a symmetrized version of f. Distinguishing between ˆF and ˆG is diﬃcult, however, this does not translate into an hardness for discrete problems since ˆF and ˆG are continuous. Therefore, Vondr´ak [51] proved also Lemma A.2, which shows how these continuous functions can be translated back into set functions with appropriate properties. Lemma A.1 (Lemma 3.2 of [51]). Consider a function f : 2N → R≥0 invariant under a group of permutations G on the ground set N. Let F(x) be the multilinear extension of F, deﬁne ¯x = Eσ∈G[1σ(x)] and ﬁx any ε > 0. Then, there is δ > 0 and functions ˆF, ˆG: [0, 1]N → R≥0 (which are also symmetric with respect to G), satisfying the following: 1. For all x ∈ [0, 1]N , ˆG(x) = ˆF(¯x). 2. For all x ∈ [0, 1]N , | ˆF(x) − F(x)| ≤ ε. 3. Whenever ∥x − ¯x∥2 ≤ δ, ˆF(x) = ˆG(x) and the value depends only on ¯x. 4. The ﬁrst partial derivatives of ˆF and ˆG are absolutely continuous. 5. If f is monotone, then, for every element u ∈ N, ∂ ˆF
Liquid velocity fluctuations and energy spectra in three-dimensional buoyancy driven bubbly flows<|sep|>To conclude, we have investigated the statistical properties of velocity ﬂuctuations in psuedo-turbulence generated by buoyancy driven bubbly ﬂows. The Re that we have explored are consistent with the Re ∼ [300−1000] used in the experiments (Riboux et al. 2010; Prakash et al. 2016; Mendez-Diaz et al. 2013). Our numerical simulations show that the shape of the p.d.f. of the velocity ﬂuctuations is consistent with experiments over a
Location Aided Energy Balancing Strategy in Green Cellular Networks<|sep|>In this paper, we present a statistical based mechanism to balance energy utilization in cellular network. In the approach, each relay station only needs to remain two factor to store its historical energy trend and then it is able to predict its future energy. The message initiator selects the relay station with potential highest energy to relay data transmission. Simulations demonstrate that our approach results in higher throughput in the network and longer average life time of relay stations over related approaches.
Dual Smoothing and Level Set Techniques for Variational Matrix Decomposition<|sep|>We have discussed both speciﬁc algorithms for the RPCA problem, and general algorithm frameworks (“TFOCS”, and “ﬂipped” variational value-function approaches) that incorporate RPCA and variants. The custom RPCA algorithm works extremely well in practice, and the process of “ﬂipping” the objective has been studied rigorously, but the inner “quasi-Newton” algorithm lacks a rigorous convergence theory. The general algorithm TFOCS, as well as similar proposals such as the PDHG algorithm, have more established theory but lack practical guidance on setting parameters, and are in practice slower than the special purpose algorithms. An obvious goal of future work is to either improve the analysis of these algorithms (for example, this may give insight into parameter selection), or derive new algorithms that inherent all the advantages. The running theme of this work has been the beneﬁts of solving variants of RPCA, in particular (ﬂip-SPCPsum) and the new variants (SPCPmax) and (ﬂip-SPCPmax). These versions sometimes allow a good estimate of τ (or ε for (SPCPmax)), thus reducing the parameter selection of the model to the single scalar λsum or λmax. Our theory allows for different regularizers and data ﬁdelity terms; using these in practice is interesting future work.
Non-Markovian finite-temperature two-time correlation functions of system operators of a pure-dephasing model<|sep|>We have evaluated the exact non-Markovian ﬁnitetemperature one-time expectation values and two-time CF’s of the system operators for the exactly solvable pure-dephasing spin-boson model. The evaluation has been performed in two ways, one by exact direct operator technique without any approximation and the other by the evolution equations (2) and (3) valid to second order in the system-environment interaction Hamiltonian. Since the non-Markovian dynamics of the pure-dephasing spin-boson model can be cast into a time-local, convolutionless form and [L, Hs] = 0, the results obtained by the second-order evolution equations (2) and (3) turn out to be exactly the same as the exact results obtained by the exact direct operator evaluation. The agreement of the results between the two diﬀerent approaches demonstrates clearly the validity of the evolution equations (2) and (3). Furthermore, it is easy to obtain Eqs. (37)–(38), Eqs. (39) and (40), and Eqs. (41)–(44) from the evolution equation (3). Other non-Markovian open quantum system models that are not exactly solvable can be proceeded in a similar way to obtain the time evolutions of their two-time system operator CF’s valid to second order in the system-environment interaction Hamiltonian. This illustrates the practical usage of the evolution equations. It is thus believed that the evolution equations (2) and (3), which generalize the QRT to the non-Markovian ﬁnite-temperature case will have applications in many diﬀerent branches of physics. We would like to acknowledge support from the National Science Council, Taiwan, under Grant No. 972112-M-002-012-MY3, support from the Frontier and Innovative Research Program of the National Taiwan University under Grants No. 97R0066-65 and No. 97R006667, and support from the focus group program of the National Center for Theoretical Sciences, Taiwan. We are grateful to the National Center for High-performance Computing, Taiwan, for computer time and facilities. To show the time evolution operator of Eq. (10), we begin from Eq. (8) with ˜HI(t) given by Eq. (9). Since ˜HI(t) in Eq. (9) contains only two major terms, which are, respectively, proportional to aλ and a† λ, one is tempting to evaluate the time-ordered exponent by the reverse operator identity of Eq. (12) valid for the commutator [A, B] commuting with both A and B. As the exponent operates at diﬀerent times and [ ˜HI(t), ˜HI(τ)] ̸= 0, it is not correct to use the precise form of Eq. (A1). The proper procedure done in [20, 32] is to separate the two terms of Eq. (9) in the time-ordered exponent of Eq. (8) by
UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning<|sep|>We have proposed a novel framework for unsupervised learning of optical ﬂow estimation by bottom-up and topdown optimize of the pyramid levels. For the interpolation problem in the bottom-up upsampling process of pyramid network, we proposed a self-guided upsample module to change the interpolation mechanism. For the top-down guidance of the pyramid network, we proposed a pyramid distillation loss to improve the optical ﬂow learning on intermediate levels of the network. Extensive experiments have shown that our method can produce high-quality optical ﬂow results, which outperform all the previous unsupervised methods on multiple leading benchmarks. Acknowledgement: This research was supported by National Natural Science Foundation of China (NSFC) under grants No.61872067 and No.61720106004.
Micropattern gas detector technologies and applications, the work of the RD51 collaboration<|sep|>Micropattern gas detectors have a great potential in science and industry, in medical and commercial applications. RD51 is committed to fulﬁll this potential. The collaboration welcomes new institutes who are interested in participating in the Wg1 Wg2 Wg3 Wg4 Wg5 Wg6 Wg7 Mpgd technology Characterization Applications Software Electronics Production Common test & new structures & physics issues & simulation facilities Tracking and triggering — Photodetection — Calorimetry — Cryogenic det. — X-ray & neutron imaging — Astroparticle physics appl. — Medical appl. — Plasma diagn. Homeland sec. Fe electronics requirements deﬁnition — General purpose pixel chip — Large area systems with pixel readout — Portable multichannel system — Discharge protection strategies development of micropattern gas detectors. Up-to-date information and relevant contacts can be found on the collaboration webpage6, or by contacting the spokespersons.
LP Formulations of sufficient statistic based strategies in Finite Horizon Two-Player Zero-Sum Stochastic Bayesian games<|sep|>This paper analyzes two-player zero-sum stochastic Bayesian games. The traditional methods used for computing the optimal strategies in Bayesian games are computationally heavy and cannot perform well for long horizon games. This paper introduces a computationally eﬃcient Algorithm which can solve long Bayesian games and it’s suﬃcient statistic is fully accessible to the players. It solves the game window-by-window and update players’ suﬃcient statistic using LP, based on which players can compute their optimal strategies. Thus it solves the accessibility issue which is a major problem of Bayesian game. The performance of this Algorithm is within the bound and gets better with increasing window size.
Are stable instances easy?<|sep|>In this work we have shown that stability, supplemented by certain properties of the input instance, allows for an efﬁcient algorithm for Max-Cut. However, if nothing is assumed about the input, we only know that n-stability is sufﬁcient. Can this be improved? Note that γ ≥ n is very far from what happens in the random n ). A bold conjecture is that there is some constant, γ∗, s.t. γ∗-stable instances can be solved in polynomial time. Our motivation in deﬁning stability and distinctness is to identify natural properties of a solution to an NPhard problem, which “make it interesting”, and allow ﬁnding it in polynomial time. Stability and distinctness indeed make Max-Cut amenable, but are in no way the only possible properties, and it would be very interesting to suggest others.
Inner structure of ZnO microspheres fabricated via laser ablation in superfluid helium<|sep|>We found that the ZnO microspheres fabricated via laser ablation in superﬂuid helium contain some voids. The existence of the voids may aﬀect the WGM resonances although the WGMs can be unaltered if the voids locate far away from the surface of the microsphere where the electromagnetic energy localizes. The location of the voids is one of the important parameters determining the loss of the WGMs and the optical properties of the fabricated microspheres. The voids could be the result of the inclusion of the helium gas or the gas phase of the ablated material within the molten ZnO particles. Large-scale fabrication of the microspheres with high quality factors from various materials requires suppressing the void formation, which may be controlled by changing the ablation conditions such as the pulse energy and the photon energy. The formation of the microspheres without voids would be possible since spheres with a size of ≲
Exact Scalar Minimum Storage Coordinated Regenerating Codes<|sep|>In this paper, we applied independent interference alignment to minimum storage coordinated regenerating codes (MSCR) and show that this technique allows exact repair if and only if k = 2. Our results also apply to adaptive regenerating codes thus providing an interesting solution for the implementation of practical systems when k = 2. To overcome the impossibility shown in this paper, several tracks can be considered: (i) considering a technique that does not align the interferences independently, (ii) building vector codes (i.e., relying on sub-packetization with β > 1 by opposition to scalar codes β = 1 considered in this paper as done in [7], [8], [17], [18]), or (iii) building minimum bandwidth coordinated regenerating codes (MBCR) (for single failure, codes exist for all parameters [6]). Finally, the related question of achievable limits for high rate exact MSCR when relying on scalar codes remains open.
Relativistic causality and clockless circuits<|sep|>The relativistic and classical frameworks lead to conceptions of time that diﬀer in a signiﬁcant way. Time-simultaneity and total ordering in time are classical properties. In contrast, relativistic time can only satisfy a partial ordering to remain compatible with observer-dependent propagation delays. Practical methods depending on the classical properties of time may reach a point where they cease to be suﬃcient for a circuit to properly function, due to propagation delays. The models used to design logic devices can then be improved by releasing these properties and looking for a better implementation of relativistic causality. In particular, causal relations can be made compatible with the impossibility for some events to satisfy a time order [9]. Delay Insensitivity represents an eﬃcient criterion for designing functional clockless circuits in a simple way. But usual models, such as the trace formalism, rely on a classical representation of time-ordering and causality. Nonetheless, traces can be generalized to R-traces which provide a similar description of the dialog between a DI component and its environment while being compatible with relativistic causality. The usual formalism [22, 23, 24] is recovered when implementations of components are restricted to a classical environment. An R-trace structure describes the speciﬁcation of a component-environment pair as the list of all time orders that can hold between the communication events of the pair dialog. When rewritten, Udding’s rules keep a simple form that clariﬁes their relation with causality properties. Although the R-trace formalism, when applied to speciﬁcations of DI components, takes a form similar to its classical analog [2, 3], interpretations diﬀer signiﬁcantly. R-trace structures describe the time-order and causality relations of all events, including signal propagations, while classical structures only refer to events that are localized in time. The biggest merit of the R-trace formalism is to clarify the semantics underlying Udding’s rules by exhibiting their intrinsic relation with time-ordering and physical causality. Further theoretical insight, with potential consequences for applications, could be gained by comparing R-traces to other existing formalisms dealing with Delay Insensitivity. Criteria have indeed been developed to perform such comparisons as, for instance, the strength of semantic properties in equivalence relationships [25]. Besides a clear representation of causal relations, another merit of the R-trace formalism is to replace the description of a distributed system as a set of individual components that communicate by a composition of component-environment pairs. This rather describes the composition of interfaces, where interface means the dialog between the two members of a pair. In contrast to alternative descriptions of causality, such as Petri nets, this conception allows a simple composition of multiple components, by considering the latter as a multiplication of interfaces. This approach also opens new and interesting possibilities for solving questions raised by the synchronization of complex distributed systems [7, 8, 10]. A further advantage of the formalism is its ability to include in the description, with minor changes, components in motion. This could also allow the treatment of distributed algorithms on a network of mobile computers within the formalism of DI systems.
Data linkage algebra, data linkage dynamics, and priority rewriting<|sep|>We have presented an algebra of which the elements are intended for modelling the states of computations in which dynamic data structures are involved. We have also presented a simple model of computation in which states of computations are modelled as elements of this algebra and state changes take place by means of certain actions. We have described the state changes and replies that result from performing those actions by means of a term rewriting system with rule priorities. We followed a rather fundamental approach. Instead of developing the model of computation on top of an existing theory or model, we started from ﬁrst principles by giving an elementary algebraic speciﬁcation [15] of the states of computations in which dynamic data structures are involved. We found out that term rewriting with priorities is a convenient technique to describe the dynamic aspects of the model in an appealing mechanical way. In particular, we managed to give a clear idea of the features related to reclamation of garbage. It stands out that the description of the dynamic aspects of the presented model of computation by means of a term rewriting system with priorities is rather sizable. However, an alternative description in the world of sets that, unlike the one that we have given in this paper, covers both deterministic and non-deterministic data linkages, would be rather sizable as well. Moreover, we believe that the use of conditional term rewriting [6] instead of priority rewriting would give rise to a less compact and more complicated description. The presented model of computation and its description are well-thought out, but the choices made can only be justiﬁed by applications. Applications to that eﬀect should not only include applications as a setting in which theoretical issues concerning dynamic data structures are studied. They should also include applications as a setting in which practical programming problems that involve dynamic data structures are studied. In [8], we have studied the programming of an interpreter for a program notation that is close to existing assembly languages in this setting. Together with thread algebra and program algebra [7], we hold the model of computation as described in this paper to be a suitable starting-point for investigations into theoretical issues concerning the interplay between programs and dynamic data structures, including issues concerning reclamation of garbage. We have studied one such issue in [10], namely the feasibility of automatically making everything garbage as soon as it can be viewed as garbage. For the study in question, the abstraction from the representation of dynamic data structures by means of pointers turned out to be really useful. This research has been partly carried out in the framework of the Jacquardproject Symbiosis, which is funded by the Netherlands Organisation for Scientiﬁc Research (NWO). We thank two anonymous referees for carefully reading a preliminary version of this paper and for suggesting improvements of the presentation of the paper.
Influence of structural disorder and Coulomb interactions in the superconductor-insulator transition applied to boron doped diamond<|sep|>The contrasting nature of on-site random potential disorder and structural disorder in the form of non-uniform random tight-binding hopping parameters has been illustrated and the results have been applied to understanding the superconductor-insulator transition in BDD and BNCD. While on-site potential disorder at the boron sites alone decreases the mean pairing amplitude and spectral gap, structural disorder can increase the mean pairing amplitude although the spectral gap still decreases as the disorder parameter increases. This signiﬁcant diﬀerence in behaviour stems from the introduction of states within the gap region in the case of structural disorder, resulting in a wide distribution of mean pairing amplitudes. The calculated temperature dependence also highlights the diﬀerent natures of structural and on-site disorder as structurally disordered systems show a marked deviation from the temperature dependence expected for a conventional BCS superconductor. This work is of particular interest when applied to BNCD due to the high level of structural disorder inherent in this unconventional superconductor. Through the combination of structural disorder, random on-site potential ﬂuctuations and the Coulomb interaction in the narrow acceptor band, the experimentally found saturation of the TC in BDD as a function of the boron concentration can be understood. This study illustrates a minimal Hamiltonian which captures some of the features found in experimental studies of BDD and BNCD and lays a foundation for further work where grain boundaries in BNCD (which act as weak links) can be built into the theory through Joseph
Detection of Keplerian dynamics in a disk around the post-AGB star AC Her<|sep|>1. We presented PdBI maps of 12CO J=2−1 emission from the nebula surrounding the post-AGB star AC Her, in which we convincingly identiﬁed a Keplerian disk (Sects. 2 and 3). Orbiting disks are suspected to play a fundamental role in the evolution of AGB and post-AGB objects (Sect. 1), but before this work, such a structure had been clearly detected and studied in only one source (the Red Rectangle). 2. AC Her and the Red Rectangle belong to a relatively wide class of post-AGB stars in which rotating disks are suspected to exist and to represent most of the nebular mass (Sect. 1). These are double stars whose surrounding nebulae systematically show peculiar CO line proﬁles (in observations with low angular resolution), similar to those expected from disks in rotation. Our maps support that Keplerian disks are systematically present in these sources and that they are, therefore, relatively common in nebulae around binary evolved stars. 3. We derived the main properties of the disk from directly interpreting the data and model ﬁtting. We ﬁnd a total disk radius of ∼ 1.7 1016 cm. Densities of 106 – 104 cm−3 and temperatures of 80 – 20 K are obtained. The velocity ﬁeld is found to be basically Keplerian; we do not ﬁnd clear evidence of gas in expansion (as found in other similar objects). 4. We ﬁnd a disk mass ∼ 1.5 10−3 M⊙ and a central mass ∼ 1.5 M⊙, after accounting for possible underestimations due to uncertainties in the modeling (Sect. 3.1). 5. Signiﬁcant diﬀerences between disks surrounding stars of this class are found. The disk in AC Her is much smaller and contains less mass than that of the Red Rectangle. It is also signiﬁcantly cooler, even though the gas is closer to the center. Our data do not show any sign of outﬂowing gas, which is conspicuous in the Red Rectangle and dominating in the nebula around the similar object 89 Her (and, of course, in most nebulae around AGB and post-AGB stars, see Sect. 1). 6. However, the existing data on AC Her are still meager, therefore our modeling is uncertain. A fraction of the total ﬂux is lost in the interferometric maps, which may come from an extended outﬂow, and the resolution is poor compared with the disk size. On the other hand, the lack of maps of optically thin lines prevents a deeper analysis of the total disk mass. We hope that future mapping of 12CO and 13CO lines will allow more accurate studies. Acknowledgements. This work is based on observations carried out with the IRAM Plateau de Bure Interferometer; IRAM is supported by INSU/CNRS (France), MPG (Germany) and IGN (Spain).
Simulating Using Deep Learning The World Trade Forecasting of Export-Import Exchange Rate Convergence Factor During COVID-19<|sep|>This research provides an idea of what the future holds for imports and exports in world  trade during the Corona period with the LSTM model. The dataset, titled "Effects of  COVID-19 on trade", was collected from the state website NZ Tatauranga Aotearoa to  provide time-series analysis and is the original source of the data. The time series analysis has been studied based on export and import data from 2015, January 1 to 2021,  May 30, for all countries, all commodities, and all transport systems. In the methodology, we have tried to predict what might happen in the next 180 days in world trade  through various graphical visualizations which have used cumulative value in global  trade in terms of imports and exports. The 180-day forecast shows that in the Covid-19  period the cumulative value of imports and exports is likely to remain unchanged from  May 30, 2021 to November 2021.   We have some limitations in this work as we have only worked and predicted cumulative value but what the global economic situation might be in terms of trade was not  discussed during this coronal period or no graphical representation was shown. In future, we’ll try to create comparative study between the time series analysis of LSTM &  ARIMA, SRIMA model.
3-regular matchstick graphs with given girth<|sep|>In Section 2 we have introduced some parameters and techniques in order to provide some easy and computationally cheap certiﬁcates for proving that a given planar graph is not realizable with unit edge lengths. These methods also work for non-rigid graphs. We have to admit that the given criteria are very far from being sufﬁcient in general, but at least they were sufﬁcient to completely solve a non-trivial matchstick puzzle. We would like to remark that we stumbled over the example of Figure 3 along the way to prove the minimality of the 2006 example of Giuseppe Mazzuoccolo consisting of 32 vertices using an exhaustive search. Indeed our ﬁrst try was to prove that the conﬁgurations from Figure 3 is not a matchstick graph. We believe that an example like those of Figure 5 would not have been discovered by playing and puzzling with matchsticks itself. Who would try such a conﬁguration? You would need very precisely sized and thin matchsticks. In our opinion a lot of more needs to be done in order to provide a solid grounding for exhaustive search methods for planar geometric graphs with given side lengths. Already the case were all edges have an equal length seems to be quite hard. To stimulate some research in this direction we ask the interested reader for an elegant proof or an algorithm which is relatively fast in practice to show that the planar graph from Figure 8 is not a matchstick graph. Actually we do not know a general algorithm which can decide whether a given planar graph is a matchstick graph.
Computation of extremes values of time averaged observables in climate models with large deviation techniques<|sep|>In this paper we have discussed techniques to compute large deviation functions of climatic observables. Direct estimation of large deviation functions in a complex chaotic system is a delicate procedure that needs care in properly checking the convergence of both the large deviation limit and of the statistical estimators [27]. A simplistic analysis relying on visual arguments for the collapse of the scaled functions on some seemingly well behaved function can lead to wrong estimates, as the functions will indeed converge, but to wrong values. In particular, wrong estimates may lead to think that the available data were enough to go beyond the Gaussian regime, while as we have seen there may be cases in which a more reﬁned analysis shows that non Gaussian behaviors can appear as an artifact of not having properly studied the convergence. The rare event algorithm described in this paper [7, 16, 21] can greatly help to compute large deviation rate functions for large values of the anomalies that cannot be accessed with a direct approach. This method was never used for systems of the complexity of a numerical climate model, until in [26] we have used it to study European heat waves. In this paper we have used it for the explicit task of computing large deviation rate functions, highlighting its connection to large deviation theory. The recipe presented in this paper can be easily replicated for climate studies on diﬀerent observables and with diﬀerent climate models. We have observed that the large deviation limit is not of direct interest for studying real heat waves. From a physical point of view, the problem is that in the model Plasim with physical parameterizations, the autocorrelation function of the European temperature has a slow decaying tail that involves time scales of about 30 days. The consequence of this slow decorrelation is that the large time asymptotics of the large deviation rate function is not attained before a few years. This makes the use of the large deviation asymptotics irrelevant for this case for time averages of the order of a few months. The large deviation limit could however be of practical relevance for quantities with faster decaying autocorrelation functions, for example precipitation, or for spatial averages of surface temperature over diﬀerent regions. Moreover, algorithms initially dedicated to the computation of large deviations are still eﬃcient to compute the probability of extreme heat waves. The results show in this paper and in [26] refer to a perpetual summer setup, with no time dependent external forcing acting on the system. In the applications envisioned by [16] and [21], the algorithm was not meant to be applied in presence of a time dependent forcing on time scales comparable with the duration of the events of interest, while in the original formulation of [7] there are not limitations in this sense. The daily cycle is shorter than the resampling time, so that including it does not present any problem. We are currently testing the performances of the algorithm in presence of seasonal cycle, that will be the subject of future studies. Even in the form presented in this paper however, the large deviation algorithm could be of extreme interest for application to more theoretical studies in which perpetual summer condition are a common setup. The choice of the size of the time block τb and the test of the convergence to the large deviation limit requires computing the autocorrelation time of the process, which sets a lower bound to the values of the averaging time that it makes sense to consider. Figure 2b) shows for the ﬁrst 50 days the autocorrelation function R(t) of the average European surface temperature Ts computed from a 1000 years long run. We can see that to a ﬁrst approximation the function is well described by a double exponential, with a ﬁrst decay time of about 4 days compatible with the time scale of synoptic variability, followed by a slowly decaying tail that at least in the ﬁrst part seems to decay exponentially on a time scale of one month. The longer time scales inducing the slow decay of the autocorrelation function could be related to the low frequency variability of the atmospheric dynamics, and/or to time scales relate to the water vapor cycle in the atmosphere and the land surface processes. The integral autocorrelation time τc is deﬁned as the integral from time lag 0 to +∞ of the autocorrelation function R(t) = E [(A(X(t)) − µ)(A(X(0)) − µ)] /σ2. An equivalent expression for τc is [3]
Statistical Estimation of Mechanical Parameters of Clarinet Reeds Using Experimental and Numerical Approaches<|sep|>Fifteen modes of vibration of the clarinet reed have been observed, while previous studies investigated 4 to 5 modes only [17, 13]. The observed resonance frequencies are often highly correlated, especially those among the “transversal” modes and, to a lesser extent, those among the “ﬂexural” modes. The nominal reed strength is surprisingly better correlated with the frequencies of “transversal” modes as with those of “ﬂexural” modes. The ﬂexural modes within the same series are poorly correlated. A principal component analysis of the resonance frequencies identiﬁes 4 main factors, capturing 91.2% of the variance of the sample. The data can therefore be reconstructed with 4 uncorrelated factors only (error: RMSD = 21.8 cents, see Appendix D.2 and E). The eﬀect of hygrometric change between both measurement series can seemingly be described with 1 factor only. These statistical facts oﬀer a guidance for modeling appropriately the mechanics of the clarinet reed.
Robust Sequential Steady-State Analysis of Cascading Outages<|sep|>In this paper, we developed a robust framework to simulate  the steady-state of sequential cascading outages. To achieve  this, we extended the current-voltage based power flow  formulation to model frequency and developed implicit  models for under frequency load shedding and under voltage  load shedding schemes. These models were shown to have  better convergence characteristics than discontinuous outerloop models of UFLS and UVLS. Importantly, the  optimization used to solve for the steady-state with  minimization of “feasibility” currents enables simulation of  large cascading outages, such as the 8k+ bus network, while  localizing and identifying any collapsed sections of the grid.
Probabilistic Latent Tensor Factorization Model for Link Pattern Prediction in Multi-relational Networks<|sep|>In this paper, we have proposed a new task of Link Pattern Prediction (LPP) problem and then developed a Probabilistic Latent Tensor Factorization (PLTF) model which represents social interaction patterns in multi-relational networks. For constructing the model, we introduce the speciﬁc latent factor for diﬀerent relation types in addition to using latent factors to characterize object features. We also provide the Hierarchical Bayesian treatment of the probabilistic model to avoid overﬁtting for solving the LPP problem. For that, we derive an eﬃcient Gibbs sampling method to learn the model parameters and hyperparameters. The experiments are conducted on several real world datasets and demonstrate signiﬁcant improvements over several existing state-of-the-art methods and the ability to capture the correlations among diﬀerent relation types, reveal the impact of distinct relation types in the multi-relational networks. There are several directions for future work that we will consider as the extensions of our proposed model. First, it would be interesting to investigate the evolutionary aspect of link patterns in the multi-relational networks over time. Second, we will consider some applications which can use link patterns to improve our understanding of social interaction and large-scale patterns of human association.
2D velocity fields of simulated interacting disc galaxies<|sep|>We have investigated 2D velocity ﬁelds of isolated and interacting spiral galaxies using N-body/SPH simulations. We focussed on the question how the full 2D velocity ﬁeld of a galaxy can be used to gain information on its internal kinematics. This issue was analysed with special emphasis on distant Tully-Fisher studies. To summarise and conclude: – We found that with the help of 2D velocity ﬁelds the nature of the interaction becomes a lot more accessible than with rotation curves from simple long-slit spectroscopy. Quantitative analysis such as an harmonic expansion, which is, for example, used by the kinemetry package of Krajnovi´c et al. (2006), oﬀer an additional possibility to identify distortions in a VF. – Tidal interactions lead to an asymmetric velocity ﬁeld, where the side pointing at the interaction is disturbed while the side remote from the interaction stays relatively unaﬀected. This behaviour can also be found in the rotation curve of the system (Kronberger et al. 2006).
Robust Distributed Accelerated Stochastic Gradient Methods for Multi-Agent Networks<|sep|>Stochastic gradient (SG) methods are workhorse algorithms in machine learning practice. There is an increasing need to run stochastic gradient methods in distributed environments, either because the data is inherently distributed (for instance when collected by autonomous units such as smart phones or sensors) and processing it in a non-distributed way is impractical for real-time decision making, or the data is non-distributed but due to its volume distributing the data to multiple computational units become unavoidable for scalability reasons. This motivates the study of the performance of SG methods on arbitrary networks where there the performance depends on the interplay between the bias, variance and network eﬀects. In this paper, we focused on distributed stochastic gradient (D-SG) and its accelerated version (D-ASG) with constant and decaying stepsize. We provided a number of convergence results for D-SG and D-ASG that improve the existing convergence results. Our performance bounds captures the trade-oﬀs in the bias, variance terms and the network eﬀects and are illustrated by our numerical experiments. We also proposed a multi-stage variant of D-ASG with an optimal dependency to bias and variance terms. In this work, we considered synchronous algorithms which require nodes to update their local copies synchronously. As part of future work, it would be interesting to study momentum acceleration in the context of asynchronous stochastic gradient algorithms where the nodes can do updates without requiring synchronization between the nodes. The authors are also grateful to the Associate Editor and three anonymous referees for helpful suggestions and comments. Mert G¨urb¨uzbalaban’s research is supported in part by the grants Oﬃce of Naval Research Award Number N00014-21-1-2244, National Science Foundation (NSF) CCF-1814888, NSF DMS-2053485, NSF DMS-1723085. Umut S¸im¸sekli’s research is partly supported by the French government under management of Agence Nationale de la Recherche as part of the “Investissements d’avenir” program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute). Lingjiong Zhu is grateful to the partial support from a Simons Foundation Collaboration Grant and the grant NSF DMS-2053454 from the National Science Foundation.
Bayesian estimation of one-parameter qubit gates<|sep|>In this paper we have analyzed estimation of one-parameter unitary gates for qubit systems. We have addressed Bayesian estimation procedures and compared their performances with the ultimate quantum limits to precision. Bayes estimator is known to be asymptotically unbiased, but for practical implementation is of interest to evaluate quantitatively how many measurements are needed to achieve the asymptotic region. To this aim, after the evaluation of the asymptotic a posteriori distribution for the gate parameter, we have compared it to the distribution obtained by Monte Carlo simulated experiments and full Bayesian analysis and shown that asymptotic optimality of Bayes estimator is achieved after a limited number of runs. We have also addressed the issue of stability, i.e the robustness of the optimal settings against ﬂuctuations of the probe and measurement parameters. It has been shown that the use of entanglement is useful to improve stability. More explicitly, we have shown that, although the Fisher information does not increase, its maximum value is achieved for a large class of probe signals, thus making the procedure more robust and increasing the overall stability of the estimation procedure. The authors thank M. Borrelli, M. Zaro, and N. Tomassoni for useful discussions. This work has been partially supported by the CNR-CNISM convention. This article was completed at a time of drastic cuts to research budgets imposed by the Italian government [48]; as a result research is becoming increasingly diﬃcult in Italian universities and may in the near future be brought to a complete halt.
Optical properties of coupled metal-semiconductor and metal-molecule nanocrystal complexes: the role of multipole effects<|sep|>We investigate the optical properties of hybrid molecules composed of SQD, dye, and MNPs. The main focus of the paper is the regime of strong exciton-plasmon interaction and multipole eﬀects. First we derive an exact analytical solution for electric ﬁelds and absorption spectra. Then, we show that the multipole eﬀects play the crucial role for the strong interaction regime. When the interparticle distance is relatively small, the multipole effect gives signiﬁcantly larger peak shifts and broadenings for the energy absorption rate and the dipole approximation fails. The results obtained in this paper can be used to analyze optical experiments on hybrid systems with a strong exciton-plasmon interaction.
Characterization of the Atmospheric Muon Flux in IceCube<|sep|>The inﬂuence of cosmic rays on IceCube data is signiﬁcant and varied. Given the presence of several energy regions where external measurements by direct detection or air shower arrays are sparse, it is necessary to develop a comprehensive picture including neutrinos, muons and surface measurements. Atmospheric muons play a privileged role, as they cover the largest energy range and provide the highest statistics. A consistent description of all experimental results will be an important contribution for the understanding of cosmic rays in general. The studies presented in this paper have outlined the opportunities to extract meaningful results from atmospheric muon data in a large-volume underground particle detector. Once systematic eﬀects are fully understood and controlled, it will be possible to measure the muon energy spectrum from 1 TeV to beyond 1 PeV by combining measurements based on angular distribution and catastrophic losses. Agreement between the two methods can then be veriﬁed in the overlap region around 10-20 TeV. There is a strong indication for the presence of a component from prompt hadron decays in the muon energy specrum, with best ﬁt values generally falling at the higher side of theoretical predictions. In the future, it will be possible for the IceCube detector to precisely measure the prompt contribution and to constrain the all-nucleon primary ﬂux before and around the knee. With more data accumulating, independent veriﬁcation of the prompt measurement based on seasonal variations of the muon ﬂux [90] will soon become feasible as well. The muon multiplicity spectrum provides access to the cosmic ray energy region beyond the knee. Even though a direct translation of the result to primary energy and average mass is impossible, combination with results from surface detectors or comparisons to model predictions provide valuable insights. In coming years, the measurement can be extended further into the transition region around the ankle. A possible contribution from heavy elements to the cosmic ray ﬂux at EeV energies should then be discernible. An important goal of this study was to verify the current understanding of systematic uncertainties. An unexplained eﬀect was demonstrated using low-level data, and appears to be present in the other analysis samples as well. In order to improve the quality of future atmospheric muon measurements with IceCube, it will be essential to determine whether the observed discrepancy requires better understanding of the detector, or of the production mechanisms of muons in air showers. Comparisons with measurements from the upcoming water-based KM3NeT detector [91] will be invaluable to decide whether the inconsistencies seen in IceCube data are due to the particular detector setup, or represent unexplained physics eﬀects.
Training Region-based Object Detectors with Online Hard Example Mining<|sep|>We presented an online hard example mining (OHEM) algorithm, a simple and effective method to train regionbased ConvNet detectors. OHEM eliminates several heuristics and hyperparameters in common use by automatically selecting hard examples, thus simplifying training. We conducted extensive experimental analysis to demonstrate the effectiveness of the proposed algorithm, which leads to better training convergence and consistent improvements in detection accuracy on standard benchmarks. We also reported state-of-the-art results on PASCAL VOC 2007 and 2012 when using OHEM with other orthogonal additions. Though we used Fast R-CNN throughout this paper, OHEM can be used for training any region-based ConvNet detector. Our experimental analysis was based on the overall detection accuracy, however it will be an interesting future direction to study the impact of various training methodologies on individual category performance. Acknowledgment. This project started as an intern project at Microsoft Research and continued at CMU. We thank Larry Zitnick, Ishan Misra and Sean Bell for many helpful discussions. AS was supported by the Microsoft Research PhD Fellowship. This work was also partially supported by ONR MURI N000141612007. We thank NVIDIA for donating GPUs.
Chern numbers of topological phonon band crossing determined with inelastic neutron scattering<|sep|>Our study demonstrates the capability of INS to measure Chern numbers of topological phonon band crossing. While scattering intensity can vary with momentum even on nontopological bands, the unique spectroscopic characteristics of Weyl and Dirac points lie in the fact that they are singu lar points for eigenvectors, hence the intensity distribution around them exhibits abrupt variations: Even on an inﬁnitely small enclosing momentum surface, the intensity modulations around the topological nodes are still present, whereas in the case of topologically trivial band crossings, the intensity distribution would approach a constant as the enclosing surface shrinks into a point. Consequently, in order to determine Chern numbers from INS experiments, it is better to study the close vicinity of the band crossing points, i.e., using a small q-surface to both avoid other bands and ensure that the intensity modulations arise solely from the topology. We also note that the intensity modulations we discuss here are universal for two-fold phonon Weyl points of any Chern number, as well as for some of the four-fold Dirac points. According to group theory [52], phonon Weyl points exist in crystals of certain space groups, and they can be found at speciﬁc highsymmetry points in the BZ. As long as they are far away from other bands and have a relatively large group velocity, intensity modulations should be observable by INS. The detection of wave functions (or, vibrational eigenvectors) is not restricted to INS experiments on phonons, and it can be a common capability of many spectroscopic methods. In polarization-dependent angle-resolved photoemission spectroscopy (ARPES), changes in the signal intensity has been suggested to reﬂect the wave functions of Dirac electrons [53]. Related measurements have also been proposed for resonant inelastic x-ray scattering (RIXS) [54]. While the wave-function texture of the quasiparticles is at the origin of all the spectroscopic observables, the speciﬁc interactions between the experimental probes and the quasiparticles may add further complexity to the interpretation of experiments. For instance, INS measurements of phonons involve neutron collisions with nuclei, giving rise to the Q · ξ term in the scattering cross section; and because Q uniquely determines V in Eq. (4), the associated projection of ξ always allows for the determination of the Chern number. On the contrary, for magnon bands, dipole-dipole interactions between neutrons and magnetic moments lead to a Q × (Q × S) term in the cross section, and the component of S parallel to Q is missing from the detection. As a result, the number of extrema in the intensity modulation on an enclosing momentum surface may change between diﬀerent choices of the measurement BZ, rendering it necessary to have extra knowledge about the magnetic system in order to correctly infer the topological invariant. In the cases of ARPES and RIXS, complexity may arise because the diﬀerent polarization channels have to be considered together. To this end, our INS measurement of phonons may be regarded as a demonstration of principles that motivates further studies. In conclusion, we have performed a comprehensive study on the topological phonon band crossings in MnSi and CoSi. Both the band dispersions and the coherent dynamical structure factors are experimentally resolved with high precision, yielding results that compare well with model calculations. The existence of spin-1 Weyl points and charge-2 Dirac points in the phonon bands are veriﬁed by the dispersions. Combining experiments and model calculations, we further demonstrate the capability of INS for unambiguously determining Chern numbers of band crossing nodes. Our general theoretical scheme based on eﬀective Hamiltonians suggests that related methods can be found in the study of other topological quasiparticles, as well as in the use of other spectroscopic methods. The work at Peking University was supported by the National Key R&D Program of China (No. 2018YFA0305602) and the National Natural Science Foundation of China (Nos. 12061131004 and 11888101). The work at Brookhaven National Laboratory was supported by Oﬃce of Basic Energy Sciences (BES), Division of Materials Sciences and Engineering, U.S. Department of Energy (DOE), under contract DE-SC0012704. Part of this research was performed at the MLF, J-PARC, Japan, under a user program (proposal Nos. 2018A0193, 2018B0201, 2019A0085). A portion of this research used resources at Spallation Neutron Source, a DOE Oﬃce of Science User Facility operated by the Oak Ridge National Laboratory. [1] K. v. Klitzing, G. Dorda, and M. Pepper, New method for highaccuracy determination of the ﬁne-structure constant based on quantized Hall resistance, Phys. Rev. Lett. 45, 494 (1980). [2] D. J. Thouless, M. Kohmoto, M. P. Nightingale, and M. den Nijs, Quantized Hall conductance in a two-dimensional periodic potential, Phys. Rev. Lett. 49, 405 (1982). [6] C.-K. Chiu, J. C. Y. Teo, A. P. Schnyder, and S. Ryu, Classiﬁcation of topological quantum matter with symmetries, Rev. Mod. Phys. 88, 035005 (2016). [7] N. P. Armitage, E. J. Mele, and A. Vishwanath, Weyl and Dirac semimetals in three-dimensional solids, Rev. Mod. Phys. 90, 015001 (2018). [8] A. Roth, C. Br¨une, H. Buhmann, L. W. Molenkamp, J. Maciejko, X.-L. Qi, and S.-C. Zhang, Nonlocal transport in the quantum spin Hall state, Science 325, 294 (2009). [9] X. Wan, A. M. Turner, A. Vishwanath, and S. Y. Savrasov, Topological semimetal and Fermi-arc surface states in the electronic structure of pyrochlore iridates, Phys. Rev. B 83, 205101 (2011). [10] L. X. Yang, Z. K. Liu, Y. Sun, H. Peng, H. F. Yang, T. Zhang, B. Zhou, Y. Zhang, Y. F. Guo, M. Rahn, D. Prabhakaran, Z. Hussain, S.-K. Mo, C. Felser, B. Yan, and Y. L. Chen, Weyl semimetal phase in the non-centrosymmetric compound TaAs, Nat. Phys 11, 728 (2015). [11] B. Q. Lv, H. M. Weng, B. B. Fu, X. P. Wang, H. Miao, J. Ma, P. Richard, X. C. Huang, L. X. Zhao, G. F. Chen, Z. Fang, X. Dai, T. Qian, and H. Ding, Experimental discovery of Weyl semimetal TaAs, Phys. Rev. X 5, 031013 (2015). [12] F. de Juan, A. G. Grushin, T. Morimoto, and J. E. Moore, Quantized circular photogalvanic eﬀect in Weyl semimetals, Nat. Commun 8, 15995 (2017). [13] F. Flicker, F. de Juan, B. Bradlyn, T. Morimoto, M. G. Vergniory, and A. G. Grushin, Chiral optical response of multifold fermions, Phys. Rev. B 98, 155145 (2018). [14] F. D. M. Haldane and S. Raghu, Possible realization of directional optical waveguides in photonic crystals with broken timereversal symmetry, Phys. Rev. Lett. 100, 013904 (2008). [15] L. Lu, L. Fu, J. D. Joannopoulos, and M. Soljaˇci´c, Weyl points and line nodes in gyroid photonic crystals, Nat. Photonics 7, 294 (2013). [16] L. Lu, Z. Wang, D. Ye, L. Ran, L. Fu, J. D. Joannopoulos, and M. Soljaˇci´c, Experimental observation of Weyl points, Science 349, 622 (2015). [17] M. Xiao, W.-J. Chen, W.-Y. He, and C. T. Chan, Synthetic gauge ﬂux and Weyl points in acoustic systems, Nat. Phys 11, 920 (2015). [18] H. Ge, X. Ni, Y. Tian, S. K. Gupta, M.-H. Lu, X. Lin, W.-D. Huang, C. T. Chan, and Y.-F. Chen, Experimental observation of acoustic Weyl points and topological surface states, Phys. Rev. Applied 10, 014017 (2018). [19] H. He, C. Qiu, L. Ye, X. Cai, X. Fan, M. Ke, F. Zhang, and Z. Liu, Topological negative refraction of surface acoustic waves in a Weyl phononic crystal, Nature 560, 61 (2018). [20] F. Li, X. Huang, J. Lu, J. Ma, and Z. Liu, Weyl points and Fermi arcs in a chiral phononic crystal, Nat. Phys. 14, 30 (2018). [21] T. Zhang, Z. Song, A. Alexandradinata, H. Weng, C. Fang, L. Lu, and Z. Fang, Double-Weyl phonons in transition-metal monosilicides, Phys. Rev. Lett. 120, 016401 (2018). [22] H. Miao, T. T. Zhang, L. Wang, D. Meyers, A. H. Said, Y. L. Wang, Y. G. Shi, H. M. Weng, Z. Fang, and M. P. M. Dean, Observation of double Weyl phonons in parity-breaking FeSi, Phys. Rev. Lett. 121, 035302 (2018). [23] J. Li, Q. Xie, S. Ullah, R. Li, H. Ma, D. Li, Y. Li, and X.-Q. Chen, Coexistent three-component and two-component Weyl phonons in TiS, ZrSe, and HfTe, Phys. Rev. B 97, 054305 (2018). [24] B. W. Xia, R. Wang, Z. J. Chen, Y. J. Zhao, and H. Xu, Symmetry-protected ideal type-II Weyl phonons in CdTe, Phys. Rev. Lett. 123, 065501 (2019). [25] H. Li, T. Zhang, A. Said, Y. Fu, G. Fabbris, D. G. Mazzone, J. Zhang, J. Lapano, H. N. Lee, H. C. Lei, M. P. M. Dean, S. Murakami, and H. Miao, Observation of a chiral wave function in the twofold-degenerate quadruple weyl system BaPtGe, Phys. Rev. B 103, 184301 (2021). [28] W. Yao, C. Li, L. Wang, S. Xue, Y. Dan, K. Iida, K. Kamazawa, K. Li, C. Fang, and Y. Li, Topological spin excitations in a three-dimensional antiferromagnet, Nat. Phys 14, 1011 (2018). [29] S. Bao, J. Wang, W. Wang, Z. Cai, S. Li, Z. Ma, D. Wang, K. Ran, Z.-Y. Dong, D. L. Abernathy, S.-L. Yu, X. Wan, J.-X. Li, and J. Wen, Discovery of coexisting Dirac and triply degenerate magnons in a three-dimensional antiferromagnet, Nat. Commun 9, 2591 (2018). [34] M. Elliot, P. A. McClarty, D. Prabhakaran, R. D. Johnson, H. C. Walker, P. Manuel, and R. Coldea, Order-by-disorder from bond-dependent exchange and intensity signature of nodal quasiparticles in a honeycomb cobaltate, Nat. Commun 12, 3936 (2021). [35] A. Scheie, P. Laurell, P. A. McClarty, G. E. Granroth, M. B. Stone, R. Moessner, and S. E. Nagler, Spin-exchange hamiltonian and topological degeneracies in elemental gadolinium, Phys. Rev. B 105, 104402 (2022). [36] A. Scheie, P. Laurell, P. A. McClarty, G. E. Granroth, M. B. Stone, R. Moessner, and S. E. Nagler, Dirac magnons, nodal lines, and nodal plane in elemental gadolinium, Phys. Rev. Lett. 128, 097201 (2022). [37] T. Zhang, R. Takahashi, C. Fang, and S. Murakami, Twofold quadruple Weyl nodes in chiral cubic crystals, Phys. Rev. B 102, 125148 (2020). [38] R. Kajimoto, M. Nakamura, Y. Inamura, F. Mizuno, K. Nakajima, S. Ohira-Kawamura, T. Yokoo, T. Nakatani, R. Maruyama, K. Soyama, K. Shibata, K. Suzuya, S. Sato, K. Aizawa, M. Arai, S. Wakimoto, M. Ishikado, S.-i. Shamoto, M. Fujita, H. Hiraka, K. Ohoyama, K. Yamada, and C.-H. Lee, The Fermi Chopper Spectrometer 4SEASONS at J-PARC, J. Phys. Soc. Jpn. 80, SB025 (2011). [39] G. E. Granroth, A. I. Kolesnikov, T. E. Sherline, J. P. Clancy, K. A. Ross, J. P. C. Ruﬀ, B. D. Gaulin, and S. E. Nagler, SEQUOIA: A newly operating chopper spectrometer at the SNS, J. Phys. Conf. Ser. 251, 012058 (2010). [41] Z. Jin, Y. Li, Z. Hu, B. Hu, Y. Liu, K. Iida, K. Kamazawa, M. B. Stone, A. I. Kolesnikov, D. L. Abernathy, X. Zhang, H. Chen, Y. Wang, C. Fang, B. Wu, I. A. Zaliznyak, J. M. Tranquada, and Y. Li, Magnetic molecular orbitals in MnSi (2022), arXiv:2206.13699. [42] Y. Inamura, T. Nakatani, J. Suzuki, and T. Otomo, Development status of software “Utsusemi” for chopper spectrometers at MLF, J-PARC, J. Phys. Soc. Jpn. 82, SA031 (2013). [43] R. Ewings, A. Buts, M. Le, J. van Duijn, I. Bustinduy, and T. Perring, Horace: Software for the analysis of data from single crystal spectroscopy experiments at time-of-ﬂight neutron instruments, Nucl. Instrum. Methods Phys. Res. Sect. 834, 132 (2016). [44] G. Xu, Z. Xu, and J. M. Tranquada, Absolute cross-section normalization of magnetic neutron scattering data, Rev. Sci. Instrum. 84, 083906 (2013). [45] X. Gonze and C. Lee, Dynamical matrices, Born eﬀective charges, dielectric permittivity tensors, and interatomic force constants from density-functional perturbation theory, Phys. Rev. B 55, 10355 (1997). [47] G. Kresse and J. Hafner, Ab initio molecular-dynamics simulation of the liquid-metal–amorphous-semiconductor transition in germanium, Phys. Rev. B 49, 14251 (1994). [48] G. Kresse and J. Furthm¨uller, Eﬃciency of ab-initio total energy calculations for metals and semiconductors using a planewave basis set, Comput. Mater. Sci. 6, 15 (1996). [49] G. Kresse and J. Furthm¨uller, Eﬃcient iterative schemes for ab initio total-energy calculations using a plane-wave basis set, Phys. Rev. B 54, 11169 (1996). [51] G. Shirane, S. M. Shapiro, and J. M. Tranquada, Neutron Scattering with a Triple-Axis Spectrometer: Basic Techniques (Cambridge University Press, 2002). [53] C. Hwang, C.-H. Park, D. A. Siegel, A. V. Fedorov, S. G. Louie, and A. Lanzara, Direct measurement of quantum phases in graphene via photoemission spectroscopy, Phys. Rev. B 84, 125422 (2011). [54] S. Kourtis, Bulk spectroscopic measurement of the topological charge of Weyl nodes with resonant x rays, Phys. Rev. B 94, 125132 (2016).
Statistical Estimation of Confounded Linear MDPs: An Instrumental Variable Approach<|sep|>In this work, we present the ﬁrst statistical result of OPE in confounded MDPs based on the tool of instrumental variables. We propose a two-stage estimator of the value function from the oﬄine dataset, which is corrupted with observable confounders. In non-asymptotic viewpoint, we provide the two-stage estimator is close to the true value function with statistical rate �O(n−1/2). In asymptotic viewpoint, we prove that the two-stage estimator is asymptotic normal with typical rate n1/2, from which we open an approach to statistical inference for confounded MDPs. However, there are some directions to extend our work. Firstly, how does one design a two-stage estimator when the instrumental variable is continuous. Secondly, it is still unknown whether our two-stage estimator achieves semiparametric eﬃciency bound. We leave these issues to future work.
Inferring Networks of Substitutable and Complementary Products<|sep|>A useful recommender system must produce recommendations that not only match our preferences, but which are also relevant to our current topic of interest. For a user browsing a particular product, two useful notions of relevant recommendations include substitutes and complements: products that can be purchased instead of each other, and products that can be purchased in addition to each other. In this paper, our goal has been to learn these concepts from product features, especially from the text of their reviews. We have presented Sceptre, a model for predicting and understanding relationships between linked products. We have applied this to the problem of identifying substitutable and complementary products on a large collection of Amazon data, in Figure 6: (a,b,c) Examples of recommendations produced by Sceptre; the top of each subﬁgure shows the query product, the left column shows substitutes recommended by Sceptre, and the right column shows complements. (d) Interface used to evaluate Sceptre on Mechanical Turk; Turkers are shown lists of items suggested by Amazon (i.e., the ground-truth) and Sceptre and must identify which lists they prefer.
Anelastic Versus Fully Compressible Turbulent Rayleigh-B\'enard Convection<|sep|>This paper has presented the ﬁrst one-to-one comparison of anelastic and fully compressible turbulent convection. Our goal was to quantify the accuracy and eﬃciency of both methods for the simple test case of turbulent Rayleigh-B´enard convection in an ideal gas. The relation between the anelastic and fully compressible equations has been carved out in detail, without invoking subgrid-scale turbulence modeling at any stage. The anelastic approximation is expected to hold in the limit of small superadiabaticity ϵ, such that the Mach number M ∼ √ϵ remains small. We have shown that the fully compressible equations can be manipulated into a particular non-dimensional form, which consists of terms representing the anelastic dynamics plus O(ϵ) correction terms that guarantee the fully compressible physics. In the limit ϵ → 0 these correction terms vanish and the usual anelastic equations, as rigorously derived by formal amplitude expansions in previous works (e.g. Gough 1969; Lantz and Fan 1999), are recovered. Our approach helps to make the relation between the anelastic and fully compressible equations fully transparent, and also reveals that the familiar Boussinesq equations result in the double limit ϵ → 0, D → 0, where D = gd/(cpTr) is the dissipation number. The requirement of small Dissipation number is equivalent to a shallow convective system with a depth that is much smaller than the typical temperature scale height. A key aspect of this work was to quantify the diﬀerences between fully compressible and anelastic results. Therefore a suite of anelastic and fully compressible numerical simulations of thermal convection has been carried out. We compared global diagnostic quantities as well as depth proﬁles of the most important statistical moments of thermodynamic variables and velocities. All simulations reveal a coherent picture, showing that the fully compressible results converge to the anelastic ones with decreasing ϵ. The relative deviation between both cases was found to be approximately equal to the superadiabaticity ϵ, indicating linear convergence as predicted by theory. For ϵ ≳ 0.3 this linear trend is broken and larger deviations are encountered. Besides depending on the superadiabaticity, the degree to which both approaches give consistent results is controlled by the density contrast of the system, i.e. the ratio of bottom to the top density. Interestingly, due to a nonlinear eﬀect, larger density contrasts reduce the quantitative diﬀerences between anelastic and fully compressible models in our simulations. A further aspect of this work dealt with the comparison of the numerical eﬃciency of the anelastic and the fully compressible approach. In cases with ϵ ≪ 1 it is usually argued that solving the anelastic equations is computationally more eﬃcient than solving their fully compressible counterparts, because numerically costly sound waves are ﬁltered out. While our results generally conﬁrm this argument, they also show that fully compressible models appear to become more eﬃcient than anelastic simulations for ϵ ≥ O(0.1). The implications of our results for the simulations of astrophysical ﬂow phenomena might be illustrated by considering the speciﬁc example of solar convection. Standard solar models like, for example, Model S (Christensen-Dalsgaard et al. 1996)5, allow to estimate the superadiabaticity ϵ(z) = [∂zT − (∂zT)A] /∂zT as a function of depth within the solar interior. The superadiabaticity is predicted to be many orders of magnitude smaller than one in the lower 99% of the convection zone. It only reaches O(1) values within the outermost one per cent, close to the solar photosphere. This suggests that results obtained using the anelastic equations are indeed highly accurate in models excluding the thin outermost layer where the approximation breaks down. The dynamical consequences of neglecting this layer, however, need further investigation. In contrast, the fully compressible approach in principle is capable of capturing the relevant physical processes throughout the entire convection zone. This, however, forces modelers to use unrealistically large values for ϵ in the bulk of the convection zone for numerical reasons. An important result of our study is that this procedure introduces moderate errors only. Even for ϵ ≈ 0.1, where fully compressible codes tend to become more eﬃcient than anelastic models, the error in global diagnostics such as the overall heat transport or the average kinetic energy was found to be of the order of 10%. The impact on the turbulent ﬂow statistics was also shown to remain modest. In comparison to other sources of errors, arising for example from the inability to reach a realistic turbulence level in numerical simulations, a ten percent error seems tolerable. The above conclusions have been drawn from numerical simulations that neglect important ingredients of stellar convection, such as spherical geometry, rotation, compositional inhomogeneities, nuclear reactions, magnetic ﬁelds, penetration and overshooting in stably stratiﬁed layers, the corresponding wave-emission, and of course they did not reach the extreme ﬂow conditions of the solar interior. While boundary layers play an important role in regulating the convection eﬃciency in the simulations presented here (Grossmann and Lohse 2000; Petschel et al. 2013), their dominance is less evident in a more realistic model involving much higher Rayleigh numbers (Kraichnan 1962; Spiegel 1971; Grossmann and Lohse 2000) and more realistic boundary conditions (Brummell et al. 2002). Applying our results to the Sun is therefore somewhat speculative and the inclusion of additional physical processes in future comparative studies is clearly desirable. In particular, an issue that might arise in rapidly rotating systems has recently been pointed out by Calkins et al. (2014), who argued that the anelastic approximation breaks down in the geo- and astrophysically relevant case of rapid rotation and low Prandtl number. A study similar to the one presented here, but including the eﬀects of rapid rotation, is needed to resolve this question and is currently underway. The computations have been carried out on the PALMA computer cluster at M¨unster University and on the supercomputer JUQUEEN at the Forschungszentrum J¨ulich. This work was supported by the the German Research Foundation under the Priority Program 1488 (Planetary Magnetism). The volume work term p(∇ · v) in equation (3) is based on the divergence of the velocity ﬁeld, which is problematic in the anelastic approach. In the anelastic approximation the velocity ﬁeld is constrained by the anelastic continuity equation ∇ · (ρAv) = 0, which misses the information of the small superadiabatic density changes driving convection. Spiegel and Veronis (1960) deal with a related problem when deriving the Boussinesq approximation for shallow convection in an ideal gas. They show that p(∇ · v) is non-negligible, as small superadiabatic density variations become important, although the Boussinesq continuity equation requires incompressibility, i.e. ∇ · v = 0. Following their procedure, the volume work term in the fully compressible energy equation (3) can be reformulated by using the full continuity equation (1) and the ideal gas law (4). In order to do this, both (dimensional) equations are reorganized as follows, Using the above expressions, the volume work term in (3) can be formulated in terms of temperature and pressure, rather than with the divergence of the velocity ﬁeld, The anelastic expression for the left hand-side of the energy equation then can be derived by decomposing the thermodynamic variables in an adiabatic and a superadiabatic part, as described in section 3.1, and by neglecting all terms involving nonlinearities of variables denoting the superadiabatic part, anelastic = (cp − cv)ρA [∂tTS + (v · ∇)(TA + TS)] + (cp − cv)∂zTAvzρS − [∂tpS + (v · ∇)(pA + pS)] . (A4) In contrast, Rogers and Glatzmaier (2005) and Glatzmaier (2014) derive a diﬀerent version of the volume work term that is not equivalent to ours. Instead of using the full continuity equation and the full ideal gas law and ﬁnally making the anelastic approximation (i.e. neglecting all terms that are proportional to nonlinearities of superadiabatic thermodynamic quantities), they apply the anelastic versions of both equations, This anelastic energy equation diﬀers from ours (A3) and essentially involves the superadiabatic temperature as the only time-dependent thermodynamic variable. This handy formulation, however, has the disadvantage that corresponding numerical simulations carried out by us neither showed conservation of energy nor matched results with fully compressible numerical simulations. When assuming δp = 1, as valid for an ideal gas, the integration of equation (B2) reveals the integrated form of the thermodynamic relation for entropy, which directly relates entropy to temperature and pressure where sr is the reference entropy evaluated at the bottom of the domain. Decomposing the thermodynamic variables as done in section 3.1 and exploiting that the dimensional adiabatic background entropy proﬁle reads allows for the reformulation of the whole set of governing equations (1-4) in terms of entropy instead of temperature. The non-dimensional forms of the entropy formulation of the energy equation (B1) and the thermodynamic relation for entropy (B3) can be derived by using (B4) and ∆s = cp∆T/Tr to scale the superadiabatic entropy,
Multi-Step Bayesian Optimization for One-Dimensional Feasibility Determination<|sep|>In this paper, we consider a class of Bayesian optimization problems where the underlying prior is a Markov process and we pay a cost for each sample. We show that the Bayes-optimal policy is computationally tractable, by way of showing that the value function is completely determined by its values on a 3- or 4dimensional set. We use this optimal cost-per-sample policy to compute the optimal value when there is
Multimodal Logical Inference System for Visual-Textual Entailment<|sep|>We have proposed a logic-based system to achieve advanced visual-textual inference, demonstrating the importance of building a framework for representing the richer semantic content of texts and images. In the experiment, we have shown that our CCG-based pipeline system, consisting of graph translator, semantic parser and inference engine, can perform visual-textual inference with semantically complex sentences, without requiring any supervised data. We thank the two anonymous reviewers for their encouragement and insightful comments. This work was partially supported by JST CREST Grant Number JPMJCR1301, Japan.
Toward Few-step Adversarial Training from a Frequency Perspective<|sep|>We described an extension of PGD called SPGD and demonstrated its eﬀectiveness at attacking modelsand improving adversarial training. SPGD ﬁnds adversarial examples with higher loss in fewer attack steps than the PGD attack. However, we also proved that SPGD is a variant of PGD attack in the pixel domain when the sign operation is omitted. On CIFAR-10, our SPGD-trained Wide ResNet-18 model can achieve higher adversarial accuracy than the PGD-trained model. When the adversarial training steps are reduced to a half, our SPGD-trained model is on par with the PGDtrained model and can remain competitive. That is, we can use fewer attack steps to train a competitive model by using SPGD in adversarial training. Lastly, our visualization of these SPGDgenerated adversarial perturbations shows their components are not necessarily concentrated at high frequencies but are also found at low frequencies. In fact, we conjecture the adversarial perturbations can happen everywhere regardless of high frequency or low frequency. We hope this paper motivates more exploration in the frequency domain for adversarial analysis and serve as an inspiration for a broader view on the landscape of the adversarial examples.
Isospin violating dark matter in St\"uckelberg portal scenarios<|sep|>In this article, we have performed a thorough study of phenomenological features of hidden sector scenarios with St¨uckelberg Z′ portals that arise as low energy eﬀective actions of certain type II string compactiﬁcations with intersecting branes. For our purposes, the crucial property of these constructions is the unavoidable extension of the SM gauge group by several (‘anomalous’) abelian gauge bosons which gain a mass and can mix with analogous bosons from hidden sectors. Many interesting phenomenological properties of such setups are determined by the charges of the SM spectrum under the extra U(1)s of the visible sector, together with a handful of mixing parameters (a, b, c, d). The possible choices for the charges are rather scarce, due to the necessary identiﬁcation of these symmetries with approximate global symmetries of the SM. We have focussed on a particular gauge structure, the Madrid models that arises in a large class of intersecting brane constructions. Some other conﬁgurations are possible, and they could be studied in analogy. We believe, nevertheless, that our analysis covers a signiﬁcant portion of the landscape of semi-realistic brane models. Once the extra visible U(1) bosons mix with those from the hidden sectors, the lightest Z′ mass eigenstate generates the dominant interactions between DM and SM fermions. A particularly appealing and characteristic feature of such models, is the natural appearance of rich patterns of isospin violating DM interactions, which contrasts with other simple portals traditionally considered in the literature. We have explored the prospects for fn/fp and an/ap in six diﬀerent BM points of the parameter space of these constructions, incorporating LHC and LUX bounds showing that in general values of these ratios tend to be dominated by the neutron contribution. Target materials with more sensitivity to neutron interactions are thus very suitable to explore these scenarios. Generically, this setup provides isospin violating couplings both in the SI and SD interactions. We have confronted our prospects with LUX and LHC bounds for a set of BM points. By using our own simulation of the LUX experiment, we have performed a check of the exclusion regions for each point using the maximum gap method. This has allowed us to analyse consistently a general scenario with SI and SD (proton and neutron contributions) interactions as well as in general cases of isospin violating couplings of DM. For the LHC we have calculated, for each point of the parameter space, the production cross section of a Z′ boson times the branching ratio of a speciﬁc decay. With this, we have included ATLAS searches for dilepton (e+e− and µ+µ−) and dijet resonances. Remarkably, all regions experimentally allowed entail much higher neutron than proton cross sections for the SI interactions while for the SD the situation is less constrained. The ﬁndings of this work open the door to generic scenarios in which the signals in direct detection experiments can be dominated by neutrons. Moreover, we show that the existing complementarity between LHC searches and direct detection experiments is specially relevant to disentangle the couplings of the Z′ boson to SM particles. It is gratifying to see how, not only diﬀerent experimental strategies, but also phenomenological and fundamental theoretical input can be combined into a single framework to shed some light into the possible properties of the so far elusive nature of dark matter. The authors are grateful to D. G. Cerde˜no, L. Iba˜nez, F. Kahlhoefer and G. Shiu for useful comments. V.M.L. and M.P. would like to thank the support of the European Union under the ERC Advanced Grant SPLE under contract ERC-2012-ADG-20120216-320421, the support of the Consolider-Ingenio 2010 programme under grant MULTIDARK CSD2009-00064, the Spanish MICINN under Grant No. FPA2012-34694, the Spanish MINECO “Centro de excelencia Severo Ochoa Program” under Grant No. SEV-2012-0249, and the Community of Madrid under Grant No. HEPHACOS S2009/ESP1473. P.S. would like to thank DESY, the University of Hamburg, and the Hong Kong IAS for kind hospitality during the completion of this work. He acknowledges support from the DOE grant DEFG-02-95ER40896 and the HKRGC grant HKUST4/CRF/13G, 604231, as well as the Collaborative Research Center SFB676 of the DFG at the University of Hamburg.
Adversarial Attacks on Deep Learning Based mmWave Beam Prediction in 5G and Beyond<|sep|>We presented an adversarial attack to fool the beam selection process of the IA using a DNN classiﬁer that uses a subset of beams to predict the beam that is best oriented to the receiver. We investigated two different attack methods, namely, the non-targeted FGM attack that only aims to fool the DNN classiﬁer with misclassiﬁcation to any other beam label, and the k-worst beam attack that not only fools the DNN classiﬁer but also enforces the label that is chosen at the DNN to be in one of the k-worst beams. We showed that the adversarial attack can signiﬁcantly decrease the accuracy of the DNN and fool the DNN into selecting the worst beam. Results demonstrate that as DL ﬁnds applications for beam prediction in mmWave communication for 5G and beyond, the IA process becomes vulnerable to adversarial attacks that can signiﬁcantly reduce the beam selection performance.
Bias-corrected methods for estimating the receiver operating characteristic surface of continuous diagnostic tests<|sep|>This paper proposed several veriﬁcation bias-corrected estimators of the ROC surface (and the VUS) of a continuous diagnostic test. These estimators, which can be considered an extension to the three-class case of estimators in [3], are partially parametric in that they require the choice of a parametric model for the estimation of the disease process, or of the veriﬁcation process, or of both processes. In some cases, wrong speciﬁcations of such models can visibly aﬀect the produced estimates, as highlighted also by our results in the simulation studies. To avoid misspeciﬁcation problems, one possibility could be to resort on fully nonparametric estimators. This topic will be the focus of our future work. In this section, we discuss validity of conditions (C1), (C2) and (C3) for the proposed estimators. The discussion covers ﬁrst the elements of the estimating functions corresponding to the parameter τ. Then, we pass to the elements of the estimating functions corresponding to the parameters θ1, θ2, θ11, β12, β22, β23, specializing the discussion to the various methods. Finally, we give the explicit form of the variance-covariance matrix in Theorem 1. Recall that α0 denotes the true value of α. Parameter τ. We noted in Section 3 that estimators FI, MSI and SPE require a multinomial logistic or probit regression model to estimate the disease probabilities ρki = Pr(Dki = 1|Ti, Ai) with k = 1, 2, 3. In the following, we adopt the multinomial logistic model, but arguments similar to those given below hold also for the multinomial probit model, despite the rather more complex algebra (see Daganzo [5], Chapter 3, as a general reference). The estimating function for the nuisance parameter τ ≡ τρ = (τρ1, τρ2)⊤, ∂βjk gτρ i (α) = 0 for each s, j, k. The second–order partial derivatives can be easily derived. Hence, for Gτρ(α), condition (C2) holds and, by assumption (A3)–(A5) condition (C3) also holds. The IPW and SPE estimators require estimates of πi = Pr(Vi = 1|Ti, Ai). With T and A as covariates, we can use the logistic or probit models to this end. In these cases, conditions (C1)–(C3) are satisﬁed by the estimating functions where φ(·) and Φ(·) are the density function and the cumulative distribution function of the standard normal random variable, respectively. Recall that τπ is the component of nuisance parameter τ corresponding the model for estimating π. The ﬁrst-order derivatives are ∂ ∂τπ gτπ i (α) = −ViUiU ⊤ i φ(U ⊤ i τπ) � −U ⊤ i τπΦ(U ⊤ i τπ) − φ(U ⊤ i τπ) � − (1 − Vi)UiU ⊤ i φ(U ⊤ i τπ) � U ⊤ i τπ(Φ(U ⊤ i τπ) − 1) + φ(U ⊤ i τπ) � FI and MSI estimators. According to equations (3.4), (3.5) and (3.6), the estimating functions Gθs ∗ (α) for FI and MSI estimators can be presented in the form for j = 1, 2 and k = 1, 2, 3. Here, the notation IE means “imputation estimator”. The estimating function corresponds to the FI estimator if m = 0, to the MSI estimator if m = 1. Using the conditional expectation and the assumption (A1), Hence, under assumption (A2), condition (C1) holds for Gθs IE(α) and Gβjk IE (α). We now verify conditions (C2) and (C3). The partial derivative of Gθs IE(α) tives of both Gθs IE(α) and Gβjk IE (α). The only not null elements of the second– order partial derivative of Gθs IE(α) and Gβjk IE (α) are those corresponding to the matrices ∂2 ∂τ∂τ ⊤ Gβjk IE (α). These elements involve the derivatives with respect to τ of quantities in (A.2). It follows that conditions (C2) and (C3) hold for Gθs IE(α) and Gβjk IE (α) for each s, j, k. IPW estimator. Recall that the estimating function for θs is E � Viπ−1 i (Dsi − θs0) � = EDs,T,A � E � Viπ−1 i (Dsi − θs0) |, Ti, Ai �� Also in this case, computation of the second–order partial derivatives develops similarly and the results imply that the conditions (C2) and (C3) hold. Asymptotic covariance matrix. Recall that the asymptotic covariance matrix of TCF estimators is obtained as The elements gi,∗(α) of the estimating functions G∗(α) are given in the previous paragraphs. Now, we derive the explicit form for ∂ ∂αgi,∗(α). with j, l, s = 1, 2, k = 1, 2, 3 and i = 1, . . . , n (see (A.1) and (A.2) for the multinomial logistic modeling of the disease process). Thus, −1 0 0 0 0 0 A11i A21i 0 −1 0 0 0 0 A12i A22i 0 0 −1 0 0 0 B111i B121i 0 0 0 −1 0 0 B112i B122i 0 0 0 0 −1 0 B212i B222i 0 0 0 0 0 −1 B213i B223i 0 0 0 0 0 0 C11i C21i 0 0 0 0 0 0 C12i C22i −1 0 0 0 0 0 H11i H21i D1i 0 −1 0 0 0 0 H12i H22i D2i 0 0 −1 0 0 0 G111i G121i E11i 0 0 0 −1 0 0 G112i G122i E12i 0 0 0 0 −1 0 G212i G222i E22i 0 0 0 0 0 −1 G213i G223i E23i 0 0 0 0 0 0 C11i C21i 0 0 0 0 0 0 0 C12i C22i 0 0 0 0 0 0 0 0 0 Ci In this section, we present results of simulations in Study 1 and Study 2. Tables 9–14 show simulation results of Study 1 for sample sizes equal to 500 and 1000, respectively. The results of Study 2 are presented in Tables 15 and 16, corresponding to the ﬁrst and third value of Λ, respectively. In this section, we give some simulation results concerning the estimators of the VUS presented in Subsection 3.6. The disease D is generated by a trinomial random vector (D1, D2, D3), such that Dk is a Bernoulli random variable with mean θk, k = 1, 2, 3. We set θ1 = 0.4, θ2 = 0.35 and θ3 = 0.25. The pairs T, A are generated from the following conditional models The true VUS value is equal to 0.9472 for the ﬁrst value of Λ and (µT , µA) = (3, 2); is equal to 0.7175 for the second value of Λ and (µT , µA) = (2, 1); is equal to 0.4778 for the third value of Λ and (µT , µA) = (2, 1). We simulate the veriﬁcation status V by using the following model The parameters (δ0, δ1, δ2) are ﬁxed equal to (1, −2.87, 4.06) when the ﬁrst value of Λ is considered, and equal to (1, −2.2, 4) otherwise. These choices give rise to a veriﬁcation rate of about 0.52. Under our data–generating setting, the disease process follows a multinomial logistic model. We consider two sample Table 9. Simulation results from 5000 replications when both models for ρk and π are correctly speciﬁed (Study 1) and the ﬁrst value of Λ is considered. “True” denotes the true parameter value. Sample size = 500. cut-point = (2,4) True 0.5000 0.4347 0.9347 FI 0.5007 0.4357 0.9343 0.0373 0.0339 0.0190 0.0343 0.0309 0.0360 0.0372 0.0340 0.0193 MSI 0.5008 0.4357 0.9343 0.0382 0.0378 0.0225 0.0353 0.0354 0.0380 0.0381 0.0380 0.0227 IPW 0.5020 0.4357 0.9345 0.0506 0.0508 0.0260 0.0493 0.0502 0.0280 0.0499 0.0507 0.0262 SPE 0.5012 0.4357 0.9345 0.0401 0.0456 0.0256 0.0399 0.0450 0.0249 0.0409 0.0457 0.0259 cut-point = (2,5) True 0.5000 0.7099 0.7752 FI 0.5007 0.7115 0.7747 0.0373 0.0329 0.0372 0.0343 0.0321 0.0436 0.0372 0.0329 0.0374 MSI 0.5008 0.7111 0.7743 0.0382 0.0361 0.0395 0.0353 0.0355 0.0455 0.0381 0.0362 0.0396 IPW 0.5020 0.7111 0.7743 0.0506 0.0496 0.0464 0.0493 0.0479 0.0500 0.0499 0.0487 0.0463 SPE 0.5012 0.7112 0.7744 0.0401 0.0434 0.0442 0.0399 0.0425 0.0433 0.0409 0.0436 0.0448 cut-point = (2,7) True 0.5000 0.9230 0.2248 FI 0.5007 0.9228 0.2241 0.0373 0.0167 0.0377 0.0343 0.0229 0.0310 0.0372 0.0169 0.0370 MSI 0.5008 0.9230 0.2242 0.0382 0.0199 0.0382 0.0353 0.0253 0.0317 0.0381 0.0202 0.0376 IPW 0.5020 0.9232 0.2242 0.0506 0.0266 0.0534 0.0493 0.0251 0.0520 0.0499 0.0263 0.0525 SPE 0.5012 0.9235 0.2245 0.0401 0.0255 0.0416 0.0399 0.0244 0.0403 0.0409 0.0253 0.0545 cut-point = (4,5) True 0.9347 0.2752 0.7752 FI 0.9349 0.2758 0.7747 0.0176 0.0285 0.0372 0.0161 0.0246 0.0436 0.0174 0.0289 0.0374 MSI 0.9348 0.2754 0.7743 0.0194 0.0326 0.0395 0.0179 0.0291 0.0455 0.0191 0.0328 0.0396 IPW 0.9352 0.2754 0.7743 0.0299 0.0472 0.0464 0.0263 0.0466 0.0500 0.0284 0.0472 0.0463 SPE 0.9353 0.2755 0.7744 0.0270 0.0407 0.0442 0.0249 0.0399 0.0433 0.0291 0.0404 0.0448 cut-point = (4,7) True 0.9347 0.4883 0.2248 FI 0.9349 0.4872 0.2241 0.0176 0.0375 0.0377 0.0161 0.0347 0.0310 0.0174 0.0375 0.0370 MSI 0.9348 0.4872 0.2242 0.0194 0.0396 0.0382 0.0179 0.0373 0.0317 0.0191 0.0398 0.0376 IPW 0.9352 0.4876 0.2242 0.0299 0.0520 0.0534 0.0263 0.0511 0.0520 0.0284 0.0516 0.0525 SPE 0.9353 0.4877 0.2245 0.0270 0.0462 0.0416 0.0249 0.0456 0.0403 0.0291 0.0463 0.0545 cut-point = (5,7) True 0.9883 0.2132 0.2248 FI 0.9882 0.2114 0.2241 0.0051 0.0310 0.0377 0.0047 0.0274 0.0310 0.0054 0.0306 0.0370 MSI 0.9882 0.2118 0.2242 0.0069 0.0330 0.0382 0.0058 0.0299 0.0317 0.0069 0.0329 0.0376 IPW 0.9886 0.2121 0.2242 0.0137 0.0467 0.0534 0.0088 0.0441 0.0520 0.0130 0.0452 0.0525 SPE 0.9886 0.2123 0.2245 0.0133 0.0398 0.0416 0.0097 0.0387 0.0403 0.0127 0.0398 0.0545 Table 10. Simulation results from 5000 replications when both models for ρk and π are correctly speciﬁed (Study 1) and the ﬁrst value of Λ is considered. “True” denotes the true parameter value. Sample size = 1000. cut-point = (2,4) True 0.5000 0.4347 0.9347 FI 0.5001 0.4346 0.9348 0.0265 0.0235 0.0133 0.0242 0.0217 0.0254 0.0262 0.0238 0.0135 MSI 0.5002 0.4349 0.9349 0.0273 0.0264 0.0157 0.0250 0.0250 0.0268 0.0269 0.0268 0.0160 IPW 0.5006 0.4357 0.9349 0.0362 0.0357 0.0184 0.0352 0.0358 0.0198 0.0353 0.0360 0.0185 SPE 0.5004 0.4353 0.9349 0.0287 0.0321 0.0180 0.0282 0.0320 0.0180 0.0283 0.0322 0.0182 cut-point = (2,5) True 0.5000 0.7099 0.7752 FI 0.5001 0.7096 0.7758 0.0265 0.0232 0.0260 0.0242 0.0227 0.0308 0.0262 0.0232 0.0263 MSI 0.5002 0.7095 0.7756 0.0273 0.0256 0.0276 0.0250 0.0251 0.0321 0.0269 0.0256 0.0279 IPW 0.5006 0.7104 0.7756 0.0362 0.0349 0.0325 0.0352 0.0342 0.0354 0.0353 0.0345 0.0327 SPE 0.5004 0.7100 0.7757 0.0287 0.0309 0.0307 0.0282 0.0303 0.0308 0.0283 0.0305 0.0310 cut-point = (2,7) True 0.5000 0.9230 0.2248 FI 0.5001 0.9228 0.2250 0.0265 0.0117 0.0260 0.0242 0.0160 0.0220 0.0262 0.0119 0.0262 MSI 0.5002 0.9230 0.2252 0.0273 0.0141 0.0265 0.0250 0.0178 0.0226 0.0269 0.0142 0.0266 IPW 0.5006 0.9233 0.2258 0.0362 0.0187 0.0383 0.0352 0.0181 0.0374 0.0353 0.0186 0.0375 SPE 0.5004 0.9235 0.2256 0.0287 0.0180 0.0286 0.0282 0.0176 0.0286 0.0283 0.0180 0.0291 cut-point = (4,5) True 0.9347 0.2752 0.7752 FI 0.9346 0.2749 0.7758 0.0124 0.0203 0.0260 0.0115 0.0173 0.0308 0.0123 0.0203 0.0263 MSI 0.9345 0.2746 0.7756 0.0137 0.0232 0.0276 0.0128 0.0205 0.0321 0.0136 0.0231 0.0279 IPW 0.9346 0.2748 0.7756 0.0213 0.0337 0.0325 0.0196 0.0332 0.0354 0.0205 0.0335 0.0327 SPE 0.9344 0.2747 0.7757 0.0190 0.0286 0.0307 0.0183 0.0283 0.0308 0.0187 0.0285 0.0310 cut-point = (4,7) True 0.9347 0.4883 0.2248 FI 0.9346 0.4882 0.2250 0.0124 0.0262 0.0260 0.0115 0.0245 0.0220 0.0123 0.0264 0.0262 MSI 0.9345 0.4881 0.2252 0.0137 0.0279 0.0265 0.0128 0.0263 0.0226 0.0136 0.0280 0.0266 IPW 0.9346 0.4876 0.2258 0.0213 0.0365 0.0383 0.0196 0.0364 0.0374 0.0205 0.0366 0.0375 SPE 0.9344 0.4882 0.2256 0.0190 0.0325 0.0286 0.0183 0.0324 0.0286 0.0187 0.0326 0.0291 cut-point = (5,7) True 0.9883 0.2132 0.2248 FI 0.9881 0.2132 0.2250 0.0036 0.0217 0.0260 0.0033 0.0194 0.0220 0.0037 0.0216 0.0262 MSI 0.9881 0.2135 0.2252 0.0048 0.0234 0.0265 0.0044 0.0212 0.0226 0.0049 0.0232 0.0266 IPW 0.9882 0.2129 0.2258 0.0100 0.0325 0.0383 0.0077 0.0317 0.0374 0.0097 0.0320 0.0375 SPE 0.9880 0.2135 0.2256 0.0097 0.0282 0.0286 0.0080 0.0276 0.0286 0.0094 0.0278 0.0291 Table 11. Simulation results from 5000 replications when both models for ρk and π are correctly speciﬁed (Study 1) and the second value of Λ is considered. “True” denotes the true parameter value. Sample size = 500. cut-point = (2,4) True 0.5000 0.3970 0.8970 FI 0.4999 0.3974 0.8965 0.0356 0.0294 0.0253 0.0326 0.0263 0.0355 0.0355 0.0298 0.0256 MSI 0.4999 0.3975 0.8961 0.0368 0.0355 0.0291 0.0339 0.0326 0.0380 0.0367 0.0355 0.0291 IPW 0.5000 0.3977 0.8962 0.0470 0.0492 0.0373 0.0460 0.0484 0.0369 0.0464 0.0487 0.0368 SPE 0.5000 0.3976 0.8963 0.0402 0.0446 0.0363 0.0397 0.0438 0.0348 0.0400 0.0442 0.0356 cut-point = (2,5) True 0.5000 0.6335 0.7365 FI 0.4999 0.6342 0.7360 0.0356 0.0303 0.0410 0.0326 0.0287 0.0439 0.0355 0.0306 0.0409 MSI 0.4999 0.6339 0.7358 0.0368 0.0356 0.0437 0.0339 0.0342 0.0463 0.0367 0.0357 0.0435 IPW 0.5000 0.6336 0.7363 0.0470 0.0477 0.0528 0.0460 0.0471 0.0529 0.0464 0.0474 0.0514 SPE 0.5000 0.6341 0.7362 0.0402 0.0440 0.0494 0.0397 0.0434 0.0479 0.0400 0.0437 0.0483 cut-point = (2,7) True 0.5000 0.8682 0.2635 FI 0.4999 0.8677 0.2631 0.0356 0.0222 0.0388 0.0326 0.0233 0.0352 0.0355 0.0219 0.0391 MSI 0.4999 0.8678 0.2633 0.0368 0.0263 0.0401 0.0339 0.0272 0.0370 0.0367 0.0261 0.0407 IPW 0.5000 0.8677 0.2638 0.0470 0.0354 0.0477 0.0460 0.0341 0.0486 0.0464 0.0349 0.0484 SPE 0.5000 0.8679 0.2635 0.0402 0.0336 0.0420 0.0397 0.0326 0.0420 0.0400 0.0331 0.0424 cut-point = (4,5) True 0.8970 0.2365 0.7365 FI 0.8972 0.2368 0.7360 0.0205 0.0257 0.0410 0.0195 0.0223 0.0439 0.0203 0.0258 0.0409 MSI 0.8968 0.2364 0.7358 0.0229 0.0310 0.0437 0.0219 0.0279 0.0463 0.0226 0.0308 0.0435 IPW 0.8969 0.2359 0.7363 0.0268 0.0421 0.0528 0.0261 0.0411 0.0529 0.0265 0.0415 0.0514 SPE 0.8967 0.2365 0.7362 0.0260 0.0374 0.0494 0.0254 0.0370 0.0479 0.0257 0.0373 0.0483 cut-point = (4,7) True 0.8970 0.4711 0.2635 FI 0.8972 0.4703 0.2631 0.0205 0.0356 0.0388 0.0195 0.0328 0.0352 0.0203 0.0356 0.0391 MSI 0.8968 0.4703 0.2633 0.0229 0.0398 0.0401 0.0219 0.0370 0.0370 0.0226 0.0394 0.0407 IPW 0.8969 0.4699 0.2638 0.0268 0.0492 0.0477 0.0261 0.0483 0.0486 0.0265 0.0486 0.0484 SPE 0.8967 0.4703 0.2635 0.0260 0.0454 0.0420 0.0254 0.0445 0.0420 0.0257 0.0449 0.0424 cut-point = (5,7) True 0.9711 0.2347 0.2635 FI 0.9710 0.2335 0.2631 0.0086 0.0283 0.0388 0.0084 0.0260 0.0352 0.0088 0.0284 0.0391 MSI 0.9711 0.2339 0.2633 0.0116 0.0327 0.0401 0.0111 0.0304 0.0370 0.0117 0.0325 0.0407 IPW 0.9711 0.2341 0.2638 0.0144 0.0402 0.0477 0.0136 0.0397 0.0486 0.0143 0.0400 0.0484 SPE 0.9711 0.2339 0.2635 0.0142 0.0376 0.0420 0.0135 0.0370 0.0420 0.0141 0.0373 0.0424 Table 12. Simulation results from 5000 replications when both models for ρk and π are correctly speciﬁed (Study 1) and the second value of Λ is considered. “True” denotes the true parameter value. Sample size = 1000. cut-point = (2,4) True 0.5000 0.3970 0.8970 FI 0.4997 0.3967 0.8966 0.0248 0.0208 0.0177 0.0230 0.0185 0.0250 0.0250 0.0209 0.0180 MSI 0.4997 0.3965 0.8966 0.0257 0.0251 0.0202 0.0240 0.0230 0.0268 0.0259 0.0250 0.0205 IPW 0.4994 0.3967 0.8967 0.0323 0.0349 0.0259 0.0327 0.0343 0.0263 0.0327 0.0344 0.0260 SPE 0.4997 0.3966 0.8967 0.0279 0.0317 0.0251 0.0281 0.0311 0.0250 0.0282 0.0311 0.0252 cut-point = (2,5) True 0.5000 0.6335 0.7365 FI 0.4997 0.6330 0.7364 0.0248 0.0215 0.0286 0.0230 0.0203 0.0310 0.0250 0.0216 0.0288 MSI 0.4997 0.6327 0.7361 0.0257 0.0253 0.0304 0.0240 0.0241 0.0327 0.0259 0.0252 0.0307 IPW 0.4994 0.6326 0.7365 0.0323 0.0339 0.0360 0.0327 0.0335 0.0375 0.0327 0.0335 0.0363 SPE 0.4997 0.6328 0.7362 0.0279 0.0314 0.0338 0.0281 0.0308 0.0340 0.0282 0.0309 0.0341 cut-point = (2,7) True 0.5000 0.8682 0.2635 FI 0.4997 0.8679 0.2640 0.0248 0.0153 0.0274 0.0230 0.0164 0.0249 0.0250 0.0154 0.0275 MSI 0.4997 0.8680 0.2643 0.0257 0.0183 0.0286 0.0240 0.0192 0.0262 0.0259 0.0184 0.0287 IPW 0.4994 0.8682 0.2645 0.0323 0.0248 0.0343 0.0327 0.0244 0.0345 0.0327 0.0246 0.0341 SPE 0.4997 0.8682 0.2644 0.0279 0.0236 0.0299 0.0281 0.0232 0.0297 0.0282 0.0234 0.0298 cut-point = (4,5) True 0.8970 0.2365 0.7365 FI 0.8971 0.2363 0.7364 0.0144 0.0180 0.0286 0.0138 0.0157 0.0310 0.0143 0.0182 0.0288 MSI 0.8971 0.2362 0.7361 0.0160 0.0217 0.0304 0.0155 0.0197 0.0327 0.0160 0.0217 0.0307 IPW 0.8972 0.2359 0.7365 0.0188 0.0297 0.0360 0.0186 0.0291 0.0375 0.0187 0.0293 0.0363 SPE 0.8972 0.2362 0.7362 0.0183 0.0264 0.0338 0.0181 0.0262 0.0340 0.0182 0.0262 0.0341 cut-point = (4,7) True 0.8970 0.4711 0.2635 FI 0.8971 0.4712 0.2640 0.0144 0.0252 0.0274 0.0138 0.0232 0.0249 0.0143 0.0250 0.0275 MSI 0.8971 0.4715 0.2643 0.0160 0.0280 0.0286 0.0155 0.0261 0.0262 0.0160 0.0278 0.0287 IPW 0.8972 0.4715 0.2645 0.0188 0.0348 0.0343 0.0186 0.0342 0.0345 0.0187 0.0343 0.0341 SPE 0.8972 0.4717 0.2644 0.0183 0.0321 0.0299 0.0181 0.0316 0.0297 0.0182 0.0316 0.0298 cut-point = (5,7) True 0.9711 0.2347 0.2635 FI 0.9709 0.2350 0.2640 0.0061 0.0201 0.0274 0.0060 0.0184 0.0249 0.0062 0.0200 0.0275 MSI 0.9709 0.2353 0.2643 0.0082 0.0229 0.0286 0.0080 0.0216 0.0262 0.0082 0.0229 0.0287 IPW 0.9709 0.2356 0.2645 0.0101 0.0285 0.0343 0.0099 0.0282 0.0345 0.0102 0.0283 0.0341 SPE 0.9710 0.2354 0.2644 0.0100 0.0266 0.0299 0.0098 0.0263 0.0297 0.0100 0.0264 0.0298 Table 13. Simulation results from 5000 replications when both models for ρk and π are correctly speciﬁed (Study 1) and the third value of Λ is considered. “True” denotes the true parameter value. Sample size = 500. cut-point = (2,4) True 0.5000 0.3031 0.8031 FI 0.5001 0.3027 0.8034 0.0356 0.0240 0.0348 0.0320 0.0206 0.0373 0.0350 0.0242 0.0346 MSI 0.5001 0.3031 0.8034 0.0375 0.0322 0.0384 0.0340 0.0291 0.0408 0.0367 0.0318 0.0383 IPW 0.5004 0.3031 0.8037 0.0454 0.0444 0.0454 0.0438 0.0439 0.0451 0.0440 0.0441 0.0452 SPE 0.5002 0.3032 0.8036 0.0410 0.0411 0.0441 0.0400 0.0406 0.0438 0.0401 0.0407 0.0440 cut-point = (2,5) True 0.5000 0.4682 0.6651 FI 0.5001 0.4681 0.6656 0.0356 0.0266 0.0438 0.0320 0.0237 0.0430 0.0350 0.0271 0.0428 MSI 0.5001 0.4679 0.6654 0.0375 0.0348 0.0469 0.0340 0.0325 0.0461 0.0367 0.0350 0.0459 IPW 0.5004 0.4676 0.6655 0.0454 0.0475 0.0538 0.0438 0.0474 0.0526 0.0440 0.0476 0.0524 SPE 0.5002 0.4678 0.6654 0.0410 0.0440 0.0513 0.0400 0.0440 0.0500 0.0401 0.0442 0.0503 cut-point = (2,7) True 0.5000 0.7027 0.3349 FI 0.5001 0.7033 0.3346 0.0356 0.0268 0.0424 0.0320 0.0246 0.0383 0.0350 0.0267 0.0412 MSI 0.5001 0.7033 0.3346 0.0375 0.0336 0.0455 0.0340 0.0318 0.0414 0.0367 0.0334 0.0441 IPW 0.5004 0.7031 0.3352 0.0454 0.0439 0.0515 0.0438 0.0437 0.0505 0.0440 0.0440 0.0504 SPE 0.5002 0.7034 0.3347 0.0410 0.0416 0.0481 0.0400 0.0413 0.0465 0.0401 0.0414 0.0468 cut-point = (4,5) True 0.8031 0.1651 0.6651 FI 0.8033 0.1654 0.6656 0.0278 0.0196 0.0438 0.0260 0.0166 0.0430 0.0274 0.0196 0.0428 MSI 0.8030 0.1648 0.6654 0.0303 0.0256 0.0469 0.0284 0.0236 0.0461 0.0297 0.0259 0.0459 IPW 0.8030 0.1645 0.6655 0.0344 0.0346 0.0538 0.0335 0.0346 0.0526 0.0337 0.0349 0.0524 SPE 0.8030 0.1645 0.6654 0.0334 0.0317 0.0513 0.0325 0.0321 0.0500 0.0326 0.0322 0.0503 cut-point = (4,7) True 0.8031 0.3996 0.3349 FI 0.8033 0.4007 0.3346 0.0278 0.0300 0.0424 0.0260 0.0268 0.0383 0.0274 0.0299 0.0412 MSI 0.8030 0.4002 0.3346 0.0303 0.0367 0.0455 0.0284 0.0339 0.0414 0.0297 0.0364 0.0441 IPW 0.8030 0.4000 0.3352 0.0344 0.0458 0.0515 0.0335 0.0456 0.0505 0.0337 0.0458 0.0504 SPE 0.8030 0.4002 0.3347 0.0334 0.0431 0.0481 0.0325 0.0429 0.0465 0.0326 0.0430 0.0468 cut-point = (5,7) True 0.8996 0.2345 0.3349 FI 0.8996 0.2353 0.3346 0.0192 0.0245 0.0424 0.0182 0.0220 0.0383 0.0190 0.0248 0.0412 MSI 0.8996 0.2354 0.3346 0.0221 0.0307 0.0455 0.0212 0.0288 0.0414 0.0219 0.0310 0.0441 IPW 0.8997 0.2355 0.3352 0.0253 0.0384 0.0515 0.0249 0.0388 0.0505 0.0252 0.0391 0.0504 SPE 0.8997 0.2356 0.3347 0.0249 0.0364 0.0481 0.0246 0.0366 0.0465 0.0247 0.0367 0.0468 Table 14. Simulation results from 5000 replications when both models for ρk and π are correctly speciﬁed (Study 1) and the third value of Λ is considered. “True” denotes the true parameter value. Sample size = 1000. cut-point = (2,4) True 0.5000 0.3031 0.8031 FI 0.5003 0.3030 0.8040 0.0242 0.0169 0.0243 0.0226 0.0145 0.0264 0.0247 0.0170 0.0243 MSI 0.5001 0.3030 0.8038 0.0256 0.0222 0.0270 0.0240 0.0206 0.0288 0.0259 0.0224 0.0270 IPW 0.5001 0.3032 0.8038 0.0310 0.0310 0.0320 0.0310 0.0311 0.0320 0.0311 0.0311 0.0319 SPE 0.5001 0.3030 0.8040 0.0281 0.0285 0.0312 0.0283 0.0288 0.0310 0.0283 0.0287 0.0311 cut-point = (2,5) True 0.5000 0.4682 0.6651 FI 0.5003 0.4682 0.6663 0.0242 0.0193 0.0301 0.0226 0.0167 0.0304 0.0247 0.0191 0.0301 MSI 0.5001 0.4681 0.6663 0.0256 0.0248 0.0320 0.0240 0.0230 0.0326 0.0259 0.0247 0.0324 IPW 0.5001 0.4683 0.6664 0.0310 0.0337 0.0368 0.0310 0.0336 0.0373 0.0311 0.0336 0.0370 SPE 0.5001 0.4682 0.6665 0.0281 0.0311 0.0350 0.0283 0.0312 0.0355 0.0283 0.0312 0.0355 cut-point = (2,7) True 0.5000 0.7027 0.3349 FI 0.5003 0.7028 0.3359 0.0242 0.0188 0.0289 0.0226 0.0173 0.0271 0.0247 0.0188 0.0290 MSI 0.5001 0.7025 0.3359 0.0256 0.0236 0.0307 0.0240 0.0225 0.0293 0.0259 0.0236 0.0311 IPW 0.5001 0.7023 0.3360 0.0310 0.0311 0.0350 0.0310 0.0310 0.0358 0.0311 0.0311 0.0356 SPE 0.5001 0.7024 0.3358 0.0281 0.0292 0.0324 0.0283 0.0293 0.0329 0.0283 0.0293 0.0330 cut-point = (4,5) True 0.8031 0.1651 0.6651 FI 0.8034 0.1652 0.6663 0.0193 0.0139 0.0301 0.0184 0.0117 0.0304 0.0193 0.0138 0.0301 MSI 0.8032 0.1652 0.6663 0.0211 0.0184 0.0320 0.0201 0.0167 0.0326 0.0209 0.0183 0.0324 IPW 0.8034 0.1651 0.6664 0.0241 0.0248 0.0368 0.0237 0.0246 0.0373 0.0237 0.0247 0.0370 SPE 0.8032 0.1653 0.6665 0.0233 0.0229 0.0350 0.0230 0.0228 0.0355 0.0230 0.0228 0.0355 cut-point = (4,7) True 0.8031 0.3996 0.3349 FI 0.8034 0.3998 0.3359 0.0193 0.0207 0.0289 0.0184 0.0189 0.0271 0.0193 0.0210 0.0290 MSI 0.8032 0.3995 0.3359 0.0211 0.0253 0.0307 0.0201 0.0240 0.0293 0.0209 0.0256 0.0311 IPW 0.8034 0.3991 0.3360 0.0241 0.0319 0.0350 0.0237 0.0323 0.0358 0.0237 0.0323 0.0356 SPE 0.8032 0.3994 0.3358 0.0233 0.0299 0.0324 0.0230 0.0303 0.0329 0.0230 0.0304 0.0330 cut-point = (5,7) True 0.8996 0.2345 0.3349 FI 0.8998 0.2346 0.3359 0.0134 0.0172 0.0289 0.0129 0.0155 0.0271 0.0134 0.0174 0.0290 MSI 0.8997 0.2343 0.3359 0.0157 0.0216 0.0307 0.0150 0.0204 0.0293 0.0155 0.0218 0.0311 IPW 0.8998 0.2340 0.3360 0.0180 0.0273 0.0350 0.0177 0.0274 0.0358 0.0177 0.0275 0.0356 SPE 0.8997 0.2342 0.3358 0.0178 0.0256 0.0324 0.0174 0.0258 0.0329 0.0174 0.0259 0.0330 Table 15. Simulation results from 5000 replications when the model for the veriﬁcation process is misspeciﬁed (Study 2) and the ﬁrst value of Λ is used. “True” indicates the true parameter value. Sample size = 1000. cut-point = (2,4) True 0.5000 0.4347 0.9347 FI 0.5011 0.4345 0.9351 0.0277 0.0238 0.0152 0.0239 0.0203 0.0262 0.0275 0.0241 0.0153 MSI 0.5011 0.4344 0.9351 0.0280 0.0259 0.0169 0.0243 0.0226 0.0271 0.0279 0.0260 0.0169 IPW 0.5822 0.4436 0.9375 0.0381 0.0407 0.0213 0.0380 0.0400 0.0255 0.0381 0.0401 0.0212 SPE 0.5011 0.4345 0.9352 0.0304 0.0334 0.0218 0.0304 0.0330 0.0214 0.0305 0.0331 0.0216 cut-point = (2,5) True 0.5000 0.7099 0.7752 FI 0.5011 0.7105 0.7765 0.0277 0.0227 0.0297 0.0239 0.0223 0.0334 0.0275 0.0228 0.0298 MSI 0.5011 0.7101 0.7762 0.0280 0.0245 0.0305 0.0243 0.0241 0.0343 0.0279 0.0245 0.0309 IPW 0.5822 0.6815 0.8046 0.0381 0.0376 0.0325 0.0380 0.0370 0.0381 0.0381 0.0371 0.0327 SPE 0.5011 0.7099 0.7760 0.0304 0.0309 0.0328 0.0304 0.0306 0.0330 0.0305 0.0307 0.0331 cut-point = (2,7) True 0.5000 0.9230 0.2248 FI 0.5011 0.9233 0.2256 0.0277 0.0144 0.0270 0.0239 0.0193 0.0250 0.0275 0.0143 0.0270 MSI 0.5011 0.9234 0.2258 0.0280 0.0161 0.0275 0.0243 0.0204 0.0256 0.0279 0.0158 0.0275 IPW 0.5822 0.9009 0.2306 0.0381 0.0276 0.0306 0.0380 0.0268 0.0316 0.0381 0.0270 0.0308 SPE 0.5011 0.9234 0.2258 0.0304 0.0225 0.0279 0.0304 0.0218 0.0280 0.0305 0.0220 0.0281 cut-point = (4,5) True 0.9347 0.2752 0.7752 FI 0.9352 0.2760 0.7765 0.0135 0.0218 0.0297 0.0127 0.0168 0.0334 0.0135 0.0215 0.0298 MSI 0.9352 0.2757 0.7762 0.0143 0.0237 0.0305 0.0135 0.0191 0.0343 0.0143 0.0233 0.0309 IPW 0.9540 0.2379 0.8046 0.0139 0.0335 0.0325 0.0138 0.0330 0.0381 0.0139 0.0331 0.0327 SPE 0.9352 0.2754 0.7760 0.0161 0.0279 0.0328 0.0160 0.0275 0.0330 0.0161 0.0275 0.0331 cut-point = (4,7) True 0.9347 0.4883 0.2248 FI 0.9352 0.4888 0.2256 0.0135 0.0290 0.0270 0.0127 0.0259 0.0250 0.0135 0.0287 0.0270 MSI 0.9352 0.4889 0.2258 0.0143 0.0302 0.0275 0.0135 0.0273 0.0256 0.0143 0.0300 0.0275 IPW 0.9540 0.4574 0.2306 0.0139 0.0391 0.0306 0.0138 0.0387 0.0316 0.0139 0.0388 0.0308 SPE 0.9352 0.4890 0.2258 0.0161 0.0328 0.0279 0.0160 0.0327 0.0280 0.0161 0.0328 0.0281 cut-point = (5,7) True 0.9883 0.2132 0.2248 FI 0.9883 0.2128 0.2256 0.0040 0.0216 0.0270 0.0038 0.0190 0.0250 0.0040 0.0215 0.0270 MSI 0.9884 0.2133 0.2258 0.0050 0.0231 0.0275 0.0046 0.0208 0.0256 0.0050 0.0231 0.0275 IPW 0.9912 0.2195 0.2306 0.0060 0.0305 0.0306 0.0054 0.0301 0.0316 0.0059 0.0302 0.0308 SPE 0.9885 0.2135 0.2258 0.0065 0.0256 0.0279 0.0060 0.0256 0.0280 0.0064 0.0257 0.0281 Table 16. Simulation results from 5000 replications when the model for the veriﬁcation process is misspeciﬁed (Study 2) and the third value of Λ is used. “True” indicates the true parameter value. Sample size = 1000. cut-point = (2,4) True 0.5000 0.3031 0.8031 FI 0.4998 0.3026 0.8043 0.0257 0.0172 0.0280 0.0221 0.0124 0.0293 0.0259 0.0171 0.0278 MSI 0.4999 0.3027 0.8044 0.0264 0.0204 0.0297 0.0230 0.0166 0.0308 0.0267 0.0204 0.0293 IPW 0.6267 0.2614 0.8259 0.0345 0.0371 0.0371 0.0346 0.0364 0.0372 0.0348 0.0366 0.0365 SPE 0.5000 0.3031 0.8047 0.0322 0.0323 0.0361 0.0324 0.0321 0.0352 0.0326 0.0322 0.0354 cut-point = (2,5) True 0.5000 0.4682 0.6651 FI 0.4998 0.4681 0.6667 0.0257 0.0192 0.0341 0.0221 0.0151 0.0342 0.0259 0.0192 0.0343 MSI 0.4999 0.4681 0.6664 0.0264 0.0227 0.0354 0.0230 0.0195 0.0357 0.0267 0.0229 0.0358 IPW 0.6267 0.3884 0.7253 0.0345 0.0403 0.0396 0.0346 0.0400 0.0413 0.0348 0.0402 0.0401 SPE 0.5000 0.4684 0.6665 0.0322 0.0352 0.0389 0.0324 0.0353 0.0391 0.0326 0.0355 0.0393 cut-point = (2,7) True 0.5000 0.7027 0.3349 FI 0.4998 0.7035 0.3360 0.0257 0.0201 0.0318 0.0221 0.0184 0.0311 0.0259 0.0203 0.0320 MSI 0.4999 0.7035 0.3360 0.0264 0.0237 0.0337 0.0230 0.0224 0.0331 0.0267 0.0240 0.0339 IPW 0.6267 0.6157 0.4102 0.0345 0.0417 0.0386 0.0346 0.0416 0.0398 0.0348 0.0417 0.0386 SPE 0.5000 0.7038 0.3360 0.0322 0.0360 0.0350 0.0324 0.0361 0.0350 0.0326 0.0364 0.0352 cut-point = (4,5) True 0.8031 0.1651 0.6651 FI 0.8032 0.1655 0.6667 0.0207 0.0139 0.0341 0.0189 0.0099 0.0342 0.0207 0.0141 0.0343 MSI 0.8031 0.1654 0.6664 0.0217 0.0165 0.0354 0.0200 0.0135 0.0357 0.0216 0.0169 0.0358 IPW 0.8512 0.1270 0.7253 0.0217 0.0245 0.0396 0.0215 0.0251 0.0413 0.0215 0.0253 0.0401 SPE 0.8030 0.1653 0.6665 0.0239 0.0225 0.0389 0.0237 0.0228 0.0391 0.0238 0.0229 0.0393 cut-point = (4,7) True 0.8031 0.3996 0.3349 FI 0.8032 0.4009 0.3360 0.0207 0.0226 0.0318 0.0189 0.0194 0.0311 0.0207 0.0227 0.0320 MSI 0.8031 0.4008 0.3360 0.0217 0.0261 0.0337 0.0200 0.0234 0.0331 0.0216 0.0262 0.0339 IPW 0.8512 0.3544 0.4102 0.0217 0.0358 0.0386 0.0215 0.0362 0.0398 0.0215 0.0363 0.0386 SPE 0.8030 0.4008 0.3360 0.0239 0.0326 0.0350 0.0237 0.0325 0.0350 0.0238 0.0327 0.0352 cut-point = (5,7) True 0.8996 0.2345 0.3349 FI 0.8997 0.2354 0.3360 0.0144 0.0183 0.0318 0.0135 0.0156 0.0311 0.0144 0.0184 0.0320 MSI 0.8995 0.2354 0.3360 0.0158 0.0223 0.0337 0.0149 0.0197 0.0331 0.0157 0.0220 0.0339 IPW 0.9149 0.2274 0.4102 0.0163 0.0303 0.0386 0.0160 0.0299 0.0398 0.0161 0.0301 0.0386 SPE 0.8995 0.2355 0.3360 0.0175 0.0273 0.0350 0.0173 0.0268 0.0350 0.0174 0.0269 0.0352 sizes, i.e., n = 200 and n = 500. Each simulation experiment was based on 1000 replications. FI, MSI, IPW and SPE estimates of VUS are computed under correct working models for both the disease and the veriﬁcation processes. Tables 17–19 show Monte Carlo means, Monte Carlo standard deviations (MC.sd), the square roots of the variances estimated via asymptotic results (Asy.sd) and bootstrap standard deviations (Boot.sd) of ˆµ. In this section, we provide some extra plots related to the analysis of the ﬁrst dataset used in the main paper. In particular, in Figure 3 we present the estimate of the ROC surface for the test CA125 based on the full data set. Figure 4 and Figure 5 present the projections of the estimated ROC surfaces to the planes deﬁned by TCF1 versus TCF2, TCF1 versus TCF3 and TCF2 versus TCF3, i.e., the ROC curves between classes 1 and 2, classes 1 and 3, classes 2 and 3. For the IPW and SPE methods, to estimate the veriﬁcation process, we make use, ﬁrstly, of a correctly speciﬁed model, i.e., a linear threshold regression model (Figure 4) and, then, of a misspeciﬁed model, i.e., a logistic model Fig 3. Estimated ROC surface for CA125 assessing the classiﬁcation into three class of EOC: benign disease, early stage (I and II) and late stage (I and II). This surface is estimated by using full data. (Figure 5). Finally, as an example, Figure 6 plots conﬁdence regions for the pair (TCF1(c1), TCF2(c1, +∞)) at three values of c1, when the MSI approach is used. An approximated 95% elliptical conﬁdence region is obtained in a standard way as the set of points � TCF2,MSI(c1, +∞) � and χ2 0.95,2 is the 95–th quantile of a Chi–square distribution with 2 degree of freedom. In the plot, the black solid line represents the full data estimated ROC curve, whereas the blue dashed line is the bias–corrected estimated ROC curve.
Cavity quantum electrodynamics with mesoscopic topological superconductors<|sep|>We studied two paradigmatic examples of 1D topological superconducting systems capacitively coupled to a microwave superconducting stripline cavity: the Kitaev chain and a 1D nanowire with strong SO interaction in the presence of a magnetic ﬁeld and in proximity of a superconductor. We analyzed the electronic charge susceptibility of these systems that is revealed in the photonic transport through the microwave cavity via its transmission τ(ω). We showed that this electronic susceptibility can actually be used to detect the topological phase transition, the occurrence of Majorana fermions and the parity of the Majorana fermionic state in a non-invasive fashion. Such eﬀects are due to the interplay between the bulk and Majorana states, either via virtual or real transitions taking place between the two, and which are mediated by the photonic ﬁeld. As an outlook, it would be interesting to use the same cavity QED setup to access the physics associated with the fractional Josephson eﬀect. Acknowledgments— We acknowledge discussions with the LPS mesoscopic group in Orsay, and in particular insightful suggestions by Helene Bouchiat, and we thank Jukka Vayrynen for valuable correspondence. This work is supported by a public grant from the “Laboratoire d’Excellence Physics Atom Light Matter” (LabEx PALM, reference: ANR-10-LABX-0039) and the French Agence Nationale de la Recherche through the ANR contract Dymesys. In this section, we provide theoretical arguments for the wire Hamiltonian utilized in Eq. (6), and the eﬀective electron-cavity Hamiltonian used in the Main Text (MT). In a continuum description, the natural way to account for the interaction between the electrons and the electromagnetic ﬁeld is via the minimal coupling, i.e. p → p − (e/c)A in the electronic Hamiltonian, with A being the electromagnetic ﬁeld vector potential and p being the momentum of the electrons in the material. In a tight-binding picture instead, one accounts for the coupling between light and matter by performing the Peierls substitution to the hopping parameters tii+1 between neighboring sites i and i + 1, namely with A(r) being the electromagnetic ﬁeld vector potential at position r, and the integration is performed between the sites i and i+1. We will focus on the derivation of the eﬀective Kitaev model in the tight-binding picture, as the microscopic, continuum model was described in great detail very recently in [40]. We thus refer the reader to that paper for a detailed calculation of the cavity eﬀects, as well as the derivation of the capacitive coupling starting from the minimal coupling. Here we give some details on the derivation of Eq. (6) in the MT starting from a non-superconducting nanowire coupled to a bulk p-wave superconductor with such a coupling being assisted by the cavity ﬁeld. For simplicity, we assume the bulk to be not s, but p-wave paired, thus the presence of spin-orbit coupling in the wire is not a necessary ingredient. However, the present calculations can be straightforwardly generalized to more realistic system, such as nanowires with SOI. The total Hamiltonian of the system reads: with p = b(bulk), w(wire), and ∆w = 0 (no intrinsic superconductivity in the wire), and ∆b ≡ ∆ the p-wave pairing in the bulk superconductor. Here, cj,p (c† j,p) and tp are the electronic annihilation (creation) operator at position j and the hopping parameter in system p = b, w, respectively. The tunneling Hamiltonian in the presence of the cavity reads: where ˆφj = ˆAjdj, with ˆAj = i(αj/ωc)(a† −a), dj, αj, ωc, and a (a†), being the cavity vector potential, the coupling strength, the cavity frequency, and the cavity photon annihilation (creation) operators, respectively. Note that we assumed that the cavity ﬁeld points perpendicularly to the wire, and it has no component along it. If instead such components would exists, we should have modiﬁed the wire Hamiltonian too in order to account for the cavity induced phase factors. In the following, we will assume that αjdj ≡ αj = α, namely it is constant along the entire wire. Finally, the Hamiltonian of the cavity reads: with ωc being the (fundamental) frequency of the cavity. Before deriving an eﬀective wire Hamiltonian, it is instructive to switch to the Fourier space, for both the where ξk,p = tp cos k−µp, with µα the chemical potential in the p = w, b system. Next we perform the so called Lang-Firsov transformation on the system Hamiltonian, which means �Hsys = exp(S)Hsys exp(−S) with S chosen as follows: which implies we excluded the photonic ﬁeld from the tunneling term at the expense of adding photondependent chemical potential shift in the wire (third term) as well as an interaction term (fourth term). Note that for tint = 0, the transformation does not aﬀect the spectrum, as it can be simply undone. However, as will see in the following, in the presence of the tunneling term the photonic ﬁeld in the form of the capacitive coupling can lead to real eﬀects. In the following, we aim at ﬁnding an eﬀective Hamiltonian describing the wire only by integrating the bulk superconductor degrees of freedom up to second order in the tunneling tint. We choose to do so by employing the Schrieﬀer-Wolﬀ transformation formalism, which means, as before, that we unitary rotate the system Hamiltonian as Heﬀ sys = eSSW �Hsyse−SSW = Hw + Hb + Hw−c + HT + Hc + [SSW , Hw + Hb + Hw−c + HT + Hc] + . . . , (A11) or SSW = (Lw+Lb)−1HT , with Lα being a superoperator whose action is deﬁned as LαA = [Hα, A], ∀A. This is equivalent to the following identity:
The Completed SDSS-IV extended Baryon Oscillation Spectroscopic Survey: BAO and RSD measurements from the anisotropic power spectrum of the Quasar sample between redshift 0.8 and 2.2<|sep|>We perform the analysis of the clustering of the complete eBOSS quasar sample. We did two separate analyses. The BAO-only analysis measures the ratio between the angular diameter distance and the sound horizon at the baryon drag epoch, and the ratio between the Hubble distance and the sound horizon at the baryon drag epoch. The Full Shape RSD analysis provides in addition a determination of the linear growth rate of structure times the amplitude of matter density ﬂuctuations. We use a dedicated mock challenge (Smith et al. 2020) to estimate the systematic errors due to the modelling of the power spectrum, and due to the dependence on the assumed ﬁducial cosmology. The errors due to the observational systematics are determined from approximate mocks where the observational eﬀects have been modelled. For both methods, the dominant source of systematic error resides in the modelling of the power spectrum. The modelling of ﬁbre collisions also has a large impact on the cosmological parameters, especially on the growth rate measurement. The overall systematic errors are at the level of 30% of the statistical errors. Therefore, improving the models is key for the next generation quasar surveys with increased statistics. A consensus analysis of our measurement in Fourier space and the measurement in conﬁguration space from Hou et al. (2020) gives the following constraints for the BAO-only analysis: These measurements are proven very robust by all tests performed. Our measurements of cosmological distances are in agreement with a ﬂat ΛCDM model using Planck Collaboration et al. (2018) and our measurement of the linear growth of structures, 𝑓 𝜎8 is 1.9-𝜎 above the Planck derived value. The cosmological interpretation of the DR16 eBOSS quasar sample measurement along with the measurements obtained for the other eBOSS tracers, and the consistency with external data sets are discussed in eBOSS Collaboration et al. (2020). R. Neveux acknowledges support from grant ANR-16-CE31-0021, eBOSS and from ANR-17-CE31-0024-01, NILAC. Funding for SDSS-III and SDSS-IV has been provided by the Alfred P. Sloan Foundation and Participating Institutions. Additional funding for SDSS-III comes from the National Science Foundation and the U.S. Department of Energy Oﬃce of Science. Further information about both projects is available at www.sdss.org. SDSS is managed by the Astrophysical Research Consortium for the Participating Institutions in both collaborations. In SDSS-III these include the University of Arizona, the Brazilian Participation Group, Brookhaven National Laboratory, Carnegie Mellon University, University of Florida, the French Participation Group, the German Participation Group, Harvard University, the Instituto de Astroﬁsica de Canarias, the Michigan State / Notre Dame / JINA Participation Group, Johns Hopkins University, Lawrence Berkeley National Laboratory, Max Planck Institute for Astro- physics, Max Planck Institute for Extraterrestrial Physics, New Mexico State University, New York University, Ohio State University, Pennsylvania State University, University of Portsmouth, Princeton University, the Spanish Participation Group, University of Tokyo, University of Utah, Vanderbilt University, University of Virginia, University of Washington, and Yale University. The Participating Institutions in SDSS-IV are Carnegie Mellon University, Colorado University, Boulder, Harvard-Smithsonian Center for Astrophysics Participation Group, Johns Hopkins University, Kavli Institute for the Physics and Mathematics of the Universe Max-Planck-Institut fuer Astrophysik (MPA Garching), MaxPlanck-Institut fuer Extraterrestrische Physik (MPE), Max-PlanckInstitut fuer Astronomie (MPIA Heidelberg), National Astronomical Observatories of China, New Mexico State University, New York University, The Ohio State University, Penn State University, Shanghai Astronomical Observatory, United Kingdom Participation Group, University of Portsmouth, University of Utah, University of Wisconsin, and Yale University. This research used resources of the Argonne Leadership Computing Facility, which is a DOE Ofﬁce of Science User Facility supported under contract DE- AC0206CH11357. This work made use of the facilities and staﬀ of the UK Sciama High Performance Computing cluster supported by the ICG, SEPNet and the University of Portsmouth. This research used resources of the National Energy Research Scientiﬁc Computing Center, a DOE Oﬃce of Science User Facility supported by the Oﬃce of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. The power spectrum, covariance matrices, and resulting likelihoods for cosmological parameters are (will be made) available (after acceptance) via the SDSS Science Archive Server (https://sas.sdss.org/), with the exact address tbd.
Data-Driven Control and Data-Poisoning attacks in Buildings: the KTH Live-In Lab case study<|sep|>In this work, we have shown the feasibility of VRFT, an input-output data-driven method, for comfort control in buildings, namely temperature control, and analyzed the impact of the maxmin data poisoning attack. VRFT has been validated on a digital replica of the KTH Live-In Lab, modeled using IDA-ICE, showing good performance and small tracking error. We then analyzed the impact of data poisoning attacks, which revealed that small changes in the dataset could disrupt the controller’s performance. Results also indicated that smaller datasets are more robust to data poisoning attacks, while datasets naively constructed are more susceptible to the attack, resulting in substantial performance degradation. This stresses the importance of securing the data used to derive the control law.
Interface solitons in quadratically nonlinear photonic lattices<|sep|>We have studied parametric localization of light at an interface separating two diﬀerent quadratically nonlinear photonic lattices and determined the conditions for FIG. 7: Excitation of interface solitons from only FF ﬁeld as input beam: FF ﬁeld distributions of generated odd (a) and twisted (b) interface solitons. (c,d) Splitting of generated twisted interface solitons at the boundary between a uniform and periodic media for (c) FF and (d) SH ﬁelds. Here β = 0, s = 0.5, and K1 = K2 = 4.
Excitonic fine structure splitting in type-II quantum dots<|sep|>In GaAs1−ySby capped InAs QDs, lateral symmetry of the hole wave functions can be to a large extent inﬂuenced by the thickness of the GaAs1−ySby layer. In particular, during the crossover between type-I and type-II conﬁnement regimes, the hole wave function is shifted upwards across the nodal plane of the piezoelectric octopole, which is accompanied by the change of the direction of lateral elongation. Due to the mechanism of compensated elongation, a crossing of the bright exciton levels and a reduction of FSS to zero is predicted for certain thicknesses of the GaAs1−ySby layer. Low natural FSS and eﬃcient photoluminescence make the GaAs1−ySby capped InAs QDs attractive as a possible source of entangled photon pairs.
Designing ML-Resilient Locking at Register-Transfer Level<|sep|>We introduced the first concepts on designing and evaluating RTL locking using ML-based attacks on operation obfuscation, and proposed two ML-resilient locking algorithms. The heuristic algorithm is a controlled procedure that decreases the imbalance of operations in an RTL design in small steps, adhering to the allowed key budget. The exact algorithm guarantees ML resilience but can exceed a key budget. We presented a security metric to assess resilience of RTL locking to ML attacks that can guide the design process of heuristic locking. Finally, we presented the first ML-based oracle-less attack on RTL locking by adapting the state-of-the-art SnapShot attack. ACKNOWLEDGMENTS R. Karri was supported in part by ONR Award # N00014-18-1-2058, NSF Grant # 1526405, NYU Center for Cybersecurity, and NYUAD Center for Cybersecurity.
A preprocessing perspective for quantum machine learning classification advantage using NISQ algorithms<|sep|>In this study, we demonstrate that speciﬁc techniques of preprocessing could play a crucial role when we talk about quantum machine learning. We focus on the encoding part, the classical one, to evaluate the effect on quantum algorithms. We show that a quantum computer can extract more meaningful information from classical data and leverage classiﬁcation results just using a few dimensions. As postulated in Schuld and Killoran [2022] quantum advantage doesn’t need to be measured by the ability to beat classical ML models but can be regarded as a better information extraction technique. The few numbers of qubits of the currently accessible quantum computers force researchers to look to new alternatives. Classical dimensionality reduction as SKPP, PCA or LDA are useful to compress classical high feature datasets (100+) into a number that can be used with a quantum computer. Here, we tested with two dimensions for two qubits. LDA shows more promising results for supervised machine learning tasks with quantum computers. The prevalence of LDA under PCA wasn’t explored in this paper but will be explored in future to understand how LDA provides a better data representation for qubits encoding. Further analysis will be needed to determine the positive effect of LDA in supervised QML. Also, we will study the potential impact of PCA in unsupervised tasks. As in classical ML, we need to determine which methods has a better effect on speciﬁc types of data. Tables 6, 7, 8, 9, 10, 11, 12, 13 show the results where the classical methods were also applied with the dimensionality reduction methods. The DR also improves the performances of these methods, but quantum algorithms are comparable to them. More investigation will be needed, but the preprocessing part of big real-world datasets plays an important role in the usability of quantum computing in the industry. Better quantum data encoding will also be required to demonstrate a strong difference between classical and quantum machine learning. Classical ML methods selected for this study weren’t tuned speciﬁcally on the two datasets. Default parameters were chosen. Only LR was used with a max iteration parameter ﬁxed at 1,000 iterations. Indeed, on the fraud detection dataset, the default limit was reached and LR wasn’t converging. KNN was trained with the number of neighbours ﬁxed at seven. In the case of quantum algorithms, the data is encoded with a feature map. The parameters of the feature map weren’t tuned but ﬁxed through the study with a number of repetitions of 2 and a feature dimension of 2. Further investigation is needed to explore the effect of the feature map on the output of the QSVC. VQA was used with an angle embedding method (rotations gates) and a strongly entangled layer. Both weren’t tuned, and alternatives will be explored in the future. An interesting perspective will also to study the impact of different ansatzes [Cerezo et al., 2021, Ostaszewski et al., 2021] on the VQA results. Also, in this study, we used only two datasets close to the real-world data in the ﬁnance domain. The results demonstrate a quantum advantage with QSVC and VQA, but we need to extend the approach with other datasets (higher number of features) in other domains to create a benchmark through a general application. We demonstrate that the quantum era needs to be seriously investigated by the industrial people, but more work is needed to fully demonstrate the advantages.
Electrohydrodynamic channeling effects in narrow fractures and pores<|sep|>Flow in highly irregular geometries with charged surfaces is commonplace in many geological and industrial settings. In some situations, even a moderate change of the local ﬂow distribution can have an impact on the precipitation and chemical reactions [31]. We have in this paper considered the electrohydrodynamic eﬀects on ﬂow by numerically solving the Stokes–Poisson–Nernst– Planck equation in narrow undulated channels. The undulated channel geometry serve as a simpliﬁed model of micro-scale fractures, which often mediate the large-scale transport e.g. in porous rock. By varying the amplitude of the channel undulation and the Debye length, we have analyzed the macroscopic ﬂow changes in terms of the streaming potential and electric viscosity. Further, we have observed an enhanced channeling of the ﬂow. In particular, we observe for the larger undulation amplitudes up to 5% ﬂux reductions, relative to a system without surface charge. The local ﬂow may vary as much as 10%. In comparison to pure hydrodynamic channeling, our results indicate that ridges may be even more prone to precipitation than valleys, leading to a positive feedback with enhanced channeling eﬀects. Our results oﬀer insight into electrodydrodynamic ﬂow in realistic pore and fracture geometries. Further studies would be of interest, primarily in larger and more complex samples, to get an even deeper understanding of electrohydrodynamic eﬀects in geological settings. Further, it would be interesting to study the precipitation and/or dissolution dynamics in the presence of surface charge. Finally, electrohydrodynamics might be important in two-phase ﬂow, where the local forces could alter the wetting properties and hence control the macroscopic ﬂuid ﬂow. This project has received funding from the Villum Foundation through the grant “Earth Patterns”, and from the European Union’s Horizon 2020 research and innovation program through Marie Curie initial training networks under grant agreement 642976 (NanoHeal). The authors are thankful to Henrik Bruus and Fran¸cois Renard for stimulating discussions. where ψ is the test function for the electric potential and c+, c− are the test functions for the cation and anion number densities respectively. We can develop a Newton method for solving the equation by viewing the weak form in Eq. (A1) as a functional called F(U), where U = (ϕ, n+, n−), and then expanding around some U0. This gives: where δU is a variation away from U0. Now, performing this for Eq. A1 and applying the appropriate boundary conditions gives the following linearized weak form:
Visualization of short-term heart period variability with network tools as a method for quantifying autonomic drive<|sep|>Network structure methods are able to visualize, describe and diﬀerentiate heart rate dynamics in healthy young subjects and HTX patients. Using these methods, the general dynamical properties of heart rate can be deﬁned as correlated or not correlated (employing the comparison of raw and shuﬄed signals) and linearly or non-linearly correlated (comparing raw signals and signals with shuﬄed phases of Fourier Transform). The essential feature of complex dependencies in nocturnal heart rhythm in our group of healthy young persons is related to large RR-increments, both decelerations and accelerations. This feature manifests itself in that large accelerations are more likely antipersistent, while large decelerations are more likely persistent. This observation also seems to be an important indicator of healthy heart rate. Moreover, since the vagal part of autonomic regulation is considered responsible for large RR-increments, we may hypothesize that vagal activity is a crucial source of complexity in short-term heart rate variability. In healthy young individuals, the change in vagal tone during sleep (e.g. change from high vagal activity to its withdrawal between non-REM and REM sleep stages) allows us to observe the speciﬁc patterns of heart rate dynamics. We interpret the non-linear relationship observed between consecutive accelerations and decelerations in the case of bigger changes (accelerations and decelerations of more than 35 ms) as an eﬀect of vagal activity. Although in HTX patients’ heart rate regulation is mostly intrinsic with no autonomic control, the relationship between consecutive accelerations and decelerations is also observed, but in this case the scale of changes is much lower. RR-increments vary as ﬂuctuations around a homeostatic state. However, the organization of this homeostatic state in the case of raw signals shows that it involves dynamical forces more strongly than if the dynamics were driven by linear forces only. In post-transplant patients, the non-linear dependencies are also characterized by the appearance of sequences made of bigger (> 20 ms) accelerations followed by smaller decelerations (less than 10 ms). This means that an increase in heart rate is not so eﬀective as in healthy individuals but is still possible. We hypothesize that this pattern of heart rate in HTX patients may be a result of gradual sympathetic reinnervation.
Distribution of the position of a driven tracer in a hardcore lattice gas<|sep|>We studied the diﬀusion of a biased tracer particle (TP) in a hardcore lattice gas in contact with a reservoir of particles. From the general master equation of the problem, we gave a detailed derivation of the equation satisﬁed by the ﬂuctuations of the position of the TP and presented in a previous publication. This equation involve the density proﬁles around the TP and cross-correlation functions, whose evolution equations are obtained in a closed form by resorting to mean-ﬁeld type approximation. Going one step further, we extended this approximation to higher-order correlation functions in order to obtain the evolution equation veriﬁed by the cumulant generating function of the TP position. This equation then yields the entire probability distribution of the TP on a lattice of arbitrary dimension. We also obtained the equation satisﬁed by the third-cumulant of the distribution, which gives information about its asymmetry. We then solved these equations in the particular case of a one-dimensional lattice. We recall the result from [31]: the diﬀusion coeﬃcient of the TP is a nonmonotonic function of the density of bath particles. Counterintuitively, it reaches a maximum value for a nonzero value of the density. Thus, the presence of bath particles on the lattice may actually enhance its diﬀusion coeﬃcient. Here, we showed that this eﬀect is related to an anomaly in the behavior of bath-tracer cross-correlation functions, that are nonmonotonic functions of the distance to the TP. Another surprising observation arises by analyzing the third cumulant of the TP position, which was shown to be nonmonotonic and to take negative values in a wide range of parameters. These analytical predictions were confronted with exact numerical samplings of the master equation, which indicate that the approximation we used is accurate in a wide range of parameters. We ﬁnally solved the equation satisﬁed by the cumulant generating function, deduced the probability distribution and showed that the position of the TP rescaled by its ﬂuctuations is Gaussian-distributed in the long-time limit. The equations presented in this paper are very general, and allow to compute the cumulants of the TP position and therefore its distribution under a mean-ﬁeld-type approximation which was shown to be accurate in a broad range of parameters. These equations are valid for lattices of arbitrary dimension. We leave to future work the study of their solutions on higher-dimensional lattices. It could be interesting to see if the results obtained on the one-dimensional lattice – enhanced diﬀusion coeﬃcient, nonmonotonic and negative third cumulant, convergence of the rescaled distribution to a Gaussian distribution – can be extended to lattices of higher dimension, or observed in experimental systems. In this appendix, we give an explicit derivation of Eq. (19), which governs the evolution of the second moment of Xt. We multiply the master equation (4) by (X · e1)2 and average over all the bath conﬁgurations η and all the positions of the TP X. We consider separately each term of the master equation: Recalling that ηr,µ a conﬁguration obtained from η by exchanging the occupation variables of two neighboring sites r and r + eµ, we obtain � Following the same procedure for the term µ = −1 and noticing that the terms obtained for µ = ±2, . . . , ±d in (A.7) cancel, we ﬁnally get Recalling that ˆηr is the conﬁguration obtained from η with the change ηr ← 1−ηr, we have the following equality � • for the same reason, the fourth term will have a zero contribution after multiplying by (X · e1)2 and averaging over X and η. In this appendix, we start from the master equation (4) in order to derive the evolution equations satisﬁed by �gr(t) = ⟨(Xt − ⟨Xt⟩)ηXt+r⟩ (Eq. (24)) and wr(u; t) = � eiuXtηXt+r � (Eq. (66)). The derivations of these two evolution equations are similar, and we will present a general method to derive the evolution equation of the following correlation function: where F is a generic function of the TP position. The equations satisﬁed by �gr and �wr will be obtained by taking F(X) = X − ⟨Xt⟩ and F(X) = eiuX respectively. Parenthetically, with this method, one retrieves the equation satisﬁed by kr(t) (Eq. (8)) by taking F(X) = 1. We multiply the master equation (4) by F(X)ηX+r and average over all the bath conﬁgurations η and all the positions of the TP X. We consider separately each term of the master equation: In the two cases we will consider, we note that ∂tF(X) is independent of X and η, so that we obtain
Equitable Scheduling for the Total Completion Time Objective<|sep|>We introduced and investigated a further natural equitable scheduling problem based on the framework by Heeger et al. [14]. After an initial strong NP-hardness result, we focused on two special cases, one where we assume the number of days to be small, and one where we consider the number of clients to be small. We believe that our results give a good picture of the computational and parameterized complexity of 1 || maxj � i Ci,j. However, some immediate questions remain unresolved. We leave three main open problems for future research: • What is the computational complexity of 1 || maxj � i Ci,j when there are exactly three days? This particular case is left open, since for two days we showed polynomial-time solvability (Theorem 4) and for four days NP-hardness (Theorem 2). • Does 1 || maxj � i Ci,j admit a pseudo-polynomial algorithm when the number of days is a constant? Note that our W[1]-hardness result (Theorem 6) implies that the degree of the polynomial running time of such an algorithm presumably must depend on the number of days. • For the case of a constant number of clients, we have a pseudo-polynomial algorithm for 1 || maxj � i Ci,j (Theorem 9). However, in this case we do not have any hardness results for unarily encoded processing times. Hence, we do not know whether the polynomial running time of our algorithm can be improved to have a degree independent of the number of clients (and only the leading constant of the running time would depend on the number of clients). In other words, is 1 || maxj � i Ci,j ﬁxed-parameter tractable or W[1]-hard with respect to the number of clients if all processing times are encoded in unary? A more generic yet important future research direction is the development of approximation algorithms for the natural optimization variant of 1 || maxj � i Ci,j, where we try to minimize k. Finally, we believe that it is promising to investigate other basic scheduling problems in the equitable setting. Natural candidates could be minimizing the maximum total lateness of any client, 1 || maxj � i Li,j in our notation, or minimizing the maximum total tardiness, 1 || maxj � i Ti,j. The former can be seen as a generalization of 1 || maxj � i Ci,j where every client j has an individual equitability parameter kj. The ﬁeld of equitable scheduling still holds a plethora of natural problems to study. For scheduling objectives that do not directly generalize 1 || maxj � i Ci,j we expect, similar to the classical scheduling setting, very diﬀerent results and techniques needed to obtain them.
Quantum harmonic oscillator with superoscillating initial datum<|sep|>We have studied the evolution of superoscillating initial data for the quantum driven harmonic oscillator. Our main result shows that superoscillations are ampliﬁed by the harmonic potential and that the analytic solution develops a singularity in ﬁnite time. Moreover, even for non-superoscillatory initial data, the harmonic oscillator displays a superoscillatory behavior since the solution contains the term that increases arbitrarily with time (up to the time of singularity). This phenomenon does not occur for the free particle. Since the leading semiclassical behavior of the propagator for any quantum-mechanical system is given by the propagator for the driven harmonic oscillator, our results directly apply to a much broader problem of establishing presence of superoscillations in any quantum system (at least in the semiclassical limit). We have also shown that for a large class of solutions of the Schr¨odinger equation, superoscillating behavior at any given time implies superoscillating behavior at any other time.
Supersymmetric Chern-Simons Theory in Presence of a Boundary<|sep|>In this paper we analysed the N = 1 Chern-Simons theory in the presence of a boundary. We used the results thus obtained to study the ABJM theory in the presence of a boundary. We ﬁrst modiﬁed the Chern-Simons theory by adding a boundary term to it such that supersymmetry variations of the bulk Chern-Simons theory were cancelled by the supersymmetry variations of this boundary term. The resultant theory was then made gauge invariant by adding new boundary degrees of freedom to it. This new boundary theory was identiﬁed as a gauged Wess-Zumino-Witten model. These results were used to obtain a superspace description of the boundary ABJM theory which was also gauge invariant. As the matter part of the ABJM theory is gauge invariant even with a boundary, it was only necessary to include a boundary term to restore SUSY. The Chern-Simons part of the ABJM was modiﬁed by both the addition of a term to make it SUSY and new boundary degrees of freedom to make it gauge invariant. Thus, we added a suitable theory on the boundary such that its gauge and supersymmetry variations exactly cancel those of the bulk ABJM theory. We also analysed the BRST and the anti-BRST symmetries of this resultant theory. Chern-Simons theories are also important in condensed matter physics due to their relevance in to fractional quantum Hall eﬀect [50, 51, 52, 53]. Fractional quantum Hall eﬀects is based on the concept of statistical transmutation, i.e. the fact that in two dimensions, fermions can be described as charged bosons carrying an odd integer number of ﬂux quanta which is achieved by analysing Chern-Simons ﬁelds coupled to the bosons. In this theory electrons in an external magnetic ﬁeld are described as bosons in a combined external and statistical magnetic ﬁeld. At special values of the ﬁlling fraction the statistical ﬁeld cancels the external ﬁeld, in the mean ﬁeld sense and the system is described as a gas of bosons feeling no net magnetic ﬁeld. These bosons condense into a homogeneous ground state. This model describes the quantisation of the Hall conductance and the existence of vortex and anti-vortex excitations. Lately supersymmetric generalisation of fractional quantum Hall eﬀect have also been investigated [54, 55, 56, 57]. In particular physical properties of the topological excitations in the supersymmetric quantum Hall liquid have been discussed in a dual supersymmetric Chern-Simons theory [58]. Boundary eﬀects for ChernSimons theories are also important in condensed matter physics. This is because in quantum Hall systems gapless edge modes exist [59]. These have important consequences for the transport properties of the system [60]. These modes have been studied in the presence of an inﬁnitely steep external conﬁning potential [61, 62]. The description of these modes has also been related to the chiral Luttinger liquid description of the edge excitations [63]. Thus, the results of this paper will be useful in analysing the supersymmetric generalisation of gapless edge modes of fractional quantum Hall systems. This can have important consequences for the transport properties of these fractional quantum hall system. In this appendix we will ﬁrst study the gauge transformations of the ABJM theory and the boundary theory in the component form. We will then analyse the BRST and anti-BRST transformations of these theories in the component form. To do so we write ghosts, anti-ghosts and the auxiliary ﬁelds in component form as Now after writing the components for all superﬁelds we can write the gauge transformations of these component ﬁelds. Thus, the component form of the gauge transformations of matter ﬁelds for the ABJM theory are given by δ xI a = i(λaxI − xI ˜λa) − i(λxI a − xI a˜λ), δ xI† a = −i(xI†λa − ˜λaxI†) + i(xI† a λ − ˜λxI† a ), After discussing the component form of the gauge transformations, we will analyse the component form of the BRST and the anti-BRST transformations. In component form the BRST transformation of the matter ﬁelds in the ABJM theory are given by s yI a = −i(yIca − ˜cayI) + i(yI ac − ˜cyI a), s yI† a = i(cayI† − yI†˜ca) − i(cyI† a − yI† a ˜c), s xI = i(cxI − xI˜c) + i(cxI − xI˜c) − 2i(caxI a − xaI˜ca), s xI† = −i(xI†c − ˜cxI†) − i(xI†c − ˜cxI†) + 2i(xaI†ca − ˜caxI† a ), s yI = −i(yIc − ˜cyI) − i(yIc − ˜cyI) + 2i(yaIca − ˜cayI a), s yI† = i(cyI† − yI†˜c) + i(cyI† − yI†˜c) − 2i(cayI† a − yaI†˜ca). (103) In component form the BRST transformations of gauge ﬁelds, ghosts, antighosts and auxiliary ﬁelds for the ABJM theory are given by The anti-BRST transformation of the gauge ﬁelds, ghosts, anti-ghosts and the auxiliary ﬁelds for the ABJM theory in component form are given by These are the component form of the BRST and the anti-BRST transformations of the ABJM theory and the boundary theory it is coupled to.
Water evaporation from solute-containing aerosol droplets: effects of internal concentration and diffusivity profiles and onset of crust formation<|sep|>We have studied the effects of non-volatile solutes on water evaporation from a solutecontaining droplet by introducing a droplet evaporation model that accounts for evaporation cooling, internal water concentration gradients, solute-induced water vapor-pressure reduction,  and the solute-concentration dependence of the water diffusivity inside the droplet. For this,  the evaporating droplet is assumed to contain two regions: an outer shell where the liquid phase  exhibits a concentration gradient and an internal core where the concentration profile remains  uniform. Accordingly, the water evaporation is considered as a two-stage process. In the first  stage, the outer shell grows from the surface towards the core of the droplet. When the shell  covers almost the whole droplet, evaporation turns into the second stage, where the water  concentration in the internal core gradually decreases to its equilibrium value. Our investigations indicate that the presence of non-volatile solutes (such as salt, proteins,  peptides, virions, etc) within a drying aqueous droplet has different, and sometimes  contradictory, effects on the evaporation process. The most well-known effect is the water  vapor-pressure reduction due to the dilution of the liquid water in the presence of solutes, which  considerably slows down the evaporation process. This is, however, not the only effect. The  solutes in the liquid phase are found to affect the evaporation-induced droplet cooling and  decrease the temperature depression at the droplet surface, which results in an increased  evaporation rate and, thus, slightly expedites the evaporation process. Our investigations  indicate that this effect is not significant compared to the first effect, but still considerable. Another important effect of solutes on the drying process is that in the presence of non-volatile  solutes, the water concentration at the droplet surface deviates from the mean water  concentration in the droplet due to the finite internal water diffusivity. This leads to a water  concentration gradient in the liquid phase, which slows down the evaporation process. This  effect can be intensified by any factor that decreases the internal water diffusivity, such as the  presence of strongly hydrated solutes. The solute-concentration dependence of the water  diffusion coefficient is, therefore, studied as a factor that influences the evaporation process.  However, our results reveal that this effect is negligible and can be safely neglected. According  to our investigations, in the presence of strongly hydrated solutes, the water-diffusion  coefficient at the droplet surface decreases due to the increased solute surface concentration.  On the other hand, the presence of solutes is found to decrease the thickness of the outer shell  and, consequently, increases the water concentration gradient at the droplet surface. As a result,  the water evaporation flux remains almost independent of the solute-induced variations of the  local diffusion coefficient. When the solute concentration at the droplet surface exceeds the solubility limit of solutes, a  crust forms on the surface, which changes the evaporation mechanism. A crust affects the  morphology of the dry particle formed at the end of the drying process by producing a hollow  particle with a size larger than expected in the absence of crust. Our results reveal three factors  that can delay the crust formation: a decrease in the initial volume fraction of solutes, an  increase in the relative humidity, or a decrease in the water activity coefficient. Also, the crust  formation is found to be slightly accelerated in the presence of solutes that decrease the internal  water diffusivity, and slightly delayed in the presence of solutes that increase the internal water  diffusivity. Although the present study provides insight into the effects of non-volatile solutes on the  evaporation process of solute-containing droplets, further work is required to answer a number  of important open questions: (I) How does the water activity coefficient vary during the lifetime  of the droplet and how do non-ideal effects due to solute-water interactions affect the drying  process? (II) How does the evaporation process continue after the crust formation? What is the  exact mechanism of water evaporation in the presence of a dry crust? Does it involve capillary  action? If so, more information about the crust porosity, its characteristic pore size, and the  amount of water located inside the pores is required. (III) What happens after the formation of  a gel-like skin including saturated polymers? We gratefully acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG) via grant  NE810/11 and by the ERC Advanced Grant NoMaMemo No. 835117.
Knowledge Association with Hyperbolic Knowledge Graph Embeddings<|sep|>sentation learning model. The proposed HyperKA method extends translational and GNN-based techniques to hyperbolic spaces, and captures associations by a hyperbolic transformation. Our method outperforms SOTA baselines using lower embedding dimensions on both entity alignment and type inference. For future work, we plan to incorporate hyperbolic RNNs (Ganea et al., 2018) to encode auxiliary information for zero-shot entity and concept representations. Another meaningful direction is to use HyperKA to infer the associations between snapshots in temporally dynamic KGs (Xu et al., 2020). We also seek to investigate the use of HyperKA for cross-domain representations of biological and medical knowledge (Hao et al., 2020). Acknowledgments. We thank the anonymous reviewers for their insightful comments. This work is supported by the National Natural Science Foundation of China (Nos. 61872172 and 61772264), and the Collaborative Innovation Center of Novel Software Technology and Industrialization.
A Penalty Approach for Normalizing Feature Distributions to Build Confounder-Free Models<|sep|>Herein, we introduce PMDN, our novel penalty method for removing bias in model training due to confounding factors. PMDN can be plugged into any neural network architecture and is independent from batch size. By removing the eﬀects from confounding relationships between training and target outputs, PMDN minimizes the bias in the learned features. We show improvement of PMDN, a layer with trainable parameters, when compared to MDN, a layer with a closed-form solution, on a synthetic and a neuroimaging dataset. The improvement in accuracy and confounder independence from PMDN represent an important step towards neuroscience, imaging, or clinical applications of machine learning prediction models. Acknowledgements This study was partially supported by NIH Grants (AA017347, MH113406, and MH098759) and Stanford Institute for Human-Centered AI (HAI) Google Cloud Platform (GCP) Credit.
Continuing EVN monitoring of HST-1 in the jet of M87<|sep|>Nearly a decade-long VLBI monitoring of HST-1 is beginning to reveal the detailed time evolution of the kinematics for the resolved substructures, where we found some changes (and slow oscillations) in both direction and speed with time (and their possible correlation). This suggests that the actual trajectories of these components are three-dimentional. If this is the case, a likely scenario is that the HST-1 complex is traveling along a helical path at a relativistic speed. Interestingly, a recent VLA polarimetric study of HST-1 independently detected a progressive rotation of HST-1’s EVPA during a part of our monitoring period [14], which could be related the observed curved trajectories. To further test this scenario, it would be fruitful to make a EVPA monitor for Figure 3: Observed (x,y) trajectories for comp1, comp2 and comp3. The underlying contour with dotted lines is a EVN 1.7 GHz image obtained in October 2013. the resolved structure with VLBI or to search for a possible year-scale periodicity in the motion by further continuing our monitoring. We ﬁnally note that the HST-1 monitor with EVN is also useful for understanding the overall formation and collimation mechanisms of M87 jet. It is suggested that the HST-1 cross section is smaller than that expected from the parabola-shape collimation proﬁle of the inner jet [9]. However as shown in Fig. 1, the recent HST-1 becomes signiﬁcantly larger (∼double) in size - not only along but also across the jet - than that seen in the past. This indicates that the underlying HST-1 structure should be much more extended than previously thought, and the individual bright components may occupy only a small part of the entire jet cross section. In order to examine this issue, we have recently conducted a deep EVN 1.7 GHz session incorporating the eMERLIN, which greatly improves the detectability for the surrounding, more extended structure. This will allow us to uncover the entire jet structure around HST-1 in more detail. Acknowledgments. We acknowledge a contribution from the Italian Foreign Affair Minister under the bilateral scientiﬁc collaboration between Italy and Japan. We acknowledge ﬁnancial contribution from grant PRIN-INAF-2011. e-VLBI research infrastructure in Europe is supported by the European Union’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. RI-261525 NEXPReS. The European VLBI Network is a joint facility of European, Chinese, South African and other radio astronomy institutes funded by their national research councils. The National Radio Astronomy Observatory is a facility of the National Science Foundation operated under cooperative agreement by AUI.
Flavor Cosmology: Dynamical Yukawas in the Froggatt-Nielsen Mechanism<|sep|>A Flavon couplings in the mass basis 26 A.1 Interactions with one ﬂavon ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . 26 A.2 Higher order interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B Experimental Constraints on a light ﬂavon 27 B.1 Heavy scalar case, mσ > MK . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.2 Light scalar case, mσ < MK . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C Constraints on a second FN ﬁeld with negligible VEV today 31 C.1 Constraints on Λχ for QFN(X) = −1 (Model B-2) . . . . . . . . . . . . . . . 31 C.2 Constraints on Λχ for QFN(X) = −1/2 (Model B-1) . . . . . . . . . . . . . . 32
Utility of the Weak Temperature Gradient Approximation for Earth-Like Tidally Locked Exoplanets<|sep|>We ﬁnd that a WTG model provides a very good approximation of the atmospheric dynamics of a tidally locked terrestrial planet orbiting an M dwarf. If we assume that the planet is (1) dry, i.e., that the eﬀects of moisture and clouds on the infrared emission to space are small, and (2) the atmosphere is suﬃciently thick, we ﬁnd that a WTG model could not easily be distinguished observationally from a full 3D GCM calculation of the atmospheric dynamics using the MIRI instrument on the JWST and assuming the planet orbits a nearby M dwarf star. Although we did tune the WTG model to Figure 4. The maximum diﬀerence between phase curve that the WTG model and the GCM would produce assuming an Earth-sized planet orbiting an M star with radius 0.2R⊙. The diﬀerence is plotted in ppm and should be compared to an instrumental precision for one-day integrations using MIRI instrument on the JWST of 129 ppm if the system is at a distance of 20 pc, 65 ppm at 10 pc, and 32 ppm at 5 pc. the GCM, the fact that this is possible with only 3 WTG model parameters indicates that the WTG approximation is an acceptable ﬁrst approximation for understanding dry Earth-like M dwarf planets. It is reasonable to expect that many planets in or near the habitable zone of M dwarfs will be fairly dry due to reduced volatile delivery (Raymond et al. 2007) and nightside ice trapping (Menou 2013). If a measured thermal phase curve is consistent with clouds having a minor or no eﬀect, then a Monte Carlo ﬁt of the three-parameter WTG model to the observed thermal phase curve should provide a good description of the dynamical behavior of the atmosphere. Of course the habitable zone planets that will be of most interest are the ones that actually have liquid water at the surface on the dayside in at least some regions, and will therefore tend to have clouds. In this case interpretation of the phase curve would require some understanding of the cloud behavior (Yang et al. 2013). Since we have shown that the WTG approximation is a good approximation of the atmospheric dynamics, it should be possible to construct a version of the WTG model incorporating cloud eﬀects with only one or two extra parameters that could be ﬁt to observed phase curves. In such cases it would be beneﬁcial to also run a GCM to conﬁrm that implied cloud behavior is reasonable, which demonstrates the value of using a hierarchy of climate models to understand the climate of exoplanets. Aside from the direct implications of using WTG models to decipher the basics of atmospheric dynamics and planetary climate from thermal phase curves, it is important to emphasize that the fact that this works reasonably well justiﬁes the use of the WTG approximation in theoretical studies, e.g., by Pierrehumbert (2011) and Kite et al. (2011). This is extremely beneﬁcial because of the physical insight that the WTG approximation can provide into the problem of understanding the climate of tidally locked terrestrial planets. We thank R. Pierrehumbert for critical discussions early in the development of this project, F. Ding for assistance with the GCM, N. Cowan and J. Bean for useful discussions of phase curve observability, and K. Stevenson for insight into observational applications of this work. We thank N. Cowan, D. Fabrycky, and an anonymous reviewer for comments on a draft of this paper. This research was begun with funding provided by a California Institute of Technology Summer Undergraduate Research Fellowship. DSA acknowledges support from an Alfred P. Sloan Research Fellowship.
Recurrent Dirichlet Belief Networks for Interpretable Dynamic Relational Data Modelling<|sep|>We have presented a probabilistic deep hierarchical structure named Recurrent Dirichlet Belief Networks (RecurrentDBN) for learning dynamic relational data. Through Recurrent-DBN, the evolution of the latent structure is characterized by both the cross-layer and the cross-time depen
A Comprehensive Study of Bright Fermi-GBM Short Gamma-Ray Bursts: II. Very Short Burst and Its Implications<|sep|>Ten very short GRBs detected by Fermi/GBM have been analyzed, whose observed spectra are ﬁtted by four models, the BAND model, the CPL model, the BAND + BB model, and the CPL + BB model. Both the derived low-energy spectral index and the peak energy imply a common hard spectrum feature for all GRBs. We found moderate evidence for the detection of thermal component in eight GRBs, which indicates a possible common feature of these very short GRBs. However, the thermal component in these short GRBs contributes a small proportion of the global prompt emission, especially above 100 keV. The modiﬁed thermalradiation mechanism could enhance the proportion signiﬁcantly, such as in subphotospheric dissipation. Funding: This work is funded by National Nature Science Foundation of China grant numbers 11903017 and 12065017, and Jiangxi Provincial Natural Science Foundation under grant 20212BAB201029.
Detection of very high energy gamma-ray emission from the gravitationally-lensed blazar QSO B0218+357 with the MAGIC telescopes<|sep|>MAGIC has detected VHE gamma-ray emission from QSO B0218+357 during the trailing component of a ﬂare in July 2014. It is currently the most distant source detected with a ground-based gamma-ray telescope, and the only gravitationally lensed source detected in VHE gamma-rays. The VHE gammaray emission lasted for two nights achieving the observed ﬂux of ∼ 30% of Crab Nebula at 100 GeV. Using the EBL model from Domínguez et al. (2011), the intrinsic spectral index in this energy range was found to be 2.35 ± 0.75stat ± 0.20syst. The VHE gamma-ray ﬂare was not accompanied by a simultaneous ﬂux increase in the optical or X-ray energy range. We have modeled the X-ray emission as a sum of two components with diﬀerent magniﬁcations, the weaker one absorbed with column density of (2.4 ± 0.5) × 1022 at. cm−2. The combined Fermi-LAT and MAGIC energy spectrum is consistent with the current EBL models. These constraints are however not very strong, with the EBL density scaling parameter being less than 2.1-2.8 of the one predicted by the tested models. The broadband emission of QSO B0218+357 is modeled in a framework of a two-zone external Compton model. According to this scenario, the quasi-stable optical and X-ray emission originates mostly in the inner zone. The enhanced gamma-ray emission during the ﬂare is produced in the second zone, located outside of the BLR. Acknowledgements. We would like to thank the Instituto de Astrofísica de Canarias for the excellent working conditions at the Observatorio del Roque de los Muchachos in La Palma. The ﬁnancial support of the German BMBF and MPG, the Italian INFN and INAF, the Swiss National Fund SNF, the ERDF under the Spanish MINECO (FPA2012-39502), and the Japanese JSPS and MEXT is gratefully acknowledged. This work was also supported by the Centro de Excelencia Severo Ochoa SEV-2012-0234, CPAN CSD2007-00042, and MultiDark CSD2009-00064 projects of the Spanish Consolider-Ingenio 2010 programme, by grant 268740 of the Academy of Finland, by the Croatian Science Foundation (HrZZ) Project 09/176 and the University of Rijeka Project 13.12.1.3.02, by the DFG Collaborative Research Centers SFB823/C4 and SFB876/C3, and by the Polish MNiSzW grant 745/N-HESS-MAGIC/2010/0. The Fermi LAT Collaboration acknowledges generous ongoing support from a number of agencies and institutes that have supported both the development and the operation of the LAT as well as scientiﬁc data analysis. These include the National Aeronautics and Space Administration and the Department of Energy in the United States, the Commissariat à l’Energie Atomique and the Centre National de la Recherche Scientiﬁque / Institut National de Physique Nucléaire et de Physique des Particules in France, the Agenzia Spaziale Italiana and the Istituto Nazionale di Fisica Nucleare in Italy, the Ministry of Education, Culture, Sports, Science and Technology (MEXT), High Energy Accelerator Research Organization (KEK) and Japan Aerospace Exploration Agency (JAXA) in Japan, and the K. A. Wallenberg Foundation, the Swedish Research Council and the Swedish National Space Board in Sweden. Additional support for science analysis during the operations phase is gratefully acknowledged from the Istituto Nazionale di Astroﬁsica in Italy and the Centre National d’Études Spatiales in France.
Collective excitations of a quantized vortex in $^3P_2$ superfluids in neutron stars<|sep|>In this article, we have derived an effective theory of 3P2 neutron superﬂuid vortices at large distances and expressed solely in terms of the phase δ of the vortex proﬁle function g. g lives in one of the off-diagonal components of the tensorial 3P2 order parameter Aα i. Here, δ plays a crucial role as it gives a nonzero spontaneous magnetisation in the core region of 3P2 neutron superﬂuid vortices for the angular independent proﬁle function g. Variations in δ change the induced magnetic moment accordingly. In our analysis we have found that at low energies the 3P2 superﬂuid vortices are described by a double sine-Gordon model in addition to the well-known Kelvin (translational zero) modes part. We have derived a kink solution of this particular double sine-Gordon model and have determined its BPS energy bound. We have found that the vortex core magnetisation ﬂips its direction at the kink on the vortex. Since the number of vortices in a rotating NS is of the order of 1019 and the NS radius is about 10 km, there should exist a large number of magnetic strings in the NS interior like the ones discussed in this paper. A pair of kink and anti-kink may be created spontaneously during the creation of the vortex through phase transition. Possible implications of these objects for NS physics should be one of the most important future problems. In this article, we have neglected higher order terms (esp. sixth order terms) in the GL free energy. We also did not take into account the effect of an external magnetic ﬁeld. However, in the core of neutron stars where it is most likely to ﬁnd neutron superﬂuid, the inclusion of high external magnetic ﬁelds and of the sixth order term would be important. So it would be interesting to study the effective free energy in the presence of high external magnetic ﬁelds and with nonzero sixth order term included. We have also considered only static conﬁguration in this article. In order to discuss dynamics, we have to include time dependence. To this end, we need a time dependent GL equation, which has not been obtained yet. For the time-dependent problem, the phase δ has to be promoted to a time-dependent function and can be treated as a ﬁeld which lies on the two-dimensional string world sheet. Fluctuations of δ may indicate changes in the magnetization on the world sheet. In the case of conventional superﬂuids, Kelvin modes will propagate with quadratic dispersion relation for small system sizes, see e. g. Ref. [51]. However, it is well known that for large system sizes the dispersion relation is given by ε = klogk with wave vector k. Recently, a formula for arbitrary system sizes has been obtained in Ref. [52]. On the other hand, the gapfull mode δ was previously unknown. We expect δ to have relativistic dynamics. In this article, we have discussed only the integer vortex. For strong magnetic ﬁelds, the ground state is in the D4biaxial nematic phase admitting half-quantized non-Abelian vortices [43]. The vortex core magnetization is ten times larger than that of the integer vortex. For this case we should also derive the low-energy collective coordinate approximation and construct a magnetic lump. This work is supported by the Ministry of Education, Culture, Sports, Science (MEXT)-Supported Program for the Strategic Research Foundation at Private Universities “Topological Science” (Grant No. S1511006). C. C. acknowledges support as an International Research Fellow of the Japan Society for the Promotion of Science (JSPS). Some of the work of M. H. was undertaken at the Department of Mathematics and Statistics, University of Massachusetts, ﬁnancially supported by FP7, Marie Curie Actions, People, International Research Staff Exchange Scheme (IRSES-606096). The work of M. N. is supported in part by JSPS Grant-in-Aid for Scientiﬁc Research (KAKENHI Grant No. 16H03984), and by a Grantin-Aid for Scientiﬁc Research on Innovative Areas “Topological Materials Science” (KAKENHI Grant No. 15H05855) and “Nuclear Matter in Neutron Stars Investigated by Experiments and Astronomical Observations” (KAKENHI Grant No. 15H00841) from the the Ministry of Education, Culture, Sports, Science (MEXT) of Japan. In this appendix, we brieﬂy discuss the Ginzburg-Landau (GL) construction of 3P2 superﬂuids and the associated GL parameter values. The Hamiltonian with 3P2 contact interaction can be written as where ρρρ denotes the space coordinates, ψ is a neutron ﬁeld, µ is a baryon chemical potential, M is the neutron mass, and g(> 0) is the coupling constant. Here, α,β are the space indices, and the tensor Tαβ represents the 3P2 pair creation and annihilation operator. The differential operator t is deﬁned as and SSS is given by (Sα)σσ′ = i(σyσα)σσ′ with α = x,y,z and pauli matrices σ. The 3P2 order parameter can be represented in general as a 3 × 3 traceless symmetric complex tensor Aµi, which is deﬁned in Refs. [42, 45] as where ∆ is the gap parameter. As above, Greek subscripts stand for spin indices while Roman indices denote the spatial coordinates. The tensor Aαi is related to Tαβ as given in Ref. [22], An ansatz for the order parameter A of a vortex state has been developed ﬁrst in Refs. [21, 22]. Here, we brieﬂy sketch its derivation. For a detailed derivation we refer the interested reader to Ref. [46]. For an isolated vortex solution with cylindrical symmetry and phase mθ, where m measures the circulation about the vortex line, we ﬁrst choose the order parameter at large distances by minimising the potential in Eq. (II.3b) as Here R(θ) is the rotation matrix which rotates the axis from (ρ,θ,z) into (x,y,z). Unlike in the case of the ground state, the order parameter can also have extra contributions from the kinetic terms in the free energy functional. Both the K1 and K2 terms in Eq. (II.3a) give logarithmic contributions to the total energy. Minimizing the coefﬁcient of the logarithmic term breaks the ground state degeneracy for all −1 ≤ η ≤ − 1
A fast approximate skeleton with guarantees for any cloud of points in a Euclidean space<|sep|>Though the current implementation was tested in R3, all steps and results work in any Rm. Here is the summary of the key contributions to data skeletonization. • The detection of deep (branched) vertices in Deﬁnition 5 uses a global structure of longest paths within MST(C), hence is more stable under a change of parameters. • To improve the Metric Graph Reconstruction by M. Aanjaneya et al. [1], we have split one parameter r (used for detecting vertex points and also for clustering later) into two separate parameters (with default values) r1 = 15, r2 ∈ [1,2], which led to more successful (20-40% rates instead of 0%) reconstructions in Table 10. • Theorem 8 proves the ﬁrst size guarantees (on a small number of vertices) for the Approximate Skeleton ASk(C), while all past methods from section 2 considered topological (mostly homotopy type) or metric properties of reconstructed graphs. • Corollary 9 says that the Approximate Skeleton ASk(C) can be quickly computed within a given error as required in the Tree Reconstruction Problem from section 1. Because of the page limit the last author couldn’t include one more result on ASk(C) with realistic conditions on an underlying tree T ⊂ Rm and its noisy sample C to guarantee that MST(C) and ASk(C) are homeomorphic to T. This is the ﬁrst advance after Giesen’s guarantees for shortest paths through sample points [14] in 1999. The C++ code of ASk(C) is at https://github.com/YuryUoL/AsKAlgorithm. In comparison with the past methods in section 2, ASk(C) starts from the most challenging input (an unorganized cloud of points C ⊂ Rm without any extra structure such a metric graph or a regular grid or a mesh), outputs an embedded graph in Rm and provides two guarantees: combinatorial in Theorem 8 and geometric in Corollary 9 and topological. Appendix A has all missed proofs. Appendix B includes more experiments. We thank all reviewers for their helpful suggestions.
Radiative hydrodynamics simulations of red supergiant stars: I. interpretation of interferometric observations<|sep|>Our radiation hydrodynamics simulations conﬁrm that only a few large granules cover the surface of RSG stars. The granules of the simulation we analyze here are 400–500R⊙ in diameter, and have lifetimes of years. Smaller scale structures develop and evolve within these large granules, on shorter timescales (a month). We demonstrate that RHD simulations are necessary for a proper quantitative analysis of interferometric observations of the surface of RSGs beyond the smooth, symmetrical, limbdarkened intensity proﬁles. We give new average limb darkening coeﬃcients within the H and K bands, that are signiﬁcantly different from commonly used UD or LD proﬁles. However, these LD coeﬃcients ﬂuctuate with time, and the average is only indicative. Our model surface granulation causes angular and temporal variations of visibility amplitudes and phases. In the ﬁrst lobe, sensitive to the radius, ﬂuctuations can be as high as 5%, and radii determinations can be aﬀected to that extent: the radius determined with a UD ﬁt is 3-5% smaller than the radius of the simulation, while the radius determined with a fully LD model is 1% smaller. The second, third, and fourth lobes, that carry the signature of limb-darkening, and of smaller scale structure, are very different from the simple LD case. The visibility amplitudes can be greater than the UD or LD case, and closure phases largely diﬀer from 0 and ±π, due to the departure from circular symmetry. The
Numerical treatment of interfaces in Quantum Mechanics<|sep|>We have shown that it is possible to implement an interface scheme of the “Penalty” type for the Schr¨odinger equation similar to the ones used for ﬁrst order hyperbolic and parabolic equations. It shares with them similar properties, only data at points at the interface need to be passed between grids, and convergence is ensured for linear, constant coeﬃcient, systems. Although the scheme seems to be third order accurate, the actual error we ﬁnd in our test, at the lowest reasonable resolution, seems to compare very well with sixth order homogeneous (centered ﬁnite diﬀerence operators) schemes. This is important for multi-block parallelizations for it implies one obtain the same quality for a solution only sharing (1/3)n, n being the space dimension, of the data one would need for a comparable (in accuracy) homogeneous scheme, that is, for low but reasonable resolutions, only passing one point at the boundary instead of three points a sixth order centered diﬀerence operator would need.
Spin alignment and violation of the OZI rule in exclusive $\omega$ and $\phi$ production in pp collisions<|sep|>In this work, exclusive φ and ω vector meson production in the reaction pp → pV p has been measured. We ﬁnd OZI violations ranging from FOZI = 3 to FOZI = 9 depending on the kinematic region. The invariant mass MpV of the forward proton and the vector meson appears to be the most important kinematic quantity in our study to discriminate processes with different mechanisms. The clear structures in the Mpω spectrum indicate the importance of pp → pN ∗,N∗ → pω in ω production. This is also supported by the signiﬁcant alignment of the spin of the ω meson with respect to the direction of the pω system. In the case of decays into a ground state vector meson, the N∗ has to transfer considerable angular momentum. The absence of structures in the Mpφ spectrum in combination with no observed alignment of the φ spin with respect to the direction of the pφ system shows that the decay of the N∗ resonances into pφ is OZI suppressed. This indicates that the ss component of such resonances must be very small. The observed OZI violation by a factor 3-4 in this region could be either due to the admixture of other processes or a genuine violation of the predicted g2 φNN/g2 ωNN ratio. Removing the resonance region by requiring Mpω > 3.3 GeV/c2, the OZI violation in the remaining kinematic range is signiﬁcantly higher, typically of order 8±1. Moreover, the spin of both ω and φ are unaligned with respect to the pV system. The behaviour of both vector mesons is the same in the system deﬁned by the transferred momentum. This indicates that the production mechanism in this region for both ω and φ is central Reggeon–Pomeron fusion, with the observed OZI violation reﬂecting a hidden ﬂavour ﬂow. This process can also be regarded as a Pomeron resolving preformed colourless objects in the proton wave function and ejecting them in a shake-out. The direction of the transferred momentum is remembered by the vector meson and is manifested in its decay angular distributions. The OZI violation then reﬂects the probability of resolving a ss state in the nucleon. We are grateful to Prof. Colin Wilkin and Prof. Stefan Leupold for helpful discussions. We also gratefully acknowledge the support of the CERN management and staff and the skill and effort of the technicians of our collaborating institutes. Special thanks go to V. Pesaro for his technical support during the installation and the running of this experiment. This work was made possible by the ﬁnancial support of our funding agencies.
Automatic morphological classification of galaxies: convolutional autoencoder and bagging-based multiclustering model<|sep|>In this paper, we present a UML method that can automatically classify galaxies with similar morphology in deep ﬁled surveys. The method consists of two steps: (1) the CAE is used to compress the dimensions from the raw data and extracts features, and (2) the bagging-based multiclustering method sorts out the assuring galaxies with analogous characteristics into one group. After discarding the galaxies with inconsistent results of voting, the remaining galaxies are well-clustered into 100 groups. To further investigate our galaxy morphologies, we merge the groups into ﬁve main categories (SPH, ELD, LTD, IRR, and UNC) by visual veriﬁcation. After discarding the lowS/N UNC category, we utilize massive galaxies (M∗ > 1010M⊙) to explore the robustness of the morphological classiﬁcations by the connection with other physical properties. We show that this classiﬁcation scheme is in a good agreement with other galaxy properties, including UVJ diag t-SNE visualization graphs based on the encoded hidden features of the CAE. The upper panels show the randomly selected galaxies from all the ﬁve CANDELS ﬁelds. Panels a and b are color coded by our results and those of the supervised method (Huertas-Company et al. 2015), respectively. The bottom panels show the randomly selected subsamples only from the GOODS-S ﬁeld. Panels c and d are color coded by our results and those of CANDELS visual classiﬁcations (Kartaltepe et al. 2015), respectively. It shows that our results (i.e., Panels a and c), have noses and other morphological parameters. The comparison suggests that our method gives a robust result of morphological classiﬁcation. Overall, the unsupervised method provides an independent feature extraction and galaxy classiﬁcation only utilizing the monochromatic H-band images. It is able to obtain the reasonable clustering of galaxies with similar features ﬁrst, and further increasing the efﬁciency of visual classiﬁcation. Since a strict voting model is applied, the clustering subsamples with high quality can be used as a training sample in other downstream tasks such as the supervised machine learning. In the future, we intend to develop the techniques for multicolor images and apply them to the data processing from the Euclid and CSST. The techniques provide the results of galaxy classiﬁcations with few human intervention. We expect that the future work would help us to have a better understanding of the morphologies of galaxies as well as their formation and evolution. This work is based on observations taken by the 3D-HST Treasury Program (GO 12177 and 12328) with the NASA/ ESA HST, which is operated by the Association of Universities for Research in Astronomy, Inc., under NASA
A Super-Jupiter Microlens Planet Characterized by High-Cadence KMTNet Microlensing Survey Observations of OGLE-2015-BLG-0954<|sep|>High-cadence Γ = 6 hr−1 observations by KMTNet resolved the extremely short 2tcc = 33 minute caustic crossing of the planetary event OGLE-2015-BLG-0954. This proved crucial to both the measurement of the Einstein radius (θE = 1.7 mas) and to the arguments that the lens must be in the near disk (from the high proper motion µrel = 18 masyr−1). Hence, it was crucial both to the mass estimates of the host (0.33 ± 0.12 M⊙) and planet (3.9 ± 1.4 Mjup), and to the recognition that the lens and source will be separately resolvable in just a few years. While such short caustic crossings are relatively rare, they are more likely to lead to complete mass and distance solutions than for typical events. This is because the most likely reason for the short crossing time is very high lens-source proper motion, which in turn is most likely due to a very nearby lens. It is easy to measure the ﬂux of such nearby lenses if they are of relatively high mass. On the other hand if they are low mass, their microlens parallax (Gould, 1992) will be large and hence more easily measurable. Either of these eﬀects can provide the second parameter needed for a complete solution, provided that the Einstein radius has been measured by resolving the caustic crossing. In the case of OGLE-2015-BLG-0954, we were able to constrain the mass of the lens from the limit on lens light. No parallax eﬀects were detected in the light curve. Hence, we could not obtain a deﬁnitive measurement. Instead we estimated the lens mass and distance based on kinematic arguments from its high proper motion. These are consistent with independent arguments derived from upper limits on lens light. They are also consistent with a white dwarf host, although this would be more surprising. The nature of the lens can be completely resolved in a few years based on high resolution imaging. This research has made use of the KMTNet system operated by KASI and the data were obtained at three host sites of CTIO in Chile, SAAO in South Africa, and SSO in Australia. This work was supported by KASI (Korea Astronomy and Space Science Institute) grant 2016-1-832-01. Work by J. C. Y. was performed under contract with the California Institute of Technology (Caltech)/Jet Propulsion Laboratory (JPL) funded by NASA through the Sagan Fellowship Program executed by the NASA Exoplanet Science Institute. The OGLE team thanks Profs. M. Kubiak and G. Pietrzy´nski, former members of the OGLE team, for their contribution to the collection of the OGLE photometric data over the past years. The OGLE project has received funding from the National Science Centre, Poland, grant MAESTRO 2014/14/A/ST9/00121 to A. U. A. G. acknowledges support from NSF grant AST1516842.
HeatER: An Efficient and Unified Network for Human Reconstruction via Heatmap-based TransformER<|sep|>In this paper, we present HeatER, a novel heatmap-based transformer architecture for HPE and HMR. HeatER can preserve the heatmap representations and effectively model global correlations between them via self-attention. By performing decomposition with th w and h dimensions, HeatER signiﬁcantly reduces the computational complexity compared with vanilla transformer architecture. Furthermore, the introduced heatmap reconstruction module improves the robustness of the estimated human pose and mesh. Extensive experiments show that HeatER improves the performance while signiﬁcantly reducing the computational cost for HPE and HMR tasks. One of the limitations is that our uniﬁed network still requires a backbone such as HRNet to extract the coarse heatmaps. Further works will focus on creating a backbone using our proposed HeatER blocks. In Table 6, we list the layer-by-layer comparison between one vanilla transformer block and one HeatER block. The shape of a stack of heatmaps is [n, h, w], where n is the number of heatmaps, h and w are the height and width of the heatmap, respectively. If h = w = 64, the embedding dimension of d would be d = hw = 4096 without discarding any information. Since d is much larger than n, the computational complexity of one vanilla transformer block and one HeatER block can be written as O(d2) and O(d3/2), respectively. To be more speciﬁc, let there be a stack of 32 heatmaps with the dimension of [32, 64, 64]. One vanilla transformer block requires 4.3G MACs when the embedding dimension is d = 64 × 64 = 4096 (i.e., ﬂattening the spatial dimension). Even if we further reduce the embedding dimension to d = 1024, it still needs 0.27G MACs. However, given heatmaps [32, 64, 64], HeatER only requires 0.09G MACs, which signiﬁcantly reduces the computational cost. Table 6: The detailed complexity comparison between one vanilla transformer block and one HeatER block. We calculate their MACs based on the input and output with the corresponding operation. x to QKV xin [n, d] QKV [n, 3d] nn.Linear(d, 3d) 3nd2 x to QKV xw in: [n, h, w] QKV [n, h, 3w] nn.Linear(w, 3w) 3nhw2 a1 = QKT Q[n, d], KT [d, n] a1[n, n] torch.matmul n2d aw 1 = QwKwT Qw[h, n, w],KwT [h, w, n] aw 1 [h, n, n] torch.matmul n2hw xattn = a1V a1[n, n], V [n, d] xattn[n, d] torch.matmul n2d xw attn = aw 1 V w aw 1 [h, n, n], V [h, n, w] xw attn[h, n, w] torch.matmul n2hw xh attn = ah 1 V h ah 1 [w, n, n],V h [w, n, h] xw attn[w, n, h] torch.matmul n2hw We ﬁrst train our HeatER on COCO dataset for the 2D HPE task. Following [3, 8], we apply the Mean Squared Loss (MSE) between the predicted heatmaps (HM) HM ∈ RK×h×w and the ground truth 2D pose HM GT ∈ RK×h×w, where K is the number of joints, h and w are the height and width of heatmaps, respectively. When the input image is 256 × 192 and the number of joints is K = 17, the heatmap size would be w = 64, and h = 48, respectively. The MSE for 2D pose is deﬁned as follows: L2D−P ose = ∥HM − HM GT ∥2 (13) We apply an L1 loss between the predicted 3D pose J ∈ RK×3 and the ground truth 3D pose JGT ∈ RK×3 following [40, 10, 31]. K is the number of joints. Following [31], we use the SMPL [48] model to output human mesh, which is obtained by ﬁtting the 3D pose J, the shape parameter β, and the rotation parameter θ into the SMPL model. We supervise the shape and rotation parameters by applying the L2 loss following [48]. The overall loss is deﬁned as follows: Loverall = L3D−P ose + w1∥β − βGT ∥ + w2∥θ − θGT ∥ (15) where w1 = 0.01 and w2 = 0.01 are the weights for the loss terms. Fig. 6 provides visulization of 17 heatmaps (COCO [33] 17 joints format) and the predicted 2D poses of the input images. Figs. 7 and 8 show the HMR visualization of HeatER on several in-the-wild images from the COCO [33] dataset. HeatER can estimate accurate human meshes of the given images with regular human articulation in Fig. 7. For some very challenging cases as shown in Fig. 8, HeatER can still output reliable human meshes. Figure 8: Mesh reconstruction qualitative results of the proposed HeatER for more challenging cases. Images are taken from the in-the-wild COCO [33] dataset. When comparing with the state-of-the-art HMR method METRO [10], HeatER clearly outperforms METRO with only 5% of Params and 16% of MACs on these in-the-wild images (taken from the COCO [33] dataset) as depicted in Fig. 9, demonstrating the superiority (in terms of both accuracy and efﬁciency) of the proposed HeatER method for practical applications. Although HeatER can estimate human mesh quite well as demonstrated in Figs. 7 and 8, there are still some inaccurate and failure cases. As presented in Fig. 10 left, the red circle indicates the inaccurate mesh part due to heavy occlusion. The proposed Heatmap Reconstruction Module is not enough to tackle this issue with limited training data. For more complex human body articulation in Fig. 10 right, HeatER fails to estimate accurate human mesh. How to further improve the generalization of HeatER to in-the-wild images would be our future work. Figure 9: Qualitative comparison with the state-of-the-art HMR method METRO [10]. Images are taken from the in-the-wild COCO [33] dataset. The red circles highlight locations where HeatER is more accurate than METRO.
Activity-Based Search for Black-Box Contraint-Programming Solvers<|sep|>idea of using the activity of variables during propagation to guide the search. A variable activity is incremented every time the propagation step ﬁlters its domain and is aged otherwise. A sampling process is used to initialize the variable activities prior to search. Activity-based search was compared experimentally to the Ibs and wdeg heuristics on a variety of benchmarks. The experimental results have shown that Abs was signiﬁcantly more robust than both Ibs and wdeg on these classes of benchmarks and often produces signiﬁcant performance improvements.
Source properties of the lowest signal-to-noise-ratio binary black hole detections<|sep|>We report on a comparison of the source property measurement of the 7 BBH sources ﬁrst presented in [1, 2]. The analysis therein (corresponding to Conﬁg. C in this paper) includes two binaries with χeﬀ signiﬁcantly deviating from zero. We also perform parameter estimation analyses using the standard algorithms of the LVC [3, 59, 85] where we use both waveform models allowing for spin-precession (Conﬁg. A) and assuming spins only (anti-)aligned to the orbital angular momentum (Conﬁg. FIG. 8: Corner plot for posterior distributions for GW170425, red for Conﬁg. A results, green for Conﬁg. B and blue for Conﬁg. C. M/M⊙ 46+16 −8 45+13 −8 48+26 −10 q 0.7+0.3 −0.3 0.7+0.3 −0.3 0.7+0.3 −0.3 χeﬀ 0.0+0.3 −0.3 0.0+0.3 −0.3 0.1+0.4 −0.4 DL/Gpc 2.8+2.0 −1.4 2.7+1.9 −1.4 3.3+2.9 −1.6 B). In the analysis for Conﬁgs. A and B, the data from the GW detectors is assumed to be described by a stationary and Gaussian noise process modelled on 4-second-long data segment under analysis using the spectral model in BayesWave, whereas Conﬁg. C assumes the noise to be well described by a PSD estimated through Welch’s method from a signiﬁcantly longer data segment surrounding the GW signal times a time-dependent normalization that is measured on a ∼ 15 second scale. The three conﬁgurations also diﬀer signiﬁcantly in their respective prior assumptions on the black hole spin parameters, which as shown by [55] could have a signiﬁcant eﬀect on the inferred posterior distributions, especially for high-mass BBH systems with low SNRs such as the ones presented in this study. Conﬁgs. A and B recover lower values for χeﬀ, as compared to [1, 2], which becomes consistent with zero or with low component spins for all reported BBHs, Fig. 2. In par
Critical Fluctuations in Polymer Solutions: Crossover from Criticality to Tricriticality<|sep|>We have shown that the nature of critical ﬂuctuations in polymer solutions, including the crossover from Ising-like critical behavior near the critical point of mixing to tricritical behavior near the theta point, has been resolved both theoretically and experimentally. Speciﬁcally, this crossover is governed by a competition between two characteristic length scales, namely, the correlation length of concentration ﬂuctuations, ξ, related to the osmotic susceptibility, which diverges at the critical point, and the polymer-chain correlation length, ξψ, related to the radius of gyration, Rg. This competition causes a coupling between the two order parameters: the conserved order parameter associated with the polymer concentration, φ, and the
Mixture Proportion Estimation via Kernel Embedding of Distributions<|sep|>Mixture proportion estimation is an interesting and important problem that arises naturally in many ‘weakly supervised learning’ settings. In this paper, we give an efﬁcient kernel mean embedding based method for this problem, and show convergence of the algorithm to the true mixture proportion under certain conditions. We also demonstrate the effectiveness of our algorithm in practice by running it on several benchmark datasets. 6The code for our algorithms KM1 and KM2 are at http: //web.eecs.umich.edu/˜cscott/code.html#kmpe. The code for ROC was taken from http://web.eecs. umich.edu/˜cscott/code/mpe.zip. The codes for the alphamax and EN algorithms were the same as in Jain et al. (2016), and acquired through personal communication.
Stability of generalized Einstein-Maxwell-scalar black holes<|sep|>We have studied the stability of static spherically symmetric black holes in generalized Einstein-Maxwell-scalar theories with an electrically charged Maxwell ﬁeld. We have obtained the master equations in the odd and even parity sectors. We found that the ghost free conditions reduce to We found that four degrees of freedom propagate at the speed of light, while a degree of freedom associated to the scalar ﬁeld could propagate faster or slower than the speed of light, its expression is given in eq.(5.26), from which we obtained the subclass of generalized Einstein-Maxwell-scalar theories where all degrees of freedom propagate at the speed of light eq. (5.28). Imposing the ghost free conditions, we found that odd-parity perturbations are always stable and we obtained explicitly the eﬀective potential matrix. Considering the even-parity sector, we derived the master equation and the procedure to calculate the eﬀective potential matrix for a given model. We also found that assuming ghost free conditions, the perturbations are unstable if f2,F +2 ¯Ff2,FF < 0 for models with f2,XF = 0. Finally, we have applied this formalism for a large number of BH for which we could easily obtain the stability conditions and calculate the QNMs. We could recover all previously derived results in the literature which guarantees the correctness of our calculations. As an application, the paper can be used to study the QNMs of gravitational waves, electromagnetic radiation and scalar radiation. The presence of these three ﬁelds appears naturally in higher dimensional theories of gravity such as supergravity or string theory. Interestingly, we didn’t ﬁnd any stable hairy black hole in scalar tensor theories. In order to facilitate future analysis of black hole stability in generalized EinsteinMaxwell-dilaton gravity, all the perturbed equations are listed in a Mathematica® notebook available online [18].
LOFT as a discovery machine for jetted Tidal Disruption Events<|sep|>Although the LAD instrument would be roughly 5 times more eﬀective in identifying TDEs, the WFM has the ability to serendipitously catch events, closer to their onset. This is shown in Fig.5, where the cumulative distribution of the trigger delay is shown as a function of redshift (from z=0.1 up to 0.6, that is the zmax of WFM TDEs). While the WFM will be able to detect TDEs at any epoch, the LAD will preferentially catch them at later times, after ≈ 10 days from their beginning. This latter delay is not due to an intrinsic problem of the LAD instrument, but it is a natural consequence of the delay introduced by the SKA trigger: the radio ﬂux increases over time, and becomes bright enough for a SKA detection only after at least ≈ 10 days from the beginning of the event (Donnarumma & Rossi, 2014). Another diﬀerence in the performance of the two instruments is the ability of LAD to go twice as far in redshift (z ∼ 1.2) than WFM (z ∼ 0.6). Synergy between the two instruments can also been envisaged. The LAD instrument can repoint within < 1 day candidates detected with WFM, allowing the study of the event lightcurve, also at early times (unlike SKA triggered TDEs). The potential of the WFM for jetted TDE discoveries can be appreciated better comparing its performance with eRosita. Currently, this survey rapresents the best chance to serendipitously discover TDEs in X-ray, in the near future (Merloni et al., 2012). Indeed, the combination of a great sensitivity in 0.2 - 12 keV band and the pointing strategy (4-year all sky survey) makes it the optimal hunter of thermal TDEs, with ∼ 1000 objects per scan at a sensitivity level several times better than RASS. However, it will be less performing at detecting non-thermal TDEs. Following the identiﬁcation strategy detailed above, eRosita will be able to detect a factor 3 to 4 less jetted TDEs than the WFM in the hard X-ray band (see also Khabibullin et al., 2014). This is because eRosita will not cover an event repeatedly, but only every 6-month. LAD can instead be compared with the Wide Field Imager (WFI) on board of Athena, the ESA L2 mission, planned to operate in 2028. We stress that radio counterparts need an X-ray follow-up to be conﬁrmed and LOFT, together with Athena, is likely to be the only eﬀective instrument operating simultaneously with SKA. If we use the very same follow-up strategy and we consider an optimistic repointing eﬃciency of 50%, then Athena may be able to identify slightly more TDEs (a factor 1.1-1.2). The reason of this small diﬀerence in identiﬁcation Figure 5: Cumulative distributions of delays in detecting the TDE from the explosion time, for diﬀerent redshifts. We show results for the WFM (black lines) and LAD follow-ups of radio triggered TDEs (blue lines). The diﬀerent line styles are for z = 0.1, 0.2, 0.37, 0.6 from right to left for WFM and vice versa for LAD. Note that the LAD instrument can repoint events only after 10 days, because the radio lightcurve becomes suﬃciently bright to be detected by SKA only after that time. rates is that only a moderate X-ray sensitivity ( >∼ 5 × 10−12 erg cm−2 s−1) is required to obtain a complete set of X-ray counterparts of the radio triggered TDEs. However, the LAD wider energy range has the advantage of better constraining the total emitted luminosity and thus the jet energy content. Most importantly, Athena will not have a wide ﬁeld monitor instrument on board and without a radio trigger will be blind to TDEs, while LOFT on the other hand can do TDE science autonomously. Thus, in this respect, LOFT is a more complete mission, combining the potential for serendipitous discoveries with the WFM and repointing ability with LAD. In summary, LOFT has the potential to ﬁnally open a new era of statistical studies of TDEs and exploit them to address open questions in galaxy formation and high energy astrophysics. With a few tens to a few hundreds of events and their radio counterparts, one may constrain the SMBH mass function below redshift 1.5 and the physics of jet formation during transient accretion episodes: e.g. jet eﬃciency, Lorentz factor, and what is the connection with a super-Eddington accretion phase.
Extended Birkhoff's Theorem in the f(T) Gravity<|sep|>In this letter we prove the validity of Birkhoﬀ’s theorem in f(T) gravity with both the diagonal and the oﬀ diagonal tetrad ﬁelds. In our previous work [33], we have detailedly discussed the physical meanings of the Birkhoﬀ’s theorem in ordinary conditions, namely, the external vacuum gravitational ﬁeld. More generally, we consider a spherically symmetric matter distribution as shown in Fig.1(a). The gravitational ﬁeld of the interlining vacuum region is spherically symmetric, because of the symmetric distribution of source matter. Accordingly, the above analysis in this letter is applicable, and the gravitational ﬁeld of the vacuum region is static. The only property of the source matter may appear in �a(r) of equation (20) is the mass of internal source MI and mass of external source ME. (A similar problem is discussed speciﬁcally in [40, 41] for black hole). The radial motion and distribution of the source matter cannot aﬀect the gravitational ﬁeld any way. The second conclusion is that the main feature of the Birkhoﬀ’s theorem is applicable in non-vacuum regions, such as the case shown in Fig.1(b). Note that we actually do not claim that the vanishing of density and pressure of matter is necessary to prove the validity of Birkhoﬀ’s theorem. To obtain Eqs.(16, 17), what we really demand is that the non-diagonal elements of the energy-momentum tensor are zero, which is always satisﬁed for perfect ﬂuid models. As a conclusion, the gravitational ﬁeld is static inside the spherically symmetric matter, such as the region denoted by dashed line in Fig.1(b). This conclusion is correct only if there is no radial motion or convection across the sphere (the dashed line in Fig.1(b)). As is known to all, Hawking’s theorem [42] states that a stationary space-time containing a black hole is a solution of the Brans-Dicke ﬁeld equations with V (φ) = 0 if
Dual-Scale Single Image Dehazing Via Neural Augmentation<|sep|>In this paper, a new type of single image dehazing algorithm is introduced by using model-based deep learning frameworks. Both transmission map and atmospheric light are obtained by a neural augmentation which consists of model-based initialization and data-driven reﬁnement. They are then applied to restore a haze-free image. Experimental results validate that the proposed algorithm removes haze well from the synthetic and real-world hazy images. The proposed neural augmentation reduces the number of training data signiﬁcantly, and the proposed neural augmentation framework converges faster than the corresponding data-driven approach. It is thus more friendly to domain adaptation and continual learning. It is worth noting that this paper focused on day-time hazy images. The proposed framework will be extended to study night-time hazy images [56] in our future research. θ denote the vector of all network parameters, θτ be the timedependence of the parameters, and ητ be the learning rate. For brevity of notation, Iτ also denotes the vector of the restored image. It can be derived that [57] � ˙θτ = −ητ(∇θIτ)T ∇Iτ Ld ˙Iτ = ∇θIτ ˙θτ = −ητΘτ∇Iτ Ld , (29) Similar to [57], consider the case that the loss function Ld is deﬁned by the l2 norm and the learning rate η is ﬁxed. It follows that If the models tm and Am in the equation (5) are accurate, Im in the equation (28) then corresponds to the datadriven approach Id,τ with an initial θ0 and the θ0 is in a neighborhood of the optimal θ∗. By using the ﬁrst-order Taylor expansion, the neural augmentation framework (28) is approximated by Let the matrix ∇θId,τ|θ=θ0(∇θId,τ|θ=θ0)T be denoted as Θ0 which is symmetric positive semideﬁnite. Since the matrix Θ0 is constant throughout training, it can be derived that Since ∥Im −T∥ is smaller than ∥T∥, the proposed neural augmentation framework converges faster than the corresponding data-driven approach. It is worth noting that the analysis will be more complicated if the loss function is not the l2 loss. It was pointed out in [24], [49], [50], [51] that the GAN is usually locally stable. Therefore, the accuracy of the modelbased components Am and tm(p) in the equation (5) is also important for the stability of the proposed neural augmentation. The proposed neural augmentation framework is actually a switched system [37] because the learning rate ητ is initially set to 10−5, and then decreased using a cosine annealing schedule.
Hydrogen bond analysis of confined water in mesoporous silica using the reactive force field<|sep|>We conducted large-scale MD simulations of conﬁned water in nanoporous silica with a pore diameter of 2.7 nm using ReaxFF. We compared Yeon’s and Pitman’s parameter sets for our system. We found that the former is suitable for the analyses of water
Statistical properties of magnetic structures and energy dissipation during turbulent reconnection in the Earth's magnetotail<|sep|>We utilized two-dimensional models of the expected magnetic signatures of plasmoids, pull current sheets, and push current sheets to automate the detection and categorization of 288 magnetic structures within a 17-minute turbulent reconnection region. The majority of these had sizes between the electron and ion skin depths, making this the ﬁrst statistical survey of mainly electron-scale structures within the same current sheet. It is possible to change the parameters of the detection algorithm to ﬁnd systematically larger structures, but the focus of this work was on the smaller-scale ones, which may potentially be embedded within larger structures. The estimated size distribution of the plasmoids was found to ﬁt a decaying exponential, which is consistent with Fermo et al. (2010)’s statistical model of plasmoid distribution, growth, and merging. The presence of push current sheets consistent with plasmoid merging provides further evidence of the importance of merging plasmoid dynamics to the overall structure of the reconnecting current sheet. The bulk motion of the structures supports the analysis of Ergun et al. (2018), who observed a large-scale reconnec tion region with turbulent outﬂows. We also noticed that structure sizes were positively correlated with the structure speeds (not shown). However, the resolution limit of the magnetic ﬁeld data prevents detection of small, fast-moving structures. Additionally, structure speed is used to calculate size, so there could be some artiﬁcial correlation. The region was shown to have signiﬁcant energy conversion from ﬁelds to particles and vice versa, but net J ·E was positive for particle energization at the expense of the ﬁeld energy. On average the structures were signiﬁcant contributors to the net J∥E∥ of the region, contributing ∼40% of the net J∥E∥. In contrast, 97% of the J⊥·E⊥ contribution was from the regions between the structures, meaning that these larger regions were the main contributor to the overall positive J·E, which was comprised of 85% J⊥· E⊥ from outside of the structures. This is consistent with a model of the structures as injection sites, with strong localized J∥E∥ able to quickly accelerate electrons, which then can be slowly accelerated along with ions in the larger-scale regions of net positive J⊥· E⊥. This indicates that the majority of the particle acceleration from these turbulent reconnection regions can be modeled using larger-scale physics, with the smaller-scale J∥E∥ injection sites largely ignored, or modeled as source terms of energetic electrons. Therefore, codes which are focused on capturing the larger-scale dynamics of reconnection regions (such as Drake et al. (2019)), perhaps with added electron injection, should accurately describe the bulk of the particle energization in the reconnection region. Fitting the plasmoids to mathematical models would yield more details about their structure. We found that the observed plasmoids did not ﬁt the constraints of force-free or non-force free cylindrical models, but more general models were not tried. The use of other methods for ascertaining magnetic ﬁeld topology, such as the ﬁrst-order Taylor expansion method outlined in Fu et al. (2015), would also provide greater insight into the structure of this turbulently reconnecting region. It would be valuable to repeat the analysis of this paper using a diﬀerent plasmoid detection algorithm, such as the method detailed in Nakamura et al. (2016), which requires strong guide ﬁeld. A machine learning algorithm could possibly be more comprehensive than our algorithm, which has inﬂexible cutoﬀs for structure detection. This work did not explore whether the observed current sheets were reconnecting or not. If a nuanced automated method was developed to detect evidence of ongoing reconnection, additional information about the dynamics of the reconnection region could be obtained. Another valuable expansion of this work would be to examine particular structures of interest from a three-dimensional viewpoint. Recent works such as Øieroset et al. (2016, 2019) have shown that structures which ﬁt simple two-dimensional models such as that of a ﬂux rope can have more complex three-dimensional topology, which can impact the onset and rate of reconnection. Given the large number of magnetic structures and potential for multiple X-line reconnection in this region, an in-depth three-dimensional exploration of even a few of the magnetic structures in this region has the potential to provide further insight into turbulent reconnection dynamics. Acknowledgments This work was supported by the U.S. Department of Energys Oﬃce of Fusion Energy Sciences under Contract No. DE-AC0209CH11466, by NASA under Grant No. NNH15AB29I, and by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-2039656. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of the funding organizations. Analysis scripts used for this manuscript can be found in the DataSpace of Princeton University (https://dataspace.princeton.edu/handle/88435/dsp01x920g025r). The data used is available from the MMS Science Data center (https://lasp.colorado.edu/mms/sdc/public/).
Heavy quark potential and jet quenching parameter in a D-instanton background<|sep|>In this paper, we have investigated the heavy quark potential and the jet quenching parameter in a D-instanton background. The dual gravitational theory is related to a near horizon limit of stack of black D3-branes with homogeneously distributed D-instantons. Although the theory is not directly applicable to QCD, the features of D3-D(-1) conﬁguration is similar to QCD. Thus, one can expect the results obtained from this theory should shed qualitative insights into analogous questions in QCD. In section III, we have investigated the heavy quark potential in this D-instanton background. The potential was obtained by calculating the Nambu-Goto action of string attaching the rectangular Wilson loop. It is shown that the presence of instantons tends to suppress the heavy quark potential and increase the dissociation length. In section IV, the jet quenching parameter has been studied in this D-instanton background as well. It is found that the nonzero instanton density has the eﬀect of enhancing the jet quenching parameter. Also, after taking some proper values of α′ and λ, we observe that the results are consistent with the experimental data. Finally, it is interesting to note that the instanton eﬀects on the heavy-quark potential has also been studied from lattice QCD recently [51].
The weighted Tower of Hanoi<|sep|>A new generalization of the Tower of Hanoi puzzle was presented in this paper, in this generalization the moves between pegs are weighted and the objective is to solve the puzzle while minimizing the total cost. The special thing about this new generalization is that it generalize the concept of the minimum number of moves in the classical Tower of Hanoi so that each move is measured using a positive weight. We gave a recursive formula of the total cost in terms of the number of discs, also we introduced an optimal algorithm to solve this new optimization problem using dynamic programming. We established also the relationship between the weighted Tower of Hanoi and its variants with restricted disc moves. Furthermore, due to the complexity time of the recursive algorithm proposed in this paper, one could think about using heuristics and meta-heuristics to solve the WTH problem.
A matter of time: Using dynamics and theory to uncover mechanisms of transcriptional bursting<|sep|>The rapid development of live-imaging technologies has opened unprecedented windows into in vivo transcriptional dynamics and the kinetics of the underlying molecular processes. We increasingly see that transcription is complex, emergent, and—above all—highly dynamic, but experiments alone still fail to reveal how individual molecular players come together to realize processes that span a wide range of temporal scales, such as transcriptional bursting. Here we have argued that theoretical models can help bridge this crucial disconnect between single-molecule dynamics and emergent transcriptional dynamics. By committing to mathematical formulations rather than qualitative cartoon models, theoretical models make concrete quantitative predictions that can be used to generate and test hypotheses about the molecular underpinnings of transcriptional control. We have also shown how, although diﬀerent models of biological phenomena might be indistinguishable in their averaged behavior, these same models often make discernible predictions at the level of the distribution of such behaviors. Moving forward, it will be critical to continue developing models that are explicit about the kinetics of their constituent molecular pieces, as well as statistical methods for connecting these models to in vivo measurements in an iterative dialogue between theory and experiment. In particular, robust model selection frameworks are needed to navigate the enormous space of possible molecular models for transcriptional control. Such theoretical advancements will be key if we are to synthesize the remarkable experimental ﬁndings from recent years into a truly mechanistic understanding of how transcriptional control emerges from the joint action of its molecular components.
Focal-plane wavefront sensing with photonic lanterns I: theoretical framework<|sep|>In this work, we provide an end-to-end mathematical analysis of the PLWFS. In Sections §2 and §3, we developed linear and higher-order mathematical models for the intensity response of the PLWFS. These models enable the reconstruction of wavefront aberrations from intensity responses, and enable the derivation of certain metrics, such as the degenerate radius, which estimates the maximum amount of RMS WFE an aberration can have before the mapping of aberrations to intensities is no longer one-to-one. Such metrics can be used to benchmark and control the sensing behavior of these devices. Higher-order reconstruction models, such as quadratic (§2.3 and §5.3) and cubic (Appendix B), can additionally enable greatly improved reconstruction accuracy over the the standard linear model, but at the cost of added computation time and potentially increased numerically instability. Through our framework, we also show that a fully mode-selective lantern cannot sense wavefront aberrations with even pupil illuminations. As a proof-of-concept, we apply our reconstruction models to a standard, hybrid, and modeselective 6-port lantern in Section §5, and successfully show that for the ﬁrst two cases wavefront reconstruction of the ﬁrst 5 non-piston Zernike modes is possible; 5 is the maximum number modes that can be sensed by either 6-port variant. We additionally conﬁrm, numerically, that mode-selectivity (at least with an even pupil) hinders wavefront sensing. Comparing the performance of the standard and hybrid lanterns at a single output wavelength of 1.55 μm, we ﬁnd that the standard lantern has the highest linear range, accurately sensing the ﬁrst ﬁve non-piston Zernike modes out to ∼ 0.5 radians, followed by the hybrid lantern. Conversely, the 6-port hybrid PL outperformed the standard PL in terms of degenerate radius. In the second part of this paper, we extend our analysis and simulate reconstruction performance for a range of PLs in various conﬁgurations. We additionally provide initial investigations into new strategies through which the sensing properties of PLs can be controlled and optimized. In the near future, we hope to verify our results with real, imperfect photonic lanterns, through experimental and on-sky testing, and in doing so, add to the next generation of focal-plane wavefront sensors. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-2034835. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of the National Science Foundation. This work was also supported by the National Science Foundation under Grant No. 2109232. The linearity criterion from §3.3 can be simpliﬁed for a standard 6-port lantern, located in the focal plane of a telescope with a ﬁlled circular aperture, in the presence of defocus. We order our basis of LP modes as (LP01, LP02, rest of the LP modes) and our output ports as (central port, rest of the ports). For simplicity, we also assume a reference phase 𝝓0 = 0. Due to symmetry, both an unaberrated wavefront and a defocused wavefront will only couple into LP01 and LP02. Furthermore, the coupling coeﬃcients will be real. Therefore, we can set where 𝑎, 𝑏, 𝑐, 𝑑 are real numbers and 𝒛 is the vector corresponding to the defocus mode. Denoting the columns of the lantern propagation matrix 𝑈 as 𝒄𝑖, we ﬁnd that equation 27 becomes We want 𝑄 to be “as imaginary as possible.” Clearly, the ﬁrst and last terms are real, so a lantern that satisﬁes 𝑎𝑑|𝒄1|2 + 𝑏 𝑓 |𝒄2|2 = 0 (33) will behave “more linearly" than one that doesn’t. The middle terms apply another condition: the each element in 𝒄1 should be 90◦ out of phase with its corresponding element in 𝒄2. In turn, this condition implies that the LP01 and LP02 components for each lantern mode must be 90◦ out of phase. We have veriﬁed this behavior numerically. It is also useful to consider the converse of the above conclusion. Suppose that the LP01 and LP02 mode coeﬃcients are in phase. Then, 𝒄1 ⊙ 𝒄∗ 2 will be real, and 𝑄 will be purely real. Consequently, the linear 𝐵 matrix will be 0 - lantern response is locally quadratic. where 𝐴 is the complex-valued transfer matrix of the overall optical system. Modifying 𝐴𝑖 𝑗 → 𝐴𝑖 𝑗𝑒𝑖𝜙0, 𝑗 and combining the above two equations, keeping only terms up to third order, yields where 𝒑out, quad is the quadratic approximation for output intensity, as per equation 12. We now expand the above model to an arbitrary modal basis. Let 𝑅 be a change-of-basis matrix, such that 𝚫𝝓 = 𝑅𝒂. The additional terms from the cubic expansion can be expressed as a single tensor multiplication of the form ∑︁ The 𝐷′ tensor has dimensions 𝑁 × 𝑀 × 𝑀 × 𝑀 for an 𝑁-port lantern sensing 𝑀 aberration modes. The full cubic model, in modal basis, is Brief empirical testing with this model shows that it can provide a signiﬁcant increase in reconstruction accuracy, especially for PLs that have already been optimized for linearity. Heatmaps of reconstruction error against total RMS WFE for 10,000 randomly sampled aberrations are shown in Figure 6g, h, and i, for various 6-port lantern designs. Notably, going to higher order consistently extends the reconstruction range of the sensor, suggesting that the main downside of going to a higher-order model is additional computational complexity rather than numerical instability, at least for the ﬁrst few orders. 2. C. Marois, D. Lafreniere, R. Doyon, B. Macintosh, and D. Nadeau, “Angular diﬀerential imaging: A powerful high-contrast imaging technique,” The Astrophys. J. 641, 556–564 (2006).
Soft vortex matter in a type-I/type-II superconducting bilayer<|sep|>We presented novel and rich vortex phases and phase transitions in a type-I/type-II superconducting bilayer, resembling known phenomena in soft matter physics. The solution-gel-glass-crystal transitions of vortex matter can be induced in our system by an external magnetic ﬁeld, current or temperature, but can be also engineered by a proper choice of the constituent materials, thinning the type-I layer to eﬀective type-II behavior, or changing the spacer material to inﬂuence the coupling strength. The proposed superconducting system is in many ways peculiar and diﬀerent from any soft matter system, but similarities arise from the competing interactions with diﬀerent length scales - present in both systems. Our superconducting system is controllable, relatively easy to fabricate and allows for convenient vortex imaging or detection of transitions between phases using neutron scattering or calorimetric measurements. Moreover, this system opens a further research direction, leaning upon the early discovery of Giaver that it is possible to make a DC transformer by using applied current in one superconductor to drag vortices through another and induce voltage there.25 The inability to ad hoc predict what would happen to soft vortex matter phases in that case, as well as the links to related studies of Coulomb drag in semiconductor heterostructures26 and bilayer graphene27, make our system a very interesting testbed for a plethora of new phenomena. This work was supported by the Flemish Science Foundation (FWO-Vl). Insightful discussions with Arkady Shanenko and Edith Cristina Euan Diaz are gratefully acknowledged.
Quantum-Reduced Loop Gravity: Cosmology<|sep|>We provided a new framework for the cosmological implementation of LQG. This new formulation was aimed to realize a quantum description for an inhomogeneous extension of the Bianchi I model, in which a residual diﬀeomorphisms invariance held and there was space left to regularize the scalar constraint as in full LQG [6]. We outlined how the implementation of a quantization scheme in reduced phase-space was not ﬁt for this purpose. This fact was due to the presence of three independent U(1) gauge symmetries (denoted by U(1)i), each one acting on the integral curves of ﬁducial vector ﬁelds ωi = ∂i. The space of invariant states under U(1)i transformations was made by elements whose U(1)i quantum numbers were preserved along each curve. The issue of this approach was that such a space is not closed under the action of the scalar constraint, regularized as in [6]. Henceforth, our new framework has been deﬁned by reversing the order of “reduction” and “quantization”, which means that we projected the kinematical Hilbert space of LQG down to a reduced Hilbert space which captured the degrees of freedom of the extended Bianchi I model. This was done by restricting admissible edges to those ones parallel to ﬁducial vectors only and by implementing a gauge-ﬁxing procedure for the internal SU(2) symmetry. The former implied that the full diﬀeomorphisms group was reduced to a proper subgroup, while the latter constituted the most technical part of our analysis. We found the solutions of the gauge-ﬁxing condition by lifting U(1)i networks to SU(2) ones. This way, we could reconstruct the quantum states describing the extended Bianchi I model out of functions of SU(2) group elements. This feature allowed us to investigate the implications of the original SU(2) invariance, so getting that non-trivial intertwiners mapping each other diﬀerent U(1)i representation. This is the paramount result of our analysis which marked the diﬀerence with the reduced quantization scheme. In fact, a true 3-dimensional vertex structure could be realized also in the reduced model. The main consequence was that one could implement the action of the scalar constraint in the reduced model as in full LQG, the only diﬀerence being that the triangulation of the spatial manifold had to be replaced by a cubulation. At the same time, the presence of reduced diﬀeomorphisms allowed to develop certain knot classes over which the scalar constraint could also be consistently regularized. Furthermore, since the volume operator was diagonal, the matrix elements of the scalar constraint can be explicitly computed. For instance, we presented the calculation for a 3-valent vertex structure. The analysis of the action of the scalar constraint on the 6-valence vertex and the dynamical implications of the extended Bianchi I model will be the subject of forthcoming investigations. These developments are expected to be highly non-trivial, because the presence of the reduced intertwiners correlates the spin quantum number along diﬀerent directions already on a kinematical level. In this respect, the construction of a proper semiclassical limit, in which the classical Bianchi I model is inferred, constitutes a tantalizing perspective for testing the proposed quantization procedure. The success of this analysis would qualify such a scheme as a well-deﬁned quantum picture describing the early Universe in terms of a discrete geometry, so opening the way to several phenomenological applications. Moreover, it is envisaged for the ﬁrst time the possibility to test the viability of the techniques developed in LQG (implementation of the scalar constraint [7, 54, 55], development of the semiclassical limit) in a simpliﬁed scenario in which the obstructions of the full theory can be overcome. This analysis constitutes the ﬁrst realization of Quantum-reduced Loop Gravity. We applied this framework to the inhomogeneous Bianchi I model, but nothing seems to prevent us for considering other symmetric sectors of the full theory, so increasing the relevance of the proposed procedure and the amount of phenomenological implications which can be extracted. The authors wish to thank T.Thiemann and K.Giesel for useful discussions. The work of F.C. was supported by funds provided by “Angelo Della Riccia” foundation and by the National Science Center under the agreement DEC2011/02/A/ST2/00294. The work of E.A. was partially supported by the grant of Polish Narodowe Centrum Nauki nr 2011/02/A/ST2/00300.
A Review of Verbal and Non-Verbal Human-Robot Interactive Communication<|sep|>An overview of research in human-robot interactive communication was presented, covering verbal as well as non-verbal aspects. Following a historical introduction reaching from roots in antiquity to well into the ninetees, and motivation towards ﬂuid human-robot communication, ten desiderata were proposed, which provided an organizational axis both of recent as well as of future research on human-robot communication. Then, the ten desiderata were explained, relevant research was examined in detail, culminating to a unifying discussion. In conclusion, although almost twenty-ﬁve years in human-robot interactive communication exist, and signiﬁcant progress has been achieved in many fronts, many sub-problems towards ﬂuid verbal and non-verbal human-robot communication remain yet unsolved, and present highly promising and exciting avenues towards research in the near future.
An unbiased estimate for the mean of a {0,1} random variable with relative error distribution independent of the mean<|sep|>A new algorithm for estimating the mean of [0, 1] variables is given with the remarkable property that the relative error in the estimate has a distribution independent of the quantity to be estimated. The estimate is unbiased. To obtain an estimate which has absolute relative error ǫ with probability at least 1 − δ requires at most 2ǫ−2(1 − (14/3)ǫ)−1p−1 ln(2δ−1) samples. The factor of 2 is an improvement over the factor of 4(e − 2) in [3]. Informal Central Limit Theorem arguments indicate that this factor of 2 in the running time is the best possible. The provable lower bound on the constant is improved from the (1/4)e−2 ≈ 0.0338 of [3] to 1/5 for {0, 1} random variables.
A Mixture of Expert Based Deep Neural Network for Improved ASR<|sep|>This paper investigates a novel architecture for acoustic modeling using Mixture of Experts in the context of automatic speech recognition. Experiments conducted on a large vocabulary ASR task show that MoE helps to reduce the overlap in distributions of different acoustic classes, which leads to signiﬁcant reduction in WER compared to strong baselines, namely, DNN and LSTM. The scatter diagram veriﬁes that the MoE features indeed have higher inter-class separation than the baseline features, which is translated to lower WER compared to baselines. Future direction includes noise robustness of MixNet in far-ﬁeld scenario. It will also be interesting to investigate the method in case of lower frame rate (LFR) [24] acoustic models.
Entanglement between two subsystems, the Wigner semicircle and extreme value statistics<|sep|>The function (i, j) �→ (˜i, ˜j) = (g(i, j), g(j, i)) is bijective and is its own inverse, since perform Lemma 3 If (i, j) �→ (g(i, j), g(j, i)) and (i′, j′) �→ (g(i′, j′), g(j′, i′)), and j ̸= j′ (i ̸= i′), where ˜i1 = g(i, j), ˜j1 = g(j, i), ˜j2 = g(j, k), ˜k2 = g(k, j), ˜k3 = g(k, i) and ˜i3 = g(i, k). m ̸= n ̸= p, m = n ̸= p (which is the same as m ̸= n = p and m ̸= p = n) and m = n = p. In unequal quadratic terms i.e. ˜i1 = ˜j1, ˜j2 = ˜k2 and ˜k3 = ˜i3, which implies that g(i, j) = g(j, i), g(j, k) = g(k, j) and g(k, i) = g(i, k). As a consequence of the Lemma 1 above, one then former possibility, ˜k3 = ˜i3, ˜i1 = ˜k2, ˜j1 = ˜j2 and ˜i1 ̸= ˜j1 i.e. g(k, i) = g(i, k), g(i, j) = g(k, j), g(j, i) = g(j, k) and g(i, j) ̸= g(j, i). Again using the Lemma 1, this implies that i = k ̸= j. In the later case, ˜k3 = ˜i3 and ˜i1 = ˜j1 = ˜j2 = ˜k2 i.e. g(k, i) = g(i, k) and g(i, j) = g(j, i) = g(j, k) = g(k, j) which implies i = k = j. Thus there are N non-vanishing terms of the form 1. One sextic term. This occurs when ˜i1 = ˜j1 = ˜j2 = ˜k2 = ˜k3 = ˜i3, that is g(i, j) = g(j, i) = g(j, k) = g(k, j) = g(k, i) = g(i, k). This in turn implies that i = j = k. Thus
On Exploiting Transaction Concurrency To Speed Up Blockchains<|sep|>We have presented our analysis of how much concurrency is available in existing blockchains. We have considered two concurrency metrics: the single-transaction conﬂict rate, and the group conﬂict rate. We have examined historical data from seven public blockchains, and discussed several ﬁndings. One ﬁnding is that there is more concurrency in UTXO-based blockchains than in account-based ones, although the amount of concurrency in the former is lower than expected. Another is that some blockchains with larger blocks have more concurrency than blockchains with small blocks. Finally, we have proposed an analytical model for estimating execution speedup given an amount of concurrency. The model estimates up to 6× speed-ups in Ethereum using 8 cores. Our work provides insights into a largely unexplored avenue for increasing blockchain performance. However, it has several limitations that we plan to address in future work. One major limitation is that we have not designed and implemented an execution engine that can exploit the available concurrency. The main challenge is to minimize overhead in building the TDG and in scheduling concurrent execution. Another limitation is that we only focused on inter-transaction concurrency at block level, which leaves other sources of concurrency such as intratransaction, inter-block and inter-blockchain unexplored. Exploiting multiple sources is likely to bring more performance gains. This research/project is supported by the National Research Foundation (NRF), Prime Minister’s Ofﬁce, Singapore, under its National Cybersecurity R&D Programme and administered by the National Satellite of Excellence in Design Science and Technology for Secure Critical Infrastructure, Award No. NSoE DeST-SCI2019-0009.
Generating Gender Augmented Data for NLP<|sep|>In this paper, we describe an initial approach towards enriching short conversational sentences with their gender variants. Unlike other related work, our approach is not limited to tackling the ﬁrst person singular phenomena, swapping third person pronouns or merely dealing with occupa tional or generally animate nouns. In addition, with our approach, the reliance on linguistic knowledge and tools is kept to a minimum in order to facilitate real-world deployment. The main hurdle for this type of research is the absence of large training sets. Although provided with some manually annotated data from the industry partner, the data provided was far from sufﬁcient to train a state-of-the-art automatic gender re-writer. Therefore, training data was extracted from OpenSubtitles using linguistic knowledge about the targeted language, namely Spanish. Re-genderable types of words (POS classes) were identiﬁed and then frequently occurring ‘re-genderable’ as well as ’neutral’ POS patterns were extracted. By applying the corresponding rules to the re-genderable sentences, a large gender-parallel Spanish data set was compiled. Next, an NMT rewriter was trained in order to ‘translate’ each re-genderable sentence into its gender alternative which showed promising performance both in terms of automatic as well as of manual evaluation. In addition, it is shown that providing additional information regarding the need for rewriting in the form of tags could be helpful for the NMT system, as similar tags have shown to be useful for other applications such as multilingual translation, controlling politeness and gender in MT, etc. While gold standard labels show better performance than the labels generated by the gender classiﬁer, the classiﬁer shows promising results given the very small training set. Further experiments should investigate a classiﬁer trained on larger amount of data. In future work, we would like to explore how a similar approach can be applied on more sentence structures in Spanish, as well as for different languages which exhibit distinct gendering rules. Furthermore, different NMT architectures, e.g. character-level NMT or an NMT system with linguistically motivated subword units could be an interesting extension to the conducted experiments, given that gender is usually marked by speciﬁc morphemes (usually not more than one or two speciﬁc characters). In addition to that, the performance of the gender classiﬁer can be improved to produce more accurate tags by using larger annotated training sets, adding more morphological information in features and using word embeddings instead of
Post-selection inference for e-value based confidence intervals<|sep|>A.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 A.2 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.3 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 A.4 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.1 Benjamini-Hochberg (BH) is a special case of BY . . . . . . . . . . . . . . . . . . . . 29 B.2 e-BH is a special case of e-BY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 B.3 Controlling the directional false discovery rate dFDR . . . . . . . . . . . . . . . . . . 31
ODMTCNet: An Interpretable Multi-view Deep Neural Network Architecture for Image Feature Representation<|sep|>In this paper, an optimal discriminant multi-view tensor convolutional network (ODMTCNet) is proposed for deep level multi-view feature representation. By integrating the SML principles with DNN architecture, the parameters of convolutional layers are determined by an analytical solution instead of the traditional BP algorithm. Hence, the proposed ODMTCNet forms a foundational platform for an interpretable machine learning model which generates feature representation of high quality from original multi-view data sources. Furthermore, the representation effectively addresses the overﬁtting problem when there is no sufﬁcient data available in training. The desirable properties of ODMTCNet are validated by numerous application examples with image data sets of different scales. To conclude this paper, it is worth noting that the number of convolutional layers corresponding to the optimal performance in ODMTCNet is smaller than or equals to 3 in all the four evaluation examples, approaching the bound predicted in Kolmogorov Superposition Theorem [93-94] in neural network-based representation.
Resonant Atom Traps for Electromagnetic Waves<|sep|>This paper demonstrates that there exist a variety of trapped solutions for  electromagnetic fields in a medium where resonant atoms are present. The total reflection of  electromagnetic waves by the medium is a necessary condition for their appearance. In  addition to total reflection, the nonlinearity of atom-wave interaction is responsible for the  phenomenon. Namely, for large intensities of the electromagnetic field, the “source term”  (see RHS of Equations (15) and (20)) saturates and the medium becomes transparent to the  wave. The region of “transparency” forms the core of the trapped mode. At the edges the  field vanishes and the medium reflects the wave completely, thus confining the localized  state with high electromagnetic energy density. The trapped modes, in principle, may have zero loss and accumulate enormous field  energy. It may be possible to use trapped modes in the same way as RF cavities or wave  guides are used, but with larger achievable fields and with potentially zero energy loss. In The author thanks A. Aleksandrov, J. Holmes, and A. Shishlo for useful discussions and  help with manuscript proofreading. Research sponsored by UT-Batelle, LLC, under contract  no. DE-AC05-00OR22725 for the U.S. Department of Energy. APPENDIX A. TRAPPED MODES IN RANDOM 1D LATTICE  Localized states also exist in random lattices. In addition to mode trapping, which is related  to nonlinearity of field-atom interaction in the semiclassical approach, the randomness adds  one more effect, known as “localization” in solid state physics [2]. We do not present a  complete review the phenomenon here. However, we do point out one mathematical  theorem by Furstenberg [5], which states that for any product of a large number of random  noncommuting 1D (2×2) matrices with unit determinant, one of the eigenvalues of the  resulting matrix will grow exponentially with the number of multiplication terms (the other  eigenvalue is, of course, the inverse of the growing one). In application to our problem, a 1D  random lattice of atoms will always yield exponentially growing (decaying) fields at infinity.  Following Poincare (see, e.g. [4]), we define the sequence of points that decay exponentially  at –∞ as the H- curve, and the sequence of points, decaying at +∞, as H+, respectively.   These curves are not trajectories, they are collections of trajectories leaving and returning to  the center of phase space at infinities. In our case we have two H+ and H- curves, each curve  having its symmetric counterpart with respect to 180 degrees rotation of the phase space  around the center.  They are related to each other only by the fact that both have the same  asymptotic growth/decay rate.  The H- curve, in general, grows to substantial values  (probably to infinity) at +∞, as seen in numerical examples, but this growth is slow because  the kick from each atomic node becomes constant for large values of the field (in our units,  large as compared to unity). The H+ curve behaves similarly, at -∞. These curves, in general,  are not the same for periodic systems, with the “small” exception of integrable systems.  When the phase space, where these curves evolve, is confined or the growth is slow, as in  our case, the curves cross each other with some nonzero angle (probably, there exist some  exceptions, but we haven’t seen them in our numerical cases).  The crossing point is our  trapped mode – it grows from zero values at –∞ and decays exponentially at +∞, because it  belongs simultaneously to both “+” and “-“curves. The crossing of curves is confirmed by  all numerical cases performed by the author (that, of course, does not constitute a  mathematical proof). Here, we present one example of a random lattice with all the  described curve and trajectory species, selected randomly, from infinitely many random  cases. For this case, we choose κ=-0.8, 100 random nodes and random phase advances from 0 to  1 radian (chosen as a fractional part of the node number after it is divided by π) between  nodes. The approximate H- line was built in the following way: we selected a very small  initial value of the forward field (see uf  in equation (11)), evolved its growth through 10  nodes and connected first and last point by a line. Thus, we approximated the separatrix, coming emerging from zero for – ∞. We approximated this line by 1000 points placed  uniformly over the line extent and tracked it over 50 nodes. The solid line in Figure 4 shows  the resulting line in the “phase space” of the map, where the x coordinate stands for the  imaginary part, and y – for the real part of the field. This line is an approximate piece of the  H- line. The same was done for the inverse map, starting from +∞, that corresponds to the  100th node, back to the center of the lattice (node 50). The dashed line in Figure 7 represents  part of the H+ curve, obtained in this approximation. One can see that the curves do cross  each other. The crossing point tracked from the 1st to the 100th node is shown as a trajectory in Figure  8 (dashed line). It starts at -2.6*10-6-i4.0*10-6 forward field value at node 100 and ends near  zero at the 1st node (the actual values obtained are at 10-3 level, due to the limited accuracy of  the numerical simulations and the strong influence of errors in the case with exponential  instability). The solid line shows the trajectory of the H- family that passes very close (within  0.01 at node 50) to the crossing point. Due to a slight error in initial conditions and the  exponential instability, this trajectory eventually diverges from that of the trapped mode. FIG. 8. Trapped mode shown by the dashed line. The horizontal coordinate corresponds to  the node number, and the vertical coordinate shows the modulus of the forward wave. The  solid line shows the field behavior for initial conditions 10% different from those of the  dashed line.
Relaxation of electrons in quantum-confined states in Pb/Si(111) thin films from master equation with first-principles-derived rates<|sep|>Simulations of electronic relaxation have been performed for a realistic system, metallic multilayer Pb ﬁlms on Si(111), with the help of a parameter-free approach based on density functional theory. Electron-electron (e-e) and electron-phonon (e-ph) scattering were both included in the master equation. Not surprisingly, e-e scattering was found to dominate over e-ph scattering for short times and highly excited electrons more than 0.5 eV above the Fermi level. Our simulation results thus justify the neglect of e-ph scattering in the analysis of experimental data11 in this regime. The simulated lifetimes of 101 fs and 21 fs for the quantum well states at 0.58 eV and 1.21 eV, respectively, are in reasonable agreement with the experimental ﬁndings. The importance of e-ph scattering shows up in the simulations at lower electron energies where the e-e scattering rate decreases strongly. Tak
Chemical abundance analysis of symbiotic giants. I. RW Hya and SY Mus<|sep|>We have performed a detailed analysis of the photospheric abundances of CNO and elements around the iron peak (Sc, Ti, Fe, and Ni) for the red giant components of RW Hya and SY Mus. Our analysis reveals a signiﬁcantly sub-solar metallicity ([Fe/H]∼-0.75) for the RW Hya giant, conﬁrming its membership in the Galactic halo population, and a near-solar metallicity in SY Mus. The very low 12C/13C isotopic ratios, ∼6–10, derived for both objects and their N enrichment indicate that the giants have experienced the ﬁrst dredge-up.
Investigation of the Spatially Dependent Charge Collection Probability in CuInS$_2$/ZnO Colloidal Nanocrystal Solar Cells<|sep|>In summary, we have investigated Cd- and Pb-free CuInS2/ZnO NC solar cells with the structure ITO/ PEDOT:PSS/CuInS2 NCs/ZnO NCs/Al. The ZnO layer was found to have a double function, both as constituent of the heterojunction and as optical spacer. Thus, varying the ZnO thickness enabled us to modulate the excess carrier generation proﬁles, which strongly aﬀected the photocurrent, as well as spectral shape of the EQE. To understand these variations in more detail, we investigated the IQE under careful consideration of optical cavity eﬀects and the parasitic absorption within the nonactive layers. Most notably, the IQE was found to exhibit a pronounced dependence on the excitation wavelength, which could be translated into a spatial dependence of the charge collection probability. With the help of modeled photon absorption proﬁles provided by TMM simulations, and making simple assumptions on the charge collection probability as a func tion of the position within the active layer, we reconstructed the measured spectral dependencies of the EQE. Using this optical approach, which does not require additional assumptions on electrical properties of the involved materials, we were able to identify pronounced dead zones for charge collection in the vicinity of the PEDOT:PSS interface. We interpret this as a hint that our CuInS2/ZnO devices are not fully depleted, with the result that excess carriers generated beyond the depletion region are not able to contribute to the external photocurrent because of the short minority carrier diﬀusion length. Moreover, even within the collection zone, the collection probability was found to be restricted to ∼50% under short-circuit conditions. Altogether, these deﬁciencies in charge collection provide a reasonable explanation for the limited device performance up to now and highlight the starting points for future improvements. Finally, from a more general point of view, we would like to emphasize that the methods presented here to study the spatial dependence of charge collection are expected to be easily transferable to other absorber systems and kind of photovoltaic devices. Pyramidally shaped wurtzite-type CuInS2 NCs with an edge length ranging from 12 to 18 nm were synthesized using a hot-injection technique, as described previously.23,25 Brieﬂy, a 1:1 (v/v) mixture of 1dodecanethiol and tert-dodecanethiol was rapidly injected into a reaction vessel containing copper acetate, indium(III) acetate, trioctylphosphine oxide, and oleylamine at 220 ◦C under argon atmosphere. The mixture was then heated up to 250 ◦C, and the reaction was stopped after 1 h by cooling to room temperature. Subsequently, the as-obtained particles were precipitated and washed with ethanol to remove residuals from the synthesis. To exchange the long-chained ligands used in the synthesis, the NCs were redissolved in 1-hexanethiol and heated under permanent stirring for 24 h at 100 ◦C. Afterward, the particles were precipitated with ethanol and re-dispersed in chlorobenzene. Quasi-spherically shaped ZnO NCs with average diameter of ∼5 nm were synthesized from zinc acetate dihydrate and potassium hydroxide, according to previous reports.51,52 The as-obtained particles were ﬁltered through a 0.2 µm syringe ﬁlter and dissolved in a 9:1 (v:v) mixture of chloroform and methanol. CuInS2/ZnO solar cells were fabricated on cleaned and patterned ITO-coated glass substrates (Pr¨azisions Glas & Optik GmbH, Germany; sheet resistance ≤10 Ω/sq) covered with PEDOT:PSS (Clevios P VP AI 4083), as described previously.25 The CuInS2 layer was deposited in a nitrogen ﬁlled glove box by spin coating a 60 mg/ml dispersion at 600 min−1, followed by a drying step at 3000 min−1. Subsequently, the ZnO layer was spincoated at 1500 min−1. Diﬀerent ZnO layer thicknesses of ∼25, ∼60, ∼95, and ∼130 nm were obtained by using dispersions with nominal concentrations of 7.5, 15, 22.5, and 30 mg/ml. Finally, the 120 nm thick Al electrode was thermally evaporated under high vacuum (10−6 mbar). For the thickness variation of the CuInS2 layer, the concentration of the CuInS2 NC dispersion was varied between 17.5 and 120 mg/ml. The samples were annealed at 180 ◦C both after deposition of the CuInS2 and the ZnO layer. The active area (∼14 mm2) was delimited by the geometric overlap of the ITO and metal electrode and precisely measured for each individual device using a stereoscopic microscope equipped with a digital camera (Cascade Microtech EPS150COAX). For the cross-sectional TEM image, a lamella was prepared with a focused ion beam system (FEI Helios NanoLab 600i) and inspected by high-resolution TEM (Jeol JEM-2100F). Film thicknesses were determined using a stylus proﬁler (Veeco Dektak 6M). J–V characteristics were recorded under ambient conditions with a semiconductor characterization system (Keithley 4200). The samples were illuminated using a class AAA simulator (Photo Emission Tech.), providing a simulated AM1.5G spectrum. The light intensity was adjusted to 100 mW/cm2 using a calibrated silicon solar cell. The spectral mismatch factor was 1.037 (not taken into account). Prior to the J–V measurements, the samples were subjected to continuous illumination for ∼10 min (light-soaking). EQE measurements were performed using a custom-built setup (Bentham PVE300), equipped with a dual xenon/quartz halogen lamp and a Czerny–Turner monochromator (Bentham TMc300) providing the monochromatic probe light. The light source was modulated at a frequency of 230 Hz by an optical chopper, and the photocurrent was recorded under shortcircuit conditions with a lock-in ampliﬁer (Stanford Research Systems SR830). During the EQE measurements, the samples were continuously biased by an additional white light xenon lamp with an intensity adjusted to ∼100 mW/cm2. Using this white light bias source, the samples were light-soaked prior to the measurements until saturation of the photocurrent was reached. The EQE setup, additionally equipped with a 150 mm integrating sphere (Bentham DTR6), was also used to measure the spectral reﬂectance of complete device stacks at an incidence angle of 8◦. A light trap was used to distinguish between specular and diﬀuse reﬂectance. Because our device showed only little diﬀuse reﬂection (≤ 3%), we used a highly specular reﬂecting silicon wafer with known op tical constants as calibration standard for the reﬂectance measurements. Transmittance through the devices was found to be negligible at all wavelengths. VASE measurements were carried out in ambient air on a rotating analyzer ellipsometer with auto retarder (J. A. Woollam) under at least 3 angles of incidence and in a wavelength range between 280 and 1700 nm (5 nm step size). Either silicon wafers or ﬂoat glass with known optical properties were used as substrate. The optical constants (i.e., n(λ) and κ(λ)) were determined by means of modeling the ellipsometric data by Kramers–Kronig consistent dispersion models using the supplied software WVASE32 (J. A. Woollam). Details regarding the measurements and data modeling can be found in the Supplemental Material. In case of aluminum, the optical constants could not be determined by ellipsometry because of the oxide layer formation on the surface in ambient air. Therefore, we used literature values from Ref. 64 instead, which were validated with the spectral reﬂectance of glass/Al samples measured from the protected glass side (see Supplemental Material, Figure S5). The optical electric ﬁeld distribution E(λ, x) within the devices (normalized to the incoming electric ﬁeld) was simulated by the help of an one-dimensional TMM model, based on a MATLAB algorithm developed by Burkhard and Hoke.43 Therefore, each of the material layers was parameterized by its thickness and optical constants (n and κ). On the basis of E(λ, x), other device characteristics such as the spectral reﬂectance R(λ), the photon absorption proﬁle A(λ, x) within the active layer, as well as the parasitic absorption PA(λ) within the non-active layers was calculated. Photocurrent densities were estimated by jph = −q � d 0 G(x) dx, where q, d, and G(x) represent the elementary charge, the active layer thickness, and the charge generation rate, respectively. G(x) was calculated from A(λ, x) and the AM1.5G spectral irradiance distribution. The authors thank Janet Neerken for practical support, Michael Richter for assistance with preliminary work on the optical simulations, and Jan Keller for help with the TEM lamella preparation. Financial support from the EWE-Nachwuchsgruppe “D¨unnschichtphotovoltaik” by the EWE AG, Oldenburg, Germany, is gratefully acknowledged. 1 I. J. Kramer and E. H. Sargent, ACS Nano 5, 8506 (2011). 2 M. Gr¨atzel, R. A. J. Janssen, D. B. Mitzi, and E. H. Sargent, Nature 488, 304 (2012). 3 E. H. Sargent, Nat. Photonics 6, 133 (2012). 4 H. Borchert, Solar Cells Based on Colloidal Nanocrystals, Springer Series in Materials Science, Vol. 196 (Springer: Cham, Switzerland, 2014). 5 R. J. Ellingson, M. C. Beard, J. C. Johnson, P. Yu, O. I. Micic, A. J. Nozik, A. Shabaev, and A. L. Efros, Nano Lett. 5, 865 (2005). 6 O. E. Semonin, J. M. Luther, S. Choi, H.-Y. Chen, J. Gao, A. J. Nozik, and M. C. Beard, Science 334, 1530 (2011). 7 I. J. Kramer and E. H. Sargent, Chem. Rev. 114, 863 (2014). 8 M. R. Kim and D. Ma, J. Phys. Chem. Lett. 6, 85 (2015). 9 I. Moreels, K. Lambert, D. Smeets, D. De Muynck, T. Nollet, J. C. Martins, F. Vanhaecke, A. Vantomme, C. Delerue, G. Allan, and Z. Hens, ACS Nano 3, 3023 (2009). 10 C.-H. M. Chuang, P. R. Brown, V. Bulovi´c, and M. G. Bawendi, Nat. Mater. 13, 796 (2014). 11 A. J. Labelle, S. M. Thon, S. Masala, M. M. Adachi, H. Dong, M. Farahani, A. H. Ip, A. Fratalocchi, and E. H. Sargent, Nano Lett. 15, 1101 (2015). 12 Y. Wu, C. Wadia, W. Ma, B. Sadtler, and A. P. Alivisatos, Nano Lett. 8, 2551 (2008). 13 D. Aldakov, A. Lefran¸coisa, and P. Reiss, J. Mater. Chem. C 1, 3756 (2013). 14 Q. Guo, H. W. Hillhouse, and R. Agrawal, J. Am. Chem. Soc. 131, 11672 (2009). 15 Q. Guo, G. M. Ford, W.-C. Yang, B. C. Walker, E. A. Stach, H. W. Hillhouse, and R. Agrawal, J. Am. Chem. Soc. 132, 17384 (2010). 16 S. Suehiro, K. Horita, K. Kumamoto, M. Yuasa, T. Tanaka, K. Fujita, K. Shimanoe, and T. Kida, J. Phys. Chem. C. 118, 804 (2014). 17 C. Steinhagen, M. G. Panthani, V. Akhavan, B. Goodfellow, B. Koo, and B. A. Korgel, J. Am. Chem. Soc. 131, 12554 (2009). 18 Q. Guo, S. J. Kim, M. Kar, W. N. Shafarman, R. W. Birkmire, E. A. Stach, R. Agrawal, and H. W. Hillhouse, Nano Lett. 8, 2982 (2008). 19 C. J. Stolle, M. G. Panthani, T. B. Harvey, V. A. Akhavan, and B. A. Korgel, ACS Appl. Mater. Interfaces 4, 2757 (2012). 20 M. G. Panthani, V. Akhavan, B. Goodfellow, J. P. Schmidtke, L. Dunn, A. Dodabalapur, P. F. Barbara, and B. A. Korgel, J. Am. Chem. Soc. 130, 16770 (2008). 21 V. A. Akhavan, M. G. Panthani, B. W. Goodfellow, D. K. Reid, and B. A. Korgel, Opt. Express 18, A411 (2010). 22 V. A. Akhavan, B. W. Goodfellow, M. G. Panthani, D. K. Reid, D. J. Hellebusch, T. Adachi, and B. A. Korgel, Energy Environ. Sci. 3, 1600 (2010). 23 M. Kruszynska, H. Borchert, J. Parisi, and J. KolnyOlesiak, J. Am. Chem. Soc. 132, 15976 (2010). 24 H. Borchert, D. Scheunemann, K. Frevert, F. Witt, A. Klein, and J. Parisi, Z. Phys. Chem. 229, 191 (2015). 25 D. Scheunemann, S. Wilken, J. Parisi, and H. Borchert, Appl. Phys. Lett. 103, 133902 (2013). 26 G. W. Guglietta, B. T. Diroll, E. A. Gaulding, J. L. Fordham, S. Li, C. B. Murray, and J. B. Baxter, ACS Nano 9, 1820 (2015). 27 T. Kirchartz, J. Bisquert, I. Mora-Ser´o, and G. GarciaBelmonte, Phys. Chem. Chem. Phys 17, 4007 (2015). 28 K. S. Jeong, J. Tang, H. Liu, J. Kim, A. W. Schaefer, K. Kemp, L. Levina, X. Wang, S. Hoogland, R. Debnath, L. Brzozowski, E. H. Sargent, and J. B. Asbury, ACS Nano 6, 89 (2012). 29 K. W. Johnston, A. G. Pattantyus-Abraham, J. P. Cliﬀord, S. H. Myrskog, S. Hoogland, H. Shukla, E. J. D. Klem, L. Levina, and E. H. Sargent, Appl. Phys. Lett. 92, 122111 (2008). 30 A. G. Pattantyus-Abraham, I. J. Kramer, A. R. Barkhouse, X. Wang, G. Konstantatos, R. Debnath, L. Levina, I. Raabe, M. K. Nazeeruddin, M. Gr¨atzel, and E. H. Sargent, ACS Nano 4, 3374 (2010). 31 S. M. Willis, C. Cheng, H. E. Assender, and A. A. R. Watt, Nano Lett. 12, 1522 (2012). 32 G. I. Koleilat, L. Levina, H. Shukla, S. H. Myrskog, S. Hinds, A. G. Pattantyus-Abraham, and E. H. Sargent, ACS Nano 2, 833 (2008). 33 G. F. A. Dibb, M.-A. Muth, T. Kirchartz, S. Engmann, H. Hoppe, G. Gobsch, M. Thelakkat, N. Blouin, S. Tierney, M. Carrasco-Orozco, J. R. Durrant, and J. Nelson, Sci. Rep. 3, 3335 (2013). 34 D. A. R. Barkhouse, I. J. Kramer, X. Wang, and E. H. Sargent, Opt. Express 18, A451 (2010). 35 I. J. Kramer, L. Levina, R. Debnath, D. Zhitomirsky, and E. H. Sargent, Nano Lett. 11, 3701 (2011). 36 K. W. Kemp, C. T. O. Wong, S. H. Hoogland, and E. H. Sargent, Appl. Phys. Lett. 103, 211101 (2013). 37 D. Bozyigit and V. Wood, J. Mater. Chem. C 2, 3172 (2014). 38 D. A. R. Barkhouse, A. G. Pattantyus-Abraham, L. Levina, and E. H. Sargent, ACS Nano 2, 2356 (2008). 39 J. J. Choi, Y.-F. Lim, M. B. Santiago-Berrios, M. Oh, B.-R. Hyun, L. Sun, A. C. Bartnik, A. Goedhart, G. G. Malliaras, H. D. Abru˜na, F. W. Wise, and T. Hanrath, Nano Lett. 9, 3749 (2009). 40 A. K. Rath, M. Bernechea, L. Martinez, and G. Konstantatos, Adv. Mater. 23, 3712 (2011). 41 L. A. A. Pettersson, L. S. Roman, and O. Ingan¨as, J. Appl. Phys. 86, 487 (1999). 42 P. Peumans, A. Yakimov, and S. R. Forrest, J. Appl. Phys. 93, 3693 (2003). 43 G. F. Burkhard, E. T. Hoke, and M. D. McGehee, Adv. Mater. 22, 3293 (2010). 44 S. Wilken, V. Wilkens, D. Scheunemann, R.-E. Nowak, K. von Maydell, J. Parisi, and H. Borchert, ACS Appl. Mater. Interfaces 7, 287 (2015). 45 J. Y. Kim, H.-H. L. Sun Hee Kim, K. Lee, W. Ma, X. Gong, and A. J. Heeger, Adv. Mater. 18, 572 (2006). 46 J. Gilot, I. Barbu, M. M. Wienk, and R. A. J. Janssen, Appl. Phys. Lett. 91, 113520 (2007). 47 S. H. Park, A. Roy, S. Beaupr´e, S. Cho, N. Coates, J. S. Moon, D. Moses, M. Leclerc, K. Lee, and A. J. Heeger, Nat. Photonics 3, 297 (2009). 48 S. B. Dkhil, D. Duch´e, M. Gaceur, A. K. Thakur, F. B. Aboura, L. Escoubas, J.-J. Simon, A. Guerrero, J. Bisquert, G. Garcia-Belmonte, Q. Bao, M. Fahlman, C. Videlot-Ackermann, O. Margeat, and J. Ackermann, Adv. Energy Mater. 4, 1400805 (2014). 49 G.-H. Kim, B. Walker, H.-B. Kim, J. Y. Kim, E. H. Sargent, J. Park, and J. Y. Kim, Adv. Mater. 26, 3321 (2014). 50 A. Armin, M. Velusamy, P. Wolfer, Y. Zhang, P. L. Burn, P. Meredith, and A. Pivrikas, ACS Photonics 1, 173 (2014). 51 S. Wilken, J. Parisi, and H. Borchert, J. Phys. Chem. C. 118, 19672 (2014). 52 C. Pacholski, A. Kornowski, and H. Weller, Angew. Chem. Int. Ed. 41, 1188 (2002). 53 K.-S. Chen, H.-L. Yip, J.-F. Salinas, Y.-X. Xu, C.-C. Chueh, and A. K.-Y. Jen, Adv. Mater. 26, 3349 (2014). 54 M. Scharber, Nat. Mater. 12, 594 (2013). 55 A. Armin, Y. Zhang, P. L. Burn, P. Meredith, and A. Pivrikas, Nat. Mater. 12, 593 (2013). 56 M. Law, M. C. Beard, S. Choi, J. M. Luther, M. C. Hanna, and A. J. Nozik, Nano Lett. 8, 3904 (2008). 57 G. F. Burkhard, E. T. Hoke, S. R. Scully, and M. D. McGehee, Nano Lett. 9, 4037 (2009). 58 L.-Y. Chang, R. R. Lunt, P. R. Brown, V. Bulovi´c, and M. G. Bawendi, Nano Lett. 13, 994 (2013). 59 J. Jasieniak, B. I. MacDonald, S. E. Watkins, and P. Mulvaney, Nano Lett. 11, 2856 (2011). 60 A. I. Braunstein, Thin Solid Films 37, 181 (1976). 61 S. S. Hegedus and W. N. Shafarman, Prog. Photovolt: Res. Appl. 12, 155 (2004). 62 P. R. Brown, R. R. Lunt, N. Zhao, T. P. Osedach, D. D. Wanger, L.-Y. Chang, M. G. Bawendi, and V. Bulovi´c, Nano Lett. 11, 2955 (2011). 63 D. Bozyigit, S. Volk, O. Yarema, and V. Wood, Nano Lett. 13, 5284 (2013). 64 A. D. Raki´c, A. B. Djuriˇsi´c, J. M. Elazar, and M. L. Majewski, Appl. Opt. 37, 5271 (1998).
A new Hamiltonian for the Topological BF phase with spinor networks<|sep|>In this paper, we have given a very detailed account for the topological phase of the SU(2) BF model. While the continuum formulation is quite well-understood at a formal level (functional determinants in the path integral), a full lattice description which can be explicitly handled was missing. It is important because the BF model is at the core of recent developments concerning topological aspects in condensed matter, quantum information and background independent quantization such as loop quantum gravity. Solutions are given by evaluations of spin networks, producing SU(2) Wigner coeﬃcients. While that was expected, our approach, through the new Hamiltonian, goes deeper than previous works into the structure of the theory, by generating the basic recursions on Wigner 6j-symbols, completing this way the program of [31]. In 2+1 dimensions, the holonomy-ﬂux algebra representation of the new Hamiltonian emphasizes the geometric content of 2+1 gravity. As an outcome, we get a quantization of ﬂat, Euclidean geometry in terms of recursions from group representation theory. We have given several kinds of such equations in the last section. Other equations which turn out to quantize simple geometric properties of ﬂat space have appeared in [48, 50]. As already noticed in [31], a surprising result is that there is not a single time reparametrization as expected from geometrodynamics, but instead diﬀerent directions of evolutions which mix spatial diﬀeomorphisms and time reparametrization. In this topological model, the long-standing issue of the ambiguity on the spin of the Wilson loops used to regularize the curvature in the Hamiltonian gets a precise answer. The fundamental representation is the only one which enables to solve the model for the group SU(2) without having to supplement it with additional initial conditions by hand. The method we have used can be easily applied to other compact Lie groups. The cases of ﬁnite or Abelian Lie groups are even simpler. The point is that the Biedenharn-Elliott identity is a non-trivial relation which is part of the machinery of representation theory. It is always possible to get from it recursion relations like those we studied here, and they will always be generated by our new Hamiltonian. As we have argued, they are better suited than the recursions which come from the plaquette operators considered in [29], though part of the same mathematical framework. Our Hamiltonian can also be used on plaquettes with an arbitrary number of links, generating this way recursion relations which are satisﬁed by generic Wigner symbols. Some of them are known from some kind of Biedenharn-Elliott identity satisﬁed by the 9j-symbol [51], but the generic case is actually new. It would deﬁnitely be interesting to study further those recursions, which certainly have a lot to do with integrability of Wigner coeﬃcients as discussed in [43]. When written with spinors, coherent states quantizations are available which change the recursions to partial derivative equations on holomorphic functions. Some of the most interesting features of those representations are the existence of a closed formula for the generating function of 6j-symbols (132) and the simple equation (146) it satisﬁes. This is a special case of an analysis performed in [73, 74] for generic Wigner coeﬃcients. The spinor variables have also been very useful in recent mathematics [36, 42]. We hope in the future to apply these tools in the context of topological order and the dynamics of loop quantum gravity. A key physical property which we have ignored here is the question of the excitations which violate the topological conditions. In the continuum, the BF model is known to feature exotic statistics [75, 76], which depending on the dimension involve particles, strings and/or branes. The dynamics is presented in [77, 78]. It would certainly be interesting to introduce them in our framework, and to compare with lattice models exhibiting similar phenomena (topological order and exotic statistics involving branes) like [79] where the model is exactly solvable. More generically, we think our model is robust enough to allow the study of theories expanded around the topological BF phase, as proposed in [80] and realized in the continuum, Yang-Mills theory in [19]. The authors also thank Laurent Freidel, who shared with us his ideas to extract the spin 1/2 recursions, which were basically the same as those we have used. Research at Perimeter Institute is supported by the Government of Canada through Industry Canada and by the Province of Ontario through the Ministry of Research and Innovation.
Measurement of the time-dependent CP asymmetry in B0 -> J/{\psi} KS0 decays<|sep|>In a dataset of 1.0 fb−1 collected with the LHCb detector, approximately 8200 ﬂavour tagged decays of B0 → J/ψK0 S are selected to measure the CP observables SJ/ψ K0 S and CJ/ψ K0 S, which are related to the CKM angle β. A ﬁt to the time-dependent decay rates of B0 and B0 decays yields with a statistical correlation coeﬃcient of ρ(SJ/ψ K0 S, CJ/ψ K0 S) = 0.42. This is the ﬁrst signiﬁcant measurement of CP violation in B0 → J/ψK0 S decays at a hadron collider [24]. The measured values are in agreement with previous measurements performed at the B factories [5,6] and with the world averages [7]. We express our gratitude to our colleagues in the CERN accelerator departments for the excellent performance of the LHC. We thank the technical and administrative staﬀ at the LHCb institutes. We acknowledge support from CERN and from the national agencies: CAPES, CNPq, FAPERJ and FINEP (Brazil); NSFC (China); CNRS/IN2P3 and Region Auvergne (France); BMBF, DFG, HGF and MPG (Germany); SFI (Ireland); INFN (Italy); FOM and NWO (The Netherlands); SCSR (Poland); ANCS/IFA (Romania); MinES, Rosatom, RFBR and NRC “Kurchatov Institute” (Russia); MinECo, XuntaGal and GENCAT (Spain); SNSF and SER (Switzerland); NAS Ukraine (Ukraine); STFC (United Kingdom); NSF (USA). We also acknowledge the support received from the ERC under FP7. The Tier1 computing centres are supported by IN2P3 (France), KIT and BMBF (Germany), INFN (Italy), NWO and SURF (The Netherlands), PIC (Spain), GridPP (United Kingdom). We are thankful for the computing resources put at our disposal by Yandex LLC (Russia), as well as to the communities behind the multiple open source software packages that we depend on.
GMRT Radio Halo Survey in galaxy clusters at z = 0.2 -- 0.4. II.The eBCS clusters and analysis of the complete sample<|sep|>The GMRT radio halo survey, carried out at 610 MHz, was designed in order to investigate the link between the presence of diﬀuse cluster scale emission and the dynamics of the hosting clusters in the redshift interval 0.2 – 0.4, where in the framework of the re–acceleration model the bulk of radio halos is expected. To this aim we selected a complete sample of massive clusters with LX(0.1–2.4 keV) > 5 × 1044 erg s−1, and 0.2 < z ≤ 0.4 from the ROSAT-ESO Flux Limited X–ray galaxy cluster catalogue and from the extended ROSAT Brightest Cluster Sample. Declination limits were also imposed (Section 2).
Maintainable Log Datasets for Evaluation of Intrusion Detection Systems<|sep|>In this paper we present a collection of eight synthetic log datasets for evaluation of intrusion detection systems. We collect our datasets from testbeds generated by a model-driven methodology for testbed setup and labeling. This enables to repeat the data collection procedure arbitrary many times while at the same time varying several parameters of the simulation with low manual effort. In addition, it is simple to scale the network and extend it with additional components or services. Our datasets are openly accessible and maintainable as all code required to deploy testbeds, run simulations, and assign labels to log events is available open-source. Our datasets thus solve several problems that are prevalent in existing datasets, including control over the simulation parameters, presence of repeated attack executions in similar environments, generation of ground truth tables, complexity of the network, preprocessing of logs to protect sensitive information, and more. Our log datasets address the common use-case of an attack on the infrastructure of a enterprise IT network. In particular, the attack scenario involves reconnaissance scans, brute-force password cracking, data exﬁltration, as well as utilization of various tools and exploits to eventually obtain system access. To generate a realistic baseline of normal behavior, we simulate user activity by extensive state machines that are speciﬁcally designed to utilize services such as mail platforms and ﬁle shares. We primarily created our dataset to provide diverse attack vectors that challenge many diﬀerent detection techniques, however, we also foresee applications that go beyond IDS evaluation, in particular, alert aggregation and user proﬁling. We see our dataset as the ﬁrst in a series and foresee to extend the labeling rules to more ﬁles and attack steps in upcoming versions. For future work, we plan to extend user simulations to run on Windows hosts and mobile devices, and to create testbeds for new use-cases such as Internet-of-Things. This work was partly funded by the FFG projects INDICAETING (868306) and DECEPT (873980), and the EU projects GUARD (833456) and PANDORA (SI2.835928).
On the Use of Policy Iteration as an Easy Way of Pricing American Options<|sep|>We show that the method of policy iteration, devised in [10] for the solution of discretised HJB equations, is a natural ﬁt to American option pricing. It is extremely simple in structure, and ﬁnite convergence in at most N + 1 steps can easily be proved. Numerical results show that, in practically relevant situations, it performs identically to a penalty scheme and improves over PSOR. The simplicity advantage of the proposed method is especially noticeable in one dimension, where the algorithm is a small modiﬁcation of a tridiagonal linear solver; in this case, the overall complexity for solving the linear complementarity problem is O(N 2) and, if used as part of an instationary American option solver, O(N(N+M)) overall. In higher dimensions, the proposed method is still a direct method in the sense that the basic iteration has ﬁnite termination, but the linear systems required by the algorithm might most suitably be solved by an iterative solver. Finally, we discuss how our approach can be interpreted as the formal limit of a Newton-type iteration applied to a penalised equation, the advantage of the policy iteration being that, in the limit, the penalisation error vanishes. However, it is also noted that policy iteration is inherently ﬁnite-dimensional, and the number of iterations increases for reﬁned meshes, whereas for semi-smooth Newton methods for a zero-order penalty term this is not the case, as there is an underlying continuous iteration which is approximated. A consequence of these two points combined is that although two penalty approximations to the same discrete HJB equation may have theoretically identical properties for ﬁxed ﬁnite dimensions, the inﬁnite dimensional limit is a helpful orientation in that it provides robustness of the method as the grid is reﬁned, a point related to that made in [20]. Acknowledgements. We thank Mike Giles and the two anonymous referees for many helpful comments and advice. We also thank Yves Achdou for bringing reference [23] to our attention.
Space Efficient Breadth-First and Level Traversals of Consistent Global States of Parallel Programs<|sep|>Algorithm 1 can perform the BFS traversal without regenerating the vector clocks for uniﬂow chain partitions. This is particularly beneﬁcial for the computations in which |E| >> n, and hence the O(|E|2) space needed to regenerate the vector clocks is expensive. Observe that any chain partition, including a uniﬂow chain partition, of a computation is only an arrangement of its graph. Hence, we can implement Algorithm 1 without regenerating new vector clocks, and by only ﬁnding the positions of the events in the uniﬂow chain partition. To do so, we assign a unique id to each event, and then place this event id on its corresponding uniﬂow chain. We also store a mapping of original vector clocks against the event ids. The space requirement for our algorithm will reduce to O(nu · n) as we do not regenerate vector clock, and computation of projections can be performed using nu × n space instead of nu × nu space. As a future work, we plan to implement and evaluate this strategy. It is easy to parallelize Algorithm 1, as it traverses cuts of rank r + 1 independently of those of rank r. We can perform a parallel traversal easily using a parallel-for loop at line 3 of Algorithm 1. We intend to implement this parallel approach and compare its performance against parallel traversal algorithms such as Paramount [8]. The ubiquity of multicore and cloud computing has signiﬁcantly increased the degree of parallelism in programs. This change has in turn made veriﬁcation and analysis of large parallel programs even more challenging. For such veriﬁcation and analysis tasks, breadth-ﬁrst-search based traversal of global states of parallel programs is a crucial routine. We have reduced the space complexity of this routine from exponential to quadratic in the size of input computation. This reduction in space complexity allows us to analyze computation with high degree of parallelism with relatively small memory footprint — a task that is practically impossible with traditional BFS implementations.
Weak Models of Distributed Computing, with Connections to Modal Logic<|sep|>10, our main focus was on devising a simulation scheme in which the simulation overhead is only proportional to maximum degree ∆ and running time T—this implies that local algorithms of a stronger model can be simulated with local algorithms in a weaker model. However, in our approach the simulation overhead is large in terms of message size. It is an open question if such a high overhead is necessary. 11, 13, and 17 as simple as possible, we introduced graph problems that were highly contrived. An interesting challenge is to come up with natural graph problems that could be used to prove the same separation results. It should be noted that prior work [13, 62] presents some separation results that use a natural graph problem—leader election. However, leader election is a global problem; it cannot be solved in VVc(1), and hence we cannot use it to separate any of the constant-time versions of the classes. Another challenge is to come up with decision problems that separate the classes. Indeed, it is not known if the separation results hold if we restrict ourselves to decision problems. This work is an extended and revised version of a preliminary conference report [35]. We thank anonymous reviewers for their helpful feedback, and J´er´emie Chalopin, Mika G¨o¨os, and Joel Kaasinen for discussions and comments. This work was supported in part by Academy of Finland (grants 129761, 132380, 132812, and 252018), the research funds of University of Helsinki, and Finnish Cultural Foundation. Part of this work was conducted while Tuomo Lempi¨ainen was with the Department of Information and Computer Science at Aalto University.
Dynamics of a planar thin shell at a Taub-FRW junction<|sep|>We have been able to demonstrate that one can use the Israel thin shell formalism to join together the Taub and FRW spacetimes, and deduce some physical consequences. In stitching the Taub and ﬂat FRW spacetimes, we ﬁrst obtained expressions for the extrinsic curvature of a generic spacelike hypersurface junction joining these two spacetimes. We have shown that, similar to the Oppenheimer-Snyder collapse, one can have a smooth transition at the junction of these spacetimes.
Fuzzy Logic based Autonomous Parking Systems -- Part I: An Integrated Multi-functional System<|sep|>In this paper, an intelligent autonomous parking system is  proposed. The system consists of two sub-systems, Fuzzy-  Based Onboard System (FBOS) and the control center. FBOS
Indirect Rate-Distortion Function of a Binary i.i.d Source<|sep|>This paper studies the indirect rate-distortion problem for a binary i.i.d. source under the Hamming distortion given its noisy observation through a binary symmetric channel. The indirect rate distortion problem is an extension of the rate distortion problem in which the encoder is provided with a noisy observation of the source sequence. We investigate the rate-distortion tradeoff for the simple scenario of a binary source, bit ﬂipping noise and Hamming distortion. Although conceptually simple, this model provides a number of key intuitions on more general models and illustrates important tradeoffs for practical systems. For instance, by deriving the relationship between rate and distortion at each noise level, we make it possible to determine how the sampling error and the communication error probabilities can be balanced in a remote sensor to obtain a desired target end-to-end quality of measurement. In this Appendix we complete the proof of Theorem III.2 by showing the existence of positive w0 and w1 that satisfy (11). We need to show that (24) and (25) are positive for any p < α < 1/2 and r = r⋆. The case where α = 1/2 were treated above and leads to w0 = w1 = 1/2. If p = α, then it follows from Proposition III.1 that RX|Y (D) is deﬁned only for D ≥ α and equals zero. We will therefore assume p ≤ D ≤ α < 1/2. Another way to write (24) and (25) is p − p > 0 in the domain of interest, it can be shown that limr→∞ w0(r) = ¯β and that the derivative of w0(r) is negative for any r > 0. This implies that w0(r) > 0 for all values of r in the domain of interest and in particular at r = r⋆. For w1 we can show that limr→0+ w1(r) = −∞, limr→∞ w1(r) = β and it is monotonically increasing for r > 0. By continuity of w1(r), it follows that there exists r0 > 0 with w1(r0) = 0 such that w1(r) < 0 whenever r < r0 and w1(r) > 0 whenever r > r0. Since we have seen in the proof of Theorem III.2 that g′(r) has similar behavior with a unique root r⋆, we conclude that if g′(r0) < 0, then r⋆ > r0 and then w1(r⋆) > 0. It is therefore enough to show that g′(r0) < 0. Indeed, at r = r0 we have we have that a(r) > g′(r = r0) for all r > 0. In addition, limr→∞ a(r) = −D + p < 0 and which is positive for all r > 0. We conclude that b(r) < a(r) < 0 for all r > 0. This proves the claim. We will show that for all r > 0, the difference between g(r) that corresponds to any p ≤ D ≤ 1/2 and the one that corresponds to α = 1/2 is always positive. This difference can be written as The result follows by noting that limr→∞ δ(r) = 0 and the derivative of δ(r) is strictly positive for any r > 0.
Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs<|sep|>This paper proves a necessary and suﬃcient condition for the existence of iterative approximate consensus algorithm in arbitrary directed graphs. As a special case, our results can also be applied to undirected graphs. We also use the necessary and suﬃcient condition to determine whether such iterative algorithms exist for certain speciﬁc graphs. In our ongoing research, we are exploring extensions of the above results by relaxing some of the assumptions made in this work.
On the Theory of Transfer Learning: The Importance of Task Diversity<|sep|>We present a framework for understanding the generalization abilities of generic models which share a common, underlying representation. In particular, our framework introduces a novel notion of task diversity through which we provide guarantees of a fast convergence rate, decaying with all of the samples for the transfer learning problem. One interesting direction for future consideration is investigating the effects of relaxing the common design and realizability assumptions on the results presented here. We also believe extending the results herein to accommodate “ﬁne-tuning” of learned representations – that is, mildly adapting the learned representation extracted from training tasks to new, related tasks – is an important direction for future work.
Metallicity of high stellar mass galaxies with signs of merger events<|sep|>Observed metallicity gradients in galaxies (e.g., Zaritsky et al. 1994; Ferguson et al. 1998; Dutil & Roy 1999; Consid`ere et al. 2000) show evidence of less enriched gas in the external regions. Thus, a large low-metallicity gas reservoir is expected in most massive star-forming galaxies. On the other hand, numerical simulations indicate that galaxy interactions and mergers may generate a signiﬁcant ﬂow of external gas onto the central region and trigger star formation. Thus, interactions may be eﬃcient in lowering the central metal content of galaxies by means of an inﬂow of this external, less enriched gas. Similar eﬀects have been reported by Michel-Dansac et al. (2008) and Ellison et al. (2008), where interacting pairs with strong morphological disturbance features have lower metallicities. This is also reinforced by the high mass outliers in the MZ relation exhibiting distorted morphologies (Peeples et al. 2009). The above considerations have led us to analyse the presence of possible relics of such process in non-interacting, high-stellarmass galaxies. To this aim, we have performed a morphological analysis of high quality images from the MGC of high-stellarmass SDSS galaxies. Objects in this mass range have a more clearly deﬁned morphology and so are suitable for exploring morphological disturbances associated with merger events. On the other hand, the oxygen abundance is a clear indicator of the gas phase metallicity and so is sensitive to the presence of recently formed stars. By construction, our subsamples of high, medium, and low metallicity objects have similar stellar-mass distributions. Moreover, we have conﬁrmed that the subsamples are free of strong biases widely diﬀerent local galaxy density environments, or widely diﬀerent concentration parameters so that we expect them to have similar bulge-to-disc ratios. Thus, the subsamples analysed comprise galaxies of similar morphology and environment. Our main result is that we detect a steadily increasing fraction of morphologically disturbed galaxies, characteristic of being merger remnants, with decreasing O/H. The three bins anal
Supervised Domain Adaptation using Graph Embedding<|sep|>Domain adaptation methods help achieve better performance on tasks where data is scarce by leveraging larger related datasets to learn good feature representations. In this work, we treat domain adaptation as a dimensionality reduction problem and propose a novel use of Graph Embedding by integrating the trace-ratio objective as a loss in a deep neural network, that is trained end-to-end. Using this Domain Adaptation Graph Embedding framework (DAGE), we test a simple LDA-inspired domain adaptation loss (DAGE-LDA) on standard benchmarking datasets and reevaluate CCSA and dSNE, two the recent state-of-the-art methods, which can be seen as instantiations of DAGE. Under identical experimental conditions, DAGE-LDA matches or beats the overall accuracy of both prior methods, highlighting the treatment of domain adaptation as a multi-view graph embedding problem.
Elliptical Insights: Understanding Statistical Methods through Elliptical Geometry<|sep|>I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the “[Elliptical] Law of Frequency of Error.” The law would have been personiﬁed by the Greeks and deiﬁed, if they had known of it.... It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshaled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along. We have taken the liberty to add the word “Elliptical” to this famous quotation from Galton (1889). His “supreme law of Unreason” referred to univariate distributions of observations tending to the Normal distribution in large samples. We believe he would not take us remiss, and might perhaps welcome us for extending this view to two and more dimensions, where ellipsoids often provide an “unsuspected and most beautiful form of regularity.” In statistical data, theory and graphical methods, one fundamental organizing distinction can be made depending on the dimensionality of the problem. A coarse but useful scale considers the essential deﬁning distinctions to be among: This scale19 at least implicitly organizes much of current statistical teaching, practice and software. But within this classiﬁcation, the data, theory and graphical methods are often treated separately (1D, 2D, nD), without regard to geometric ideas and visualizations that help to unify them. This paper starts from the premise that one geometric form—the ellipsoid—provides a unifying framework for many statistical phenomena, with simple representations in 1D (a line) and 2D (an ellipse) 19This idea, as a unifying classiﬁcation principle for data analysis and graphics, was ﬁrst suggested to the ﬁrst author in seminars by John Hartigan at Princeton, c. 1968. that extend naturally to higher dimensions (an ellipsoid). The intellectual leap in statistical thinking from ONE to TWO in Galton (1886) was enormous. Galton’s visual insights derived from the ellipse quickly led to an understanding of the ellipse as a contour of a bivariate normal surface. From here, the step from TWO to MANY would take another 20–30 years, but it is hard to escape the conclusion that geometric insight from the ellipse to the general ellipsoid in nD played an important role in the development of multivariate statistical methods. In this paper, we have tried to show how ellipsoids can be useful tools for visual statistical thinking, data analysis and pedagogy in a variety of contexts often treated separately and from a univariate perspective. Even in bivariate and multivariate problems, ﬁrst-moment summaries (a 1D regression line or 2 + D regression surface) show only part of the story—that of the expectation of a response y given predictors X. In many cases, the more interesting part of the story concerns the precision of various methods of estimation, which we’ve shown to be easily revealed through data ellipsoids and elliptical conﬁdence regions for parameters. The general relationships among statistical methods, matrix algebra and geometry are not new here. To our knowledge, Dempster (1969) was the ﬁrst to exploit these relationships in a systematic fashion, establishing the connections among abstract vector spaces, algebraic coordinate systems, matrix operations and properties, the dualities between observation space and variable space, and the geometry of ellipses and projections. The roots of these connections go back much further—to Cram´er (1946) (the idea of the concentration ellipsoid), to Pearson (1901) and Hotelling (1933) (principal components), and, we maintain, ultimately to Galton (1886). Throughout this development, elliptical geometry has played a fundamental role, leading to important visual insights. The separate and joint roles of statistical computation and computational graphics should not be underestimated in appreciation of these developments. Dempster’s analysis of the connections among geometry, algebra and statistical methods was fueled by the development and software implementation of algorithms [Gram-Schmidt orthogonalization, Cholesky decomposition, sweep and multistandardize operators from Beaton (1964)] that allowed him to show precisely the translation of theoretical relations from abstract algebra to numbers and from there to graphs and diagrams. Monette (1990) took these ideas several steps further, developing interactive 3D graphics focused on linear models, geometry and ellipsoids, and demonstrating how many statistical properties and results could be understood through the geometry of ellipsoids. Yet, even at this later date, the graphical facilities of readily available statistical software were still rather primitive, and 3D graphics was available only on high-end workstations. Several features of the current discussion may help to present these ideas in a new light. First, the examples we have presented rely heavily on software for statistical graphics developed separately and jointly by all three authors. These have allowed us to create what we hope are compelling illustrations, all statistically and geometrically exact, of the principles and ideas that form the body of the paper. Moreover, these are now general methods, implemented in a variety of R packages, for example, Fox and Weisberg (2011), Friendly (2007b), and a large collection of SAS macros (http://datavis.ca/sasmac), so we hope this paper will contribute to turning the theory we describe into practice. Second, we have illustrated, in a wide variety of contexts, comprising all classical (Gaussian) linear models, multivariate linear models and several extensions, how ellipsoids can contribute substantially to the understanding of statistical relationships, both in data analysis and in pedagogy. One graphical theme underlying a number of our examples is how the simple addition of ellipses to standard 2D graphical displays provides an eﬃcient visual summary of important bivariate statistical quantities (means, variances, correlation, regression slopes, etc.). While ﬁrst-moment visual summaries are now common adjuncts to graphical displays in standard software, often by default, we believe that the second-moment visual summaries of ellipses (and ellipsoids in 3D) now deserve a similar place in statistical practice. Finally, we have illustrated several recent or entirely new visualizations of statistical methods and results, all based on elliptical geometry. HE plots for MANOVA designs [Friendly (2007b)] and their projections into canonical space (Section 5) provide one class of examples where ellipsoids provide simple visual summaries of otherwise complex statistical results. Our analysis of the geometry of added variable-plots suggested the idea of superposing marginal and conditional plots, as in Figure 18, leading to direct visualization of the diﬀerence between marginal and conditional relationships in linear models. The bivariate ridge trace plots described in Sec
Stabilizing Open Quantum Systems by Markovian Reservoir Engineering<|sep|>We have theoretically investigated the convex set of the steady states and the invariant set of the Lindblad master equation, derived several suﬃcient conditions for the existence of a unique steady state, and applied these to diﬀerent physical systems. One interesting result is that if one Lindblad term corresponds to an annihilation operator of the system then the stationary state is unique. Another useful result is that a composite system has a unique steady state if the Lindblad equation contains dissipation terms corresponding to annihilation operators for each subsystem. In both cases the result still holds if other dissipation terms are present, and regardless of the Hamiltonian. We also show that uniqueness implies asymptotic stability of the steady state and hence global attractivity. On the other hand, if there are at least two steady states, then there is a convex set of steady states, none of which are asymptotically stable. Furthermore, in this case even convergence to a steady state is not guaranteed as there can be a larger invariant set surrounding the steady states, corresponding to the center manifold generated by the eigenspaces of the Bloch superoperator A with purely imaginary eigenvalues. The invariant set is closely related to decoherence-free subspaces; in particular any state ρ with support on a DFS belongs to the invariant set. This characterization of the set of steady states and the invariant set, can be used to stabilize desired states using Hamiltonian and reservoir engineering, and we illustate how in principle any state, pure or mixed, can be stabilized this way. This can be extended to engineering decoherence-free subspaces. The latter are naturally attractive but attractivity of a subspace is a weak property in that almost all initial states will generally converge to mixed state trajectories with support on the subspace, not stationary pure states. One possibility of implementing such reservoir engineering is via direct feedback, e.g., by homodyne detection, which yields a feedback-modiﬁed master equation [11] with Lindblad terms depending on the measurement and feedback Hamiltonians. This dependence shows that feedback can change the reservoir operators, and we have shown that in the absence of restrictions on the control, measurement, and feedback operators, any state can be rendered asympotically stable by means of direct feedback. It will be interesting to consider what states can be stabilized, for example, given a restricted set of available measurement, control, and feedback Hamiltonians for speciﬁc physical models. S.G.S. acknowledges funding from EPSRC ARF Grant EP/D07192X/1, the EPSRC QIP Interdisciplinary Research Collaboration (IRC), Hitachi, and NSF Grant PHY05-51164. We gratefully acknowledge Francesco Ticozzi, Howard Wiseman, Heide Narnhofer, Bernhard Baumgartner, Marj Batchelor, and Tsung-Lung Tsai for various helpful discussions and comments. We can use Brouwer’s ﬁxed point theorem and Cantor’s intersection theorem to prove that any dynamical system whose ﬂow is a continuous map φt from a disk Dn to itself, must have a ﬁxed point, and the assumption that the domain is the disk Dn can be relaxed to any simple-connected compact set. Speciﬁcally, we have: Theorem 3. Let ˙x = f(x) be a dynamical system with a ﬂow φt from a simply connected compact set D to itself. If φt is continuous, then there exists a ﬁxed point. Proof. For any given T > 0, φT : D → D is a continuous map from D to itself. Applying Brouwer’s ﬁxed point theorem, there exists at least one ﬁxed point. Denote the set of ﬁxed points as ST and observe that as a closed subset of a compact set ST is compact. Similarly, we can ﬁnd the set of ﬁxed points ST/2 for φT/2, which is also compact and satisﬁes S t 2 ⊂ ST as a ﬁxed point of φT/2 is also a ﬁxed point of φT . By iterating this procedure we can construct a sequence of nonempty compact netting sets {ST/2k} : · · · ⊂ ST/2k ⊂ ST/2k−1 ⊂ · · · S T 2 ⊂ ST . By Cantor intersection theorem, the intersection of {ST/2k} is nonempty. Let x0 be one of the points in the intersection. Then for any T ′ = nT/2k, we have φT ′(x0) = x0. Since such T ′ is dense for [0, +∞) and φt a continuous ﬂow, we know that for any time t, φt(x0) = x0, i.e. x0 is a ﬁxed of the dynamical system. Since the set of physical states is a compact simply connected set and the master equation clearly continuous, we can conclude that any system governed by a Lindblad master equation has a physical stationary state. Lemma 1. If ρs = sρ0 + (1 − s)ρ1 is a convex combination of the positive operators ρ0, ρ1 with 0 < s < 1 Proof. If rank(ρs) = k then there exists a basis such that ρs = diag(r1, . . . , rk, 0, . . .) with rℓ ≥ 0 and �k ℓ=1 rk = 1, i.e., the last N −k rows and columns of ρs are 0. Since ρ0 and ρ1 are positive operators and s > 0, this is possible only if the last N − k rows and columns of ρ0 and ρ1 are zero, and thus the support of ρ0 and ρ1 is contained in the support of ρs. Furthermore, the rank of all ρs on the open line segment 0 < s < 1 must be the same. If there were two intermediate points with rank(ρs) < rank(ρt) and 0 < s < t < 1 then the support of ρ0 and ρt would have to be contained in the support of ρs by the previous argument, which is impossible as rank(ρs) < rank(ρt). Similarly, for 0 < t < s < 1. Theorem 4. Let Hs be the smallest subspace of H that contains the support of all steady states. There exist a ﬁnite number of extremal steady states ρk such that Ess is the convex hull of {ρk} and Hs = ⊕ksupp(ρk). Proof. We know that a convex set is the convex hull of its extremal points but there may be many extremal points with nonorthogonal supports. Thus, what we need to show is that we can always choose a subset of the extremal points with mutually orthogonal supports that generates the entire convex set of steady states. Given two extremal steady states ρ1, ρ2, either supp(ρ1) ⊥ supp(ρ2), or we can ﬁnd another steady state ρ3 with supp(ρ2) ⊂ supp(ρ1) + supp(ρ2) such that supp(ρ1) ⊥ supp(ρ3). Assuming we have already constructed H0 = ⊕k−1 ℓ supp(ρℓ) with supp(ρℓ) mutually orthogonal, let ρk be another extremal point with supp(ρk) not included in H0 and deﬁne H1 = H0 + supp(ρk). By connecting ρk and a ﬁxed point with full rank in H0, we can ﬁnd another steady state with full rank in H1. So H1 is an invariant subspace under the dynamics on H. Therefore, in the following, we will restrict the dynamics H and Vk on the subspace H1. Deﬁne P to be the projection operator of H0 and P ⊥ to be the orthogonal projection operator of P with respect to H1. In the block-diagonal diagonal form with respect to P and P ⊥, where without loss of generality we only consider one Lindblad term. Since H0 is an invariant subspace under the dynamics, we have For a steady state ρk, we have 0 = ˙ρ = −i[H, ρk] + D[V ]ρ. Since ρk is an extremal point, ρ22 has full rank. which means V12 = 0 since ρ22 has full rank. Together with (B1) we have [H, P] = [V, P] = 0. Hence P ⊥H1 is also an invariant space under the dynamics restricted on H1. Combining the condition that H1 is an invariant subspace under the dynamics on H, we conclude that P ⊥H1 is also an invariant subspace under the dynamics on H. There must exist an extremal ﬁxed point ¯ρk in P ⊥H1 with support orthogonal to H0. We have ¯H1 = H0 ⊕ supp(¯ρk). Continuing this process until all ﬁxed points are included in ⊕ksupp(ρk), we ﬁnally obtain Hs = ⊕ksupp(ρk). This construction can be completed in a ﬁnite number of steps as the dimension of Hs is ﬁnite. Suppose A has a pair of purely imaginary eigenvalues ±iα. Let e be an eigenvector of A corresponding to the eigenvalue +iα with α > 0, i.e., Ltot(e) = A e = iαe. In the Schrodinger picture, we have et Ae = eiαte. Let E be the operator, corresponding to e, in the adjoint operator space, with L† tot(E) = iαE. In the Heisenberg picture, the adjoint dynamics gives E(t) = eiαtE, and E(t)†E(t) = E†E, with E†E always positive. We can scale E such that ∥E†E∥∞ = 1. Thus, E†E is a positive matrix with maximum eigenvalue λmax = 1 and |φ0⟩ as the associated eigenvector. Hence, we have E†E|φ0⟩ = |φ0⟩ and Tr(E†Eρ0) = Tr(λmaxρ0) = 1, where ρ0 = |φ0⟩⟨φ0|. Let us consider in the Schrodinger picture, the evolution of ρ(t) with initial state ρ(0) = ρ0. We deﬁne the average state
Differential Cross Section of DP-Elastic Scattering at Intermediate Energies<|sep|>In the paper we have presented a method to calculate the amplitude of the deuteron–proton elastic scattering at intermediate energies. Special attention was given to the questions connected with the relativistic eﬀects. The transformation of the deuteron wave function in the rest frame to a moving system was performed, that allowed us to use the non-relativistic DWF at rather high energies. In order to describe nucleon–nucleon interactions in a wide energy range, we have used parameterization of the NN t-matrix. The spin transformation technique has been also applied to relate this t-matrix given in the c.m. to that in the reference frame. Using this method we have managed to describe the experimental data on the diﬀerential cross section at four energies: 395, 500, 880, and 1200 MeV. Good agreements have been obtained between the experimental data and the theoretical model calculations taking into consideration the one-nucleon-exchange, single- and double-scattering for all the four energies up to the scattering angle of 1400. The distinctions between the data and theoretical predictions at the backward angles should be studied more precisely, that is the subject for further investigations. The author is grateful to Dr. V.P. Ladygin for fruitful discussions. This work has been supported by the Russian Foundation for Basic Research under grant N 0
Sequential Decentralized Parameter Estimation under Randomly Observed Fisher Information<|sep|>Fig. 8. Average stopping time, i.e., E[ ˜T ], vs. the bounding constant of x, i.e., X , for the optimal centralized estimator, and (11). We have E[(ˆxt − x)2|Ht] = Var(ˆxt|Ht) = 1/Ut = 1/Ic t , i.e., efﬁciency. If we have Ut a.s. → To prove the ﬁrst part of the theorem it is sufﬁcient to show that ˜xtI L1 → x, i.e., E[|˜xtI − x|] → 0, since ˜xtI L1 → x implies both ˜xtI p→ x, and E[˜xtI −x] → 0. Since we have E[|˜xtI −x|] = E[|˜xtI −ˆxtI +ˆxtI −x|] ≤ E[|˜xtI − ˆxtI|] + E[|ˆxtI − x|], and E[|ˆxtI − x|] → 0 implied by ˆxtI L2 → x from Lemma 1, we only need to show that E[|˜xtI − ˆxtI|] → 0. Using (7) and (15) we write |˜xtI − ˆxtI| = | ˜VtI −VtI | The quantizer in DMLE is designed so that E � | ˜V k tI − V k tI| � < tIφk σ2 k = Θ(tI) as it is assumed in (A2) that 0 < |hk|2 < ∞, ∀k. Hence, using (30) we write As stated in the proof of Theorem 1, it is sufﬁcient to show that E[|˜xtI − ˆxtI|] → 0. Note from (8) that V k tI = �N k tI n=1 vk n + �tI τ=tk Nk tI ,V +1 2ℜ((yk τ )∗hk τ) σ2 k , and from (19) that ˜V k tI = �N k tI n=1 ˜vk n. Thus, following (30) we where in the second term of the right hand-side we can write ��� �tI τ=tk Nk tI ,V +1 2ℜ((yk τ )∗hk) known that no sampling occurs between the last sampling time, tk N k tI ,V , and the stopping time, tI. Taking where the term outside the parentheses is O(1) as I → ∞ due to (A2). In the ﬁrst term inside the the n-th intersampling period, i.e., {yk τ }, τ ∈ (tk n−1,V , tk n,V ], and thus {|˜vk n − vk n|}n are i.i.d.. Hence, the term �N k tI n=1 |˜vk n − vk n| in (33) is a renewal reward process. Note from (12) and (A2) that tI = Θ(I), i.e., as I → ∞, where E[tk 1,V ] is the average sampling interval of sensor k. Then, it is sufﬁcient to show that as I → ∞. If dk → ∞ such that dk = o(I), i.e., dk = o(tI), ∀k, then both conditions in (35) will be design in LT-DMLE, and E[tk 1,V ] → ∞ as dk → ∞ [cf. (16)] as shown in Appendix D, concluding the dk or −dk, hence its expectation is given by E[vk 1] = (1−αk)(dk +E[qk 1|vk 1 ≥ dk])+αk(−dk −E[qk 1|vk 1 ≤ −dk]) = (1 − 2αk)dk + E[qk 1], where αk ≜ P(vk n ≤ −dk), and qk 1 is the overshoot bounded by E � |ℜ((yk t )∗hk)| � ≤ |x||hk|2 + E � |ℜ(wk 1)| � |ℜ(hk)| + E � |ℑ(wk 1)| � |ℑ(hk)|, (37) E[vk 1] = Θ(dk) and E[tk 1,V ] = Θ(dk). Note that E[|˜vk 1 − vk 1|] = E[|˜qk 1 − qk 1|] < φk
Efficient Data Exchange in Unmanned Aerial Vehicle Networks Utilizing Unsupervised Learning-Based Clustering<|sep|>exchange would be more efﬁcient with less overhead and energy consumption. In this paper, a novel data exchange scheme is developed with two main components, a clustering approach and a data exchange mechanism. The agglomerative hierarchical clustering, a type of unsupervised learning, is employed to design the clustering approach, where UAVs are assigned to different clusters according to their received packets and UAVs that can supply each other’s lost packets will be clustered together. In the designed data exchange mechanism, unlike UAVs carry out data exchange in a random order with tradition methods, like CSMA/CA, the order of UAVs executing data change is optimized according to the number of their lost packets or requested packets that they can provide. By this way, each requestreply process can not only maximize the lost packet retrieval of the UAV sending request, but also help other UAVs in the same cluster retrieve their lost packets. To avoid the undesired situation that UAVs in a cluster do not possess a full set of packets, the optimal number of clusters is studied for UAVs networks with different system parameters. Then, under the optimal number of clusters, simulation studies indicate that our developed data exchange scheme can signiﬁcantly improve the performance on data exchange numbers and delay.
Lagrange formalism of memory circuit elements: classical and quantum formulations<|sep|>To summarize, in this work we introduced the general Lagrangian formulation for the three basic memory elements: memristive, memcapacitive and meminductive systems and deﬁned a fourth memory element, a mutual meminductive system, for which we also gave the Lagrange formalism. We showed how to write the Lagrangian for a general circuit, including one with current sources. The examples given for the Lagrangian formalism demonstrated that writing the Lagrangian and dissipation potential should be the preferred choice for ﬁnding the EOMs of large memory element networks. The Hamiltonian formalism for electric circuits was also generalized to include memory, although only for non-dissipative elements. As in previous works26, we have found that the canonically conjugate momentum of charge is the ﬂux and vice versa. The Generalized Joule’s ﬁrst law was given for general circuits including ones with memory elements. This law can be used to verify the correctness of a given Lagrangian formulation. Lastly, we presented a scheme for the quantization of a general non-dissipative memory element circuit. The quantum treatment of memory elements, and in particular the example given in the text of a memcapacitor in series with an inductor (Fig. 10), begs the question of under which conditions one can measure quantum effects in these systems. For quantum eﬀects to be easily measurable, both the thermal ﬂuctuation energy and the width of the energy levels should be smaller than the oscillator energy quantum26, i.e., kBT ≪ ¯hω0 and Q ≫ 1, where Q = ω0R/L is the quality factor of the oscillator, R the loop resistance, and L is the inductance. Possible values for the capacitance and inductance in mesoscopic circuits can be taken to be 10−15 F42 and 10−10 H43, respectively. If one assumes a temperature of T = 20 mK and circuit resistance of 10 Ω or less, both conditions mentioned above are satisﬁed. As noted for the example above, the degeneracy condition, satisﬁable by a memcapacitor44, will lead to an experimentally detectable splitting of the degenerate energy levels as a result of the interaction between the memory quanta and charge quanta. Future research in this ﬁeld may include extending the Hamiltonian formalism to dissipative circuits. One way to do this is, e.g., via a path-integral formulation45 of memory elements. Along a parallel line, we expect the Lagrangian formalism discussed here to be of great value in the analysis of complex networks with memory, which oﬀer both fundamental and applied research opportunities. This work has been partially funded by the NSF grant No. DMR-0802830. One of us (MD) is grateful to the Scuola Normale Superiore of Pisa for the hospitality during a visit where part of this work has been written, and to S. Pugnetti and R. Fazio for useful discussions.
Asymptotic High Energy Total Cross Sections and Theories with Extra Dimensions<|sep|>We argue on general grounds, and via concrete calculations, that if extra space dimensions are accessible to at least some excitations produced in high energy hadronic collisions above some energy scale Ecritical one should see an increase in the rate at which total cross sections rise. This suggests a general means to search for such extra dimensions by looking at cross sections as a function of energy. Such searches are particularly attractive since they are highly model-independent and largely unaﬀected by single events which might, due to statistical ﬂuctuations, seem to support some model with higher dimensions. Since the data so far apparently saturate the Froissart-Martin bound in 3+1 dimensions, there appears to be no room for any compactiﬁed dimensions whose thresholds are below 100 TeV. This limit can be extended as data from ever higher energy collisions are obtained, but for the near future this will have to come from cosmic ray data. Any events which might seem to be signals of extra spacetime dimensions at the LHC would have to be attributed to some other sort of new physics. Since this paper was written, Block and Halzen [26](who have accepted our work with an aim to extending it) have suggested that higher dimensions might be ruled out to arbitrarily high energies via the same argument, but they use a Froissart bound which is tied to 3+1 dimensions, which as discussed above, is dimensiondependent. Further recent discussions include that of Fagundes, Menon and Silva [27, 28] (see also the commentary by Block and Halzen[29]) who suggest that energy dependence of hadronic total cross section at high energies may be an open problem still). Block and Halzen have also recently argued that the Auger cross section data supports the idea that proton asymptotically develops into a disk[30]. None of these publications changes the conclusions of our paper.
On fits to correlated and auto-correlated data<|sep|>Estimating the covariance matrix C from limited correlated and auto-correlated data, in particular the lower part of its spectrum, often proves to be challenging. Here we investigated a method that bypasses the need for the inverse covariance matrix and is numerically robust against ﬂuctuations of its small eigenvalues. By studying the expansion of the χ2 function about its minimum, we have obtained expressions to compute the expected value of χ2 and the corresponding p-value, for arbitrary positive weight matrices W. The formulae contain the covariance matrix in such a way that the upper part of the spectrum of C predominantly determines the expected χ2 and the p-value. They therefore provide robust statistical tests for the ﬁts. We observe that it is important to use the p-value to discriminate between good and bad ﬁts, not just the reduced χ2. An investigation in a toy model illustrates how the method works with moderate statistics and with auto-correlations present. Finally, we would like to emphasize that correlated ﬁts are the preferred choice, when the correlation of the data is known. The issue is “just” that as yet a robust estimator of C−1 is missing when we have ﬁnite statistics, auto-correlated data. Acknowledgements. RS would like to thank U. Wolﬀ and H. Meyer for sharing their notes on correlated ﬁts. RS also recalls B. Bunk mentioning in the 1980’s that one should estimate the quality of ﬁt for uncorrelated ﬁts from the data. We would like to thank the members of the ALPHA collaboration for useful discussions. In particular we thank A. Ramos for sharing important insights, S. Lottini for collaborating at early stages of the project. MB thanks M. Hansen, C. Kelly, T. Izubuchi and C. Lehner for several useful discussions on the topic. We would also like to thank J. Frison, S. Kuberski and T. Vladikas for comments on earlier versions of the manuscript. The research of MB is funded through the MIUR program for young researchers “Rita Levi Montalcini”. Here we derive the explicit form of the projector and other useful relations. We begin from the minimum condition, which we rewrite as which tells how to propagate the errors from the data to the ﬁtted parameters. More speciﬁcally the covariance matrix of ¯aα reads combined with eq. (A.3) gives us the explicit form of the contribution to the error of ⟨χ2(¯a)⟩ that originates from the dependence of P upon ¯aα, which we neglect. In the derivation of the result above, we have used A reference implementation in MATLAB/Octave and Python is publicly available at this link https://github.com/mbruno46/chiexp. Documentation on the syntax can be consulted online, https://mbruno46.github.io/chiexp, from the git repository, locally after downloading it or by using the corresponding help functions in the MATLAB or Python interactive sessions.
Investigating signatures of cosmological time dilation in duration measures of prompt gamma-ray burst light curves<|sep|>In this work we have investigated whether duration measures of Swift/BAT and Fermi/GBM detected GRBs exhibit the eﬀects of cosmological time dilation. We ﬁrst verify the results of Zhang et al. (2013) and investigate which method of averaging individual durations is the most robust as shown in Figure 2. As a power-law model is employed, we choose the geometric average. We ﬁnd that, when accounting for the measured errors in T90,obs, a power-law is a statistically unacceptable ﬁt to individual bright GRBs as shown in Figure 3. We then updated the Swift/BAT sample to include an additional 93 GRBs that have occurred since the original analysis by Zhang et al. (2013). Using this total sample of 232 bursts we investigate the evolution of average durations T90,obs, T50,obs and TR45,obs as a function of redshift in the 140/ (1 + z)– 350/ (1 + z) keV range. All three durations exhibit a trend of increasing with increasing redshift. The power-law indices of these trends were reduced when ﬁltering out bursts with poorly measured values of duration. The model power-law index obtained ﬁtting the geometric average of T50,obs (140/ (1 + z) − 350/ (1 + z) keV) has a value most consistent with that expected from time-dilation. We do ﬁnd, however, that the large scatter of the distribution of individual duration values is large, leading to large statistical errors on each average bin, and therefore a large error in the ﬁtted parameters of the power-law model.
Alternating minimal energy methods for linear systems in higher dimensions. Part II: Faster algorithm and application to nonsymmetric systems<|sep|>In this paper we develop a new version of the fast rank–adaptive solver for tensor–structured symmetric positive deﬁnite linear systems in higher dimensions. Similarly to the algorithms from [9], the proposed AMEn method combines the one-dimensional local updates with the steps where the basis is expanded using the information about the global residual of the high–dimensional problem. However, in AMEn the same steps are ordered in such a way that only one or two neighboring cores are modiﬁed at once. Both methods from [9] and the AMEn converge globally, and the convergence rate is established w.r.t. the one of the steepest descent algorithm. The practical convergence in the numerical experiments is signiﬁcantly faster than the theoretical estimate. The AMEn algorithm appears to be more accurate in practical computations than the previously known methods, especially if local problems are solved roughly. The asymptotic complexity of the AMEn is linear in the dimension and mode size, similarly to the algorithms from [9]. The complexity w.r.t. the rank parameter is suﬃciently improved taking into the account that a limiting step is the approximation of the residual, where the high accuracy is not always essential for the convergence of the whole method. We propose several cheaper alternatives to the SVD-based TT-approximation, namely the Cholesky decomposition and the inner ALS algorithm. The ALS approach provides a signiﬁcant speedup, while maintaining almost the same convergence of the algorithm. Finally, we apply the developed AMEn algorithm to general (non-SPD) systems, which arise from high–dimensional Fokker–Planck and chemical master equations. Theoretical convergence analysis can be made similarly to the FOM method, which is rather pessimistic and puts very strong requirements on the matrix spectrum. In numerical experiments we observe a surprisingly fast convergence, even for strongly non–symmetric systems. Here the AMEn demonstrates a signiﬁcant advantage over the DMRG technique, which is known to stagnate, especially in high dimensions, see [23, 5] and Fig. 5. There are many directions of a further research based on the ideas of [9] and this paper. First, the ideas developed in this paper can be generalized to other problems, e.g. ﬁnding the ground state of a many-body quantum system or a particular state close to a prescribed energy. The combination of update and basis enrichment steps looks very promising for a wide class of problems, as soon as the corresponding classical iterative algorithms can be adapted to provide a proper basis expansion in higher dimensions. A huge work is done in the community of greedy approximation methods, where the cornerstone is a subsequent rank-one update of the solution. Second, there is a certain mismatch between the theoretical convergence estimates, which are at the level of the one–step steepest descent algorithm, and the practical convergence pattern, which looks more like the one of the GMRES. This indicates that there are further possibilities to improve our understanding of the convergence of the AMEn and similar methods. Our rates can beneﬁt from sharp estimates of the progress of the onedimensional update steps, which at the moment are available only in a small vicinity of a true solution, which is hard to satisfy in practice, see [28]. The superlinear convergence observed in numerical experiments inspires us to look for possible connections with the theory of Krylov–type iterative methods and a family of Newton methods. Finally, we look forward to solving more high–dimensional problems, and are sure that they will bring new understanding of the advantages and drawbacks of the proposed method, and new questions and directions for a future research.
A Portable Diagnostic Device for Cardiac Magnetic Field Mapping<|sep|>We have presented a new design for a device to perform magnetic ﬁeld mapping and demonstrated that the device collects useful magnetocardiography data in shielded and unshielded environments. The shielded measurements prove that the coil sensor system has suﬃciently low inherent noise for cycle averaged MCG and suﬃcient spatial resolution for ﬁeld map angle measurement. However, operation of the device within an unshielded environment imposes coloured noise on the signal of an amplitude comparable to the repolarisation signals (ECG T wave). This potentially limits the diagnostic capability of our device within unshielded environments. Though the depolarisation signals (QRS) are reliably observable above this noise. This result may be improved by the application of recent developments in denoising algorithms [45–47]. Further clinical testing will be required to determine if it is capable of detecting recent onset of NSTEMI in patients with the same accuracy as previous devices. A future device may want to use more sensors to increase the measurement area and may also consider smaller coils to achieve a higher resolution. The addition of a second layer of coils would provide a vertical baseline for synthetic gradiometry which may improve environmental noise suppression [48]. The low frequency performance could be improved to reach DC by lock-in to a global excitation ﬁeld [41,42,49–51].
Linear-Response Dynamics from the Time-Dependent Gutzwiller Approximation<|sep|>In this paper we have developed the time-dependent Gutzwiller approximation for multiband Hubbard models. This approach is based on a time-dependent variational principle where expectation values are evaluated with the Gutzwiller variational wave-function in the limit of inﬁnite dimensions. In contrast to the standard Gutzwiller approximation [9, 10, 11] both, variational parameters and the underlying Slater determinant, acquire a time dependence. In this regard our calculations generalise earlier investigations by Schir´o and Fabrizio [21, 22] who have studied quantum quenches in homogeneous systems where the time dependence of the density matrix does not couple to that of the variational parameters. On the other hand, momentum (or space) dependent out-of equilibrium displacements of the system require such a coupling as evident from our generalised equations of motion Eqs. (33),(36). We have applied this theory in the small-amplitude, i.e., linear-response limit and exempliﬁed for the case of dynamical charge correlations in the single-band Hubbard model. In an earlier formulation of the TDGA the so-called ‘antiadiabaticity approximation’ [5, 6] has been applied, where the dynamics of the double-occupancy parameters was slaved by that of the density matrix. In contrast, the present approach explicitly incorporates the time dependence of the double-occupancy variational parameters which agrees with the previous formulation in the static limit. In addition it improves the theory in Refs. [5, 6] by incorporating the high-energy features which are on the scale of the Hubbard repulsion for small densities and which position is in good agreement with that of exact diagonalisation. On the other hand, the spectral weight of the high-energy excitations is overestimated within the TDGA although it signiﬁcantly improves the standard HF+RPA approach in this regard. Further reﬁnement of the theory could be achieved by including the coupling between particle-hole and particleparticle excitations which have been studied in Ref. [8] in the framework of the GA. It is interesting that in the present approach the Brinkman-Rice transition appears signaled by a collective mode whose frequency goes to zero. This is not due to the doublon ﬂuctuation stiﬀness becoming soft but because the mass of the ﬂuctuations diverges. We have shown that this divergence appears each time the double occupancy becomes a conserved quantity which is the case in the Brinkman-Rice case where D = 0. It remains to be seen which of these feature remain in an exact description although the similarity of the TDGA results with dynamical mean ﬁeld theory (DMFT) suggest that at least in an approximate way this physics survives in real Mott transitions. Within the DMFT it is quite diﬀult to study systems in which the momentum dependence of collective excitations is important as, for example, spin waves in insulators [37]. In such cases, the TDGA provides us with an important additional tool which complements the DMFT.
Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction<|sep|>In this paper, we propose a new method to boost the performance of video compression artifact reduction. Our method consists of two novel modules: the Recursive Fusion (RF) module and the Deformable Spatiotemporal Attention (DSTA) module. The former is proposed to capture spatiotemporal information from frames in a large temporal scope, while the latter aims at highlighting the artifact-rich areas in each frame, such as the boundary areas of moving objects. Our extensive experiments demonstrate that our proposed method can achieve superior performance over the state-of-the-art methods. The proposed modules also can be easily adapted to existing multi-frame methods and video-related lowlevel tasks.
Neutrino masses and cosmology with Lyman-alpha forest power spectrum<|sep|>In this paper, we present an update on the constraints we derive on several cosmological parameters using Lyα data, either taken alone or in combination with CMB and BAO data. We improve upon our previous study of Paper I [2] on several fronts that we summarize below. We improved the likelihood describing the Lyα data in several ways. We relaxed our model of the IGM, now modeling the redshift-dependence of the IGM temperature T0 and its δ dependence (i.e., the logarithmic slope γ) as, respectively, a broken and a single power law. The likelihood now includes additional freedom to account for the systematic uncertainties that we identiﬁed. We ran new hydrodynamical simulations that allow us to improve our model of the impact of the splicing technique on the 1D power spectrum. This splicing technique is used in the simulation grid to mimic large high-resolution simulations equivalent to 30923 particles per species in a (100 h−1 Mpc)3 box. We also better estimate the contribution of sample variance to the simulation uncertainties. Using this updated likelihood, we set a robust upper bound on the sum of the neutrino masses � mν < 1.1 eV (95% C.L.) from Lyα data alone, prioritizing over all known systematics. Compared to the Planck 2015 measurement [5], the constraints we derive on cosmological parameters Ωm and σ8 are in excellent agreement, but we note a 2.3 σ discrepancy on ns. In a second step, we combine our new Lyα likelihood with the MCMC chains from Planck 2015. In the context of a ﬂat ΛCDMν cosmology, this combined study leads to the tightest limit published on � mν, with an upper bound of 0.12 eV (95% C.L.) using Lyα and Planck TT+lowP data. The addition of BAO does not further improve this limit. In a third step, we investigate the improvement, over CMB data alone, that Lyα data can provide on other parameters. In particular, we show that through the correlation between the reionization optical depth τ and σ8, Lyα data can reduce the uncertainties on the measurement of τ. The central value remains fully compatible with the one derived from CMB. Finally, we consider extensions to the ΛCDMν cosmology, in particular focusing on a possible running of the scalar spectral index. We note a clear improvement of the total χ2 when allowing for running, in agreement with the observed tension on ns between Planck and Lyα data sets. However, this improvement, and the subsequent 3 σ detection of running, could result from a coincidence between a ∼ 1 σ eﬀect in Planck (due to the mismatch between the high and low multipoles in the temperature power spectrum) on the one hand, and a ∼ 2.3 σ eﬀect in Lyα data (possibly coming from an unidentiﬁed systematic eﬀect) on the other hand. Allowing for dns/d ln k to vary in the ﬁt does not change our best limit on � mν, but in that case BAO data are needed to reach that best limit. The combination of Lyα with Planck (TT, TE, EE + lowP) and BAO still leads to � mν < 0.12 eV at 95% C.L., even when allowing for running. As an interesting consequence of our precise measurement of dns/d ln k, we can derive constraints on the inﬂation potential. In the context of slow-roll inﬂation, we use the second-order expansion in Hubble ﬂow-functions to relate the slow-roll parameters ϵ1, ϵ2 and ϵ3 to the power spectrum parameters ns, nt, and dns/d ln k. The uncertainties on the inﬂation parameters are reduced signiﬁcantly compared to their measurement with CMB data. We acknowledge PRACE (Partnership for Advanced Computing in Europe) for awarding us access to resources Curie thin nodes and Curie fat nodes, based in France at TGCC. This work was also granted access to the resources of CCRT under the allocation 2013-t2013047004 made by GENCI (Grand Equipement National de Calcul Intensif). N.P.-D., Ch.Y. and G.R. acknowledge support from grant ANR-11-JS04-011-01 of Agence Nationale de la Recherche. M.V. is supported by ERC-StG ”CosmoIGM”. The work of G.R. is also supported by the National Research Foundation of Korea (NRF) through NRF-SGER 2014055950 funded by the Korea government (MOE), and by the faculty research fund of Sejong University in 2014. We thank Volker Springel for making GADGET-3 available to our team.
Electronic Properties of Carbon Nanostructures<|sep|>We performed the calculations of the electronic structure for the graphitic nanocone and the graphene wormhole. In the ﬁrst case, our aim was to ﬁnd the quadratically integrable solution which includes the boundary eﬀects and considers the real geometry. This goal was partially achieved, but we need to verify the properties of the found solution close to the tip. The precision of the calculations could be improved by the better choice of the corresponding geometry, consideration of the discretion of the energetic spectrum coming from the ﬁnite size of the nanostructure and by the inclusion of next eﬀects coming from the overlap of the neighboring atomic orbitals close to the tip [2]. The localization of the electrons shown in Figures 12 and 13, especially in the case of 3 defects, makes the graphitic nanocone a possible candidate for the construction of the scanning probe in atomic force microscopy. In the second case of the graphene wormhole, we presented the mathematical motivation for our prediction of the eﬀects which should appear close to the wormhole bridge. Our predictions will be veriﬁed with the help of the After the substitution into the corresponding system and comparison of the coeﬃcients which correspond to the particular powers, we get ξ = ξ1 − 2 and αa0 = 0, αa1 + ξa0 + (F − 1)a0 + iDb0 = 0, αb0 = 0, αb1 − ξb0 + (F − 1)b0 + iCa0 = 0, (A.4) αa2 − βa0 + (F − ξ − 2)a1 + iDb1 = 0, αb2 − βb0 + (F − ξ − 2)b1 + iCa1 = 0, (A.5) αa3 − βa1 + (F − ξ − 3)a2 + iDb2 = 0, αb3 − βb1 + (F − ξ − 3)b2 + iCa2 = 0. (A.6) −αak+βak−2−(F −ξ1+2−k)ak−1−iDbk−1 = −Eck−4, −αbk+βbk−2−(F −ξ1+2−k)bk−1−iCak−1 = −Edk−4. (A.8) If we suppose that α ̸= 0, we get the zero solution. So, for the nontrivial solution α = 0 and as follows from the ﬁrst and the third equation in Eq. (A.3), in this case the coeﬃcients a0 and b0 must be also zero. Then, from the system
Orientational instability and spontaneous rotation of active nematic droplets<|sep|>In this paper, we develop a minimal model for an active nematic liquid crystal (LC) drop suspended in the bulk of a surfactant solution in the case of homeotropic anchoring at the drop surface. This model is intrinsically isotropic, in the sense that there is no built-in asymmetry or chirality. Yet, we demonstrate that besides the symmetry-breaking transition to self-propulsion already established for active droplets, this model is also able to capture the spontaneous emergence of chirality and development of helical trajectories. Success of our approach is not accidental, as our model retains two key features of the transition from hedgehog to boojum defect observed in active LC microdrops. First, we note that in the course of this transition, nematic conﬁguration within the droplet remains cylindrically symmetric. In this situation, orientation of the symmetry axis provides the most basic characteristic of the drop’s nematic conﬁguration and orientation of q implements this characteristic in our model. Second, it is apparent that the ﬂow can not push the boojum defect beyond the drop’s surface. That is, there must be certain mechanism limiting the eﬀect of the ﬂow ﬁeld on the nematic conﬁguration. In our model this eﬀect is naturally limited, since we postulate that in the steady state |q| ∝ V , where V is droplet self propulsion velocity that is saturated due to nonlinearity of surfactant advection, as shown in Fig. 3. The main ﬁnding of the paper is that these two key features are enough to explain the emergence of helical tra jectories in active LC microprops. Indeed, a novel mode of instability emerges due to the coupling of ﬂow-induced nematic ordering and surfactant transport around a steadily self-propelling LC drop. This instability does not exist for isotropic active drops and results from the competition of two nematic reorientation mechanisms: the ﬂow-induced nematic reordering (e.g. hedgehog defect displacement) and solid body rotation of the droplet. In particular, in the case when hedgehog defect displacement reduces mobility at the front of a moving drop (m1 < 0), self-propelling ﬂow strives to align q along the drop’s velocity U, while any non-axisymmetric perturbation results in q rotating away from the swimming direction (Figure 2). Our linear stability analysis further reveals that the eigenmode of this novel instability features a drop’s rotation velocity Ω nearly orthogonal to U. The drop trajectory near the onset of instability should be close to a circle which is indeed conﬁrmed in fully nonlinear simulations, before growing perturbations of U become signiﬁcant yielding U ̸⊥ Ω and the drop trajectory becomes helical (Figure 6). Our nonlinear simulations also indicate that for sufﬁciently high P´eclet numbers (slow diﬀusion), both isotropic and nematic drops exhibit transition to chaos and random trajectories (Figure 7). The onset of chaos is preceded by an oscillatory instability characterized by highly symmetric eigenmodes, whose threshold is nearly insensitive to problem parameters besides Pe, hinting that surfactant advection is the main nonlinear mechanism enabling the transition to chaos in both isotropic and nematic droplets. Interestingly, and in contrast with the axisymmetric problem, 3D chaotic motion is not associated with a signiﬁcant decrease in self-propulsion velocity. The present minimal model correctly captures the transitions between three main dynamical regimes recently observed in active LC drops, namely, selfpropulsion with straight, helical, and random trajecto
Stochastic Processes, Slaves and Supersymmetry<|sep|>We have studied a stochastic system comprising a particle with degrees of freedom {Xn} together with associated slave variables {Ξma} that represent inﬁnitesimal line elements carried along by the diﬀusing system. We refer to the equations satisﬁed by these slave variables as slave equations. The states of the system are described by a joint probability distribution P(x, ξ), which may depend on time and which satisﬁes an appropriately generalised diﬀusion equation. Here {xn} and {ξma} are values attained by the stochastic variables {Xn} and {Ξma} . The slave
The Herschel Virgo Cluster Survey: II. Truncated dust disks in HI-deficient spirals<|sep|>In this paper, we have shown that in Hi-deﬁcient galaxies the dust disk is signiﬁcantly less extended than in gas-rich systems. This result, combined with the evidence that Hi-deﬁcient objects show a reduction in their submm-to-K-band ﬂux density ratios, suggests that when the atomic hydrogen is stripped part of the dust is removed as well. However, the dust stripping appears efﬁcient only when very gas-poor spirals are considered, implying that in order to be signiﬁcant the stripping has to occur well within the optical radius. This is consistent with Thomas et al. (2004) who found that the 850 µm scale-length of nearby galaxies is smaller than the Hi, suggesting that outside the optical radius the gas-to-dust ratio is higher than in the inner parts. Our analysis provides evidence that the cluster environment is able to signiﬁcantly alter the dust properties of infalling spirals. We note that this has only been possible thanks to the unique spatial resolution and high sensitivity in detecting cold dust provided by the Herschel-SPIRE instrument and to the wide range of Hi-deﬁciencies covered by our sample. Once combined with the direct detection of stripped dust presented by Cortese et al. (2010) and Gomez et al. (2010), our results highlight dust stripping by environmental eﬀects as an important mechanism for injecting dust grains into the intra-cluster medium, thus contributing to its metal enrichment. This is consistent with numerical simulations which predict that ram pressure alone can already contribute ∼10% of the enrichment of the ICM in clusters (Domainko et al. 2006). Interestingly, the stripped grains should survive in the hot ICM long enough to be observed (Popescu et al. 2000; Clemens et al. 2010). Once completed, HeViCS will allow a search for additional evidence of dust stripping and place important constraints on the amount of intra-cluster dust present in Virgo. Moreover, in combination with the Herschel Reference Survey (Boselli et al. 2010b), it will be eventually possible to accurately quantify the degree of dust-deﬁciency in Virgo spirals. Acknowledgements. We thank the referee, Richard Tuﬀs, for useful comments which improved the clarity of this manuscript. We thank all the people involved in the construction and launch of Herschel. In particular, the Herschel Project Scientist G. Pilbratt, and the PACS and SPIRE instrument teams.
On the origin of M81 group extended dust emission<|sep|>Galactic cirrus emission is prominent in all of the Herschel PACS and SPIRE bands making it diﬃcult to unambiguously detect emission from cold diﬀuse dust in the extra-galactic environment. Some previous measurements of the M81 group’s diﬀuse emission over wavelengths ranging from the optical to the far-infrared have underestimated the signiﬁcant contribution from Galactic cirrus. We ﬁnd no evidence for extended dust emission from the M81 group, all of the prominent features can be accounted for by Galactic cirrus. An important result is that not all velocity components of the Galactic cirrus gas have dust associated with them, which is maybe a reﬂection of the diﬀerent origins of the gas i.e. infalling primordial gas or expelled enriched disc gas. Also the good relationship between cirrus far-infrared emission and HI seems to break down over spatial scales below about 10′. Our conclusions provide us with a dilemma at a number of levels. Firstly, the apparent connection of the far-infrared emission with M81, within a galaxy group that has clearly undergone some tidal interaction, makes it very diﬃcult to believe that they are not associated. A particularly nasty astrophysical coincidence! Secondly, given that most of the emission is from Galactic cirrus then what other sources of assumed extra-galactic far-infrared emission may actually also be due to cirrus (Cortese et al. 2010)? Thirdly, there are a number of active star forming regions identiﬁed in both the ultra-violet and the HI which have no associated far-infrared emission: far-infrared emission is not a good proxy for star formation in these cases. Fourthly, how can you uniquely distinguish cirrus from extra-galactic emission? Our analysis indicates that you need to select a very narrow velocity range for the HI, but even then there is too much scatter in the Boulanger et al. (1996) relation to be very useful unless you are interested in scales greater than about 10′. This is also true for the variation of far-infrared colours (temperatures) from region to region which can ﬂuctuate considerably over smaller spatial scales. Fourier ﬁltering (Roy et al. 2010) is a possibility for removing cirrus contamination of deep cosmological surveys that are primarily concerned with point sources (Eales et al. 2010), but for nearby galaxy groups and clusters (Davies et al. 2010) the size of the galaxies (of order 10′) compared to the cirrus (Fig. 1) makes this problematic. ACKNOWLEDGEMENTS PACS has been developed by a consortium of institutes led by MPE (Germany) and including UVIE (Austria); KU Leuven, CSL, IMEC (Belgium); CEA, LAM (France); MPIA (Germany); INAF- IFSI/OAA/OAP/OAT, LENS, SISSA (Italy); IAC (Spain). This development has been supported by the funding agencies BMVIT (Austria), ESA-PRODEX (Belgium), CEA/CNES (France), DLR (Germany), ASI/INAF (Italy), and CICYT/MCYT (Spain). SPIRE has been developed by a consortium of institutes led by Cardiﬀ University (UK) and including Univ. Lethbridge (Canada); NAOC (China); CEA, LAM (France); IFSI, Univ. Padua (Italy); IAC (Spain); Stockholm Observatory (Sweden); Imperial College London, RAL, UCL-MSSL, UKATC, Univ. Sussex (UK); and Caltech, JPL, NHSC, Univ. Colorado (USA). This development has been supported by national funding agencies: CSA (Canada); NAOC (China); CEA, CNES, CNRS (France); ASI (Italy); MCINN (Spain); Stockholm Observatory (Sweden); STFC (UK); and NASA (USA). GALEX (Galaxy Evolution Explorer) is a NASA Small Explorer, launched in April 2003. We gratefully acknowledge NASA’s support for construction, operation, and science analysis for the GALEX mission. We thank Katie Chynoweth for reprocessing her M81 HI data for us and the THINGS team for providing us with their HI data cube. REFERENCES Arp H., 1965, Science, 148, 363 Bendo G. et al., 2010, A&A, 518, 65 Bot et al., 2009, ApJ, 695, 469 Boulanger F., et al., 1996, A&A, 312, 256 Chynoweth K., et al., 2008, 135, 1983
Dark Photons from the Center of the Earth: Smoking-Gun Signals of Dark Matter<|sep|>We have presented a novel method to discover dark matter that interacts with the known particles through dark photons that kinetically mix with the SM photon. The dark matter is captured by the Earth and thermalized in the Earth’s center, and then annihilates to dark photons. The dark photons then travel to near the surface of the Earth and decay. We have determined the signal rates without simplifying assumptions concerning the dark matter and dark photon masses. In viable regions of the model parameter space, thousands of such dark photon decays are possible in IceCube, and smaller, but still detectable, signals in space-based detectors such as Fermi and AMS are also possible. As with traditional indirect detection signals that rely on annihilation to neutrinos, the dark photon signal points back to the center of the Earth, diﬀerentiating it from astrophysical backgrounds. In contrast to the neutrino signal, however, the dark photon decays to two visible particles. The dark photon signal is therefore even more striking, as it is monoenergetic if fully contained. In addition, in principle both particles could be detected simultaneously yielding, for example, parallel muon tracks in IceCube with separations of ∼ O(10 m). We have shown distributions of these separations for representative points in model parameter space. As discussed in Sec. III A, the leading uncertainty in the signal rate predictions is from the capture rate analysis. The escape velocity of the Earth is not large, and so this capture rate is subject to detailed modeling, including the eﬀects of the Earth, Sun, Jupiter, and Venus. In addition, a cold “dark disk” population of dark matter may signiﬁcantly enhance the capture rates. The implications of these eﬀects for WIMP dark matter have been considered in Refs. [20–24, 56]; it would be interesting to determine their eﬀects on dark matter with dark photon-mediated interactions. In this study, we have assumed the dark matter X is a Dirac fermion and the mediator is a dark photon that mixes only with the SM photon, and so couples only to charged particles. It would be interesting to consider cases where X is a pseudo-Dirac fermion or a scalar, and cases where the dark photon mixes with the Z (and so couples to neutrinos, for example), or is replaced by a scalar (for which the dark matter may also be Majorana). Dark matter that collects and annihilates at the center of the Sun is also a promising source of decaying dark photons and will probe diﬀerent regions of parameter space [90]. Finally, the experiments have been modeled very roughly here; detailed analyses, preferably by the experimental collaborations themselves, are required to evaluate the accuracy of the signal rate estimates. However, our conclusion that there are viable regions of parameter space that predict thousands of signal events indicates that there are certainly regions of parameter space where the indirect detection signals discussed here are the most sensitive probes, surpassing direct detection detectors, beam dump experiments, and cosmological probes. The possibility of discovering signals of dark matter that, unlike so many other indirect detection signals, are essentially free of diﬃcult-to-quantify astrophysical backgrounds, provides a strong motivation for these searches.
Deep Cross-Modality and Resolution Graph Integration for Universal Brain Connectivity Mapping and Augmentation<|sep|>In this paper, we proposed Multi-modal Multi-resolution Graph Integrator which is the ﬁrst graph neural network framework that estimates a connectome population ﬁngerprint given multimodal multi-resolution brain networks. Our method has three compelling strengths: (i) the autoencoder learning task with joint multi-resolution GCN-based autoencoders, facilitating its customizability to any graph resolution, (ii) the design of the clustering-based training sampling in the centeredness loss computation to learn a well-representative CBT of the population heterogeneity and (iii) the proposal of the topology loss to estimate a topologically sound CBT. Our estimated CBTs will not only pave the way for easier brain disorder diagnosis by revealing deviations from the healthy population but also remedy data scarcity by augmenting new brain networks. In our future work, we will use our model to learn universal CBTs of various healthy and disordered brain connectivity datasets including functional, morphological, and structural connectomes. Besides, we will reﬁne our architecture by integrating a novel graph new edge-convolution that operates on large-scale graphs without memory overloading.
Multipartite entanglement in qubit systems<|sep|>In this paper we have studied the properties of the potential of multipartite entanglement and of its minimizers, the MMES, for a system of n qubits. In particular our focus has been on perfect MMES, that saturate the lower bound of the potential, and by using a probabilistic approach, we have proven a theorem on the structure of their population probability vectors. This allowed us to consider a particular simple class of solutions, those with uniform population. We have shown by explicit construction that (apart for the case n = 7 which is still open, but probably is frustrated) there always exist uniform perfect MMES with real phases, a class of states that can be mapped to the classical binary sequences of length 2n. In fact, we have shown that also for n = 4, the lowest number at which frustration occurs and hinders the existence of perfect MMES, the (conjectured) minimum of the potential of multipartite entanglement is attained by uniform states with real phases. This represents a great advantage, because in this situation one can investigate the structure of quantum multipartite entanglement by studying the simplest problem a classical Hamiltonian deﬁned on binary sequences. I would like to thank G. Florio, U. Marzolino, G. Parisi, and S. Pascazio for many conversations and stimulating discussions on multipartite entanglement. One of the reasons for having written this article is the enthusiasm of S. Graﬃ for the subject; I would like to thank him for this. This work is partly supported by the European Community through the Integrated Project EuroSQIP.
SDSSJ143244.91+301435.3 at VLBI: a compact radio galaxy in a narrow-line Seyfert 1<|sep|>We have presented VLBI observations carried out with EVN of SDSSJ143244.91+301435.3, a steep spectrum RL NLS1 that has been recently classiﬁed as a possible CSS source (C14). The results can be summarized as follows: • EVN observations have resolved SDSSJ143244.91+301435.3 which shows a quite complex structure without a clear morphology. The total ﬂux density accounted for in the EVN image (34 mJy) is divided into 3 components: a central, more compact (but resolved), emission, containing more than half of the total correlated ﬂux density, plus two extended structures. The lack of a strong unresolved component (core) is consistent with what is usually observed in steep-spectrum young radio sources. • The high brightness temperature of the radio components detected in the EVN map (TB ∼5×106-1.3×108 K) dis-favours the hypothesis that star-forming activity (detected in the mid-IR band) is the main origin of the observed radio emission and strongly supports a non-thermal origin. • The size of the extended structure (100-150 m.a.s., corresponding to ∼0.5 kpc) is smaller than the value inferred from the linear size/turnover relation valid for the CSS/GPS galaxies. This is likely due to the non-detection of a low surface brightness component in the EVN map combined to projection eﬀects. Overall, the EVN observations support the idea that SDSSJ143244.91+301435.3 is a CSS source with the relativistic jet observed at larger angles than the ﬂat-spectrum RL NLS1 studied so far. The fact of observing directly the nuclear emission in the optical, without signiﬁcant obscuration, suggests that SDSSJ143244.91+301435.3 is probably not oriented on the plane of the sky but at intermediate angles. VLBI observations at high frequencies would prove this hypothesis since they are expected to detect a stronger, mildly boosted, radio core. These results conﬁrm that SDSSJ143244.91+301435.3 likely belongs to the parent population of the RL NLS1 with a blazar spectrum and favour the idea that these sources can be directly related to young radio galaxies. A systematic follow-up at VLBI resolution, similar to the one discussed here, of all the RL NLS1 will be fundamental to test this hypothesis on a ﬁrm statistical basis. We thank the referee for his/her useful comments that improved the quality of the paper. The European VLBI Network is a joint facility of independent European, African, Asian, and North American radio astronomy institutes. Scientiﬁc results from data presented in this publication are derived from the following EVN project code: EC048. LB acknowledges support from the Italian Space Agency (contract ASI INAF NuSTAR I/037/12/0).
Pose Guided Human Image Synthesis with Partially Decoupled GAN<|sep|>In this paper, we have proposed a novel model PD-GAN for PGHIS by explicitly controlling the pose of the reference human images. Speciﬁcally, we decouple the body parts to help the generation of realistic human images. For PGHIS, we design a new transformer module to handle human images with complex topology, which not only can capture the long-range dependency, but also focus on the details of human images using the FFT block. Extensive experiments show that the results of PD-GAN are qualitatively and quantitatively better Table 2: Quantitative results of image quality on ablation study. w/o Trans: without transformer module, w/o FFT: Transformer module without FFT block, w/o Face: lack of the part of face in the body decoupled part module, w/o Pants: lack of the part of pants in the body decoupled part module, w/o Hair: lack of the part of hair in the body decoupled part module, Full: our complete model. than or competitive with existing state-of-the-art methods. Otherwise, the size of PD-GAN is the smallest, which is a big highlight in the application. Limitation.Although our method produces satisfactory results in most cases, it still fails when the materials (e.g., the stool in 7) in the datasets are extremely lacking. We show some failure cases in Figure 7, inconsistencies and blur appeared in these images. We believe that training the model on a larger dataset would alleviate this situation where the person and object in images are confused. Furthermore, applying this model to more tasks (e.g., virtual try-on) is a future research direction for us, and we are committed to working on a generalized human image synthesis model. Figure 7: Failure cases. Our method generates less satisfactory images for very rare items in the dataset (e.g., stools), or when there are obvious interferences.
Extracting Conflict-free Information from Multi-labeled Trees<|sep|>We introduced an efﬁcient algorithm to reduce a multi-labeled MUL-tree to a maximally reduced form with the same information content, deﬁned as the set of nonconﬂicting quartets it resolves. We also showed that the information content of a MUL-tree uniquely identiﬁes the MUL-tree’s maximally reduced form. This has potential application in comparing MUL-trees by signiﬁcantly reducing the number of comparisons as well as in extracting species-level information efﬁciently and conservatively from large sets of trees, irrespective of the underlying cause of multiple labels. Our algorithm can easily be adapted to work for rooted trees. Further work investigating the relationship of the MRF to the original tree under various biological circumstances is also underway. We might expect, for example, that well-sampled nuclear gene families reduce to very small MRF trees, and that annotation errors in chloroplast gene sequences (in which we expect little gene duplication), result in relatively large MRF trees. Comparing the MRF to the original MUL-tree may well provide a method for efﬁciently assessing and segregating data sets with respect to the causes of multiple labels. It would be interesting to compare our results with some of the other approaches for reducing MUL-trees to singly-labeled trees (e.g., [18]) or, indeed, to evaluate if our method can beneﬁt from being used in conjunction with such approaches. We thank Mike Sanderson for helping to motivate this work, for many discussions about the problem formulation, and for our ongoing collaboration in the SearchTree project. Sylvain Guillemot listened to numerous early versions of our proofs and offered many insightful comments. We also thank the anonymous reviewers for their comments, which helped to improve this paper.
The Smart Mask: Active Closed-Loop Protection against Airborne Pathogens<|sep|>We have presented a new paradigm of active closed-loop defense, in the form of a smart mask, against airborne pathogens including SARS-CoV-2. We have presented the system design, and using a functional prototype of the mask, also demonstrated the major operational characteristics. Various levels of smartness can be incorporated into the proposed system to control the time, duration, and intensity of mitigation based on awareness of the location (e.g., hospitals, quarantined zones, or care centers with infected patients), ambient conditions (e.g., humidity and air temperature, human occupancy), and overall health of the user (e.g., age, pre-existing conditions, etc.). The proposed mask can also be extended to protect against pollutants, dust particles, and pollen, e.g., for vulnerable populations with pollen and/or dust allergies. It can also be extended to other usage scenarios, e.g., military personnel exposed to harmful airborne particles; dentists performing dental procedures; and day-care or elementary schools where social distancing is hard to maintain. While more testing and evaluation is needed to fully establish the merits of the smart mask and identify remaining design challenges and associated trade-offs, the initial results are highly promising. The proposed defense can be applied to existing masks as an add-on reusable assembly, as well as to new mask designs as demonstrated here. It also has the potential to completely replace traditional masks for speciﬁc applications. Our future studies will focus on these topics.
CHIP: CHannel Independence-based Pruning for Compact Neural Networks<|sep|>In this paper, we propose to use channel independence, an inter-channel perspective-motivated metric, to evaluate the importance of ﬁlters for network pruning. By systematically exploring the quantiﬁcation metric, measuring scheme, and sensitiveness and reliability of channel independence, we develop CHIP, a CHannel Independence-based ﬁlter pruning for neural network compression. Extensive evaluation results on different datasets show our proposed approach brings signiﬁcant storage and computational cost reductions while still preserving high model accuracy. As technology advances, cell phones, laptops, wearable gadgets and intelligent connected vehicles with speciﬁc chips are required to handle more complicated tasks by deploying neural networks. However, more powerful networks will cost more memory size and running time. Network pruning is the main strategy to reduce the memory size and accelerate the run-time during the inference stage. Beneﬁting from pruning techniques and speciﬁc designs for hardware [62, 4], IoT (Internet of Things) devices are able to execute complex projects based on small and efﬁcient models. Bo Yuan would like to thank the support from National Science Foundation (NSF) award CCF1937403. Saman Zonouz would like to thank the support from NSF CPS and SATC programs.
The Cost of OSCORE and EDHOC for Constrained Devices<|sep|>In this paper we presented the design of µOSCORE and µEDHOC ﬁrmware libraries for constrained regular microcontrollers, which are based on the newest state of the OSCORE and EDHOC speciﬁcations and consider all modes of operation. Additionally, we presented the design of µOSCORE-TEE and µEDHOC-TEE ﬁrmware libraries for microcontrollers featuring a TEE, which provide protection against attackers exploiting software vulnerabilities. This is achieved by separating the cryptographic keys and routines from the rest of the ﬁrmware, which may be vulnerable. We evaluated our libraries extensively on several broadly used microcontroller architectures. Our evaluation shows that when µOSCORE and µEDHOC are used together they require a total of ≈25 KB FLASH and between ≈1.8 KB and ≈4.2 KB RAM depending on the EDHOC mode. We also show that a typical CoAP packet can be protected with OSCORE within a few milliseconds. Our computing time evaluation of the EDHOC protocol shows that authentication with static DH keys is 45-50 % faster than authentication with signatures, when RPKs are used. When certiﬁcates are used, static DH key authentication is 25-30 % faster than authentication with signatures. Our libraries for microcontrollers with a TEE show low overhead in terms of computing time and FLASH requirements. However, the RAM overhead is 30-40 %, which is still acceptable for the majority of IoT SoCs.
Human brain ferritin studied by muon Spin Rotation: a pilot study<|sep|>We isolated ferritin from the brain of an AD patient and a healthy age- and gender-matched individual. The protein solution was characterized by biochemical and physical techniques assessing the protein concentration and verifying the superparamagnetic properties, and the presence of iron in the sample. The characterization shows that the isolation protocol needs further improvements, in order to signiﬁcantly increase the concentration and purity of the ferritin solution. µSR was then used to probe the spin dynamics of the iron core of human-brain ferritin, as a function of the temperature. Our pilot experiment showed Asymmetry spectra that signiﬁcantly resembled those of commercial and highly pure horse-spleen ferritin, which was used as a reference in this study. A broad peak in the spin-lattice relaxation rate of the muons stopping in the core allowed us to identify the blocking temperature of ferritin. We proposed a model to interpret the spin-lattice relaxation rate, based on the N´eel theory of superparamagnetism, and on the experimental size distribution of the obtained ferritin particles. The comparison between simulation and experimental data gave us an indication of the magnetocrystalline anisotropy constant of the ferritin iron core. Our analysis suggests that ferritin isolated from the control subject is in agreement with a mineral phase compatible with ’physiological’ ferrihydrite, while the ferritin isolated from the Alzheimer’s patient contains an iron mineralization form with a larger K constant, in the range observed for magnetoferritin, when the size distribution obtained from electron microscopy is taken into account. However, to draw further conclusions about the composition of ferritin in relation with AD, ferritin isolated from diﬀerent AD patients should be investigated. This pilot study shows that µSR is a promising approach to the study of iron-loaded human-brain proteins. We are grateful to M. de Wit, A. Amato and C. Baines for proving help during the muon Spin Rotation experiment and for useful discussion. We thank L. Cristofolini and J. Wagenaar for fruitful discussions. G. Lamers, J. Willemse, L. van der Graaf, J. Aarts, N. Lebedev and C. Koeleman for technical assistance during ferritin freezedrying and characterization. We thank W. Breimer for helping during the MRI data acquisition. This work was supported by the Dutch Foundation for Fundamental Research on Matter (FOM), by the Netherlands Organization for Scientiﬁc Research (NWO) through a VICI fellowship to T. H. O. and through the Nanofront Program. One of us (M. B.) was supported by the FP7 European Union Marie Curie IAPP Program, BRAINPATH, under grant number 612360. Partial funding was provided by European Research Council, Advanced Grant 670629 NOMA MRI.
Dynamical evolution of massive perturbers in realistic multi-component galaxy models I: implementation and validation<|sep|>In this paper, we developed a semi-analytical framework to integrate the motion of particles in realistic galactic potentials composed of multiple components. Among several implemented potentialdensity pairs (spherical, axis-symmetric and triaxial), we included the cases of exponential discs with vertical density decaying either as e−|𝑧 |/𝑧𝑑 or sech2(𝑧/𝑧𝑑), for which we developed an eﬃcient setup to numerically compute its potential and, consequently, the acceleration ﬁeld. Though more complex to tackle, these exponen tial proﬁles have to be preferred to other analytical proﬁles, that can only partially reproduce the exponential decay of the surface brightness of observed disc galaxies. For example, the Miyamoto-Nagai (MN, Miyamoto & Nagai 1975) disc is one of the most common analytical proﬁles employed to model the potential of disc galaxies. Still, this proﬁle can signiﬁcantly diﬀer than the one of realistic galaxies, especially at large radii where it falls oﬀ as a power law rather than exponentially (see discussion in chapter 2 of Binney & Tremaine 2008). To enhance the density decay, a combination of three MN discs is usually employed, although this requires one of the discs to have a negative mass, often leading to the appearance of non-physical negative density regions in the (𝑅 − 𝑧) plane (see e.g. Barros et al. 2016; Bonetti et al. 2019). This strongly supports our choice about the direct employment of the exponential disc proﬁle. In addition to a more realistic description of the density and potential of disc galaxies, our semi-analytical framework can accurately reproduce the orbital decay of MPs due to DF. For an arbitrary mass distribution, an analytical description of the MP orbital decay from ﬁrst principles is generally unfeasible, since such complex phenomenon depends both on local and non-local eﬀects (see, e.g., the recent discussion in Tamfal et al. 2020). Any semi-analytical formulation must therefore introduce a number of approximations. We leverage on the fact that generally, at a ﬁrst level of approximation, local eﬀects are more relevant in the vast majority of common astrophysical situations. This allows us to employ the formalism derived by Chandrasekhar (1943) based on two-body interactions. We described the DF for both the spherical proﬁles and the disc. For the latter, which features a net rotation, we describe DF as a function of the relative velocity between the MP and the local surrounding medium. We compared the evolution in the semi-analytical framework against full N-body simulations, and we showed that, when additional information about the limited resolution of the latter is included in our setup, the agreement between the two is excellent. We conclude by discussing some possible short and mid-term improvements of the newly discussed model. First, even considering non-evolving hosts with smooth density proﬁles, the DF description could be improved in multiple ways. For instance when computing the DF expression for the disc we assumed an isotropic Gaussian as distribution function. A natural extension is to consider an anisotropic version of such distribution function encoding diﬀerent velocity dispersion along diﬀerent direction, such as 𝜎𝑅 = 𝜎𝜙 > 𝜎𝑧 (see e.g. Binney 1977). Despite this choice inevitably requires the numerical computation of some integrals, the task could likely be optimised in order to maintain good computational eﬃciency. A further (indirect) improvement for disc DF consists in the development of more realistic expressions for the rotational pattern vrot and 𝜎𝑅 proﬁle, such that a well-behaved form is maintained also in the central regions. A more challenging aspect concerns how the distribution functions of the individual galactic components are evaluated. In principle, such functions should be computed self-consistently considering the whole galaxy potential, while in the current analysis the velocity distributions of the bulge and halo are computed as if they were in isolation. In addition, we follow the approximation discussed in Chandrasekhar (1943), in which only stars moving slower than the MP contribute to DF. Such approximation is less and less valid for cored spherical systems (Antonini & Merritt 2012). While such approximation has a limited impact for Hernquist-like proﬁles (justifying the agreement we ﬁnd with numerical simulations), it We also limited our analysis to MPs with ﬁxed masses, while in general a substantial mass evolution is expected in the case of galaxy mergers. For instance, during the early stages of the inspiral, the decaying MBH is expected to be embedded in a sizable fraction of its original stellar core. The resulting MP is therefore characterised by both an increased eﬀective mass (determining a more eﬃcient DF) and a larger size. However, as the decay proceeds, tidal eﬀects exerted by the host galaxy on the MP tend to gradually erode the residual stellar envelope, until we are left with a “naked” MBH (see e.g. Van Wassenhove et al. 2014). On the other hand, if large reservoirs of gas are available and promptly funnelled toward the secondary nucleus, the decaying MBH could accrete a considerable amount of it and thus gradually increasing its mass as time passes (Callegari et al. 2011b; Capelo et al. 2015; Capelo & Dotti 2017). Additional improvements can be considered relaxing the assumption of smooth/axi-symmetric density distributions. Speciﬁcally, when considering the galactic gaseous medium, it has been found that in young galaxies gas can be quite turbulent and even clumpy. This introduces stochastic patterns in the motion of MPs, that depending on the speciﬁc gas conditions can signiﬁcantly alter the otherwise smooth decay (Fiacconi et al. 2013b; Roškar et al. 2015; Tamburello et al. 2017; Souza Lima et al. 2017). Stocasticity is also typical in marginally Toomre unstable galactic discs, which may lead to the formation of bars and/or spirals. Such structures strongly deviates from axi-symmetry and the torques that they exert on inspiralling MPs can signiﬁcantly disturb their orbits, in the most extreme situations even scattering them away (Bortolas et al. 2020). Finally, a more realistic semi-analytical model should also consider the possible time evolution of the host galaxy during the orbital evolution of the MP. If the decay from large separation lasts for a quite long time, of the order of several Gyr, the host galaxy can signiﬁcantly evolve, in particular at high redshift, where accretion of pristine gas from the cosmic ﬁlaments can substantially change the total mass and possibly aﬀect the main galaxy geometry (see e.g. the discussion in Rosas-Guevara et al. 2020). Major galaxy mergers can also strongly perturb the galaxy structure and determining radical changes for any MP evolution. Unfortunately, given the very complex physics involved in such mergers, a semi-analytical description is challenging and full numerical simulation should be considered. We plan to address the above-mentioned caveats by including them in our semi-analytical framework, to provide a fast and as accurate as possible description of the orbital decay of satellites onto evolving disc galaxies. We thank Jo Bovy for insightful discussion and for his precious help in setting up an exponential disc with sech2 vertical proﬁle within galpy. Numerical calculations have been made possible through a CINECA-INFN agreement, providing access to resources on GALILEO and MARCONI at CINECA. MD, MB, and 10 We stress that the results presented in Antonini & Merritt (2012) refer to the speciﬁc case of spherical stellar systems in quasi-Keplerian potentials, i.e. it is valid only for the ﬁnal stages of DF within the inﬂuence sphere of the host central MBH. While the same distribution function has been used for MPs at much larger separations (e.g. Li et al. 2020), a self-consistent implementation would require the use of the correct distribution function at every scale.
Constraints on the rate of supernovae lasting for more than a year from Subaru/Hyper Suprime-Cam<|sep|>We present the results of our survey for SNe lasting for more than a year by using long-baseline deep (around 26 mag) and wide (1.75 deg2) HSC time-domain data obtained for 4 seasons from late 2016 to early 2020 in the g, r, i, and z band. We discovered no SNe lasting for 3 years, one SNe lasting for 2 years (HSC16aayt), and two SNe lasting for 1 year (HSC19edgb and HSC19edge). Therefore, the discovery rates of SNe lasting for more than a year in the transient surveys with a typical limiting magnitude of 26 mag are estimated to be 1.4+1.3 −0.7(stat.)+0.2 −0.3(sys.) events deg−2 yr−1. The statistical error corresponds to the 84% conﬁdence limits assuming the Poisson statistics and the systematic uncertainty is from the uncertainty in the discovery eﬃciency. The three long-lasting SNe we found are all consistent with being a SN IIn. Assuming that they are all SNe IIn, we estimate that about 40% of SNe IIn have long-lasting LCs. No plausible PISN candidates lasting for more than a year were discovered. By comparing survey simulations and the survey results, we constrain that the PISN rate up to z ≃ 3 is less than 100 Gpc−3 yr−1. In other words, the PISN rate is less than 0.01 − 0.1 per cent of the core-collapse SN rate at these redshifts. The exploration of the long-timescale (years or more) transient phenomena requires a patient long-term mon itoring of the same ﬁeld. Our HSC data currently have the baseline of around 1000 days. We discovered a couple of long-lasting SNe but longer monitoring the same ﬁeld is required to explore the frontier of the long-lasting transients. There likely exist many long-lasting rare transients such as PISNe that require longer persistent monitoring of the same ﬁeld to discover. Our exploration has just started and it is important to keep the monitoring observations for even longer. We thank the anonymous referee for constructive comments that improved this paper. T.J.M. is supported by the Grants-in-Aid for Scientiﬁc Research of the Japan Society for the Promotion of Science (JP17H02864, JP18K13585, JP20H00174). J.C. would like to acknowledge funding by the Australian Research Council Centre of Excellence for Gravitational Wave Discovery (OzGrav), CE170100004. L.G. was funded by the European Union’s Horizon 2020 research and innovation programme under the Marie Sk�lodowska-Curie grant agreement No. 839090. This work has been partially supported by the Spanish grant PGC2018-095317-B-C21 within the European Funds for Regional Development (FEDER). G.P. acknowledge support from the Ministry of Economy, Development, and Tourism’s Millennium Science Initiative through grant IC120009, awarded to The Millennium Institute of Astrophysics, MAS. This work is supported by the Japan Society for the Promotion of Science Open Partnership Bilateral Joint Research Project between Japan and Chile (JPJSBP120209937). The Hyper Suprime-Cam (HSC) collaboration includes the astronomical communities of Japan and Taiwan, and Princeton University. The HSC instrumentation and software were developed by the National Astronomical Observatory of Japan (NAOJ), the Kavli Institute for the Physics and Mathematics of the Universe (Kavli IPMU), the University of Tokyo, the High Energy Accelerator Research Organization (KEK), the Academia Sinica Institute for Astronomy and Astrophysics in Taiwan (ASIAA), and Princeton University. Funding was contributed by the FIRST program from the Japanese Cabinet Oﬃce, the Ministry of Education, Culture, Sports, Science and Technology (MEXT), the Japan Society for the Promotion of Science (JSPS), Japan Science and Technology Agency (JST), the Toray Science Foundation, NAOJ, Kavli IPMU, KEK, ASIAA, and Princeton University. This paper makes use of software developed for the Large Synoptic Survey Telescope. We thank the LSST Project for making their code available as free software at http://dm.lsst.org. This paper is based on data collected at the Subaru Telescope and retrieved from the HSC data archive system, which is operated by Subaru Telescope and Astronomy Data Center (ADC) at NAOJ. Data analysis was in part carried out with the cooperation of Center for Computational Astrophysics (CfCA), NAOJ. The Pan-STARRS1 Surveys (PS1) and the PS1 public science archive have been made possible through contributions by the Institute for Astronomy, the University of Hawaii, the Pan-STARRS Project Oﬃce, the Max Planck Society and its participating institutes, the Max Planck Institute for Astronomy, Heidelberg, and the Max Planck Institute for Extraterrestrial Physics, Garching, The Johns Hopkins University, Durham University, the University of Edinburgh, the Queen’s University Belfast, the Harvard-Smithsonian Center for Astrophysics, the Las Cumbres Observatory Global Telescope Network Incorporated, the National Central Uni versity of Taiwan, the Space Telescope Science Institute, the National Aeronautics and Space Administration under grant No. NNX08AR22G issued through the Planetary Science Division of the NASA Science Mission Directorate, the National Science Foundation grant No. AST-1238877, the University of Maryland, Eotvos Lorand University (ELTE), the Los Alamos National Laboratory, and the Gordon and Betty Moore Foundation. The authors wish to recognize and acknowledge the very signiﬁcant cultural role and reverence that the summit of Maunakea has always had within the indigenous Hawaiian community. We are most fortunate to have the opportunity to conduct observations from this mountain. We describe the SLSN LC templates used for the survey simulations. The SLSNe we consider in this paper are Type I SLSNe that do not have hydrogen emission features (Quimby et al. 2011). We take the similar approach to make the SLSN templates as in Prajs et al. (2017). We use the magnetarpowered model (Kasen et al. 2011; Woosley 2010) to make the SLSN LC templates. Although the magnetar model is not necessarily the complete model for SLSNe (Moriya et al. 2018, for a recent review), it reproduces the SLSN LCs at around the peak well (e.g., Inserra et al. 2013; Wang et al. 2015; Nicholl et al. 2017b). We take the same approach described in Inserra et al. (2013) to obtain the multi-color LCs of magnetarpowered SNe. The bolometric luminosity is obtained by assuming the central energy input through the dipole radiation, where Ep is the initial rotational energy of the magnetar and tp is the spin-down timescale of the magnetar. Assuming the momentum of inertia of the magnetar is ≃ 1045 g cm2, Ep ≃ 2×1052P −2 ms , where Pms is the initial rotational period scaled with 1 ms. Similarly, assuming the magnetar radius of ≃ 10 km, tp ≃ 4.1 × 105B−2 14 P 2 ms, Figure 9. The Slow SLSN LC template (top) and the Fast SLSN LC template (bottom) compared with the observed LCs of SN 2015bn (Nicholl et al. 2016).
Microphone array post-filter for separation of simultaneous non-stationary sources<|sep|>We proposed a microphone array post-ﬁlter designed in the context of separation of multiple simultaneous sources. It is based on a loudness-domain MMSE estimator in the frequency domain with a noise estimate that is computed as the sum of a stationary noise estimate and an estimation of leakage due to the linear source separation (LSS) algorithm. Experimental results show a reduction in log spectral distortion of up to 12 dB compared to the output of the LSS and up to 4 dB over the single-channel post-ﬁlter. The proposed post-ﬁlter is general enough to be applicable to most source separation algorithms. A possible improvement to the algorithm would be to derive a method that automatically adapts the leakage factor η to track the leakage of an adaptive LSS algorithm.
Guided self-assembly of magnetic beads for biomedical applications<|sep|>Circulating tumor cells are captured due to mechanical or aﬃnity properties. In this work we try to combine both methods with softmagnetic bead arrays. Other than in our preliminary work [9], where magnetic beads self-organize just by a magnetic gradient ﬁeld and the ﬂuid ﬂow, the arrangement is guided with a regular pattern of thin Ni discs. The array is magnetized with applied NdFeB permanent magnets or homogeneous magnetic ﬁelds. To understand the behavior of the beads in the microﬂuidic device a simulation environment combining macro- and microscale was developed. Analytical ﬁeld calculations of the applied permanent magnets are the base for numerical analysis of the seeding array. The importance of ﬁeld strength is shown on the dominant magnetization of the thin Ni discs as well as the neglectable ﬁeld from bead agglomerations acting on the seeding points. Magnetic particle dynamics with a large number of beads can get computational expensive. It can be controlled by limiting interaction radii of every single bead. Calculation of magnetic ﬁeld in every single bead is also time consuming. Therefore a particle-in-cell method was implemented into the simulation environment. Thus the ﬁeld calculations from permanent magnets and Ni seeding points do not need to be calculated in every timestep. During initialization of the simulation a regular grid is ﬁlled with magnetic ﬁeld values. Additional numerical derivatives complete the Cartesian grid. During simulation magnetic ﬁeld values and derivatives for every single bead are gained through tricubic interpolation. Resulting softmagnetic bead structures have shown to be independent on the Ni array using permanent magnets as external ﬁeld source. High magnetic ﬁeld gradients suppress relatively weak seeding ﬁeld gradients. Ni patterns aﬀect the resulting bead structure much more using a homogeneous ﬁeld source. The investigation of the microﬂuidic device can be extended using diﬀerent materials and geometries. Which particular setup is most suitable for ﬁltering CTCs need to be examined in further studies. Blood ﬂow simulations (see preliminary work for details [9, 16, 17]) help to improve the design of the device for optimal bead arrangements. Acknowledgment The authors gratefully acknowledge the ﬁnancial support of the N ¨O Forschungs- und Bildungsges.m.b.H. (NFB) through the Life Science Calls
Domain growth and aging scaling in coarsening disordered systems<|sep|>Phase ordering in disordered systems, both with or without frustration, still poses many challenges. Due to the very slow dynamics, it is usually not possible to enter the asymptotic regime. Instead, most studies have been done in the initial, transient regime where the typical length in the system increases approximately like a power-law of time. Still, an increasing number of studies noticed deviations from the simple algebraic growth at later time, pointing to a crossover from the algebraic-like regime to the true asymptotic regime.
Thermodynamic properties of the new multiferroic material (NH$_4$)$_2$[FeCl$_5$(H$_2$O)]<|sep|>The results of the present investigations of a series of thermodynamic properties reveal that (NH4)2[FeCl5(H2O)] is a new multiferroic material. Multiferroicity in this compound arises at ∼ 6.9 K, with preceeding AFM ordering at ∼ 7.3 K. While for low applied magnetic ﬁelds an electric polarization of about 3 µC/m2 is lying within the ab plane, it can be signiﬁcantly enhanced and rotated to c for magnetic ﬁelds above about 5 T. The rather complex temperature versus magnetic ﬁeld phase diagrams of (NH4)2[FeCl5(H2O)] show several multiferroic/magnetoelectric phases, that diﬀer in orientation and magnitude of the electric polarization and also in the orientation of the magnetic moments of iron. In order to characterize the diﬀerent multiferroic/magnetoelectric phases of (NH4)2[FeCl5(H2O)] on the microscopic scale, and thus to enable an understanding of the underlying mechanism of multiferroicity of this compound, a detailed determination of the magnetic structures by means of neutron scattering is essential as a future investigation. However, because of the high content of hydrogen atoms (10 H per formula unit) this will require the use of the analogous deuterium compound (ND4)2[FeCl5(D2O)]. Of course, a substantial inﬂuence of the isotope exchange on the relevant crystal properties is expected and will need a detailed investigation. We thank T. Bardenheuer and J. Brand for assistance during the X-ray powder measurements and S. Heijligen for measurements of the magnetic susceptibility. This work was supported by the Deutsche Forschungsgemeinschaft via SFB 608 and via the Center of Excellence Quantum Matter and Materials (QM2) at the University of Cologne.
Robustness Threats of Differential Privacy<|sep|>It is well-known that neural networks trained with standard gradient descent methods, such as SGD, are brittle and not secure to diﬀerent out-of-distribution examples. In this paper, we experimentally demonstrated that models, trained by diﬀerentially private SGD, that strongly protects the privacy of the model and underlying training data, might be even less secure, i.e. more susceptible to adversarial examples and common corruptions. In our experiments, we studied ﬁve diﬀerent robustness measurements: accuracy drop after a single FGSM attack, robustness against PGD attack and a number of iterations to achieve adversarial class, distance to the closest decision boundary in ℓ2 and ℓ∞, curvature proﬁle of the decision boundary and performance on a corrupted dataset. We also studied how the main components of diﬀerentially private training (gradient clipping and noise addition) aﬀect the robustness. The results indicate, that diﬀerential privacy reduces not only model’s accuracy, but also aﬀect the robustness of the model, decreasing or increasing. This threatens diﬀerential privacy to be deployed in security-critical scenarios. Interesting eﬀect is observed with big gradient noise, when privacy and robustness are not in conﬂict. This eﬀect and solid mathematical explanation of connection between diﬀerential privacy and robustness are the perspective directions for the future work.
An Optimal Fully Distributed Algorithm to Minimize the Resource Consumption of Cloud Applications<|sep|>resources consumed by an application during its execution  on tree-structured and hierarchical networks. The problem  was solved in an optimal way (tree-structured networks) by  proposing a fully distributed algorithm guaranteeing  convergence. Our future plans include enhancing the  functionality of DRA to take into consideration other  network structures besides tree and hierarchical networks.
Robust Factorization of Real-world Tensor Streams with Patterns, Missing Values, and Outliers<|sep|>In this work, we propose SOFIA, an online algorithm for factorizing real-world tensors that evolve over time with missing entries and outliers. By smoothly and tightly combining tensor factorization, outlier detection, and temporal-pattern detection, SOFIA achieves the following strengths over state-of-the-art competitors: • Robust and accurate: SOFIA yields up to 76% and 71% lower imputation and forecasting error than its best competitors (Figures 3, 4, and 6). • Scalable: SOFIA incrementally processes new entries in a time-evolving tensor, and it scales linearly with the number of new entries per time step (Figure 7 and Lemma 2). Reproducibility: The code and datasets used in the paper are available at https://github.com/wooner49/soﬁa. This work was supported by Samsung Electronics Co., Ltd. and Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)).
On the readability of overlap digraphs<|sep|>In this paper, we deﬁne a graph parameter called readability, and initiate a study of its asymptotic behavior. We give purely graph theoretic parameters (i.e., without reference to strings) that are exactly (respectively, asymptotically) equivalent to readability for trees (respectively, C4-free graphs); however, for general graphs, the HUB number is equivalent to readability only in the sense that it is bounded on the same set of graphs. While an ℓ-decomposition always satisﬁes the HUB-rule, the converse is not true. For example, a decomposition of P4 with weights 4, 5, 3 satisﬁes the HUB-rule but cannot be achieved by an overlap labeling (by Lemma 3.1). For this reason, the upper bound given by Lemma 3.5 leaves a gap with the lower bound of Lemma 3.4. We are able to describe other properties that an ℓ-decomposition must satisfy (not included in the paper), however, we are not able to exploit them to close the gap. It is a very interesting direction to ﬁnd other necessary rules that would lead to a graph theoretic parameter that would more tightly match readability on general graphs than the HUB number. Consider r(n) = max{r(D) | D is a digraph on n vertices}. We have shown r(n) = Ω(n) and know from [BM02] that r(n) = O(2n). Can this gap be closed? Do there exist graphs with readability Θ(2n) (as we conjecture), or, for example, is readability always bounded by a polynomial in n? Questions regarding complexity are also unexplored, e.g., given a digraph, is it NP-hard to compute its readability? For applications to bioinformatics, the length of reads can be said to be polylogarithmic in the number of vertices. It would thus be interesting to further study the structure of graphs that have poly-logarithmic readability. Acknowledgements. P.M. and M.M. would like to thank Marcin Kami´nski for preliminary discussions. P.M. was supported in part by NSF awards DBI-1356529 and CAREER award IIS-1453527. M.M. was supported in part by the Slovenian Research Agency (I0-0035, research program P1-0285 and research projects N1-0032, J1-5433, J1-6720, and J1-6743). S.R. was supported in part by NSF CAREER award CCF-0845701, NSF award AF-1422975 and the Hariri Institute for Computing and Computational Science and Engineering at Boston University.
Enhancing magic sets with an application to ontological reasoning<|sep|>Magic sets aim at optimizing query answering, but they may introduce recursive deﬁnitions that possibly deteriorate the performance of a bottom-up evaluation of the rewritten program. Previous works in the literature noted the problem for programs with stratiﬁed negation, and proposed several solutions to the associated semantic issue. By imposing some restriction on SIPS, this paper provides a simple solution to semantic issues arising for programs with stratiﬁed negation and aggregations, which also inhibits the creation of new positive recursive deﬁnitions (Section 3.1). The role of magic atoms is to restrict the range of variables in the original rules of the processed program. When all arguments of a predicate have to be considered free, a full-free adornment is generated. Any other adornment associated with such a predicate only introduces overhead in the evaluation of the rewritten program. This paper proposes a post-processing of the rewritten program to purge full-free adornments, in contrast to more complex unroll procedures (Section 3.2). Further overhead is associated with subsumed rules. Their identiﬁcation is nontrivial and addressed by a backtracking algorithm. Even if there are few branching points, actually only if there are multiple occurrences of the same predicate in rule bodies, running the backtracking algorithm for all pairs of rules in the rewritten program is expensive. The hashing technique given in Section 3.3 provides a drastic reduction on the number of checks. This work has been partially supported by MIUR under project “Declarative Reasoning over Streams” (CUP H24I17000080001)– PRIN 2017, by MISE under project “S2BDW” (F/050389/0103/X32) – “Horizon2020”PON I&C2014-20,by Regione Calabria under project “DLV LargeScale” (CUP J28C17000220006) – POR Calabria 2014-20, and by GNCS-INdAM.
Three family unification in higher dimensional models<|sep|>We studied the higher dimensional models in which gauge, Higgs and three families of matter ﬁelds are uniﬁed in a SUSY gauge multiplet. The gauge multiplet contains three chiral super ﬁelds Σi as well as a vector multiplet in 4D point of view. When both Higgs and matter ﬁelds are extracted as zero modes by orbifold projection, the Yukawa interaction in 4D is generated from the bulk gauge interaction. We classiﬁed the cases in which three generations are included in the zero modes of the bulk superﬁelds. The bulk gauge symmetry is broken down to the 4D gauge symmetry which contains the SM gauge group by orbifolding, and some of non-Abelian gauged ﬂavor symmetry can also remain in 4D. For example, E8 has a subgroup SO(10)×SU(4), and thus in the SO(10) basis, there can be maximally SU(4) gauged ﬂavor symmetry. Furthermore, since there are three chiral superﬁelds, there is a global symmetry which originates from R symmetry in the bulk. If the gravity is taken into account, the R symmetry is also gauged, but we consider it as a global symmetry in a ﬂat limit. Therefore, to obtain three generations, there are cases where the gauged or global ﬂavor symmetry remains. One can assign to make SU(2) and SU(3) ﬂavor symmetries remain for both global and gauged symmetries. Also, it can be considered that the both global and gauged ﬂavor symmetry is completely broken by orbifolding. Totally, there are 5 cases to obtain three generations from the bulk ﬁelds. We investigated each case and gave examples for the discrete charge assignment of the orbifold. Due to the bulk ﬂavor symmetry, the Yukawa structure originated from the bulk gauge interaction is restricted. Many of the cases, two eigenvalues are degenerate in the limit where the couplings in brane-localized terms are zero. There are two situations in those cases: Two eigenvalues are degenerate and one eigenvalue is zero. Or, two eigenvalues are zero, and one eigenstate is massive. Since the brane-localized terms are suppressed by a factor from the volume of the extra dimension and the bulk interaction gives a dominant contribution to the Yukawa couplings, the former situation is not a good situation phenomenologically. To make a phenomenologically viable model, one needs to mix one of the eigenstate with a brane ﬁeld, and the brane ﬁeld is a light eigenstate of our quarks and leptons. In the latter situation, on the other hand, one can explain why only third generation is heavy. The two other generations can become massive by brane terms. The hierarchy between ﬁrst and second generation can be explained by the remaining ﬂavor symmetry. When the non-Abelian ﬂavor symmetry for both global and gauged symmetry is broken by orbifold, the degeneracy of eigenvalues can be resolved. In that case, the discrete charge assign ment is restricted, and higher gauge symmetry often remains in 4D. Actually, when all three quark doublets have zero modes in that case, at least SU(3)L gauge symmetry (which contains SU(2)L) remains in 4D. In other words, if the gauge symmetry is broken down to SU(2)L, all three quark doublets can not have zero modes, and at least one of the eigenvalues is zero. It can interestingly explain why the ﬁrst generation mass is tiny. We gave an example where all three generations of quarks have non-degenerate masses from the bulk Yukawa interaction in the model in which triniﬁcation symmetry remains in 4D. We also gave an example where two In conclusion, the Yukawa coupling is generated from the bulk gauge interaction when leftand right-handed matter ﬁelds as well as Higgs ﬁelds are zero modes of the bulk ﬁelds. We considered the cases where three generations of matter are contained in the zero modes. In that cases, due to the bulk ﬂavor symmetries, the structure of bulk Yukawa coupling is restricted and the hierarchy of fermion masses can be explained by a nature of extra dimensions. We obtain the discrete charge assignments for the SM decomposed representations in the E7 adjoint. The branch E7 → SU(5) × SU(3) × U(1) is useful to arrange the representations. When x3+a ≡ 0 is satisﬁed for example, SU(4)c×SU(2)L×U(1)R symmetry remains in 4D. When x3 − a ≡ 0 is satisﬁed, SU(3)c × SU(2)L × SU(2)R × U(1)B−L symmetry remains. When x3 + a ≡ 0 and 2a ≡ 0 are satisﬁed, Pati-Salam symmetry remains. When x2 ≡ x3 ≡ a and 3a ≡ 0 are satisﬁed, triniﬁcation symmetry remains. When a ≡ 0 is satisﬁed, SU(5) symmetry remains. When x3 ≡ 0 is satisﬁed, ﬂipped-SU(5) remains. When x3 ≡ a ≡ 0 is satisﬁed, SO(10) symmetry remains. When x2 ≡ x3 ≡ a ≡ 0 is satisﬁed, E6 symmetry remains. Next, let us obtain the discrete charge assignments for the SM decomposed representations in the E8 adjoint. The branch E8 → SU(5) × SU(5) is useful to arrange the representations. For the adjoint representations for GSM and SU(5), the charges are where Sij is a SU(5) adjoint representation (i = 1, 2, 3, 4, 5). The matter and anti-matter representations are given as follows: Discrete charges for the representations in (10, ¯5) and ( We thank Z. Tavartkiladze for useful discussions. This work of Y. M. is supported in part by the grant from the US Department of Energy, grant number DE-FG02-95ER40917, and the work of S. N. was supported in part by the grants from the US Department of Energy, grant numbers DE-FG02-04ER41306 and DE-FG02-ER46140.
Optics in a nonlinear gravitational plane wave<|sep|>We have derived the exact time delays (24), frequency shifts (31), sky positions (39), area distances (43), and luminosity distances (44) associated with optical observations in the presence of arbitrarily-varying gravitational plane waves. Together, these results provide a simple and non-perturbative framework with which to explore the physics of nonlinear gravitational waves. One conclusion is that the optical eﬀects associated with gravitational plane waves appear particularly simple when those waves are described in terms of ξij(u), a nonlocal variable which generalizes the familiar waveforms of TT-gauge perturbation theory. Non-perturbatively, ξij is a square root of the transverse metric γij = ξkiξkj in a Rosen-type coordinate system. It also represents the nontrivial components of a Jacobi propagator, and therefore describes separations between neighboring families of geodesics. The physical character of ξij can be understood by noting that it satisﬁes an ordinary diﬀerential equation ¨ξij = Hikξkj which also describes a set of coupled parametric oscillators. The instantaneous natural frequencies of these oscillators directly correspond to those curvature components Hij(u) = −ξ−1 ki ξ−1 lj Rukul which represent the freely-speciﬁable degrees of freedom associated with the gravitational wave. Although the equation satisﬁed by ξij is linear, its solutions depend nonlinearly on the “local waveform” Hij. It is through this nonlinearity—which is more closely connected to the geodesic equation than to Einstein’s equation—that interesting optical eﬀects can arise on large scales. Much of our discussion examines these eﬀects perturbatively. An expansion ξij = ξ(0) ij + ϵξ(1) ij + ϵ2ξ(2) ij + . . . for the metric square root is obtained in Section 4.1, where ϵ is a small parameter which controls only the overall scale of Hij. The various optical eﬀects considered non-perturbatively in Section 3 are then specialized in Section 4.2 for freely-falling sources and observers, and also expanded through second order in ϵ. Two main results emerge: i) Higher-order perturbations secularly grow at large source-observer distances, and ii) some higher-order metric perturbations produce observable eﬀects with angular dependencies which are completely diﬀerent from those associated with ﬁrst-order eﬀects. The ﬁrst of these statements can be understood directly from (56), which shows that the second-order metric perturbation involves the ﬁrst-order metric perturbation hij via a double integral of the “energy density” ˙hij ˙hij. This density is non-negative, so its integrals—and therefore the metric itself—grow wherever ˙hij ̸= 0. As a consequence, second-order optical eﬀects can be much more important on large scales than naive estimates might suggest. This can be interpreted as a kind of memory eﬀect when the curvature is signiﬁcant only for a short time (i.e., for gravitational wave bursts). Standard assumptions have long been known to imply that at ﬁrst order, bursts can exhibit a displacement-type memory, but no “velocity memory;” initially-comoving particles remain comoving after a gravitational wave has passed. We show that this picture changes at second order in the gravitational wave amplitude. The secularly-growing portion of the second-order metric physically corresponds to a ﬁnite kick imparted to initially-comoving test particles, a nontrivial velocity memory. Even a small eﬀect of this sort can produce signiﬁcant displacements over suﬃciently long times. If a particular burst is characterized as N oscillations with angular frequency ω and strain amplitude ϵ, the associated nonlinear eﬀects on observables are fractionally of order ϵ2N(ωr) over lengthscales of order r ≫ Nω−1. The analogous scaling is diﬀerent for continuous gravitational waves which maintain their amplitudes over all relevant lengthscales. Second-order eﬀects are then shown to grow like ϵ2N 2, where N ∼ ωr now denotes the number of gravitational wave cycles between a source and its observer. This number is typically of order unity or less for gravitational waves intended to be observed using standard interferometer designs such as LIGO, so nonlinear eﬀects are negligible in those cases. Much larger values of N can arise in pulsar timing, however. Consider, for example, a gravitational wave with ω/2π = 300 nHz and with an approximately constant amplitude between the earth and a pulsar where r ∼ 10 kpc. Second-order terms in this case are then ampliﬁed by N 2 ∼ 1012. Although this factor is large, multiplying it by a realistic strain magnitude results in an O(ϵ2N 2) eﬀect which would still be challenging to detect. Regardless of the precise type of gravitational wave under consideration, we show that the growing higher-order metric perturbations also have nonvanishing traces. While the familiar trace-free property of TT gauge can be extended to all odd-order metric perturbations, even orders generically acquire ﬁnite traces. This distinction physically results in diﬀerent azimuthal dependencies for the optical eﬀects associated with, e.g., ﬁrst and second-order metric perturbations. Focusing only on the aforementioned second-order terms which grow at large distances, many secondorder observables in general relativity appear similar to linear observables, but with hij replaced by an “eﬀective metric perturbation” which possesses a small nonzero trace. This trace has optical eﬀects similar to those associated with breathing-type polarization modes in other theories of gravity. It diﬀers, however, in that nonlinear perturbations in general relativity do not represent physically independent degrees of freedom; such terms are entirely determined by the ordinary + and × polarization modes. Furthermore, this eﬀect arises only for nonlocal measurements. Both distinctions can be observationally subtle, however, and might complicate eﬀorts [46] to constrain alternative theories of gravity via the presence of additional gravitational wave polarization modes. I thank Stanislav Babak for posing the questions which eventually led to this work, and also for a number of useful discussions along the way.
Analytical solution of the cylindrical torsion problem for the relaxed micromorphic continuum and other generalized continua (including full derivations)<|sep|>We have derived the analytical expressions of the torsional rigidity for a family of generalized continua capable of modelling size-dependence in the sense that more slender specimens are comparatively stiﬀer. We only consider (simpliﬁed) isotropic expressions so as to better compare the diﬀerent models with each other. For example, a strain gradient continuum, by construction, does not have mixed energy terms. Therefore, we omitted these terms in all models. Excluding the mixed terms like ⟨sym Du, sym Du − P ⟩ also simpliﬁes considerably the investigation of positive deﬁniteness. Indeed, all presented models are positive deﬁnite if the usual relations are satisﬁed together with individual positivity of all curvature parameters. In all cases, the displacement follows the classical pure torsion solution. Despite the conceptual simplicity of the models, we observe already a delicate interplay between the used kinematics and the assumed curvature energy expression. For example, let us compare the relaxed micromorphic model with the micro-strain model (Section 10). Both models have a similar looking lower order energy term (if the Cosserat couple modulus µc ≡ 0), but diﬀerent kinematics and diﬀerent curvature energy. For arbitrary slender specimens, the torsional stiﬀness of the micro-strain model is governed by µe, whereas the torsional stiﬀness of the relaxed micromorphic model is determined by µmicro. Thus, the physical interpretation of the material parameters in both models is completely diﬀerent. This is surprising at ﬁrst glance but the reason for this response is ﬁnally explained in Appendix D. In the end, the more restricted the used kinematics, the less viable a model may become. In this respect, only the full micromorphic kinematics degree of freedom (12 DOFS) can be advised. In addition, the curvature energy should not intervene too strongly. For example, penalizing a full gradient DP in the curvature energy of the classical micromorphic model leads to a stiﬀness singularity for arbitrary slender specimens, while penalizing only Curl P in the relaxed micromorphic model does not show the same singular response. Moreover, in the relaxed micromorphic model the interpretation of the lower order material parameters (µe, µmicro, µmicro, etc.) does not in principle change when diﬀerent curvature energies are considered. In the end, it is therefore the relaxed micromorphic model that produces sensible and consistent response in all considered cases. It remains to be investigated if, together with the previously developed solution for bending and shear [60, 61], the present analytical solution allows to identify the complete set of micromorphic parameters of a material from bending, shear and torsion experiments at specimens with diﬀerent diameters.
Zero-shot Visual Commonsense Immorality Prediction<|sep|>Predicting immorality from images is of paramount importance regarding social safety. In this work, we ﬁrst utilized CLIP-based text-image joint embedding space and trained a (textbased) commonsense immorality classiﬁer. Given these, we then predicted visual commonsense immorality from an unseen image in a zero-shot manner. Using seven benchmarks in image classiﬁcation, we demonstrated that our model successfully estimates visual commonsense immorality. Our analysis with the XD-Violence dataset also showed consistency in its prediction. In fact, we observed that widely-used image classiﬁcation benchmarks, such as ImageNet, contain immoral visual scenes, potentially negatively impacting the trained model’s behavior. Further, we created a new Visual Commonsense Immorality benchmark, a more general image benchmark toward commonsense immorality. We hope our paper could be an initial point in discussing the importance of visual commonsense immorality towards ethical AI. Acknowledgements. This work was supported by Institute of Information & communications Technology Planning & Evaluation (2022-0-00043,Adaptive Personality for Intelligent Agents) and ICT Creative Consilience program (IITP-2022-2022-0-01819).
X-Vector based voice activity detection for multi-genre broadcast speech-to-text<|sep|>This study introduces a novel x-vector based VAD system.  We show that x-vectors extracted from broadcast audio and  everyday noise retain latent information that discriminates  speech from noise. The binary classifier trained with xvectors (x-vector-vad) achieves the best reported score in  detecting clean speech on the AVA-Speech dataset, whilst  retaining scores for detecting speech in the presence of noise  and music comparable with previously reported best scores.  Importantly, the x-vector-vad model improves the  accuracy of the STT system over the baseline system with  WebRTC VAD on the multi-genre TV dataset (MGB) and an  internal radio dataset. We conclude that x-vector-vad is a  robust and accurate VAD system which enhances the quality  of downstream STT transcription on a diverse range of  broadcast audio materials. Table 4: Evaluation results of Kaldi-based STT system with baseline (WebRTC VAD) and proposed (x-vector-vad)  VAD algorithms on the MGB eval.std and bbc-radio-eval datasets. #TOT: total number of words, #ERR: total number  of errors (#INS + #DEL + #SUB), #INS: number of insertion errors, #DEL: number of deletion errors, # SUB: number of  substitution errors, %WER: percentage word error rate (#ERR / #TOT * 100). Best %WER scores are in bold.
Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems<|sep|>In this work we have formulated the jamming problem as a general constraint satisfaction problem with continuous variables (Sec. II). We have then specialized on the random perceptron, a well known machine learning model, which is a prototype of this class of problems (Sec. III). In the non-convex regime, the model shows a complex zero temperature phase diagram in the plane of the two control parameters (α, σ), which has been fully characterized in Sec. IV. In particular, we have shown that for σ < 0 and large enough |σ|, the phase diagram as a function of α shows the characteristic phenomenology associated with the Random First Order Transition (RFOT) mean ﬁeld
Credulous Users and Fake News: a Real Case Study on the Propagation in Twitter<|sep|>Nowadays Online Social Media are very important and the channel of information, preferred by people, especially by youngsters, to traditional media, like newspapers, radio and television. Their pervasiveness, favoured by the widespread use of mobile devices, and people’s compulsive check of their social proﬁles has enabled faster news dissemination and a wider audience. This has brought one of the bigger problem of our time, misinformation. The absence of any control of the news published on OSM, sort of common in newspapers, has stimulated production and diﬀusion of fake news. In many cases, this is done using automated accounts, called bots, which actively interact with people to induce bias in their opinion, generate misconception, incite hate-speech, etc. Current approaches, which eﬀectively counter malicious bots activities, are based on their detection and removal from OSM. But whenever a bot is removed, it is easy to introduce new ones that are able to deceive detectors, giving rise to an arms race between botnet masters and OSM’s administrators. Inspired and stimulated by recent literature, that stresses the susceptibility of humanoperated accounts to activities of malicious bots, we have studied the relationship between fake news and so called credulous Twitter users, i.e., human-operated accounts following a high percentage of bots over their social contacts. Starting by a publicly available dataset of fake and real news (concerned with politics and gossips) and by using bots and credulous detectors, we provided evidence of the actual involvement of credulous users in the diﬀusion/production of fake news. The experimental results showed that, regardless of the news topic, credulous users tweet a larger number of fake news than not-credulous one. Although this superiority is also conﬁrmed for the number of tweets containing real news, it is worth saying that: (i) real news are harmless and (ii) the number of such tweets is still lower than those containing fake news. Furthermore, we observed the numerical participation of users, counting how many of each category have tweeted fake and/or real news; in this case we noticed a discordance depending on the news’s topic. Credulous who have tweeted true political news, are in greater numbers than those who have published fake news, unlike the case of gossip news. But either way, the number of credulous users tweeted fake news is always higher than the number of not-credulous users. Because of this, we are pretty sure about the contribution of credulous detection techniques for improving fake news detectors; they would make it possible to focus on the content posted by credulous users using NLP and text mining techniques. We believe that the study of this category of users can help researchers to better understand misinformation and users’ polarization phenomenons and can give an extra edge to ﬁght the propagation of fake news. It is worth to notice that the application of this kind of approach, that focuses on credulous users, is dependent from the classiﬁcation performance of the adopted bot and credulous detectors. Among the possible future work research directions, we plan to check if the ﬁndings of this work are conﬁrmed by using other fake news datasets and apply fake news detection approaches (based on NLP and content inspection) to credulous users’ tweets.
EvoGAN: An Evolutionary Computation Assisted GAN<|sep|>In this work, we propose an evolutionary algorithm assisted GAN framework called EvoGAN to generate various compound expressions. Speciﬁcally, we transfer the compound expression synthesis into an optimization problem and use EA to search for the optimal. GAN and FER are used as the ﬁtness function to evaluate the individuals in EA population. The experimental results demonstrate the feasibility and the potential of EvoGAN. Moreover, all our experiments are performed based on GANimation and VGG-19, which are not sort-of-the-art GANs or FERs and they do not ﬁts our model well. EvoGAN framework can also be embedded by many other GAN and FER variants and some changes can be made to advance the whole model. However, since the initial purpose of this paper is applying to some psychological researches or making compound expression datasets, in applications that more close to daily life, it might be sufﬁcient to use models like GANimation instead of EvoGAN. Moreover, compared with these models, though EvoGAN can use pre-trained models, EvoGAN costs pretty more time than these models when generating images. For example, If the population size is 50 and generation is 50, EvoGAN would generate 2500 images using GANimation in all. But for doing researches, we can afford to spend more time to generate compound expression and to maintain diversity. Our future work includes:
Detection of a pair of prominent X-ray cavities in Abell 3847<|sep|>Using the deep Chandra observation and 4.89 GHz VLA radio map, we investigated the properties of X-ray deﬁcient regions, shocks and other substructures in the ICM of A3847 cluster. The results obtained from our study are summarized as follows. 1 A pair of giant X-ray deﬁcient cavities along North and South directions were clearly detected in the residual images of the cluster. 2 X-ray and 4.8 GHz radio images revealed the peculiar positioning of the cavities and radio bubbles in A3847. The radio lobes and X-ray cavities are apparently not spatially coincident and they exhibit offset, by ∼ 61 kpc and ∼ 77 kpc from each other along the North and South directions, respectively. 3 Presence of shock is detected in the ICM of cluster A3847. 4 Using results obtained from the radial thermodynamical proﬁles, it is conﬁrmed that the radio bubbles are responsible for removing substantial amount of matter from the centre of the cluster. 5 The AGN feedback power that heats up the ICM was estimated to be ∼ 6.3 ×1044 erg s−1 and is sufﬁciently large to offset the cooling (Lcool ∼ 8.30 × 1043 erg s−1 ) of the ICM in A3847 cluster. 6 The ratio of estimated accretion rate to the Eddington rate and The authors are grateful to the anonymous referee for constructive comments and suggestions on the paper. The authors thank A. R. Rao for careful reading of the manuscript. This work has made use of data from the Chandra, VLA and Gemini-South archive, NASA’s Astrophysics Data System(ADS), Extragalactic Database (NED), software provided by the Chandra X-ray Center (CXC), HEASOFT for spectral ﬁtting and Veusz plotting tools.
Exact calculations of first-passage properties on the pseudofractal scale-free web<|sep|>We have proposed a general method to calculate exactly the ﬁrst passage probability for random walks on the PSFW in the presence of an absorbing domain located at some nodes of high coordination. From the knowledge of the ﬁrst passage probability one can derive a detailed description of the problem, getting, for instance, the survival probability, the mean and the variance of ﬁrst passage time. We calculated explicitly results of the ﬁrst passage properties for some illustrative examples, corresponding to the case that the absorbing domain is located at one of the three main hubs and to the case that there are two absorbing nodes located at two nodes among those with the second largest degree. In all these cases we evidenced that the variance of the ﬁrst passage time scales quadratically with the mean itself. Of course, the method proposed here is also suitable for other cases where the absorbing domain is located at one or more nodes of low-degree on the PSFW. The method can also be used on other self-similar graph such as (u, v) ﬂower, T-graph, recursive fractal scale-free trees, the recursive non-fractal scale-free trees and etc. This work was supported by the scientiﬁc research program of Guangzhou municipal colleges and universities under Grant No. 2012A022. ZZZ was supported by the National Natural Science Foundation of China under Grant No. 11275049. (A1) where x ∼ y means that there is an edge between x and y and dx is the degree of node x. Therefore we can calculate the probability generating function of the ﬁrst passage time directly by where Φ(z) = (φxy(z))6×6 and φxy(z) is the probability generating function of passage time from node x to y. But if y is an absorbing node, φxy(z) is just the probability generating function of ﬁrst passage time from node x to y. Exact calculation of θ(1, z). Let two hubs B and C be the absorbing nodes. Then, all coordinates of the second and the third row of the transition probability matrix Π are assigned 0. That is
Flux: FunctionaL Updates for XML (extended report)<|sep|>The problem of updating XML databases poses many challenges to language design. In previous work, we introduced FLUX, a simple core language for XML updates, inspired in large part by the language CPL+ introduced by Liefke and Davidson (1999) for updating complex object databases. In contrast to all other update proposals of which we are aware, FLUX preserves the good features of XQuery such as its purely functional semantics, while offering features convenient for updating XML. Moreover, FLUX is the ﬁrst proposal for updating XML to be equipped with a sound, static type system. In this paper we have further developed the foundations of FLUX, relaxing the limitations present in our preliminary proposal. First, we have extended its operational semantics and type system to handle recursive types and updates. This turned out to be straightforward. Second, although the FLUX core language is easy to understand, typecheck and optimize, it is not easy to use. Therefore, we have developed a high-level source language for updates, and shown how to translate it to core FLUX. Since it is difﬁcult to propagate useful type error information from translated updates back to source updates, we have also developed a type system for the source language, and validated its design by proving that the translation both preserves and reﬂects typability. Third, we developed a novel deﬁnition of update path-errors, a form of dead code analysis, and introduced a static analysis that identiﬁes them. At present we have implemented a proof-of-concept prototype FLUX interpreter, including typechecking for the source language and core language, normalization, and path-error analysis. There are many possible directions for future work; the most immediate is to develop efﬁcient optimizing implementations of FLUX within existing XML databases or other XML-processing systems.
Resonant cyclotron scattering in pulsar magnetospheres and its application to isolated neutron stars<|sep|>We consider the RCS process in pulsar magnetospheres. The photon diffusion equation (Kompaneets equation) for RCS is presented. It can produce not only up scattering but also down scattering depending on the parameter space. Its possible applications to magnetar soft X-ray spectrum and INSs are point out. The application to INSs is calculated in detail. We show that the optical/UV excess of INSs may be due to down scattering of RCS. The RCS model has the same number of parameters as the double blackbody model. Mean while, it has a clear physical meaning. The initial blackbody spectrum from the stellar surface is down scattered by the RCS process when passing through its magnetosphere. This can account for the optical/UV excess of INSs. The low pulsation amplitude of INSs is a natural consequence in our model. The calculations for RX J1856.5-3754 and RX J0720.4-3125 are presented and compared with their observational data. The model parameters for RX J1856.5-3754 and RX J0720.4-3125 are similar. This may in part reﬂect the similarities between these two INSs. Finally, we point out that the quark star hypothesis (e.g. Xu 2002) can still not be ruled out. The photon diffusion equation (Kompaneets equation) for RCS is calculated semi-analytically. The calculations for the magnetar and INS cases are all for surface thermal emission. Of course, its application is not limited to the thermal emission case. Using the Kompaneets equation (both the resonant and non-resonant ones, or a uniﬁed one which will be presented in the future), a thorough and quantitative study of scattering process in pulsar magnetospheres could be possible. This can help us make clear the physical process in CFLRs of pulsar magnetospheres. The authors would like to thank van Kerkwijk very much for providing the observational data. H.T. would like to thank Yue You Ling, Liu Dang Bo for helpful discussions. H.T. would like to thank
Composite system in noncommutative space and the equivalence principle<|sep|>In this Letter we have examined the motion of a composite system made of N particles in a noncommutative space. The two-body problem has been considered. We have introduced the coordinates of the center-of-mass, the total momentum, the coordinates and the momentum of relative motion. Therefore, the two-body problem has been reduced to an equivalent one-body problem. An eﬀective parameter of noncommutativity has been introduced to describe motion of a composite system where i and j run over 1, 2, ..., N. It has been shown that in the case of m1 = m2 = ... = mN and θ1 = θ2 = ... = θN = θ the eﬀective parameter of noncommutativity is given by ˜θ = θ/N. So, the value of reduction of the eﬀective parameter with respect to the parameter of noncommutativity for the individual particles depends on the number of particles in the system. Therefore, the upper bounds of the parameter of noncommutativity obtained for the macroscopic body (the Mercury planet) have been re-examined. We have concluded that the coordinates of the center-of-mass and relative motion do not commute in noncommutative space. So, they are not independent. Note, however, that we can avoid an inﬂuence between the motion of the center-of-mass and the relative motion in the case of where γ = const. It has been shown that the same condition (72) gives the possibility to solve the problem of violation of the equivalence principle in noncommutative space. Namely we have shown that the equivalence principle is recovered on condition (72). Furthermore, expression (72) has been derived from the condition of the independence of kinetic energy on the composition. So, at least three problems caused by the coordinates noncommutativity can be solved by assuming that the parameter of noncommutativity is determined by the mass of a particle (72). It is important to note that the γ constant has a time dimension. In order to estimate the value of it let us suppose that the parameter of noncommutativity for the electron θe satisﬁes the following relation � here λe = h/mec = 2.43 · 10−12m is Compton wavelength of the electron and Tp = 5.4 · 10−44s is the Planck time. In this case the parameter of noncommutativity θi for a particle of mass mi reads So, the value of reduction of the parameter of noncommutativity for a particle of mass mi with respect to the parameter of noncommutativity for the electrons depends on the ratio me/mi. Note, that in the case of mi = mnuc we ﬁnd ¯hθnuc = L2 p/1840. Acknowledgments The author thanks Professor V. M. Tkachuk for his advices and great support during research studies.
Ghost-vibrational resonance<|sep|>In a linear system driven by a single periodic force the output contains only the frequency present in the driving force. The response of a nonlinear system to a sinusoidal signal with a single frequency contains the input frequency and its harmonics. When a linear system is subjected to a multifrequency force, the frequencies present in the output are the same as those in the input. However, changes occur in the magnitudes and phases of the various frequency components. In the case of a nonlinear system driven by a multi-frequency signal, the response not only contains the harmonics of the various input frequencies but inter-modulation components of harmonics can also be generated. In the present work we have shown the enhancement of response amplitude of a nonlinear system at the missing fundamental frequency in the input multi-frequency signal. In the nonlinear system driven by multi-frequency force and noise, resonance at the missing fundamental frequency is the dominant one and it occurs at a relatively lower value of the noise intensity compared to the resonance at the frequencies present in the input signal. For these types of resonance to occur, the system must have a bistability or excitability. High-frequency induced ghost resonance can occur even in single-well nonlinear systems. As shown in Fig. 1, the diﬀerence between, for example, Q(2ω0) when the input signal contains only the frequency 2ω0 and its value when other frequencies are also present in the input signal is negligible. That is, the response amplitude at a frequency ω present in the input signal is not aﬀected appreciably by the presence of the other frequencies Ωi, as long as Ωi are not widely separated from ω. If any of the Ωi ≫ ω, then the vibrational resonance at the frequency ω occurs. When the input signal contains only a very few number of periodic components, then it is easy to obtain an analytical expression for the amplitudes of the periodic components with various frequencies. For the single oscillator, Q(ω0) decays with the parameters k and ∆ω0, while it reaches a saturation with the number of forces. In the network where the oscillators are coupled unidirectionally and only the ﬁrst oscillator is driven by the external forces, an enhanced and undamped signal propagation at the missing fundamental frequency takes place above a certain critical value of the coupling strength even in the absence of high-frequency force. Moreover, Qi(ω0) becomes constant for suﬃciently large values of i. Finally, an interesting result is that, the limiting value of Q is independent of the values of g, k and ∆ω0. S. Rajamani expresses her gratitude to University Grants Commission (U.G.C.), Government of India for ﬁnancial support in the form of U.G.C. meritorious fellowship. MAFS acknowledges ﬁnancial support from the Spanish Ministry of Science and Innovation under Project No. FIS2009-09898.
GRB 110530A: Peculiar Broad Bump and Delayed Plateau in Early Optical Afterglows<|sep|>We have reported our very early optical observations for GRB 110530A and investigate its jet properties together with its X-ray afterglow data. A broad bump with signiﬁcant ﬂares is observed in the optical lightcurve at t < 2000 seconds, which is followed by a plateau with transition to a normal decaying segment at t = 3000 seconds. The X-ray afterglow lightcurve shows almost the same feature. Our joint spectral ﬁts of the optical and X-ray data show that they are in the same regime, with a photon index of ∼ 1.70. The extinction of the host galaxy is negligible, but the equivalent hydrogen column density of host galaxy is approximately 1.0×1021cm−2. We model the optical and X-ray lightcurves with the standard external shock model by considering delayed energy injection and assuming its redshift as 1. Our best parameters derived from a MCMC approach are Γ0 = 91 ± 8, ǫe = 0.086 ± 0.008, ǫB = (1.64±0.25)×10−6, n = 13.3±2.6 cm−3, EK,iso = (2.28±0.27)×1053 erg, and θj ∼ 0.15 rad. The energy injection can be described as Lin/1050erg s−1 = (4.0 ± 2.5) × t−0.18, which starts at ∼ 2390 seconds and lasts only about 700 seconds. Based on our modeling results, the radiative eﬃciency of the GRB ﬁreball is ∼ 1%, the magnetic ﬁeld strength and the magnetization parameter of the afterglow jet are B = 0.165 G and σ < 0.04, respectively. We propose that the jet would be matter dominated and possible sources of the delayed energy injection are also discussed. The most striking observation of GRB 110530A is its early broad bump following by a plateau in its R band afterglow lightcurve. We have shown that the standard forward shock model with a delayed injection can roughly ﬁt the global feature of the lightcurves. We address the ﬂickerings in the optical and X-ray lightcurves as superimposed ﬂares that may have internal origins. We should note that these ﬂickering, especially the signiﬁcant ﬂickering at around 3000 seconds in the X-ray band, may be also due to the delayed energy injection. Zhang & M´esz´aros (2001) analyzed the energy injection and corresponding signature that could be shown up in afterglow lightcurves. They showed that injection by a Poynting-ﬂuxdominated shell that has an energy comparable to that of the initial ﬁreball would lead to a gradual achromatic bump. In the case when the injection is kinetic-energy-dominated, the results depend on the situation of the collision between the injected (rear) shells and initial (leading) shells. If the collision is mild, the signature showed in the lightcurves may be analogous to the Poynting-ﬂux-dominated injection case. In case of a violent collision a signiﬁcant ﬂare-like bump may be observed (see Figure 5 of Zhang & M´esz´aros 2001). In the case that the delayed energy injection is fed by the fall-back materials, the delayed energy would also cause a notable rise to the Lorentz factor of the external shock, which will generate a bump in the multiple band afterglows as seen in GRB 081029 and GRB 100621A (Nardini et al. 2011; Greiner et al. 2013; Geng et al. 2013).
Deep Learning for Procedural Content Generation<|sep|>The work surveyed in this paper is the result of two convergent trends from the last few years. One is the increasing use of deep learning for generative tasks in non-game contexts, such as GANs and VAEs used for generating pictures of faces and RNNs used for generating voices and music. The other is the increasing use of machine learning in PCG, something that was unheard of until ﬁve years or so ago. Both of these trends build on the deep learning revolution itself, which has made machine learning eﬀective on completely new classes of problems. As a result, interest in deep learning for PCG has exploded. Examples abound, as our survey shows. It is very likely that we will see rapid progress in this research direction in the near future. This survey paper attempts to contribute to this progress by surveying and systematizing this work and implicitly and explicitly pointing out relevant and fertile research problems. We believe that this is a very timely eﬀort given the exciting pace of this ﬁeld. Deep learning methods have been applied alone or in collaboration with other PCG methods to generate game content and to analyze, play and experience content. Due to the characteristics of diﬀerent types of content, diﬀerent types of deep neural architectures have been used. Among the reviewed work, the widely used neural architectures include convolutional neural networks for supervised learning tasks, varying from generating texture or music for target emotion to predicting game outcomes or diﬃculty rate; long shortterm memory for generating sequential data like charts for rhythm and narrative or for predicting action sequences; deep variational autoencoders, mostly used for generating level maps and sometimes for classifying NPCs’ or players’ behaviors; and generative adversarial networks for creating image-like content (e.g., level maps, landscapes, faces and sprites). A part from the direct use of deep learning methods or their alliance with evolutionary computation to generate game content, they have also been used for evaluating content and content generators in an indirect manner. Although a variety of game content (e.g., levels, text, character models, textures, music and sound) have been investigated, the generation of content like event, goals or character features with skill-depth can be exploited more. As a future research, evolving or training game-playing agents and content generators in parallel, such as in the recent work of Dharna et al. [19], is of great interest, as well as the generalization across games. Besides those, online generation of game content to adapt players’ skill and preferences in real-time will accelerate the realization of personalized games. Acknowledgements J. Liu was supported by the National Natural Science Foundation of China (Grant No. 61906083), the Guangdong Provincial Key Laboratory (Grant No. 2020B121201001), the Program for Guangdong Introducing Innovative and Entrepreneurial Teams (Grant No. 2017ZT07X386), the Science and Technology Innovation Committee Foundation of Shenzhen (Grant No. JCYJ20190809121403553), the Shenzhen Science and Technology Program (Grant No. KQTD2016112514355531) and the Program for University Key Laboratory of Guangdong Province (Grant No. 2017KSYS008). S. Risi was supported by a Google Faculty Research award and a Sapere Aude:DFFStarting Grant. A. Khalifa and J. Togelius acknowledge the ﬁnancial support from National Science Foundation (NSF) award number 1717324 - “RI: Small: General Intelligence through Algorithm Invention and Selection”. G. N. Yannakakis was supported by European Union’s Horizon 2020 AI4Media (951911) and TAMED (101003397) projects. This is a pre-print of an article published in Neural Computing and Applications. The ﬁnal authenticated version is available online at: https://doi.org/10.1007/s00521-020-05383-8. S. Snodgrass, S. Risi, G. N. Yannakakis, and J. Togelius declare that they a ﬁnancial interest in modl.ai, which develops AI technologies for games.
Demonstrating Delay-based Reservoir Computing Using a Compact Photonic Integrated Chip<|sep|>We have studied the performance of a delay-based reservoir computer, which is designed on a photonic integrated chip. The integrated approach leads to a compact design as well as high computation speeds. We have studied the performance through the Santa-Fe timeseries benchmarking task and we calculated the memory capacity. With the conventional reservoir computing scheme, where the mask-imposed nodes coincide with the virtual nodes, we get a performance (best NMSE = 0.135) which is slightly worse than those found in other works (NMSE around 0.1). However, we are working in diﬀerent regimes. While previous works, such as [9,10,12,14], operate in sub- or near threshold regimes, we operate our laser at pump currents well above the threshold current. We achieve a signiﬁcant speed up compared to others [7,9], who achieved speeds in the order of kSa/s and MSa/s respectively. The computation speed of our setup is 0.87 GSa/s, which is comparable to what Takano et al. achieved with additional pre- and post-processing steps. We were able to improve the performance of the reservoir computer by using diﬀerent postprocessing routines. The ﬁrst routine is using both readout samples within one mask-imposed node to form the output layer, unlike the conventional routine where we utilize one sample per mask-imposed node. The availability of extra states in the output layer, causes the reservoir computer to perform better. The extra states are not redundant in comparison with the rest, but rather enhance the state space. Since the mask-imposed node has a slightly longer duration than the timescale of the laser, we get two diﬀerent state values from the transient response on the input. The best performance we achieved here is NMSE = 0.062. The second post-processing routines takes the reservoir output for a duration of two delay times. This way we have a richer state space to perform the task and furthermore have access to a longer temporal memory inside this state space, since the last two input data points are present in the two delay times. This post-processing routine has consistently been the best performing out of the three and reaches an NMSE as low as 0.049. We have seen that the best performance for Santa Fe timeseries prediction was found when we the injected signal’s wavelength was close to a side-mode, with zero detuning between the injected wavelength and side-mode. We also observed that delay-based RC using semiconductor lasers can achieve very good performances at pump currents well above threshold, where most studies have focused on near-threshold operation. Lastly, we studied the memory capacity of our RC setup as the feedback in the setup is increased and we see a clear increase. Even when the SOAs in the delay line are turned oﬀ, we get a linear memory capacity around 8, which suggests that there is enough feedback already in the system without extra ampliﬁcation. Research Foundation Flanders (FWO) (G028618N, G024715N, G029519N); EU Horizon 2020 PHRESCO Grant (688579); EU Horizon 2020 Fun-COMP Grant (780848); BELSPO IAP P7-35 program Photonics@be; Hercules Foundation; Research Council of the VUB. References 1. G. Van der Sande, D. Brunner, and M. C. Soriano, “Advances in photonic reservoir computing,” Nanophotonics 6, 561–576 (2017). 2. K. Vandoorne, W. Dierckx, B. Schrauwen, D. Verstraeten, R. Baets, P. Bienstman, and J. Van Campenhout, “Toward optical signal processing using photonic reservoir computing,” Opt. express 16, 11182–11192 (2008). 3. K. Vandoorne, J. Dambre, D. Verstraeten, B. Schrauwen, and P. Bienstman, “Parallel reservoir computing using optical ampliﬁers,” IEEE transactions on neural networks 22, 1469–1481 (2011). 4. K. Vandoorne, P. Mechet, T. Van Vaerenbergh, M. Fiers, G. Morthier, D. Verstraeten, B. Schrauwen, J. Dambre, and P. Bienstman, “Experimental demonstration of reservoir computing on a silicon photonics chip,” Nat. communications 5 (2014).
Imprints of feedback in young gasless clusters?<|sep|>We have taken the end states of ﬁve pairs of hydrodynamical simulations (Dale et al. 2012, 2013) and evolved them as Nbody simulations for 10 Myr to look for diﬀerences between the clusters in the simulations which formed under the in
$(O,G)$-granular variable precision fuzzy rough sets based on overlap and grouping functions<|sep|>In this paper, a new type of fuzzy rough set model on arbitrary fuzzy relations was deﬁned by using overlap and grouping functions, which called (O,G)-GVPFRSs. Meanwhile, we gave two equivalent expressions of the upper and lower approximation operators applying fuzzy implications and co-implications, which facilitate more eﬃcient calculations. In particular, some special conclusions were further discussed, when fuzzy relations and sets degenerated to crisp relations and sets. In addition, we characterized the (O,G)-GVPFRSs based on diverse fuzzy relations. Finally, the richer conclusions about (O,G)-GVPFRSs were gave under some addtional conditions. In general, this paper further explored the GVPFRSs from a theoretical perspective based on overlap and grouping functions. This research was supported by the National Natural Science Foundation of China (Grant nos. 11901465, 12101500), the Science and Technology Program of Gansu Province (20JR10RA101), the Scientiﬁc Research Fund for Young Teachers of Northwest Normal University (NWNU-LKQN-18-28), the Doctoral Research Fund of Northwest Normal University (6014/0002020202) and the Chinese Universities Scientiﬁc Fund (Grant no. 2452018054).
Bridging the gap between paired and unpaired medical image translation<|sep|>Our results show that CT→MR and MR→CT translation with unpaired CT and MR, using the MR and MRCAT pairs as an auxiliary supervision, produces more realistic translated CT and MR images. This additional supervision reduces artifacts and improves alignment between the input and the translated images. The proposed pix2pixM→C and pix2pixC→M, outperformed the baseline pix2pix, pix2pixHD and CycleGAN, in terms of FID and KID scores. pix2pixM→C and pix2pixC→M, like other GAN-based methods, may be useful in producing realistic-looking translated images for research purposes. Since these methods can hallucinate features in images [5], they require extensive validation of their image quality and ﬁdelity before clinical use. It remains as task for future research to develop improved translation methods and design quantitative metrics which better capture the quality of translations.
Enhancing the German Transmission Grid Through Dynamic Line Rating<|sep|>In this paper, we evaluate the effect of DLR on two study cases, namely the German power system in a historical setup for the year 2019 and in a future setup for the year 2030. We show that DLR offers a considerable and feasible complement to transmission capacity expansion. For the 2019 scenario, we observe cost savings of over 403 mil Euro due to better wind power integration and a shift away from fossil energy carriers. The 2030 study case shows that an implementation of DLR with a parallel expansion of renewable power saves 908 mil Euro of annual capital and operational system costs. It also reduces the need for hydrogen storage and fuel cells, which must be widely deployed for a renewable energy supply of 80% targeted by 2030. This reduction is mainly due to a better integration of offshore wind power and less transmission congestion. We conclude that in urgent need for decarbonizing the German power system, DLR is a viable complement to the current transmission capacity expansion, not only increasing the total welfare but also reducing the grid congestion.
Atmospheric gas dynamics in the Perseus cluster observed with Hitomi<|sep|>1. We have resolved and measured the line widths of He-like and H-like ions of Si, S, Ar, Ca, and Fe
Bounded Cohomology of Groups acting on Cantor sets<|sep|>A.1. Topological properties of the Cantor set . . . . . . . . . . . . . . . . . . 62 B.1. Filters and Ultraﬁlters . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 B.2. Limits along Filters and Ultralimits . . . . . . . . . . . . . . . . . . . . 69 1. Iterative action of the dissipator γi on Xi on the top; as above, together with the action of φ(g) on the bottom . . . . . . . . . . . . . . . . . . . 25
Protein Function Prediction Based on Kernel Logistic Regression with 2-order Graphic Neighbor Information<|sep|>A 2-order graphic neighbor information extraction method is proposed for PPI  prediction, and the chi–square based feature combination is also involved to improve  the prediction accuracy. To demonstrate its effectiveness in one function prediction,  LR, DKLR, PKLR and RBF KLR are involved in protein function prediction. The  experimental results show that RBF KLR can achieve high average overall percentage  value for PPI especially with our two new 5-top chi–square based 2-order graphic  neighbor combination features. The future work will focus on applying the graphic  features and kernel logistic regression models to unknown protein function prediction  and liver cancer microRNA network discovery .     Acknowledgements   This project was supported by 863 Project of China (2008AA02Z306). The author  would like to thank Professor Minping Qian for valuable discussion, MS. Shuang Hu  for typing some tables, MS. Kang Jin for data preparation, discussion and some  analysis.
Learning Permutations with Sinkhorn Policy Gradient<|sep|>We introduced the SPG algorithm for solving combinatorial optimization problems involving permutations. Our algorithm is able to solve all tasks under consideration reasonably well and demonstrated its ability to learn strong representations for policies over permutations. A current limitation of SPG is that generalizing to different problem sizes requires retraining. This can partially be addressed with an inductive graph-embedding layer [10] that removes the embedding layer’s dependency on the dimension of the problem size, possibly allowing for positive transfer to larger problem sizes. We note that there are many other ways to potentially improve SPG’s performance that are orthogonal to the contributions of this paper, such as incorporating prioritized experience replay [31] or parameter noise exploration [29].
Testing the Manifold Hypothesis<|sep|>We developed an algorithm for testing if data drawn from a distribution supported in a separable Hilbert space has an expected squared distance of O(ϵ) to a submanifold (of the unit ball) of dimension d, volume at most V and reach at least τ. The number of data points required is of the order of and the number of arithmetic operations and calls to the black-box that evaluates inner products in the ambient Hilbert space is
Communication-Efficient Network-Distributed Optimization with Differential-Coded Compressors<|sep|>algorithm for communication-efﬁcient network-distributed optimization. The key features of our DC-DGD algorithm include: i) DC-DGD works with general compression schemes that are only constrained by SNR (signal-to-noise ratio); ii) By exchanging the differentials between successive iterations (hence the name differential-coded), the DC-DGD algorithm converges at the same O(1/t) rate as the original DGD; ii) DCDGD enjoys the same low-complexity algorithmic structure as the original DGD algorithm and does not require additional mechanisms to tame compression noise thanks to its self compression noise reduction effect. Based on the above theoretical insights, we proposed a new family of hybrid SNR-constrained compressors that integrate sparsiﬁer and ternary operators. We showed that our hybrid compressor has a controllable SNR-threshold and offers a systematic framework to minimize communication costs. Moreover, by leveraging the special problem structure, we developed an efﬁcient greedy algorithm to reduce the communication cost.
Dialectics of Knowledge Representation in a Granular Rough Set Theory<|sep|>In this research we have developed a new general rough set theory over proto transitive relations, the representation of rough objects and deﬁnite objects. The knowledge interpretation of rough sets is also generalized to provide sensible knowledge interpretations across diﬀerent semantic domains. This paves the way for possible semantics, measures and/or logics of knowledge consistency. We have also shown in concrete terms that granular knowledge interpretation and classical knowledge interpretation are very diﬀerent things.
Characterizing (Un)moderated Textual Data in Social Systems<|sep|>In this work, we provide an in-depth quantitative analysis of moderated data from Twitter and unmoderated data from Gab, a social network that received several criticisms regarding the content shared on it. We perform linguistic analysis and conduct an investigation of hateful posts and the various shades of hate that are displayed in this right-leaning echo chamber. Our analysis on Gab, put into perspective with Twitter, showed that the unmoderated posts on Gab present more negative sentiment, higher toxicity, and different psycholinguistic features. Our ﬁndings support that unmoderated environments may have proportionally more hate speech. Furthermore, we categorize hate speech and its different forms in both environments, unveiling a highly toxic discourse in Gab in the many forms that hate speech can manifest itself. Our work makes an important step towards the development of automated hate speech detectors. We believe that an unmoderated hate dataset7, as the one analyzed, can help the development of hate speech detection approaches in future works. For this reason, our ﬁnal contribution consists of making our hate-labeled Gab posts available to the community.
Direct QR factorizations for tall-and-skinny matrices in MapReduce architectures<|sep|>If numerical stability is required, the Direct TSQR method discussed in this paper is the best choice of algorithm. It is guaranteed to produce a numerically orthogonal matrix. It usually takes no more than twice the time of the fastest, but unstable method, and it often outperforms conceptually simplier methods. It is also orders of magnitude faster than the Householder QR method implemented in MapReduce. All of the code used for this paper is openly available online, see: This software runs on any system supporting Hadoop streaming, including cluster management systems like Mesos [12]. In the future we plan to investigate mixed MPI and Hadoop code. The idea is that once all the local mappers have run in the ﬁrst step of the Direct TSQR method, the resulting Ri matrices constitute a much smaller input. If we run a standard, in-memory MPI implementation to compute the QR factorization of this smaller matrix, then we could remove two iterations from the direct TSQR method. Also, we would remove much of the disk IO associated with saving the Qi matrices. We believe these changes would make our MapReduce codes signiﬁcantly faster. Acknowledgment Austin Benson is supported by an Oﬃce of Technology Licensing Stanford Graduate Fellowship. Many implementation optimizations were done by Austin Benson for the CS 267 (instructed by James Demmel and Kathy Yelick) and Math 221 (instructed by James Demmel) courses at UC-Berkeley. Thanks to the team at NERSC, including Lavanya Ramakrishnan and Shane Canon, for help with MapReduce codes on the Magellan cluster. David F. Gleich is supported by a DOE CSAR grant. Research supported by Microsoft (Award #024263) and Intel (Award #024894) funding and by matching funding by U.C. Discovery (Award #DIG07-10227). Additional support comes from Par Lab aﬃliates National Instruments, Nokia, NVIDIA, Oracle, and Samsung. Research is also supported by DOE grants DESC0003959 and de-sc0004938. We are grateful to Stanford’s Institute for Computational and Mathematical Engineering for letting us use their MapReduce cluster for these computations. We are grateful to Paul Constantine for working on the initial TSQR method and for continual discussions about using these routines in simulation data analysis problems.
Gamma-ray Flaring Emission in Blazar OJ287 Located in the Jet >14 pc from the Black Hole<|sep|>The two 0.1–200GeV ﬂares in OJ287 allow us to assess the correspondence between γ-ray and lowerfrequency variations. We ﬁnd that two kinds of events at millimeter wavelengths are related to these γ-ray outbursts at high signiﬁcance: (1) the early, rising
Evidence for Warped Disks of Young Stars in the Galactic Center<|sep|>Our latest sample of early-type stars between 0.8” and 12” from the Galactic Center contains 90 WR/O stars. The most important results are: • the clockwise disk shows a large warp (in total (64 ± 6)◦, at a signiﬁcance level of > 10 sigma), the disk angular momentum direction is a function of the projected distance. • the clockwise disk is not locally ﬂat, the inclinations of the clockwise system are compatible with a two-dimensional Gaussian distribution with a sigma of 10◦ • 20% of the WR/O stars are candidate members of a coherent feature amongst the counter-clockwise stars (pre-trial signiﬁcance of 4.5 sigma for the interval of projected distances between 3.5” and 7”, corresponding to a 2% chance that this feature is a statistical ﬂuctuation in an isotropic cusp star distribution). • the counter-clockwise system may be a disk in a dissolving state: 10 out of 29 counterclockwise rotating stars have angular separations larger than 30◦ from the counterclockwise system, while only 11 out of 61 clockwise moving stars have a separation larger than 30◦ from the clockwise disk. The observation of a warped disk of WR/O stars in the Galactic Center has substantially strengthened our previous conclusion that the population of 100 young, massive stars arranged mainly in two coherent structures between 0.8” and 12” from Sgr A* is due to in situ star formation (as opposed to an infalling cluster scenario) from gas that fell into the central region about 6×106 years ago and became dense enough to overcome the tidal forces. As in (Paumard et al. 2006) we used the usual Cartesian coordinate system in oﬀsets from Sgr A∗: x = cos δdα increased eastward, y = dδ increased northward and z = dD increased along the line of sight away from the observer. We used a spherical coordinate system (x, y, z) ⇔ (r, θ, φ): Figure 25 illustrates the deﬁnition of the classical orbital elements used in this work in the framework of the chosen coordinate system in this work. We thank J. Cuadra for his useful comments. TA is supported by ISF grant 968/06, Minerva grant 8563 and ERC Starting Grant 202996.
Quasinormal mode spectra for odd parity perturbations in spacetimes with smeared matter sources<|sep|>In this work, we have studied the QNM frequency spectrum for the static spherically symmetric spacetime having a smeared (Gaussian type) matter distribution. This type of matter distribution, involving a length scale, can be motivated from astrophysical perspectives (in the context of star clusters). Hence our result can be relevant for those scenarios depending on a proper choice of the length of smearing scale. Also, such a length scale is crucial to identify the character of the source density. As a demonstration with astrophysical objects, we found that the resulting frequency change due to smearing is of O(Hz) and hence within the current limits of the terrestrial GW detectors. Originally such metrics with smeared matter distribution was motivated by quantum gravity with the smearing length tentatively identiﬁed with Planck length. However, in that case, due to the smallness of Planck length scale such quantum gravity motivated corrections will be diﬃcult to observe. In summary, our analysis here focuses on the gravitational perturbations of a background geometry, that are odd multipoles under parity transformation. The gravitational perturba tions do posses an even parity component as well. For the case of conventional spherically symmetric Schwarschild geometry with a delta-function source, there is a special property for the perturbation spectrum that ensures that the QNM spectra for odd and even parity perturbations are equal. In technical terms, one says that the QNM spectrum of odd parity perturbations is iso-spectral with the QNM spectrum of even parity perturbations [24]. However, it is not clear whether the same would hold true for spacetime with a smeared matter distribution, that has been studied here. With Gaussian distributed matter density, the potential for even-parity perturbation would have a new (Θ-dependent) scale. This feature may restrict the validity of the iso-spectral character between perturbations of opposite parity. This requires an explicit computation of the QNM spectrum of even-parity perturbations that we have left as a future work. Acknowledgement: It is a pleasure to thank Professor Roman Konoplya for helpful suggestions and informing us about the relevant references for this work and specially for sending us the necessary code for computation. Also we thank the referee for constructive comments.
Magnetic properties and domain structure of ultrathin yttrium iron garnet/Pt bilayers<|sep|>Our study shows that the magnetic properties and domain conﬁguration of epitaxial YIG(111) ﬁlms grown by PLD on GGG and capped by Pt depend strongly on the thickness of the YIG layer. Despite the high structural and interface quality indicated by XRD and TEM, the saturation magnetization decreases from Ms = 122 kA/m (15% below bulk value) in 90 nm-thick YIG to 22 kA/m in 3.4 nm-thick YIG. The gradual decrease of Ms suggests a continuous degradation of Ms rather than the formation of a dead layer. All ﬁlms except the thinner one (tYIG = 3.4 nm) present a rather strong easy-plane anisotropy in addition to the shape anisotropy, of the order of 103 to 104 J/m3. Additionally, all ﬁlms except the thinner one present weak in-plane uniaxial anisotropy, of the order of 3 − 10 J/m3. This anisotropy deﬁnes a preferential orientation of the magnetization in each sample, which, however, is found to vary not only as a function of thickness but also between samples with the same nominal thickness and even for samples patterned on the same substrate. The origin of this variation is tentatively attributed to local inhomogeneities of the growth or patterning process, which could lead to small strain diﬀerences not detectable by XRD. Besides these ﬁndings, we underline the fact that SMR measurements performed for external magnetic ﬁelds smaller or comparable to the eﬀective anisotropy ﬁelds allow for the accurate characterization of both the magnitude and direction of the magnetic anisotropy of YIG/Pt bilayers, with an accuracy better than 10 μT. YIG ﬁlms with tYIG = 90 − 10 nm present large, mm size, in-plane domains delimited by zigzag boundaries and N´eel domain walls. The apices of the zigzags are pinned by defects, whereas the straight sections of the walls incorporate Bloch-like singularities, which separate regions of the walls with opposite magnetization chirality. The domain morphology changes abruptly in the thinner ﬁlms. Whereas the 8.7 nm thick YIG presents elongated domains that are tens of μm long, the domains in the 3.7 nm thick YIG are smaller and more irregular, consistent with the reduction of the easy-plane and uniaxial anisotropy reported for this sample. Our measurements indicate that the performance of YIG-based spintronic devices may be strongly inﬂuenced by the thickness as well as by local variations of the YIG magnetic properties. We thank Rolf Allenspach for valuable discussions. Furthermore, we acknowledge funding by the Swiss National Science Foundation under Grant no. 200020172775. Jaianth Vijayakumar is supported by the Swiss National Science Foundation through Grant no. 200021153540. David Bracher is supported by the Swiss Nanoscience Institute (Grant no. P1502). Part of this work was performed at the SIM beamline of the Swiss Light Source, Paul Scherrer Institut, Villigen, Switzerland. 2 M. Schreier, N. Roschewsky, E. Dobler, S. Meyer, H. Huebl, R. Gross, and S. T. Goennenwein, Applied Physics Letters 103, 242404 (2013). 4 N. Vlietstra, J. Shan, B. Van Wees, M. Isasa, F. Casanova, and J. B. Youssef, Physical Review B 90, 174436 (2014). 5 B. Heinrich, C. Burrowes, E. Montoya, B. Kardasz, E. Girt, Y.-Y. Song, Y. Sun, and M. Wu, Physical Review Letters 107, 066604 (2011).
An Edge-Cloud Integrated Framework for Flexible and Dynamic Stream Analytics<|sep|>Stream analytics aims to analyze and process high volumes of streaming data continuously. In this paper, we study how to best leverage edge and cloud resources to achieve better accuracy and latency for RNN-based stream analytics and better adapt concept drift in stream data. We propose three ﬂexible deployments for the hybrid stream analytics framework in order to achieve the proper trade-oﬀ between latency and accuracy for stream analytics. We also propose an adaptive and dynamic hybrid learning model with two weight combination algorithms for solving the concept drift during stream analytics. The evaluation with real-world stream datasets shows the proposed edge-cloud deployment can archive similar latency performance as edge-centric deployment without worrying about capacity limitations, and our dynamic weighting algorithm performs the best among other hybrid learning model approaches for all three concept drift scenarios in terms of accuracy. For future work, we will mainly focus on the following three aspects of the hybrid stream analytics framework. First, the Spark-based speed training module can be extended to multiple edge devices as a distributed master-worker computing. Second, we will study more variants of the proposed dynamic weighting algorithm, like stacking the most resent n speed layer models or stacking speed layer models continuously. Last, we
Arctic curves of the six-vertex model on generic domains: the Tangent Method<|sep|>Which models next? This paper sets the basis for a method aimed at the determination of the Arctic curve in statistical mechanics models on planar graphs, with (piecewise) local translational invariance of the lattice and weights, showing phase separation phenomena, in light of a conserved quantity in the associated transfer matrix, that can be seen as the number of lines in a suitable line representation. This is the case for a variety of dimer or free-fermionic models (for which, however, in most cases more powerful general methods already exist), for the six-vertex model, treated here in detail, and for variants of it in which some spectral lines may contain higher spin or q-bosons. The constraint of having a line representation may appear as a strong limitation of the method. Let us however stress how, up to bijections, families of non-intersecting (possibly interacting) lattice paths constitute a very general and ﬂexible language for representing a variety of mathematical structures, ranging from Young diagrams [57] and tableaux [58] to Q-systems and cluster algebras [59]. The application of the Tangent Method to quite diﬀerent classes of models, and the study of its interplay with other existing techniques, is in our opinion a direction of research deserving to be explored. What more for the six-vertex model? It shall be clear that also for the sixvertex model, the main subject of this paper, the analysis is far from complete. A variety of domains still asks for the determination of their Arctic curve, and in particular it would be quite interesting to obtain the analytic expression for an Arctic curve in a domain presenting cusps, for a system out of free-fermionic points. As we said, the main obstacle in these derivations is the lack of knowledge of reﬁned enumerations, called here one-point boundary correlation functions. The most promising candidate seems to be the six-vertex model in the domain presented in Figure 5. We hope that some progress in this direction will be available in the light of our results on a generalisation of the Emptiness Formation Probability observable [54]. We also remind that the Tangent Method, in its present formulation is not adapted to the determination of the internal portions of the Arctic curve, i.e. the arcs between two cusps. Or the internal components of Arctic curves in the cases where, as for the triangoloid domain, there are internal frozen regions. We hope that the puzzling features of the internal component of the curve outlined at the end of Section 5 may be clariﬁed in the future. Which Tangent Method? Another natural question is how to make precise the assumptions listed in Section 3.2. In principle, the short discussion following the assumption gives a clear roadmap to this task. However, a further aspect of the method that we have not discussed here is the fact that it exists in several variants, which exploit in slightly diﬀerent ways the peculiar behaviour of one thick path
How Does Metallicity Affect the Gas and Dust Properties of Galaxies?<|sep|>From studies of the MIR and FIR gas properties and MIR to submm dust properties, a picture of the structure of low-metallicity galaxies is emerging. The decreased attenuation and the presence of intense, hard radiation ﬁelds conspire together to highlight the unique place the low-metallicity star-forming galaxies take compared to their more
segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection<|sep|>Figure 6: Qualitative results. We show the top scoring detections for each ground-truth class. For our method, we also show the segment chosen by our model. didate box to select a segment and score the agreement between them. We additionally proposed a sequential localization scheme, where we iterate between scoring our model and re-positioning the box (changing the spatial scope of the input to the model). We demonstrated that our approach achieves a signiﬁcant boost over the RCNN base line, 4.1% on PASCAL VOC 2010 test in the 7-layer setting and 4.3% in the 16-layer setting. The ﬁnal result places segDeepM at the top of the current PASCAL’s leaderboard. Acknowledgments. This research was supported in part by Toyota. The GPUs used in this research were generously donated by NVIDIA Corporation.
Massive Black Hole Binaries from the TNG50-3 Simulation: II. Using Dual AGNs to Predict the Rate of Black Hole Mergers<|sep|>dAGNs are a product of galaxy mergers and trace the population of future MBH coalescences. In this paper, we combined the MBH dynamical evolution calculations from Paper I with estimates of AGN luminosity to explore how the luminosities and separations of dAGNs change as the MBH pair evolves in its host galaxy. In addition, we were able to calculate the fraction of the dAGN population at a redshift zdAGN that lead to a MBH merger at redshift zcoal, including determining the fraction that lead to a LISA SNR> 8. We ﬁnd that, in the absence of RF effects, the dAGN population in our model, with total bolometric luminosity Lbol > 1043 erg s−1, peaks at zdAGN≈ 0.4 and is dominated by systems with separations a ≳ 0.7 kpc (Fig. 1). However, a majority of these dAGNs will not lead to MBH mergers before zcoal= 0 (Fig. 3). In fact, the majority of low-zdAGN dAGNs that are precursors to MBH mergers are separated by ≲ 0.5 kpc (Fig. 2). This is a result of the orbital decay times of the sMBH– there is simply not enough time for most a ∼ 0.7 kpc dAGNs at zdAGN≈ 0.4 to evolve through to coalescence before zcoal= 0. Therefore, a closer connection between dAGNs and MBH mergers can be most easily obtained by detecting dAGNs at zdAGN≳ 1, where the coalescence fraction exceeds 0.5. The separation of dAGNs at zdAGN≳ 1 is ∼ 0.7–1 kpc and the total Lbol ≈ 1043−46 erg s−1. The orbital evolution of the sMBH depends on the properties of the post-merger galaxy and MBH pair (e.g., Li et al.
The Earliest Phases of Star Formation (EPoS): A Herschel Key Program - The precursors to high-mass stars and clusters<|sep|>We present an overview of the ﬁrst results of the Earliest Phases of Star Formation survey with Herschel, focusing on the sample of 45 high-mass regions. The goal of the work presented here is to present the EPoS sample of IRDCs and ISOSS sources as a whole and proﬁle the population of unresolved point sources, which we call cores, within them. We use PACS point source ﬂux densities to construct and ﬁt modiﬁed blackbodies to the spectral energy distributions of each core and use the ﬁt to estimate its temperature, luminosity, and mass. The main results of this work are as follows: – We extract 496 point sources in the 45 IRDC structures in our sample. Their sizes range from 0.05 to 0.3 pc, which IRDC18454 10 18:47:49.5 -1:55:27 17 354 138 IRDC18182 22 18:21:22.9 -14:33:02 12 37 89 IRDC028.34 18 18:42:51.2 -3:59:24 16 110 53 IRDC18223 9 18:25:11.2 -12:41:48 17 157 51 beside IRAS18223-1243
Training Data Independent Image Registration With GANs Using Transfer Learning And Segmentation Information<|sep|>We propose a novel deep learning framework to register brain MR images by training on lung Xray images. We leverage segmentation information and transfer learning with generative adversarial networks. Experimental results show our approach achieves registration with almost similar accuracy as one would obtain when the training and test dataset consist of similar images.
A Case for Practical Configuration Management Using Hardware-based Security Tokens<|sep|>In the light of increasing complexities in future smart factories, the actual security of factory networks will strongly depend on the proper deployment of specific protection mechanisms. This deployment hinges on two factors. First, factory operators, who are no domain experts for IT security, must understand the protection mechanism and the mechanism itself must be as frictionless as possible so as to not impede the productive work of the factory. In this work, we made the case for a novel mechanism that allows to easily and understandably configure encryption gateways, which will be important building blocks for the security architecture of future factories. We implemented a configuration scheme that employed hardware security tokens. These tokens cannot be forged and reduce the actions necessary to implement a security policy to a simple physical action, comparable to locking a door with a key. Our results showed, that a reduction of the complexity of configuration must not necessarily be accompanied with a loss of security. This work was co-funded by the SAB (Development Bank of Saxony) under frameworks from both the ERDF (European Regional Development Fund) as well as the ESF (European Social Fund), by public funding of the state of Saxony/Germany and by the German Research Foundation (DFG) as part of Germany’s Excellence Strategy EXC 2050/1 – Project ID 390696704 – Cluster of Excellence Centre for Tactile Internet with Human-in-the-Loop (CeTI) of TU Dresden.
GYES, a multifibre spectrograph for the CFHT<|sep|>At the time of writing the estimates of costs and eﬃciency for GYES are not yet consolidated. It nevertheless appears likely that the instrument can be built within an envelope of hardware cost of 5 Me . The progress of the project and updated information can be found on its web site http://gyes.obspm.fr.
A comparative study of attention mechanism and generative adversarial network in facade damage segmentation<|sep|>This paper presents a comparative study for semantic segmentation focusing on on-site inspection. Inspired by the attention mechanism and the design of the U-net, two novel architectures integrated with attention gates are proposed, i.e., the advanced attention U-net and the full attention U-net. Furthermore, four novel architectures of discriminators in U-net based GANs are proposed as well, namely the D-4 U-net, the D-6 U-net, the D4V U-net, and the D-5V U-net. A lightweight experiment is executed to compare these enhanced U-net architectures in two groups. The U-net, the attention U-net and the two brand-newly proposed U-nets with attention mechanisms are marked as the ﬁrst group, in which the advanced attention U-net achieves the best results of semantic segmentation. Four newly proposed GANs are compared with the U-net in the second group. Consequently, the D-6 U-net shows the highest segmentation accuracy. Hence, the advanced U-net, the D-6 U-net, and the U-net combined of both are selected for the further implementation, where the graphical data acquired from a pseudo on-site inspection. The results prove the improved capability of the tested architectures in semantic segmentation. PA and mIoU are calculated to designate the strengths of the advanced U-net and the D-6 U-net in segmenting ten selected images and the whole dataset. Meanwhile, the adverse eﬀects occasioned by the combination of a discriminator and attention mechanism are discovered. For further investigation, another group is trained and tested in the experiment, including the full attention Unet, the D-4 U-net, and the U-net combined with both. These test results demonstrate positive combination eﬀects and thus stand on the opposite of the previous test. The comparison with all the groups ascribes this phenomenon to the diﬀerent distribution of attention gates in the proposed enhanced U-nets. It is to be noticed that the inherent shortcoming of the U-net lies in segmenting slim objects. Though the spall is well recognized in this research work, semantic segmentation of slim damage such as cracks can not be easily segmented. Therefore, other strategies, e.g., featured pyramid network and residual operation, could be integrated in the future to promote the semantic segmentation quality of the proposed CNNs. The authors would like to acknowledge the organization committee of The 2nd International Competition for Structural Health Monitoring (IC-SHM, 2021) that shared the graphical data for training and test, and the hardware support from Microintelligenc Inc.
Structure of Superheavy Nuclei Along Element 115 Decay Chains<|sep|>In summary, we studied shell structure of superheavy nuclei within the self-consistent SHFB approach and macroscopic-microscopic NS model. Detailed predictions have been made for the quasi-proton and quasi-neutron structures of nuclei belonging to the α-decay chains of 287115, 287Lv, 289Lv, and 293117. The unedf1 and unedf1SO SEDF models diﬀer in the strength of the spinorbit term, and this impacts detailed predictions for the deformed nuclei around Z = 110 and N = 168. The recent observation of low-energy E1 transitions in 276Mt [9] provides a stringent constraint on theoretical models. Indeed, the recently proposed unedf1SO parametrization that performs well in the transfermium region does not oﬀer a simple explanation of the E1 data, whereas the global unedf1 parametrization explains the data in terms of the proton π[505]9/2 → π[615]11/2 transition. The MO models suggests two competing scenarios: a proton transition similar to that of unedf1, and an alternative neutron ν[716]13/2 → ν[606]11/2 E1 transition. To conﬁrm or disprove these scenarios, theory strongly recommends a search for E1 transitions in neighboring odd-A systems 275,277Mt, 275Hs, and 277Ds. Experi TABLE V. Similar as in Table I but for one-quasi-particle excitations in the odd-A neighbors of 276 109Mt167 predicted with unedf1SO and unedf1. 275 109Mt166 [615]11/2 0 29.7 0.23 [512]3/2 0.243 29.5 0.23 [521]1/2 0.402 29.2 0.22 [512]5/2 0.500 29.7 0.23 [510]1/2 0.512 29.7 0.23 277 109Mt168 [615]11/2 0 28.3 0.23 [521]1/2 0.174 27.4 0.21 [512]3/2 0.269 28.3 0.23 [510]1/2 0.480 28.3 0.23 [512]5/2 0.532 28.3 0.23 275 108Hs167 [613]5/2 0 28.9 0.22 [611]3/2 0.016 28.9 0.22 [611]1/2 0.117 28.9 0.22 [604]9/2 0.235 28.2 0.22 [716]13/2 0.479 29.4 0.23 277 110Ds167 [611]3/2 0 28.6 0.22 [613]5/2 0.046 28.9 0.22 [611]1/2 0.107 28.7 0.22 [604]9/2 0.335 28.6 0.22 [716]13/2 0.564 29.2 0.22 unedf1 275 109Mt166 [512]3/2 0 30.0 0.23 [615]11/2 0.159 29.6 0.23 [505]9/2 0.167 29.0 0.22 [510]1/2 0.173 30.1 0.23 [624]9/2 0.318 29.9 0.23 277 109Mt168 [512]3/2 0 28.6 0.22 [615]11/2 0.131 28.2 0.21 [505]9/2 0.136 27.5 0.21 [512]5/2 0.182 28.6 0.22 [624]9/2 0.300 28.5 0.21 275 108Hs167 [613]5/2 0 29.4 0.22 [611]3/2 0.067 29.4 0.22 [611]1/2 0.104 29.5 0.22 [716]13/2 0.173 29.7 0.23 [604]9/2 0.242 29.1 0.22 277 110Ds167 [611]3/2 0 28.9 0.22 [613]5/2 0.035 29.0 0.22 [611]1/2 0.090 29.0 0.22 [716]13/2 0.157 29.2 0.22 [604]9/2 0.227 28.7 0.22 mentally, this calls for high-resolution α-photon coincidence spectroscopy of decay chains starting from 293117, 287,289115, or 285,287Fl, respectively. However, the observation of these systems is hampered either by relatively low production cross-sections or large spontenous ﬁssion branches on the way to the nuclei of structural interest [1–9]. A solution to this spectroscopic puzzle may signiﬁcantly contribute to our understanding of shell structure in superheavy nuclei, and the strength of the spin-orbit splitting in particular.
Is the Policy Gradient a Gradient?<|sep|>We conclude by emphasizing the while ∇J?(θ) is not a gradient (Section 4), can in some cases result in pessimal behavior (Section 5), and is commonly misrepresented in the literature (Section 6), it has remained the most popular estimator of the policy gradient due to its effectiveness when applied to practical problems. The precise reason for this effectiveness, especially in the episodic setting, remains an open question. We begin by hypothesizing that ∇J?(θ) takes the form of a weighted distribution over ∂ ∂θ V θγ (s), given some time-dependent weights, w(t), on each term in the state distribution. That is, we hypothesize that equality: We then must prove that this holds for some choice ofw(t), and then derive the satisfying choice of w(t). Sutton et al. [25] established that: since Pr(At+k = a|St+k = x,θ) = Pr(At+k = a|St+k = x,St = s,θ) and by the law of total probability. The key point is that we have removed the term Pr(St = s|θ). Continuing, starting with the fact that Pr(St+k = x|θ) Pr(At+k = a|St+k = x,θ) = Pr(St+k = x,At+k = a|θ), we have that: by substitution of the variable i = t + k. Continuing, we can move the summation inside the expectation and reorder the summation: In order to derive ∇J?(θ), we simply need to choose a w(t) such that ∀i : �i t=0 w(t)γ i−t = 1. This is satisfied by the choice: w(t) = 1 if t = 0, and 1 − γ otherwise. This trivially holds for i = 0, as w(0)γ 0 = (1)(1) = 1. For i > 0: We continue from the example given in Figure 1. First, we compute dθγ in terms of θ for each state using the definition of the MDP and π: Next, we compute V θγ in each state in terms of θ. Note that Qθγ (s1,a1) = γV θγ (s2) because taking a1 in s1 leads to s2 and has zero reward.
Spark Parameter Tuning via Trial-and-Error<|sep|>This work deals with conﬁguring Spark applications in an eﬃcient manner. We focus on 12 key application instance-speciﬁc conﬁgurable parameters and assess their impact using real runs on a petaﬂop supecomputer. Based on the results and the knowledge about the role of these parameters, we derive a trial-anderror methodology, which requires a very small number of experimental runs. We evaluate the eﬀectiveness of our methodology using three case studies, and the results show that we can achieve up to more than a 10-fold speedup. Although our results are signiﬁcant, further research is required to investigate additional infrastructures, benchmark applications, parameters and combinations.
Random Tight Frames<|sep|>First, we introduced probabilistic frames and veriﬁed that many properties from ﬁnite frames can be adopted. Secondly, we used probabilistic tight frames to signiﬁcantly improve a result by Goyal, Vetterli, and Thao in [18] about the random choice of points on the sphere. We still approximate a tight frame while allowing for a much wider class of probability measures, namely any probabilistic tight frame. The requirement of identical distributions is also removed. We also veriﬁed that many random matrices, which are used in compressed sensing, are induced by probabilistic tight frames. Thirdly, we extended results about the frame potential as introduced by Benedetto and Fickus in [2]. In fact, we demonstrated that probabilistic tight frames are the minimizers of the probabilistic frame potential. The author was supported by the Intramural Research Program of the National Institute of Child Health and Human Development and by NIH/DFG Research Career Transition Awards Program (EH 405/1-1/575910).
Yangian Symmetry for the Action of Planar N=4 Super Yang-Mills and N=6 Super Chern-Simons Theories<|sep|>In this work we have elaborated upon the results of our previous letter [16] by spelling out in detail the diﬀerent kinds of planar invariance conditions for the equations of motion of N = 4 supersymmetric Yang–Mills theory and ABJ(M) theory under a Yangian algebra. Subsequently, we have reformulated the strong version of this invariance as an invariance of the action and proved explicitly that this invariance holds indeed. This not only shows that these two classical gauge theory models possess Yangian symmetry in the planar limit, but it may also be viewed as a formal deﬁnition for their integrability. A follow-up work [20] will address Yangian-invariance of ﬁeld correlators at tree level: for a ﬁeld theory with some set of symmetries, one should expect correlation functions of the ﬁelds to obey a corresponding set of Ward–Takahashi-identities reﬂecting these symmetries. An important aspect in the computation of correlators within a gauge theory is gauge ﬁxing. It will be shown how Yangian symmetry can be made compatible with Faddeev–Popov gauge ﬁxing and BRST symmetry and how to formulate Slavnov–Taylor identities for Yangian symmetry which properly take into account the eﬀects of gauge ﬁxing. To that end, the algebraic relations of the Yangian need to be studied in detail because they are intertwined with gauge transformations which are henceforth deformed by the gauge ﬁxing procedure. A curious side eﬀect is that this will amount to an extension of the gauge algebra by bi-local and non-local gauge symmetries. Our works lay the foundations for many directions of further study. Most importantly, ﬁrm contact should be made with corresponding symmetry structures of the world-sheet theory for strings on AdS5 ×S5. By the AdS/CFT correspondence such structures should exist, but quantum eﬀects on both sides, potentially including quantum anomalies, may play a signiﬁcant role for a precise matching, see [26] for considerations on the string theory side. For example, the algebraic complications due to non-ultra-locality of the string theory sigma model [27] may well have a counterpart in the Yangian algebra for gauge theory. Another obvious question is which other models beyond N = 4 sYM and ABJ(M) theory enjoy a Yangian (or related) symmetry in the planar limit? There are several deformations and orbifolds of N = 4 sYM which are apparently integrable [28] and which should therefore possess an extended symmetry. This has been shown explicitly for the beta-deformation in [29]. The simplistic ﬁshnet theory [30] represents a particular contraction limit of an integrable deformation; however due to the non-vanishing Coxeter number of the level-zero symmetry, some adjustments to our treatment will be inevitable, see [31]. Furthermore, massive deformations of supersymmetric Chern–Simons theories [32] have an interesting non-conformal supersymmetry algebra [33] on which a Yangian algebra could in principle be established. One may also wonder whether and how the partial integrability of N < 4 sYM theories within certain sectors, see [34], can be formulated in analogy to our framework. Yangian symmetry could also prove helpful in constructing gauge theory models which are predicted by the AdS/CFT correspondence but remain to be formulated. Such models include two-dimensional gauge theories as duals of the integrable string theories on AdS3× S3 × S3 × S1 and AdS3 × S3 × T 4, a q-deformation of N = 4 sYM as a dual of the eta/kappa-deformation of string theory on AdS5×S5 (where the Yangian symmetry would be deformed to a quantum aﬃne group) as well as the infamous six-dimensional N = (2, 0) theory which is intrinsically non-perturbative. Another direction for research is to ﬁrmly derive the applications of integrability to various observables from our framework of Yangian symmetry. For example, the Bethe equations for the spectrum of one-loop anomalous dimensions follow directly from the well-established Bethe ansatz framework for quantum integrable spin chains in connection to (conventional) quantum algebra. More interestingly, the methods developed for the spectrum at higher loops and at ﬁnite coupling should have a formal justiﬁcation in Yangian symmetry of the model (potentially after implementing quantum corrections). Along the same lines, it will be useful to understand the origin of the extended psu(2|2) Yangian for the magnon scattering picture including the various master, boost and secret symmetries. Similarly, symmetries and techniques for other observables such as Wilson loops, scattering amplitudes, correlation functions of local operators and form factors might be traced back to Yangian symmetry. Further open questions beyond generalisations and applications include: How to generalise Yangian symmetry to a superspace formulation of N = 4 sYM (either in a light-cone superspace [35] or in a full superspace [15])? Does the (somewhat non-local) Yangian symmetry have some associated Noether charges? What is the role of the Casimir operators (central elements) of the Yangian algebra? Can they be applied to a wider class of observables? Finally, our framework for Yangian symmetry raises several questions of mathematical nature: First and foremost, does the Yangian algebra actually close, i.e. do the Serrerelations hold and how to formulate the closure precisely? How to construct the singleﬁeld action of the level-one generators abstractly? Does it follow from some fundamental principles beyond the consistency requirements employed to derive it here? Last but not least, in what sense is the proposed action of the Yangian generators a representation, in particular when acting on cyclic states such as a the action functional. We thank N. Drukker, J. Plefka and M. Staudacher for discussions related to the work. The work of NB and AG is partially supported by grant no. 615203 from the European Research Council under the FP7 and by the Swiss National Science Foundation through the NCCR SwissMAP. The work of MR was supported by the grant PL 457/3-1 “Yangian Algebra. The supersymmetry algebra is spanned by the supersymmetry generators Qaγ, ¯Q ˙αc and the momentum generator P ˙αγ. The special conformal generators are given by two fermionic generators Sαc, ¯Sa ˙γ and the bosonic generator Kα ˙γ. Furthermore, the superconformal algebra includes the Lorentz and internal rotation generators Lαγ, ¯L ˙α ˙γ, Rac (whose trace over the indices vanishes) and the dilatation generator D. Finally, we will also need gauge transformations to discuss the gauge-covariant representations. These are generated by G[X] where the ﬁeld X serves as the gauge parameter matrix. Although we do not explicitly refer to real algebras, a suitable set of reality conditions for N = 4 sYM is given by The below representations will be unitary w.r.t. these reality conditions. In the following we will list the most relevant algebra relations. The Lorentz and internal algebra relations take the form [¯L ˙α ˙β, ¯L ˙γ ˙δ] = i(δ ˙γ ˙β¯L ˙α ˙δ − δ ˙α ˙δ ¯L ˙γ ˙β) + G[. . .], Here, the Lorentz algebra relations involve gauge transformations G[. . .], where the omitted gauge parameter is a term of the form xx∇A representing the ﬁeld strength contracted with the Killing spinors of the two rotations. We do not present the long list of algebra relations with the remaining generators, as these can easily be inferred as the transformations of spinor indices compatible with the above relations. The algebra of the scaling generator D measures the scaling dimension ∆J of the other generators J [D, J] = −i∆J J + G[. . .] (A.4) [P ˙αβ, Kγ ˙ϵ] = iδ ˙ϵ ˙αLγ β + iδγ β¯L˙ϵ ˙α + iδβ γδ ˙ϵ ˙αD + G[. . .], Again, these relations may involve some gauge transformation G[. . .] in addition to the pure conformal generators. The non-trivial relations of the fermionic generators read {Qbα, ¯Q ˙γ d} = 2δd bP ˙γα, {Qaβ, Sγd} = 2iδγ βRd a − 2iδd aLγ β − iδd aδγ βD, {Sαb, ¯Sc ˙ϵ} = 2δb cKα˙ϵ. {¯Q ˙α b, ¯Sc ˙ϵ} = 2iδ ˙ϵ ˙αRb c + 2iδb c¯L˙ϵ ˙α + iδb cδ ˙ϵ ˙αD, (A.7) [P ˙αβ, Sγe] = δγ β ¯Q ˙α e − ε ˙α ˙κG[xγ ˙κΨ e β], [P ˙αβ, ¯Sc ˙ϵ] = −δ ˙ϵ ˙αQcβ + εβδG[xδ ˙ϵ ¯Ψ ˙αc], [Kα ˙γ, Qdϵ] = −δα ϵ ¯Sd ˙γ + G[. . .], [Kα ˙γ, ¯Q˙ϵ d] = δ ˙γ ˙ϵ Sαd + G[. . .]. (A.8) Qaβ·Φcd = δc aΨ d β − δd aΨ c β, ¯Q ˙α b· ¯Φcd = δb c ¯Ψ ˙αd − δb d ¯Ψ ˙αc, Qaβ·Ψ c δ = −2δc aFβδ + iεβδ[Φce, ¯Φae], ¯Q ˙α b· ¯Ψ ˙γd = −2δb d ¯F ˙α ˙γ − iε ˙α ˙γ[Φbe, ¯Φde], 30In analogy to the curious assignment ∆A = 0 for scaling transformations, the spacetime indices of the gauge ﬁeld A are not transformed explicitly. Sαb·Ψ c δ = ixα˙ϵ ¯Q˙ϵ bΨ c δ − 2δα δ Φbc, ¯Sa ˙γ·Ψ e δ = −ixβ ˙γQaβ·Ψ e δ, Sαb· ¯Ψ ˙γd = ixα˙ϵ ¯Q˙ϵ b ¯Ψ ˙γd, ¯Sa ˙γ· ¯Ψ˙ϵd = −ixβ ˙γQaβ· ¯Ψ˙ϵd + 2δ ˙γ ˙ϵ ¯Φad, Kα ˙γ·Ψ e δ = ixα ˙κxβ ˙γ∇β ˙κΨ e δ + ixα ˙γΨ e δ + iδα δ xκ ˙γΨ e κ, Kα ˙γ· ¯Ψ˙ϵd = ixα ˙κxβ ˙γ∇β ˙κ ¯Ψ˙ϵd + ixα ˙γ ¯Ψ˙ϵd + iδ ˙γ ˙ϵ xα ˙κ ¯Ψ ˙κd, G[X]·Φcd = [X, Φcd], G[X]·Ψ c δ = [X, Ψ c δ], G[X]· ¯Ψ ˙γd = [X, ¯Ψ ˙γd], G[X]·A ˙γδ = i∇ ˙γδX. (A.17) Extension. When discussing the level-one momentum generator �P it turns out useful to superﬁcially extend the level-zero algebra by an operator B. This operator neither represents a symmetry of the action nor does its action describe a proper representation of some algebra. We simply deﬁne its action as Furthermore, we deﬁne two 2 × 2 matrices of operators L′ and ¯L′ and a 4 × 4 matrix of operators R′ as the combinations of Lorentz and internal rotations, scale transformations and B These operators have a reasonably simpler action on the ﬁelds of N = 4 sYM than the corresponding Lorentz generators L In this appendix we will give explicit expressions for the coproducts as well as single-ﬁeld actions of the level-one Yangian generators These are the level-one generators which commute with the ordinary momentum P (up to gauge artefacts) and hence their single-ﬁeld action can be expected to have no explicit dependence on the position x. �P ˙αβ·Φcd := 0, �P ˙αβ·Ψ c δ := −εβδ � Φce, ¯Ψ ˙αe � , �P ˙αβ· ¯Ψ ˙γd := −ε ˙α ˙γ � ¯Φde, Ψ e γ � , �P ˙αβ·A ˙γδ := i ∆�P ˙αβ = �P ˙αβ ⊗ 1 + 1 ⊗ �P ˙αβ + P ˙αγ ∧ Lγ β + P ˙γβ ∧ ¯L ˙γ ˙α + P ˙αβ ∧ D − i
New parallel programming language design: a bridge between brain models and multi-core/many-core computers?<|sep|>Most programs for AI tasks are ineﬃcient on traditional computers with their Von-Neumann architecture and using imperative programming style. The proposed dataﬂow machines from eighties and nineties, speciﬁc for AI tasks, never had a major impact on the market. What we see in the recently proposed TRIPS architectures is a combination of dataﬂow and VonNeumann styles, particularly using speculative execution of long blocks of instructions on the computers’ arrays of ALUs. The speculation on possible executions of the paths in a program, used to increase the processing speed, looks somehow similar to the prediction process in the HTM models of the brain. A comparison between these two computing models may be worthwhile for both ﬁelds. For instance, while the computer prediction is in the narrow window of the user program demand, the HTM models of the brain are more open, interactive, agent-like - here the prediction is mixed with possible actions of the human being who can change the course of the forthcoming input data. This may explain why humans are good on interactive tasks, while current computers with their predeﬁned program-captured behavior are not. We plan to develop the ideas from this paper in both directions: (1) to get a rich set of Agapia programs for HTMs models, particularly for those used by Numenta platform [9]; and (2) to explore the possibility of getting a running environment for Agapia programs on TRIPS computers [10]. Acknowledgments. Part of Camelia Chira’s work on this paper was carried out during a visit to the Department of Computer Science of the University of Illinois at Urbana-Champaign, which was supported by a grant from CNCSIS, Romania. Gheorghe Stefanescu would like to thank Mircea Stan for pointers to TRIPS architectures.
Magnetic Transformations in the Organic Conductor kappa-(BETS)2Mn[N(CN)2]3 at the Metal-Insulator Transition<|sep|>We have performed dc magnetization, 1H NMR and magnetic torque studies of organic conductor κ(BETS)2Mn[N(CN)2]3 which undergoes a MI transition at 25 K. The magnetization above TMI follows the CurieWeiss law, revealing antiferromagnetic interactions between S = 5/2, L = 0 Mn2+ spins, and violates it at lower temperature. The 1H NMR spectrum determined by dipolar ﬁelds from Mn2+ moments, exhibits vast broadening of the resonance peaks below 25 K resulting from freezing of Mn2+ spins. Both the magnetization and NMR data obtained at ﬁelds below 70 kOe, therefore suggest a transition from paramagnetic to a frozen state in the manganese subsystem taking place at TMI. However, the resulting state clearly diﬀers from the conventional commensurate N´eel type antiferromagnetic order. Yet another signature of a magnetic transition is the kink in the ﬁeld-dependent torque emerging right below TMI. This one takes place above ∼ 70 kOe and possesses many properties of a spin-ﬂop transition. Most likely it occurs within the spins of Mott-localized π electrons of BETS molecules. If so, it would be more natural to consider the Mott localization as a driving force for both the MI transition and magnetic transformations in the system. An additional argument towards this conclusion is a low θ ∼ −5 K in the Curie-Weiss temperature dependence of the spin susceptibility (compared to TMI ≈ 25 K): in a conventional antiferromagnet the AF transition usually occurs at temperature lower than |θ|.25
What ignites on the neutron star of 4U 0614+091?<|sep|>We can understand several aspects of the type I X-ray bursts observed from 4U 0614+091. The column depths and energy release per gram in the bursts can be estimated by comparing the observed light curves with models, and by considering the burst energetics. Both methods are in good agreement. Helium ignition naturally explains the observed ignition depths for accretion at ˙m∼1% ˙mEdd. Furthermore, the sensitive dependence of the He ignition depth on temperature means that small (factor of two) changes in accretion rate can lead to an order of magnitude variation among the depths, and can thereby explain the occurrence of both short bursts and intermediate-duration bursts. The ignition depth for the superburst inferred from the light curve is the lowest of the current sample of superbursts and agrees well with the constraint from the observed quenching time scale. However, several puzzles remain. First, the amount of He required to achieve the required ignition conditions without stably burning away is signiﬁcantly larger than the ≲10% limit from the optical spectra. Recently, however, formation studies using evolutionary calculations indicate that the donor in 4U 0614+091 may be a hybrid white dwarf or very evolved helium star (Nelemans et al. 2009). This suggests the donor still to be a possible supplier of a signiﬁcant amount of He to the accretion disk, onto the neutron star. Further investigations have to be done, why the He does not show up then in the optical observations. Second, understanding the superburst remains problematic. Unstable C ignition is not known to be possible at these low accretion rates: the accumulating layer is too cold, and even if heated the C burns stably. A superburst powered by a large He pile is possible, but takes several years to accumulate if the accretion rate is ∼1% Eddington. Such an accumulation time is much greater than the observed time of one year. Finally, the reason for the low energy per gram Qnuc≲0.6 MeV per nucleon released in the bursts is not clear. It is important to emphasize, however, that the ignition models and light curve models used in this paper are simpliﬁed. The model light curves assume uniform and instantaneous energy deposition in the fuel layer, and do not follow the detailed nucleosynthesis. These models cannot address the early part of the light curve, for example the interesting ripples in the early light curve of the intermediate duration burst (Fig. 10), nor whether the burning is expected to be incomplete (as we infer from the burst energetics). Incomplete burning, either across the neutron star surface or with depth in the fuel layer, may at least partly solve the above puzzles. Our ignition models rely on a simpliﬁed one-zone ignition criterion. Numerical models of accumulating and burning of He/CO mixtures at low accretion rates are needed to conﬁrm our conclusions, for example, regarding the amount of helium needed to avoid stable burning. It is interesting that at least in principle He can power a superburst-like event. The requirement is that the accumulating fuel layer remains cold. Recently, Cooper et al. (2009) ruled out He as a fuel for superbursts, but their argument assumed an ignition temperature of T=5×108 K that is much larger than the He ignition temperature at superburst columns. Even in the superburst sources with ˙m≳0.1 ˙mEdd, He-powered superbursts could occur if the accumulating layer is cold enough. One way to keep the layer cold would be to have direct URCA neutrino emission in the neutron star core, so that most of the energy release in the crust ﬂows inwards rather than outwards. Triple alpha ignition becomes mostly sensitive to density rather than temperature when the column depth reaches y∼3×1011 g cm−2, as the ignition becomes pycnonuclear and therefore temperature independent. This could potentially explain the narrow range of superburst ignition columns. Further work on this possibility is needed. Acknowledgements. Partly based on observations with INTEGRAL, an ESA project with instruments and science data centre funded by ESA member states (especially the PI countries: Denmark, France, Germany, Italy, Switzerland, Spain), Czech Republic and Poland, and with the participation of Russia and the USA. The RXTE/ASM dwell average results are provided by the ASM/RXTE teams at MIT and at the RXTE SOF and GOF at NASA’s GSFC. The Swift/BAT transient monitor results are provided by the Swift/BAT team. We thank Jean Swank and Lucien Kuiper for discussions regarding the bursts seen with OSO-8 and in 2005 with INTEGRAL, respectively. EK thanks Andy Pollock for discussion regarding the burst recurrence times and small-number statistics. SB was partly supported by the Danish Space Board. AC acknowledges support from an NSERC Discovery Grant and the Canadian Institute for Advanced Research (CIFAR). This research has made use of the SIMBAD database, operated at CDS, Strasbourg, France.
Control of Optical Dynamic Memory Capacity of an Atomic Bose-Einstein Condensate<|sep|>We have investigated the problem of injecting multiple ultraslow pulses through a BEC. We have examined three methods for that aim. This paper ﬁrst reviews our two earlier results where nonlinear optical response is ignored, then also presents our new results where nonlinearity is present. In our ﬁrst study, we have considered short ultraslow pulse propagation in one dimension. Taking into account group velocity dispersion, an optimum set of experimental parameters, in particular Rabi frequency of control ﬁeld, that would give maximum pulse storage capacity are determined. Secondly, transverse direction is taken into consideration, and waveguiding of ultraslow pulses is studied. Existence of ultraslow transverse modes is revealed and their beneﬁts in enhancement of pulse storage capacity is pointed out. Finally, in our new results, we have introduced nonlinear optical response and showed that it is possible to compensate dispersion of a short ultraslow pulse for a speciﬁcally chosen peak power of the pulse. This may be more eﬃcient in optical storage than group velocity dispersion by the Rabi frequency of the control ﬁeld. We hope these studies complement ongoing experimental eﬀorts for practical storage of coherent optical information in atomic Bose-Einstein condensates.
General risk measures for robust machine learning<|sep|>We have highlighted that risk measures oﬀer versatile tools for addressing machine learning problems in a robust manner. By assuming that the loss function is convex, the related optimization problem has been recast as a convex one. We have shown that various classes of risk measures, e.g. those based on ϕ-divergences or on the Wasserstein distance, lead to a common convex formulation. In addition, an eﬃcient convex optimization algorithm has been proposed to cope with the non trivial constrained problem resulting from this formulation. We have conducted numerical experiments in which various ambiguity sets are tackled thanks to the same algorithm. We have also illustrated that the considered robust models can outperform classical ones in challenging contexts when the size of the training set is limited, or The second author wants to thank Université Paris-Est and Labex Bézout for the ﬁnancial support particularly for the funding of his PhD program. The work of J.-C. Pesquet was supported by Institut Universitaire de France.
Lorentz factor distribution of blazars from the optical Fundamental plane of black hole activity<|sep|>In this paper, we present a new and independent method of calculating the Lorentz factor distribution of a blazar population using the optical fundamental plane of black hole activity. We use the VIPS blazar sample and ﬁnd the bulk Lorentz factor distribution to be in the form of a power law given as N(Γ) ∝ Γ−2.1±0.4, with a Γ range of 1 to 40. The optical fundamental plane is an eﬃcient and insightful platform to study high-luminosity blazars, since, unlike the X-ray and radio luminosities, the [OIII] emission line luminosity of a blazar is direction-independent and hence not aﬀected by relativistic boosting. Another advantage of using the optical FP is that the blazars are high mass sources and hence the synchrotron cut-oﬀ of the jet spectrum can already be below the X-ray band, specially for the low-frequency-peaked BL Lacs. Thus, the observed X-ray luminosity will be dominated by synchrotron self-Compton emission and hence can not be used as a proxy for the accretion rate.
A Type System for the Vectorial Aspect of the Linear-Algebraic Lambda-Calculus<|sep|>In this paper we deﬁne a strongly normalising, conﬂuent, typed, algebraic λ-calculus satisfying a weak subject reduction. The language allows making arbitrary linear combinations of λ-terms α ·t+β ·u. Its vectorial type system is a ﬁne-grained analysis tool describing the “vectorial” properties of typed terms: First, it keeps track of the ‘amplitude of a term’, i.e. if t and u both have the same type U, then α ·t+β ·u has type (α +β)·U. Then it keeps track of the ‘direction of a term’, i.e. if t and u have types U and V respectively, then α ·t+β ·u has type α ·U +β ·V. This type system is expressive enough to be able to type the encoding of matrices and vectors. The resulting type system has the property that if Γ ⊢ t: ∑i αi ·Ui then there exists t′ such that t →∗ t′ and t′ = ∑i αi·bi, where each bi is a basis term of type Ui. Such a t′ is obtained by normalising t under all rules but the factorisation rules. Within such a t′ there may be subterms of the form α1·b + α2·b of type α1·V1 + α2·V2, which are redexes for the factorisation rules. Under our type system, the reduct (α1 +α2)·b can be given both the types (α1 +α2)·V1 and (α1 +α2)·V2. The tool we propose in this paper is a ﬁrst step towards lifting the “quantumness” of algebraic lambda-calculi to the level of a type based analysis. It is also a step towards a “quantum theoretical logic” coming readily with a Curry-Howard isomorphism. The logic we are sketching merges intuitionistic logic and vectorial structure. It results into a novel and intriguing tool. The next step in the study of the quantumness of the linear algebraic lambda-calculus is the exploration of the notion of orthogonality between terms, and the validation of this notion by means of a compilation into quantum circuits. The work in [15] shows that it is worthwhile pursuing in this direction. Acknowledgements We would like to thank Michele Pagani and Barbara Petit for enlightening discussions. This work was partially supported by the ANR–JCJC project CausaQ and grants from DIGITEO and Rgion ˆIle-de-France.
Self-Rule to Multi-Adapt: Generalized Multi-source Feature Learning Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Detection<|sep|>In this work, we explore the usefulness of self-supervised learning and UDA for the identiﬁcation of histological tissue types. Motivated by the difﬁculty of obtaining expert annotations, we explore different UDA models using a variety of label-scarce colorectal cancer histopathology datasets. As our main contribution, we present a new label transferring approach from partially labeled, public datasets (source domain) to unlabeled target domains. This is more practical than most previous UDA approaches which are often tailored to fully annotated source domain data or tied to additional network branches dedicated to auxiliary tasks. Instead, we perform progressive cross-entropy minimization based on the similarity distribution among the unlabeled target and source domain samples, yielding discriminative and domain-agnostic features for domain adaptation. In reality, not all tissue types are equally present in a WSI, and some are quite rare. Thus, the extracted patches are imbalanced in regards to class labels (categories), which imposes signiﬁcant challenges for the trained models to generalize well. For example, mucin is frequently present in mucinous carcinoma but is scarcely found in adenocarcinomas. Throughout various label transfer tasks, we show that our proposed Self-Rule to Multi-Adapt (SRMA) method can discover the relevant semantic information even in the presence of few labeled source samples, and yields a better generalization on different target domain datasets. Moreover, we show that our model deﬁnition can be generalized to a multi-source setting. As a result, the proposed model is able to learn rich data representation using multiple source domains. Another example is the complex stroma class, which can be further divided into three subcategories (immature, intermediate, or mature), whose occurrences are highly variable and which are linked to patients’ prognostic factor [Okuyama et al., 2020]. Possible future work could take this class imbalance across WSIs into account and aim to improve the quality and variety of the provided positive and negative examples. In addition, publicly available datasets are so far mostly composed of curated and thus homogeneous patches in terms of tissue types. This data, however, do not capture the heterogeneity and complexity of patches extracted from images in the diagnostic routine. This can lead to erroneous detections, e.g., background and stroma interaction being interpreted as adipose tissue. Thus, ﬁnding a self-supervised learning approach that can also properly embed mixed patches is a possible future extension of this work. Furthermore, the SRMA framework is also highly modular and can thus be used for similar problems in other image analysis research ﬁelds. The selected backbone can be replaced, and the used data augmentations adapted to better ﬁt with the task and data at hand. Lastly, the patch-based segmentation using our method can also be applied in a clinical context. Many clinically relevant downstream tasks depend on accurate tissue segmentation, such as tumor-stroma ratio calculation, disease-free survival prediction, or adjuvant treatment decision-making. This work was supported by the Personalized Health and Related Technologies grant number 2018-327, and the Rising Tide foundation with the grant number CCR-18-130. The authors would like to thank Dr. Felix Müller for the annotation of the WSI crops, M. Med. Philipp Zens for his feedback on the complex stroma detection, Dr. Christophe Ecabert for providing GPU resources, and Guillaume Vray for the computation of the CycleGAN baseline, which all greatly helped the evaluation of our method.
A Converse for Fault-tolerant Quantum Computation<|sep|>In this paper, we obtain a lower bound on the required redundancy for fault-tolerant computation by a quantum circuit. The bound is gate size g divided by the coherent information of a g-fold noisy channel. This bound is tighter than the best existing bound and can be extended to the case where the noise on the outputs of a gate are correlated. This bound leads to a tighter noise threshold. For obtaining this bound, inspired by the information theoretic works for noisy classical circuits [18], [19], a connection between fault-tolerant computation and ﬁnite blocklength channel coding is cultivated. We believe that our bound can be further tightened by combining our approach with techniques developed in [1], [11], [12]. This would be an interesting direction for future exploration. We outline an approach for obtaining a tighter second order bound in Appendix E. As discussed in Sec. III, obtaining Ic(N ⊗k) involves a 2k-dimensional non-convex optimization problem. As understanding non-convex optimization is of mathematical interest due to its usefulness in modern machine learning, an exploration for efﬁcient computation of Ic(N ⊗k) can be of independent mathematical interest. For proving 1, we ﬁrst state an important lemma. Lemma 1: Suppose the conditions (i)–(iii) in Theorem 1 hold. Then, for ǫ-accurate computation we need 2 ) ≥ 1 − ǫ L. Proof of Theorem 1: Proof of this lemma is presented in Appendix B. Here, we prove Theorem 1 using this lemma. First, by using the fact that 1 − x ≤ exp(−x), we obtain the following upper bound. Clearly the above upper bound is bounded by the sum of the maximum of the following optimization problems: P1 and P2. As P1 is a maximization of sum of convex functions subject to a linear constraint, P1 is maximized at an extreme point, as it is a convex maximization. Under condition (iii) in Theorem 1, 2 ln 1
Strong Baselines for Complex Word Identification across Multiple Languages<|sep|>The monolingual and cross-lingual models presented achieve comparable results against more complex, language-speciﬁc state-of-the-art models, and thus can serve as strong baselines for future research in CWI. In addition, our analysis of the dataset could help in the design of better guidelines when crowdsourcing annotations for the task. Dataset creators may wish to only allow single words to be chosen as complex to avoid labelling inconsistencies. In case MWEs are being permitted, we suggest instructing annotators to chose the smallest part of a phrase they ﬁnd complex (French annotators for the Second CWI Shared Task sometimes grouped individual complex words into a complex MWE (Yimam et al., 2018)). This work was initiated in a class project for the NLP module at the University of Shefﬁeld. The authors would like to acknowledge the contributions of Thomas Dakin, Sanjana Khot and Harry Wells who contributed their project code to this work. Andreas Vlachos is supported by the EPSRC grant eNeMILP (EP/R021643/1).
Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks<|sep|>In this paper, we introduced a novel dataset for vehicle identiﬁcation that, to the best of our knowledge, is the ﬁrst to consider the same camera view of most city systems used to enforce trafﬁc laws; thus, it enables to extract features with quality and also to retrieve accurate information about each vehicle, reducing ambiguity in recognition. To explore the Vehicle-Rear dataset, we designed a twostream CNN architecture that combines the discriminatory power of two key attributes: the vehicle appearance and license plate recognition. For this purpose, we proposed a novel approach to compute textual similarities from a pair of license plate regions which were then combined with shape similarities extracted from a Siamese architecture. The proposed architecture achieved precision, recall and F-score values of 99.35%, 98.5%, 98.92%, respectively. The combination of both features (vehicle shape and OCR) brought an F-score boost of nearly 5%, solving very chal lenging instances of this problem such as distinct vehicles with very similar shapes or license plate identiﬁers. Finally, although we achieved an F-score of 98.92% there is still room for improvement. Some open research problems are (i) designing novel networks that could extract vehicle information with the same quality from even smaller image patches; (ii) designing a one-stream architecture that has performance comparable to multi-stream architectures; and (iii) exploring other ﬁne-grained attributes or temporal sequences for vehicle identiﬁcation. This work was supported in part by the National Council for Scientiﬁc and Technological Development (CNPq) under Grants 428333/2016-8, 313423/2017-2, 309292/2020-4 and 308879/2020-1, and in part by the Coordination for the Improvement of Higher Education Personnel (CAPES) under Grant 88887.516264/2020-00. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. Additionally, we thank all the support given by Curitiba’s City Hall, Aditya Choudhary for his help with the code, and Diogo C. Luvizon for all his support in recording the videos used in our experiments.
Narrow-band anisotropic electronic structure of ReS$_2$<|sep|>In conclusion, our study has revealed the bulk electronic structure of the TMDC semiconductor ReS2 for the ﬁrst time. We have imaged how the presence of an inplane structural distortion leading to the formation of Re zig-zag chains, combined with weak interlayer coupling in this compound, drives the formation of quasi-onedimensional electronic states in its valence bands, underpinning its anisotropic optoelectronic properties.. However, we have also shown how this breaks down both near to the band edges, and at higher binding energies, where a more signiﬁcant three-dimensionally of the electronic states results due to the mixing in of atomic orbitals with out-of-plane character. The resulting electronic structure still leads to a direct or nearly-direct band gap semiconductor in bulk, unlike for other bulk TMDCs such as MoS2, but with the band extrema located away from the zone centre along kz. The kz-dispersion of the band edge states naturally explains the blueshift observed in the peak energy of photoluminescence when ReS2 is thinned to a single monolayer [5], as quantum conﬁnement in the z-direction will increase the energy of the direct band gap in this compound. Together, this opens exciting new possibilities for tuning and utilising the anisotropic electrical and optical response of ReS2 bulk and thin-ﬁlm samples. Note: While ﬁnalising our work, a paper appeared on arXiv also reporting a non-negligible out-of-plane dispersion of ReS2 [46]. Their data seems consistent with ours. We are also grateful to Dr. Daniel Wolverson, University of Bath, UK, for discussing the results of their simi
Deep Learning Based MAC via Joint Channel Access and Rate Adaptation<|sep|>In this work, we have investigated an intelligent DL-MAC protocol that can achieve joint channel access and MCS selection. Our DL-MAC is designed using DNN under supervised learning. DL-MAC aims at improving the spectrum efﬁciency of Wi-Fi networks on the 2.4GHz band. Our DL-MAC exploits the real wireless data sampled on the 2.4GHz band in actual wireless environments. Then we experimentally verify the performance of DL-MAC, and the experimental results show that the DL-MAC can achieve more efﬁcient channel access with higher transmission rates compared to the CSMA/CA channel access and traditional rate adaptation schemes. Since our DL-MAC is developed from real sampled data, it is very practical and can work in real wireless networks.
Sound-Triggered Collapse of Stably Oscillating Low-Mass Cores in a Two-Phase Interstellar Medium<|sep|>We consider the linear and nonlinear stabilities of a twophase gas in the low-mass star forming regions, where a dense core of adiabatic index γ = 0.7 is surrounded by an almost isothermal warm diﬀuse gas of γ = 1.1 and in between is a blanket of transition layer. We show that a soft dense core can be stably oscillating as a result of a small average density gradient. A molecular core of 10 K temperature, 0.2 pc size and 1.5 M⊙ mass yields a typical oscillation period about 2 Myr. However the core will relax and slowly evolves to spontaneous collapse in tens of Myr if left unperturbed. This relaxation of dense core is found possible due to the presence of a vortex ﬂow, which is generated by the very existence of entropy gradient around the soft dense core. We have also demonstrated a triggered collapse mechanism of a stably oscillating soft core by means of a smallamplitude background wave. This mechanism involves resonant interactions, but the resonance is broad-band, indicative of low-quality factor resonance. The conditions for the triggered collapse are that wave frequencies are comparable to and up to a factor a few higher than the core oscillation frequency, and that the encountering phase should be within about ±90 degrees of the optimal phase. The latter condi Figure 7. Four sequential snapshots of a Gaussian sound wave propagating from left to right interacting multiple ellipsoidal cores. It is noted that two ((a) and (b)), out of ﬁve, cores collapse during the wave passage. The ﬁve cores have identical oscillation frequencies but diﬀer by the wave encounter phases. tion means that this mechanism has about 50% eﬃciency even when the resonance condition is met. The triggered collapse mechanism however requires long waves. Sound waves in the warm HII region of 10, 000 K must have a wavelength ranging from 6 to 36 pc to produce eﬃcient interactions. These small-amplitude long waves may be originated from active sources located tens of pc, or even one hundred pc, away from the star forming regions through the push of ionization fronts and outﬂows that varies on the time scale of 0.7 − 4.5 × 106 years. The passage of the sound wave can trigger several collapsing cores at diﬀerent locations and diﬀerent times. Thus, stars may form with tight spatial-temporal correlation tracing the propagation of the wave front, yielding propagating star formation for cores within a range of masses. If the temperature of diﬀerent cores remains roughly about 10 K, the oscillation period will scale linearly with the core size, and if these cores are not much below the Jeans instability boundary, they will have masses scaled linearly with the core size, thus the oscillation period. A single wave passage can therefore trigger collapse of cores within a mass range slightly short of one order of magnitude. It follows that young stellar objects (YSO) may have spatial-temporal correlation. This prediction can be tested by measuring the locations, ages and spectral types of YSO in a region of 100 pc. Over this region, the sound transit time is on the order of 107 years. This time scale coincides the life time of YSO before the YSO enters the main sequence, during which the time stamps of various evolutionary stages are distinct, permitting the propagating star formation scenario to be testable. In this work, we do not consider the eﬀects of rota
PAC-Bayesian Bounds for Deep Gaussian Processes<|sep|>The contribution of this paper is the derivation of PAC-Bayesian statements for several DGP models. Precise answers in different situations when modeling with DGPs e.g. designing controllers, are helpful in the overall modeling process. With the shown PAC-Bayesian statements we can give these answers. We see, that we can have the same intuition about the link of PAC-Bayesian bound and the marginal likelihood for our models, which use a variational bound instead of the the marginal likelihood. To show consistency, we extended the loss property to a quadratic-form-Gaussian loss function property, with convergence of order O � 1 N → ∞. We further showed several new forms of the bound in Theorem 5 and following, which give insights in the theory and practice. Our experiments show the evolution of the convergence for many data-sets used throughout the community for the DGPs of Mattos et al. (2016) and F¨oll et al. (2019) for the case of dynamic modeling.
Correlated density-dependent chiral forces for infinite matter calculations within the Green's function approach<|sep|>We have presented calculations for SNM and PNM exploiting the recently extended SCGF method. We include consistently chiral 2B and 3B forces in the ladder approximation. To take into account the eﬀect of 3NFs without the need to explicitly calculate the 3B dressed GF, we have used 1B and 2B eﬀective interactions. These include reducible eﬀects of 3B physics in both 1B and 2B interactions. The eﬀective interactions are averages of the original 3B force performed with the use of dressed propagators. Unlike previous calculations, the results presented here include consistently dressed internal propagators in the construction of a correlated density-dependent 2B and 1B forces. The use of consistent 1B Green’s functions in the averaging procedure is the main innovation of this paper. In fact, to evaluate the eﬀective 2B operator, we have implemented the correlated SP momentum distribution function obtained consistently at each step of iteration in the SCGF approach. The density-dependent force has been calculated from the contraction of the 3B terms appearing at N2LO in the chiral expansion. Particular attention has been paid in considering all possible terms arising in the averaging, following previous works from Refs. [21, 45]. We have tested the modiﬁcations of partial waves when including the density-dependent force on top of the 2B N3LO force. Furthermore, we have analyzed diﬀerent ways of evaluating the 3B force average. These include the use of an uncorrelated momentum distribution, as well as variations on the regulator function in the momentum integrals of the 3B chiral force. We ﬁnd small diﬀerences between diﬀerent averaging procedures. This validates results obtained with uncorrelated averages in the literature. The small diﬀerences between approximations can be ascribed to the availability of momenta when integrating over the third particle. We have subsequently analyzed the eﬀect of including the correlated density-dependent 2B force in microscopic and macroscopic properties of SNM. The momentum distribution at diﬀerent densities is rather insensitive to the inclusion of 3NFs. In addition, it is independent from the average procedure used in the construction of the contracted force. A similar independence has been observed for the total energy of SNM. We ﬁnd a small enhancement of the absolute values of the total energy in going from a lower to a higher availability of momentum states in the average. For the ﬁrst time, we have presented results for PNM in the framework of the extended SCGF method. We ﬁnd a repulsive eﬀect associated to the inclusion of the contracted 3NFs in the entire density range. We have explored the dependence of our results on the underlying uncertainties associated to the c1 and c3 LECs in the 2B and the 3B nuclear force. By applying a low-momentum evolution on the 2B part, we have tested the perturbative nature of PNM. We have observed that the LECs uncertainty dominates at high densities, however other sources of error in the calculation must be taken into account in order to drive a sound conclusion over this aspect. We have also performed calculations for PNM exploiting the N2LOopt 2BF. This has allowed for a consistent comparison with inﬁnite-matter coupled-cluster calculations. The agreement between both methods is very good, which indicates that many-body errors are under control. The work presented here has focused on the construction of correlated chiral density-dependent forces consistent with the many-body framework used. The dressed average takes into account the correlations which characterize the system in the speciﬁc conditions under study and therefore goes beyond previous uncorrelated calculations. Further improvements include two-body averages using fully dressed two-body potentials and systematic studies of the uncertainties associated to the interaction. We consider these a step forward in the consistent inclusion of 3NFs in inﬁnite nuclear-matter calculations. In this appendix, we provide explicit expressions for the density-dependent interaction obtained by applying the averaging procedure of Eq. (13) to a chiral N2LO 3NF. We follow the notation of Ref. [45], but we highlight explicitly the eﬀect of correlations and regulators in the one-body momentum integrations. In the limit of a a zero-temperature uncorrelated momentum distribution and an external regulator (as deﬁned in the text), we recover the expressions of Ref. [45]. Whatever the structure of the 2B density-dependent force, it can always be expressed in a generic form [56]. Consider, for instance, the most general form for the matrix elements of a two-nucleon potential which is charge independent, Hermitian and invariant under translation, particle exchange, rotation, space reﬂection and time reversal: V (k, q) = V s c + τ1 · τ2V v c (A1) +[V s σ + τ1 · τ2V v σ ]σ1 · σ2 +[V s σq + τ1 · τ2V v σq]σ1 · q σ2 · q +[V s SL + τ1 · τ2V v SL]i(σ1 + σ2) · (q × k) +[V s σL + τ1 · τ2V v σL]σ1 · (q × k)σ2 · (q × k) . The subscripts denote the following: c for the central term, σ for the spin-spin term, σq for the tensor term, SL for the spin-orbit term, and σL for the quadratic spin-orbit term. All contributions are presented in an isoscalar V s and isovector V v form. This expression is useful in identifying the diﬀerent contributions of the density-dependent interaction which arise from contractions of the 3NF terms written in Eqs. (7)-(9). Furthermore, this form is helpful in ﬁnding the partial-wave decomposition of the matrix elements [45, 56, 63]. We only consider matrix elements which are diagonal in relative momenta, i.e. |k| = |k′| = k. The generalization of the previous expression to oﬀ-diagonal elements is possible. For non-diagonal momentum matrix elements, however, Eq. (A1) includes a further operatorial structure [56], which complicates the partial-wave decomposition. We therefore extrapolate oﬀ-diagonal momentum matrix elements from diagonal ones, with the prescription k2 → (k2 + k′2)/2 as proposed in Ref. [45].
High-precision molecular dynamics simulation of UO2-PuO2: pair potentials comparison in UO2<|sep|>of uranium dioxide (UO2), the use of graphics processors  (GPU) and NVIDIA CUDA technology has allowed  performing a large amount of numerical experiments for  10 sets of pair potentials (SPP) in a wide range of  temperatures (from 300K up to melting point) with a  step of 1K which guaranteed high accuracy of the  temperature dependences charts for characteristic  thermophysical quantities. revealed the λ-peak of heat capacity with each of 10  considered SPPs. Although λ-peaks were not always  visible or unclear in a CP chart, they are unambiguously  characterized in a CV chart. properties is demonstrated by two recent SPPs MOX-07  [12] and Yakub-09 [20], which both had been fitted to  the recommended thermal expansion in the range of  temperatures  300–3100K.  They  agree  with  the experimental data better at temperatures above 2500K  than the widely used SPPs Basak-03 [10] and  Morelon-03 [8], which were chosen as the best in the  review of Govers et al. [14] [15] (because of MOX-07  and Yakub-09 later publication). The divergence of  model and recommended dependences above 2700K is  presumably due to absence of Schottky defects  formation in MD simulations without surfaces. [11] and Goel-08 [7] potentials, but the worst were the  oldest SPPs: Busker-02 [6], Yamada-00 [9] and  Walker-81 [5] (including its “ionicity” modification  Nekrasov-08 [18], which corrects the lattice constant). revealed an interesting anomalies: anionic sublattice  instability and corresponding first-order phase transition  with inverse density jump (superionic phase is denser  than the crystal), not found by other authors [9] [13] [15]  probably due to coarse temperature step of their  simulations. Instead of a continuous anionic disordering  Yamada-00 have a region of metastable coexistence of  two phases with spontaneous step-wise changes of  characteristics (lattice constant, enthalpy and, as will be  shown in the next article, anion self-diffusion  coefficient). review, we were asked to assess the new shell-core  potentials of Read and Jackson [37] (Read-10) and ab  initio potentials of Tiwary et al. [17]. We examined  Read-10 SPP in the approximation of rigid ions, and its  results with almost linear temperature dependences,  melting point of ~6600K and CV λ-peak temperature of  ~4700K are placed between the results of Busker-02 and  Nekrasov-08. Therefore (from results of Busker-02 and  Read-10 SPPs) one can see that shell-core potentials with formal charges are unsuitable for use without  shells. In contrast, for example, to the shell-core  potentials Goel-08 with ionicity of 0.725, which provide  more adequate behavior in approximation of rigid ions.  Regarding ab initio potentials of Tiwary et al., they are  going to be considered in our future article on MDsimulation  of  PuO2  and  MOX  (including  the examination of melting [31], superionic transition [36]  and diffusion in both quasi-infinite periodic crystals and  finite nanoscopic crystals with surface (surrounded by  vacuum), as well as simulation of plutonium dioxide and  MOX fuel of (U,Pu)O2 type.
Politics of Adversarial Machine Learning<|sep|>Adversarial ML is at a pivotal moment. As these systems become more widely deployed, theoretical attacks and defenses rooted in the academic literature will become the stuff of people’s lives. We have merely scratched the surface of what a political analysis of adversarial machine learning attacks and defenses might illuminate. The adversarial ML community has the opportunity to learn from scholars of science and technology studies, anthropology, and critical race theory — as well as human rights and ethics literature more generally — and to be in conversation with protesters, researchers, and others who seek to attack systems for socially beneﬁcial reasons. Through understanding lived experiences of resistance, applying the lessons of other disciplines, as well as reﬂecting upon the work of those seeking to prevent similar outcomes with spyware, the adversarial ML community can not just understand its work as political but take afﬁrmative steps to ensure that it is used primarily for good.
Transposing Noninvertible Polynomials<|sep|>n � and group G = ⟨J⟩, and m ∈ N representing the number of variables in a candidate W T , it is impossible to construct AW,G ∼= BW T ,{0} in the following
A proposed atom interferometry determination of $G$ at $10^{-5}$ using a cold atomic fountain<|sep|>In this paper a preliminary study of a determination of G at 10 ppm using a cold atom fountain is reported. With proper implementation of the method described in [9], systematic eﬀects due to the cloud size, temperature and trajectories are suppressed. In order to push the accuracy towards the 10−6 level, the implementation of ultra-cold atomic sources and large momentum transfer atom optics can be useful to enhance short term sensitivity and optimize the control over systematic shifts. If spurious magnetic ﬁelds represents the ultimate accuracy limit, using 88Sr instead of 87Rb could be a viable option [20]. However, it is likely that main issues might come from the source mass itself. In this case shape characterization below 1 µm seems unavoidable and inhomogeneities in the source mass material could be really hard to characterize.
One-Dimensional Traps, Two-Body Interactions, Few-Body Symmetries: I. One, Two, and Three Particles<|sep|>work was done and much of it discussed in Spring 2014. Special thanks goes to D. Blume, K. Daily, N. Mehta,
The singularity theorems of General Relativity and their low regularity extensions<|sep|>We begin this ﬁnal section with a summary and some conclusions drawn from the results presented here. Then we will discuss some further perspectives of the approach at hand as well as alternative approaches to singularity theorems beyond the smooth setting. In this review we have discussed the extension to C1-spacetimes of the classical singularity theorems of GR, which under physically reasonable condition assert causal geodesic incompleteness. These results, as well as the C1-extension of the Gannon-Lee theorem in [SS21], which we have avoided to discuss here, are entirely in the spirit of the classical theorems. That is, they extend the original conditions in a natural way, use essentially the same line of arguments and come to the same conclusions as the classical results, however, with one noteworthy extension: The Hawking-Penrose and the Gannon-Lee theorem add a further alternative to causal geodesic incompleteness, namely the branching of maximising causal geodesics. The proofs are, at the one hand, based on the recent extensions of causality theory to low regularity Lorentzian metrics and, on the other hand, rely on an extension of the focusing results for causal geodesics. Indeed the latter, which is achieved via a regularisation approach, is the main technical advance presented here. Next we brieﬂy discuss the further prospects of this approach. It is generally expected that the causality parts of the results extend to C0,1-metrics,29 since such metrics still belong to the so-called causally plain ones, cf. [CG12, Def. 1.16]. The latter allow for essentially the same causality theory as smooth spacetimes, see [CG12, Thm. 1.25]. Indeed, it is only below Lipschitz regularity that such core features of causality theory as the push-up principle and the openness of I+ become an issue [GKS19], and the lightcones may form subsets of full measure [CG12, Ex. 1.11]. On the analytic side, it seems also feasible to extend the recent techniques to locally Lipschitz metrics. Here, one primary task is to extend the Friedrichs Lemma 4.4, which lies at the analytical core of the regularisation techniques, as it allows to derive from the distributional energy conditions of the singular metric useful surrogate energy conditions for the smooth approximations. Next, Lemma 4.2 which gives the precise speed of convergence of the regularised metrics to the singular one would have to be revisited. Also, the formulation of the energy conditions becomes more subtle since the curvature ceases to be an order-one distribution which forecloses the insertion of C1-vector ﬁelds which was essential at least for the genericity condition. Finally, one faces the problem that the right hand side of the geodesic equation now is merely locally bounded and it seems unavoidable to resort to non-classical solution concepts such as Filippov solutions [Fil88], see also the discussion in Section 4.1. Still this is actually a long way from the largest possible class where the curvature can be (stably) deﬁned in an analytical (distributional) way, i.e. the GT-class H2 loc ∩ L∞ loc, see Section 4.2. However, as remarked there, the quest is to at least go into the direction of regularity classes more closely linked to the PDE-approach to GR, e.g. g ∈ H5/2+ε or ∇ ∈ L2. A somewhat related topic is singularities in semi-classical and quantum theories of gravity. There the energy conditions are expected to be violated and to hold only in some averaged sense, see e.g. [Vis95]. The question then arises whether under such assumptions the focusing eﬀect persists, or whether the singularity theorems vanish altogether in the quantum regime, see [SG15, Sec. 8.2], [Sen98, Sec. 6.2]. Indeed, focusing can be maintained under energy conditions averaged along causal geodesics, see e.g. [FG11] and the references therein. Similar results using index 29A class, for which Hawking and Ellis [HE73, p. 268f] still speculate the singularity theorems to hold, cf. also Section 4.1. form techniques have appeared in [FK20] and recent work is concerned with energy conditions directly related to quantum energy inequalities, see e.g. [FFK21]. There is certainly a technical proximity of the methods used there and the ones described in this review, and future research will investigate their interrelations more closely. Now shifting away from the more analytic parts of the singularity theorems we ﬁrst turn to causality theory. It has long been clear that causality theory is very robust in general. Most arguments are rather topological in nature and can actually be seen as belonging to frameworks more abstract than Lorentzian diﬀerential geometry. Indeed, Ettore Minguzzi in [Min19a] has recently put forward a very general theory of causal cone structures, that is a theory of upper semi-continuous distributions of cones over manifolds (which generalise the lightcone of Lorentzian metrics). In this setting it is indeed possible to establish the causal core of some of the singularity theorems: One may see the analytic concepts like the energy conditions, focusing results, etc. used throughout this note merely as tools that produce subsets in M that possess speciﬁc causality properties. Completely removing them from the arguments one arrives at purely causal results. To give some ﬂavour of these, we quote a version of the Penrose theorem which appeared as [Min19a, Thm. 2.67], for more details see Sec. 2.15 there. Theorem 5.1 (Causal Penrose Theorem). Let (M, C) be a globally hyperbolic closed cone structure admitting a non-compact stable Cauchy hypersurface. Then there are no compact future trapped sets and if S is non-empty and compact there is an inextendible future null geodesic entirely contained in E+(S). Another and quite diﬀerent approach to singularity theorems has been opened up in the context of the recently developed synthetic approach to Lorentzian geometry put forward by Michael Kunzinger and Clemens S¨amann in [KS18]. The Lorentzian length spaces introduced there are the analogue of metric length spaces, which have long been used as an essential tool to extract the metric core of many notions and results in Riemannian geometry, see e.g. [BBI01]. Lorentzian pre-length spaces (X, d, ≤, ≪, τ) are metric spaces (X, d) together with with a preorder ≤ and a transitive relation ≪ contained in ≤ (which model the causal and timelike relations of Lorentzian geometry) and a lower semi-continuous map τ : X × X → [0, ∞] that satisﬁes the reverse triangle inequality (and models the Lorentzian distance function). Such a space is called a Lorentzian length space, if, in addition to some technical conditions, τ is intrinsic in the sense that the distance between points deﬁned via the sup of the τ-length of connecting causal curves coincides with their τ-distance. Causality theory in Lorentzian length spaces [KS18, GKS19, AHCPS20] extends standard causality theory beyond the spacetime setting to which it reduces for continuous spacetime with strongly causal and causally plain metric. In this setting it becomes possible to formulate synthetic versions of the singularity theorems, in the sense that the energy conditions are implemented as synthetic curvature bounds. A ﬁrst theorem in Lorentzian length spaces that are warped products and which uses suitable sectional curvature bounds (implying Ricci curvature bounds in such geometries), based on triangle comparison is the following version of the Hawking theorem, cf. [AGKC22, Cor 6.2(ii)]: and let Y = I×f X be a warped procduct with I = (a, b) and f : I → (0, ∞) smooth.31. Assume that Y has timelike sectional curvature bounded below by 0 and that f is nonconstant. Then a > −∞ or b < ∞ and hence Y is past or future timelike geodesically incomplete. Of course, implementing the classical energy conditions (SEC) or (NEC) rather amounts to Ricci curvature bounds than to sectional ones. Indeed, synthetic Ricci curvature bounds have been intensively studied in Riemannian geometry using optimal transport, see e.g. [Vil09]. These techniques have recently been transferred to the smooth Lorentzian setting setting in [McC20, MS22] and further extended to the synthetic setting of Lorentzian length spaces by Fabio Cavaletti and Andrea Mondino in [CM20]. The basic idea is that timelike lower Ricci bounds can be characterised in terms of the convexity of an entropy functional along lp-geodesics in the space of probability measures, where lp is the Lorentz-Wasserstein distance. The corresponding timelike curvature-dimension conditions TCD(K,N) and its weaker variant the timelike measure contraction property TMCP(K,N) then allow to formulate a version of the Hawking singularity theorem which we here quote in a loose way omitting technicalities, cf. [CM20, Thm. 5.2] for the precise version: Theorem 5.3 (TMCP-Hawking Theorem). Let X be a timelike non-branching, globally hyperbolic Lorentzian pre-length space satisfying a TMCP-property. Let V be a Borel achronal future timelike complete subset with mean curvature bounded above. Then every future timelike geodesic starting in V has a bounded maximal domain of existence. To sum up, in this review we have discussed the classical singularity theorems of GR and sketched the main arguments leading to their proofs. One may actually identify two main lines in these arguments, the analytical and the causal one. The former is concerned with providing focusing results for causal geodesics using the energy conditions which lead to the occurrence of conjugate or focal points and hence provide estimates on when causal geodesics stop maximising the Lorentzian length. The second, causal line of arguments gives criteria for maximising causal geodesic to exist. Confronting these two threads leads to a contradiction unless some of the causal geodesics become incomplete. In the main part of this work we have extended the causal and the analytic line of arguments to Lorentzian metrics of regularity C1 and have presented corresponding extensions of the classical theorems. The main achievements presented are on the analytic side using a regularisation approach that allows to deal with the distributional curvature of associated with a C1-metric. In the ﬁnal discussion we have complemented these results with an overview of recent versions of singularity theorems in a purely causal setting as well as in a 30That is a metric length space where each pair of points can be joined by a minimising curve. 31In the space Y , τ is deﬁned via the sup of the length of future directed causal curves γ = (α, β), synthetic setting. It is clear that these results and techniques are still fresh, and that many interesting lines of research emerge from here. Also, it is unclear to date how the synthetic results precisely relate to the analytical approach put forward here and its possible extension to even lower regularity discussed above. In any case, it can ﬁrmly be stated that the singularity theorems are not only an integral part of GR and Lorentzian geometry but, even more than half a century after they ﬁrst emerged, are still an interesting ﬁeld of research holding many quests to be resolved in the future—both from the physical side, see e.g. [SG15, Sec. 8], as well as from a mathematical perspective as laid out here. The author wishes to thank his frequent collaborators M.K., C.S., J.A.V., and J.D.E.G. for their friendship and support as well as our joint (former) students M.S., M.G., B.S., and A.O., who have contributed so much to the whole enterprise. This work was supported by FWF-project P33594 of the Austrian Science Fund.
Global hot-star wind models for stars from Magellanic Clouds<|sep|>We calculated global (uniﬁed, hydrodynamic) model atmospheres for a set of model O stars with metallicities corresponding to those in Large and Small Magellanic Clouds. Our models solve CMF radiative transfer, kinetic equilibrium (NLTE), and hydrodynamical equations from (quasi-)hydrostatic atmosphere outwards to expanding stellar wind. Therefore, the models predict the radial varia
State selective detection of hyperfine qubits<|sep|>We generalize two detection methods [1, 2] for qubits with only a single possible state change during the detection process. This generalized treatment is applicable to qubits that undergo several state changes during the detection procedure such as, for example, hyperﬁne qubits realized with trapped ions or neutral atoms, or solid state qubits such as NV centers in diamond. By introducing matrices of probabilities instead of single probability functions, numerical simulations as well as real-time experimental detection procedures of the generalized qubit detection methods can be eﬃciently implemented. Experiments carried out using a hyperﬁne qubit in 171Y b+ agree well with results of
Bipartite Mixed Membership Distribution-Free Model. A novel model for community detection in overlapping bipartite weighted networks<|sep|>In this paper, we propose a novel model, named the Bipartite Mixed Membership Distribution-Free (BiMMDF) model, to model the general bipartite weighted networks in which nodes can belong to multiple communities and all entries of adjacency matrices can be any ﬁnite real numbers. To the best of our knowledge, this is the ﬁrst model to generate overlapping bipartite weighted networks. BiMMDF has no constraint on distributions of the adjacency matrix but only requires that the expectation adjacency matrix has a block structure that is directly related to node memberships for a bipartite weighted network. In particular, BiMMDF can model overlapping bipartite signed networks by choosing a carefully designed discrete distribution. An eﬃcient spectral algorithm is used to ﬁt BiMMDF. This algorithm can exactly return node memberships when using the expectation adjacency matrix Ω to replace the adjacency matrix A as input, and this guarantees BiMMDF’s identiﬁability in turn. By considering the sparsity parameter ρ and distribution variance parameter γ, we build a general theoretical guarantee on estimation consistency for the algorithm under mild conditions under BiMMDF for any distribution. For a speciﬁc distribution, we can obtain respective theoretical upper bounds of error rates from the general theoretical results immediately as long as we have computed the distribution’s variance parameter γ. The inﬂuence of ρ on the algorithm’s performance, the range of ρ, and the upper bound of γ under diﬀerent distributions are carefully analyzed based on our theoretical results in Examples 1-8. We also obtain the separation conditions of a standard bipartite weighted network for diﬀerent distributions in Equation (2), where the algorithm’s error rates are small with high probability as long as Equation (2) holds even when the network size is ﬁxed. The separation conditions are carefully analyzed for diﬀerent distributions in Examples 1-8, and we ﬁnd that diﬀerent distributions have diﬀerent separation conditions. The inﬂuence of ρ on the algorithm’s performance and the behavior diﬀerence phenomenon on separation conditions of diﬀerent distributions are veriﬁed by substantial computer-generated bipartite weighted networks under BiMMDF. We also apply the algorithm to estimate node memberships for some real-world directed weighted networks with encouraging results. Our model BiMMDF is useful to model overlapping bipartite weighted networks with true node memberships under different distributions, and the algorithm DiSP used to ﬁt BiMMDF can eﬃciently estimate node memberships for both computer-generated and real-world overlapping bipartite weighted networks. We expect that our BiMMDF model will have wide applications in complex networks to study the properties of bipartite weighted networks generated from any distribution, just as the mixed membership stochastic blockmodels has been widely studied in recent years. One limitation of the algorithm used in this paper is, the number of communities K should be known in advance. Though we estimate K by eigengap like Rohe et al. (2016), rigorous methods should be developed to estimate K for bipartite weighted networks generated under BiMMDF under diﬀerent distributions. Meanwhile, more than our BiMMDF, estimating K for all models listed in Table 2 is an open problem. Developing methods to estimate K for models in Table 2 is a challenging, interesting, and prospective topic. There are many ways to extend our work. First, in this paper, we use a spectral algorithm to ﬁt BiMMDF because this spectral algorithm works for any distribution. It is possible to design new algorithms based on the ideas of nonnegative matrix factorization or likelihood maximization or tensor methods mentioned in Mao et al. (2020) to estimate node memberships for bipartite weighted networks generated under BiMMDF for a speciﬁc distribution. Second, like Rohe et al. (2011); Qin and Rohe (2013); Joseph and Yu (2016); Rohe et al. (2016), it is possible to design spectral algorithms based on applications of modiﬁed Laplacian matrix or regularized Laplacian matrix to ﬁt BiMMDF. Third, it is also important to extend the idea developed in this paper to dynamic bipartite weighted networks and networks with covariates. Fourth, network mining task like link-prediction is an attractive topic for bipartite weighted networks generated under BiMMDF. There are many other ways to extend our work because BiMMDF is a generative model like SBM and MMSB. These ideas can also be applied to models in Table 2 for weighted networks. We leave studies of these interesting topics for our future work.
Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models<|sep|>In this paper, we evaluated several scalable and efﬁcient exploration algorithms for reinforcement learning in tasks with complex, high-dimensional observations. Our results show that a new method based on assigning exploration bonuses most consistently achieves the largest improvement on a range of challenging Atari games, particularly those on which human players outperform prior learning methods. Our exploration method learns a model of the dynamics concurrently with the policy. This model predicts a learned representation of the state, and a function of this prediction error is added to the reward as an exploration bonus to encourage the policy to visit states with high novelty. One of the limitations of our approach is that the misprediction error metric assumes that any misprediction in the state is caused by inaccuracies in the model. While this is true in determinstic environments, stochastic dynamics violate this assumption. An extension of our approach to stochastic systems requires a more nuanced treatment of the distinction between stochastic dynamics and uncertain dynamics, which we hope to explore in future work. Another intriguing direction for future work is to examine how the learned dynamics model can be incorporated into the policy learning process, beyond just providing exploration bonuses. This could in principle enable substantially faster learning than purely model-free approaches.
Secret Key Agreement Using Conferencing in State- Dependent Multiple Access Channels with An Eavesdropper<|sep|>In this paper, we investigated the problem of interactive secret key sharing over a state-dependent multiple access channel with an eavesdropper. In our proposed model, the transmitters share a common key with the receiver over multiple access channel in the ﬁrst round. The conferencing scheme has a beneﬁcial role in the common key sharing. In the second round, the receiver agrees on two independent private keys with the corresponding transmitters using the public channel. The inner and outer bounds on the capacity region have been established for the common and the private keys capacity region.
Background and Imaging Simulations for the Hard X-Ray Camera of the MIRAX Mission<|sep|>The design and development of space astronomy X- and γray experiments require a reliable estimation of the background levels against which the sources of interest will be observed. Particularly, in coded aperture experiments a large detector area is fundamental for achieving competitive sensitivities. In those cases the backgrounds are usually very intense and show inhomogeneities across the detector plane due to geometrical factors. In this paper we show detailed Monte Carlo simulations of the background and imaging observations of a hard X-ray imaging camera that is being developed in the scope of the MIRAX space mission. This instrument is a prototype that is going to be tested in stratospheric balloon ﬂights. The MIRAX mission will play a very important role in the study of hard X-ray sources and transient phenomena, since it will constantly monitor a large area around the central Galactic plane region. We present separate spectra and count distribution across the detector plane for each relevant radiation ﬁeld impinging in the instrument and discuss their contribution. We also take into account the angular distribution of the diﬀerent incident photons and particles. We provide detailed background characteristics and levels for two diﬀerent environments in which the camera is supposed to operate: stratospheric balloon altitudes and near-equatorial low-Earth orbit. The simulations reported here allowed us to deﬁne with high accuracy the best conﬁguration for the shielding structure and coded mask materials of the instrument. In order
Dynamical localization of chaotic eigenstates in the mixed-type systems: spectral statistics in a billiard system after separation of regular and chaotic eigenstates<|sep|>We have used a billiard system of the mixed type (11), with λ = 0.15, as introduced in [46]-[47], and have shown that using the Poincar´e Husimi functions we can separate the regular and chaotic eigenstates. The successful separation of course also entirely conﬁrms the Berry-Robnik picture [5] of separating the regular and chaotic levels in the semiclassical limit, where the tunneling eﬀects can be neglected. With great and unprecedented statistical signiﬁcance we have shown that the chaotic levels exhibit Brody level spacing distribution, whilst the regular levels obey Poissonian statistics. This analysis not only conﬁrms the Berry-Robnik picture [5] of conceptually separating the regular and chaotic levels, based on the PUSC and embodied in formula (1), but also demonstrates that the dynamical localization eﬀects of the chaotic eigenstates are very well captured by the Brody distribution, in analogy with the same ﬁnding in
Code Properties of the Holographic Sierpinski Triangle<|sep|>To summarise our work, we have studied the holographic QECC properties of a boundary region in the shape of a Sierpinski triangle in AdS4/CFT3, where our boundary region was precisely a fractal embedded in a ﬂat 2 dimensional plane. This is relevant for mainstream quantum computation because, while topological codes are the current state of the art, holography-inspired codes seem to have at least one advantage over them, speciﬁcally that holographic codes can be robust against fractal noise while topological codes cannot in three dimensions. That said, we are working in the large N limit, which is infeasible in real life; it is possible that subleading corrections would change this story. We have also argued that this propertie does not generalize to AdS5/CFT4. However, topological codes with fractal geometries are more easily constructed in higher dimensions, whereas holographic codes dominate in lower dimensions. This presents an elegant conceptual picture for when one may favor holographic or holographic-inspired QECC’s over their topological cousins. There are a few potential directions for future study. First, one could study boundary subregions of other fractal shapes, and ask if they also have nice bulk reconstruction properties. Secondly, one can study whether other fractals in higher dimensions have nice bulk reconstruction properties, if they are generalized from lower dimensional analogs that are not Sierpinski. Finally, one could build a practical holographic-inspired QECC and run it on near-term quantum hardware to experimentally demonstrate these robustness properties. 1Note that here we ignore the trivial extension of tensoring the Sierpinski triangle to R1, to promote them to "strips;" this would certainly work, but is not particularly natural in fractal sense.
Multilinear Subspace Clustering<|sep|>In this paper we presented a new model, namely the UOMS model and presented an algorithm unsupervised clustering under this model. We showed that the resulting algorithm is competitive with existing methods while being computationally more efﬁcient. For future work, we will investigate how to deal with the outliers when we are drawing rows and columns from the data and not the entire data itself. Another important avenue is to develop a systematic method with provable guarantees for combining various graph realizations. As is well known, subspace clustering performance depends on the distribution of the data. Therefore, if the relative importance of different instances can be characterized one may perform a weighted graph combination. We will also extend this method for automatic clustering of 3-D data sets such as action videos.
Type 1 low z AGN. I. Emission properties<|sep|>We present and analyze a new sample (T1) of 3 579 broad Hα selected AGN from the SDSS DR7, with logLbHα = 40 − 44, which spans m = 6 − 9 and l = −3 − 0. We add UV (GALEX), IR (2MASS), and X-ray (ROSAT) luminosities to form the mean SED. The main results are: (i) The Hα FWHM velocity distribution dN/dlog∆v is independent of luminosity and falls exponentially with ∆v. The origin of this distribution remains to be understood. (ii) The observed mean 9000 ˚A–1500 ˚A SED, as a function of LbHα, is consistent with a sum of the mean SED of luminous quasars, which scales linearly with LbHα, and a host galaxy contribution. (iii) The host galaxy r-band luminosity function of T1 objects with an extended morphology follows the NEG luminosity function, with a relative normalization of ∼ 3%, suggesting that the host of broad line AGN are NEG, and the AGN probability of occurrence is independent of the host mass. (iv) The mean z band luminosity and the u−z band colour of the lowest luminosity T1 host is identical to NEG. The host colour becomes progressively bluer with increasing luminosity. The implied mean SFR versus Lbol is similar to that found in type 2 AGN. (v) The dispersion in the optical-UV SED in luminous AGN (logLbHα ⩾ 43), is consistent with reddening. This indicates the intrinsic SED of AGN is blue, with a small dispersion, as predicted from thermal thin accretion disc models. (vi) Reddening by dust along the line of sight to T1 AGN is common (40% with E(B−V) ≳ 0.1). (vii) The LbHα versus LX correlation provides a useful probe for unobscured narrow line AGN. It can be used to test if the absence of a broad Hα, in X-ray detected AGN, is signiﬁcant. (viii) The primary parameter which drives αFUV,X is the luminosity, rather than MBH or L/LEdd. (ix) The primary parameter which drives logνLν(FUV)/log LbHα is L/LEdd, which may indicate that lower L/LEdd AGN are more likely to be reddened. This publication makes use of data products from the SDSS project, funded by the Alfred P. Sloan Foundation, data from GALEX supported by NASA, data from the Two Micron All Sky Survey, funded by the NASA and the NSF, and from the ROSAT Data Archive of the Max-Planck-Institut fr extraterrestrische Physik (MPE) at Garching, Germany. A.L. acknowledges support by the Israel Science Foundation grant 407/08.
A new zero-knowledge code based identification scheme with reduced communication<|sep|>In this paper we propose a new variation on Stern’s authentication scheme. Our protocol permits to obtain a gain of more than 40% compared to previous schemes and it is the ﬁrst code based zero knowledge scheme to obtain a signature length of less than 100kb with strong security and small size of keys.
Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image Segmentation using Partially Labeled Data<|sep|>In this paper, we propose a holistic segmentation network (Omni-Seg) that segments multiple tissue types using partially labeled images via a single dynamic head. The proposed method achieves superior segmentation performance with less computational resource consumption. In the testing stage, our single dynamic network achieved “completely labeled” tissue segmentation aggregation from only “partially labeled” training images.
Self-gravitating systems of ideal gases in the 1PN approximation<|sep|>We have built an astrophysical model based on the Maxwell-J¨uttner distribution function within the framework of general relativity which is described using postNewtonian formalism. In order to obtain this expression, we have integrated over the peculiar velocity 3D space, keeping only the relevant terms up to 1PN order. With this ergodic distribution at hand, we have obtained the general form of Newtonian density along with the postNewtonian density and pressure terms which enter in the energy-momentum tensor by using several Gaussian integrals (cf. Appendix B). This allowed us to demonstrate that our general expression coincides with the one reported by Weinberg [9], validating our procedure. As an example of how to apply this procedure, we considered the case of particle four-ﬂow and calculated its temporal and spatial components. From the energy density and pressure terms at 1PN, we looked for static solution by analyzing the boundary value problem. We found that the energy density, pressure and gravitational potentials proﬁles in terms of dimensionless radial coordinate by solving the aforesaid equations numerically. In particular, we found the energy density vanishes for large radii but approaches to a constant value at the origin, further, such value becomes larger for smaller values of ζ0 [see Fig. 1]. Contrary to the case of polytropic ergodic distribution explored in [8], the energy density remains always positive at 1PN order. As part of the process of evaluating the circular velocity proﬁles, we obtained that post-Newtonian curves reach larger values in relation with the Newtonian case. In both cases, we have found that these curves exhibit a linear behavior near the center and then pass through a cusp [see Fig. 2]. Interestingly enough, we have found the behavior of the circular velocity for ideal gases diﬀers from the one where the distribution function is characterized by a polytropic function of the energy [8], since in the latter case the values of the circular velocity in the 1PN approximation are smaller than the corresponding Newtonian approximation. In our case, the large values of the circular velocity in the 1PN approximation are due to the fact that the increase of the temperature of the gas, increases the thermal velocity of the particles of the gas � kT0/m. We have examined the behavior of the dimensionless gravitational potential energy �U. So we have found that the Newtonian gravitational potential energy is always negative, while the post-Newtonian gravitational potential energies change their sign for large values of the radial distance from the conﬁguration center [see Fig. 3]. The temperature of the gas in the post-Newtonian term �φ2/2ζ0 determines the sign change of the gravitational potential energy. The gravitational potential energy exhibits the same behavior that the one associated with a polytropic distribution function with polytropic index n = 3. In addition, we found a parametric proﬁle of the equation of state p(ρ) in terms of the dimensionless radial coordinate, see Fig. 5. At small radii, the ratio pressure-density becomes almost constant; increasing ζ0 the constant reaches larger values or equivalently the thermal energy of the ideal gas decreases considerably. The situation is reversed at large radii. We have patched together two diﬀerent kinds of gravitational potentials at a critical radius, called �rc, in order to have a well-deﬁned boundary problem, provided the potentials became complex beyond this radius. The physical motivation in dealing with this issue was to select gravitational potentials which reproduce a ﬂatten behavior in the rotation curve at large radii, provided the linear and the cusp zones were already described by those potentials obtained by integrating numerically the energy density and pressure coming from the Maxwell-J¨uttner distribution function at 1PN. In doing so, we have shown that one of the potentials has a Coulomb form while the other one presents two kinds of terms, a Yukawa contribution plus a logarithmic term. Indeed, the term responsible for the constant circular velocity at large radii turned out to be the logarithmic term. Regarding the changes introduced by these solutions, we have found that the Coulomb potential led to an inverse square power law for the Newtonian density, as one could expect. However, we could not disentangle the post-Newtonian density from the post-Newtonian pressure, thus we found �ρP N + �pP N = −γe−�r/�r − δ/�r2 which leads to a positive quantity, vanishing for large radii only. G. M. K. and M. G. R. are supported by Conselho Nacional de Desenvolvimento Cient´ıﬁco e Tecnol´ogico (CNPq) and K. W. by Coordena¸c˜ao de Aperfei¸coamento de Pessoal de N´ıvel Superior (CAPES). G. M. K. thanks J. F. Pedraza for suggestions. Here φ is the Newtonian gravitational potential, and ψ and ξi are gravitational potentials in the 1PN approximation. These gravitational potentials are connected with the energy-momentum tensor by Einstein’s ﬁeld equations. Appendix B: GAUSSIAN INTEGRALS Let us summarize the most interesting cases that we have employed within this article. Just to make things more familiar, let us consider integration over a 3D space associated to the peculiar velocity space deﬁned as W∗ = {(w∗, θ, φ) : 0 ≤ w∗ ≤ ∞, 0 ≤ θ ≤ 2π, 0 ≤ φ ≤ π}, where w∗ is a dimensionless peculiar velocity and the volume element is given by d3W ∗ = w∗2dw∗dΩ2 with dΩ2 the 2-dimensional element of solid angle. Let us consider the Gaussian distribution F(w∗) = e−w∗2/2 so the following expressions can be obtained:
Vacuum stability from vector dark matter<|sep|>The vector dark matter model is a theory, which can explain the observed DM relic abundance. It leads to the attractive prediction of the second Higgs boson and allows to alleviate the problem of the vacuum stability. This model can be further tested by sensitive DM direct detection experiments and at LHC by reﬁned measurements of the Higgs or searches for other scalars. A two-component extension of the model is possible.
A classification of transitive links and periodic links<|sep|>One may found similar classiﬁcation theorems of vertex transitive graphs on torus and projective plane by Carsten Thomassen [23]. If one wants to extend our results for these vertex transitive graphs, one might have to choose transitive links in right 3-manifold (maybe torus×I or torus×S1), but we do not know yet. We believe the problem is the symmetry group for transitive link (diagram on S2) or in S3 are just not seriously diﬀerent from Aut(Γ) where Γ is the crossingless graph of the transitive link while this phenomena no longer works for the vertex transitive graphs on torus and projective plane. Acknowledgements The TEX macro package PSTricks [22] was essential for typesetting the equations and ﬁgures. This work was supported by Kyonggi University Research Grant 2012.
Detecting Majorana Bound States by Nanomechanics<|sep|>To conclude, we have presented an idea of coupling Majorana bound states to a sensitive nanoelectromechanical measurement device. We have shown that a setup where an oscillating, doubly clamped beam is tunnel coupled to a topological superconductor gives rise to unique transport signatures based on the interplay between the mechanical excitations and the Majorana bound states. A smoking gun signature of Majorana bound states has been identiﬁed for an oscillator close to the quantum ground state which can be achieved by cooling a 500 MHz resonator to T ≈ 20 mK. This energy scale is well below the large parameter of our model, i.e. the superconducting gap ∆SC. When we, for instance, take an InAs wire proximity coupled to an Nb s-wave superconductor, the superconducting gap of Nb ∆Nb SC ∼ 15 K can induce a superconducting gap of ∆InAs SC ∼ 1 K in the wire15. Hence, our predictions are in principle observable at dilution refrigerator temperatures. SW and BT acknowledge ﬁnancial support from the DFG-JST Research Unit Topological Electronics. TLS acknowledges support from the Swiss National Science Foundation and KB from the Research Council of Norway, FRINAT Grant 191576/V30. We would like to thank Jan Budich, Matthew Gilbert, Taylor Hughes, and Ronny Thomale for interesting discussions.
Indian Language Wordnets and their Linkages with Princeton WordNet<|sep|>In this paper, we describe two resources released along with this paper. We discussed the Indian language wordnets that are part of the IndoWordNet project. We enlisted the statistics of the latest version, which we provide as a single bundle along with this paper. Next, we described the linkage process for creating English-Indian language links using English-Hindi language links. We then enlisted the statistics of the latest version of this linked data, which is also provided along with this paper. In future, we plan to continue building the wordnets and increase linkage. We will also investigate semi-automatic linkage tools such as the ones created by Joshi et al. (2012b), etc. so that the workload on our lexicographers and researchers can be reduced to a certain extent
CAIXA: a Catalogue of AGN In the XMM-Newton Archive I. Spectral analysis<|sep|>Fig. 10. Hard X-ray powerlaw index distribution in CAIXA. Red histograms are narrow-line objects, black histograms are broad line objects and white histograms are for objects with no measure in CAIXA for the Hβ FWHM. See text for details. scured (NH < 2×1022 cm−2) AGN observed by XMM-Newton in targeted observations, whose data are public as of March 2007. We performed a complete and homogeneous spectral analysis of the X-ray data, whose main results can be summarised as follows: – The X-ray spectral index. The average 2-10 keV spectral index for the whole catalogue is 1.73 ± 0.04 (σ = 0.49 ± 0.02) and appears signiﬁcantly steeper in narrow-line objects and quasars with respect to broad-line objects and Seyfert galaxies. The average value for CAIXA and the differences between source populations do not signiﬁcantly change if a
Simultaneous Partial Inverses and Decoding Interleaved Reed-Solomon Codes<|sep|>We have introduced the SPI problem for polynomials and used it to generalize and to harmonize a number of ideas from the literature on decoding interleaved Reed–Solomon codes beyond half the minimum distance. The SPI problem has a unique solution (up to a scale factor), which can be computed by a (new) multi-sequence reverse Berlekamp– Massey algorithm. The SPI problem with general moduli can always (and efﬁciently) be reduced to an SPI problem with monomial moduli. For monomial moduli, the reverse Berlekamp–Massey algorithm looks very much like (and has the same complexity as) the multi-sequence Berlekamp–Massey algorithm of [7], [8]. The SPI problem can be used to analyze syndrome-based decoding of interleaved Reed–Solomon codes. Speciﬁcally, we pointed out a natural partial-inverse condition for the error pattern, which is always satisﬁed up to half the minimum distance and very likely to be satisﬁed almost up to the full minimum distance. If that condition is satisﬁed, the (true) error locator polynomial is the unique solution of a standard key equation and can be computed in many different ways, including the algorithm of [7], [8] and the reverse Berlekamp– Massey algorithm of this paper. Two of the best performance bounds (for two different decoding algorithms) from the literature were rederived and generalized so that they apply to the partial-inverse condition, and thus simultaneously to many different decoding algorithms. In Appendix B, we also give two easy variations of the reverse Berlekamp–Massey algorithm, one of which is a To prove the correctness of Algorithm 3, we augment it with some extra variables and some assertions as shown in Algorithm 9. We will prove these assertions one by one, except that the proof of Assertion (A.1) is deferred to the end of this section. Assertion (A.2) is obvious both from the initialization and from (A.11). Assertion (A.3) is the result of the repeat loop, as discussed at the beginning of Section V-A. Assertion (A.4) is obvious. Assertions (A.5)–(A.8) follow from (A.2)–(A.4), followed by the swap in lines 21–23. As for (A.9), when b(i)(x) is visited for the very ﬁrst time (i.e., the ﬁrst execution of line 26 for some index i), we have d = deg m(i)(x) and rd(i)(Λ) < d is obvious. For all later executions of line 26, we have d = rd(i)(Λ) and d(i) = rd(i)(Λ(i)) before line 26, and rd(i)(Λ) < d after line 26 follows from Lemma 4. To prove (A.10) and (A.11), we note that Line 26 changes the degree of Λ(x) only in iterations where lines 21–24 are executed, see (176) below; every later executions of Line 26 for the ﬁxed i does not change deg Λ(x) because of Lemma 4 and that Λ(i)(x) and d(i) remain the same during the inner repeat loop. If lines 21–24 are executed, then line 26 changes the degree of Λ(x) to which is (A.10). With (A.7), the left-hand side of (176) yields also (A.11). It remains to prove (A.1). First, we note that (A.1) clearly holds when the loop is entered for the ﬁrst time. But if (A.1) holds, then Λ(i)(x) in (A.6) satisﬁes which is (A.1). (Note that (A.1) and (A.4) together provide an alternative proof of Proposition 3.) Finally, we note that the algorithm is guaranteed to terminate because every execution of the repeat loop (lines 10–19) strictly decreases δmax(Λ) or imax(Λ) according to Lemma 4 and the swap in lines 21–23 strictly decreases d(i). 1 for i = 1, . . . , L begin 2 Λ(i)(x) := 0 3 d(i) := deg m(i)(x) 4 κ(i) := lcf m(i)(x) 5 end 6 Λ(x) := 1 7 δ := maxi∈{1,...,L} � deg m(i)(x) − τ (i)�
Composition Properties of Bayesian Differential Privacy<|sep|>Bayesian differential privacy has been recently proposed to broaden the application scenarios of differential privacy when data records have dependencies. In this paper, we formally show that Bayesian differential privacy preserves three nice properties of differential privacy: sequential composability, parallel composability, and post-processing.
Working Principles of Binary Differential Evolution<|sep|>We have conducted the ﬁrst fundamental analysis of the working principles of BDE and found that BDE behaves quite diﬀerently from classic evolutionary algo rithms or distribution-based methods. The dependencies stemming from reusing the same individuals in the mutation operator and from the selection operator appear to be the main reason for this. Unfortunately, they also lead to more diﬃcult mathematical analyses compared with the general univariate algorithms. While many classic evolutionary algorithms and EDAs can generate any search point from the current population, this is diﬀerent for BDE. We proved that from the random initial population, only an exponentially small fraction of the search space is reachable in one iteration. This does not necessarily harm the performance, but it makes it harder to decide whether convergence to the optimum is still possible from the current population. We gave an example showing that this question is more diﬃcult for BDE than for most other evolutionary algorithms. One interesting feature of BDE is that it is more stable (frequencies not subject to a ﬁtness signal stay around 1/2 for a long time) than most other algorithms. This enables BDE to quickly optimize decision variables which initially behave neutral, but then become important (as in the LeadingOnes benchmark function). The potential downside of this is that highly symmetric functions like OneMax, in which each bit position only has a small inﬂuence on the ﬁtness, could be more diﬃcult to optimize. In particular from the view-point of quickly ﬁnding a good, but not necessarily optimal solution, the property to quickly optimize the currently crucial bits appears to outweigh possible performance losses on OneMax type functions. Overall this work shows that BDE has a number of interesting feature not present in most classic evolutionary algorithms (including EDAs). This ﬁrst work on the working principles of BDE suggests to explore these in more detail. This work has not identiﬁed a reason why diﬀerential evolution should in discrete search spaces not be similarly successful as in continuous one search spaces. One clear challenge arising from this work is to devise mathematical analysis methods that can cope with the inherent stochastic dependencies. At the moment, they make it hard to use the rigorous runtime analysis methodology which greatly improved the understanding of classic evolutionary algorithms. The obvious particular problems left open in this work are a completely rigorous runtime analysis (without mean-ﬁeld arguments) for the LeadingOnes, BinaryValue, and OneMax benchmark functions. This work was supported in part by the National Key R&D Program of China (Grant No. 2017YFA0604500), and by the National Natural Science Foundation of China (Grant No.5171101179, 61702297, 91530323).
Synthetically Trained Neural Networks for Learning Human-Readable Plans from Real-World Demonstrations<|sep|>We have presented a system to generate human-readable programs from a real-world demonstration. The system consists of a sequence of neural networks to perform tasks associated with perception, program generation, and program execution. For perception, we introduce image-centric domain randomization leveraging convolutional pose machines, which results in a vision-based network that can be applied to any camera, without assumptions about the camera Fig. 10. Three demonstrations (left column), and snapshots of the robot executing the automatically generated program (remaining columns). From top to bottom, the programs are as follows: “Place yellow on red, green on yellow, and blue on green” (top row), “Place yellow on green, and blue on red” (middle row), and “Place car on yellow” (bottom row). In the second row, notice that the robot recovered from an error in initially misplacing the yellow cube. pose or the presence of speciﬁc background features in the scene. For program generation and execution, we show that fully connected networks, despite their simplicity, generalize surprisingly well when considering relationships, states, and programs not encountered during training. Although training individual networks separately may be suboptimal compared with end-to-end training, it facilitates componentwise testing, interpretability, and modularity. There remain many issues to explore, such as increasing robustness of domain randomization to variations in lighting and color, incorporating context to better handle occlusion, leveraging past execution information to overcome limitations in sensing, and expanding the vocabulary of the human-readable programs. The authors would like to thank the reviewers for their helpful comments. Appreciation also goes to Omer Shapira, Mark Brophy, Hai Loc Lu, and Bryan Dudesh for sharing their expertise and insights regarding the design and implementation of the simulator used for domain randomization, as well as for many helpful discussions.
From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification<|sep|>We introduced the sparsemax transformation, which has similar properties to the traditional softmax, but is able to output sparse probability distributions. We derived a closed-form expression for its Jacobian, needed for the backpropagation algorithm, and we proposed a novel “sparsemax loss” function, a sparse analogue of the logistic loss, which is smooth and convex. Empirical results in multi-label classiﬁcation and in attention networks for natural language inference attest the validity of our approach. The connection between sparse modeling and interpretability is key in signal processing (Hastie et al., 2015). Our approach is distinctive: it is not the model that is assumed sparse, but the label posteriors that the model parametrizes. Sparsity is also a desirable (and biologically plausible) property in neural networks, present in rectiﬁed units (Glorot et al., 2011) and maxout nets (Goodfellow et al., 2013). There are several avenues for future research. The ability of sparsemax-activated attention to select only a few variables to attend makes it potentially relevant to neural architectures with random access memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015), since it offers a compromise between soft and hard operations, maintaining differentiability. In fact, “harder” forms of attention are often useful, arising as word alignments in machine translation pipelines, or latent variables as in Xu et al. (2015). Sparsemax is also appealing for hierarchical attention: if we deﬁne a top-down product of distributions along the hierarchy, the sparse distributions produced by sparsemax will automatically prune the hierarchy, leading to computational savings. A possible disadvantage of sparsemax over softmax is that it seems less GPU-friendly, since it requires sort operations or linear-selection algorithms. There is, however, recent work providing efﬁcient implementations of these algorithms on GPUs (Alabi et al., 2012). We would like to thank Tim Rockt¨aschel for answering various implementation questions, and M´ario Figueiredo and Chris Dyer for helpful comments on a draft of this report. This work was partially supported by Fundac¸˜ao para a Ciˆencia e Tecnologia (FCT), through contracts UID/EEA/50008/2013 and UID/CEC/50021/2013.
Mesh-TensorFlow: Deep Learning for Supercomputers<|sep|>In this paper, we introduce the Mesh-TensorFlow language, facilitating a broad class of SPMD distributed tensor computations. Applying Mesh-TensorFlow to the Transformer model, we are able to train models with 5 billion parameters on up to 512-core clusters, establishing new state-of-the-art results for WMT14 En-Fr translation task and the One Billion Word language modeling benchmark.
The bow-shock and high-speed jet in the faint, 40 arcmin diameter, outer halo of the evolved Helix planetary nebula (NGC 7293)<|sep|>The jet–like feature in the SW outer quadrant of the giant halo of NGC 7293 is conﬁrmed kinematically to have a jet origin for it is has a collimated outﬂow velocity of ≈ 300 km s−1. This jet has an Hα/[N II] 6548, 6584 ˚A brightness ratio of ⩽ 0.2 which conﬁrms its origin as collisionally ionized, nitrogen enriched, material from the progenitor star. The limited length of the jet’s collimated outﬂow suggests it was emitted before the inner helical structure of NGC 7293 was ejected. The bow-shaped outer ﬁlamentary structure in the NE quadrant of the outer halo of NGC 7293 is conﬁrmed kinematically as a bow-shock created as NGC 7293 ploughs through its ambient medium. This interpretation is also consistent both with the magnitude and direction of the PM of the central star. JM is grateful for the hospitality of the National Observatory of Athens in April 2013 when this paper was initiated. PB would like to thank the staff at SPM and Helmos Observatories for their excellent support during these observations. The Aristarchos telescope operated on Helmos Observatory by the Institute of Astronomy, Astrophysics, Space Applications and Remote Sensing of the National Observatory of Athens. GALEX (Galaxy Evolution Explorer) is a NASA Small Explorer, launched in 2003 April. We gratefully acknowledge NASAs support for construction, operation, and science analysis for the GALEX mission, developed in cooperation with the Centre National d’Etudes Spatiales of France and the Korean Ministry of Science and Technology.
Design and Analysis of Dynamic Auto Scaling Algorithm (DASA) for 5G Mobile Networks<|sep|>In this paper, we propose DASA to address the tradeoff between performance and operation cost. We develop analytical and simulation models to study the average job response time Wq and operation cost S. The results quantify
Silicon carbide absorption features: dust formation in the outflows of extreme carbon stars<|sep|>We have presented three previously unrecognized SiC absorption features in the spectra of extreme carbon stars. Together with the seven known SiC absorption stars, this brings the total of known extreme carbon stars with SiC absorption features to ten. Previous radiative transfer models of extreme carbon stars utilized relatively low condensation temperatures. Here, theoretical condensation models have been used to justify much higher condensation temperatures. In addition, our models use graphite instead of amorphous carbon, because of the preferential formation of graphite at higher temperature and the meteoritic evidence. Both the higher condensation temperature (through a decrease in the inner radius of the dust shell) and to a smaller extent the use of graphite will greatly increase the acceleration felt by the dust grains in the shell relative to parameters used in previous research. We have shown that grain-size issues cannot be ignored in the production of models that accurately ﬁt the observed spectra of extreme carbon stars. The size distribution that is needed is not clearly deﬁned because of the inherent degeneracy in radiative transfer modeling. Meteoritic grain-size distributions are as valid as other size distributions with the advantage of being model independent. However, they may underestimate the contribution from small grains. With the exception of IRAS 17534−3030, all sample stars could be modeled with either the generic (MRN) or “meteoritic grain-size distribution IRAS 17534−3030’s narrow SED required the use of the “meteoric” distribution. Furthermore, there is no evidence for any molecular absorption in its spectrum. Because the 11µm feature is still present in the absence of the other molecular features, it supports the attribution of this feature to a solid state carrier. The various parameters compiled in the course of this research (both through radiative transfer modeling and from observations) have been compared in order to identify any correlations, with the result that the cause of diﬀerences is the spectra cannot be attributed to mass-loss rate or gas pressure in the dust condensation zone. In fact the paucity of correlations between parameters echoes the results of Thompson et al. (2006) and suggests that even amongst the extreme carbon stars, variations in C/O, s-process and nitrogen enhancements make 11µm a poor probe of the details of dust shell parameters. The timescales associated with the heavy mass-loss experienced by these extreme carbon stars are very short (tens to hundred of years) and are not consistent with timescales for the superwind. This indicates that the heavy mass-loss phase of carbon stars is not a direct result of thermal pulse (although thermal pulses may be the root cause). This work is supported by NSF AST-0607341. We are very grateful to the referee, Albert Zijlstra, for his comments which have signiﬁcantly improved this paper. Kevin Volk is also thanked for helpful advice on this paper.
Neural Template: Topology-aware Reconstruction and Disentangled Generation of 3D Meshes<|sep|>We presented a novel framework called DT-Net that enables a topology-aware mesh reconstruction and promotes mesh generation with disentangled controls. A key design is to learn to form a topology-aware neural template specific to each input then deform it to reconstruct a detailed 3D object. This scheme decouples the 3D reconstruction process into two sub-tasks, effectively accommodating for the variations in topology. Importantly, our new design provides a disentangled representation of topology and shape in the latent space, enabling controllable object generations by manipulating the learned topology code and shape code, which are not achievable by the existing reconstruction methods. Extensive experiments also manifest that our method produces high-quality meshes with diverse topologies and fine details, performing favorably over the state of the arts. Acknowledgments. We thank anonymous reviewers for the valuable comments. This work is supported by the Research Grants Council of the Hong Kong Special Administrative Region (Project No. CUHK 14206320 & 14201921).
Two Higgs Doublet Model with Scalar Mediation via Yukawa Interactions<|sep|>Despite the simplicity of the two Higgs doublet models in comparison to richer alternatives, such as the SM, the model possesses a number of interesting features even when the interaction vertices are considered with their tree level form up to a renormalization scheme dependent constant. There are two most interesting features in the model. First, there is strong indication of universality of Yukawa coupling in the model which does not depend upon the Higgs (bare) mass. The renormalized coupling is also free of cutoﬀ eﬀects for all practical purposes. Second, the renormalized mass consistently decreases as the bare mass is lowered, irrespective of the couplings. It encourages for studying the phenomenon of dynamical mass generation, for example, to search for critical coupling in the model. Diﬀerent quantities calculated in the model have diﬀerent cutoﬀ eﬀects. Among the propagators, the two Higgs propagators are least eﬀected. The model contains only two cubic vertices which are (up to the renormalization terms) kept ﬁxed at their respective couplings. If there is a non-trivial phase structure in the parameter space for the chosen form of the vertices, it should have appeared as a deviation of qualitative behavior of the quantities calculated here. In absence of such deviations, it is concluded that the model exhibits the same physics for the chosen form of the interaction vertices. In other words, the correlation functions and the parameters describe the same phase in the parameter space of the studied model.
Inferring the star formation histories of the most massive and passive early-type galaxies at z<0.3<|sep|>In this work we analysed the median stacked spectra of a sample of 24488 SDSS DR4 ETGs at z < 0.3 in order to derive the properties of their stellar populations and to reconstruct their star formation histories. This study focused on the extreme cases of passive ETGs with the highest stellar masses (log(M/M⊙) > 10.75) which, in turn, represent the passive envelope of the galaxy population at 0 < z < 0.3. The sample was divided into four bins with increasing stellar mass. The stacked optical spectra were analysed by means of the full-spectrum ﬁtting technique using the public code STARLIGHT in order to derive constraints in a way complementary to the traditional method of Lick indices based on a few absorption lines. Using this archaeological approach, we also inferred the properties of the progenitors of massive and passive ETGs. Our main results can be summarized as follows. – First of all, STARLIGHT was tested against stellar population synthesis models in order to assess its reliability in the case of spectra similar to those of ETGs. It was found that the software retrieves the stellar population main properties (age, metallicity, SFH, dust extinction) and the velocity dispersion with a percentage accuracy higher than 10 % for SNR ≳ 10 – 20, even if more complex SFHs are considered. In order to minimize the uncertainties, the STARLIGHT analysis was applied to stacked spectra with typical SNR ∼ 80. – Mass-weighted stellar ages are very old, increasing with cosmic time from ∼ 10 to ∼ 13 Gyr, and show a clear tendency to increase with mass despite the rather limited mass leverage of our sample, which is selected to include only the most massive systems (log(M/M⊙) > 10.75). This result provides an additional support to the downsizing evolutionary scenario where more massive galaxies are older than less massive ones. The derived ages are broadly compatible with those found with diﬀerent full-spectrum ﬁtting codes (e.g. McDermid et al. 2015 and Conroy et al. 2014) and with the Lick indices approach (e.g. Thomas et al. 2010; Graves & Schiavon 2008b). The bottom line is that the most massive and passive ETGs represent the oldest galaxies in the present-day Universe, with ages close to the age of the Universe itself in the most extreme cases. – Mass-weighted metallicities are slightly supersolar, with a median Z ∼ 0.029±0.0015 (Z ∼ 0.027±0.0020 for MS11 models), increase with stellar mass, and do not show any signiﬁcant trends with redshift. This supports the interpretation that the analyzed galaxies are very old objects which formed the bulk of their stars much earlier and did not enrich signiﬁcantly their interstellar medium with new metals at later epochs (however, we remind that we have a limited leverage in cosmic time, i.e. ∼ 3.3 Gyr). This contrasts with the metallicity evolution of star-forming galaxies (e.g. Maiolino et al. 2008, Mannucci et al. 2009, Foster et al. 2012, Zahid et al. 2013, De Rossi et al. 2015). Although our metallicities are broadly consistent with other results at a ﬁxed stellar mass, a large scatter is present amongst the estimates of the metallicity (Z) obtained with diﬀerent methods. – The SFHs inferred with the full-spectrum ﬁtting suggest that the star formation occurred during an extended period of time. The SFHs are globally compatible with a parametric function of the form SFR(t) ∝ τ −(c+1)tc exp(−t/τ), where the typical value of τ and c are always short, with τ decreasing from 0.8 to 0.6 Gyr (with a dispersion of ± 0.1) from lower to higher masses and c ∼ 0.1 (with a dispersion of ± 0.05) regardless of mass, reproducing the fast rise of the SFR at the beginning of the SFH. Other works highlighted a stronger dependence of the star formation timescale on the mass (e.g. Thomas et al. 2010, McDermid et al. 2015). – Based on the inferred SFHs, we derive that the ETGs of our sample formed about 50 % of their stellar mass at early epochs, i.e., on average, at z ≳ 5 (which decrease to z ≳ 3 if the small 0.5 Gyr systematic introduced by median stacked spectra is taken into account). Moreover, the most massive galaxies formed it ∼ 0.2 Gyr (∼ 0.1 Gyr for MS11 models) before than less massive systems. – Low dust extinction (AV ≲ 0.2−0.25 mag) are required to ﬁt the spectra. In addition, in the case of BC03 models, a trend is present showing that AV tends to decrease for increasing stellar mass. The reliability, the signiﬁcance and the interpretation of this trend requires further analysis beyond the scope of this paper, but a qualitative interpretation may be that the most massive galaxies were able to "clear" their interstellar medium more eﬃciently probably due to more intense feedback processes due to star formation and/or AGN activity. – The stellar velocity dispersions 200 ≲ σ ≲ 300 km s−1 estimated with STARLIGHT are consistent with estimates by other groups within the uncertainties, and increase with stellar mass. – Based on the SFHs, we reconstructed the mass assembly history and the properties of the progenitors of the most massive ETGs of the present-day Universe, assuming that no coeval mergers have occurred during the evolution of the analyzed galaxies. The SFHs imply that these galaxies were vigorously forming stars and assembled large stellar masses (≳ 75 % of the total stellar mass) by z ∼ 5+3.8 −1.4, with SFR up to 50−370 M⊙ yr−1 (SFR ∼ 140−750 M⊙ yr−1 for MS11 models). Possible star-forming progenitors with these characteristics have indeed been found in samples of galaxies at 4 < z < 6.5 selected in the submm/mm. The inferred SFHs also predict the existence of quiescent galaxies at 2 < z < 4 characterized by large stellar masses and low speciﬁc star formation rate. Galaxies with these properties have been spectroscopically identiﬁed at z ∼ 2 − 3, and photometric candidates at higher redshifts also exist. – The Re of our sample ETGs are signiﬁcantly smaller than those of the parent sample of SDSS DR4 ETGs with log(M/M⊙) > 10.75. This implies higher stellar mass densities and suggest that they should have formed from high density progenitors – Based on the number density of the analyzed galaxies in the present-day Universe (log(ρn) ∼ −3.98 ± 0.1 Mpc−3), the inferred number densities of the progenitors are consistent with the literature, within the uncertainties. Moreover, the mean star formation rate density implied by the star-forming progenitors does not violate the Madau & Dickinson (2014) relation at high redshift (z ∼ 5), in the case of BC03 models. – We ﬁnd a good agreement among our results concerning ages, metallicities and SFHs and the ones obtained using the ﬁt to individual spectral features (Lick indices) and α-elements abundances. We thus suggest the full-spectrum ﬁtting to be a complementary and valid approach to derive the stellar population properties and the star formation histories of early-type galaxies. ACKOWLEDGEMENTS The authors are grateful to Emanuele Daddi for useful discussion, Maria Cebrián for providing the eﬀective radii from the NYU-VAGC catalogue and Helena Domínguez-Sánchez, Kenneth Duncan, Andrea Grazian, Chiara Mancini and Adam Muzzin for providing their mass functions. AC is also gratetful to Alfonso Veropalumbo for helpful discussion. We also acknowledge the support of the grant PRIN MIUR 2010 The dark Universe and the cosmic evolution of baryons: from current surveys to Euclid. Funding for the SDSS and SDSS-II has been provided by the Alfred P. Sloan Foundation, the Participating Institutions, the National Science Foundation, the U.S. Department of Energy, the National Aeronautics and Space Administration, the Japanese Monbukagakusho, the Max Planck Society, and the Higher Education Funding Council for England. The SDSS Web Site is http://www.sdss.org/. The SDSS is managed by the Astrophysical Research Consortium for the Participating Institutions. The Participating Institutions are the American Museum of Natural History, Astrophysical Institute Potsdam, University of Basel, University of Cambridge, Case Western Reserve University, University of Chicago, Drexel University, Fermilab, the Institute for Advanced Study, the Japan Participation Group, Johns Hopkins University, the Joint Institute for Nuclear Astrophysics, the Kavli Institute for Particle Astrophysics and Cosmology, the Korean Scientist Group, the Chinese Academy of Sciences (LAMOST), Los Alamos National Laboratory, the Max-Planck-Institute for Astronomy (MPIA), the Max-Planck-Institute for Astrophysics (MPA), New Mexico State University, Ohio State University, University of Pittsburgh, University of Portsmouth, Princeton University, the United States Naval Observatory, and the University of Washington.
Lie-B\"acklund symmetry and non-invariant solutions of nonlinear evolution equations<|sep|>We have constructed solutions of nonlinear evolution equations describing the diﬀusion processes in nonhomogeneous medium by using the generalization of Svirshchevskii method given in [10]. We show that the method gives us the possibility to obtain solutions which are not invariant ones in the classical Lie sense. We use the Lie-B¨acklund symmetry operators of the third order ordinary diﬀerential equations. The corresponding ansatzes reduce nonlinear diﬀusion equations to the systems of three ordinary diﬀerential equations. This way one is able to obtain the solutions which cannot be constructed by classical Lie method in the cases when the dimension of invariance Lie algebra is equal to 1,2,3. If the Lie algebra of Lie invariance group of the transport equation under consideration is four-dimensional, then the solutions obtained by using our method could also be obtained via classical Lie symmetry method as is shown in Section 2. These results agree with the ones given in [12] if the solutions are found with the help of point conditional symmetry operators. The approach can be also applied to construct other classes of diﬀusion-type equations (and exact solutions) by using Lie-B¨acklund symmetry of other ordinary diﬀerential equations given in Appendix.
Infrared scaling for a graviton condensate<|sep|>In this paper we have found two results. First, by tracing out the charged source, i.e. the electron, we have found that the photon vacuum is displaced. This is analogous to the displaced coherent state of a photon vacuum with an occupation number of photons, Np, which scales as the ﬁne structure constant. Since the ﬁne structure constant remains less than one, it implies that the electron remains a quantum system for energies below the Planck scale. Furthermore, the photon number is always bounded by the Bekenstein’s entropy bound. All the computations are based on the adiabatic evolution of the charged source and the photon vacuum. The second result, we have shown that the gravitational interaction with the matter is entirely diﬀerent as expected. By tracing out the matter, we found that the graviton vacuum is also displaced. Still, now the occupation number of gravitons is proportional to the Area. The current result generalises our previous result [18], where we have generalised the computation for an arbitrary energy momentum tensor and beyond EinsteinHilbert action. Motivated by [21, 22], we have found that by including the large degeneracy provided by the occupation number of the gravitons in the displaced vacuum, the infrared scale emerges. This infrared scale can be larger than the gravitational radius. In fact, in the simple toy model we have studied, i.e. an in-falling thin shell of matter, the emergence of the infrared length scales appears to be L ≤ 3.8rg. We have further noticed that the appearance of this infrared scale in gravity persists even if we go beyond Einstein-Hilbert action. Apparently, such a new scale is always determined by the large occupation number of gravitons in the displaced vacuum, see Eq. (66). Our results prompt us to investigate further open questions such as the new scale of gravity in a generic collapsing geometry, particularly in the context of cosmology [40], and in the formation of an ultra compact object. Would the appearance of a new scale alleviate cosmological Big Bang singularity or resolve black hole singularity? Would the appearance of a new scale in gravity alter the way we view traditional black holes? Would there be associated observational signatures which can be falsiﬁable by future gravitational wave detectors? All these questions remain outstanding, indeed they go beyond the scope of the current paper, and deserves a detailed investigation. We would like to thank Samir Mathur for helpful discussions. MT and SB would like to acknowledge EPSRC grant No.EP/N031105/1, SB the EPSRC grant EP/S000267/1, and MT funding by the Leverhulme Trust (RPG-2020-197). AM’s research is funded by the Netherlands Organisation for Science and Research (NWO) grant number 680-91-119.
Anomaly Detection via Self-organizing Map<|sep|>We have presented a novel method called SOMAD for anomaly detection and localization which is based on selforganizing map. It achieves state-of-the-art performance on MVTec AD datasets. Moreover, our method needs less time consumption and performs better than other methods, especially on non-aligned data, which is more in line with the actual industrial production situation. Acknowledgements. This work is funded by National Key Research and Development Project of China under Grant No. 2020AAA0105600 and 2019YFB1312000, National Natural Science Foundation of China under Grant No. 62006183, 62076195 and 62072367, and by China Postdoctoral Science Foundation under Grant No. 2020M683489.
Gravitational field of one uniformly moving extended body and N arbitrarily moving pointlike bodies in post-Minkowskian approximation<|sep|>Extremely high precision astrometry, high precision space missions and certain tests of General Relativity, require the knowledge of the metric tensor of the solar system, or more generally, of a gravitational N-body system in post-Minkowskian approximation. So far, the metric outside of massive and moving bodies in only known in post-Newtonian approximation. In our study, we have considered the metric of massive bodies in motion in post-Minkowskian approximation, that is valid to any order in velocity v/c. Two diﬀerent scenarios were on the scope of our investigation: (i) the case of one body with full mass and spin multipole structure in uniform motion (v = const) in post-Minkowskian approximation, and (ii) the case of N arbitrarily moving pointlike bodies with time-dependent speed v (t) in post-Minkowskian approximation. For the ﬁrst problem, a co-moving inertial system of coordinates has been introduced and the starting point is the local metric given in terms of Damour-Iyer moments. A Poincar´e transformation then yields the metric tensor in the global system (13) in post-Minkowskian approximation. We have demonstrated that our results are in agreement with known results for pointlike masses having monopole and spin structure and moving uniformly. Then we have derived the global metric for pointlike massive bodies in arbitrary motion having monopole structure (43) and spin structure (56). We have shown that our results are exact to post-Minkowskian order for the problem of pointlike mass-monopoles and spindipoles in arbitrary motion. The problem to ﬁnd a global metric for a system of N arbitrarily moving and arbitrarily shaped bodies in post-Minkowskian approximation is highly complex and one encounters many subtle diﬃculties. Especially (in contrast to the case of pointlike bodies), such a metric cannot be obtained by a simple instantaneous Poincar´e transformation of the metric (2) for extended bodies. Moreover, it is obvious that for this problem a corresponding accelerated local reference system has to be constructed. It is clear that such a local system can be deﬁned in many diﬀerent ways (e.g., Fermi normal coordinates or special harmonic ones). As is well known, however, that even in the case of vanishing gravitational ﬁelds, i.e., in Minkowski space, such a construction is highly problematic; the reader is referred to [21–28]. At the moment being, we consider our study as one more step towards the aim of a global metric for a system of N arbitrarily shaped and arbitrarily moving massive bodies in post-Minkowskian approximation. The authors acknowledge detailed and fruitful discussions with Professor Sergei A. Klioner. This work was supported by the Deutsche Forschungsgemeinschaft (DFG). All relations given here will be valid to ﬁrst order in G, without explicit indication. For weak gravitational ﬁelds the metric diﬀers only slightly from ﬂat space metric, that means −G implies that the gothic metric is not a tensor but a tensor density. Let us further note the following relations for the trace-reversed metric perturbation:
Analyzing the Flux Anomalies of the Large-Separation Lensed Quasar SDSS J1029+2623<|sep|>The most probable conﬁguration of SDSS J1029+2623 is a quasar at zs = 2.197 triply imaged by a cluster of galaxies at zl ≃ 0.60 and a dark matter clump with a mass ∼ 109 M⊙ slightly oﬀset from the position of image B. From our analysis, we also have found that oﬀset optical and radio emission regions caused by extended radio jets are a probable explanation for the disagreement between the optical and radio ﬂux ratios. Higher resolution radio data acquired with VLBI could possibly resolve the radio jets of SDSS J1029+2623 (see More et al. 2009, Figure 1 vs. Figure 4), although an extremely long exposure time would be necessary. Additionally, 60 ksec ACIS-S observations made with Chandra on 2010 March 11 detected all three quasar images as well as the lensing cluster. Conclusions about SDSS J1029+2623’s X-ray emission have yet to be made, but we are hoping to compare the Xray properties with our independent strong lensing constraints and deﬁnitively eliminate microlensing and dust extinction as possible causes of the ﬂux anomaly by studying the spectrum of the faintest image (Oguri et al., in prep.). Finally, the Hubble Space Telescope is set to observe SDSS J1029+2623 for 7 orbits in cycle 18, increasing the resolution and depth of our optical data. We plan on searching for faint perturbers near images B and C that may be responsible for the ﬂux anomalies and will try to detect the lensed quasar’s host galaxy in order to study the lens potential around components B and C in more detail. Hopefully our multi-bandpass data will ﬁrmly establish RMK would like to thank Wendy B. Harris for her lensmodel expertise. GTR was supported in part by an Alfred P. Sloan Research Fellowship. CSK is supported by NSF grants AST-0708082 and AST-1009756. RHB’s work was supported in part under the auspices of the US Department of Energy by Lawrence Livermore National Laboratory under contract W-7405-ENG-48. Based in part on data collected at Subaru Telescope, which is operated by the National Astronomical Observatory of Japan.
Self-Organizing Maps as a Storage and Transfer Mechanism in Reinforcement Learning<|sep|>We described an approach to efficiently store and reuse the knowledge of learned tasks using self organizing maps. We applied this approach to an agent in a simulated multi-task navigation environment, and compared its performance to that of an ϵ−greedy approach for different values of the exploration parameter ϵ. Results from the simulations reveal that a modified exploration strategy that exploits the knowledge of previously learned tasks improves the agent’s learning performance on related target tasks. Overall, our results indicate that the approach proposed here transfers knowledge across tasks relatively safely, while simultaneously storing relevant task knowledge in a scalable manner. Such an approach could prove to be useful for agents that operate using the reinforcement learning framework, especially for real-world applications such as autonomous robots, where scalable knowledge storage and sample efficiency are critical factors.
Warm molecular gas and kinematics in the disc around HD 100546<|sep|>– We present evidence for warm molecular gas associated with the disc around HD 100546, in the regions within 400 AU from the star, successfully separated from more extended material in our CHAMP+ observations; – The gas kinematics are consistent with Keplerian rotation around an 2.5 M⊙ star of a disc with a 400 AU radius, viewed at an inclination of 50◦ from face-on; – The 12CO (6–5)/(3–2) line ratio of 1.1±0.6 is higher than measured towards discs around T Tauri stars, likely due to a more eﬃcient gas heating of the disc containing PAHs by the stronger UV radiation from the B9 star; – Our data testify to the signiﬁcant molecular gas reservoir in the disc, consistent with the total disc masses of more than 10−3 M⊙. We exclude the possibility of a low-density disc and optically thin 12CO emission. – Line asymmetry seen in 12CO J =6–5 and J =3–2 lines is explained by a temperature asymmetry, with one side of the disc slightly colder than the other, possibly due to a partial obscuration of one side by a warped inner disc or a high disc rim. We exclude radial asymmetry, midplane density asymmetry and mispointing as possible causes; – Our modelling shows that, due to the eﬃcient heating of the disc gas by the star, both low-J and high-J 12CO lines are dominated by the outermost regions of the disc, though slightly diﬀerent vertical disc layers with ∆T=15-20 K; – We ﬁnd that in ‘colder’ discs where temperatures of the emitting regions are close to 20-30 K in the outer disc – colder Herbig Ae discs and especially T Tauri discs – the high-J lines probe a larger extent of the disc, starting from as little as 50 AU; – The non detection of [C I] J =2–1 line may indicate the presence of more carbon-ionising photons than assumed in the B9 model atmosphere. Future observations with ALMA will be crucial to characterise the disc around HD 100546, and spatially resolve its kinematics and structure. In particular, these observations will allow a detailed comparison between the spatial distribution of the gas traced by the rotational transitions of 12CO and its isotopologues, and the dust traced with the millimetre continuum emission. Herschel far-infrared data can probe even higher J 12CO transitions, as well as [O I] and [C II] lines originating from the disc surface. Being a bright, isolated source suspected to harbour a planet, the disc around HD 100546 is one of the prime targets to probe disc structure in the early planet-building phases. Acknowledgements. The research of O. P. and M. R. H. is supported through a VIDI grant from the Netherlands Organisation for Scientiﬁc Research (NWO). We thank L. Kristensen for assistance with the reduction of the APEX data, and C. M. Wright for useful discussions. We thank the APEX staﬀ for assistance during the observations. Construction of CHAMP+ is a collaboration between the Max-Planck-Institut f¨ur Radioastronomie Bonn, the Netherlands Institute for Space Research (SRON), the Netherlands Research School for Astronomy (NOVA), and the Kavli Institute of Nanoscience at Delft University of Technology, with support from NWO grant 60006331010.
ProposalCLIP: Unsupervised Open-Category Object Proposal Generation via Exploiting CLIP Cues<|sep|>In this paper, we have presented ProposalCLIP, an effective approach for unsupervised open-category object proposal generation. In our approach, a proposal selection model is ﬁrst introduced to recognize open-category objects and select proposals for them by CLIP-based objectness scores. Secondly, a graph-based merging model uniﬁes fragmented proposals based on CLIP feature cues. Thirdly, we introduce a regressor module that leverages CLIP cues to reﬁne proposals. Experimental results have demonstrated that our proposed method is capable to ﬁnd open-category proposals in an unsupervised manner. Our method also outperforms existing state-of-the-art methods by a large margin on three popular datasets, and shows the beneﬁts for downstream tasks. Social impacts: We test our method on three popular datasets, which might contain ethic biases. It would be better to collect more unbiased data in real world to reduce this impact.
3D Numerical Simulations of f-Mode Propagation Through Magnetic Flux Tubes<|sep|>We have carried out a series of numerical simulations of plane-wave propagation through ﬂux tubes of diﬀerent sizes. In the ﬁrst part of this study, we tested the convergence in order to determine the minimum-sized tube that we could reliably simulate with the resolutions that we had available. We then investigated how diﬀerent-sized tubes interacted with an incoming f-mode wavepacket. Diﬀerent scattered wave-ﬁeld patterns were observed for ﬂux tubes with diﬀerent radii. In agreement with previous suggestions, we found that when the ﬂux tube was small compared with the wavelength of the incoming wave, mainly the m = 1 kink modes were excited. For mid-ranged tubes both the m = 1 kink and m = 0 sausage modes were excited, and for large tubes numerous m and radial order modes were excited. Our results demonstrate that numerical simulations are an eﬃcient, robust, and straightforward method to treat the action of ﬂux tubes on waves regardless of their size. The simulations are a rich source of information, which can be used to better understand the observations in future studies.
You Only Need One Model for Open-domain Question Answering<|sep|>In this paper, we propose a novel language model architecture that embeds the retriever and the reranker as internal passage-wise attention mechanisms and a training method to effectively train this model. This singular model architecture efﬁciently uses model capacity by cascading and sharing the representations from retriever to reranker to the reader leading to better gradient ﬂow for end-toend training. We evaluate our model on Natural Questions and TriviaQA open datasets and for a ﬁxed parameter budget, our model outperforms the previous state-of-the-art model by 1.0 and 0.7 exact match scores. We show detailed ablations and analyses of each component of our approach. Our future work is to conduct more experiments on various knowledge-intensive tasks and extend this model to match query and passage in multiple or hierarchical representation spaces. One caveat of sharing representation for multiple tasks like retrieval, reranking, and reading is that these show different over-ﬁtting tendencies during ﬁne-tuning where the training data is limited. We found that answer generation over-ﬁts more easily compared to the retrieval. Answer generation relies on more expressive representation via cross attention, which may make it easier to memorize the output and hence make it more vulnerable to over-ﬁtting. Furthermore, at the ﬁrst ﬁne-tuning iteration, the model is trained by zero-shot retrieval results from the pre-trained model that has a relatively low recall rate and can harm the answer generation training. To refresh the over-ﬁtted answer generation parameters, and to start from training data with a high recall rate, we simply re-initialize the model with the pre-trained YONO model after a few ﬁne-tuning iterations. However, we believe that this issue should be addressed carefully using a more sophisticated solution. We further discuss the over-ﬁtting issue and effect of re-initialization in Appendix A.
Efficient Cavity Searching for Gene Network of Influenza A Virus<|sep|>In this paper, we provide a method to discover potential cavities in Flu-Network based on hypergraph representation learning. We propose a HyperSearch algorithm based on the k-clique-reconstruction module and hypergraph convolution module. This algorithm has been veriﬁed in experiments that the algorithm can be used to search for high order cavity structures in a computationally complex network. By analyzing the ablation studies, the k-clique-reconstruction module which we provide in this paper can improve the performance of general hypergraph convolution modules.
Strategies for spectroscopy on Extremely Large Telescopes. I - Image Slicing<|sep|>This is the first part of a study of strategies to produce instruments for Extremely  Large Telescopes that are both affordable and usable before the full benefits of  Adaptive Optics delivering near-diffraction-limited imaging become available. By slicing the relatively wide slit matched to natural or partially-corrected seeing, the  size of spectrographs can be reduced since the required beam diameter scales  inversely with the slicing factor for fixed resolving power. However, other effects act  against this trend, placing limits on the amount by which the instrument volume can  be reduced. This has taken careful account of in a simple spectrograph model  that  includes scaling parameters which we set to give a good fit to three different types of  spectrograph used currently on 8-m telescopes. The model allows us to explore the  range of fitting parameters consistent with the calibrators. We adopt 4 different models, based on a pair of  binary options. The first specifies  whether the extra slices are accommodated within the same instrument or by multiple  replicated spectrographs designed for a single slice. The second option specifies  whether the camera focal ratio or the detector oversampling is modified to provide  correct image sampling, subject to limitations imposed by the Nyquist citerion and the  fastest practicable camera design. The cost of the instrument is estimated , following  industry practice, on the volume of the instrument and by scaling the detector subsystem independently by the number of pixels. • Slicing can potentially reduce the cost of an instrument by large factors: 2-8  for  medium spectral resolution to at least 70  for high resolution spectrographs. • For visible/near-infrared multiplexed spectrographs working at low-medium  spectral resolution (R = 5000) a slicing factor of about 5 is optimal • These conclusions assume a harsh scaling law for the detector sub-system.  Relaxing this produces a substantial reduction in cost for large slicing factors,  making the case for slicing more compelling. • There is an upgrade path to integral field spectroscopy as the quality of the input  images improves simply by modifying the data reduction software. However this  may require larger slicing factors than the cost-optimised values proposed above,  thereby requiring a larger initial cost. Finally, this model may be applied to many other instrument concepts: only a very  basic, and rather conservative, set has been examined here. The author will be happy  to discuss this further with interested people.
Capacity Region of Vector Gaussian Interference Channels with Generally Strong Interference<|sep|>Let i denote the index of the messages transmitted by user 1 and i ∈ � 1, 2, · · · , enR1� . For each i, Let j denote the index of the message transmitted by user 2 and j ∈ � 1, 2, · · · , enR2� . For each j, Receiver 1 looks for unique indices (ˆi, ˆj) such that � qn, xn 1 � ˆi � , xn 2 � ˆj � , yn 1 � ∈ A(n) ǫ (Q, X1, X2, Y1) (167) Receiver 2 looks for unique indices � ˆi, ˆj � such that � qn, xn 1 � ˆi � , xn 2 � ˆj � , yn 2 � ∈ A(n) ǫ (Q, X1, X2, Y2) . (168) By symmetry, we assume that the transmitted indices are i = j = 1. For user 1, we deﬁne the following E1 ij = � (qn, xn 1 (i) , xn 2 (j) , yn 1 ) ∈ A(n) ǫ (Q, X1, X2, Y1) � . (169) We denote the region deﬁned in (15a)-(15d) as R. To show that R is a subset of the Han and Kobayashi
Towards time-dependent, non-equilibrium charge-transfer force fields: Contact electrification and history-dependent dissociation limits<|sep|>In this paper, we showed that introducing formal oxidation states to the split charge model allows one to reproduce various generic charge transfer features that occur during the contact formation and breaking of two solid clusters. The main idea behind the approach is that oxidation states of atoms can only change via a redox reaction involving two or more nearby atoms. This way partial charges are neither automatically constant nor a welldeﬁned function of the instantaneous conﬁguration but may be history dependent. An important many-body eﬀect implicitly contained in the model is that integer charge transfer can occur between two dielectrics of diﬀerent electron aﬃnity. This happens even though breaking (adiabatically) any molecule consisting of two (stable) atoms in an inert environment automatically results in two neutral atoms, at least for realistic values of their atomic hardnesses and electronegativities. The reason for this behavior lies in a reduced ionization energy I for collective systems in comparison to isolated atoms. In a future publication, we will demonstrate that the methodology explored in this work moreover reproduces a variety of generic features that occur during the discharge and the aging of batteries [4]. No currently popular force ﬁeld can achieve this, because they all assume a one-to-one correspondence between atomic coordinates and interatomic forces. Time-dependent density functional theory approaches are likewise likely to fail to produce such features due to the serious constraints on particle numbers and simulation times. Our case study reproducing the generic features of contact-induced charging between two dielectrics assumes implicitly that electric current is eﬀected by the passage of integer charges, i.e., electrons, from one solid to the other. We nevertheless do not intend to take sides in the discussion of what charge types are mainly responsible for charge transfer [30,32]. Protons, hydroxides, or other ions might be equally or even more important than electrons. Our purpose is to provide a formalism with which this issue can be addressed in terms of many-atom, force-ﬁeld based simulations. This includes the ability to ascertain whether an atom leaves a solid or a cluster as a neutral atom or as an ion. Moreover, the redoxSQE method allows for the (local) electron or ion exchange between stoichiometrically similar materials, as long as local properties materials are distinct, e.g., because of chemical heterogeneity or diﬀerent local radii of curvature. Thus, there is a chance to identify reasons for the frequently observed charge transfer between chemically identical materials. Of course, recognizing the main mechanism of contact electriﬁcation for a speciﬁc system requires a realistic and chemistry-speciﬁc parameterization. It is encouraging that the original SQE formalism reproduces fractional charges and polarizabilities quite accurately for systems in which atoms have a well-deﬁned oxidation state [8,10,13,12,18,20,17]. Moreover, it has been shown that the reactant and the product of a redox reaction, namely terminally blocked and zwitterionic pentaalanine, can be described quite accurately if the functional groups are subjected to constraints [20]. The eﬀects of the constraints are very similar to those induced by assigning discrete oxidation states to atoms. We are thus conﬁdent that it is possible to describe products and reactants of redox reactions reasonably well within one uniﬁed framework, although it might be necessary to deﬁne (eﬀective) chemical hardness values for all relevant oxidation states. What still has to be achieved, however, is a way to assign reasonable rates for the redox reactions, in particular near the transition state. In this context it needs to be mentioned that our classical description of an isolated diatomic molecule produces a zero gap between ground state and ﬁrst excited state at the transition state. This violates the non-crossing rule of quantum mechanics [37]. As for any coarse-graining eﬀort, the important question is whether the information lost is relevant to the problem of interest. To answer this question, we ﬁrst note that coarse-graining — as we do it here implicitly from a full quantum-mechanical description of the electronic structure to partial charges and oxidation states — must entail loss of information, unless the full description is not optimal. As a beneﬁt, larger systems and longer time scales become accessible. Second, most experiments studying contact charging between dielectrics [30,32], in particular those where electron hopping in the dielectrics are negligible, ﬁnd that the rate of withdrawal of the surfaces barely aﬀects the ﬁnal charge. This observation can be rationalized quite easily by noticing that the tunneling rate of electrons or the transfer rates of ions decrease roughly exponentially with the distance between two solids. Last but not least, in the all-atom battery simulations that will be presented in a future publication, we ﬁnd that both cutoﬀ rl and the rate of trial integer-charge transfer moves do not change results within the statistical error bars. We therefore conclude that it should indeed be possible to parameterize redoxSQE for many applications in such a way that the outcome of the simulations produce the correct ensemble average, e.g., accurate distributions of charge transfer between colliding clusters or rubbing solids. We thank the J¨ulich Supercomputing Centre for computing time and Razvan Nistor and Ling Ti Kong for useful discussions. Also, we thank Michael Springborg and Jorge Vargas for assistance with the MP2 calculations.
Expolring Architectures for CNN-Based Word Spotting<|sep|>In this work, we explored four CNN architectures on three standard word spotting benchmarks for Query-by-Example and Query-by-String. The experiments conducted show that deeper CNN architectures do not necessarely perform better on word spotting tasks. On the contrary, the recently proposed DenseNet [8] performs the worst on all three benchmarks excluding the PHOCLeNet. This leads us to the hyperthesis that nether increasing the depth nor the number of parameters in the convolutional part achieves signiﬁcantly higher performance in word spotting. Instead, future research needs to focus on word embeddings incorporating more informations than only character occurence or position.
Exact (1+1)-dimensional flows of a perfect fluid<|sep|>Let us summarize our results: i) We have derived a basis of solutions for the (1+1) -dimensional ﬂows of a perfect ﬂuid with arbitrary constant speed of sound. It spans an inﬁnite-dimensional linear vectorial space of solutions. ii) The basis elements can be indexed by a positive or negative integer number K ∈ Z, where the negative indices K < 0 correspond to singular solutions while K ∈ N correspond to regular ones. iii) The singular basis can be obtained by successive diﬀerentiation of the “harmonic ﬂow” solution, while the regular one arises from successive integration. iv) The general regular solution is characterized by a Khalatnikov potential which is an arbitrary combination of components of a linear basis
Welsch Based Multiview Disparity Estimation<|sep|>In this paper we experimentally show that occlusions present a signiﬁcant problem for multiview disparity estimation algorithms. These occlusions can cause some algorithms, such as those that use an L2 norm for the data-term to perform worse when more views are used. We propose a Welsch-L1 disparity estimation algorithm as a possible solution to this problem. We also deﬁne a disciplined warping strategy and method for progressively including views which can be used to avoid removing high frequency content from views. Even in cases with very high numbers of views we show that these approaches produce excellent results both on our synthetic dataset and a 4D Light Field Dataset [19] when compared with the L2-L2, L2-L1 and L1-L1 approaches.
Learning to generate one-sentence biographies from Wikidata<|sep|>We present a neural model for mapping between structured and unstructured data, focusing on creating Wikipedia biographic summary sentences from Wikidata slot-value pairs. We introduce a sequence-to-sequence autoencoding RNN which improves upon base models by jointly learning to generate text and reconstruct facts. Our analysis of the task suggests evaluation in this domain is challenging. In place of a single score, we analyse statistical measures, human preference judgements and manual annotation to help characterise the task and understand system performance. In the human preference evaluation, our best model outperforms template baselines and is preferred 40% of the time to the gold standard Wikipedia reference. Code and data is available at https:// github.com/andychisholm/mimo. This work was supported by a Google Faculty Research Award (Chisholm) and an Australian Research Council Discovery Early Career Researcher Award (DE120102900, Hachey). Many thanks to reviewers for insightful comments and suggestions, and to Glen Pink, Kellie Webster, Art Harol and Bo Han for feedback at various stages.
The Consistency of Fermi-LAT Observations of the Galactic Center with a Millisecond Pulsar Population in the Central Stellar Cluster<|sep|>The gamma-ray ﬂux spectrum and proﬁle observed by HG towards the Central stellar cluster of the Milky Way is consistent with other stellar cluster populations in observed globular clusters. The spectral index, exponential cutoﬀ and peak ﬂux energy is consistent with four of the eight detected globular clusters in the gamma-ray by Fermi-LAT. Therefore, there exists is no feature of the HG spectral source that necessarily requires a dark matter annihilation interpretation. Of the detected gamma-ray observed globular clusters, only 5 of 8 harbor detected MSPs [3]. The gamma-ray emission from stellar clusters such as the Central stellar cluster as well as globular clusters is expected to be dominated by MSP emission due to enhanced binary formation in these systems, where MSPs are spun up by accretion from their binary partners. Therefore, it is likely that the detection of the peaked spectrum towards the Galactic Central stellar cluster by Hooper & Goodenough is that of a population of millisecond pulsars bound within this massive stellar cluster. Obscuration, crowding, and absorption towards the central region of the Milky Way makes direct detection of the MSPs diﬃcult, though this identiﬁcation of the spectral signature of stellar cluster-like MSPs in the Galactic Center may motivate deeper searches. Acknowledgments — I would like to thank Prateek Agrawal, Steve Blanchet, Z. Chacko, J. Pat Harding, Manoj Kaplinghat, J¨urgen Kn¨odlseder, Julie McEnery, and Paul Ray for useful discussions. In particular, I would like to thank Elizabeth Ferrara for discussions and detailed comments on the manuscript, as well as her and Chris Shrader for organizing a GSFC Fermi-LAT Science Workshop on November 16, 2010 which led to this analysis. K.A. is supported by NSF Grant 07-57966 and NSF CAREER Award 09-55415.
A slow gravity compensated Atom Laser<|sep|>We have realized a slow guided atom laser beam outcoupled from a Bose-Einstein condensate of 87Rb atoms in a hybrid trap. The hybrid trap uses the combination of a magnetic quadrupole and an optical dipole potential to conﬁne the atoms. The acceleration of the atom laser beam can be controlled by compensating the gravitational acceleration with the magnetic quadrupole ﬁeld and allows us to reach residual accelerations as low as 0.0027 g. In this case the atom laser beam can be monitored for 500 ms (only limited by the ﬁeld of view of our imaging system), and even the highest velocity at the tip of the beam is only 13 mm/s. An analysis of the beam shows that the outcoupling mechanism allows for the production of a constant ﬂux of 4.5 × 106 atoms per second, while the depletion of the BEC is small. Since the magnetic quadrupole ﬁeld also acts as a transverse guide, this beam is well collimated and we obtain a mean beam width of 4.6 µm, far superior to an atom laser beam coupled out of an all optical potential. A time-of-ﬂight analysis shows that the beam has a transverse velocity spread of only 0.2 mm/s and thus an upper limit for the beam quality parameter M2 = 2.5 is obtained, comparable with the results for radio frequency outcoupling mechanisms. Finally, the beneﬁt of the long interrogation time available with this slow beam is demonstrated. A single image of the beam oscillating within the quadrupole potential can be used to determine the trap frequency, simplifying repeated measurements with individual BECs. The small beam width together with the long evolution and interrogation time makes this atom laser beam a promising tool for continuous interferometric measurements. Acknowledgements We acknowledge support from the Centre for Quantum Engineering and Space-Time Research QUEST and from the Deutsche Forschungsgemeinschaft (European Graduate College Quantum Interference and Applications)
Analytic solutions for marginal deformations in open superstring field theory<|sep|>We have constructed analytic solutions for marginal deformations in open superstring ﬁeld theory when operator products made of V1’s and V1/2’s are regular. Our solutions are very simple and remarkably similar to the solutions in the bosonic case [16, 17]. We expect that there will be further progress of analytic methods in open superstring ﬁeld theory. It would be interesting to study the rolling tachyon in open superstring ﬁeld theory, and we expect that marginal deformations associated with the rolling tachyon solutions satisfy the regularity conditions discussed in the preceding section. However, deformations we are interested in typically have singular operator products of the marginal operator. In the bosonic case, solutions to third order in λ have been constructed when the operator product of the marginal operator is singular [17]. We hope that a procedure similar to the one developed in the bosonic case will work in the superstring case, and it is important to carry out the program to all orders in the deformation parameter. Our choice of Φ(2) in (4.11) was based on a technical reason, and it is not clear if this gauge choice is physically suitable. In particular, the solution Φλ in (5.2) does not satisfy the reality condition on the string ﬁeld. However, it is diﬃcult for us to imagine that there are two inequivalent solutions generated by a single marginal operator which coincide to linear order in λ, and we expect that our solution is related to a real one by a gauge transformation. In fact, we can explicitly conﬁrm this at O(λ2). In order to see this, it is useful to write the solution in the original deﬁnition of the string ﬁeld by inverting the ﬁeld redeﬁnition (3.6):
Breakdown of light transport models in photonic scattering slabs with strong absorption and anisotropy<|sep|>We have studied the unphysical ranges of the P1, P3, and P3 + δE(4) approximations to the radiative transfer equation, for the complete (a,g,b,∆n2) parameter space. The unphysical parameter ranges are characterized by unphysical negative energy densities and ﬂuxes of the wrong sign. These ranges are crucial when the positiondependent energy density inside the photonic scattering slab is being investigated. Typically researchers want to extract the transport parameters ℓtr and ℓabs from total transmission and reﬂection experiments. We have calculated the relative errors in the transport parameters for all possible albedo and anisotropy and for realistically chosen optical thickness and refractive index contrast, by comparing the analytical results with Monte Carlo simulations. In the unphysical ranges the relative errors are as large as 104%, but also in the physical ranges the errors can be substantial. We emphasize that the relative error maps provided here are for slabs with widely studied realistic optical thickness and refractive index contrast. Maps for any other kind of samples with their speciﬁc parameters require characterization using the methodology explained here. We conclude that the P1 approximation is not viable to extract either the transport parameters or the position dependent energy density, unless the scattering of the sample is purely isotropic and elastic. The P3 and P3 + δE(4) approximations are safer to use than the P1 approximation, unless there is strong absorption (a < 0.3) or extreme anisotropy (g > 0.9 and g < −0.9). The approximations should not be used if the samples are in the unphysical parameter range, even though the relative errors are low in some parts of these unphysical ranges. Especially, the P3 + δE(4) approximation is suited for enhancing the accuracy in the forward direction (g > 0) and should not be used in the backscattering range (g < 0). Our results provide a guideline for the applicability of the P1, P3, and P3 + δE(4) approximations to the radiative transfer equation, to interpret experiments on light transport in photonic scattering slabs.
BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification<|sep|>In this paper, we propose a new technique for generating adversarial examples combining contextual perturbations based on BERT-MLM, synonym replacement of biomedical entities, typographical errors and numeric entity expansion. We explore several classiﬁcation models to demonstrate the efﬁcacy of our method. Experiments conducted on two benchmark biomedical datasets demonstrate the strength and effectiveness of our attack. As a future work, we would like to explore more about retraining the models with the perturbed samples in order to improve model robustness. The author would like to thank the annotators for hard work, and also the anonymous reviewers for their insightful comments and feedback.
Enhancing or suppressing spin Hall effect of light in layered nanostructures<|sep|>In conclusion, we have revealed a tunable SHE of light in layered nanostructures. From the viewpoint of classical electrodynamics, we have established a general propagation model to describe the spin-dependent transverse splitting of wave packet in the SHE of light. By modulating the structure parameters, the transverse displacements exhibit tunable values ranging from negative to positive, including zero, which means that the SHE of light can be greatly enhanced or suppressed, or completely eliminated. We have shown that the physical mechanism underlying this intriguing phenomenon is the optical Fabry-Perot resonance in the layered nanostructure. These ﬁndings provide a pathway for modulating the SHE of light, and thereby open the possibility for developing new nano-photonic devices. In this appendix we give a detailed calculation of the reﬂected and transmitted angular spectra. From the central frame xiyizi to the local frame XiYiZi, the following three steps should be carried out. First, we transform the electric ﬁeld from the reference frame xiyizi around the y axis by the incident angle θi to the frame xyz: ˜Exyz = mxiyizi→xyz ˜Exiyizi, where Then, we transform the electric ﬁeld from the reference frame xyz around the y axis by an angle kiy/(k0 sin θi) to the frame XY Z, and the correspondingly matrix is given by where k0 is the wave number in vacuum. Finally, we transform the electric ﬁeld from the reference frame XY Z around the y axis by an angle −θi to the frame XiYiZi, and the matrix can be written as Thus, the rotation matrix from the central frame xiyizi to the local frame XiYiZi can be written as Mxiyizi→XiYiZi = mXY Z→XiYiZimxyz→XY Zmxiyizi→xyz, and we have
Reflexive spatial behaviour does not guarantee evolution advantage in prey--predator communities<|sep|>An issue of the targeted (non-diﬀuse, etc.) migration is heavily dependent on very important idea that is information access and knowledge available to make a decision. Both these ideas are hard to deﬁne and hard to study. Deﬁnitely, very few animals may exhibit a behaviour based on “knowledge” that is similar, to some extent, to that one peculiar for human beings. Nonetheless, a tremendous number of examples make an evidence of a non-random and reasonable behaviour of the beings of extremely diﬀerent taxa. Thus, observations over animals contribute a lot the methodology of ﬁnite automata [52, 55, 56]. This approach forces to ﬁgure out what is the information necessary to act (in some proper way), exactly. In general, such information could be separated into two (generalized) issues: the former is related to an outer, external situation, and the latter describes the status of a being (say, hunger, experience, pain, etc.; see, e. g., [47, 48, 50]). An external information concerns, ﬁrst of all, the environmental conditions of a site: local population density is number one among them [42–44, 47, 48, 53]. Unlike a general approach in modelling, where an average population density is used to model (or simulate) the dynamics of a community, the reality is based on the local density ﬁgures. Yet, it is not the end of a problem. The information used by an individual to evaluate a situation and make a decision to realize some speciﬁc behavioural act significantly diﬀers in the value of a speciﬁc radius of access of that former. Here the problem of a scale arises; there are two natural ways to determine that latter: – a body scale distance; and – an individual speciﬁc transfer distance. So, an eﬀective local population density should be determined through a comparison of the traces of a number of individuals. The ways to identify those traces are numerous and extremely diverse. Basically, they could be arranged into three groups. The most widespread and eﬀective way is a chemical communication (smell); some media do not allow such way of communication, and rather advanced techniques have been elabourated [53]. The species occupying extended territories use visual labelling. In other words, some information may come in diverse ways, and from diﬀerent distances. Probably, different kinds of the information have diﬀerent scales. Apparently, a communication related to reproduction behaviour has the largest scale. Reciprocally, a trophic behaviour patterns are at the opposite pole of the scale. Finally, there are socially determined eﬀects in the problem of information accessibility and scaling: reﬂexivity caused by a competitive (with neither respect to a nature of this competition) origin of the behavioural patterns (see, e. g., [54] Simulation shows that reﬂexivity of either species impacts both the average abundances, and the dynamics
Variant-based Equational Unification under Constructor Symbols<|sep|>The variant-based equational uniﬁcation algorithm implemented in the most recent version of Maude, version 3.0, may compute many more uniﬁers than the necessary or may not be able to stop immediately. Constructor symbols are extensively used in computer science, but they have not been integrated into the variant-based equational uniﬁcation procedure of Maude. In this paper, we have redeﬁned the
Contrast stability and "stripe" formation in Scanning Tunnelling Microscopy imaging of highly oriented pyrolytic graphite: The role of STM-tip orientations<|sep|>In this work we studied the STM image contrast of the HOPG(0001) surface in the tunnelling regime as a function of the local orientation of a set of tungsten tips. Employing a three-dimensional (3D) Wentzel-Kramers-Brillouin (WKB) tunnelling approach, we demonstrated that the relative local orientation of the STM-tip apex with respect to the HOPG substrate can have a considerable eﬀect on the HOPG STM contrast. Depending on the STM tip-apex structure and composition, applied bias, and relative orientation with respect to the substrate, substantially diﬀerent eﬀects, ranging from conservation to inversion of the STM contrast, were observed. These results were rationalised in terms of the tip-rotation mediated contribution of tip-apex electronic states of diﬀerent orbital characters to the tunnelling current. For a sharp tungsten tip the HOPG contrast inversion between opposite bias polarities was explained by the diﬀerent weights of the tip orbital characters involved in the tunnelling that is due to the asymmetry of the tip electronic structure with respect to its Fermi level. We also compared the 3D-WKB and Bardeen STM simulation models with each other and with experiments in terms of bias-voltage-dependent STM topography brightness correlations. We found quantitatively good agreement for particular tip models and bias voltage ranges, and discussed the identiﬁed diﬀerences in view of the construction of the two tunnelling models. In view of the experiments, we can also conclude that the two tunnelling methods perform at the same quantitative reliability. Importantly for experimental STM analysis of HOPG, the simulations indicate that particular local tipreconstructions with no orientational change of the dominating d3z2−r2 tip-apex orbital state aﬀect only the secondary features of the HOPG STM contrast, leaving the primary contrast unchanged, thus resulting in a stable tip. Such tip orientations are found to be responsible for ”striped” images observed in experiments. Conversely, tip-rotations leading to enhanced contributions from m ̸= 0 tip-apex electronic states can cause a triangular-hexagonal change in the primary contrast, indicating a likely tip instability. The authors thank E. Inami, J. Kanasaki, and K. Tanimura at Osaka University for the experimental brightness data, and A. L. Shluger for useful comments on the manuscript. Financial support of the Magyary Foundation, EEA and Norway Grants, the Hungarian Scientiﬁc Research Fund project OTKA PD83353, the Bolyai Research Grant of the Hungarian Academy of Sciences, and the New Sz´echenyi Plan of Hungary (Project ID: T´AMOP-4.2.2.B-10/1–2010-0009) is gratefully acknowledged. G. T. is supported by EPSRC-UK (EP/I004483/1). Usage of the computing facilities of the Wigner Research Centre for Physics, and the BME HPC Cluster is kindly acknowledged. M´andi et al. developed an orbital-dependent electron tunnelling model with arbitrary tip orientations [11] for simulating scanning tunnelling microscopy (STM) measurements within the three-dimensional (3D) Wentzel-Kramers-Brillouin (WKB) framework based on previous atom-superposition theories [3, 9, 36, 37, 48, 49, 50]. Here, we brieﬂy describe this method used in the paper for the highly oriented pyrolytic graphite, HOPG(0001) surface in combination with tungsten tips. The model assumes that electrons tunnel through one tip apex atom, and individual transitions between the tip apex and a suitable number of sample surface atoms, each described by the onedimensional (1D) WKB approximation, are superimposed [9, 35]. Since the 3D geometry of the tunnel junction is considered, the method is a 3D-WKB atom-superposition approach. The advantages, particularly computational eﬃciency, limitations, and the potential of the 3D-WKB method were discussed in Ref. [38]. The electronic structure of the surface and the tip is included in the model by taking the atom-projected electron density of states (PDOS) obtained by ab initio electronic structure calculations [36]. The orbital-decomposition of the PDOS is necessary for the description of the orbital-dependent electron tunnelling [9]. We denote the energydependent orbital-decomposed PDOS function of the ith sample surface atom with orbital symmetry σ and the tip apex atom with orbital symmetry τ by ni Sσ(E) and nTτ(E), respectively. In the present work we consider σ ∈ {s, py, pz, px} atomic orbitals for the carbon atoms on the HOPG surface, τ ∈ {s, py, pz, px, dxy, dyz, d3z2−r2, dxz, dx2−y2} orbitals for a blunt and sharp tungsten tip apex atom, and τ ∈ {s, py, pz, px} orbitals for a carbon apex atom on a sharp tungsten tip. The total PDOS function is the sum of the orbital-decomposed contributions: Note that a similar decomposition of the Green’s functions was reported within the linear combination of atomic orbitals (LCAO) framework in Ref. [8]. Assuming elastic electron tunnelling at temperature T = 0 K, the tunnelling current at the tip position RTIP and bias voltage V is given by the superposition of atomic contributions from the sample surface (sum over i) and the superposition of transitions from all atomic orbital combinations between the sample and the tip (sum over σ and τ): One particular current contribution can be calculated as an integral in an energy window corresponding to the bias voltage V as
Determining topological order from a local ground state correlation function<|sep|>To conclude, in this work we proved that the Z and Z2 topological indices can be extracted from the groundstate equal-time two-point correlation function at zero ﬂux, given on any section larger than some correlation length. This implies that the order in such topological phases can be thought of as a local ground-state property. In the case of 3D TI it was suggested that the strong index is indeed local in the above sense, while the weak indices carries some global information. Heuristically, one can reach a similar conclusion using entanglement spectrum [22]. Indeed, Pij on a ﬁnite region is related to the reduced density matrix. A gapless spectrum of Pij is therefore an indication of nontrivial topology [23, 24]. Nevertheless, given a ﬁnite region one cannot diﬀerentiate a gapped spectrum from a gapless one without inserting ﬂuxes. It will be interesting to establish this approach in interacting systems and to apply it on fractional quantum Hall states. Preliminary numerical results indicate that the electron two-point correlation function of the ground state at ﬁlling factor 1/3 is proportional to the one of ﬁlling factor 1. Hence, it may be possible to extract the topological order of fractional states in a similar way. We hope that our viewpoint will encourage the eﬀorts for ﬁnding manifestations of the Z2 index through local bulk properties. We thank Ady Stern and Ehud Altman for useful discussions. Z.R. thanks A. Soroker, and Y.E.K. thanks M. Kraus. Z.R. Acknowledges ISF Grant No. 700822030182, and Y.E.K. thanks the U.S.-Israel Binational Science Foundation and the Minerva foundation for ﬁnancial support. In the main text we avoided edge eﬀects by considering geometries with periodic dimensions and an inﬁnite open coordinate. In practice, a ﬁnite open coordinate must be used, causing numerical artifacts that should be ﬁltered out in order to reveal the bulk behavior. This can be done in a controlled manner provided that the 1D Wannier functions of the bulk and the edge are distinguishable. The ﬁrst part of this Appendix establishes this distinction, and the second part presents the actual numerical procedure. The starting point of the proofs in Sec. IV was expanding the spectral projector P as a series in powers of the Hamiltonian H, with the spectral gap ∆ as a control parameter. An edge may give rise to gapless edge states and so our ﬁrst task is to establish the expansion in the presence of such states. We assume that H is characterized as before, only now the spectrum of H includes both bulk states with energy greater than the gap, and edge states with subgap energy, which are exponentially localized along the edge. The projector P can than be divided into bulk and edge parts where |n⟩ denotes an eigenstate with eigenvalue En, and µ is the chemical potential, as before. Since P edge is composed only of the edge states, it decays exponentially into the bulk. Therefore [P edge]ij is exponentially small, if either i or j (or both) reside within the bulk. In a similar manner to what we have done above, we introduce an error function approximation to the projector
Novel Collider and Dark Matter Phenomenology of a Top-philic Z'<|sep|>Extending the Standard Model by an additional U(1)′ gauge symmetry provides an economical way to address potential signals that are diﬃcult to reconcile in the context of the SM alone. We have considered the case in which the right-handed top quark is the only SM state charged under the exotic U(1). The corresponding Z′ gauge boson is then top-philic. The U(1)′ is clearly anomalous and requires the presence of spectator fermions at the scale ΛUV . Rather than committing to a particular UV completion, we have taken a more general approach and considered the low energy eﬀective description where the only new states are the Z′ and a Dirac fermion charged under the U(1)′ which is our dark matter candidate. Previous studies considering a top-philic Z′ have largely focused on masses above the t¯t threshold. We have considered lighter Z′ masses in the range 150-450 GeV, which leads to a richer phenomenology. The Z′ can undergo kinetic mixing with the Standard Model hypercharge gauge boson. However, the mixing is constrained to be small by electroweak precision measurements. The dominant production mechanisms for the Z′ are then the tree-level top quark associated production (t¯tZ′) and the loop-induced Z′+ jets process. The Z′ phenomenology can be divided into three distinct regions based on the dominant decays of the Z′. In the low-mass region (150 ≲ mZ′ ≲ 220 GeV) the Z′ decays dominantly into b¯b. This region can be probed by searches for ttZ′ associated production where the Z′ subsequently decays to b¯b. Other ﬁnal states, such as Zγ, can also provide constraints despite their suppressed branching ratios. Decays to Zh dominate in the intermediate mass range (220 ≲ mZ′ ≲ 300 GeV). This region is already strongly constrained by existing resonance searches. In the high mass region (mZ′ ≳ 300 GeV) the Z′ decays dominantly into t¯t(∗) and the most stringent constraints arise from searches for four top quark production. We ﬁnd that with 300 fb−1 at √s = 13 TeV the LHC will be able to exclude Z′ masses in the range 150-450 GeV for couplings gt ≳ 0.2. However, the collider bounds can be signiﬁcantly weakened below the t¯t threshold if the Z′ is allowed to decay invisibly, for example to the dark matter candidate. The combined ATLAS and CMS Higgs measurements currently show a 2.3σ excess in t¯th production, driven by the same-sign dilepton channel. We ﬁnd that including a contribution from ttZ′ associated production yields an improved ﬁt to the data and could provide an explanation of the excess for m′ Z ≳ 300 GeV and gt ≳ 0.8. Interestingly the lepton+jets search for four top quark production, which is sensitive to the relevant region of parameter space, has also observed a mild excess. The inclusion of a SM singlet Dirac fermion, charged only under the U(1)′, also provides a viable dark matter candidate. It is stabilised by a residual Z2 symmetry after the U(1)′ is spontaneously broken. The dark matter interacts with the SM via the Z′ portal and depending on the dark matter mass several channels can dominate the annihilation. We show that production via standard thermal freeze out can yield the correct relic density for a range of dark matter and Z′ masses. The Fermi-LAT collaboration has observed an excess of γ-rays emanating from the galactic center. While there are astrophysical explanations for the source of the excess photons, dark matter annihilation also provides a good ﬁt to the observed spectrum. We ﬁnd that our dark matter candidate can provide an explanation for the excess via annihilation into b¯b or t¯t while simultaneously satisfying the relic density constraint. We anticipate that annihilation into Zh may also be able to explain the excess, however currently no ﬁt has been performed studying this ﬁnal state. Finally, we ﬁnd regions in parameter space which can simultaneously provide an explanation for the galactic center excess, give an improved ﬁt to the tth data, and satisfy all current collider constraints. This region of parameter space will be probed during the 13 TeV run of the LHC. We thank Marco Taoso, Carlos Wagner for helpful discussions. TSR acknowledges the ISIRD Grant, IIT Kharagpur, India. This work was supported by IBS under the project code, IBSR018-D1. This work has been supported in part by the European Research Council (ERC) Advanced Grant Higgs@LHC. This work was supported by the Australian Research Council. For reference, we list the expressions for the Z′ decay widths here. Loop integrals are written in terms of Passarino-Veltman functions [71]. We note that the width to γγ is identically The only SM state the Z′ directly couples to is the right-handed top. Below threshold, this translates to decays to tWb and WWbb. We can express these decays through two functions, the standard triangle function I deﬁned in Eq. (15) and a related function F: q0 = (mW + mb)2 , q1 = (mZ′ − mW − mb)2 , q2(x) = (mZ′ − √x)2 . (22) The integration variables Q2 i are the momenta of the intermediate top quarks. When one or both top quarks are on-shell, we can use the narrow width approximation
Quantitative probing: Validating causal models using quantitative domain knowledge<|sep|>In summary, we have introduced the method of quantitative probing for validating causal models. We identiﬁed the additional difﬁculties in validating causal models when compared to traditional correlation-based machine learning models by reviewing the role of the i.i.d. assumption. In analogy to the model-agnostic train/test split, we proposed a validation strategy that allows us to treat the underlying causal model as a black box that answers causal queries. The strategy was put into context as an extension of the already existing technique of using refutation checks, which is in line with the established logic of scientiﬁc discovery. After illustrating the motivation behind the concept of quantitative probing using Pearl’s sprinkler example, we presented and discussed the results of a thorough simulation study. While being mostly supportive of our hypothesis that quantitative probes can be used as an indicator for model ﬁtness, the study also revealed shortcomings of the method, which were further analyzed using exemplary failing runs. To conclude the article, we revisit the assumptions from Chapter 5.2 and the ideas from Chapter 6.4, in order to identify topics for future research. • Is there a way to explain the roughly linear association between the hit rate and the edge/effect differences in Figure 3? Answering this question would provide a solid theoretical backing for the quantitative probing method, in addition to the simulation-based evidence presented in this article. • Can we quantify the "usefulness" of each employed quantitative probe for detecting wrong models? An example would be to plot the results for a single ﬁxed hit rate, but each data point could represent runs that used only speciﬁc probes, e.g. only those whose variables lie within a maximum distance from the target variables. What other criteria could make a speciﬁc probe useful? Answering this question would aid practitioners in eliciting speciﬁc quantitative knowledge from domain experts. • Given the probe coverage issue in Figure 7, is there a way to model how the effectiveness of the method depends on the number of used quantitative probes? Answering this question would aid practitioners in gauging the amount of required quantitative domain knowledge for model validation. • Given the probe tolerance issues in Figure 8, how much more useful do the probes become for detecting wrong models if we narrow the allowed bounds in the validation effects? How can we determine ideal bounds to avoid overreject and underreject? Answering this question would provide important guidance to both researchers and practitioners, as the bounds must be actively chosen by the investigator in each application of quantitative probing. Similarly to how qualitative knowledge by domain experts can assist causal discovery procedures in recovering the correct causal graph from observational data, we believe that quantitative probing can serve as a natural method of incorporating quantitative domain knowledge into causal analyses. By answering the above questions, the proposed validation strategy can evolve into a tool that builds trust in causal models, thereby facilitating the adoption of causal inference techniques in various application domains.
Learning to Selectively Transfer: Reinforced Transfer Learning for Deep Text Matching<|sep|>In this paper, we proposed a reinforced data selection method in a DNN based transfer learning setting. Specifically, we used reinforcement learning to train a data selection policy to select high-quality source domain data with the purpose of preventing negative transfer. We investigated different settings of states, rewards, and policy optimization methods to test the robustness of our model. Extensive experiments on PI and NLI tasks indicate that our model can outperform existing methods with statistically significant improvements. Finally, we used Wasserstein distance to measure the source and target domain distances before and after the data selection. This study indicates that our method is capable of selecting source domain data that has a similar probability distribution to the target domain data. Future work will consider to explore more effective state representations and adapt our method to other tasks. This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
Dynamics of a Reaction-Diffusion Benthic-Drift Model with Strong Allee Effect Growth<|sep|>For a aquatic species that reproduce on the bottom of the river and release their larval stages into the water column, the longitudinal movement occurs only in the drift zone and individuals in the benthic zone in stream channel stays immobile. Through a benthic-drift model, we investigated the population persistence and extinction regarding the strength of interacting between zones. Moreover, this benthic-drift model has the feature of a coupled partial diﬀerential equation (PDE) for the drift population and an “ordinary diﬀerential equation” (ODE) for the benthic population. This degenerate model causes a lack of the compactness of the solution orbits, which brings extra obstacles in the analysis. To overcome these diﬃculties, we turn to the Kuratowski measure of noncompactness in order to use the Lyapunov function. For single compartment reaction-diﬀusion-advection equation, when the growth rate exhibits logistic type, it is well-known that the dynamics is either the population extinction or convergence to a positive steady state (monostable). If the species follows the strong Allee eﬀect growth, when both the diﬀusion coeﬃcient and the advection rate are small, there exist multiple positive steady state solutions hence the dynamics is bistable so that diﬀerent initial conditions lead to diﬀerent asymptotic behavior. On the other hand, when the advection rate is large, the population becomes extinct regardless of initial condition under most boundary conditions [45]. Unlike the single compartment reaction-diﬀusion-advection equation with a strong Allee eﬀect growth rate, in which the advection rate q plays a important role in the persistence/extinction dynamics, the benthic-drift model dynamics with strong Allee eﬀect relies more critically on the strength of interacting between zones, especially the transfer rate from the benthic zone to the drift zone µ. In this paper, we show that how the transfer rates between benthic zone to the drift zone inﬂuence the population dynamics. We divided the µ (transfer rate from benthic zone to drift zone) and σ (transfer rate from drift zone to benthic zone) phase plane into regions and studied the dynamical behavior on these parameter regions. When we have a relatively large µ (in H1), population extinction will happen independent of the initial conditions, the boundary condition, the diﬀusive and advective movement and the transfer rate from the drift zone to the benthic zone σ. For small µ (in (H3)), for large initial conditions, population persistence will happen regardless of the boundary condition, the diﬀusive and advective movement and the transfer rate from the drift zone to the benthic zone σ. Along with the locally stability of the zero steady state solution, bistable dynamical behavior can be conﬁrmed. When the transfer rate µ is in the intermediate range (in (H2)), the persistence or extinction depends on the diﬀusive and advective movement. And under closed environment, a multiplicity result for the steady state solutions is also obtained for small advection rate.
Stream-orbit misalignment II: A new algorithm to constrain the Galactic potential<|sep|>Tidal streams are very attractive structures for probing the Galactic potential on large scales, but it is essential that appropriate algorithms are developed for their study. In Paper I we showed that orbit-ﬁtting is inappropriate for many streams in the Milky Way, and can lead to order one errors in estimation of parameters of the Galactic potential. Motivated by the need for an improvement over orbit-ﬁtting, we have presented an algorithm for using tidal-stream data to constrain the potential of the Galaxy without assuming that the stream delineates an orbit. Instead it identiﬁes the true potential as the one that yields corresponding patterns in angle and frequency space. The algorithm was tested by evolving an N-body simulation of a King cluster in a two-parameter logarithmic potential until a stream is formed. The degree of correlation between the angle and frequency structure was maximised with respect to trial potentials. The correct parameters were recovered within the estimated systematic errors of the method. As tidal streams are very distant structures we expect large errors in the observables. Therefore, it is imperative that any streamﬁtting method is shown to function for large observational errors. We have shown that the observational errors in the distances and proper motions for individual stars in tidal streams are currently too large to use the above technique with any conﬁdence. However, if the data are ﬁrst binned and averaged in observable space on the sky, we can recover the correct potential parameters even for large observational errors. The current observational errors may be small enough for close streams such as GD-1 and it seems promising that in the era of Gaia the data for more streams will be sufﬁciently accurate to use this algorithm. We have shown that longer streams produce superior potential parameter estimates, so there is hope for using higher-mass streams to produce better constraints on the potential. We have seen that observing a stream at different times results in different constraints on the potential. Therefore there is much to be gained by using several streams simultaneously to constrain the potential. Hopefully this would also remove local minima and make a global minimum more apparent. Similarly the approach taken here uses a constant prior for the potential parameters. In reality, a more informative prior could be used, that would rule out regions of the parameter space which are populated by local minima. We have only investigated how well a very simple twoparameter potential may be constrained. In reality, models of the Galactic potential have many more parameters. In this case we expect many more degeneracies in parameter space and a much fuller Monte Carlo search of this higher dimensional space would be required. From the test results here we have seen that we can constrain the circular speed of the potential more effectively than the shape. It would be interesting to explore with a more complex potential which parameters/features of the potential we are able to constrain more accurately than others. For instance, when applying an orbit-ﬁtting algorithm to the GD-1 stream Koposov et al. (2010) found that the mass of the disc in their Galaxy model could be more accurately constrained than the shape of the halo. Finally, this algorithm, unlike an orbit-ﬁtting algorithm, requires full 6D data for the stream. The Galaxy is being increasingly mapped in 6D and we have 6D data for the GD-1 stream (Koposov et al. 2010). However it may be some time before we have accurate 6D data for many distant streams. Therefore, to make the presented algorithm more usable in the near future it would be interesting to investigate what alterations we can make to deal with reduced phase-space information. We intend to adapt the above approach to produce a Bayesian model of the stream structure in angle-action space, which will be useful in this respect. JS acknowledges the support of the Science and Technology Facilities Council and we thank Paul McMillan for useful discussions. We also thank the anonymous referee for their useful suggestions which have improved the paper.
First-order Optimization for Superquantile-based Supervised Learning<|sep|>Risk-sensitive optimization plays a major role in the design of safer models for decision-making and has recently gained interest in machine learning. We provide a toolbox to tackle superquantile-based learning problems using ﬁrst-order optimization algorithms. Numerical illustrations on regression tasks show an improved statistical behavior in terms of higher quantiles of the testing loss.
Quiescent X-Ray/Optical Counterparts of the Black Hole Transient H 1705-250<|sep|>We have observed the black hole transient H 1705–250 during its quiescent state with Chandra. The 50 ks long exposure reveals 5 photons at the source position (within 1.5′′ radius), yielding a source luminosity of LX ∼ 9.1×1030 erg s−1 (in 0.5-10 keV) when assuming a distance of 8.6 kpc. This improves the quiescent sensitivity of the source by a factor of ∼ 1000 compared to the previous reported ROSAT value. In the context of the ADAF models, the quiescent luminosities depend on the orbital period of the system. The very low luminosity of H1705-250 together with its orbital period (∼ 0.5 days) thus consistent with this framework. We have also examined the optical monitoring data of H 1705–250 obtained by the Faulkes Telescope North, reﬁned the best position of the source and obtained a long-term lightcurve. It has been observed that the quiescent X-ray luminosity of black hole binaries are usually very dim (e.g. Garcia et al. 2001; Kong et al. 2002). They are ∼100 times fainter than neutron stars with similar orbital periods. It is unclear why the X-ray luminosity is much dimmer for black hole systems compared with the neutron star systems apart from the reason of a diﬀerence between a solid surface and an event horizon. The emission may originate from a truncated accretion disc that is detached from the central advective ﬂow, or perhaps from the jet or outﬂow launched during quiescence or perhaps it is a combination for both disc and jet components. Several theoretical studies have attempted to explain the origin of the weak X-ray emission in the quiescent state of black hole binaries, yet none of the models can fully satisfy the observed properties. The ADAF is by far the most used scenario (as described in the introduction section). However, it has recently been realized that the jet/outﬂow can also play an important role in carrying away some fraction of the accretion energy from black holes in quiescence (Gallo et al. 2006), possibly resulting in weaker emissions comparing to emissions from neutron star systems. The true picture might fall in between the two scenarios (ADAF + outﬂows). Due to this, some theoretical models have evolved into more complicated forms to incorporate both disk and jet (or wind) properties (Blandford and Begelman 1999, Yuan et al. 2005). It is still a work in progress since most of the models are only able to explain some sources but not all. In addition, how do we know what we observed is the true quiescent state of the black hole? Due to current sensitivities of X-ray instruments, we are not able to observe most of the very faint black hole systems in detail and resolve their spectral properties in quiescence. However, several quiescent optical studies have revealed evidence for strong optical activities in quiescent lightcurves of several black hole systems (Zurita et al. 2003, Casares et al. 2009, Shahbaz et al. 2010). Most of these systems show short-term variability or ﬂares superimposed on the weak ellipsoidal modulation of companion star, and sometimes show long-term aperiodic variability or magnitude color changes implying optical state changes (Cantrell et al. 2008). From our 6 years long-term optical lightcurve, we observed two dips and an increasing brightness of the source since the end of 2011, indicating that the source is still active even at this very low rate of accretion. The origin of this variability is not yet fully understood, but it is probably associated with the accretion disc. There are several possible explanations: it could be due to the X-ray reprocessing in the accretion disc (Kong et al. 2001, Hynes et al. 2002, 2004), magnetic reconnection events (Zurita et al. 2003), or the emission from the ADAF (Shahbaz et al. 2003, 2010). Unfortunately, our optical data are not suﬃciently sensitive for such detailed analysis. Future observations are needed for further investigation. In addition, the next generation X-ray observatory with improved sensitivity will certainly bring us new insights and shed light on understanding accretion physics of binary systems in very low accretion regimes. YJY and RW acknowledge support from the European Communitys Seventh Framework Programme (FP7/20072013) under grant agreement number ITN 215212 Black Hole Universe. AKHK acknowledges support from the National Science Council of the Republic of China (Taiwan) through grants NSC100-2628-M-007-002-MY3 and NSC1002923-M-007-001-MY3, and the Kenda Foundation Golden Jade Fellowship. DMR acknowledges support from the Marie Curie Intra European Fellowship within the 7th European Community Framework Programme under contract no. IEF 274805. FL acknowledges support from the Dill Faulkes Educational Trust. RW acknowledges support from a European Research Council (ERC) starting grant. We thank Phil Charles and Tom Maccarone for helpful discussions, and Michiel van der Klis for reading the earlier version of this manuscript. The Faulkes Telescopes are maintained and operated by Las Cumbres Observatory Global Telescope Network.
DeepFaceEditing: Deep Face Generation and Editing with Disentangled Geometry and Appearance Control<|sep|>This work presented a structured disentanglement framework for face generation and editing, carried out in a local-to-global manner. Our key observation is that geometry and appearance features of face images can be effectively disentangled, and sketches serve as an ideal intermediate representation for geometry features. Therefore, Fig. 15. Face synthesis results with hand-drawn sketches. With the control of appearance, our method can generate diverse photo-realistic faces from hand-drawn sketches. Original images courtesy of David Shankbone,Greg Mooney and danieljordahl. sketches can impose a strong constraint during disentanglement. The component-level geometry and appearance disentanglement is achieved by Local Disentanglement modules which are trained using a swapping strategy, while the Global Fusion module performs coherent local-to-global image generation from feature maps of facial image patches. Through extensive experiments, we prove that our approach can generate much more realistic results than existing methods. We also adapted our system for novel applications such as sketch editing, hand-drawn sketch based face image generation and disentangled face morphing. One limitation of this work is that we only disentangle the geometry and appearance while other attributes such as head pose have not been considered in our current implementation. As shown in Figs. 17 and 18, when there is a large rotation of the subject’s head or there is substantial occlusion, there may be some color bias in the generated results. Lighting is another challenging problem for most existing face synthesis methods and is not explicitly disentangled in our framework, so it is difficult to finely control complicated lighting conditions by sketches or reference images. As future work, it would be useful to explore disentanglement of other attributes such as head pose and lighting to make the method more general. Besides, sketches have semantic ambiguity and in some extreme cases, it is hard even for humans to distinguish the accurate boundary between neck, hair and background. This ambiguity may sometimes cause some artifacts on the outer boundary of the face foreground, leading to blurred hair. As future work, semantic masks may be combined with sketches to generate more attractive results. Furthermore, while we allow detailed editing of geometry through Fig. 16. The results of disentangled morphing by interpolating the topleft and bottom-right faces (in red boxes). The remaining face images are generated automatically with our method. In each row, the appearance of the images is interpolated while keeping the geometry unchanged, while in each column, the geometry is interpolated while retaining the appearance. Original images courtesy of Lydia Liu and SISTERKLAAS Lore. Fig. 17. A less successful example. When an appearance image contains a large occlusion, there may be some color bias in the generated result. Original images courtesy of UC Davis College of Engineering and taymtaym. sketching, our current approach uses an appearance reference image to control the appearance of the generated face image. This could be improved using other forms of input, such as strokes with color, which would be more flexible, such as [Sangkloy et al. 2017]. This is a promising research direction in the future but it remains challenging, because the face appearance not only contains the color but also the material attributes, and it is very difficult to stroke the complex materials of the face. The consistency between the color strokes should also be maintained. These will be explored in the future research work. This work was supported by National Natural Science Foundation of China (No. 61872440 and No. 62061136007), Science and Technology Service Network Initiative of the Chinese Academy of Fig. 18. The results for non-frontal faces. Images in the first column provide appearance and the rest images are generated by our method. The inputs of sketches can be seen in the supplementary material. As the rotation angle increases, the quality of the results decreases. Original images courtesy of Senterpartiet (Sp) and Phillip Nguyen. Sciences (No. KFJ-STS-ZDTP-070, No. KFJ-STS-QYZD-129 and No. KFJ-STS-QYZD-2021-11-001), Royal Society Newton Advanced Fellowship (No. NAF\R2\192151), Youth Innovation Promotion Association CAS, and Beijing Program for International S&T Cooperation Project (No. Z191100001619003). Hongbo Fu was supported by HKSAR RGC General Research Fund (No. 11212119) and City University of Hong Kong (SCM ACIM Collaborative Research Fellowship).
Quantum Communication and Quantum Multivariate Polynomial Interpolation<|sep|>We have seen that for any qudit message, one can use the quantum multivariate interpolation to make secrecy of sending the information following the general scheme of Quantum Secret Sharing. In case of multivariate polynomial interpolation, we have some probability of correctly decoding the message, rather than in the one variable case with certainty.
Initiation of the detonation in the gravitationally confined detonation model of Type Ia supernovae<|sep|>The high-resolution simulations reported in this paper suggest that the detonations in lower resolution simulations (i.e., the 4 km case in this paper and the lower resolution simulations in earlier studies) are largely numerical. Two high-resolution simulations (500 m and 250 m) demonstrate that pre-conditioning of a fuel pocket surrounded by Kelvin-Helmholtz billows by a compression wave moving through the pocket can lead to a gradient initiated detonation. For the resolutions explored in this paper, the spatial extent of the temperature gradient decreases as the resolution becomes ﬁner, causing the temperature gradient to steepen with increasing resolution. Consequently, the temperature gradient becomes too steep (and therefore the induction time becomes too short) in the highest resolution simulation (125 m) for a detonation to occur at the time and location at which it occurs in the 500 m and 250 m simulations, and did not occur by the time we had to stop the simulation because of its computational demand. However, it may detonate later. In addition, shearinduced turbulence, which surrounds the jet and is largely suppressed at low resolution, may broaden the temperature gradient, leading to conditions favorable for the initiation of a detonation, especially if the inﬂuence of compression waves on the turbulent layer is considered. This suggests that future studies should be done (ideally in 3D) at even higher resolution, beginning earlier and running for longer times, in order to capture the development of the turbulence surrounding the jet more self consistently. We would like to thank Dean Townsley and George C. Jordan IV for their helpful suggestions and discussions. This work is supported in part by the U.S. Department of Energy under contract B523820 to the ASC Alliances Center for Astrophysical Flashes and in part by the National Science Foundation under grant PHY 02-16783 for the Frontier Center ”Joint Institute for Nuclear Astrophysics” (JINA) and in part by the Emmy Noether Program of the German Research Foundation (DFG; RO 3676/1-1). JWT acknowledges support from Argonne National Laboratory, operated under contract No. W-31-109-ENG38 with the DOE.
A Model of the IEEE 802.11 DCF in Presence of Non Ideal Transmission Channel and Capture Effects<|sep|>In this paper, we have provided an extension of the Markov model characterizing the DCF behavior at the MAC layer of the IEEE802.11 series of standards by accounting for channel
Distribution of spectral linear statistics on random matrices beyond the large deviation function -- Wigner time delay in multichannel disordered wires<|sep|>i f(λi) of eigenvalues of random matrices from invariant ensembles. Applying the Coulomb gas technique, we have provided a compact form for the distribution, Eq. (47), in terms of simple properties of the Coulomb gas : the Lagrange multiplier µ∗ 1(s), “conjugated variable” to the rescaled linear statistics s = N −ηL, and the entropy of the optimal charge distribution (η was deﬁned in the introduction). Our conjecture (47) has been successfully tested in several cases : • Trace of matrices of the Laguerre ensemble.— This is a case where the form deduced from the conjecture can be compared to an exact result (§ 3.1). • Conductance of chaotic cavities.— The distribution of the conductance of two terminal chaotic quantum dots has been well studied in the literature. This corresponds to analyse the trace of matrices from the shifted Jacobi ensemble (§ 3.2). • Wigner time delay in chaotic cavities.— Another test of the conjecture (47) is to consider the distribution of the Wigner-time delay for quantum dots studied in Ref. [56]. This is again related to the Laguerre ensemble, but with exponent θ = 1. The large deviations for s → 0 are described by [56] The exponent ζ, describing subleading contributions (not studied in Ref. [56]), is expected to depend linearly on N, and can thus be determined by inspection of the known distributions for N = 1 [29] and N = 2 [49] (see also [54]) : This result can also be easily recovered from Eq. (47) as follows. In the limit s → 0, the density ρ∗ is similar to the one analysed in Subsection 4.4 (as it is controlled by the linear part of the potential (11), independent of θ). This leads to the dependence of the entropy S[ρ∗] ≃ −(1/2) ln s (cf. § 4.6.1), which explains the O(N) term of the exponent ζ. The O(N 0) term is simply related to the behaviour µ∗ 1(s) ≃ 1/s2 given in Ref. [56]. For the large deviation for s → ∞ (after the freezing transition) the application of the conjecture (47) is more subtle : it must be slightly adapted to account for the integration over the isolated charge, allowing to recover the subleading contribution to the exponent of the power law PN(s) ∼ s−2−βN/2. • Wigner time delay in disordered wires.— Finally, in Section 4, we have applied the conjecture to a case where the pre-exponential function of the distribution (47) plays a crucial role due to the divergence of the moments of the linear statistics : this occurs when studying the distribution of the Wigner time delay for multichannel weakly disordered semi-inﬁnite wires. We have obtained the Wigner time delay distribution for diﬀerent symmetry classes in this case and checked our result with numerical simulations. The agreement is excellent. Laguerre (∀ θ) trace x 2 § 3.1 Laguerre (θ = 1) Wigner time delay (cavity) 1/x −1 Ref. [56] Laguerre (θ = 0) Wigner time delay (wire) 1/x 0 § 4 Jacobi conductance x 1 Ref. [59], § 3.2 The importance of the pre-exponential function in (47), i.e. the necessity to go beyond the large deviation function analysis, will occur each time the optimal charge distribution corresponds to an inﬁnite value of the linear statistics � dx ρ0∗(x) f(x) = ∞ . (123) In this case Φ(s) is monotonous (Φ(s) → 0 for s → ∞), and only the pre-exponential function AN,β(s) in Eq. (3) can capture the behaviour of the distribution at inﬁnity. Still considering the case of the Laguerre ensemble (4) for θ = 0, i.e. the distribution (87), this occurs for example for the quantity L = (1/N) �N i=1 λ−α i when α ⩾ 1/2. Another interesting situation was considered in Ref. [2] : a mean ﬁeld approach has led to express the spin-glass susceptibility in terms of the eigenvalues {λi} of a N × N Gaussian random matrix, as χ = (1/N) � i(a−λi)−2, where the parameter a = T +1/T is related to the temperature T. When a ∈ supp(ρ0∗), the condition (123) is realised. In all the cases which we have studied, the conjecture (47) has always provided the full s-dependence of the distribution, although the constant cN,β remains most of the time unexplained. We also stress that our conjecture was only tested in situations where the charge density has a compact support. It would be interesting to discuss also the case of density with a support made of disconnected intervals. A rigorous derivation of Eq. (47) and its range of valididty are therefore still needed. The most promising route seems to be to clarify the connection with the loop expansion method used by Eynard and collaborators [25, 26, 8, 45]. This method provides an expansion for the generating function where ⟨· · ·⟩M denotes the averaging over the matrices. In the unitary case, the expansion only involves even powers : Our result (46) hence corresponds to the absence of a p-dependent contribution at order N −2. An explicit expression for the ﬁrst correction F1(p) has been obtained in Ref. [14] when the density has soft edges over disconnected intervals (the generalisation when both soft and hard edges are present is provided in Ref. [13]). For a density with a compact support with two soft edges, ρ∗(x) = (2π)−1M(x) � (x − a)(b − x), Chekhov and Eynard’s result reads F1(p) = −(1/24) ln [M(a) M(b) (b − a)4]. As an illustration, we apply this formula to the case analysed in Section 3.1, we ﬁnd that M(a) M(b) (b − a)4 = 28(1 + 1/θ)2 is indeed independent of s, i.e. on p = µ∗ 1(s). However this expression for F1(p) does not describe the situation studied in Section 4 with a transition between a soft and hard edge when s → ∞ [recall that c ≃ 2a → 0 in (93) in this limit]. Several questions therefore remain : In other terms what is the condition for the simpliﬁcation leading to a trivial contribution F1(p) = const ? How can one treat the transition between soft and hard edge (Section 4) ? What about the case where one single charge is driven away from the bulk, like in Ref. [56] ? This should make possible a proof of our conjecture (47) and clarify its range of validity. We acknowledge stimulating discussions with Satya Majumdar, Gr´egory Schehr and Pierpaolo Vivo. We are grateful to Bertrand Eynard for enlightening discussions and pointing to our attention Refs. [13, 14]. We thank the referee for many valuable remarks and Satya Majumdar for comments on the manuscript. We recall here a theorem due to Tricomi [57, 19, 59] useful at several places in the article. Consider the integral equation where g(x) is a known function. We assume that the solution ρ(x) has a compact support [a, b] (which also requires some conditions on the function g). The solution is
A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin<|sep|>Bitcoin, and cryptocurrencies, are known for their volatile nature. We propose a cutting-edge multimodal model to predict extreme Bitcoin price movements (up/down 2 or 5 percent). In order to train our model, we created a new publicly available dataset, which includes 9,435,437 tweets that include the keyword ‘Bitcoin’ from 1 Jan 2015 until 31 May 2021. We also included based candlestick price/volume data, as well as selected technical indicators and correlated asset prices (Ethereum and gold). The resulting multimodal ensemble model uses normalized data, as well as the FinBERT context embeddings to provide a meaningful representation of our Twitter data. In a thorough experiment, we perform an ablation study to compare the inﬂuence of adding the Twitter model or TA model to our hybrid model. This shows that adding prediction based on Twitter content improves the overall performance of the model for upward Bitcoin price prediction. Our proposed Fusion models demonstrate superior performance in positive class F1-score as well as overall accuracy in upward price prediction tasks compared to the TA SVM which uses only price and technical analysis data. To further evaluate our model’s performance and demonstrate its practical use, we propose a simple (long only) trading strategy and reported the backtesting results for our models that predict Up 5% price movement. During this backtesting, we explored the inﬂuence of tweaking the predictive threshold on risk management. The results conﬁrm the superior performance of our proposed TA SVM model as well as our multimodal Fusion model with 0.95 threshold in risk-adjusted measures such as Sortino ratio and maximum drawdown. While Buy and Hold strategies typically work well for Bitcoin and obtain huge proﬁts, the risks can be substantial, with max. drawdown reaching 45.5% in our test period. Our models substantially reduce this risk while maintaining an impressive proﬁt ratio. The usefulness of our proposed approach becomes especially apparent during the bear market, when our Fusion strategy manages achieved 32% Proﬁt (with long positions only), despite the fact that the Bitcoin price was down by -40.5%. We further observe that a carefully selected probability threshold can signiﬁcantly improve the trading performance and lower the market exposure risk. This work opens up many avenues for future research. For instance, the Twitter dataset could be more eﬀective for predictions if it only includes tweets by inﬂuencers in the ‘crypto-Twitter sphere’, such as Elon Musk, CEOs of cryptocurrency exchanges, and many more. In addition, we may ﬁnetune the FinBERT model to better capture cryptocurrency-speciﬁc as well as Twitter-speciﬁc lingo. Finally, the resulting multimodal model’s prediction threshold may be further ﬁnetuned with a more complex trading strategy, possibly including the model’s class probability to size positions, to outperform the benchmark provided for our new dataset in this research.
Magneto-transport of graphene and quantum phase transitions in the quantum Hall regime<|sep|>Summarizing, we have studied the magneto-transport of graphene and the nature of the quantum phase transitions that characterize the quantum Hall regime. Concerning the ν = −2 to ν = 0 plateau-insulator transition, we have observed the QPT up to 44.5 K, pointing out its robustness. Using the standard scaling theory analysis we have experimentally obtained its critical exponent, ﬁnding that it is consistent with the universal value of semiconductor 2DEG when the sample is doped away from the Dirac point κ = 0.58(1), while it tends to the classical full percolation limit value κ = 0.697(5) when VG approaches the charge neutrality point. Our measurement indicates that the electron-electron interaction plays a major role on the magneto-transport close to the charge neutrality point, modifying the value of the critical exponent. Moreover we have found no traces of Kosterlitz-Thouless transitions approaching the CNP in contrast with the previous ﬁnding [31]. As regards the other QPT, the plateauplateau one, we have studied it up to 230 K obtaining the value for the critical exponent κ = 0.25, and we revealed its non universality. Our results evidence that magneto-transport measurements in graphene in the quantum Hall regime, and the analysis of the observed quantum phase transitions, are powerful tools to elucidate the interplay between disorder and the electron-electron interaction, which controls the charge transport in the graphene nano-devices.
Measurement of the solar system acceleration using the Earth scale factor<|sep|>We introduce a new method to detect the secular aberration drift induced by the Galactocentric acceleration from the geodetic VLBI measurements. It is based on ﬁtting the scale factor corrections estimated for each source individually within a global solution. In Titov & Lambert (2013) we used the individual proper motion of reference radio sources to visualise the dipole eﬀect caused by the Galactocentric acceleration. However, because of contamination by large apparent motions induced by relativistic jet, the proper motions have large random noise almost hiding the dipole systematics. However, if the dipole components in proper motion are not estimated, the systematic eﬀect comes to the scale eﬀect that, in contrast to the proper motion, is free of the relativistic jet problems. From ﬁtting the individual scale factor corrections of sources with more than 50 observations during 1979.7 - 2016.5 we obtained a Galactocentric acceleration vector with an amplitude of 5.2 ± 0.2 µas/yr and direction αG = 281◦±3◦ and δG = −35◦±3◦. The SAD was also estimated directly within a global adjustment of the VLBI data. The Galactocentric acceleration vector determined from the selected large network IVS sessions after 1993 (A = 5.4 ± 0.4 µas/yr, αG = 273◦ ± 4◦, δG = −27◦ ± 8◦) is closer to the value reported by recent papers (e.g. Reid & Brunthaler (2004) and de Grijs & Bono (2016)) than the estimate from the entire VLBI history. The formal error of the amplitude of the acceleration vector estimated by means of the scale factor is about ﬁve times better than the estimate from the proper motions (Titov & Lambert 2016) and comparable to the accuracy expected to be obtained with the Gaia mission (Mignard & Klioner 2012). This demonstrates the advantage of the new approach with respect to the traditional procedure. Our result also points out the potential impact of neglecting the Galactocentric acceleration on the scale factor and, ﬁnally, on the geodetic positions of the radio telescopes. The scale factor becomes dependent on the positions of the reference radio sources, i.e. larger in the direction of the Galactic anti-centre, and smaller in the direction of the Galactic centre. In a global sense, the quality of the geodetic results is worsening and, moreover, the loss of quality would increase linearly with time. The results presented in this paper were computed with the VieVS software and veriﬁed with the OCCAM software package. Acknowledgements. The authors thank the anonymous referee for her/his constructive comments. The authors acknowledge the IVS and all its components for providing VLBI data (Nothnagel et al. 2015). The Long Baseline Array is part of the Australia Telescope National Facility which is funded by the Australian Government for operation as a National Facility managed by CSIRO (P483 and V515). This study also made use of data collected through the AuScope initiative. AuScope Ltd is funded under the National Collaborative Research Infrastructure Strategy (NCRIS), an Australian Commonwealth Government Programme. The Very Long Baseline Array (VLBA) is operated by the National Radio Astronomy Observatory, which is a facility of the National Science Foundation, and operated under cooperative agreement by Associated Universities, Inc. Hana Krásná works within the Hertha Firnberg position T697-N29, funded by the Austrian Science Fund (FWF). This paper has been published with the permission of the Geoscience Australia CEO.
Structural correlations and phase separation in binary mixtures of charged and uncharged colloids<|sep|>To summarize, we have calculated the pair correlations in a binary mixture of charged and neutral colloids using the primitive model with explicit counterions and compared the results to the Yukawa-DLVO model. We found that the traditional DLVO based approach is suﬃcient to describe the pair correlations in aqueous suspensions with high dielectric constant. However, in less polar solvents with reduced permittivity, the Coulomb coupling without screening between the charge species is getting much stronger which results in nonlinear screening eﬀects. In this case the Yukawa-DLVO approach completely fails to describe the PM results for the pair correlations. We predict, for the ﬁrst time, indications for a ﬂuid-ﬂuid phase separation in such strongly nonlinear systems. Our simulations show that there are two regions either rich with charged or with neutral colloidal particles. Additional two macroion simulations were staged to calculate interaction potentials between colloids in the binary runs that show ﬂuid-ﬂuid phase separation. We found that the interaction between the charged colloids is strongly repulsive, whereas it is moderately repulsive between the charged and neutral colloids. More interestingly, the interaction between neutral colloids is strongly attractive. We believe that the latter is the main reason for the ﬂuid-ﬂuid phase separation in the simulated binary system with strong Coulomb coupling. Acknowledgments The work of H.L. was supported by the DFG within project LO418/23-1. E.A. thanks the ﬁnancial support from the Ministry of Science and Higher Education of the Russian Federation (State Assignment No. 075-01056-22-00). In the PM simulations, the excluded volume of the macroions initiates entropic forces arising from the contact density of counterions and neutral colloids on the macroion surface. The entropic force acting on a i-th macroion of species α at the position ⃗r (α) i with i ∈ 1, ..., Nα (α = Z, z) is deﬁned as [60,61,65–68]. where ⃗f is a surface normal vector pointing outwards the macroion’s core, S(α) i is the surface of the hard core of the i-th macroion centered around ⃗r (α) i with diameter (σα + σβ)/2, β=z for neutral colloids, and β=c for the counterions, and ρβ(⃗r) is the density of particles of sort β around the i-th macroion. The entropic force, usually neglected in weakly charged macroion systems, strongly modiﬁes the macroion interactions in highly charged and dense colloidal systems. Likewise, the canonically averaged electrostatic force acting on the i-th macroion of species α is deﬁned as, where α = Z, z and β = z, c. The Kronecker delta functions in this expression nullify the self-interaction of macroions. Clearly, in Eq.(A2) the electrostatic pair interaction forces ⃗F (αβ) are deﬁned as, parametrically depends on the ﬁxed macroion positions ρc(⃗r) ≡ ρc(⃗r, {⃗r (Z) i ,⃗r (z) j }), {⃗r (Z) i ,⃗r (z) j , i = 1, ..., NZ; j = 1, ...Nz}, where the counterion averaging ⟨ · · · ⟩c and the macroion averaging ⟨ · · · ⟩α (α = Z, z ) are done for ﬁxed macroion positions. As seen from Figure B1, where the run series Ai and Bi are analyzed, the counterion density at the surface of neutral colloids in the PM binary runs is larger than in the bulk. This eﬀect appears to be more solid for η=0.2 simulations. Because no counterions are originally associated with neutral colloids, it is the counterions stemming from the charged macroions that exist near neutral colloids. This conclusion is supported by the appearance of the minimum in the counterion distribution at around r = 1.2σ, see the red line, below which the macroion-neutral association develops in the PM binary runs in Figure 3, see the blue line there.
Particle generation through restrictive planes in GEANT4 simulations for potential applications of cosmic ray muon tomography<|sep|>All in all, by setting out our restrictive generation scheme, we optimize the particle loss by keeping an angular disparity that is directly dependent on the VOI geometry as well as the vertical position of the restrictive plane for a tomographic system of a ﬁnite size. Upon our simulation outcomes, we show that the particle generation by means of restrictive planes is an eﬀective strategy that is ﬂexible towards a variety of computational objectives in GEANT4. Into the bargain, we explicitly observe that the oﬀ-target loss is a characteristic parameter that varies in an ascending order from aluminum to uranium.
Robust Tracking Control for Constrained Robots<|sep|>This paper proposes a simple robust controller for motion tracking of constrained robots under unknown stiffness  environment. The proposed approach takes care on the compromise between robustness and safety for the tracking control  problem. The stability and the robustness of the controller are proved using a Lyapunov-based approach. Future  investigations will concern the application of the proposed approach for humanoid and rehabilitation robotic devices.
Lowest-ID with Adaptive ID Reassignment: A Novel Mobile Ad-Hoc Networks Clustering Algorithm<|sep|>We introduced an algorithm for efficient and energybalanced  clustering  of  mobile  ad-hoc  networks.  Its  contributions, compared to existing solutions, are summarized  in the following: (a) clustering procedure is completed within  two ‘Hello’ cycles; (b) both mobility and battery power metrics  are taken into account in clustering process, so that suitable  nodes are elected as CHs and energy consumption is uniformly  distributed among network nodes; (c) for relatively static  network topologies, control traffic volume is minimized; (d)  fast packet forwarding and delivery is enabled, as clusters are  pro-actively formed and topology information is available  when actual user data exchange is required.  As a future extension, we intend to incorporate node degree  metric within the calculated weight function, and also introduce  a mobility prediction method in LIDAR. Finally, a variation of  LIDAR algorithm where node IDs are received, sorted and reassigned by a single (centralized) node will be implemented  and compared in terms of cost against the distributed ID reassignment method described in this article.
Interferometric probe of paired states<|sep|>In this paper we addressed questions of application of interference experiments to detect paired states of either fermions or bosons in low dimensions. We showed that direct generalizations of approaches used in analyzing interference of independent bosonic condensates do not work due to overwhelming shot noise contribution. Thus we proposed and analyzed two alternative schemes of interference experiments which can be used to study anomalous correlation functions, which contain information about pairing amplitudes. These (gaugenoninvariant) correlation functions provide complimentary information to normal correlation functions and can be used to characterize the properties of the superﬂuids. It was shown how the method of studying the anomalous functions can be used to detect various pairing orders. One of the scheme we propose is based on the phase sensitive detection employed earlier in the condensed matter systems. On the other hand, another scheme deals with two superﬂuids weakly coupled by interlayer tunneling. We establish the condition of validity of this scheme which involves the tunneling strength, imaging area and the system size. We emphasized important roles of diﬀerent form of expansion (transversal and longitudinal) and directions of observation. In the case of bosonic superﬂuids anomalous correlation functions have an unusual property that they increase with the separation between quasiparticles.
Radio-Optical Galaxy Shape Correlations in the COSMOS Field<|sep|>In this paper we have performed a detailed comparison analysis of the shapes of galaxies in the COSMOS ﬁeld, as measured in the optical using HST, and as measured in the radio, using the VLA. Our study has been motivated by the scientiﬁc potential of cross-correlation cosmic shear analyses of future overlapping optical and radio surveys (Brown et al. 2015; Harrison et al. 2016; Bonaldi et al. 2016; Camera et al. 2016). In order to fully exploit such future cross-correlations, one needs to understand in detail the correlations in intrinsic optical and radio shapes, which is the issue that we have attempted to address in this study. In the course of our analysis, we have highlighted some of the challenges involved in extracting shapes from radio observations. For this analysis we have chosen to measure galaxy shapes from images reconstructed from the VLA radio data. In particular, we have used simulations, composed of a known distribution of galaxy shapes combined with a realistic radio interferometer observation and imaging pipeline, to show the eﬀectiveness of position angle (α) recovery through typical radio data reduction and image creation techniques. We have investigated the use of two variations of the widely used H¨ogbom-clean image creation algorithm in the radio: natural and uniform visibility weighting. We ﬁnd the choice in weighting scheme aﬀects the measured shape parameters greatly. In particular the choice of a uniform weighting scheme yields largely discrepant values of multiplicative bias in ellipticity components for a given galaxy. We found this problem was diminished when natu ral weighting was used, which produced highly similar multiplicative bias on ellipticities, which later cancels when position angles are derived from the measured ellipticities. This allowed us to obtain unbiased estimates of the position angles, which were important for this study. Quantifying the bias in position angle (α) achieved in this study with a linear bias model, a link to weak lensing requirements for future surveys was established via the position angle only shear estimators of Whittaker et al. (2014). A key result is shown in Figure 6 which allows one to translate from the usually quoted requirements on shear bias to requirements on position-angle bias. We ﬁnd that although our position angle recovery appears relatively unbiased given our measurement errors, the uncertainty on the position angle bias that we are able to achieve using current data is still larger than current weak lensing requirements. After quantifying the expected uncertainties through simulations we have applied our shape measurement pipeline to the real COSMOS radio (VLA) observations. We used the resulting radio galaxy shape measurements (and associated uncertainties) to place a lower limit on the astrophysical scatter in source position angles between the continuum radio and optical emission of the sources which are detected in both our VLA data and an optical HST study of the same ﬁeld. We ﬁnd this lower limit to be σscatter α ≳ 0.212π (or 38.2◦) at a 95% conﬁdence level. This appears consistent with results from previous studies (Patel et al. 2010) considering the low absolute number of sources in our sample. Understanding the radio-optical shape correlations is important as it will aﬀect the noise term on the cosmological power spectra measured by radio-optical cross-correlation weak lensing studies (Harrison et al. 2016; Camera et al. 2016). High levels of correlation will increase the shot noise on shear measurement, but allow for cross-waveband calibration against some systematics, whilst low levels of correlation eﬀectively increase the source number density being used to probe the cosmic shear ﬁeld. In the near future deep, high-resolution optical and radio surveys such as SuperCLASS10 will further constrain this correlation, better informing forecasts and survey design considerations for SKA. We thank Chris Hales, Anita Richards, Neal Jackson, Lee Whittaker and Joe Zuntz for useful discussions. We further thank Chris Hales, Eva Schinnerer and Vernesa Smolcic for providing us with the partially processed VLA data. MLB is an STFC Advanced/Halliday fellow. BT, IH and MLB are supported by an ERC Starting Grant (grant no. 280127).
Automated One-Loop Calculations with GoSam<|sep|>We have presented the program package GoSam which produces, in a fully automated way, the code required to perform the evaluation of one-loop matrix elements for multi-particle processes. The program is publicly available at http://projects.hepforge.org/gosam/ and can be used to calculate one-loop amplitudes within QCD, electroweak theory, or other models which can be imported via an interface to LanHEP and UFO, also included in the release. Monte Carlo programs for the real radiation can be easily linked through the BLHA interface. GoSam is extremely ﬂexible, allowing for both unitarity-based reduction at integrand level and traditional tensor reduction, or even for a combination of the two approaches when required. The amplitudes are generated in terms of Feynman diagrams within the dimensional regularisation scheme, and optionally the calculation can be carried out either in the ’t Hooft Veltman or in the dimensional reduction variant. The user
Conductivity of two-dimensional narrow gap semiconductors subjected to strong Coulomb disorder<|sep|>In this paper, we have considered the temperaturedependent conductivity of a two-dimensional insulator subjected to the random potential created by Coulomb impurities in the substrate. Our primary results can be summarized as follows. First, the random potential of charged impurities necessarily produces large band bending. We focus here on the case where the impurity concentration is large enough that the disorder potential Γ ≫ ∆, and the system can be described as a network of large and closely spaced fractal puddles (Fig. 1) separated by narrow insulating barriers (Fig. 3). This case is characterized by the “three-mechanism sequence” of temperature-dependent conductivity illustrated in Fig. 2. Only the highest-temperature regime has an activation energy Ea equal to half the band gap ∆. The middle regime, with activated hopping between puddles (AH), exhibits a parametrically smaller activation energy whose value depends on the impurity concentration, while the lowest-temperature regime corresponds to Efros-Shklovskii conductivity, which may appear as an even smaller activation energy when measured over a limited temperature range. Second, when the impurity concentration N exceeds some critical value, the tunnel barriers between puddles become thin enough to be nearly transparent, and electrons are delocalized across many puddles. In this limit, the localization length grows exponentially with increased disorder, and the corresponding activation energy falls exponentially, so that in mesoscopic samples one effectively has a disorder-induced insulator-to-metal transition. Our results have implications for a wide variety of experiments on 2D electron systems with a narrow energy gap. Some of these include 2D and thin 3D TIs, Bernal bilayer graphene with a perpendicular displacement ﬁeld, and twisted bilayer graphene, as mentioned in Sec. I. In such systems, the temperature-dependentconductivity is often used as a primary way to diagnose the magnitude of energy gaps. Our results here suggest that such studies suffer an essentially unavoidable limitation, since the apparent activation energy Ea at low temperature has no simple relation to the energy gap, and, in general, Ea can be taken only as a weak lower bound. No wonder that the transport activation energy in many cases is 100 times smaller than the value expected theoretically or mea sured by probes such as optical absorption or tunneling spectroscopy. In principle, one can infer the band gap by measuring the activation energy at the highest-temperature regime. However, the existence of this regime practically requires a low enough disorder that electron and hole puddles are small and well separated from each other. Even in this case, experiments using transport to estimate ∆ should ﬁrst demonstrate two distinct regimes of constant activation energy, and then use only the value from the higher-T regime as an estimate of ∆. The existence of an apparent disorder-induced IMT at Γ/∆ > (Γ/∆)c is an especially striking result of our analysis. For conventional insulators, this apparent transition cannot be called a true IMT, since in 2D the zero-temperature conductance ﬂows toward zero in the thermodynamic limit for any ﬁnite amount of disorder [66]. However, the situation may be different for thin TI ﬁlms, since the spin-orbit coupling of the TI surface states permits a stable metallic phase [67, 68]. A full theory of this IMT in TI ﬁlms is beyond the scope of our current analysis. We are grateful to Stevan Nadj-Perge, Koji Muraki, Shahal Ilani, Ilya Gruzberg, and David Goldhaber-Gordon for helpful conversations. Y.H. is supported by the William I. Fine Theoretical Physics Institute. B.S. was partly supported by NSF Grant No. DMR-2045742.
Reflection and Rotation Symmetry Detection via Equivariant Learning<|sep|>In this paper, we have proposed a novel symmetry detection framework, EquiSym, using equivariant learning to obtain group-equivariant and invariant scores for both reﬂection and rotation symmetries. In addition, we have intro duced a new dataset DENse and DIverse symmetry dataset (DENDI) for reﬂection and rotation symmetries. The proposed EquiSym achieves the state-of-the-art on LDRS and DENDI datasets. Acknowledgements. This work was supported by Samsung Advanced Institute of Technology (SAIT) and also by the NRF grant (NRF-2021R1A2C3012728) and the IITP grant (No.2021-0-02068: AI Innovation Hub, No.2019-001906: Artiﬁcial Intelligence Graduate School Program at POSTECH) funded by the Korea government (MSIT). We like to thank Yunseon Choi for her contribution to DENDI.
Feature Guided Search for Creative Problem Solving Through Tool Construction<|sep|>In this work, we presented the Feature Guided Search (FGS) approach that allows existing heuristic search algorithms to be efﬁciently applied to the problem of tool construction in the context of task planning. Our approach enables the robot to effectively construct and use tools in cases where the required tools for performing the task are unavailable. We relaxed key assumptions of the prior work in terms of eliminating the need to specify an input action, instead integrating tool construction within a task planning framework. Our key ﬁndings can be summarized as follows: • FGS signiﬁcantly reduces the number of nodes expanded by ≈ 82%, and the number of construction attempts by ≈ 93%, compared to standard heuristic search baselines. • The approach achieves a success rate of 87% within a resource budget of 8 attempts when sensors are fully trusted, and 100% within a budget of 39 attempts, when the sensors are not fully trusted. • FGS enables ﬂexible generation of task plans based on objects in the environment, by adapting the task plan to appropriately use the constructed tool. Our work is one of the ﬁrst to integrate tool construction within a task planning framework, but there remain many unaddressed manipulation challenges in tool construction that are beyond the scope of this paper. Tool construction is a challenging manipulation problem that involves appropriately grasping and combining the objects to successfully construct the tool. That is, once the robot has correctly identiﬁed the objects that need to be combined (focus of this paper), the robot would then have to physically combine the objects, and use the constructed tool for the task. Currently, our work pre-speciﬁes the trajectories to be followed for tool construction, although existing research in robot assembly can be leveraged to potentially accomplish this [49]. Further, a key question to be addressed is, how can the robot learn to appropriately use the constructed tool? Future work could address this problem by leveraging Figure 6: Collage indicating sample tool constructions output for two test cases per task. The solid and dashed brackets indicate the test set of objects provided in each case, along with the tool constructed for it. As the objects are changed, the corresponding constructed tool and action is different. Note that the problem and domain deﬁnitions are ﬁxed for each task, and unchanged across the test cases per task. existing research in tool use [46, 42, 43], and trajectory-based skill adaptation [14, 16]. Upon successful construction of the tool, the research problem reduces to that of using the tool appropriately. In this case, the robot can either learn how to use the tool as described in [46, 42, 43] or, the robot can adapt previously known tool manipulation skills to the newly constructed tool as described in [14, 16]. Addressing these challenges is important to further ensure practical applicability of tool construction. Additionally, creation of tools through the attachment types discussed in this work is currently restricted to a limited number of use cases, in which two objects that have the speciﬁc attachment capabilities already exist, and are available to the robot. In the future, we seek to expand to more diverse types of attachments, including gluing or welding the objects together, as well as creation of tools from deformable materials, in order to improve the usability of our work. We further seek to expand on this work by investigating the application of feature scoring to domains other than tool construction. In particular, we seek to investigate the different ways in which feature score can be effectively combined with the cost function for other domains involving tool-use such as tool substitution. While our proposed cost function is dependent on the values of the feature score and is shown to perform well for tool construction, it is important to further investigate the cost function and its inﬂuence on the guarantees of the search to allow for a more generalized application of FGS. FGS enables the robot to perform high-level decision making with respect to the objects that must be combined in order to construct a required tool. In this work, we use physical sensors (RGBD sensors and SCiO spectrometer) that produce partial point clouds and noisy spectral scans, leading to some challenges that commonly arise in the real world. Nevertheless, there are several open research questions that need to be addressed before this work can be deployed in a real setting. Thus, FGS is the ﬁrst step within a larger pipeline, and we envision this work to be complementary to existing frameworks that are aimed at resilient and creative task execution, such as [4, 47]. In summary, FGS presents a promising direction for dealing with tool-based problems in the area of creative problem solving.
A new approach for numerical simulation of the time-dependent Ginzburg-Landau equations<|sep|>We have introduced a new approach for the numerical simulation of the time-dependent Ginzburg–Landau model of superconductivity in a general curved polygon which may contain reentrant corners, by reformulating the equations under the Lorentz gauge into an equivalent system of equations. Mathematically speaking, this new approach is more suitable for Ginzburg–Landau equations with strong corner singularities. Indeed, numerical simulations demonstrate that it is more stable and accurate than the traditional approaches in the presence of a reentrant corner, and comparably accurate as the traditional approaches in a convex domain. Figure 15. Contour of |ψ|2 with H = 0.8 by solving the TDGL under the temporal gauge with a locally reﬁned mesh.
An Improved Traffic Matrix Decomposition Method with Frequency-Domain Regularization<|sep|>This letter presents a novel traﬃc matrix decomposition method named SPCP-FDR, which improves SPCP by using frequency-domain regularization. We propose an APG algorithm for SPCP-FDR, its convergence rate is O(1/k2), and demonstrate it on a real-world dataset. SPCP-FDR efﬁciently eliminates the low-frequency periodic traﬃc from the noise traﬃc, maintains deterministic traﬃc’s low-rank property, and shows lower cross-correlation between these two kinds of traﬃc than SPCP. Therefore, SPCP-FDR achieves more rational traﬃc decompositions.
Penalized Push-Sum Algorithm for Constrained Distributed Optimization with Application to Energy Management in Smart Grid<|sep|>In this paper we extended the distributed push-sum algorithm to the case of constrained convex optimization. The penalty-based push-sum algorithm was presented and its convergence to a system’s optimum was proven. We demonstrated applicability of the proposed procedure to distributed energy management in smart grid. The future work will focus on such questions as the convergence rate
Tracking without bells and whistles<|sep|>We have shown that the bounding box regressor of a trained Faster-RCNN detector is enough to solve most tracking scenarios present in current benchmarks. A detector converted to Tracktor needs no speciﬁc training on tracking ground truth data and is able to work in an online fashion. In addition, we have shown that our Tracktor is extendable with re-identiﬁcation and camera motion compensation, providing a substantial new state-of-the-art on the MOTChallenge. We analyzed the performance of multiple dedicated tracking methods on challenging tracking scenarios and none yielded substantially better performance compared to our regression based Tracktor. We hope this work establishes a new tracking paradigm, utilizing the object detector’s full capabilities.
SenTion: A framework for Sensing Facial Expressions<|sep|>This paper has presented a computationally efﬁcient facial expression recognition system (SenTion) for accurate and robust classiﬁcation of the six universal expressions. We proposed a novel approach of taking Inter Vector Angles (IVA) as geometric features, which proved to be scale invariant and person independent. It’s combination with appearance based Histogram of Gradients, provided a hybrid model which was reliable and stable across databases without additional adaptation. The effectiveness of the proposed approach of IVA’s and appearance HOG based features is validated by it’s performance on the CK+ and JAFFE database and comparison with other state-of-the art methods. We outperformed the state of the art algorithm on the CK+ database, achieving an accuracy of 97.38%. The proposed approach being simplistic, consistent, computationally sensitive and real time can be potentially applied into many applications, such as human emotional state detection, patient monitoring, fatigue detection and tutoring systems. In our future work, we will extend our approach to in the wild settings and also test it’s performance on video based facial expression recognition systems.
Sharp recovery bounds for convex demixing, with applications<|sep|>The results in this work demonstrate the power of spherical integral geometry in the context of demixing. Our bounds are often tight, and they are broadly applicable. This approach raises many questions worth further attention. We conclude with a list of directions for future work. During the period that our original manuscript was under review, several of these areas have seen signiﬁcant progress. We augment our original list of open problems with a summary of progress made in the interim. Tight results for Lagrangian demixing. The Lagrange penalized demixing method (1.3) is important because it requires less knowledge about the unobserved vectors (x0, y0) than the corresponding constrained method (1.2). The results in this work give information regarding the potential for, and the limits of, the penalized demixing approach (1.2). Nevertheless, a precise analysis of the penalized problem (1.3) and its dependence on the penalty parameter λ would have real practical value. While a sharp phase transition characterization for the Lagrange demixing problem (1.3) remains open, the recent work [37] offers theoretical guarantees and explicit choices of Lagrange parameters. A study of this problem appears in [83]. The present authors provide sharp phase transition characterizations for demixing an arbitrary number of signals in [53]. Spherical intrinsic volumes for more cones. Computation of additional decay thresholds will provide new bounds for convex demixing methods. The sharpest decay thresholds appear to require formulas for spherical intrinsic volumes. For instance, an asymptotic analysis of the spherical intrinsic volumes for feasible cones of the Schatten 1-norm would provide sharp recovery results for low-rank matrix demixing problems. Amelunxen & Bürgisser have made some recent progress in this direction by developing a formula for the spherical intrinsic volumes for the semideﬁnite cone [4]. Log-concavity of spherical intrinsic volumes. Bürgisser & Amelunxen [10, Conj. 2.19] conjecture that the sequence of spherical intrinsic volumes is log-concave. This conjecture is closely related to the question of whether the upper and lower decay thresholds match. In recent work by the present authors and collaborators, the intrinsic volumes are shown to have a nontrivial log-concave upper bound [5, Sec. 6.1]. While this result implies that the upper and lower decay thresholds are often equal (Section 4.2.2), the log-concavity conjecture remains open. Extensions to more general probability measures. The analysis in this work focuses on a speciﬁc random model. It would be interesting to incorporate more general probability measures into our framework. This may be a difﬁcult problem; by the results of Section 5.2, this question is closely related to the observed universality phenomenon in basis pursuit [30]. Bayati et al. [6] provide a rigorous version the universality property for basis pursuit observed in [30]. It remains unclear whether their methods adapt to demixing problems considered here. This appendix provides a geometric account of the equivalence between the constrained (1.2) and penalized (1.3) convex demixing methods. The results in this section let us interpret our conditions for the success of the constrained demixing method (1.2) as limits on, and opportunities for, the Lagrange demixing method (1.3). We begin with the following well-known result; it holds without any technical restrictions. We omit the demonstration, which is an easy exercise in proof by contradiction. (See also [65, Cor. 28.1.1].) Before stating a partial converse to Proposition A.1, we require a technical deﬁnition. We say that a proper convex function f is typical at x if f is subdifferentiable at x but does not achieve its minimum at x. With this technical condition in place, we have the following complement to Proposition A.1. Proposition A.2. Suppose f is typical at x0 and g is typical at y0. If the constrained method (1.2) succeeds, then there exists a parameter λ > 0 such that (x0, y0) is an optimal point for the Lagrange method (1.3). Note that there is a subtlety here: the Lagrange program may have strictly more optimal points than the corresponding constrained problem even for the best choice of λ, so that we cannot guarantee that (x0, y0) is the unique optimum. See [65, Sec. 28] for more details. Proof of Proposition A.2. The key idea is the construction of a subgradient that certiﬁes the optimality of the pair (x0, y0) for the Lagrange penalized problem (1.3) for an appropriate choice of parameter λ. As with many results in convex analysis, a separating hyperplane plays an important role. By Lemma 2.4, the constrained problem (1.2) succeeds if and only if F(f ,x0)∩−QF(g, y0) = {0}. The trivial intersection of the feasible cones implies that there exists a hyperplane that separates these cones. (This fact is a special case of the Hahn–Banach separation theorem for convex cones due to Klee [50].) In other words, there exists some vector u ̸= 0 such that In the language of polar cones, the ﬁrst separation inequality is simply the statement that u ∈ F(f ,x0)◦, while the second inequality is equivalent to Q∗u ∈ F(g, y0)◦. We will now show that u generates a subgradient optimality certiﬁcate for the point (x0, y0) in problem (1.3) for an appropriate choice of parameter λ > 0. We denote the subdifferential map by ∂. At this point, we invoke our technical assumption. Since f is typical at x0, the polar to the feasible cone is generated by the subdifferential of f at x0 [65, Thm 23.7]. In particular, there exists a number λf ≥ 0 such that u ∈ λf ∂f (x0). In fact, the stronger inequality λf > 0 holds because u ̸= 0. For the same reason, there exists a number λg > 0 such that Q∗u ∈ λg ∂g(y0). Deﬁne h(x) := λf f (x) + λg g(Q∗(z0 − x)). By standard transformation rules for subdifferentials [65, Thms. 23.8, 23.9], we have ∂h(x0) ⊃ λf ∂f (x0)−λgQ∂g(y0), where A−B := A+(−B) is the Minkowski sum of the sets A and −B. Since u ∈ λf ∂f (x0) and u ∈ λgQ∂g(y0), we see 0 ∈ ∂h(x0). By the deﬁnition of subgradients, x0 is a global minimizer of h. Introducing the variable y = Q∗(z0 − x), it follows that (x0, y0) is a global minimizer of We now present the proofs of the results of Section 4 concerning regions of failure (Theorem 4.3) and strong demixing guarantees (Theorem 4.11) for the convex demixing method. These demonstrations closely follow the pattern laid down by the proof of Theorem 4.2. Theorem B.1. Let {K (d) ⊂ Rd : d ∈ D} and { ˜K (d) ⊂ Rd : d ∈ D} be two ensembles of closed convex cones with lower decay thresholds κ⋆ and ˜κ⋆. If κ⋆+ ˜κ⋆ > 1, then there exists an ε > 0 such that P � K (d)∩Q ˜K (d) ̸= {0} � ≥ 1−e−εd for all sufﬁciently large d. Proof of Theorem 4.3 from Theorem B.1. By the assumptions in Theorem 4.3, the ensembles � F(f (d),x(d) 0 ) � and � −F(g (d), y(d) 0 ) � of closed cones satisfy the hypotheses of Theorem B.1. Therefore, there is an ε > 0 such that the closure of the feasible cones have nontrivial intersection with probability at least 1−e−εd, for all large enough d. It follows from Remark 4.5 that the probability of the event F(f (d),x(d) 0 )∩−QF(g (d), y(d) 0 ) ̸= {0} is equal to the probability of the event F(f (d),x(d) 0 ) ∩ −QF(g (d), y(d) 0 ) ̸= {0}. Applying the geometric optimality condition of Lemma 2.4 immediately implies that (1.2) fails with probability at least 1−e−εd. Fact B.2 (Spherical Gauss–Bonnet formula [69, P. 258]). For any closed convex cone K ⊂ Rd that is not a subspace, we have d−1 � In the proof below, the Gauss–Bonnet formula is crucial for dealing with the parity term (1+(−1)k) that arises in the spherical kinematic formula (3.2). Proof of Theorem B.1. Since the Gauss–Bonnet formula only applies to cones that are not subspaces, we split the demonstration into three cases: neither ensemble {K (d)} nor { ˜K (d)} contains a subspace, one ensemble consists of subspaces, or both ensembles consist of subspaces. We assume without loss that each case holds for every dimension d ∈ D; the proof extends to the general case by considering subsequences where only a single case applies. We drop the superscript d for clarity. Assume ﬁrst that neither K nor ˜K is a subspace. Let ϖ(k) = (1+(−1)k) be the parity term in the spherical kinematic formula (3.2). Changing the order of summation in the spherical kinematic formula, we ﬁnd Let κ < κ⋆ and ˜κ < ˜κ⋆ with κ+ ˜κ > 1; such scalars exist because κ⋆ + ˜κ⋆ > 1. By positivity of the spherical intrinsic volumes (Fact 3.5.1), we have where ˜ξi is a discrepancy term (see (B.3) below). The second equality follows by the spherical Gauss–Bonnet formula (Fact B.2), and the assumption that ˜K is not a subspace. We now bound the discrepancy term ˜ξi uniformly over i ≥ ⌈κd⌉+1. Since κ+ ˜κ > 1, for any i ≥ ⌈κd⌉+1 we have d −2−i ≤ ⌈˜κd⌉. By deﬁnition of the lower decay threshold, we see that the discrepancy term must be small: for any i ≥ ⌈κd⌉+1, where the second inequality follows from Fact 3.5: the spherical intrinsic volumes are positive and sum to one. We now reindex the sum on the right-hand side of (B.4) over i = −1,0,...,d −1 with only exponentially small loss: d−1 � for some ε′′ > 0 and all sufﬁciently large d by deﬁnition of the lower decay threshold. Applying these observations to (B.4), we deduce that for some ε > 0 and all sufﬁciently large d. Since �d−1 i=−1 vi(K ) = 1 by Fact 3.5.2, this shows the result when both K and ˜K are not subspaces, completing the ﬁrst case. For the second case, suppose that only one of the cones is a subspace. Without loss, we may assume ˜K is the subspace by the symmetry of the spherical kinematic formula (see Remark 3.8). Denote the dimension of the subspace ˜K by ˜n := dim( ˜K ), and take parameters κ and ˜κ as above. By Proposition 3.2, the spherical intrinsic volumes of ˜K are given by vi( ˜K ) = δi, ˜n−1. Inserting this Kronecker δ into the spherical kinematic formula (3.2) and simplifying the resulting expression, we ﬁnd the probability of interest is given by We now show that ˜n is relatively large. The deﬁnition of the lower decay threshold implies that there exists an ε′ > 0 such that vi( ˜K ) = δi, ˜n−1 ≤ e−ε′d for all i ≤ ⌈˜κd⌉ when d is sufﬁciently large. This inequality cannot hold if ˜n −1 ≤ ⌈˜κd⌉, so we deduce that ˜n ≥ ˜κd for all sufﬁciently large d. Since ˜n ≥ ˜κd and κ + ˜κ > 1, we must have d − ˜n − 1 < ⌈κd⌉ for all sufﬁciently large d. Applying the deﬁnition of the lower decay threshold, we ﬁnd the sum on the right-hand side of (B.5) is exponentially small: there exists an ε′′ > 0 such that for all sufﬁciently large d. The result for the second case follows immediately. Finally, we consider the case when both of the cones are subspaces. Suppose K has dimension n, while ˜K has dimension ˜n, and let κ, ˜κ be as above. As in the second case, we ﬁnd that n ≥ κd and ˜n ≥ ˜κd when d is sufﬁciently large. Then the inequality κ⋆ + ˜κ⋆ > 1 implies that n + ˜n > d, that is, the sum of the dimensions of the subspaces is larger than the ambient dimension. A standard fact from linear algebra implies K ∩Q ˜K ̸= {0} for any unitary Q—in other words, for all d large enough, the probability of nontrivial intersection is one. This completes the third case, and we are done. Proof of Theorem 4.11. For clarity, we drop the superscript d in this proof. We begin with the union bound: the probability of interest P is bounded above by P := P � K ∩Q ˜K ̸= {0} for any K ∈ K , ˜K ∈ ˜ K � ≤ |K |·| ˜ K |·P � K ∩Q ˜K ̸= {0} � . (B.6) From here, the proof closely parallels the proof of Theorem 4.4, so we compress the demonstration. We consider two cases, one where at least one cone is a subspace, the other where neither cone is a subspace; the result extends to the mixed case by considering subsequences. Suppose ﬁrst that at least one cone is a subspace. Let θ > θ⋆ and ˜θ > ˜θ⋆ with θ + ˜θ < 1. We bound the probability on the right-hand side of (B.6) by
Full $\mathcal{O}(\alpha)$ electroweak radiative corrections to $e^+e^- \rightarrow t \bar{t} \gamma$ with GRACE-Loop<|sep|>We have presented the full O(α) electroweak radiative corrections to the process e+e− → t¯tγ and e+e− → t¯t at ILC. The calculations were done with the help of the GRACE-Loop system. GRACE-Loop have implemented a generalised non-linear gauge ﬁxing condition which includes ﬁve gauge parameters. With the UV, IR ﬁniteness and gauge parameters inde
Variance Based Algorithm for Grouped-Subcarrier Allocation in OFDMA Wireless Systems<|sep|>A variance based adaptive grouped subcarrier allocation algorithm is proposed for the enhancement of the system sum capacity in multiuser OFDMA packet access systems. Compared to other algorithms, the proposed allocation algorithm reduces efﬁciently the complexity while adaptively enhance the system capacity under proportional fairness constraints.
Y(9.46 GeV) and the gluon discovery (a critical recollection of PLUTO results)<|sep|>likely to be a bound state of a new heavy quark-antiquark pair   ̅ as confirmed by measuring the  excitation curve and the partial and total widths (PLUTO at DORIS and other experiments, see Tab.   1). The opening of the threshold for bottom quark production was confirmed by PLUTO at DORIS  by measuring the relative increase of <S> and <1-T> just above the  excitation region, showing  the development of a step in R above the  energy (the increased value being confirmed also at  PETRA). expected by QCD, was discovered by PLUTO at DORIS with the data collected in the spring of 1978.  The favourable results (initially, using only reconstructed charged tracks giving larger systematic  errors for the topological variables; later using also neutrals) were interpreted as 3 gluons,  because of the 3-jet topology, of the slightly larger multiplicity and of the convincing agreement  with the 3-gluon dynamics. The 3-jet topology was shown first independently by event <pT>,  <pout>, momentum distribution (see Chap. 5.5 and Tab. 4) and by average values of topological  variables (see Chap. 5.2 and Tab. 3), especially first by sphericity (see Fig 3), and later by detailed  comparison of differential distributions with models (see Chap. 5.4); it could not come from 3  quarks because  is a neutral boson, like the virtual photon from e+e- annihilation, nor from the    ̅  having too little energy left for hard bremsstrahlung as a gluon jet in such a large fraction of  events. Moreover, the  γgg decay fraction was expected to be only about 3%. appearance of one energetic jet in the final state (of <E1> = 4.2 GeV, as a quark jet at 8.4 GeV  c.m.s. energy), determining in every event the thrust or triplicity axis direction (Fig. 8). It  manifested itself in PLUTO also as an event <pT> of 0.34±0.01 GeV, the same as that of 2 quark jets  in the continuum of 0.33±0.01 GeV, both are uncorrected but measured in the same way with  large acceptance (Fig. 11 and Tab. 4). This “jettiness” (helped by the well separated second  energetic jet of <E2> = 3.4 GeV) was the peculiar feature found experimentally without use of  models that allowed PLUTO to give a physical meaning to the thrust axis and to see the correlation  with the beam axis (Fig. 10) and to find out that gluons at that energy fragment very similar to  quarks and therefore, of course, in a “jetlike” fashion (Tab. 4). Without those jets (modelled with  independent quark fragmentation found experimentally) the PLUTO data would have widely  disagreed with any model. continuum in a model independent way by looking at event characteristics like jettiness (<pT>),  more than 2-jets (momentum slope and <S>) and flatness (<pout>), was confirmed by comparing  the differential distribution of topological variables constructed using all the detected particles  and averaged over the plenty of events, with the expectations of different models. More than 15  different variables (including dynamic ones) confirmed the decay topology and the spin-parity 1— of the gluons already in the first half of 1979. As said by Koller and Walsh [9-11+: “If the 3-gluon jet  decay of a heavy   ̅ state is found, it will in our opinion provide a striking confirmation of QCD”.  As J. Ellis anticipated [137+: “topological variables as antenna patterns could be … used to extract  statistical evidence for gluon radiation, even if individual 3-jet events could not be distinguished”.  This is what PLUTO did using the   3-gluon decay (with gluons having a hard energy spectrum,  contrary to that of gluon  bremsstrahlung). density distributions were measured and agreed quantitatively with the 3-gluon hypothesis and  the spin 1— of the original parton of the jets was demonstrated. All other possible model  explanations were rejected and an important confirmation of QCD was provided; the forming and  hadronization of “identified” gluon jets were observed and studied for the first time. 1978, at a factor two (and then up to four) higher c.m.s. energies. At PETRA, PLUTO was the only  experiment which had analyzed data from 3 GeV (at DORIS) to 32 GeV c.m.s. energy with the same  detector, at the J/ψ and  and in the extensive e+e- continuum.  The advantage of being ready  (with a well known detector) to operate from the first day allowed PLUTO to find the first hadronic  events (November 1978) and to achieve a study of the jets at PETRA in February 1979 [35] and  contributed to minimizing the systematic uncertainties. As TASSO did in June 1979 [45,46], PLUTO  [49] provided further evidence for gluon emission inclusively and quantitatively in all hadronic  events, by confirming the jet broadening due to gluon radiation as expected by QCD (plus  hadronization). The exclusive 3-jet topology was then found in a fraction of events by scanning  (Fig. 17); the much less frequent (compared to the 3-gluon decays) visual topology found in a  few events being interpreted as   ̅ + hard gluon radiation. PLUTO used from the beginning both  charged and neutral particles, as did MARK-J (but without a magnetic field) and a month later  JADE. The TASSO results used only charged particles.  The earlier results of PLUTO at DORIS were  also a stimulus to the new experiments to search for the 3-jet topology in the larger phase space  and easier kinematics of the higher energy PETRA machine. Jets (in analogy to molecules15) were  directly visible here in a fraction of selected events. PLUTO had observed the effect of gluon jets  one year earlier, statistically and quantitatively in direct hadronic decays. and publications in the years 1978 and first half of 1979 (and later, for completeness) on the  (9.46) to 3-jet final state interpreted as 3 gluon jets. We have demonstrated that the 3-gluon MC  was able to describe all possible inclusive variables as well as the proposed parton dynamics even  after hadronization (Fig. 8 mostly and Figs. 4,6,9,10,12) and explored then for the first time the   hadronization of the “identified” gluons exhibiting jet features (Fig. 11 and Tab. 4), and all  together, as expected by QCD, a larger multiplicity. Agreement with the expected QCD matrix  element and the gluon 1— spin-parity was demonstrated as well. We have shown that no other  reasonable model was (and is) able to describe the data (Figs. 4,5,6,10). other experiments, some with significantly better detectors. Also the PLUTO MC model for gluon   hadronization was confirmed a posteriori (Fig. 14). At PETRA, the observation of jet broadening,  the evidence of gluon bremsstrahlung found by 4 parallel experiments (including PLUTO) and the  measurement of the gluon spin confirmed the existence and properties of QCD gluons that PLUTO  had already put in evidence at lower energy in  decays. 15 As Perrin confirmed the existence of invisible molecules by the brownian motion of visible grains hit by a  large number of invisible molecules and by the Einstein theory of it (an analogy suggested by S. Brandt  (PLUTO, TASSO) in episode 94 in his recent historical book [6]). Acknowledgements. We thank the PLUTO members Hinrich Meyer for his support and  encouragement and for emphasizing the importance of detection of the neutral particles, Gerhard  Knies for recalling the quasi-two-jets event topology, Claus Grupen and Robin Devenish for the  carefully reading of the first draft (DESY report 10-130). We thank Ahmed Ali and Mario Greco for  reading the first draft and for discussions on various aspects of the  decay, QCD and jet models.  The PLUTO Collaboration is warmly acknowledged for the results. quark and gluon jets and its impact on QCD” *140+, Ali and Kramer deal also with our subject,  (9.46) and the gluon discovery, within the Chapters 1 “Introduction” (see page 247), 4 “Gluon  jets in  decays” (see page 264) and 8 “Summary” (see page 311). Their evaluation is summarized  at page 311: “A clear three-jet topology using en vogue jet definitions was not established in  (9.46) decays for lack of energy [24].” Actually in the present paper we advocate the three-jet  topology both by comparing many differential distributions of topological variables with the threegluon MC expectations and by comparing average event variables on resonance with the same  ones in the continuum. (1s) was expected by QCD to decay into 3 gluons hadronizing in jets of  average energy <E1>=4.2, <E2>=3.4 and <E3>=1.8 GeV, compared to PLUTO experimental values  4.08±0.01, 3.38±0.02 and 2.00±0.02 GeV, respectively [97]. It must be recalled that jets were  observed at SPEAR at 6.2 and 7.4 GeV c.m.s. energies [65] (later even extended down to 4.8 GeV  c.m.s. energy [66,67]), corresponding to half of it for single jets (2.4, 3.1 and 3.7 GeV): only <E3>  is  smaller than those, but it is fixed just by energy and momentum conservation. SPEAR studied  sphericity (as PLUTO did later) and was credited for having discovered quark jets. There was then  no lack of energy at (9.46), exhibiting jets in the same range of energy as SPEAR or larger, and  indeed Ali and Kramer recognize that the jets were measurable at page 264 of their text : “Average  energies of the three partons were measured as  <E1>4.1 GeV for the most energetic of the three  gluons, with the other two having energies <E2>3.4 and <E3>2.0 GeV, respectively [111], in  approximate accord with the lowest order QCD matrix elements.” Also the three angles between  the jets were measured to be in accord (Fig. 9). A parton can be observed and measured only as a  jet of hadrons, so that the previous quoted sentence is referred necessarily to the 3 gluon jets.  They continue: “However, only the fastest of the three partons yielded a collimated jet of hadrons  and its detailed phenomenological profile was studied by PLUTO [111,112].” Ali and Kramer admit  here implicitly that PLUTO has identified for the first time single gluon jets. [85] corresponds to the  present paper; check our Chapter 5.4: ”The exclusive three gluon dynamics.”. If PLUTO had  demonstrated the  main decay to be in approximate accord with QCD and identified at least a  single hadron jet per event in a wide range of energies (Fig. 9, x1), with the event <pT> as small as  for quark jet events measured by the same experiment, and if this fastest jet was found by PLUTO  to have the gluon spin and the events had a larger charged multiplicity than the corresponding  experimental quark jets, everyone can ask himself what could be the remaining objects, measured  to have energies (and angles) “in approximate accord with the lowest order QCD matrix elements”  for 3 gluons, in case PLUTO had indeed not observed the three-jet topology and found the three  jet axes?  The only remaining conclusion is that we have observed three gluon jets. accord with the assumption that the  decays via a three gluon intermediate state. This is in strong  support of QCD.” G. Alexander, J. Bürger, L. Criegee, H.C. Dehne, K. Derikum, R. Devenish, G. Flügge, G. Franke, Ch.  Gerke, E. Hackmack, P. Harms, G. Horlitz, Th. Kahl, G. Knies, E. Lehman, B. Neumann, B. Stella, R.L.  Thompson, U. Timm, P. Waloschek, G.G. Winter, S. Wolff, and W. Zimmermann (DESY); O. Achterberg, V. Blobel, L. Boesten, H. Daumann, A.F. Garfinkel, H. Kapitza, B. Koppitz, W.  Lührsen, R. Maschuw, H. Spitzer, R. van Staa, and G. Wetjen (Hamburg II Inst.);
Temperature dependence of low-energy phonons in magnetic nonsuperconducting TbNi2B2C<|sep|>Strong softening of two low-energy phonon branches was observed by neutron scattering in superconducting RNi2B2C single crystals with R = Lu, Y, and Er [12– 15]. Point-contact spectroscopy revealed strong electronphonon interaction in the superconducting RNi2B2C compounds with R = Y and Ho, in contrast to the nonsuperconducting LaNi2B2C [22], which suggests that the presence of superconductivity is controlled by the strength of electron-phonon coupling as opposed to magnetic pair breaking. However, our inelastic neutron scattering experiments on nonsuperconducting TbNi2B2C clearly demonstrate strong electron-phonon coupling in this compound. Thus we conclude that superconductivity in TbNi2B2C is absent due to magnetic pair breaking. Previous studies of the competition between magnetism and superconductivity in RNi2B2C superconductors with the magnetic rare-earth elements R = Lu, Tm, and Er revealed a very weak coupling between the rare-earth magnetic moments and the conduction electrons due to a small conduction-electron density at the rare-earth site [1]. In contrast, a strong magnetic pairbreaking eﬀect has been observed in Dy and Tb samples [1] which gives additional evidence that superconductivity in magnetic TbNi2B2C is indeed destroyed by the substantially strong interaction between the local magnetic moments and the conduction electrons. In summary, electron-phonon coupling in magnetic nonsuperconducting TbNi2B2C is strong enough to mediate superconductivity. This implies that its absence can only result from magnetic pair breaking.
Semantic Hierarchy Preserving Deep Hashing for Large-scale Image Retrieval<|sep|>We propose a novel deep hashing model towards fully utilizing the hierarchy structure of the semantic information. Our method is based on a multi-level Gaussian loss function, and it takes the advantages of class-level similarity learning and full-level hierarchy labels in training. Experiments on two hierarchical datasets show that our method not only helps improve the ﬁne-level retrieval performance but also results in state-of-the-art results regarding hierarchical retrieval.
Sector coupling via hydrogen to lower the cost of energy system decarbonization<|sep|>The interest in H2 for decarbonizing energy systems is unquestionably increasing, in part driven by declining technology costs, greater policy emphasis on decarbonizing non-electric end-uses, and recognition of the limitations of direct electriﬁcation in certain applications. The unique versatility of H2 as an energy carrier and its multiple uses, however, require a holistic view to accurately explore its role in future low-carbon energy systems and the accompanying technology pathways. Additionally, such a view could demonstrate the relative economic and environmental merits of H2 and electricity use for various end-uses as well as their complementarity as vectors for decarbonizing the energy system. To this end, we developed a generalized framework for cost-optimal energy infrastructure investment and operations for decarbonizing multiple end-use sectors based on coordinated use of electric Figure 7: The amounts of transported H2 per year via different transport modes under different pipeline cost scenarios and dominating H2 generation modes. The FCEV penetration is 20%. LH2: liquid H2; GH2: Gaseous H2. ity and H2 supply chains to manage spatio-temporal variations in renewable energy inputs and energy demands. This modelling approach provides numerous insights on the technological make-ups of these energy supply chains, spanning production, transport, storage and end-use, and their impacts on the cost of decarbonization, as highlighted via the U.S Northeast case study. First, in the coupled energy system, CCS is deployed at lower carbon prices in the H2 sector than the power sector, which can be interpreted as CCS being more competitive in the H2 supply chain than the power supply chain. This conclusion, however, goes counter to the observation that six times more CCS projects will be online before this decade in the power sector than for H2 production48. For regions like Europe, where decarbonization via H2 is part of the government’s decarbonization roadmap, our study highlights the importance of prioritizing CCS deployment for H2 production. Second, power and H2 sector coupling via ﬂexible electrolysis and H2 storage enables increased VRE penetration in the power sector, thereby reducing the need for alternative ﬂexible resources for managing VRE variability (e.g. gas generation, battery storage, etc.) and in turn reducing total system cost. Moreover, as opposed to other power-sector focused studies that emphasize H2’s value as a grid-scale storage resource6–8, our multi-sector view highlights the greater system value of P2G as a ﬂexible demand resource that avoids the additional efﬁciency losses and capital cost incurred with P2G2P pathways. This conclusion is found to be robust to future expectations on the capital costs of electrolyzer and G2P systems. Since electrolyzers and H2 storage are commercially available, this ﬁnding also suggests that H2 playing a role for grid balancing could be sooner than the full P2G2P routes becoming cost-effective. Third, as compared to the independent optimization of each supply chain, we ﬁnd that sectorcoupling via P2G, reduces the cost of energy system decarbonization and that this beneﬁt grows as the demand for H2 in other end-use sectors increases. Realizing the beneﬁts of such cross-sector coordination, however, calls for policy and market reforms. For example, H2 prices need to be settled at similar spatiotemporal resolution as electricity prices, to provide incentives and signals for H2 infrastructure owners, and electrolyzers should be allowed to provide ancillary services to power systems. Moreover, both integrated operation and planning of power and H2 sectors, through a shared independent system operator, could help fully exploit the sector-coupling beneﬁts. Finally, we found that the choice of H2 transport infrastructure is intricately dependent on the choice of H2 production infrastructure, with pipelines more synergistic with natural gas based production pathways because of the matching scale of production and transmission capacity of the two assets. There are a number of areas for future work that can build on this analysis. While this study has focused on H2 used in transportation, greater H2 demand might be realized from decarbonizing heating and industrial sectors, such as ammonia production and steel manufacturing. Those H2 demands will have different temporal proﬁles and ﬂexibility compared to FCEV charging, and thus may affect the H2 supply-demand balance in different ways. Accounting for these heterogeneous H2 demands should be further explored. Investigating the last-mile delivery of H2 is out of scope of this study but is deﬁnitely a key area for future work. The last-mile H2 distribution network will affect the total number of gas trucks needed for H2 transport in the system. Increased trafﬁc congestion and safety considerations of a very large H2 truck ﬂeet may limit the deployment of trucks and increase the values of pipeline based transport, and further analysis in conjunction with trafﬁc simulations is needed to understand this aspect. Regional factors, including resource availability and demand level, could signiﬁcantly affect the optimal technology portfolios and the costs in both power and H2 sectors. For example, while natural gas supply is abundant in the studied U.S. Northeast region, it may be insufﬁcient or expensive for many regions in the world, leading to higher shares of VRE, electrolytic H2 production, and storage. The ability to cost-effectively store H2 at various time scales and capacities is a critical factor in determining the optimal system architecture. Underground caverns, where available, can provide cheap H2 storage for VRE and electrolysis deployment. The competing role of CCS with H2 storage in underground resources also need to be considered for relevant regions. This work was partially supported by Shell New Energies Research and Technology, Amsterdam, Netherlands, and the Low-Carbon Energy Centers on Electric Power Systems and Carbon Capture Sequestration and Utilization at MIT Energy Initiative. We would like to thank Dr. Joe Powell, Dr. Mark Klokkenburg, and Prof. Robert Armstrong for their valuable advice on this work. We acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing resources that have contributed to the research results in this paper.
Demand forecasting techniques for build-to-order lean manufacturing supply chains<|sep|>This work introduced Diagonal Feeding, a technique specially useful for forecasting Build-To-Order products. It helps improving accuracy whenever future delivery dates are known. This approach does not require domain knowledge, extensive feature engineering or advanced technical skills. The results from multiple experiments show that there is no go-to technique for time series prediction. In addition, this research made available a highly-relevant and novel data set. A challenge in developing methods for demand forecasting of BTO products is the lack of public data sets. As a further line of work, it is worth exploring the impact on Diagonal Feeding of transforming the target variable, as it was shown, certain transformations perform better than others. From an algorithmic point of view the approach can be strengthened with non-parametric pre-processing techniques to ﬁlter out anomalies such as in [29], [26], [16], [3], including multichannel anomaly detection, [9], performing online aggregation of diﬀerent forecasting models via long-term aggregation strategies, [20], along with approaches to model quasi-periodic data, [6], and extraction of trends in the presence of non-stationary noise with long tails, [4], [5]. Given the potential of BTO supply chains, it is expected that an increasing number of researchers will direct their attention to this area and add additional entries to the literature. The research in section 2 was partially supported by the Russian Foundation for Basic Research grant 16-29-09649 oﬁ m. The research presented in other sections was supported by the Mexican National Council for Science and Technology (CONACYT), 2018-000009-01EXTF-00154.
Deeply Exploit Depth Information for Object Detection<|sep|>In this paper, we addressed the problem of deeply exploiting the deep information for RGB-D object detection, and proposed a novel framework. Speciﬁcally, we ﬁrst derived more properties by mining the provided RGB and depth data. Particularly, several properties that could be directly derived from the color/depth or pairs were adopted here, which included the geometry contour, horizontal disparity, height above ground, and angle with gravity. Then we systematically investigated the fusion schemes of different properties under the CNN model. By the means of analysis and evaluation, it was recommended that the features encoding the different object properties should be learned independently and fused at the highest level, i.e. not joint until passing into the classiﬁer. Finally, we experimentally veriﬁed the effectiveness of the proposed method, which indeed achieved state-of-the-art performance on NYUD2. And it was gained with no data augmentation or region features. Besides, we only considered the properties that could be computed in relatively straightforward methods. Exploring more useful properties is one of our future works, e.g., equipping more powerful sensors or developing more advanced algorithms for property derivation. Table 4. Test set performance on NYUD2 for RGB-D object detection. We compare the results of our ﬁnal system with the three existing remarkable methods: RGBD DPM, RGB+D CNN, RGB+DHA CNN. The ﬁnetuning set for CNN is shown in the Column 2 and the SVM is trained on the trainval set. All the results are AP b in percentage. See Section. 4.3
Fast B-spline Curve Fitting by L-BFGS<|sep|>In this paper, we propose a new curve ﬁtting method based on the L-BFGS optimization technique. The unique features of this algorithm are that it does not need to perform the time-consuming foot point projection in every iteration as in traditional approaches and that it does not need to formulate and solve a linear system of equations in every iteration; instead, it uses only eﬃcient vector multiplications. As a result, this new method is much faster than other traditional methods, as demonstrated by a number of experimental results presented. In the future we will extend this method to solving the B-spline surface ﬁtting problem, for which we expect even more signiﬁcant improvements over the existing methods because of the large number of data points as well as the large number of control points involved in surface ﬁtting. Figure 8: A Chinese character with 30 control points and 600 data points which means "mountain". The coeﬃcients of fairing term are α = 5 · 10−4 and β = 0. Figure 9: Flame with 66 control points and 2000 data points. The coeﬃcients of fairing term are α = 10−3 and β = 10−2.
ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data<|sep|>The simple nature of the RNCM, and its adherence to a few statistical assumptions, makes it more appealing for the researchers. Therefore, it has become the prominent approach in social sciences, biostatistics, political science, economics and other disciplines. Many toolkits have been developed for performing casual inference ´a la this framework that depends on statistical software such as SAS, SPSS, or R project. However, these toolkits do not scale to large datasets. This work introduce ZaliQL, a SQL-based framework for drawing causal inference that circumvents the scalability issue with the existing tools. ZaliQL supports state-of-the-art methods for causal inference and runs at scale within a database engine. Causality has been studied extensively in databases [20,21,29,32,31,34,33]. We note that this line of work is diﬀerent than the present paper in the sense that, it aims to identify causes for an observed output of a data transformation. While these works share some aspects of the notion of causality as studied in this paper, the problems that they address are fundamentally diﬀerent.
PGPE theory of finite temperature collective modes for a trapped Bose gas<|sep|>We have presented a comprehensive study of the excitation spectrum of a Bose cloud at ﬁnite temperature by modelling the experiment of Jin et al. [2] with the PGPE formalism. Our results for mode frequencies are in good agreement with experiment and other theories up to about 0.65Tc. At temperatures above this our theory continues to provide a good description of the m = 2 condensate mode. Currently our theory fails to predict the sudden increase in the frequency of the m = 0 condensate mode at temperatures above 0.65Tc. The origin of this failure in the current formalism is that we only provide a dynamical description for the portion of the noncondensate in the C region. We have also examined the dependence of PGPE results on energy cutoff used to deﬁne the C region. Importantly, we demonstrated the insensitivity of the equilibrium predictions to energy cutoff. The study of cutoff dependence in the collective mode results clearly reveals the importance of the interplay between condensate and noncondensate components in the mode behavior, and suggests a new practical validity check for the PGPE theory: Dynamical predictions (in the absence of a dynamical theory for the I region) should be veriﬁed to be independent of the cutoff energy. The results of this ﬁrst study with the PGPE give us great conﬁdence that this theory is capable of providing a full description of the JILA experiments. To do this would require us to implement a dynamical description of the I region, which we are currently pursuing. AB acknowledges support of a TEC Top Achiever Doctoral Grant. PBB wishes to acknowledge useful discussions with A. S. Bradley, M. J. Davis, and D. A. W. Hutchinson. This work was supported by the New Zealand Foundation for Research, Science and Technology under Contract Nos. NERFUOOX0703. [1] D. S. Jin, J. R. Ensher, M. R. Matthews, C. E. Wieman, and E. A. Cornell, Phys. Rev. Lett. 77, 420 (1996). [2] D. S. Jin, M. R. Matthews, J. R. Ensher, C. E. Wieman, and E. A. Cornell, Phys. Rev. Lett. 78, 764 (1997). [3] M.-O. Mewes, M. R. Andrews, N. J. van Druten, D. M. Kurn, D. S. Durfee, C. G. Townsend, and W. Ketterle, Phys. Rev. Lett.
Tale of tails using rule augmented sequence labeling for event extraction<|sep|>In this paper, we introduce a hybrid approach for automatic extraction of events and arguments. We present a new dataset in the disaster domain for ﬁve languages consisting of large number of tags than usual datasets. We propose several variants of rule based system to augment deep learning based models. Extensive experimental results demonstrate that our rule augmented methods outperforms deep learning based models on lesser annotated data and low resource languages. We further shows more improvement on tail labels using our approach. For future work, we plan to integrate cross linking between events and its arguments.
Nonequilibrium Green's function theory for predicting device-to-device variability<|sep|>In this work, we have developed two theoretical formalisms based on CPA and LCA to predict device-to-device variability induced by random dopant ﬂuctuation. The advantage of our theory is that statistical averaging due to RDF is carried out analytically to avoid large number of dopant conﬁguration sampling in device simulations. FIG. 12: (color online) Transmission ﬂuctuation in the tight-binding model with periodic cross section shown in the inset of (a). (a) Transmission T(E) in the clean limit. (b) Transmission ﬂuctuation δT(E) for the doping concentration x = 0.001. For comparison, δT is calculated with two methods: LCA-ﬁnite and LCA-periodic. CPA and LCA solutions are satisfactory, as shown in the comparison to the exact solution of 1D TB model. For x ⩾ 0.2, both CPA and LCA become numerically less accurate even though they still capture a rough trend of the transmission ﬂuctuation as demonstrated by the 1D TB model. In LCA we have neglected high order terms in the x-expansion, while in CPA we have neglected the crossing diagrams. These approximations limit the accuracy of the theory to the relatively low impurity concentrations. We note that for essentially all the practical semiconductor devices, the dopant concentration is well within the applicability range of our formalisms. In numerical modeling, the LCA is easier and perhaps more practical for realistic nanoelectronic devices because an explicit formula Eq.(27) is available and the computational cost is much cheaper than that of CPA. We have also implemented the LCA theory into the ﬁrst principles device modeling package NanoDsim so that ﬁrst principles analysis of device-todevice variability can now be carried out without any phenomenological parameters. Preliminary studies indicate that transmission ﬂuctuation is most pronounced in the energy regime where the number of the conducting channels varies rapidly. In addition, angular momentum states of the conducting channels also play an essential role. Since the ﬂuctuation strongly depends on the electron energy, our numerical simulation suggests that the FIG. 13: (color online) Transmission ﬂuctuation of the 3D Cu twoprobe lattice with 1% random vacancy defects. (a) Transmission ﬂuctuation δT on top of Transmission T versus energy E. The area of the unitcell cross section is 5.64 ˚ A2. (b) Total and angular momentum resolved density of states versus E. RDF induced transmission ﬂuctuation could be suppressed by engineering the bias voltage window to a proper energy regime. Finally, we have so far focused on investigating the RDF induced transmission ﬂuctuation in nanostructures, our theory and numerical implementation can be applied to study many other physical quantities such as the shot noise, the ﬂuctuation of threshold voltage, as well as the device variability in spintronics. We hope to report these and other investigations in future publications. We wish to thank Dr. Jianing Zhuang and Prof. Jian Wang for valuable discussions concerning their work in Ref.16. We thank Dr. Ferdows Zahid for bringing our attention to Ref.5 and discussions on practical device issues of RDF. In this appendix, we present how to calculate quantities Gr 0, tr iq, Ga 0, ta iq by using the CPA condition Eq.(20). As mentioned in Section II, there are some freedom to partition H into H0 and V , i.e. Eqs.(7,8). CPA takes the advantage of this freedom and chooses a special partition such that the disorder averaged scattering vanishes, i.e. Eq.(20). Assume that the Hamiltonian matrix is written as H = T + ε where T is the off-diagonal part of the Hamiltonian and ε the diagonal part. T is a deﬁnite matrix and does not have any randomness. In contrast, the diagonal matrix ε contains discrete random variables, the i-th diagonal element εi can take the value εiq with the probability xiq and � q xiq = 1. One can introduce a diagonal quantity called coherent potential ˜εr ≡ diag ([˜εr 1, ˜εr 2, · · · ]) and deﬁne H0 and V as By imposing CPA condition Eq.(20) to the above partition of H0 and V , ˜εr can be solved from the following CPA equations:               Once ˜εr is solved, Gr 0 = Gr and tr iq are also known from Eq.(A1). Finally, Ga 0 and ta iq are simply Hermitian conjugates of Gr 0 and tr iq, respectively. In this appendix, we provide an analytical proof of Eq.(23). By using the vertex correction, the left hand side of Eq.(23) can be obtained as where Λ is the vertex correction determined by Eq.(22) with X = Σra. By using the expressions of Gr and Ga in CPA, the right hand side of Eq.(23) can be transformed into a similar form as the left hand side: Comparing Eq.(B1) and Eq.(B2), it is inferred that ˜Λ and Λ must be identical. Also note that the vertex correction Eq.(22) for Λ is an inhomogeneous linear equation thus has a unique solution. Hence the identity is proved if and only if ˜Λ satisﬁes Eq.(22). By using CPA condition Eq.(A1) and its Hermitian conjugate ta iq − tr iq + tr iq(Gr i − Ga i )ta iq = � 1 + tr iqGr i � ˜Λi � 1 + Ga i ta iq � . (B4) 2 A. Asenov, IEEE Trans. Electron Deveces 45, 2505 (1998). 3 A. Brown, A. Asenov and J. Watling, IEEE Transactions on Nanotechnology 1, 195 (2002).
Behaviour of light transmission channels in random media with inhomogeneous disorder<|sep|>We have carried out a detailed numerical investigation on how the inhomogeneity of disorder inﬂuence the light transport properties and the statistics of transmission channels in 2D disordered waveguides. For waveguides with longitudinal inhomogeneity of disorder, transmission channels are not modiﬁed and the transport of light can be equivalent to that in waveguides with eﬀective homogeneous disorder. However, for waveguides with transverse inhomogeneity of disorder, the statistics of the transmission channels are considerably modiﬁed and the light transport reveals hybrid behaviours of diﬀerent regimes, which leads to the additional repulsion of large and small transmission eigenvalues and broadening of the distributions of the total transmission. The results in the present paper may promote both the theoretical and experimental investigations on light transport in more extensive disordered materials.
Determining the outcome of cosmic bubble collisions in full General Relativity<|sep|>This work establishes the groundwork for rigorously determining the connection between an underlying potential landscape giving rise to eternal inﬂation and the cosmological signal of bubble collisions. Such a goal requires the ability to study the coupled Einstein and scalar ﬁeld equations without approximations. To this end we have developed FIG. 23: Contour plots of ϕ for the collision of bubbles in the potential S1. On the top left, we simulate the collision between two identical bubbles. In the future light cone of the collision, inﬂation is completely disrupted (as can be seen by comparing the color bar to the ﬁeld values on the potential in Fig. 5). On the top right, we simulate the collision of two diﬀerent bubbles in the case where the width of the barrier T1 is adjusted to be a quarter of the width of the barrier T2. Again, inﬂation is completely disrupted by the collision. On the bottom, we simulate the collision of two diﬀerent bubbles in the case where the width of the barrier T1 is adjusted to be four times the width of the barrier T2. In this case, the inﬂaton inside the Observation bubble is hardly perturbed, and inﬂation continues to the future of the collision. In all cases, the boundaries of the simulation lie at the center of each bubble. a robust code that implements the full system of relativistic ﬁeld equations, thoroughly tested it on a number of cases and applied it to determine the outcome of collisions between bubbles. Our results highlight the utility of two complementary analytic tools for predicting the outcome of bubble collisions. For the collision of vacuum bubbles, we have conﬁrmed that the thin-wall solutions obtained from the Israel junction condition formalism approximate the numerical solutions to good accuracy. In particular, the junction condition solutions give a good quantitative prediction for the metric functions to the future of the collision, as well as a good qualitative prediction for the late-time behavior of the post-collision domain wall. The limitation is that the junction condition formalism cannot capture the dynamics of the collision event. This is particularly problematic when considering bubbles with an interior cosmology. The dynamics are, however, captured to a certain extent by the free passage approximation [18] which states that, in the immediate future of the collision, the ﬁeld proﬁles making up the two bubbles simply superpose. This illustrates that the structure of the potential can be more important for determining the outcome of a collision than the kinematics. The free passage approximation breaks down shortly after the collision, at which point the full ﬁeld dynamics must be considered. A full numerical solution is necessary to bridge the gap between the realm of validity of the free passage approximation and the junction condition formalism. Some of the main conclusions we have reached based on our numerical solutions are as follows: • Classical transitions occur in the presence of gravity, and lasting regions of higher energy density can be created through gravitational eﬀects. • The structure of the potential is the dominant factor in determining the immediate outcome of a collision. For the collision between identical bubbles, the ﬁeld is initially displaced a distance equal to the barrier width towards its minimum. For the collision of non-identical bubbles, when the potential barriers are symmetric, the collision excites a breathing mode of the post-collision domain wall. For asymmetric barriers, the post-collision domain wall forms with some initial velocity in the direction of the thicker barrier, with no breathing modes. • We have demonstrated conclusively that slow-roll inﬂation can occur to the future of a collision. Large ﬁeld models of inﬂation are more robust to collisions than small-ﬁeld models, which are completely disrupted unless the potential barriers are highly asymmetric (with the barrier T1 much thicker than the barrier T2). • The direction in which the post-collision wall accelerates is determined not only by the height of the potential at the exit from the barrier, but also by the slope of the potential as the ﬁeld tends towards the minimum. Therefore, if there is an inﬂationary potential in the Observation bubble, for the Collision bubble not to intrude, the interior of the latter is required to be either a vacuum (no cosmology) or an inﬂationary plateau that is either higher or broader than the one in the Observation bubble. • The amplitude of the perturbation to the inﬂaton is set mostly by the barrier widths in the potential, with only a mild dependence on the kinematics. In addition, for the examples we have studied, there are always fewer e-folds to the future of the collision than in the undisturbed portions of the bubble. Based on these observations, we can determine which potentials are most likely to give rise to cosmologies including bubble collisions that are compatible with our current observable universe, yet leave detectable signatures. Potentials resembling L2 in Fig. 5 are good candidates. Since this potential includes a large-ﬁeld model of inﬂation, the collision does not disrupt cosmological evolution inside the bubble. Adjusting the relative width of the two barriers changes the amplitude of the perturbation to the inﬂaton. The intrinsic amplitude of the perturbation along with the total duration of inﬂation inside the Observation bubble determines the amplitude of the signal in the CMB or other cosmological observables. While the existing code is able to accurately obtain the dynamics of the collision, it cannot yet be used for studying the ensuing cosmology: the uniform grid employed and coordinate conditions adopted are unable to track the increasingly narrow structures that develop around walls in a computationally eﬃcient manner. To address this issue, we will extend our code to adopt adaptive mesh reﬁnement (see e.g. Refs. [40, 60] for a ﬁrst example of its application in numerical relativity). We will present the derivation of the observational signatures from these simulations in a forthcoming publication. We thank Anthony Aguirre, Jim Cline, Richard Easther, Stephen Feeney, Eugene Lim, and David Neilsen for valuable input at various stages of this project. This work was partially supported by a grant from the Foundational Questions Institute (FQXi) Fund, a donor advised fund of the Silicon Valley Community Foundation on the basis of proposal FQXi-RFP3-1015 to the Foundational Questions Institute. The work was also supported by NSERC through a Discovery Grant. MCJ and HVP acknowledge the hospitality of the Benasque Science Center for Science, where this project was initiated, and UC Santa Cruz, for hospitality. MCJ thanks University College London, where part of this work was completed, for its hospitality. Research at Perimeter Institute is supported by the Government of Canada through Industry Canada and by the Province of Ontario through the Ministry of Research and Innovation. HVP is supported in part by Marie Curie grant MIRG-CT-2007-203314 from the European Commission, and by STFC and the Leverhulme Trust. LL is supported in part by CIFAR. In this appendix, we derive the general form of the equations of motion assuming the line element Eq. 9. In this appendix, we work in units where Mpl = 1. The results of this appendix are used in Sec. III to derive the gauge-ﬁxed equations of motion used in the simulations. We begin by ﬁnding the energy momentum tensor and ﬁeld equations for the scalar ﬁeld. The energy momentum tensor for the scalar ﬁeld is given by: z2a3b2 � a3 − 2z2bb′a′ + z2a � b′2 + 2bb′′�� + 4Kx xKχ χ + 2Kχ χ 2 , (A14) completely determine the equations of motion for the scalar ﬁeld ϕ. The evolution equations for the three-metric are obtained from the deﬁnition of the extrinsic curvature. The z − z and χ − χ components yield: ˙Ki j = βk∂kKi j − ∂kβiKk j + ∂jβkKi k − DiDjα + α � Ri j + KKi j + 4π(S − ρ)δi j − 8πSi j � . (A25) The system of equations Eq. 29, 30, 31 and 32 should recover the hyperbolic foliation of dS space in the case where Φ = Π = 0 and V = const. In this appendix, we work in units where Mpl = 1. The metric in hyperbolic dS is given by:
Multi-granularity Item-based Contrastive Recommendation<|sep|>In this work, we propose a novel MicRec framework, which designs three eﬀective and universal feature-level, semantic-level, and session-level item-based CL to improve the matching performances. In real-world systems, MicRec achieves signiﬁcant online and oﬄine improvements with diﬀerent matching models and datasets, and has been deployed in online. Although the proposed three itembased CL tasks seem to be straightforward, we have veriﬁed their eﬀectiveness and universality on diﬀerent scenarios and models in oﬄine and online, highlighting the under-explored but promising directions of item-based CL in practice. In the future, we will design more eﬀective item-based CL tasks, and jointly combine user-based CL with item-based CL tasks. We will also explore the universality of MicRec in ranking and better CL-based optimization strategies.
Polaritonic Rabi and Josephson Oscillations<|sep|>The former, the crossed ﬁrst order correlation function G1 ab(τ), is related to coherence between the states and suggests as an extension for steady states of σ, the relative phase in autocorrelation time ϕ ≡ arg(G(1) ab ). The latter is related to ﬂuctuations of the population imbalance in autocorrelation time. Correspondingly, the question pauses itself whether these two observables are geometrically connected or not. Using Eqs. (7) and the quantum regression theorem, the following relationship can be obtained: 4 + (δ − iΓ−)2. By also writing the equation of motion for G(2) ab (τ), it can be shown that Eqs. (27) are related through the relation: This shows that G(1) ab and G(2) ab are coupled indeed, with a possible similar Josephson interpretation that one is driving the other. However, their connection is through a two-sheet hyperboloid and is thus completely diﬀerent than the the dynamics of real-time observables, even for the transient dynamics, where variables are connected via a sphere of variable radius (the Paria sphere). Figure 10 shows the trajectory in the hyperboloid at resonance (δ = 0). In this case, the relative phase is a two-valued function, oscillating between ±π/2 (Fig. 10 (d)), which corresponds to the the oscillatory regime of the relative phase. Correspondingly, G(1) ab and G(2) ab oscillate in time with a decay toward zero for G(1) ab and N 2/4 for G(2) ab . Comparing with panel (d), it is observed that ϕ changes value whenever G(1) ab becomes zero. This shows a similar behaviour than in the real-time behaviour where whenever the relative phase becomes illdeﬁned due to one state becomings zero avoid, it changes its regime. Here, ill-deﬁned coherence changes the value of phase instead. Simultaneously, G(2) ab reaches N 2/4, which is the point of lowest possible ﬂuctuations. The corresponding trajectory on the hyperboloid in panel (c) shows a simple line (in the curved space) with Re[G(1) ab ] = 0 and G(2) ab swinging around the lowest point of the hyperboloid. The detuned case (δ ̸= 0) is shown in Fig. 11. This time, as seen in panel (d), the relative phase is running. Both G(1) ab and G(2) ab oscillate and decay toward diﬀerent steady points as compared to resonance. The trajectory on the hyperboloid shows this time an open orbit, encircling the hyperboloid G(2) ab axis without ever touching it. In contrast to the real-time dynamics on the Paria sphere, in autocorrelation time, there is a reduced phenomenology and, in particular, no preferred change of basis, e.g., there is no counterpart of a regime of drifting phase out of resonance and two-valued ±π/2 phase at resonance. This is due to the Hamiltonian dynamics being washed out by the incoherent pumping.
Multivariate Chebyshev Inequality with Estimated Mean and Variance<|sep|>We have derived a generalization of the empirical Chebyshev inequality in multiple dimensions with the only requirement that the given samples are independent and identically distributed. The Since many of the common distributions studied in both theory and practice are unimodal, an interesting improvement of this result could be to introduce the assumption of unimodality in order to derive less pessimistic bounds. Another possible extension is to investigate other norms (e.g. ∞ or 1-norm) and compare the right-hand side of the respective reformulations to understand which one is more appropriate for diﬀerent kinds of distributions. There are many possible application of this theoretical result appearing whenever the Chebyshev inequality is employed without knowing the population distribution. In particular, the inequality can be exploited to construct conﬁdence sets which can be used in several situations such as outliers detection or stochastic programs reformulations.
Iteration Complexity of Variational Quantum Algorithms<|sep|>We have analyzed the convergence guarantees of variational quantum algorithms in the presence of bias in the quantum part. Let us interpret these results. In regards to the asymptotic bias in the approximate measure of stationarity, we can notice that, predictably, the greater the bias in the function evaluation, the greater the error lower bound in the ultimate optimality criterion. A coarser discretization for estimating the gradient appears to stabilize the eﬀect of the function evaluation bias but carries the trade-oﬀ in introducing additional asymptotic optimality bias in the overall rougher accuracy in gradient estimation. In addition, as typical for zero-order methods, the asymptotic bias scales poorly as a function of the problem dimension. With respect to the iteration complexity, we can see that the overall rate is not aﬀected by the presence of bias. The rate of convergence is the same as in a standard stochastic gradient analysis. That said, for the two terms in the convergence guarantee that do drop to zero with more iterations, exhibit a larger constant with greater bias. More broadly, this is a small step towards a better understanding of the performance of variational quantum algorithms. Until recently, these have been seen as heuristics with no guarantees whatsoever. This and recent related work [19, e.g.] suggests that their performance can be characterized in some detail. Acknowledgement This work is a collaboration between Fidelity Center for Applied Technology, Fidelity Labs, LLC., and the Czech Technical University in Prague.
Searching for Intermediate Mass Black Holes in the Milky Way's galactic halo<|sep|>The detection of periodic and non-periodic variable sources has allowed conﬁrming the goodness of the developed pipeline. In particular, the two years time window, the sampling and the eﬃciency of the DECAM instrument have shown the possibility to detect microlensing events. So far, many known variable sources have been identiﬁed in four DECAM ﬁelds of view - toward LMC - and some unknown variables are waiting to be studied more deeply. A few possible microlensing event candidates (e.g. that shown in Fig. 2), which wait for conﬁrmation, have been also found in the data. The contents presented in this paper will be discussed in more details in the the paper “Searching for galactic halo Intermediate Mass Black Holes through gravitational microlensing”, by A. Franco et al., currently under preparation. We acknowledge the support by the Euclid and TAsP (Theoretical Astroparticle Physics) projects of the Istituto Nazionale di Fisica Nucleare (INFN).
Interaction between magnetic vortex cores in a pair of nonidentical nanodisks<|sep|>In this paper we have studied the interaction between pairs of magnetic nanodisks of diﬀerent diameters and vortex ground state; from an ensemble of magnetic nanodisks with a gaussian distribution of diameters, we created ﬁfty pairs of nanodisks. In this study we have a) derived analytically the expressions of the coupling integrals Ix and Iy that describe this interaction; b) from the time dependent magnetizations derived from the numerical solution of Thiele’s equation we applied the vortex echo method17 to derive the dependence of the interaction with distance; c) we made a micromagnetic simulation to obtain M(t) and again applied the echo method to evaluate the strength of the interaction between the disks. We have also obtained the variation with distance between the disks, of the coupling frequencies, derived from Thiele’s equation. The coupling integrals Ix and Iy vary depending on distance in a way comparable to the results obtained by other authors. The relaxation times T ∗ 2 , that also measure the interaction strength, derived using two methods based on the magnetic vortex echoes, are comparable, diﬀering by about 10% (Fig. 5). The ﬁtting to the T ∗ 2 curves obtained from these two techniques show an approximate dependence of the form ∝ d−n, with values of n that vary between 5.2 ± 0.2 (micromagnetic simulation) and 3.7 ± 0.2 (Thiele’s equation), comparable to
On travelling wave solutions of a model of a liquid film flowing down a fibre<|sep|>The main contribution of this paper is showing the existence and long-time behaviour of non-negative weak solutions for the generalised nonlinear PDE (2.1)—(2.3) using a priori estimates for energy-entropy functionals. Typical numerical studies of the energy functional minimizers and dynamic simulations of the PDE with and without gravitational eﬀects are presented in support of the analytical results. The travelling wave solutions of the model are investigated both analytically and numerically. As t → ∞, with proper system parameters and initial conditions, the solution to (2.1) converges to a travelling wave solution characterized by (4.1). H. Ji was supported by the Simons Foundation Math+X investigator award number 510776. The authors are also grateful to Prof. Andrea L. Bertozzi for helpful discussions.
Analyzing Whole-Body Pose Transitions in Multi-Contact Motions<|sep|>We have presented an analysis of support poses of more than 100 motion recordings showing different locomotion and manipulation tasks. Our method allowed us to retrieve the sequence of used support poses and the time spent in each of them, providing segmented representations of multicontact motions. Although the most common pose transitions are the ones involved in walking, we have shown that the 1Foot-1Hand and the 2Foot-1Hand poses also play a crucial role in multicontacts motions. We have classiﬁed our data into short and long locomotion transitions and transitions for supporting a task, depending on the time spent on them. We have observed that very short locomotion transitions are found in clusters that can be grouped as complex transitions with more than one contact change. The data-driven generated taxonomy validates the transitions proposed in our previous work. We believe that our motions segmented by support poses and time spent per transition provides a meaningful semantic representation of a motion. This work opens the door to many exciting future directions. First, we are interested in analyzing our motion representations to ﬁnd semantic rules that can help deﬁne new motions for different situations, with the objective of building a grammar of motion poses. Storing each transition as motion primitives, we are also interested in performing path planning at a semantic level based on support poses. Finally, we are still assuming very simpliﬁed poses that do not consider directions of support, represented by simple sketch ﬁgures. However, for each class of poses there is an inﬁnite number of possible body conﬁgurations depending on location and orientation of contacts. Future work directions include ﬁnding the most relevant whole-body eigen-grasps, that is, we will perform principal component analysis to reduce the dimensionality of the space that can realize each pose. In conclusion, this work presents a step further in the comprehension of how humans can utilize their bodies to enhance stability for locomotion and manipulation tasks.
Thick smectic shells<|sep|>In this paper we presented a phenomenological approach to treat smectic ﬁlms conﬁned between two spherical surfaces. Within the continuum Frank free energy for liquid crystals and the assumptions made, we have shown that the ground state of two-dimensional ﬁlms with director aligned along geodesics of a sphere, becomes unstable for thick ﬁlms when K3 > RWa. The instability is driven by ﬁnite curvature of the sphere 1/R and the requirement of the bend-free 7 conﬁguration in the vicinity of the nematic–smectic phase transition when K3 diverges. The resulting geometric frustration can be relieved by the tilt of the director out of the tangent plane of a sphere, as well as by in-plane undulations. The constructed simpliﬁed solution for the plausible organization of smectic layers allows to identify the onset of the instability towards periodic chevron-like texture and provides the dependence of the critical wavelength on the critical thickness ε = h/R of smectic shells. Future comparison between theory and experimental data [1, 2, 3] can be useful to extract the values of elastic constant K3 and the anchoring strength Wa and eventually propose the equilibrium 3D organizations of smectic layers within thick spherical shells. This issue is dedicated to Professor Martine Ben Amar. The present paper, in particular, would never have been possible without Martine who encouraged one of the authors (OVM) to study pattern formation in liquid crystals. The approaches in this paper are inspired by Martine’s enthusiasm for exact solutions, complex analysis and perturbation theory. Her high scientiﬁc standards and the elegant way to tackle the most intricate and interesting scientiﬁc problems are highly appreciated. The authors acknowledge ﬁnancial support from the Soft Matter Program of Syracuse University.
Contour Detection Using Contrast Formulas in the Framework of Logarithmic Models<|sep|>The algebraical model of logarithmic type  worked out in [1,2] allowed us to define some  contrast formulas. These were used in contour  operation of images. This contour procedure  proves to be more efficient than the classical  methods (that is those which use classical  operations), as it was shown by all the  examples.     The next step involves an establishment of  some quantitative evaluation criterions of the  contour quality.
Assessing the Performance of Leja and Clenshaw-Curtis Collocation for Computational Electromagnetics with Random Input Data<|sep|>In this work we have employed the stochastic collocation method for UQ in EMF problems with bounded random inputs. Both uniform and non-uniform random inputs have been considered. In the latter case, we have employed positively skewed beta distributions as a typical example. A dimension-adaptive algorithm based on nested univariate collocation points has been employed in all multivariate cases. Two families of nested collocation points, provided by the Clenshaw-Curtis and Leja rules, have been examined in terms of interpolation and quadrature accuracy. In the case of uniform input distributions, the Leja rule is found to be advantageous compared to the Clenshaw-Curtis rule in terms of interpolation accuracy, considering an analytical, academic model with a very smooth input-to-output map. For the same model, the Clenshaw-Curtis rule outperforms the Leja rule in terms of quadrature accuracy, as measured in statistical moment computations. No obvious advantages can be observed for either choice of collocation points in the numerical studies concerning the real-world EMF application, where both rules can be regarded as comparable. Interesting results are reported in the case of beta-distributed inputs. In the univariate case, the Clenshaw-Curtis rule is found to be advantageous in terms of worst-case performance, but inferior in terms of average performance. A similar result is obtained when comparing unweighted and weighted Leja rules. This result indicates that surrogate models based on uniform input distributions should probably be preferred if worst-case performance is of main interest. However, the same result is not observed in the multivariate case, where the weighted Leja rule outperforms both the ClenshawCurtis and the unweighted Leja rules in all metrics, i.e. in terms of both average and worst-case performance. Finally, considering moment estimations, the weighted Leja rule is found to outperform the Clenshaw-Curtis rule with modiﬁed weights in all occasions, i.e. for all considered moments and in both univariate and multivariate cases. Based on the results of the numerical experiments presented in this paper, we may conclude that Leja rules present a reliable choice of nested collocation points, producing accurate results both in the context of interpolation and quadrature. The versatility of Leja points allows the rule to be employed for arbitrary input PDFs, however, its performance against competitive rules in a wider variety of cases should be further examined. An extension of this work should consider cases where dedicated nested collocation points exist, e.g. truncated normal [8] or other distributions. All authors would like to thank our colleague, Andreas Pels, for providing the Stern-Gerlach magnet model. We would also like to thank the two anonymous reviewers for their comments and suggestions. The ﬁrst and third author would like to acknowledge the support of the Graduate School of Computational Engineering, Technische Universit¨at Darmstadt.
Asteroseismology of solar-type stars<|sep|>In this work we have reviewed the theory behind the asteroseismic techniques applied to study main-sequence solar-like dwarfs, as well as the latest results obtained from ground-based and space-borne missions such as CoRoT, Kepler, K2 and TESS. This is a young research topic that has reached its constant pace during the last decade. It has been shown that asteroseismology of MS solar-like stars is providing strong constraints about the structure and dynamics from the surface to the core of stars. It is possible today to infer masses, radii and ages with a precision and accuracy never reached before. This is impacting many ﬁelds in stellar astronomy. For example, precise and accurate ages of ﬁeld stars at all stages of evolution in the main sequence have shown that stars seem to stop braking when they reach a given Rossby number and from there, they seem to continue with a quite constant surface rotation (van Saders et al., 2016). This could be the consequence of a change in the properties of external magnetism, which could also impact stellar magnetic cycles (Metcalfe and van Saders, 2017) and the possibility of giving a correct age to middle aged stars through gyrochronology. An extended sample of stars and complementary ground-based observations of the magnetic activity will be necessary to progress in this area. Asteroseismology is also helping to improve exoplanet research (for a full review on the synergies between asteroseismology and exoplanetary science see Huber, 2018). For example, precise stellar ages are a key parameter to date the full planetary systems and thus, better understand the theory of formation and evolution of planet-hosting stars and the extrasolar planet systems as a whole. The asteroseismic improvement in the determination of stellar radius is directly impacting the precision of planet radius extracted with the transit method. Thanks to this new increased precision, it is now possible to properly characterize the so-called “radius valley” or “photoevaporation desert” at around 2 R⊕ (Lundkvist et al., 2016; Van Eylen et al., 2018a). Finally we mention that the combination of transit photometry with asteroseismology allows a systematic measurement of orbital eccentricities of transiting planets (e.g. Van Eylen and Albrecht, 2015), which was only possible before in relatively large gas-giant planets, or for multiplanet systems where the eﬀects of eccentricities and masses could be successfully distinguished, New internal rotation proﬁles have encouraged stellar astrophysicists to study angular momentum transport and eﬃcient mixing processes and develop new mechanisms explaining the quasi-uniform rotation found in the outer part of the radiative zone, the external convective zone and the surface. In conclusion, asteroseismology of solar-type stars is in very good shape. At the time of writing these conclusions, the community is actively analyzing K2 data, where dozens of new pulsators are foreseen, as well as the ﬁrst two sectors of TESS data that are already available. New missions will contribute to enhance our known sample of pulsating star. The future is already here because of the work engaged to prepare the ESA’s M3 PLATO mission, which will be able to characterize tens of thousand of these MS cool dwarfs after 2026 for which many of them will be planet hosts. Asteroseismology of MS solar-like dwarfs is just at the dawn of its potential.
Strong Optimistic Solving for Dynamic Symbolic Execution<|sep|>We propose strong optimistic solving and implement it in our dynamic symbolic execution tool Sydr [9]. It allows to eliminate irrelevant constraints from the path predicate based on the information about symbolic branches nesting. Additionally, symbolic constraints, which may affect the program control ﬂow, are not eliminated from the resulting predicate. The proposed method uses control dependency and call stack analysis to determine relevant branches. The strong optimistic strategy allows to discover new execution paths by inverting more symbolic branches along the single execution trace. We evaluate the method on the set of real-world applications [17] measuring the analysis efﬁciency (i.e. correctly inverted branches number, accuracy, and speed) and the code coverage. According to the results, applying the strong optimistic strategy along with the optimistic strategy [8] leads to either discovering more correct symbolic branches at the same time without great performance loss, or increasing the explored code coverage after a complete program exploration. At the same time, there is no sense in using the strong optimistic strategy alone, as in this case signiﬁcant performance and accuracy loss is detected. So, the optimal way is applying the combination of the solving strategies, depending on the corresponding queries satisﬁability.
Voronoi Density Estimator for High-Dimensional Data: Computation, Compactification and Convergence<|sep|>In this work, we deﬁned an extension of the Voronoi Density Estimator suitable for high-dimensional data, providing efﬁcient methods for approximate computation and sampling. Additionally, we proved convergence to the underlying data density. A promising line of future research lies in exploring both theory and applications of the VDE and CVDE to metric spaces beyond the Euclidean one, in particular higher-dimensional Riemannian manifolds. Spheres, for example, naturally appear in the context of normalised data, while complex projective spaces of arbitrary dimension arise as Kendall shape spaces on the plane (Mardia and Jupp 2009).
Testing the isotropy of the Universe by using the JLA compilation of type-Ia supernovae<|sep|>In this paper, we probed the possibly anisotropic expansion of the Universe by using the recently released JLA compilation of SNe Ia. We considered the dipolemodulated deviation from the isotropy in three diﬀerent dark energy models. We obtained similar constraints on the anisotropic amplitude and direction in three cases. This indicates that the preferred direction anisotropy is insensitive to the isotropic background models. Especially, our MCMC studies show that the anisotropic amplitude has an upper bound D < 1.98 × 10−3 at 95% C.L., and the dipole direction points towards (l, b) = (316◦ +107◦ −60◦) for the dipole-modulated ΛCDM model. These results imply that the there is no signiﬁcant evidence for anisotropy in the JLA dataset. For comparison, we also applied MCMC method to the Union2 dataset, and we got AD < 1.40 × 10−3 at 95% C.L. The dipole direction of the Union2 points towards (l, b) = (142◦ +41◦ −27◦) at 68% C.L., which is consistent with previous results. We surprisingly found that the dipole direction derived from the JLA is approximately opposite to that from the Union2. We are grateful to Prof. Perivolaropoulos L. for useful comments and suggestions. This work has been funded by the National Natural Science Fund of China under grants Nos. 11375203, 11305181, 11322545, 11335012 and 11575271.
OwlEyes-Online: A Fully Automated Platform for Detecting and Localizing UI Display Issues<|sep|>Improving the quality of mobile applications, especially in a proactive way, is of great value and always encouraged. In this demo, we show OwlEyes-Online, a fully automated UI display issue detection and localization tool. We use dynamic analysis to explore the application automatically and get its screenshots. And users can customize the exploration time, exploration strategy and so on. Then we can complete the detection and localization of UI display issues based on CNN and Grad-CAM. Finally, we automatically generate test reports and send them to users. We evaluate it from two aspects of detection accuracy and tool practicability. The OwlEyes-Online is proven to be effective in real-world practice, i.e., 64 confirmed or fixed previously undetected UI display issues from popular Android apps. It also achieves boosts of more than 17% and 23% in recall and precision compared with the best baseline. The evaluation shows that OwlEyes-Online is a good starting point for UI display issue detection. In the future, we will further study the root cause of UI display issue. Finally, according to the issue category, we will devise a set of tools for recommending patches to developers to fix the UI display issues. This work is supported by the National Key Research and Development Program of China under grant No.2018YFB1403400, National Natural Science Foundation of China under Grant No. 62072442, No. 62002348.
Trading Strategies Generated by Lyapunov Functions<|sep|>Fernholz (1999, 2001, 2002) provides a systematic approach to generate trading strategies that can be implemented without the need of heavy statistical estimates and whose performance in a frictionless market can be guaranteed by suitable and weak assumption on the market’s volatility structure. The present paper takes a systematic approach to functional generation and makes the following three contributions. 1. Introduces an alternative, “additive” way to generate trading strategies functionally, and compares it to E.R. Fernholz’ “multiplicative” functional generation of trading strategies. Given a sufﬁciently large time horizon T∗ > 0 and suitable conditions on the volatility structure of the market, the multiplicative version yields, for each T > T∗ , a portfolio that strongly outperforms the market on [0, T]; this portfolio, however, depends on T. By contrast, the additive version yields a single trading strategy that outperforms the market over all horizons [0, T] for T ≥ T∗. 2. Extends the class of functions that generate a trading strategy. This paper introduces the notion of a regular function. Such a function can generate a trading strategy. Modulo necessary technical conditions on the boundary behavior, concave functions are shown to be regular. This weakens the assumption of twice continuous differentiability, normally used in the literature of the subject, and provides a uniﬁed framework for standard and rank-based generation, a long-standing open issue. 3. Weakens the assumptions on the market model. Functional generation is shown to work in markets where asset prices are continuous semimartingales that also might completely devaluate. Moreover, major technical assumptions in rank-based generation are removed; for example, it is not necessary anymore to exclude models where the times when two asset prices are identical have strictly positive Lebesgue measure.
A Probabilistic Framework for Moving-Horizon Estimation: Stability and Privacy Guarantees<|sep|>In this work, we laid out a unifying probabilistic framework for moving-horizon estimation. We clearly established the connection between the classical notion of strong local observability and the stability of moving-horizon estimation, for nonlinear discrete-time systems. We then proposed a diﬀerentially private mechanism based on entropic regularization and derived conditions under which ϵ-diﬀerential privacy is guaranteed at any given time instant and over time horizons. As an extension to this work, we intend to include distributional constraints in the moving-horizon estimation framework. An important consideration in the estimation problem, in addition to the asymptotic stability, is the rate of convergence of the observer. It is of interest to obtain convergence rate bounds for the moving-horizon estimators proposed in this paper, and to compare their performance for various choices of the metric (or divergence) in the unifying formulation, which will be undertaken in our future work.
Modelling Concurrent Behaviors in the Process Specification Language<|sep|>In this paper, we proposed in our knowledge the ﬁrst version of a ﬁrst-order theory for gso-structures in [2,5]. We avoid the difﬁculty of not being able to quantify over relations in ﬁrst-order logic by introducing the relations observed simult which take an observation as one of their parameters. Using model-theoretic ontological techniques introduced in [4], we classiﬁed all possible models of Tgso, where our key results are the satisﬁability theorem and axiomatizability theorem for Tgso. In our opinion, the classiﬁcation of models of Tspec, which decomposes the earlier than and nonsimultaneous into smaller graphs, is especially insightful in understanding these three relations. Although the classiﬁcation of observations using ranking structures is quite artiﬁcial, we could not ﬁgure out any simpler characterization. We also give a very intuitive interpretation of the weaker theory T − gso into Tpslcore, which shows that Tpslcore is strong enough to prove most of the theorems in Tgso. The main philosophical difference between Tgso and Tpslcore is that causality relations are treated as logical relations without mentioning the concept of time in Tgso while the causality relations in Tpslcore are directly connected to timepoints of a reference timeline. The fact that T − gso can be correctly interpreted inside Tpslcore also suggests that the soundness and completeness conditions might be too restrictive. One way to relax these conditions is to partition the observation set into “legal” and “illegal” observations, where legal observations are the ones satisfying the soundness and completeness conditions. This approach would also give us the ability to talk about illegal observations.
Robust Benchmarking for Machine Learning of Clinical Entity Extraction<|sep|>Medical notes oﬀer us an incredibly rich view into patient narratives, but they have remained a largely untapped resource for clinical and machine learning research. Clinical studies often resort to manual chart review, which is time-consuming and diﬃcult to scale. While automated concept extraction systems provide a way to exploit clinical text, as we have demonstrated in this work, they cannot yet be reliably deployed. Developing truly robust and accurate clinical entity recognition and normalization algorithms will require both substantially more labeled training data and automated evaluation metrics that account for the subtlety and ambiguity of the task. We present a new annotation framework and a small-scale annotated dataset for evaluation of end-to-end concept recognition and normalization. The time is ripe for the ﬁeld to invest in substantially larger labeled training sets to spur new machine learning approaches for clinical entity recognition and normalization. Future work should consider incorporating the new annotation framework as the basis for such a data set. The authors thank Irene Chen and Divya Gopinath for their helpful comments on the manuscript, and thank Noemie Elhadad, Anna Rumshisky, and Ozlem Uzuner for their valuable conversations and perspective on the clinical annotation process. They would also like to the thank all the organizers of the 2019 n2c2 Workshop for sharing the outputs of top systems for analysis.
Large-N behavior of three-dimensional lattice CP(N-1) models<|sep|>In this paper we have analyzed the phase diagram of the CPN−1 model with Hamiltonian (2), with the objective of understanding the nature of the ﬁnite-temperature transition as a function of the number N of components. The numerical data indicate that the transition is of ﬁrst order. For all values of N we consider, N > 2 up to N = 100, the correlation length ξ obtained from correlations of the gauge-invariant order parameter Qab deﬁned in Eq. (3) is of order one on the HT side of the transition and diverges in the inﬁnite-volume limit on the LT side. Moreover, the transition becomes stronger as N increases: both the latent heat and the surface tension, which parametrize the free energy barrier between the two phases, increase with N. Vector and gauge correlations are massive for any ﬁnite N. On the HT side of the transition, the corresponding correlation lengths ξz and ξP are always of order one, for any value of N: gauge ﬂuctuations are always uncorrelated, even for N = ∞. In the low-temperature phase, instead, we ﬁnd that ξz, ξP ∼ N, so that gauge modes become massless in the large-N limit. Our results are consistent with a simple scenario in which, for any N including N = ∞, the HT phase is always disordered, up to the transition point, where both correlations of the order parameter Q and gauge correlations decay with a typical length scale of the order of one lattice spacing. In the LT phase, Q condenses, while ξz, ξP ∼ N, so that for N = ∞ both gauge-invariant and gauge-dependent degrees of freedom are massless. The transition is therefore of ﬁrst order, even for N = ∞. These results contradict the analytic predictions of the many papers that investigated the large-N limit, see, [18, 13] and references therein. The disagreement can be traced back to one of the standard assumptions which is used in the large-N analysis, both for continuum and lattice models [17, 9, 18, 13]. In the calculation, one usually assumes that the relevant saddle point that controls the behavior of the large-N free energy is translation invariant. For the gauge ﬁelds, this assumption implies that one can set λx,µ = 1 on any lattice link: gauge ﬁelds are assumed to play no role for N = ∞. Our results show that the assumption is correct in the LT phase, but fails in the HT phase: even for N = ∞ the gauge ﬁelds are disordered for any β < βc. This is essentially consistent with the results of [21], that observed that hedgehog conﬁgurations forbid the ordering of gauge ﬁelds in the HT phase, at least if one takes the limit N → ∞ before the limit ξ → ∞. The present results are in agreement with the predictions obtained in the so-called LGW approach deﬁned in terms of the order parameter Qab, provided one assumes that the presence of a Φ3 term in the LGW Hamiltonian implies the absence of continuous transitions. It should be stressed that this assumption should not be taken for granted as it relies on an extrapolation of mean-ﬁeld results to three dimensions. Note that, although we ﬁnd no evidence of a large-N critical transition, our results do not exclude it either, as it is a priori possible that our model is outside the attraction domain of this elusive ﬁxed point. It is interesting to compare our results with those of [3, 4, 5] for SU(N) quantum antiferromagnets. Reference [3] studied a bilayer two-dimensional system and found a behavior analogous to what we ﬁnd here. The transition is of ﬁrst order for N ≥ 4 and becomes stronger as N increases from N = 4 to N = 6. On the other hand, for a single-layer two-dimensional system [4], an apparently continuous transition was always observed. The main diﬀerence between the two models is the topological nature of the allowed conﬁgurations. In the bilayer system, monopoles are allowed, while in the single-layer case monopoles are suppressed. In the model we consider, monopoles are allowed and we expect the transition to be characterized by their binding/unbinding: the monopole density should be positive in the HT phase and vanishing in the LT phase. Thus, on the basis of the results of [3, 4] and consistently with the discussion of [21], one may blame monopoles for the absence of a continuous transition. As the suppression of monopoles corresponds to adding an ordering interaction in the HT phase, it is conceivable—this would be consistent with the results of [4, 5]—that a continuous transition can be observed in a model in which monopoles are completely, or at least partially, suppressed. Clearly, additional work is needed to identify the role that monopoles play in the large-N limit.
Closed-loop robots driven by short-term synaptic plasticity: Emergent explorative vs. limit-cycle locomotion<|sep|>We have shown here, that a robot controlled by only a very limited number of neurons, three in our case, may show complex behavior which may be interpreted as explorative or playful. This is possible when locomotion results from self-organizing processes in the sensorimotor loop. The driving control dynamics, for which we have considered here short-term synaptic plasticity, then adapts itself seemingless to the physical requirements. No central controller is needed to detect an external object (Rai et al., 2014), or to switch direction when colliding with it. Stable and unstable limit cycles, together with chaotic attractors, arise in the phase space of internal (control and robot body) variables. These attractors form continua in the space of physical location and overall propagation direction, with the chaotic locomotion transitioning between unstable limit cycles. Transitions may either be between different types of regular locomotion, bounded circular or propagation meandering modes, or between the directions of unstable propagating limit cycles. We note that the formation of a continuum of attractors is possible, whenever internal and external variables can be separated, such that internal variables span an independent subset of the phase space of the dynamical system. Here, the position of the robot (on the ground plane, in the absence of obstacles) acts as an external variable, all the other variables being independent of it. The limit cycles and chaotic attractors, living in the subspace of internal variables, exist thus for all position vectors, generating a continuous degeneracy of locomotion modes. The interactions with other robots and obstacles then results in a transient breakdown of this degeneracy, which is restored instantaneously with the termination of physical contact. Within this context, higher order control mechanisms would correspond to an external-variable dependent feedback, shaping the attractors either intermittently or slowly (with respect to the internal dynamics), thus leading possibly to the emergence of transiently stable attractors. Our result, that the three-rod robot switches spontaneously between a continuous set of attractors, in the chaotic state, can be seen as a realization of chaotic wandering (Tsuda, 2001), which has been argued in turn to occur in the brain in the form of self-organized instabilities (Friston et al., 2012), viz as transient-state dynamics (Gros, 2007). There is furthermore a close relation to the concept of attractor metadynamics (Gros et al., 2014), which denotes the either induced or spontaneous switching between attracting sets. The here simulated robot is furthermore compliant both on the level of control and actuators, showing a highly ﬂexible response. The actuators are implemented by specifying a target position for a limb, here a moving weight on a rod. The force acting on the weight then results from the interplay between the internal driving, provided by a damped spring (between the actual and the target position), with the physical restoring forces acting on the weights, which in turn depend on the body dynamics determined by the interaction with the ground, obstacles and other robots (Floreano et al., 2014). The isolated controlling network (realized in the limit of inﬁnitely strong actuators) can be interpreted in addition as a central pattern generator (Steingrube et al., 2010), having a single intrinsic limit-cycle attractor. The open-loop control incorporates however the feedback of the environment through the induced forces. We ﬁnd here, that the resulting embodiment (Cangelosi et al., 2015) does morph the driving dynamics of the central pattern generator not only quantitatively, but also qualitatively, giving rise to a vast array of modes which differ in part topologically from the dynamics of the underlying central pattern generator. We believe that this dynamical systems approach of the locomotion of simple robots has not been fully exploited yet, having many interesting features and applications in store for the ﬁeld of neurorobotics. The authors declare that the research was conducted in the absence of any commercial or ﬁnancial relationships that could be construed as a potential conﬂict of interest. Figure 11. Screenshot of the sphere robot in a chaotic mode for Umax = 4 and (w0, z0) = (180, 80), indicated by the arrow in Fig. 9. The blue line retraces the past trajectory. Note that the chaotic wandering is substantially smoother than the one observed for the Umax = 1 case (compare Fig. 6). Figure 12. As a function of time the positions of the three weights, compare Fig. 1, along the corresponding rods. Top: For the Umax = 1 chaotic mode with (w0, z0) = (210, 400) shown in Fig. 6. Bottom: For the Umax = 4 chaotic mode (w0, z0) = (180, 80) shown in Fig. 11. Both modes are locally akin to an S2 mode, albeit with substantial ﬂuctuations (e. g. compare the bottom curvatures of the green line for Umax = 4, see also Fig. 5). Note that phase slips do occur for the case of Umax = 1, but not for Umax = 4. The experiments were conceived and designed by CG, BS and LM, performed mainly by LM with BS adding some data. The data was analyzed by CG, BS and LM, most of the plots produced by LM. The manuscript was mostly written by CG, with BS adding some paragraphs and revising it with LM.
QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding<|sep|>We have presented QSGD, a family of SGD algorithms which allow a smooth trade off between the amount of communication per iteration and the running time. Experiments suggest that QSGD is highly competitive with the full-precision variant on a variety of tasks. There are a number of optimizations we did not explore. The most signiﬁcant is leveraging the sparsity created by QSGD. Current implementations of MPI do not provide support for sparse types, but we plan to explore such support in future work. Further, we plan to examine the potential of QSGD in larger-scale applications, such as super-computing. On the theoretical side, it is interesting to consider applications of quantization beyond SGD.
A Bayesian Foundation for Physical Theories<|sep|>The main objective of this work was to develop a formalisation of the scientiﬁc method using the framework of Bayesian inference. We used this framework to describe the basic processes that compose the scientiﬁc methodology. By relying on this structure, we were able to give formal deﬁnitions to many important, albeit previously only intuitive, concepts that are used to characterise theories of physics. Many of these concepts lack a clear boundary between themselves. The proposed formalisation allowed a sharper deﬁnition of this boundary and a more precise study of them, providing a better understanding of their relevance in the structure of physical theories. The sequence of deﬁnitions presented culminated in the two most important ones in the paper, those of a scientiﬁc theory and of the scientiﬁc method, both agreeing with all our intuitive requirements. Many important conclusions can then be drawn by the use of the formal structure developed in this work. The ﬁrst important insight is that any theory is only deﬁned with respect to some set of questions and their precise deﬁnition is absolutely crucial for comparing two or more theories. This implies that comparison between theories that answer diﬀerent question sets is not a sensible procedure in general. The question sets deﬁne the scope of the theories to be compared, which do not need to be theories about everything but can be restricted to a limited number of questions of interest to the researcher of some scientiﬁc area or simply some speciﬁc system. This allows the application of the methods presented here to select the most appropriate theory to describe that limited set of questions which, as already argued in the main text, does not need to be the most powerful or fundamental theory in general. Another very important insight obtained from this work is the role played by the theory’s fundamental constants, which in physical theories are actually what is meant by their fundamental physical constants. As a consequence of this identiﬁcation, it was possible to make a clear distinction between these constants π and theory’s algorithm α, which then allowed the formalisation of the concept of fundamental status of a theory, which is in full agreement with the term as used in theoretical physics today. The more diﬃcult question of the complexity of a theory then boils down to the analysis of the complexity of the algorithm α. We have discussed the diﬃculties in deﬁning a unique measure of complexity, with many attempts to do so in the literature. Still, based on our studies, we argued that it is sensible to conjecture that the more fundamental a theory is, the more complex it tends to become although we still cannot prove this assertion rigorously as a theorem, as there is still a lacking of agreement about what should be meant rigorously by this complexity. Still, this conjecture can be seen as an instance of the well known and thoroughly studied issue of the interplay between generalisation ability and ﬁtting in machine learning, and its content is supported by the results in this area. Note that the inclusion of the noise term in the modeling of the theory instead of in its testing is an incorporation of the ideas expressed by Duhem [33] that the interpretation of experiments is theory-dependent. Although Duhem do not recognise simplicity as a valid criterion to choose one theory among others as we considered in this work, this does not invalidate our analysis. The reason for this is that simplicity in our work is an a priori requirement that enters in the construction of the prior distribution. Once simplicity is incorporated to it, this concept is not used again during inference process itself. Another key issue is that it becomes clear that although Bayes’ rule is a fundamental principle of theory selection, and therefore of the scientiﬁc method, it is not suﬃcient to capture all the concepts of importance to science. Other elements, like falsiﬁability and fundamental status, which are not directly related to inductive reasoning are also important. One possibility is that these principles are still derivable from a more fundamental principle. In our opinion, which is based on the general feeling of the Bayesian community, this principle is probably maximum entropy. One indication that this may be the correct path to take is the fact that even Bayes’s rule seems to be derivable from it. By applying the developed framework to some cosmological problems, we arrived at the following conclusions. With respect to the typicality of the human observer, we saw that the exact formulation of the question is of utmost importance and no proper answer can be given without a better posed question. We concluded that there is no basis for favouring multiverse theories on the proposed theoretical basis only and any preference can only be attributed to experimental data. Finally, we argued that attributing the status of a physical principle to anthropic reasoning is not appropriate as it can be viewed as a very simple case of inferring the structure of a physical theory from two sources: (i) the very trivial observation (or datapoint) that we exist, (ii) the posterior probability resulting from all experimental available data (collected literally through thousands of years) about the conditions that we need for our survival. The last result of this work is another philosophical important one. We showed, through means of a problem we called the isolated worlds problem, that the framework we developed can show that through pure inference, science can address situations where a strictly positivist approach would render it useless and, above that, would not be considered a valid scientiﬁc question. This shows that using Bayesian inference, the extent to which science can be used is enlarged to situations which were beyond it in other formulations. Although we did not discussed the concept of truth in the whole paper, as we indeed said we would not, we need to include a comment about it. The representation of the probabilistic dependencies of a theory, which we argued can be expressed in the form of a Bayesian network, allows for the introduction in this same theory of hidden nodes. When some variable appears in a theory only as a hidden node, it is fair to discuss on philosophical grounds if any reality can be attributed to this variable. The positivist viewpoint answers this question as a “no”, while the mathematical universe hypothesis, on the contrary, would answer it as a “yes”. Concerning the diﬀerences between our framework and Minimum Description Length [6], MDL suggests to choose theories by minimising the complexity of the description of the hypothesis plus the dataset. Although MDL may be desirable for many applications and a good approximation to Bayesian inference, our framework allows us to address questions that do not appear in MDL. These questions, like falsiﬁability and uniﬁcation, play a very important role in the development and selection of physical theories and we hope that the use of our formalism can lead to a better understanding of them. As a ﬁnal comment, let us highlight that by using ideas coming from probability theory and machine learning we were able to give a mathematical framework to questions that could be considered to lie only on the sphere of philosophy. This shows how important is the role played by pure philosophy in the scientiﬁc endeavour, something that seems to be forgotten nowadays. Acknowledgements I would like to thank Dr. Juan P. Neirotti and Prof. David Saad for stimulating discussions. The comments and suggestions made by Prof. Ariel Caticha about the ideas in the manuscript were deeply inspiring and thoughtful. I would like to thank him for taking the time to read this work and apologise for those points where I did not follow his suggestions, for what I should take alone all the responsibility. I also would like to thank Prof. Nestor Caticha from the University of Sao Paulo, where the main part of this work was done, for introducing me to Bayesian theory and helping me to see its relevance not only to physics, but to science as a whole.
Probabilistic Handshake in All-to-all Broadcast Coded Slotted ALOHA<|sep|>In this paper, we proposed a probabilistic handshake algorithm for vehicular communications based on B-CSA. In the rare cases of communication failure between two users, this event can be detected by one of the users. The simulation results show that around 30% of such events can be detected for the considered distributions. We also proposed analytical bounds on the performance of the handshake algorithm, which match well the simulation results. The analytical bounds rely on the UEP property of CSA, for which a rigorous proof for ﬁnite frame lengths is left for future work.
A New Automatic Method to Identify Galaxy Mergers I. Description and Application to the STAGES Survey<|sep|>We present a new structural merger diagnostic geared towards the structural detection of minor mergers which is entirely based in the morphological properties of the residual images of galaxies after the subtraction of a smooth S´ersic model. The new indicator makes use of the asymmetry of the residuals and of the Residual Flux Fraction of the ﬁt, both calculated over the Kron aperture of the galaxies. This diagnostic has been objectively proven to be able of producing merger samples of equal or better statistical quality than samples obtained using other well established meth Figure 14. Visually classiﬁed minor merger candidates found by the RF F − A(Res) diagnostic but missed by the CAS method adopting its usual limits. COMBO-17 IDs, environments, morphological types and B-band absolute magnitudes from the STAGES public catalogue are shown for each galaxy. The panels also give an indication of the angular extent of the image insets and the approximate fractional contribution of the light included in the circle to the total SEXTRACTOR automatic ﬂux measurement. Objects 20213 and 7479 might be localized star formation episodes in irregular galaxies. Also, it is not straightforward to identify the putative satellite in object 40654, which presents an alternative satellite marked in red. ods based on the morphological properties of the original images. In particular, objects with symmetric residuals for which the Residual Flux Fraction is larger than 0.2 or objects with more asymmetric residuals for which the RFF is larger than 0.1 are very good candidates to be mergers. We have also found that the Gini index of the residuals could also produce merger samples of high statistical purity. In this case, objects for which the Gini index of the residual Figure 14 – continued Visually classiﬁed minor mergers found by the RF F − A(Res) diagnostic but missed by the CAS method adopting its usual limits. COMBO-17 IDs, environments, morphological types and B-band absolute magnitudes from the STAGES public catalogue are shown for each galaxy. The panels also give an indication of the angular extent of the image insets and the approximate fractional contribution of the light included in the circle to the total SEXTRACTOR automatic ﬂux measurement. image calculated within the Kron aperture is higher than 0.5 are also good merger candidates. Using the structural parameters of the residuals and the limits provided by the F −score optimization process shown here, we have split the whole population of galaxies into two diﬀerent set of galaxies. The ﬁrst set, sharing the structural trends and properties of the mergers included in the training set is shown to contain the majority of major and minor mergers. The second set has been shown to be almost completely free of mergers, as exposed by the very low negative contamination rates. However, given the relative dearth of mergers among the general galaxy population and the selfimposed goal of detecting the more elusive minor mergers, it turns out that the RFF − A(Res) diagnostic introduced in this paper works best as a negative merger test. In other words, it is very eﬀective at selecting non-merging galaxies. In common with all the currently-available automatic methods, the sample of both major and minor merger candidates selected by our test is heavily contaminated by non-mergers, and further steps are needed to produce a clean merger sample from the ﬁrst set of galaxies. Nevertheless, the method
Josephson effect in topological superconducting rings coupled to a microwave cavity<|sep|>In this paper, we studied theoretically a Kitaev ring interrupted by a weak link and pierced by a magnetic ﬂux, and coupled inductively to a microwave cavity. We established an input-output description for the cavity transmission, and showed that the cavity response depends strongly on various electronic parameters, such as the chemical potential, the magnetic ﬂux, and the parity of the superconducting ground state. We found a 4π (2π) variation of the cavity response with respect to the external magnetic ﬂux in the topological (trivial) phase, and related such dependence to the fractional (normal) Josephson eﬀect. As opposed to previous works, our theory takes into account, on equal footing, the low-energy Majorana modes, the gaped bulk states, and the interplay between these states in the presence of the cavity. Such a description allows one to describe not only the superconducting ring deep in the topological regime, but also the topological transition, and the crossover from the fractional to the normal Josephson eﬀect in the presence of a magnetic ﬂux. While our theory treats the Kitaev toy model, we stress that it has been shown theoretically that such a model can be emulated by the spectrum of a spin-orbit semiconducting ring subject to an external magnetic ﬁeld and coupled by the proximity eﬀect to an s-wave superconductor. The main diﬀerence between such a model and the Kitaev chain is the number of states, which is doubled, as one needs to account for the spin degree of freedom. While such a study is beyond the scope of our paper, we strongly believe, based on our previous ﬁndings in Ref. 34, that our main results on the visualization of the transition from the fractional to the normal Josephson eﬀect by utilizing a microwave cavity are robust. We would like to thank Helene Bouchiat for useful discussions. This work was supported by the French Agence Nationale de la Recherche through the contract ANR Mistral. 1 N. Read and D. Green, Phys. Rev. B 61, 10267 (2000). 2 D. A. Ivanov, Phys. Rev. Lett. 86, 268 (2001). 3 A. Y. Kitaev, Physics-Uspekhi 44, 131 (2001). 4 H.-J. Kwon, K. Sengupta, and V.M. Yakovenko, Eur. Phys. J. B 37, 349 (2004). 5 L. Fu and C. L. Kane, Phys. Rev. Lett. 100, 096407 (2008). 6 L. Fu and C. L. Kane, Phys. Rev. B 79, 161408 (2009). 7 S. Hart, H. Ren, T. Wagner, P. Leubner, M. M¨uhlbauer, C. Br¨une, H. Buhmann, L. W. Molenkamp, and A. Yacoby, Nature Physics (2014).
A CRC-aided Hybrid Decoding for Turbo Codes<|sep|>In this letter, a CRC-aided hybrid decoding algorithm is proposed, in which the CRC bits are not utilized as error detection but as error correction in the OSD process. An alternative error correction criterion based on the normalized Euclidean distance is also proposed in case that the CRC bits have lost the error detection ability. Simulation results show that our proposed scheme can signiﬁcantly improve the error performance of the turbo-CRC codes with short information lengths. At the same time, the undetected error rates are pretty low for practice applications such as VoIP services.
Shock Dynamics In Relativistic Jets<|sep|>We have developed a new formalism that describes the dynamics of an internal WS in a relativistic jet produced by variations in the source injection velocity. The WS is formed when a fast ﬂow overtakes a previous slower ﬂow. This formalism takes into account that the momentum is not conserved because relativistic mass is lost by radiation, in contrast with non relativistic ﬂows. Assuming step function variations of the injection velocity and mass-loss rate we ﬁnd analytic solutions for the WS velocity and luminosity. We consider two cases: when a pulse of fast material reaches the slow downstream wind (Case I); and when a pulse of slow material is pushed by fast upstream wind (Case II). In the initial phase, the WS is bounded by 2 shocks: one shock incorporates the material from the fast (slow) pulse, the other shock incorporates material from the slow (fast) wind. In this phase, the velocity of the WS is constant. When the material from the fast (slow) pulse is completely incorporated into the WS, only one shock remains: in Case I, the WS is decelerated as more mass is added to the shock by the slow downstream wind; in Case II, the WS accelerates, pushed by the fast upstream wind. The WS luminosity in the constant velocity phase is constant, and decreases with time when one of the shocks disappears. To apply these models to observed GRBs we assume that a constant fraction ǫ of this energy is emitted in gamma rays. In the UR limit, the ratio of the Lorentz factors r = γ2/γ1 and the mass-loss rates b = m2/m1 of the relativistic ﬂows that collide can be obtained directly by ﬁtting the light curves of GRBs. As an example, we ﬁt the light curves of the GRBs 080413B and 070318 with the Case I and Case II models, respectively. For GRB 080413B we obtain the ratios γ2/γ1 = 22.5 and ˙m2/ ˙m1 = 233.7; for GRB 070318 the ﬁt gives lower ratios, γ2/γ1 = 2.6 and ˙m2/ ˙m1 = 0.1. Since the WS is moving towards the observer at relativistic speeds (γws0 ∼ 100 − 800), one has to correct the observed gamma ray ﬂuxes for Doppler boosting. Assuming γ1 = 100, we estimate mass-loss rates of the jets between ˙m1 ∼ 10−5 − 1 M⊙ yr−1. Note that the jet kinetic power is Pkin = γ ˙mc2 ∼ 1044−48ergs−1. This is much smaller than the associated isotropic luminosity, Liso = 4πD2FGRB/ǫ ∼ 1053−54ergs−1, uncorrected for relativistic eﬀects. In fact, the isotropic luminosity has no physical meaning for our relativistic jet models, which are able to produce the Doppler boosted gamma ray ﬂux observed at the Earth. We also evaluate the fraction of the injected mass lost by radiation. For the model of the source GRB 070318 this fraction is ∼ 7%, while for the source GRB 080413B, one ﬁnds that more than 50% of the injected mass is lost by radiation. Therefore, in the latter source, radiation losses change signiﬁcantly the relativistic mass of the WS and affect its dynamics. The step function variability of the source injection velocity and mass-loss rate is a simple approximation to the real time variability of the injection parameters. Other functional time variations of these parameters can be easily implemented in our formalism by integrating the equations in §3 numerically. Nevertheless, in the UR regime our analytic model is very useful to determine the ratios of important physical parameters (the gamma ratio and the mass-loss rate ratio) without introducing any further assumptions. In a future work we will study the high energy emission produced by these internal shocks, in particular, the fraction of energy emitted as gamma rays. J.C., S.L., M.F., R.F.G., and A.H. were supported by PAPIIT-UNAM IN100412 and IN100511. J. C. was also supported by CONACyT 61547. We thank Jose Ignacio Cabrera for providing us with the observational data and Anabella Araudo for useful suggestions. We also thank an anonymous referee for useful comments that helped to improve the paper.
A Probabilistic Characterization of Random Proximity Catch Digraphs and the Associated Tools<|sep|>In this article, we provide a probabilistic characterization of proximity maps, associated regions, and digraphs and related quantities. In particular, we discuss the probabilistic behavior of proximity regions, superset regions, and Γ1-regions; construct digraphs (proximity catch digraphs(PCDs)) and investigate related quantities such as domination number, η and kappa values. We also provide auxiliary tools such as vertex and edge regions for the construction of proximity regions. Although Γ1-regions and superset regions were introduced before (Ceyhan and Priebe (2005),Ceyhan et al. (2006), and Ceyhan and Priebe (2007)) a thorough investigation is only performed in this article. G1-regions are a sort of a “dual” of proximity regions and are associated with domination number being equal to 1. We provide a probabilistic characterization of Γ1-regions for general proximity maps N and data points from distribution F. We also extend this concept by introducing Γk-regions, which are associated with domination number being equal to k. We introduce the quantities related to Γ1-regions and domination number, namely, η-values and κvalues. η-value is the minimum number of points in a set required to determine the Γ1-region for that set. We determine some general conditions that make ηn(N) ≤ 3 for data in the triangle T (Y3). κ-value is the a.s. least upper bound for the domination number of the PCDs. We also determine some general conditions that make κn(N) ≤ 3 for data in the triangle T (Y3). We provide two PCD families, namely proportional-edge PCDs (Ceyhan et al. (2006) and central similarity PCDs (Ceyhan et al. (2007)) as illustrative examples. We discuss the construction of proximity regions and Γ1-regions for these PCDs. Furthermore, we calculate the expected area of Γ1-regions for these PCDs in the limit. Determining Γ1-regions, η and κ values for spherical and arc-slice PCDs contain many open problems and are subjects of ongoing research. With the above characterizations, given another PCD, then we can determine how it behaves in terms of Γ1-regions and related quantities; in particular we can determine a.s. least upper bounds for the domination number of the new PCD.
Filtering Procedures for Sensor Data in Basketball<|sep|>In the era of the Information Technology and Big Data Analytics, team sports’ managers beneﬁt from the availability of advanced statistics. However, statistics are just a tip in the iceberg, while there is an hard work behind, which concerns the steps of tracking, collecting, storing and processing the data. This paper concerned with basketball data processing, and aimed to suggest an ad-hoc procedure to automatically ﬁlter a data matrix containing players’ movement information to the moments in which the game is active, and by dividing the game into sorted and labelled actions. In this regard, we placed this work within the area of the Human Activity Recognition, as we used players’ actions to recognize a speciﬁc game state (i.e. inactive game moments). The algorithm has been tested on three diﬀerent real games, and a series of robustness checks has been done, including a validation for the parameters to be used in the algorithm. Results of the validation suggests a stability of the two parameters along diﬀerent games. Practitioners which are in possession of a data matrix as the one described in this paper can replicate this procedure to analyse basketball matches. The novelty of this procedure is that, unlike existing works, for example [16], it works when the ball’s trajectory is unavailable. However, further research is to be planned in order to validate the algorithm also with respect to a visual analysis of the same match. Research carried out in collaboration with the Big&Open Data Innovation Laboratory (BODaI-Lab), University of Brescia (project nr. 03-2016, title Big Data Analytics in Sports, www.bodai.unibs.it/BDSports/), granted by Fondazione Cariplo and Regione Lombardia. The author thanks Paola Zuccolotto, Marica Manisera (University of Brescia) and Tullio Facchinetti (University of Pavia) for valuable suggestions.
Incorporating Monitors in Reactive Synthesis without Paying the Price<|sep|>We have explored synthesis for speciﬁcations that combine modelling and declarative aspects, in the form of symbolic monitors triggers for LTL formulas. We have shown how this extends the scope of synthesis by allowing parts of a speciﬁcation that are hard for synthesis to be instead handled in the monitor part. The synthesis algorithm we give synthesises the LTL part without requiring the need to reason about the monitor. Moreover, we have implemented this approach and applied it to several case studies involving counting and monitoring multiple sequences of events that can be impossible or hard for LTL synthesis. We showed how by exploiting the symbolic nature of the monitors we can create ﬁxed-size parameterised controllers for some parameterised speciﬁcations. Future Work Our work opens the door to a number of interesting research avenues, both by using richer monitor triggers and by exploring diﬀerent interactions between triggers and controllers. We discuss below just a few such possibilities. In all the cases below the challenges lie not only in providing a new language to capture the extension but rather in the theoretical framework with a proof that the integration is sound. A ﬁrst intuitive extension is to add real-time to the monitors, to express properties like “compute the average use of a certain resource every week and activate the controller to act diﬀerently depending on whether the average is bigger (or smaller) than a certain amount”. While extending the monitor with real-time is quite straightforward (our monitors are restricted versions of DATEs [8] which already contain timers and stopwatches), the challenge will be to combine it with the controller in a suitable manner. Having real-time monitors running in parallel with controllers would enable for instance the possibility to add timeouts to activities performed by the controllers. Currently we have a strict alternation between the execution of the monitor and the controller: we would like to explore under which conditions the two can instead run in parallel. This would allow the controller to react to the monitor only when certain complex condition hold while the controller is active doing other things (e.g., the monitor might send an interruption request to the controller when a certain sequence of events happens within a certain amount of time, while the controller is busy ensuring a fairness property). We could also have many triggers that run in parallel activating diﬀerent controllers, or even some meta-monitor that acts as an orchestrator to enable and disable controllers depending on certain conditions. This might require to extend/modify the semantics since the interaction might be done asynchronously. We would like to address the limitation of controller synthesis concerning what to do when the assumptions are not satisﬁed. It is well-known that in order to be able to automatically synthesise a controller very often one must have strong assumptions, and nothing is said in case the assumptions are not satisﬁed. We would like to explore the use of monitors to monitor the violation of assumptions and interact with the controller in order to coordinate how to handle those situations (we can for instance envisage a procedure that automatically extends the controller with transitions that takes the controller to a recovery state if the assumptions are violated).
Dark Matter Signals from Cascade Annihilations<|sep|>The possibility of indirect detection of dark matter has been considered for over 25 years [55], but the annihilation rates expected from WIMP thermal relics are typically too small to give Table 6: Bounds from gamma rays on the branching fractions of a → γγ and a → π+π−π0 in the minimal axion portal (a → µ+µ−). These are obtained neglecting all other sources of gamma rays and correspond to the best ﬁt values for mDM and B and the propagation model giving smallest χ2. The bounds assume an equal boost factor for e± and gamma rays, and should be multiplied by Be,astro/Bγ,astro if the boost factors diﬀer. appreciable gamma ray or neutrino ﬂuxes from the galactic center unless a very peaked dark matter halo proﬁle is assumed. If the PAMELA/ATIC data is indicative of dark matter annihilation, however, then the galactic annihilation rate must be boosted by O(1000). This large boost factor considerably enhances the potential for galactic gamma ray and neutrino signals from the dark sector. In this context, hints from the WMAP Haze may also point towards an annihilation explanation of PAMELA/ATIC. In this paper, we have explored the robustness of dark matter annihilation predictions by considering cascade scenarios where dark matter annihilates into new resonances that in turn decay in one or more steps into standard model leptons. These cascade annihilation scenarios are directly motivated by the PAMELA/ATIC data, since light resonances can enhance the galactic annihilation rate through nonperturbative eﬀects and explain the lepton-richness of the annihilation through kinematic thresholds. We have shown that electron and muon cascades give reasonable ﬁts to the PAMELA/ATIC data. As a rule of thumb, the best ﬁt dark matter mass and boost factor both scale as 2n for n-step cascade decays. We then compared these best ﬁt values to constraints from gamma rays and neutrinos. The gamma ray bounds from FSR can be weakened by an order of magnitude through cascade decays, although increasing the length of cascades does not further weaken the bounds. Neutrino bounds for dark matter annihilating into muons are robust to changing the length of the cascade, which is particularly relevant for models with large branching fractions to muons such as the minimal axion portal. Assuming standard NFW or Einasto halo proﬁles, there is tension between a dark matter annihilation interpretation of PAMELA/ATIC and the non-observation of galactic gamma rays or neutrinos. Such tension does not invalidate a dark matter annihilation hypothesis since there is considerable uncertainty in the dark matter halo distribution and velocity proﬁle, and the constraints are uniformly weaker for shallower halo proﬁles. For gamma rays in particular, the galactic center and galactic ridge constraints assume an understanding of the dark matter halo proﬁle in the inner 100 pc of the galaxy, where there is considerable uncertainty. The dark matter halo proﬁle in the inner 4′′ of the galaxy is even more uncertain, so we do not consider radio measurements of synchrotron to be constraining. Also, for both gamma rays and neutrinos, the bounds can be weakened if the astrophysical boost factor for electrons/positrons is larger than those for gamma rays and neutrinos. If a dark matter annihilation scenario is realized in nature with the boost factor suggested by PAMELA/ATIC, then one would expect future experiments to see a gamma ray or neutrino ﬂux given standard halo assumptions. ANTARES [56], IceCube [57], and KM3NeT [58] will greatly increase current sensitivity to upward-going muons resulting from galactic neutrinos. Future atmospheric Cerenkov telescopes as envisioned in Ref. [59] will also improve the prospects of ﬁnding gamma rays from dark matter annihilation. While we did not include the eﬀect of ICS in our gamma ray analysis, ICS is expected to be a dominant dark matter annihilation signal in the energy range available to the Fermi Gamma-ray Space Telescope [60]. Ultimately, one hopes that future experiments could probe the detailed energy spectra of dark matter annihilation products to distinguish between direct annihilation and the cascade scenarios considered here. We thank Shantanu Desai from the Super-K collaboration for providing us with 95% conﬁdence bounds on the upward-going muon ﬂux. This work was supported in part by the Director, Oﬃce of Science, Oﬃce of High Energy and Nuclear Physics, of the US Department of Energy under Contract DE-AC02-05CH11231, and in part by the National Science Foundation under grant PHY-0457315. The work of Y.N. was supported by the National Science Foundation under grant PHY-0555661, by a DOE OJI, and by the Alfred P. Sloan Foundation. D.S. is supported by the National Science Foundation, and J.T. is supported by the Miller Institute for Basic Research in Science. In this appendix, we present formulae for the energy spectra used in the text. In general, the energy spectra of ﬁnal state particles in cascade annihilations are functions of all the intermediate masses and helicities. In the limit of large mass hierarchies and scalar decays, however, the energy spectra greatly simplify, and we use these simpliﬁed formulae in our analysis. Consider cascading ﬁelds φi of mass mi (mi+1 > 2mi) and a ﬁnal state ψ with mass mψ. Cascade annihilation occurs through φi+1 → φiφi (i = 1, 2, · · ·), and in the last stage, φ1 decays into ψ + X. Let the energy of ψ in the φ1 rest frame be E0. Deﬁning where ǫ0 ≤ x0 ≤ 1. In the case where dark matter χ annihilates directly into ψ + X, we can regard φ1 as the initial state of dark matter annihilation, χχ. In this case d ˜Nψ/dx0 is the primary injection spectrum with m1 = 2mDM. Now consider the previous step in the cascade annihilation, φ2 → φ1φ1, with one of the φ1 decaying into ψ + X. Let the energy of ψ in the φ2 rest frame be E1 and deﬁne where θ is the angle between the ψ momentum and the φ1 boost axis as measured in the φ1 rest frame. Equation (27) is complicated to solve in general, but in the limit ǫi → 0 (i = 0, 1, · · ·), it reduces to a simple convolution: where 0 ≤ x1 ≤ 1 up to O(ǫ2 i ) eﬀects. This convolution can be iterated as many times as necessary to build up the desired energy spectrum for an n-step cascade decay: where xn−1 = 2En−1/mn with En−1 being the energy of ψ in the φn rest frame, and 0 ≤ xn ≤ 1 up to O(ǫ2 i ) eﬀects. Note that we here adopt the normalization convention of
Optical counterparts of undetermined type $\gamma$-ray Active Galactic Nuclei with blazar-like Spectral Energy Distributions<|sep|>In this study we presented the optical spectra of six targets that have been associated to γ-ray sources of still undetermined type in 3LAC. In some cases, the spectra of this type of objects can still be obtained from large spectroscopic surveys, such as the SDSS, above all, but, in others, we need speciﬁcally planned observations. The increase of sensitivity and resolution achieved by the Fermi-LAT at γ-ray energies is now providing better opportunities to identify candidate counterparts to extra-Galactic γ-ray emission and, therefore, to improve the selection of targets. With an improved ability to detect γ-rays from faint sources we can now investigate the occurrence of nuclear activity on diﬀerent power scales. The detection of faint blazar-like activity in low luminosity AGNs or even apparently normal galaxies opens a new window on the demographics of γ-ray sources, as well as on the mechanisms that contribute to black hole growth and jet formation. The possibility that such objects may represent an important contribution to the γ-ray radiation of undetermined origin deserves further investigation. Searching for radiation from hidden AGNs is a fundamental science case for instruments designed to observe high energy photons and other hints of jet activity, such as light polarization. Therefore, we plan to further investigate the spectroscopic properties of candidate counterparts to γ-ray emission, taking possibly into account light polarization studies as well, through an extensive observational campaign designed for middle class telescopes. The Fermi-LAT Collaboration acknowledges support for LAT development, operation and data analysis from NASA and DOE (United States), CEA/Irfu and IN2P3/CNRS (France), ASI and INFN (Italy), MEXT, KEK, and JAXA (Japan), and the K.A. Wallenberg Foundation, the Swedish Research Council and the National Space Board (Sweden). Science analysis support in the operations phase from INAF (Italy) and CNES (France) is also gratefully acknowledged. This work is based on observations collected at Copernico (or/and Schmidt) telescope(s) (Asiago, Italy) of the INAF - Osservatorio Astronomico di Padova. Part of this work is based on archival data, software or online services provided by the ASI SCIENCE DATA CENTER (ASDC).
Stability of concentrated suspensions under Couette and Poiseuille flow<|sep|>In this work, we studied the stability properties of a multiphase model for concentrated suspensions for Couette and Poiseuille ﬂow. Our linear stability analysis showed two instabilities exhibited by the proposed model in case of plane Couette ﬂow: a collision pressure driven ill-posedness and a convection induced instability. An analytic ansatz showed that the ill-posedness stems from a competition between the solid phase viscosity and the collision pressure and poses a necessary stability condition on the size of the solid phase viscosity compared to the collision pressure. This has been reaﬃrmed by comparison between numerical and analytical results. The convection driven instability has been analyzed using a Kelvin-mode ansatz. The resulting time dependent ordinary diﬀerential equations showed a transient instability. We note that this might prohibit an experiment from showing the Couette or Poiseuille ﬂow base state, because of the onset of turbulence or the occurrence of shocks for highly concentrated suspensions. The consequence of the convection driven instability for the studied base states can best be analyzed using a direct numerical simulation of the full model, which will be part of our future work. In addition, further extensions of the underlying model that include intermolecular short range forces may become necessary for the stability properties that relate to the formation of anisotropic microstructures for high enough volume fractions, see e.g. [35, 36]. In case of the Poiseuille ﬂow, we also retrieved the multi-phase instabilities and compared the multiphase model to the stability of the Bingham ﬂow. We also note here, that since Poiseuille ﬂow for our two-phase model contains diﬀerent velocities of the solid and liquid phase, the problem of loss-of-hyperbolicity might arise here too. Our numerical studies therefore focused on cases with large velocity diﬀerences between solid and liquid phases, as one would expect this transition to occur in those cases. However, as has been shown in [37, 38], while the loss-of-hyperbolicity and the associated ill-posedness can only be observed in the long-wave limit and additionally with ﬁne meshes, our numerical results did not yield new unstable modes, even for rather small wave numbers, such as α < 0.01. It remains to be shown if this picture changes for higher resolutions, smaller viscosity terms or perhaps also diﬀerent base states. It would thus be interesting to see if for our two-phase ﬂow model the ill-posedness can also be connected to the existence of a singular shock, such as has been seen in applications detailed in Carpio et al. [39] or Bell etal. [40], or in connection with other operators studied by Zhou et al. [41] and Cook et al. [42]. TA gratefully acknowledges the support by the Federal Ministry of Education (BMBF) and the state government of Berlin (SENBWF) in the framework of the program Spitzenforschung und Innovation in den Neuen L¨andern (Grant Number 03IS2151).
A Ku-Band Novel Micromachined Bandpass Filter with Two Transmission Zeros<|sep|>designed, fabricated and measured. The explicit design  procedure is described. All the grounded sections of the  filter are realized by micromachined via-hole. The filter  has two transmission zeros due to its novel structure.  Measured results have shown the excellent performance  and advantages of this type of filters. And the filter size is  much smaller than the conventional one.
Quantum Model-Discovery<|sep|>We demonstrated that techniques from scientiﬁc machine learning can be placed in a quantum computational setting. In particular, our numerical experiments show that relatively small quantum circuits can be used for meaningful computations in scientiﬁc machine learning. We have demonstrated how to implement an automated parameter inference and equation discovery framework (QMoD) using a differentiable quantum circuit strategy, and reached results that are on par with the classical machine learning method DeepMoD. With this demonstration of what can be achieved with DQCs the ﬁelds of quantum computing and scientiﬁc machine learning are moving closer together. We should note that all experiments described here involve classical simulators of quantum computers with unrealistic assumptions such as zero sampling noise, no environment noise and no measurement noise. Although many error mitigation strategies for NISQ hardware exist, it should still be investigated how well the strategies described work in more realistic settings and importantly on real quantum hardware, and which quantum hardware is most compatible/natural for this strategy. Furthermore, due to the intrinsic scaling issues of simulating quantum many-body systems, we did not consider more than 12 qubits in the largest simulations described here, and thus also compared with only small-sized classical neural networks. It will be important to investigate the performance for larger number of qubits on quantum hardware, and compare them to realistic classical PINN architectures. The strategies described above work for systems governed by sets of Partial Differential Equations, but also for those governed by stochastic processes modelled as Stochastic (Partial) Differential Equations (SDEs), through the perspective of quantile mechanics combined with DQCs as described in the recent paper on Quantum Quantile Mechanics (QQM) [49]. In such setting, an SDE may be learned (‘discovered’) based on just statistical data. This could prove valuable in the generative modelling case, where synthetic data generation of time-series is learned based on a combination of data and a parameterized SDE with a library of candidate model operators. In such real-world setting as ﬁnancial markets, there is often not exact knowledge of the underlying model dynamics but there is access to historical data, where QMoD could infer the SDE from data and make better predictions compared to a purely data-only approach. An interesting area of potential yet unexplored, is the identiﬁcation of quantum computational speedup for scientiﬁc machine learning in other aspects than Neural Network enhancement, for example by combining QMoD with combinatorial optimization. Ethics declaration. A patent application for the method described in this manuscript has been submitted by Qu&Co.
Reduced Physics Model of the Tokamak Scrape-off-Layer for Pulse Design<|sep|>In this paper we present the SOL Box Model, a reduced model for SOL power and particle balance. The Box Model overall produces reasonable results for separatrix electron
A New Representation of Successor Features for Transfer across Dissimilar Environments<|sep|>In this paper we proposed a novel transfer learning approach based on successor features in RL. Our approach is for the scenarios wherein the source and the target environments have dissimilar reward functions as well as dissimilar environment dynamics. We propose the use of Gaussian Processes to model the source successor features functions as noisy measurements of the target successor functions. We provide a theoretical analysis on the convergence of our method by proving an upper bound on the error of the optimal policy. We evaluate our method on 3 benchmark problems and showed that our method outperform existing methods. This research was partially funded by the Australian Government through the Australian Research Council (ARC). Prof Venkatesh is the recipient of an ARC Australian Laureate Fellowship (FL170100006).
Approximation of Search Times for On-street Parking Based on Supply and Demand<|sep|>We propose an algorithm for the approximate estimation of a drivers’ cruising time that is based on the  high-resolution maps of a city’s parking demand and supply, and the probability to search closer or  further away from the destination estimated in a serious game. As a by-product, we obtain the  distribution of the distance to destination and parking occupation. The proposed method can be applied  to every city where the patterns of parking demand and supply are known at a resolution of buildings,  roads and parking facilities. The estimates obtained are very close to those obtained in a dynamic  simulation model. Applying our algorithm to the Israeli city of Bat Yam, we demonstrate that despite a  low, ca. 61%, average demand to supply ratio, spatial heterogeneity of the demand and supply patterns  results in lengthy parking searches for a significant fraction of drivers. It should be stressed that the  perspectives of agent-based modeling of human-driven systems such as parking, critically depend on our  knowledge of agents’ behavior. In this respect, we consider serious games as a framework for capturing  and further formalizing human behavior in dynamic self-organizing systems. The paper does not include a study of the goodness-of-fit of the proposed approximation as dependent  on the parameters of a parking system, such as the ratio of employees to visitors, the duration of  parking, and variations in the rate of arrivals throughout the day. However, our preliminary investigation  that focused on the sensitivity of the proposed approximation to the employee:visitor ratio clearly  manifests low or very low sensitivity. From a practical point of view, parking search time is a basic parameter for the urban planner who aims  at assessing the consequences of construction of a new office, commercial or residential building. If  parking supply in the area is insufficient for the anticipated demand, a planner can choose to increase  supply by adding parking facilities or, in contrast, reduce the availability of parking spaces in order to  discourage drivers from entering the area. Our method can be applied for predicting changes in parking  search times and the overall area affected by the proposed changes. It is very natural that parking studies, including ours, focus on the estimation of system parameters in  the situation when parking supply is insufficient and drivers can cruise for lengthy periods. However, no  matter how deep the knowledge regarding parking search times, the goal of a planner is to improve the  traffic in city centers [Marsden, 2006]. A self-evident and highly unpopular solution here is to reduce the  number of cars that enter the city center, and this can be achieved in different ways. Besides rigid direct  limitations on vehicular entrance to a designated area, policy makers can also decrease the demand by  increasing parking prices [Shoup, 2006; Gragera and Albalate, 2016; Pierce and Shoup, 2013; SFMTA,  2016; Chatman and Manville, 2014; Millard-ball et al., 2014; Cats et al., 2016; Alemi et al., 2018].  Furthermore, parking prices can be flexible, adapting to the fluctuations in the demand for parking  throughout the day. Incorporation of cruising drivers’ reactions to parking prices would extend our  approach, and the first step in this direction is presented in [Fulman and Benenson, 2019].
Grain Surface Classification via Machine Learning Methods<|sep|>quickly and efficiently with the proposed method. The method proposed as a machine learning-based system can accurately classify the  3-D grain surface. In the literature, studies have been intensively focused on detection of grain amount than grain surface classification.  Therefore, this problem remained unsolvable. This study showed that grain surface classification can be implemented with high  performance. STFT + GLCM + SVM machine learning method showed the highest performance in accordance with obtained results.  In future studies, performance can be increased by using deep learning methods.
CrossFill: Foam Structures with Graded Density for Continuous Material Extrusion<|sep|>In this paper, we have introduced a new inﬁll structure, CrossFill, which can provide spatially graded density to match a userspeciﬁed density distribution. CrossFill is carefully designed so that it is self-supporting and can be fabricated from a single, continuous and overlap-free toolpath on each layer. Algorithms for generating the lower bound subdivision levels and dithering the subdivision levels have been developed to accurately match the prescribed density distribution. To use CrossFill as inﬁll structures of a given 3D model, we have presented an algorithm to Fig. 24. Various examples of applications of CrossFill. (a) A bicycle saddle with a density speciﬁcation. A weight of 33 N is added on various locations to show the diﬀerent response of diﬀerent density inﬁll. (b) A teddy bear with a density speciﬁcation. (c) A shoe sole with densities based on a pressure map of a foot. (d) The Stanford bunny painted with a density speciﬁcation. (e) A medical phantom with an example density distribution for calibrating an MRI scanning procedure. connect the toolpaths of CrossFill and the toolpaths for the given models shell into a single continuous extrusion path. The performance of CrossFill has been veriﬁed on a variety of experimental tests and applications. The study of experimental tests shows that CrossFill acts very much like a foam although future work needs to be conducted to further explore the mapping between density and other material properties. Another line of research is to further enhance the dithering technique, e.g. changing the weighing scheme of error diﬀusion.
Diffractive mechanisms in $pp \to pp \pi^{0}$ reaction at high energies<|sep|>We have included the π0-bremsstrahlung from the initial or ﬁnal proton, diffractive π0-rescattering, photon-photon, photon-omega and photon-odderon fusion processes. We have found very large cross sections of the order of mb. The dominant contributions are placed at large rapidities where they could be measured with the help of the forward detectors at the LHC. The total (integrated over phase space) cross section is almost energy independent. Absorptive effects lower the cross section by a factor 2 to 3; see Table I in [1]. At the LHC the two-photon fusion mechanism “wins" with the diffractive mechanisms at midrapidity. However, the transverse momenta of neutral pions in this region are very small and therefore such pions are very difﬁcult to measure. The γω or ωγ exchanges have been found to be signiﬁcant only in backward or forward rapidities, respectively, and are small at midrapidities due to ω reggeization. We have also presented ﬁrst estimates of the photon-odderon and odderon-photon contributions based on the upper limit of the γ p → π0p cross section obtained at the HERA as well as
Flatland Competition 2020: MAPF and MARL for Efficient Train Coordination on a Grid World<|sep|>The results show that RL solutions are still a considerable distance away from OR based solutions. OR solutions have the advantage that they can search the joint conﬁguration space in a centralized manner, and, therefore, allow agents to exhibit coordinated maneuvers that can handle complex situations. A single deadlock in Flatland can have a domino eﬀect and further work needs to be done to address such events eﬀectively. Deﬁning the right observations has proven to be diﬃcult and more research is needed to investigate the inﬂuence of speciﬁc features. For example, the local observation of an agent could be signiﬁcantly enriched by combining a tree observation with a local grid observation to enable long-term planning and spatial reasoning in the region surrounding each agent. Also network structures with a temporal component such as an LSTM could improve the long-term planning, but accommodating such an approach with state masking is challenging. In the competition, heuristics customised to the speciﬁcs of Flatland and the task at hand, such as departure schedules and prioritisation, greatly improve the performance of solutions. On the other hand, coordination mechanisms such as trafﬁc light policies (see Section 4.4) and the graph based prioritisation (see Section 4.3) could potentially be improved by introducing RL-based controllers instead of manually implemented ones. Overall, the Flatland competition 2020 demonstrated the potential of RL approaches but also showed that more systematic research is needed to ﬁnd eﬀective methods to solve the VRSP for problems such as Flatland. We would like to thank the competition sponsors Swiss Federal Railways Company (SBB), Deutsche Bahn (DB) and Société Nationale des Chemins de fer Français (SNCF) as well as all the people that helped with the planning, preparation and facilitation of the competition. Special thanks to Irene Sturm and Gereon Vienken from DB and François Ramond from SNCF. We are also grateful to Shivam Khandelwal, Jyotish Poonganam and Yoogottam Khandelwal from AIcrowd for their parts in the setup and maintenance of the competition. An_Old_Driver: We thank Han Zhang for initially trying some ideas. We thank our team advisors Danial Harabor, Peter J. Stuckey, Hang Ma, and Sven Koenig for helpful discussions and comments. The research at the University of Southern California was supported by the National Science Foundation (NSF) under grant numbers 1409987, 1724392, 1817189, 1837779, and 1935712 as well as a gift from Amazon. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the oﬃcial policies, either expressed or implied, of the sponsoring organizations, agencies, or the U.S. government. JBR_HSE: This research was supported in part through computational resources of HPC facilities at HSE University, Russian Federation. Support from the Basic Research Program of the National Research University Higher School of Economics is gratefully acknowledged. Netcetera: We thank Netcetera for the support for the participation in the Flatland challenge. We also thank Darko Filipovski, Nikola Velichkovski and Zaﬁr Stojanovski that were part of the previous rounds of the challenge.
Stateless multicast switching in software defined networks<|sep|>A stateless multicast solution for SDN has been presented. The proposal builds on previous efforts to use Bloom Filters for multicast switching but shows, for the ﬁrst time, that it can be deployed in contemporary SDN switches without any changes. The state in the TCAM of the switches in realistic network topologies are analysed showing that this multicast mechanism has signiﬁcantly less state than unicast switching of the most common alternatives such as layer-2 switching or MPLS. Furthermore, the mechanism only requires SDN ﬂow entries to be inserted proactively, signiﬁcantly solving the scalability and delay considerations of solutions that require reactive insertion from the centralised SDN controller. While the proposed solution has been shown to work with existing SDN switches, there are two areas where the work should be extended. The ﬁrst is to extend suitable SDN controllers to support the arbitrary match that the OpenFlow protocol requires and the authors intend to do this through the OpenDaylight project. A second extension of the mechanism is the resolution of the scalability problem caused by the false positives inherent to Bloom Filters. One suggested proposal is to divide the network into zones of suitable sizes so that a larger network can be implemented using a subset of smaller zones that have no false positives. Future work will model this solution to demonstrate that this stateless multicast solution for SDN can scale to any network size.
High-Performance Neural Networks for Visual Object Classification<|sep|>We presented high-performance GPU-based CNN variants trained by on-line gradient descent, with sparse random connectivity, computationally more eﬃcient and biologically more plausible than fully connected CNNs. Principal advantages include state-of-the-art generalization capabilities, great ﬂexibility and speed. All structural CNN parameters such as input image size, number of hidden layers, number of maps per layer, kernel sizes, skipping factors and connection tables are adaptable to any particular application. We applied our networks to benchmark datasets for digit recognition (MNIST), 3D object recognition (NORB), and natural images (CIFAR10). On MNIST the best network achieved a recognition test error rate of 0.35%, on NORB 2.53% and on CIFAR10 19.51%. Our results are raising the bars for all three benchmarks. Currently the particular CNN types discussed in this paper seem to be the best adaptive image recognizers, provided there is a labeled dataset of suﬃcient size. No unsupervised pretraining is required. Good results require big and deep but sparsely connected CNNs, computationally prohibitive on CPUs, but feasible on current GPUs, where our implementation is 10 to 60 times faster than a compiler-optimized CPU version. This work was partially funded by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF: Intelligent Fill in Form.
Bidirectional deep learning of polarization transfer in liquid crystals with application to quantum state preparation<|sep|>We reported on modeling the transfer function of the complex multi-cell twisted nematic liquid crystal device using deep neural networks. The model was trained using an experimentally acquired dataset containing control voltages and tomographically measured polarization states. The trained model predicts the output polarization from the control voltages at the unprecedented ﬁdelity level. The accuracy of the model was compared to commonly used approaches, namely linear interpolation and radial basis function interpolation. The deep learning model is more accurate and faster than both the reference methods by orders of magnitude. Also, the deep learning model is resource-eﬃcient; it requires signiﬁcantly fewer samples than other tested approaches for the given accuracy. Our main result lies in solving the ambiguity of the control voltages in the inverse transformation. Here the optimum control voltages are predicted for a given polarization. Various combinations of control voltages can result in almost similar polarization states. Furthermore, there is no preferred metric in the space of classical control signals (control voltages in our case). We solved these issues by creating the compound model consisting of a trainable inverse part and a ﬁxed direct part trained in the previous step. We further veriﬁed our results by employing the deep learning models in local single-photon polarization state preparation and remote quantum state preparation. Our results open the path to ultra-precise polarimetry using liquid crystals with classical light as well as with single-photon signals in quantum information processing. Even though our work focuses on polarization encoding, we expect similar behavior and scaling in systems transforming diﬀerent degrees of freedom of light such as spatial modes or which-way information in interferometric networks. The developed approach allows for nearperfect bidirectional classical control of the polarizationencoded quantum system and is easily transferable to other photonics quantum systems.
Reconfiguration Dynamics in folded and intrinsically disordered protein with internal friction: Effect of solvent quality and denaturant<|sep|>FIG. 10: Log(τ SDRCRIF N0 ) vs Log(N) at diﬀerent values of kc for good solvent (ν = 3/5). The values are kc = 0, b = 3.8 × 10−10m, ξ = 9.42 × 10−12kgs−1, ξint,0 = 0, kB = 1.38 × 10−23JK−1 and (∼ 340 ns) which is ∼ 3 times higher than the same (∼ 100 ns) in good solvent (ν = 3/5) for kc = 0, b = 3.8 × 10−10m, ξ = 9.42 × 10−12kgs−1, ξint,0 = 100 × ξ, kB = 1.38 × 10−23JK−1 and
Informational Substitutes<|sep|>6.1 Contributions and discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 6.2 Future work: game theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 6.3 Future work: algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.4 Future work: structure of S&C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 A.0.1 Substitutes and complements . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 A.0.2 Information in markets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 A.0.3 Other game-theoretic settings. . . . . . . . . . . . . . . . . . . . . . . . . . . 55 A.0.4 Algorithms for information acquisition. . . . . . . . . . . . . . . . . . . . . . 56 B.0.1 Deﬁning the value of information . . . . . . . . . . . . . . . . . . . . . . . . . 57 B.0.2 The approach of Börgers et al. . . . . . . . . . . . . . . . . . . . . . . . . . . 57 B.0.4 Capturing “diminishing” marginal value . . . . . . . . . . . . . . . . . . . . . 60
End-to-End Memristive HTM System for Pattern Recognition and Sequence Prediction<|sep|>This paper proposes a memristor-based mixed-signal architecture of the HTM network including the spatial and temporal aspects of the algorithm. The proposed architecture incorporates several plasticity mechanisms such as synaptogenesis, neurogenesis, etc. that endow the network a high-degree of plasticity with lifelong learning and minimal energy dissipation. The high-level behavioral model of the architecture is veriﬁed for time-series data prediction. It is found that the MAPE of the hardware model is more than that in the software counterpart by 1.129X. This degradation is mainly attributed to the memristor devices’ nonidealities and the use of synthetic synapses representation. The proposed architecture is also evaluated for latency and lifespan. We found that the mixed-signal implementation is ≈3.46X faster than the pure CMOS implementation and its less affected by network scale, while the network elasticity (lifespan) can be up to 8 years, assuming that learning occurs every 10ms. When it comes to network robustness, it is observed that the HTM network is robust to device failure, but this is not the case for its SDR classiﬁer, which is impacted by stuck-on faults. Furthermore, it is observed that the power consumption in the proposed architecture is dominated by the cells, particularly the proximal and distal segments. Thus, in our design, we strive to limit their use to minimum number of cycles and thereby reduce the average total power consumption of the network to 29.38mW.
Universal network deployment model for universal connectivity<|sep|>Universality of access does not come from thin air, it needs a pervasive infrastructure, that can be built thanks to the returns from private deployments in public land, the way leave over public land, that results in a minimum cost infrastructure commons for the use of public services and shared usage. This return prevents the privatization of public land or, in other words, extractive or anticompetitive practices that build on limited access or exclusion from the Internet. The beneﬁciaries are public digital services, and everyone in a community, including non-proﬁt and for-proﬁt initiatives. To some extent is equivalent in metaphorical terms to that no roads for private usage are allowed unless a lane is given to public usage and another for anyone to share it. This model, coming from the guiﬁ.net ordinance12 template, proposes to separate three types of uses and clarify how a city council has to regulate that a private entity can use public land, in a clear way, preventing privatization, for the beneﬁt of all: the private pays the deployment and maintenance in exchange for creating and giving a new path for the public and another for shared usage. 12The current version (30) is in Catalan language. Ramon Roca, Lluís Dalmau and Roger Baig from the guiﬁ.net Foundation have created and coordinated the development of this document that can be found in https://fundacio.guiﬁ.net/en_US/page/documentos In today’s technology, this implies that any private deployment of ﬁbers in land (including undersea) results in a return for everyone of ﬁbers for public and shared use. This regulation goes beyond the recommended infrastructure sharing like the cost-reduction directive in the EC, formulating a mandatory return by default. The proportion of return may vary according to the cost-beneﬁt conditions in each context, ranging from municipal land to regional, national, international land terrestrial and underseas. It can be seen as a public-private-citizen collaboration that results in beneﬁts for all, the private that can deploy the infrastructure he needs, and an infrastructure commons that beneﬁts everyone. Combined with redistributive universal service funds, community networks, Internet exchanges, it can result on a shared infrastructure to support the need of universal access. Its implementation by the public authorities can vary in terms of policy instruments (municipal ordinances is one), or can come from voluntary adoption (corporate social responsibility actions) by private Internet companies, and the necessary oversight of practices by a global organization. In a speciﬁc case, a city, regional or national authority in a country could authorize, without any damage to the public and social interest, a private provider (such as a telecom operator, an energy company that needs ﬁber to monitor its network, any company willing to connect its different sites) will be able to deploy ﬁber in exchange for giving one part for public use and another for open shared use. Under this model any private investment for the deployment of ﬁber for private needs and beneﬁt would enable public, open-access or alternative telecom operators to reach new places at minimal cost and enable interconnecting the municipal headquarters and services in that city and beyond. Acknowledgements This work was funded by the EU netCommons contract H2020-688768, the Spanish Government contract TIN2016-77836-C2-2-R, the Generalitat de Catalunya as Consolidated Research Group 2017-SGR-990, the Industrial doctorate contract DI-006, and the guiﬁ.net Foundation.
Control Barrier Functions With Unmodeled Dynamics Using Integral Quadratic Constraints<|sep|>This paper presented a method to design control barrier functions (CBFs) that are robust to unmodeled dynamics at the plant input, e.g. unmodeled actuator dynamics or time delays. The approach uses α-IQCs to bound the input/output behavior of the uncertainty. A robust CBF condition is derived using a version of the Gr¨onwall-Bellman lemma. This work was funded by the Ford/U-M Faculty Summer Sabbatical Program. The author acknowledges useful discussions with A. Wiese, Y. Rahman, A. Sharma, D. Sumer, M. Srinivasan, J. Buch, and S.-C. Liao.
Isoscaling in statistical fragment emission in an extended compound nucleus model<|sep|>In conclusion, in the framework of an extended compound nucleus model, isoscaling behavior and its relation to the nuclear symmetry energy is revealed. The symmetry energy coeﬃcient is found to be Csym ∼ 27 MeV from an analysis of theoretical “data”, suggesting that isoscaling data can be interpreted in terms of the symmetry energy. In addition, in this work surface entropy inﬂuences on the isoscaling phenomenon have been studied. The present ECN model approach leaves suﬃcient room for further improved treatment of interesting physical eﬀects [29], such as nuclear expansion. Such a more detailed analysis is required to study the evolution of the symmetry energy with the excitation energy, as has been explored recently also by Shetty et al. [35]. The model will be utilized to deduce the eﬀects of nuclear expansion on the isoscaling parameters and will correspondingly illustrate how to deduce the density dependence of the symmetry energy in such more general and realistic scenarios. Work along this direction is in progress. This work is supported by the U.S. Department of Energy Grant No. DEFG02-88ER40414. The work of W.Y is also partially supported by the NSFC under Grant No. 10405007 and China scholarship council. W.Y is also grateful to Rochester University for support and hospitality extended to him.
Cosmic microwave background anomalies viewed via Gumbel Statistics<|sep|>We have introduced the statistics of sample extremes, Gumbel statistics, to the study of CMB maps. We have shown the statistics to provide a good description of the WMAP data set despite the highly correlated ﬂuctuations therein. We have investigated the use of Gumbel statistics in non-Gaussianity detection, the principal advantage of our method being its lack of restrictions as to the underlying statistics of the CMB. The simplest methods, based on Gumbel statistics, are unlikely to detect any fNL < 1000. There are weak hints of non-Gaussianity at low ℓ, as we can only reproduce the observed statistics some 10% of the time in Gaussian realisations, but no hard evidence. We also ﬁnd that there is a preferred hemispheric cut that optimises the Gumbel discriminant. We emphasize that fNL is a limited and non-generic description of non-Gaussianity. Gumbel statistics can provide a potential probe of generic nonGaussianity in CMB data sets.
Template based Graph Neural Network with Optimal Transport Distances<|sep|>We have introduced a new GNN layer whose goal is to represent a graph by its distances to template graphs, according to the optimal transport metric FGW. The proposed layer can be used directly on raw graph data as the ﬁrst layer of a GNN or can also beneﬁt from more involved node embedding using classical GNN layers. In a graph classiﬁcation context, we combined this TFGW layer with a simple MLP model. We demonstrated on several benchmark datasets that this approach compared favorably with state-of-the-art GNN and kernel based classiﬁers. A sensitivity analysis and an ablation study were presented to justify the choice of several parameters explaining the good generalization performances. We believe that the new way to represent complex structured data provided by TFGW will open the door to novel and hopefully more interpretable GNN architectures. From a practical perspective, future works will be dedicated to combine TFGW with fast GPU solvers for network ﬂow [51]. This would greatly accelerate our approach and more generally OT based deep learning methods. We also believe that the FGW distance and its existing extensions can be used with other learning strategies including semi-relaxed FGW [61] for sub-graph detection. This work is partially funded through the projects OATMIL ANR-17-CE23-0012, OTTOPIA ANR20-CHIA-0030 and 3IA Côte d’Azur Investments ANR-19-P3IA-0002 of the French National Research Agency (ANR). This research was produced within the framework of Energy4Climate Interdisciplinary Center (E4C) of IP Paris and Ecole des Ponts ParisTech. This research was supported by 3rd Programme d’Investissements d’Avenir ANR-18-EUR-0006-02. This action beneﬁted from the support of the Chair "Challenging Technology for Responsible Energy" led by l’X – Ecole polytechnique and the Fondation de l’Ecole polytechnique, sponsored by TOTAL. This work is supported by the ACADEMICS grant of the IDEXLYON, project of the Université de Lyon, PIA operated by ANR-16-IDEX-0005. The authors are grateful to the OPAL infrastructure from Université Côte d’Azur for providing resources and support.
Effects of vertex corrections on diagrammatic approximations applied to the study of transport through a quantum dot<|sep|>In this work we obtain the conductance as a function of both, temperature and gate voltage, for a quantum dot modeled by the Anderson impurity Hamiltonian. We calculate the spectral density of the quantum dot by using the ﬁnite Coulomb repulsion Non Crossing and One Crossing Approximations. The comparison of both schemes let us conclude about the role of vertex corrections when calculating transport properties. At high temperatures, as compared with the hybridization strength, there is no qualitative diﬀerence between NCA and OCA conductances as a function of gate voltage as well as between the occupations of the dot. On the contrary, at low temperatures, the OCA conductance as a function of gate voltage displays a plateau, while the NCA one still shows a dip in the Kondo regime. This results from the underestimated Kondo scale within NCA. When studying the scaling properties, we ﬁnd that the additional processes incorporated by the vertex functions modify the functional temperature dependence of the conductance in addition to the Kondo temperature. Thus, NCA and OCA show diﬀerent scaling behaviors in the whole range of temperatures. Finally, when compared with the NRG results, the OCA conductance as a function of temperature is more reliable than the NCA one. For large values of the Coulomb repulsion, away from the symmetric case, the OCA and NRG conductances agree very well. However, close to the symmetric case, in order to recover the NRG prediction, it is still necessary to go beyond OCA corrections.
Dark Energy Coupled with Dark Matter in Viscous Fluid Cosmology<|sep|>In this paper we have studied examples of Little Rip, Pseudo Rip, and bounce cosmology, described in ﬂat Friedmann-Robertson-Walker spacetime when the cosmic ﬂuid is viscous and is coupled with dark matter. We have found corrections in the thermodynamical parameter, and to the bulk viscosity, in the equation of state for the dark energy. It should be mentioned that there are common properties of the corrections present in these models. Thus ∆ζm(H, t) = ρm/(3H) represents the correction to the bulk viscosity from the coupling. Further, ∆ζρ(H, t) = (A0/3H)ρα is related to the choice of thermodynamical parameter w(ρ) in the equation of state. From a general perspective this paper may be considered as inhomogeneous cosmic ﬂuid theory with changeable thermodynamical parameter w coupled to dark matter Elizalde et al. (2014), applied to the Little Rip, the Pseudo Rip, and the bounce phenomena. It is known, cf. Nojiri and Odintsov (2011), that a viscous ﬂuid may be understood also as a modiﬁed gravity model, for instance of the F(R) type. It is moreover known that F(R) gravity may provide a uniﬁcation of early-time inﬂation with a special version of dark energy, as was proposed by Nojiri and Odintsov (2003), or with the nonlinear model proposed by Nojiri and Odintsov (2008). Having that in mind, we expect that there is a natural possibility to unify these epochs with inﬂation in an extended viscous model. This will be considered elsewhere.
A frequency quintupled laser at 308 nm for spectroscopy of intercombination lines in zinc<|sep|>We developed a novel frequency quintupling laser using a three-stage cascaded conversion scheme resulting in UV laser light around 308 nm. We achieved a maximum output power of 500 µW, and demonstrated linear absorption spectroscopy on an intercombination line in zinc. Thus future applications, like performing spectroscopy on the clock transition of zinc or cooling of zinc or beryllium atoms, may be employed using the proposed scheme. To increase the output power, future work will focus on enhancing the conversion eﬃciency of the last stage. This could be achieved by using the BBO crystal in a single-resonant [8] or double-resonant [9] bow-tie conﬁguration [10]. An enhancement of the output power up to the 100-mW level appears feasible and would generate suﬃcient power for laser cooling applications [11]. With such a resonant enhancement, the alternative approach of frequency quadrupling, 1ν + 4ν = 5ν, could be revisited.
On Reliability-Aware Server Consolidation in Cloud Datacenters<|sep|>Today, the energy consumption of Cloud datacenters (DCs) is one of the most important issues in technology world. Many techniques in different levels have been developed to make these DCs more energy-efﬁcient, which one of them is server consolidation. In this technique, virtual machines (VMs) are packed on the minimum number of physical machines (PMs) and idle PMs are turned off to save energy. However, server consolidation could be utilized considering various parameters and factors, e.g. performance, network trafﬁc, rack inlet temperature, and most recently, hardware reliability. Hardware reliability plays an important role in DC costs. Because ﬁrstly it could cause service outage which is expensive for DC managers. And secondly, it could highly affect the maintenance and replacement costs. In fact, in this paper, in addition to short-term energy savings, we also took long-term reliability and maintenance costs and lifetime of the PMs into account. In this work, we presented a reliability-aware server consolidation approach with the aim of minimizing total DC cost. This total cost consists of total DC energy including PMs, cooling and networking devices in each rack, and VM migration costs and also reliability costs including disk and processor on-off costs. Based on above considerations, we provided a mathematical model in form of Mixed Integer Linear Programming (MILP) which is NP-complete. We ﬁnally evaluated the performance of the proposed mathematical model using extensive numerical MATLAB simulations. As future work directions, there are some interesting open challenges to discover. These days, Software Deﬁned Networking (SDN) is an emerging paradigm which decouples network data plane and control plane. Using its centralized, networkwide abstraction of the control plane, SDN allows policies, conﬁguration, and management of the DC to be applied in efﬁciently in short timescales. Therefore, in this area, there are some worthwhile problems to address, such as, developing SDN-based server consolidation and DC management frameworks, their SDN controller extensions, performance and resilience analysis. After all, considering the real-time nature of DC operation, providing heuristic/meta-heuristic approaches to ﬁnd approximate solutions for the formulated problem can be an interesting challenge to explore.
Efficiency of Exponentiality Tests Based on a Special Property of Exponential Distribution<|sep|>We have proposed in this paper two new tests of exponentiality which use a particular property of the exponential law but are not consistent against any alternative. In the same time they are rather sensitive against the deviations from exponentiality. This is sustained by their high local Bahadur eﬃciency and considerable power under common alternatives. Our tests were able to reject the exponentiality of the sample of reigns of Roman emperors which was claimed by Khmaladze and his coauthors in [10], [19], [20]. We hope that our tests will be useful in other delicate cases when one has to conﬁrm the rejection of exponentiality hypothesis. Finally we have described the structure of ”most favorable” alternatives to exponentiality under which our tests become locally optimal in Bahadur sense.
Free Lunch for Few-shot Learning: Distribution Calibration<|sep|>We propose a simple but effective distribution calibration strategy for few-shot classiﬁcation. Without complex generative models, loss functions and extra parameters to learn, a simple logistic regression trained with features generated by our strategy outperforms the current state-of-the-art methods by ∼ 5% on miniImageNet. The calibrated distribution is visualized and demonstrates an accurate estimation of the feature distribution. Future works will explore the applicability of distribution calibration on more problem settings, such as multi-domain few-shot classiﬁcation, and more methods, such as metric-based meta-learning algorithms. We provide the generalization error analysis of the proposed Distribution Calibration method in Yang et al. (2021).
Applicability of Large Corporate Credit Models to Small Business Risk Assessment<|sep|>Credit risk modeling is murky. While a number of agencies are recognized as rating authorities, the underlying models they use are proprietary and opaque. Further, these models have known deﬁciencies [15]. Yet these models are widely used and represent the best available measures of corporate credit risk. DNN’s can approximate the predictions of these models relatively well, though not entirely, using just ﬁnancial data. And while these models show some predictive capability outside the dataset, it is not clear how signiﬁcant these are. Many extensions of this work are possible. I would like to undertake a model interpretability exercise. Classical ﬁnance relies heavily on certain ﬁnancial statement ratios in assessing credit risk [16]. Our model may conﬁrm some of these are key predictors, or unearth new ones that perform better. Our regression model also needs reﬁnement with more sophisticated architectures and an expansion of the datasets (which were tedious to collect and could be expanded with better access to the right databases.) Finally, with enough data from Experian (we had a limit of 30 companies) we could train models to ﬁt those scores directly, or combine that data with our dataset to train a single model.
FormuLog: Datalog for static analysis involving logical formulae<|sep|>FormuLog is an extension of Datalog that makes it possible to represent, manipulate, and reason about logical formulae. In turn, this enables one to declaratively implement static analyses such as symbolic execution and abstract model checking. While we anticipate that major engineering work will be required to scale FormuLog to real-world static analysis problems, we nonetheless believe that languages like it have the potential to help address some concrete challenges faced by modern static analyses.
Contrastive Entropy: A new evaluation metric for unnormalized language models<|sep|>In this paper we proposed a new evaluation criteria which can be used to evaluate unnormalized language models and showed, using examples, its efﬁcacy in comparing sentence level models among themselves and to word level models. As both WER and contrastive entropy are discriminative measures, we hypothesize that contrastive entropy should have a better correlation with WER as compared to perplexity. We also proposed a discriminatively trained sentence level formulation of recurrent neural networks which outperformed the current state of the art RNN models on our new metric. We hypothesize that this formulation of RNN does a better job at discriminative tasks like lattice re-scoring as compared to standard RNN and other traditional language modeling techniques. We conclude by restating that a metric is meaningful only if it can measure improvements in real world applications. Further experiments evaluating contrastive entropy’s correlation with the WER and BLEU metrics over a wide range of datasets are required to unquestionably demonstrate the usefulness of this metric. Similarly, to establish superior discriminative ability of sentence level RNNs over standard RNNs, we must compare their performance on real word discriminative tasks like n-best list re-scoring.
Variational Depth from Focus Reconstruction<|sep|>In this paper we proposed a variational approach to the shape from focus problem, which can be seen as a generalization of current common shape from focus approaches. It uses an eﬃcient nonconvex minimization scheme to determine depth maps which are based on prior knowledge such as a realistic depth map often being piecewise constant. We showed in several numerical experiments that the proposed approach often yields results superior to classical depth from focus techniques. In future work we will incorporate more sophisticated regularizations in our studies. Additionally, we’ll work on correcting the inherent loss of contrast caused by the total variation regularization by applying nonlinear Bregman iterations in the fashion of [31] to our nonconvex optimization problem. M.B. and C.S. were supported by EPSRC GrantEP/F047991/1, and the Microsoft Research Connections. C.S. additionally acknowledges the support of the KAUST Award No. KUK-I1-007-43. M.M. and D.C. were supported by the ERC Starting Grant ”ConvexVision”.
Distortion of Magnetic Fields in a Starless Core IV: Magnetic Field Scaling on Density and Mass-to-flux Ratio Distribution in FeSt 1-457<|sep|>In the present study, the magnetic ﬁeld scaling on density, |B| ∝ ρκ, was revealed in a single starless core for the ﬁrst time. The index κ was obtained to be 0.78 ± 0.10 toward the starless dense core FeSt 1-457 based on the analysis of the radial distribution of the polarization angle dispersion of background stars measured at the near-infrared wavelengths. The result prefers κ = 2/3 (isotropic contraction), and the diﬀerence of the observed value from κ = 1/2 is 2.8 sigma. The relatively large κ value indicates that the magnetic ﬁeld in FeSt 1-457 is not very strong. This is consistent with the slightly magnetically supercritical feature of the core. The magnetic ﬁeld in FeSt 1-457 can be strong enough to control the contraction of the core, because the magnetic ﬁeld direction of the core is perpendicular to the elongation axis of the core. Observations of ordered magnetic ﬁeld lines around the core also support this conclusion. These results are consistent with the recent theoretical MHD simulation calculated under the slightly magnetically supercritical condition. The total magnetic ﬁeld strengths at the center and boundary of the core are 132 µG and 17 µG, respectively. The boundary value can be used as the estimation of the magnetic ﬁeld strength in the diﬀuse inter-clump medium surrounding the core. On the basis of κ and known density structure, the distribution of the ratio of mass to magnetic ﬂux was evaluated. FeSt 1-457 was found to be magnetically supercritical near the center (λ ≈ 2), whereas nearly critical (slightly subcritical) at the core boundary (λ ≈ 0.98). Thus, the diﬀuse inter-clump medium surrounding the core can also be nearly magnetically critical. Ambipolar diﬀusion regulated star formation models for the case of moderate magnetic ﬁeld strength may explain the physical status of FeSt 1-457. Note that though our obtained index of κ = 0.78 does not ﬁt to the case of strong magnetic ﬁelds, it may not be inconsistent with the moderate magnetic ﬁeld case. The mass-to-ﬂux ratio distribution for typical dense cores (critical Bonnor–Ebert sphere with central λ = 2 and κ = 1/2–2/3) was found to be magnetically critical/subcritical at the core edge, which indicates that typical dense cores are embedded in and evolve from critical/subcritical diﬀuse surrounding medium. We are grateful to the staﬀ of SAAO for their kind help during the observations. We wish to thank Tetsuo Nishino, Chie Nagashima, and Noboru Ebizuka for their support in the development of SIRPOL, its calibration, and its stable operation with the IRSF telescope. The IRSF/SIRPOL project was initiated and supported by Nagoya University, National Astronomical Observatory of Japan, and the University of Tokyo in collaboration with the South African Astronomical Observatory under the ﬁnancial support of Grants-in-Aid for Scientiﬁc Research on Priority Area (A) Nos. 10147207 and 10147214, and Grants-in-Aid Nos. 13573001 and 16340061 of the Ministry of Education, Culture, Sports, Science, and Technology of Japan. RK, MT, NK, KT (Kohji Tomisaka), and MS also acknowledge support by additional Grantsin-Aid Nos. 16077101, 16077204, 16340061, 21740147, 26800111, 16K13791, 15K05032, and 16K05303.
Shape and Angular Distribution of the 4.438-MeV Line from Proton Inelastic Scattering off 12C<|sep|>The most striking improvement in the actual study over previous calculations is the much better reproduction of the measured line shapes in the region of prominent CN resonances and the simultaneous excellent description of γ-ray angular distributions. This is illustrated in Figure 3, where weighted sums P(Ep) in the range Ep = 8.52 - 19.75 MeV, with P(Ep) following a typical interaction probability as encountered in solar-ﬂare γ-ray emission, of calculated and measured line shapes of the Orsay-1997 experiment [13] are shown. Furthermore, data from the Orsay-2002 experiment [14] that extend the energy range of available line shapes down to threshold and up to Ep = 25 MeV are also well reproduced.
Application of Coherent State Approach for the cancellation of Infrared divergences to all orders in LFQED<|sep|>We have demonstrated, using the coherent state formalism, that the true IR divergences in self energy correction cancel to all order in LFQED. Acknowledgements J.M. would like to thank ILCAC for honouring with Mc Cartor Travel Grants for attending LC2014 and would also like to thank the LC 2014 organizers for their kind hospitality. AM would like to thank organizers of LC2014 and Department of Physics, NCSU for their warm hospitality and DAE-BRNS for Grant No. 2010/37P/47/BRNS under which the work was done.
Constraining the formation of black-holes in short-period Black-Hole Low-Mass X-ray Binaries<|sep|>(i) The lower limit on the NK is not greatly affected by the binary evolution of the sources. It is mostly affected by the kinematics of the system, and therefore by the uncertainty on the distance. In this respect, our results are consistent with the ones in Repetto et al. (2012), who calculated lower limits on BH natal kicks basing their study on kinematical arguments. Variations of the assumptions on MB and mass transfer give very similar results. (ii) Even if the lower limit on the NK is not affected by the binary evolution of the system, in order to unravel what are the optimal combinations of NK and mass ejected in the SN, it is necessary to follow the details of the whole evolutionary paths of BHLMXBs. In particular, this method allowed us to ﬁnd binaries consistent with a neutrino-driven NK. (iii) Our work enables us to highlight three possible scenarios for the birth of the BH. Two of these scenarios have been discussed previously in the literature: either the BH does not receive any NK, or it receives a NS-like NK. The third scenario that we suggest, is a BH having formed with a NK and zero baryonic mass ejection. It is the ﬁrst time that this scenario has been applied to the evolution of BH-LMXBs, whereas it was ﬁrst found to be consistent with the formation of few BHs in high mass X-ray binaries (see Valsecchi et al. 2010, Mirabel & Rodrigues 2003). (v) Our population study highlights that, due to the limits of optical spectroscopy, there exists a bias towards BH-LMXBs being close (within 10 kpc) to the Sun. For the same reason, NK estimates are biased towards low/mild NKs (less than 100 − 200 km/s).
Gravitational instability of filamentary molecular clouds, including ambipolar diffusion<|sep|>Perturbation analysis is a powerful tool for exploring diﬀerent kinds of instability in ﬂuid dynamics. In this work, we investigate the eﬀect of AD on gravitational stability of a magnetised ﬁlamentary MC with an isothermal equation of state. We compute numerically the dispersion relation by linearising the governing equations and performing global perturbation analysis. By including AD, we demonstrate that in the low magnetic strength regime, applying non-ideal MHD to the system could not alter its stability, whereas in the highly magnetised one, introduction of AD could destabilise it. It is worthy to mention that AD also destabilises the weakly ionised accretion disks, for example see Kunz & Balbus (2004) where the eﬀects of AD on the magnetorotational instability have been studied. Moreover, we ﬁnd that in the highly magnetised regime, the length scale of the fragmentation λ fast, which is inversely related to the k fast, could be increased at most ∼ 6 per cent. This in turn means the fragmentation mass scale must be increased. As a ﬁnal remark we mention that, in this paper, we consider the global stability of an isothermal ﬁlament. On the other hand, recent observations have shown that interstellar ﬁlaments are not isothermal conﬁgurations (Palmeirim et al. 2013). Therefore a more constructive study can be done by assuming a general density proﬁle, and not necessarily an isothermal one, and investigating the fragmentation in the presence of AD. For such a study in the absence of magnetic ﬁeld eﬀects, we refer the reader to Freundlich et al. (2014). Another possibility is to consider a more general magnetic ﬁeld geometry and also to investigate non-axisymmetric perturbation forms. We leave these issues as a matter of study for future works. The authors would like to thank Sami Dib for valuable and constructive comments. Also M. Hosseinirad thanks Najme Mohammad-Salehi and Maryam Samadi for useful discussions. S. Abbassi acknowledges support from the International Center for Theoretical Physics (ICTP) for a visit through the regular associateship scheme. We also like to thank the anonymous referee for his/her helpful comments which improved the paper.
Tetrahedron in F-theory Compactification<|sep|>Motivated by F-theory- GUT models building along the line of the BHV approach [9, 10, 11] and guided by special properties of the toric ﬁbration of complex surfaces, we have studied in this paper two families of blowing up of the complex tetrahedral surfaces T0. These families, which were respectively denoted as Tn with n ≤ 15 and T ′ k with k ≤ 35 are as follows: 1) the blowing up of the complex three dimension space CP 3 up to ﬁfteen isolated points by projective planes CP 2. Four of these blow ups are of toric type and have been explicitly studied by using the power of the standard toric graph representation and n-simplex description. If denoting by the blowing ups of the CP 3 at n isolated points, then the link between these (CP 3)n,0s and the blown up tetrahedral surfaces Tn is given by means of toric geometry where roughly the Tns appear as their toric boundary; see also footnote (2). Notice that viewed from the CP 3 side, the toric singularity at the tetrahedron vertices P(abc) is given the shrinking of a real 3-torus of the ﬁbration CP 3 ∼ T 3 × ∆CP 3. On the complex tetrahedral surface side however, the visible toric singularity at is given by simultaneous shrinking of three 2- tori namely the shrinking of the toric ﬁbers T 2 a , T 2 b and T 2 c of the respective divisors Da, Db and Dc; see eq(4.6). 2) the blowing up of CP 3 up to thirty ﬁve projective lines. Six of these blow ups are of toric type. These blow ups are diﬀerent from the (CP 3)n,0 ones since they concern the blown up of A1 singularities. We may refer to them as, in order to distinguish them of the previous (CP 3)n,0 family. In this case, the toric singularity living on the tetrahedron edges is associated with the shrinking of a real 2-torus of the ﬁbration CP 3 ∼ T 3 ×∆CP 3 down to S1 as shown on eq(4.8). Viewed from the divisors Da and Db, the singularity on the edge corresponds to the shrinking of a 1-cycle along the intersection of Da and Db. Through this study we learned a set of special features amongst which the two following: a) the toric blown ups Tn and T ′ k of the complex tetrahedral surface T0 are mainly given by intersecting del Pezzo surfaces dPk. This property is expected from general arguments since the blowing of the tetrahedron together with the relations (5.2-5.4), amounts to blowing the divisors Da. But these divisors homeomorphic to CP 2s embedded in CP 3. We have checked this property for the toric blow ups type; but we don’t have yet the answer whether this result is true as well for the non toric blow ups. b) Toric geometry has a nice feature which can be used in the engineering of F-theory GUT- like models building. In going from the faces to the vertices of the tetrahedron, cycles of the toric ﬁbers shrink down as shown in the following table In the ﬁeld theory language, these shrinking generate massless modes which may be interpreted in terms of massless gauge ﬁelds and so gauge symmetry enhancements at the level of the 4D space time eﬀective ﬁeld theory. More precisely, given a gauge symmetry Gr that is visible 4D space time, the gauge symmetry associated with the faces Da of the tetrahedron and its blow ups would be where the U (1) factors may be interpreted in terms of branes wrapping cycles in the toric ﬁbration. The bulk invariance (5.7) gets enhanced to a Gr+1 × U (1) invariance on the edges Σ(ab) and further to a Gr+2 gauge symmetry at the vertices P(abc). In the case where Gr = SU (5) for example, the gauge enhanced symmetry on the edges could be either SU (6) or SO (10) and at the vertices it may be one of the following enhanced gauge symmetries We end this conclusion by adding a comment regarding the way the tetrahedron surface and its blown up cousins Tn and T ′ k could be used in practice. They should be thought of as the base surface of the elliptically K3 ﬁbered Calabi-Yau four- folds in the F-theory compactiﬁcation to 4D space time, These complex surfaces are wrapped by seven branes with intersections along the edges and at the vertices. On the edges localize chiral matters Φa Ra in bi-fundamental representations while at the vertices of the toric graphs live a 4D N = 1 supersymmetric Yukawa couplings with chiral potential,
Knowledge-Augmented Language Models for Cause-Effect Relation Classification<|sep|>GLUCOSE. Our results show that commonsense knowledge-augmented PLMs outperform the original PLMs on cause-effect pair classiﬁcation and answering commonsense causal reasoning questions. As the next step, it would be interesting to see how the previously proposed model improvement methods or using unbiased ﬁne-tuning datasets can potentially enhance the performance of our knowledge-augmented models.
Query Language for Complex Similarity Queries<|sep|>The two main contributions of this paper are the analysis of requirements for a query language, and the proposal of a query language for retrieval over complex data domains. The presented language is backed by a general model of data structures and operations, which is applicable to a wide range of search systems that oﬀer diﬀerent types of content-based functionality. Moreover, the support for data indexing and query optimization is inherently contained in the model. The SimSeQL language extends the standard SQL by new primitives that allow to formulate content-based queries in a ﬂexible way, taking into account the functionality oﬀered by a particular search engine. The proposal of the language was inﬂuenced by the MESSIF framework that oﬀers the functionality of executing complex similarity queries on arbitrary index structures but lacks a user-friendly interface for advanced querying. Having laid the formal foundations of the query interface here, we will proceed with the implementation of a language parser which will translate the query into MESSIF for the actual evaluation. In the future, we plan to research the possibilities of adapting the existing optimization strategies to utilize the reformulation capabilities of the proposed
Ergodic and localized regions in quantum spin glasses on the Bethe lattice<|sep|>We studied the localization properties of the transverse-ﬁeld Ising spin glass model on the 3-regular random graph in the limit where the trasverse-ﬁeld is weak compared to the disordered interactions. This model is known to exhibit a transition from a paramagnetic to a glassy phase at low temperatures and weak transverse-ﬁeld. The classical Ising spin glass model is widely believed to capture the complicated combinatorial structure of general NP-hard computational problems while the zero-temperature, weak transverse-ﬁeld regime describes the ﬁnal stage of a quantum annealing protocol designed to ﬁnd the ground-state energy of the Ising spin glass. Many-body localization has been argued to be an obstacle to eﬃcient quantum annealing due to the presence of exponentially-closing gaps in the localized phase. We computed numerically the many-body mobility edge of the system in the forward approximation, ﬁnding that the energy eigenstates of the system indeed localize for small values of the transverse ﬁeld at ﬁnite system sizes. When plotted against the equibilbrium phase diagram of the model, we discovered that the localized region does not coincide with the glassy phase. In particular, evidence points to the fact that the glassy phase is partitioned into a delocalized region and a localized one. We conjecture that the glassy, delocalized region will exhibit the same clustering of eigenstates observed in [27] for the p-spin model, where the eigenstates were found to form clusters inside of which the energies are distributed according to Wigner-Dyson while the global distribution of the energy levels of the model is Poissonian. Moreover, we expect that classical methods that exploit the ﬁne-tuning of thermal relaxation (such as simulated annealing) will perform poorly in the entire glassy phase while quantum annealers will perform poorly only once localization sets in. Therefore we conjecture that in the glassy, delocalized region of the phase space quantum annealing algorithms can outperform any classical thermal annealing protocol. A natural future direction outlined by our work would be to check whether the same localization/delocalization transition is present when the disordered term of the Hamiltonian encodes a real-life computational problems such as 3SAT. In the aﬃrmative case, a detailed comparison of the performance of e.g. simulated annealing and quantum annealing (either simulated numerically or by an actual experiment) inside of the region that is both glassy and delocalized would help shed light on the realistic capabilities of quantum annealers over classical thermal annealing and other algorithms based on stochastic local optimization.
Asynchronous Snapshots of Actor Systems for Latency-Sensitive Applications<|sep|>The actor model has become popular for implementation of responsive server applications. Unfortunately, many snapshotting approaches are blocking, and cause applications to become unresponsive for the duration of snapshot creation. In this paper, we presented a novel approach for creating asynchronous snapshots of actor programs transparently and without VM modifications or GC integration. Our approach uses the isolation of state of the actor model to reduce snapshot latency by capturing partial heaps and allowing actors to make progress before all their state is persisted. We evaluated the impact of our snapshotting approach on application latency with the Acme Air benchmark. Our results show that with frequent snapshotting enabled, the latency of most requests remains below 100ms. Only 0.007% of 20 million requests had a latency over 100ms. While the number of such requests did increase by 5.43%, their total is still small. We conclude that our approach does not negatively impact the user experience of such a web service. Future Work As future work, we plan to apply our approach to JavaScript with a wider range of benchmarks. This would also enable a direct comparison with Jardis. Time Travel Debugging. While our snapshotting can already be combined with record & replay, further research is needed to enable time travel debugging. For example, restoring a snapshot currently requires a fresh start of the program. This is a problem as time travel debugging requires frequent snapshot restoration, which means, we need to be able to replace the whole application state within a VM. Application Redeployment. To be practical for moving applications between machines, our approach has to be enhanced so that required resources are also moved. This could be achieved by bundling the source code of all loaded classes with the snapshot and tracking all used external resources so they can be made available in the new environment. This research is funded in part by a collaboration grant of the Austrian Science Fund (FWF) and the Research Foundation Flanders (FWO Belgium) as project I2491-N31 and G004816N.
Technology Ethics in Action: Critical and Interdisciplinary Perspectives<|sep|>A  sociotechnical  lens  on  tech  ethics  will  not  provide clear answers for how to improve digital technologies. The technological, social, legal, economic, and political challenges  are  far  too  entangled  and  entrenched  for simple  solutions  or  prescriptions.  Nonetheless,  a sociotechnical  approach  can  help  us  reason  about  the benefits and limits of tech ethics in practice. Doing so will  inform  efforts  to  develop  rigorous  strategies  for reforming digital technologies. That  is  the  task  of  this  special  issue: “Technology Ethics  in  Action:  Critical  and  Interdisciplinary Perspectives”. The articles in this issue provide a range of perspectives regarding the value of tech ethics and the desirable  paths  forward.  By  interrogating  the relationships  between  ethics,  technology,  and  society, we hope to prompt reflection, debate, and action in the service of a more just society. B. Green thanks Elettra Bietti, Anna Lauren Hoffmann, Jenny  Korn,  Kathy  Pham,  and  Luke  Stark  for  their comments  on  this  article.  B.  Green  also  thanks  the Harvard STS community, particularly Sam Weiss Evans, for feedback on an earlier iteration of this article.
On the Exponentially Weighted Aggregate with the Laplace Prior<|sep|>We have considered the model of regression with ﬁxed design and established risk bounds for the exponentially weighted aggregate with the Laplace prior. This class of estimators encompasses important particular cases such as the lasso and the Bayesian lasso. The risk bounds established in the present work exhibit a range of values for the temperature parameter for which the EWA with the Laplace prior has a risk bound of the same order as the lasso. This oﬀers a valuable complement to the negative results by Castillo et al. (2015), which show that the Bayesian lasso is not rate-optimal in the sparsity scenario. Note that the Bayesian lasso corresponds to the EWA with the Laplace prior for the temperature parameter τ = σ2/n, where σ2 is the variance of the noise. Our results imply that in order to get rate-optimality in the sparsity scenario, it is suﬃcient to choose τ smaller than σ2/(np). We have extended the result outlined in the previous paragraph in two directions. First, we have shown that one can replace the pseudo-posterior mean by any random sample from the pseudo-posterior distribution. This eventually increases the risk by a negligible additional term, but might be useful from a computational point of view. Second, we have established risk bounds of the same ﬂavour in the case of trace-regression, when the unknown parameter is a nearly low-rank large matrix. This result extends those of (Koltchinskii et al., 2011) and uniﬁes the risks bounds leading to the “slow” and “fast” rates. Furthermore, our result oﬀers an interpolation between these two extreme cases, see the discussion following Theorem 3. With some additional work, all the results established in the present work can be extended to the model of regression with random design. Furthermore, the case of a partially labelled sample can be handled by coupling the methodology of the present work with that of (Bellec et al., 2016a). An interesting line of future research is to apply our approach to other priors constructed from convex penalties such as the mixed ℓ1/ℓ2-norm used in the group-lasso (Yuan and Lin, 2006), or the weighted ℓ1-norm of ordered entries used in the slope (Bogdan et al., 2015). Another highly relevant and challenging topic for future work will be to investigate the computational complexity of various methods for approximating the pseudo-posterior mean or for drawing a sample from the pseudo-posterior density.
Global Synchronization of Pulse-Coupled Oscillator Networks Under Byzantine Attacks<|sep|>Due to unique advantages in simplicity, scalability, and energy efﬁciency over conventional packet-based synchronization approaches, pulse-based synchronization has been widely studied recently. However, all existing attack resilient pulsebased synchronization results are obtained either under all-toall coupling topology or restricted initial phase distributions. In this paper, we propose a new pulse-based interaction mechanism to improve the resilience of PCO networks against Byzantine attackers. The new mechanism can enable synchronization in the presence of multiple Byzantine attackers even when the PCO network is not restricted to all-to-all and the initial phases are distributed arbitrarily. This is in distinct difference from most of the existing attack resilience algorithms which require a priori (almost) synchronization among all legitimate oscillators. The approach is also applicable when the Figure 9: The length of the containing arc of 22 legitimate oscillators under Mechanism 2 and approaches in [32]–[35] in the presence of 2 Byzantine attackers (oscillators 1 and 8). The attack pulse time instants were represented by asterisks. The coupling strength in [32]–[35] was set to l = 1, N was unknown to individual oscillators, and ε was set to 0.01T. total number of oscillators is unknown to individual oscillators. Numerical simulations conﬁrmed the analytical results. In future work, we plan to relax the condition that all legitimate oscillators start at the same time instant and allow different oscillators to be turned on at different time instants.
Technical Report: A Receding Horizon Algorithm for Informative Path Planning with Temporal Logic Constraints<|sep|>In this paper, we considered planning an informative path for a robotic agent subject to temporal logic speciﬁcations. We modeled the robot as moving deterministically on a graph with noisy sensor measurements at each node. We proposed a receding horizon algorithm for solving this problem in an on-line, computationally efﬁcient manner while still ensuring speciﬁcation satisfaction. We compared the performance of our algorithm with an off-line exhaustive search method in a simulation study. Our algorithm out performed the exhaustive search method, producing lower entropy estimates with less computational overhead. One natural extension to this work is to plan a path that optimizes some other quantity (e.g. path length or graph distance) subject to a minimum level of mutual information. That is, we make the information content of the path a constraint rather than an objective. Another possible extension is to consider cases in which the satisfaction of the temporal logic speciﬁcation relies on some unknown quantity. Consider, for instance, a rescue robotics scenario in which the robot is tasked not only with ﬁnding survivors, but also moving the survivors to a medical station. In this case, planning a path to the medical station is impossible until the robot knows the survivor’s location. This extension would allow us to use formal synthesis methods in a more Fig. 2. Histograms of (a) the pmf of the terminal entropy found when following the path from Algorithm 1 and (b) the empirical pmf of the terminal entropy that resulted when the paths were calculated using Algorithm 2. These histograms show that the mean and variance of the pmf of the terminal entropy is lower for the paths generated by Algorithm 2 than the for the path generated by Algorithm 1. The lower mean indicates that using Algorithm 2 will result in a lower entropy estimate on average. The lower variability means that we are less likely to have a high entropy estimate when using Algorithm 2. Algorithm 1 took 1741 s of CPU time to complete and Algorithm 2 took an average of 2.94 s of CPU time per execution to complete. reactive manner. More generally, we expect that the fusion of information theoretic tools with formal control synthesis will yield robotic control policies that are reactive to noisy, realworld environments while still providing provably correct performance.
Early and Late-time Cosmic Acceleration in Non-minimal Yang-Mills-$f(G)$ Gravity<|sep|>To summarize, the non-minimal gravitational coupling of YM ﬁeld with Gauss-Bonnet invariant function, f(G), has been considered in Friedmann-Robertson-Walker background metric. Such a non-minimal coupling has been examined in the framework of general relativity. We have shown that power law inﬂation can be realized due to non-minimal coupling of YM ﬁeld in this model which is described by action (1). We have also studied cosmology in non-minimally coupled YM ﬁeld in the framework of modiﬁed Gauss-Bonnet gravity, F(G). It has been shown that both inﬂation and late-time acceleration of the universe can be realized in such a model proposed in Ref. [32]. Clearly, more checks of this theory such as stability/instability of inﬂation should be done in order to conclude if the model is realistic or not. The conditions for stability of f(G) rarity have been derived in [17]. It has been shown that the condition d2f dG2 > 0 needs to be fulﬁlled in order to ensure the stability of a late-time de-sitter solution as well as the existence of standard radiation and matter dominated epochs [17]. Studying stability/instability conditions for our model and models of this kind such as the Maxwell-f(G) model [31] will be our plan for future works. It is also interesting to extend our formulation for more complicated theories. For instance one can investigate non-minimal coupling of YM Lagrangian with non-local f(G) gravity proposed in [39] or one can study our model in the case that instead of f(G) gravity an action with higher order string loop corrections replaced. This kind of superstring inspired action has been considered in [40].
Effect of Cd2+ on the Growth and Thermal Properties of K2SO4 crystal<|sep|>Pure K2SO4 and Cd doped K2SO4 crystals can be grown from aqueous solution by the slow evaporation technique. The incorporation of Cd is weak: From solutions with 4 wt.% CdSO4 one obtains Cd2+:K2SO4 crystals with only 0.014 wt.% of the dopant. Nevertheless, the inﬂuence of cadmium is remarkable: Cd doped crystals contain less water, and the remaining water is loosely bond compared with water in undoped crystals. The optical transparency of Cd2+:K2SO4 in the UV region is larger. Acknowledgements S. Bin Anooz was awarded a scholarship from the “Germany Academic Exchange Service” (DAAD) for this work that is gratefully acknowledged.
Stochastic Bound Majorization<|sep|>We have proposed a new stochastic bound majorization method for optimizing the partition function of log-linear models that uses second-order curvature through a global bound (rather than a local Hessian). The method is obtained by applying Sherman-Morrison to the batch update rule to convert it into an iterative summation over the data which can easily be made stochastic by interleaving parameter updates. This (full-rank) stochastic method requires no parameter tuning. A low-rank version of this stochastic update rule makes this effectively second-order method remain linear in the dimensionality of the data. We showed experimentally that the method has signiﬁcant advantage over the state-of-the-art ﬁrst-order stochastic methods like SGD and ASGD making majorization competitive in both stochastic and batch settings [27]. Stochastic bound majorization achieves convergence in fewer iterations, in less computation time (when using the low-rank version), and with better ﬁnal solutions. Future work will involve providing theoretical guarantees for the method as well as application to deep architectures with cascaded linear combinations of soft-max functions.
Intriguing microstructures of five-dimensional neutral Gauss-Bonnet AdS black hole<|sep|>In this paper, we analytically studied the thermodynamic phase transition and Ruppeiner geometry for the ﬁvedimensional neutral GB-AdS black hole. Combining with the phase transition, we understood the microscopic properties of the black hole by analyzing the behavior of the scalar curvature of the Ruppeiner geometry along the coexistence curve and near the critical point. By using analytical form of the coexistence curve of the small and large black holes, we showed the phase structure of the black hole in the ˜P- ˜T and ˜T- ˜V diagrams, respectively. Especially, in the ˜T- ˜V diagram, two metastable black hole phases, the superheated small black hole phase and supercooled large black hole phase, were displayed. The spinodal curves of the small and large black holes were also shown in the phase diagrams, along which the heat capacity diverges. We also obtained the thermodynamic volumes along the saturated small and large black holes. Their change ∆ ˜V decreases with the phase transition temperature or pressure, and vanishes at the critical point. The result also shows that ∆ ˜V has a universal exponent 1 2 at the critical point. Therefore, ∆ ˜V can serve as an order parameter to describe the small-large black hole phase transition. Then we constructed the Ruppeiner geometry and calculated the scalar curvature. Following Ref. [33], we adopted the normalized scalar curvature RN to test the microstructure of the ﬁve-dimensional neutral GB-AdS black hole. In order to give a clear comparison of the properties of RN for the VdW ﬂuid, charged AdS black hole, and ﬁvedimensional neutral GB-AdS black hole, we summarize the results in Table I. From the table, we ﬁnd some results of RN for the ﬁve-dimensional neutral GB-AdS BH: i) RN is negative, which indicates attractive interaction of the microstructure of the thermodynamic systems. ii) At the critical point, RN goes to negative inﬁnity. iii) RN has a critical exponent 2 and RN(1 − ˜T)2 = − 1 8. One more intriguing property we observed for the neutral GB-AdS black hole is that, the normalized scalar curvature RN shares the same form for both the saturated small and large black holes. As we know, when the black hole system crosses the ﬁrst order phase transition, its microstructures will experience a signiﬁcant change. However, from the result of the normalized scalar curvature RN, these two diﬀerent black hole systems with diﬀerent microstructures have the same interaction. This reveals a particular interesting nature for the AdS black hole in GB gravity. In summary, we analytically investigated the property of the microstructure for the neutral AdS black hole in GB gravity, by combining with the thermodynamic phase transition and Ruppeiner geometry. The results show that the attractive interaction is dominant for the black hole system. Of particular interest is that during the small-large black hole phase transition, the scalar curvature implies that the interaction keeps unchanged while the microstructure of the black hole system does change. This is an interesting and important result for the black hole in the GB gravity. Our study is also worthwhile generalizing to charged and higher-dimensional AdS black hole in GB gravity. However, there may be no analytic result, and one needs to use the numerical calculation. Nevertheless, these will strengthen our knowledge on the black hole microstructures.
Towards efficient representation identification in supervised learning<|sep|>In this work, we analyzed the problem of disentanglement in a natural setting, where latent factors cause the labels, a setting not well studied in the ICA literature. We show that if ERM is constrained to learn independent representations, then we can have latent recovery from learnt representations even when the number of tasks is small. We propose a simple two step approximate procedure (ERM-ICA) to solve the constrained ERM problem, and show that it is effective in a variety of experiments. Our analysis highlights the importance of learning independent representations and motivates the development of further approaches to achieve the same in practice. We thank Dimitris Koukoulopoulos for discussions regarding the proof for indentiﬁcation under a single task. Kartik Ahuja acknowledges the support provided by IVADO postdoctoral fellowship funding program. Ioannis Mitliagkas acknowledges support from an NSERC Discovery grant (RGPIN-2019-06512), a Samsung grant, Canada CIFAR AI chair and MSR collaborative research grant.
Cosmological constraints for the Cosmic Defect theory<|sep|>We have submitted the CD theory to a consistency test with respect to three main cosmological constraints: primordial nuclear isotopic abundances; large scale structure formation in the universe; luminosity distances of type Ia supernovae. One relevant quantity, the sound horizon, deducible from the CMB anisotropy data has also been evaluated. According to the CD theory space-time is endowed with a strain energy density, whose presence in the
An Efficient Optimal Planning and Control Framework For Quadrupedal Locomotion<|sep|>In this work, we investigated a framework for legged robot optimal motion planning and control. The main contributions of our approach are: switch system modeling for the legged robot, a multi–level optimization approach for optimizing the switching time and the continuous control inputs, and ﬁnally the development of a constrained SLQ algorithm. Our switched system formulation facilitates us to use more efﬁcient tools for solving the optimal control problem in legged robots. In this paper, we have chosen a multi– level optimization approach to ﬁnd the switching times and the continuous control inputs. As we have shown in our experiments, this optimization approach converges rapidly on both levels. This manifests the discussion that the two– level optimization framework decomposes the problem in two simpler optimizations which potentially can be solved more efﬁciently with existing algorithms. Finally in this work, for the ﬁrst time we proposed a continuous–time, constrained DP approach based on the SLQ framework. This algorithm similar to its unconstrained ancestors has O(n) time–complexity with the advantage that it can also handle state and input equality constraints. This work not only extends the state-of-the-art available SLQ algorithm to the constrained problems but also introduces a new constrained, time–varying LQR solver. This constrained LQR method can be also used for designing the feedback controller for the open–loop planners such as the ones in the TO framework. One of the immediate extensions to our constrained SLQ algorithm is to use an active set approach in order to incorporate the state and input inequality constraints. Furthermore, we are planning to implement this approach on our robot, HyQ, using an MPC framework. This research has been supported in part by a Max-Planck ETH Center for Learning Systems Ph.D. fellowship to Farbod Farshidian and a Swiss National Science Foundation Professorship Award to Jonas Buchli and the NCCR Robotics.
An Efficient Schmidt-EKF for 3D Visual-Inertial SLAM<|sep|>In this paper, we have developed the high-precision, efﬁcient SEVIS algorithm that adapts the SKF formulation for long-term visual-inertial SLAM. In particular, the probabilistic inclusion of map features within SEVIS allows for Figure 6: Selected views during the night multi-ﬂoor trajectory show the high noise, poor lighting conditions, and motion blur that greatly challenge visual feature tracking. Figure 7: Estimated trajectories of the baseline VIO (blue) and SEVIS (black) show the improved performance due to inclusion of map features. The start-end positions are denoted with a green square and red diamond respectively. bounded navigation drifts while retaining linear computational complexity. To achieves this, the keyframe-aided 2D-to-2D feature matching of current visual measurements to 3D map features greatly facilitates the full utilization of the map information. We then performed extensive MonteCarlo simulations and real-world experiments whose results showed that the inclusion of map features greatly impact the long-term accuracy while the proposed SEVIS still allows for real-time performance without effecting estimator performance. In the future, we will investigate how to reﬁne the quality of map features added for long-term localization and further evaluate our system on resource-constrained mobile sensor systems.
Enhanced Transfer Learning Through Medical Imaging and Patient Demographic Data Fusion<|sep|>Our results show that adding metadata to image features can signiﬁcantly enhance classiﬁcation performance in transfer learning. We observe that when metadata is used in the classiﬁcation performance generally improves or remains the same for a range of convolutional architectures as assessed by several evaluation metrics. This indicates that this may be a general property in classiﬁcation of images with deep neural networks. The performance enhancement depends on the type of data and the speciﬁc conﬁguration of the transfer learning. Our experiments indicate that natural images (e.g. the ISIC data) typically exhibit greater enhancements with networks pre-trained on ImageNet directly as a feature extractor and retraining the classiﬁcation layer; whereas non-natural image representation of other data (e.g. PTB XL) beneﬁt from ﬁne tuning the network ImageNet weights. This may arise from the similarity of characteristics in the target data with the
Deep Ordinal Regression for Pledge Specificity Prediction<|sep|>In this work we present a new dataset of election campaign texts, annotated with pledge speciﬁcity on a ﬁne-grained scale. We study the use of deep ordinal regression approaches using an auxiliary uni-modal distributional loss for this task. The proposed approaches provide large gains in performance under both supervised and semi-supervised settings. Speciﬁcity weight across policy issues beneﬁts ideology prediction and also better captures issue salience, compared to the traditional policy theme count-based representation. This aligns with previous studies done based on manual annotations (Praprotnik, 2017). In future work, we aim to expand this study to multiple languages. We thank Robert Thomson for his inputs on the task deﬁnition. This work was funded in part by the Australian Government Research Training Program Scholarship, and the Australian Research Council.
A model of discrete Kolmogorov-type competitive interaction in a two-species ecosystem<|sep|>Three related aspects of the dynamics of the complex, discrete-time Kolmogorov system (3)  were studied in this paper through numerical simulation. We find the inter-species coupling  coefficients  2c and  3c  to be important determinants in the long-term dynamics and consequent emergence of patterns in the “toy” model. When their values are chosen in close  proximity to each other, or are chosen to be identical, the dynamics of the system (3) enters a  chaotic regime, indicated both, by the bifurcation diagrams and the Lyapunov exponents for  the system. Existence of chaos in the evolution of populations of the competing species may  therefore be considered as an emergent feature in the long-term behaviour of the ecosystem. In this context, it is worth noting that a discrete dynamical model of competitive outcomes  mentions emergent dynamical scenarios even in contradiction to the well-known Competitive  Exclusion Principle [23]. However, we observe for values of the above coefficients chosen distant apart, the dynamical  behaviour of the competitive outcomes remain regular. It may thus be conjectured that the  two components of the ecosystem, namely the populations of species A and B, would  continue to competitively coexist through the long-term evolution of the system (3), provided  that the intra-specific competitive interactions have an appreciable difference in strengths,  thus yielding the emergent phenomenon of population coexistence in the long-term dynamics  of the ecosystem.
Sampling Strategy for Fine-Tuning Segmentation Models to Crisis Area under Scarcity of Data<|sep|>Our main conclusion is that selection of samples for ﬁnetuning does matter if the size of ﬁne-tuning dataset is highly limited. With random samples, we can get lucky or not, which should not be the basis for crisis response decisions. If deep models for building detection are to be included in the toolset of humanitarian mission teams in the ﬁeld, they should be adaptable, ﬂexible, and allow users to understand decisions. The role of building detection task in humanitarian response is undeniable – information about detected buildings is being used, for example, to estimate region populations. This knowledge guides humanitarian efforts in distribution of food, water and other basic resources for people affected by the crisis, Figure 6: Bar chart with differences between IoU of two sampling strategies for all sizes of ﬁne-tuning dataset. Blue bars indicates that the score was higher for prioritization strategy and red bar for random strategy. We can see that when the more data is available the better is model trained with randomly selected samples but the less samples are available ﬁne-tuning with samples priorities gives better results. Figure 7: Bar chart with differences between IoU of two sampling strategies for initial seven sizes of ﬁne-tuning dataset: that consecutively contains 250, 350, 450, 550, 650, 750, 850 samples. Blue bars indicates that the score was higher for prioritization strategy and red bar for random strategy. We can see that at least initial 20% of samples might have a huge role in ﬁne-tuning. and for creating strategies for epidemiology prevention. The search for a method that minimizes the number of required labeled instances for ﬁne-tuning, based on a smart prediction of their importance may therefore have a big impact on adoption of segmentation models into current humanitarian workﬂows.
Service-Based Drone Delivery<|sep|>We presented a novel paradigm for service-based drone delivery to utilize a multi-route skyway infrastructure in drone deliveries. We highlighted the beneﬁts of service-based drone deliveries from service providers and consumers perspectives. We then proposed a service-oriented framework that utilizes a sensor-cloud infrastructure for optimal drones operations in a smart city. Finally, we reviewed current solutions and open challenges, as well as future visioned directions for research in service-based drone deliveries. This research was partly made possible by DP160103595 and LE180100158 grants from the Australian Research Council. The statements made herein are solely the responsibility of the authors.
Strongly Universal Reversible Gate Sets<|sep|>We have been able to precisely determine the revital generated by a ﬁnite set of generators over an even order alphabet and show that over an odd alphabet, a ﬁnite collection of mappings generates the whole revital. The ﬁrst result conﬁrms a conjecture in [2] and the second gives a simpler proof of the same result from that paper. Moreover, we have shown that the alternating conservative revital is ﬁnitely generated on all alphabets, but the conservative revital is never ﬁnitely generated. The methods are rather general: We have developed an induction result (Lemma 2) for ﬁnding generating sets for revitals of controlled permutations, allowing us to determine ﬁnite generating sets for some revitals with uniform methods. We also prove the nonexistence of a ﬁnite generating family for conserved gates with a general method in Theorem 5, when borrowed bits are not used. We only need particular properties of the weight function in the proof of Theorem 4, where it is shown that the (usual) conservative revital is not ﬁnitely generated even when borrowed bits are allowed. In [1] the full list of reversible gate families in the binary case is listed, when the use of auxiliary bits is allowed. This includes the conservative revital, various modular revitals and nonaﬃne revitals. As we do not allow the use of auxiliary bits, we are not limited to these revitals; still, it is an interesting question which of them are ﬁnitely generated in our strict sense. While this paper develops strong techniques for showing ﬁnitely generatedness and non-ﬁnitely generatedness of revitals, our generating sets are rather abstract, and do not correspond very well to known generating sets. It would be of value to replace the constructions found by computer search in section 8 by more understandable constructions, in order to ﬁnd more concrete generating sets in the case of general alphabets in the case of conservative gates.
Shortcuts to adiabaticity applied to nonequilibrium entropy production: An information geometry viewpoint<|sep|>In conclusion, we have discussed nonequilibrium properties of thermally-isolated systems by using STA. The main conclusions in this paper are (i) STA is applicable to any dynamical systems, (ii) the entropy production is separated into two parts and the information-geometric interpretation is possible and (iii) the entropy production has a lower limit which is used to derive a trade-oﬀ relation. We have stressed that the idea of STA is applicable to any nonequilibrium processes. The property that the Hamiltonian is separated into two parts is directly reﬂected to the nonequilibrium entropy production. The Pythagorean theorem opens up a novel perspective in studying the nonequilibrium systems. Separation of the Hamiltonian can be used not only to solve the dynamical problems but also to characterize the nonequilibrium properties. It will also be useful to ﬁnd an eﬃcient algorithm for a dynamical system. The lower bound of the entropy production gives a new type of trade-oﬀ relation between time and a notion of distance to equilibrium. To derive the lower bound, we used the improved Jensen inequality. Although there is no physical meaning of this inequality, the lower limit is represented by work ﬂuctuations and is related to the geometric distance of two states. It is interesting to note that the entropy plays a role of velocity. This can be understood intuitively since the entropy becomes small for a quasistatic process where a large time is required to change the state to a diﬀerent one. Although it is still a diﬃcult problem to ﬁnd the proper separation of a given general Hamiltonian, we can invent, for example, a new approximation method from an information-geometric point of view. Actually, the projection theorem is utilized to ﬁnd an optimized solution in the problem of information processing. The present work is only the beginning for applications of the concept of the information geometry to nonequilibrium dynamics. We expect that we can ﬁnd a new eﬃcient algorithm based on a picture that we discussed in this paper. The author is grateful to Ken Funo, Tomoyuki Obuchi and Keiji Saito for useful discussions and comments. This work was supported by JSPS KAKENHI Grant No. 26400385. where ⟨ ⟩ denotes the average with respect to X. The standard Jensen inequality is obtained by setting f(X) = eX. To improve the inequality, Decoster used the convex function [49] where N is integer. The case N = 1 gives the standard inequality ⟨eX⟩ ≥ e⟨X⟩. Here we take N = 2 to improve the inequality. By using the replacement X → X − ⟨X⟩ + α where α is real, we have To ﬁnd the tightest inequality, we choose α so that the right-hand side of this equation is maximized. We ﬁnd α = −⟨(X − ⟨X⟩)3⟩/(3⟨(X − ⟨X⟩)2⟩) and obtain
Density of warm ionized gas near the Galactic Center: Low radio frequency observations<|sep|>Deep observations of the GC region at 0.154 and 0.255 GHz bands with the GMRT have allowed detection of 62 small diameter sources, most of which appear to be extragalactic. Their intrinsic sizes and scattering diameters at λ = 1 m were derived from multifrequency observations. Derived scattering diameter is 30′′ ± 2′′.3 at the GC and goes down with a slope of 27′′±2′′.5 per degree within about 1◦ from the GC. Considering the scattering size towards the sources seen within 1◦ from the GC, we estimate an electron density of ∼ 1 cm−3 in the region as responsible for the scattering. Low frequency free-free absorption towards the extended sources within 0.6◦ from the GC shows high absorption at 0.154 GHz. Electron density of the ionized gas responsible for absorption is likely to have ne ∼ 10 cm−3. The probability of encountering this absorbing screen is found to be ∼90% within the region. Following the Hyperstrong scattering model (NE2001), this ionized medium would cause heavy scatter broadening of the EG sources and they would be resolved out in 1.4 GHz interferometric observations. However, we ﬁnd 8 out of 10 sources expected are still present in the 1.4 GHz catalog. Ionized surfaces of molecular clouds are thought to provide the required high electron density gas that causes scattering and low radio frequency absorption. Emission from these clouds as traced by CS J = 1 − 0 transition shows that these clouds are located close to the Galactic plane with an width of ∼ 0.1 − 0.2◦. Since most of the extended Galactic objects discussed here are close to the Galactic plane, they are mostly seen in absorption. EG sources observed towards 359.5◦ < l < 0.5◦ and −0.5◦ < b < 0.5◦ of the Galaxy are seen through the so called Hyperstrong scattering screen, but most of them are not seen through the dense molecular clouds near the GC and hence are not resolved out due to heavy scattering through the molecular cloud. I thank the staﬀ of GMRT that allowed these observations to be made. GMRT is run by National Centre for Radio Astrophysics of the Tata Institute of fundamental research. A. Pramesh Rao was involved with this project, and made the 0.154 GHz observations with the GMRT. I thank him for his contribution including useful comments in improving the manuscript. I also thank the anonymous referee for useful comments in improving the quality of the paper and Aritra Basu for going through the manuscript. This research has made use of the NED which is operated by the Jet Propulsion Laboratory, California Institute of Technology, under contract with the National Aeronautics and Space Administration.
Response Aware Model-Based Collaborative Filtering<|sep|>In this paper, we propose two response models, rating dominant and context-aware response models, to capture users’ response patterns. Further, we unify the response models with one of famous collaborative ﬁltering model-based methods, the Probabilistic Matrix Factorization, to establish the Response Aware Probabilistic Matrix Factorization framework (RAPMF). The RAPMF also generalizes PMF as its special case. Empirically, we verify the performance of RAPMF under carefully designed experimental protocols and show that RAPMF performs best when it tries to fulﬁll the ultimate goal of real-world recommender systems, i.e., recommending items to those who may be interested in. The empirical evaluation demonstrates the potential of our RAPMF model in real-world recommender system deployment. There are several interesting directions worthy of considering for future study. One direction is to study how to model the response when the response patterns are hidden. The second fascinating avenue is to study how to speed up our RAPMF through parallelization, online learning, or sampling techniques. The third direction is to design a smart way to eﬃciently tune the hyper-parameters or to design the learning scheme to automatically learn the model parameters.
Investigation of activation cross-section data of proton induced nuclear reactions on rhenium<|sep|>Activation cross-sections of proton induced nuclear reactions on rhenium were measured for 12 reaction products up to 70 MeV, out of them data for 7 reactions are presented here for the ﬁrst time. Model calculations were done by using the EMPIRE and Alice-IPPE codes. The results were also compared with the data of the TALYS based TENDL 2011 online library. The predictions of theoretical calculations could be considered only moderately successful, especially for isomeric
An organ deformation model using Bayesian inference to combine population and patient-specific data<|sep|>We have implemented and evaluated two Bayesian methods for modelling organ deformation occuring during RT treatment. The NIW and the variational Bayes models both outperformed previous organ deformation models when applied to the rectal wall of prostate cancer patients.
Inducing the Lovelock action<|sep|>We have demonstrated that the addition of a Gauss-Bonnet term to an action consisting of the Einstein-Hilbert term plus cosmological term induces new on shell divergences at one loop order of the form of the six-dimensional Euler density E6. From a strict four-dimensional point of view this implies that the Gauss-Bonnet term has no inﬂuence on the one loop renormalizability of gravity. We thus lay to rest this issue, raised long ago by Capper and Kimber [1], at least for the divergent parts of one loop diagrams. The question remains open for ﬁnite one loop scattering processes and also for a possible inﬂuence of the Gauss-Bonnet term on the two loop divergences of gravity. Still, given the topological nature of the Gauss-Bonnet term in d = 4, our result is as expected. Ex nihilo nihil ﬁt. In retrospect we might say that ’t Hooft and Veltman [12] showed that if one starts from the Einstein-Hilbert term one induces the Gauss-Bonnet term at one loop. Later, Christensen and Duﬀ [31] added a cosmological constant to the classical action with again the Gauss-Bonnet term being induced. Of course, power counting suﬃces to predict this, because there is only a single scalar quadratic in the Weyl tensor. This is not anymore so if one includes the Gauss-Bonnet term in the classical action as we did. There then exist two diﬀerent cubic scalars, but our calculations show that they appear only in the combination corresponding to the six-dimensional Euler density. It leads us to the conclusion that from the d-dimensional point of view advocated by Capper and Kimber [1], one loop renormalizability of Einstein gravity requires one to extend it to a Lovelock theory. However, preliminary calculations show that at quartic order in the Weyl tensor it is not the eight-dimensional Euler density E8 which appears, but rather several non-topological invariants which vanish in d = 4. It is conceivable that one can arrange for such extra terms to produce just E8 by a special choice of coupling constants. That would constitute an interesting constraint on the parameters of the class of all Lovelock theories. So far, such constraints were based on stability [32, 20] or causality [33]; see also [34]. We hope to return to this issue in future. Our work can also be seen as an interesting setting in which non-minimal wave operators occur. We have veriﬁed the one loop algorithm of [14] for such operators and extended it to curved Ricci ﬂat spaces, both via ’t Hooft’s non-covariant method and by using the covariant Schwinger-DeWitt method. Our result does not depend on the precise coeﬃcients in the algorithm: Every contribution individually reduces to the sixdimensional Euler density. Possibly our work can be continued to all orders in the non-minimal term by extending the methods of [13, 16, 17, 24] to the case of divergenceless rather than covariantly constant W-tensor. Jan-Peter B¨ornsen is grateful for ﬁnancial support of the German Research Foundation (DFG) BO1968/1. Anton van de Ven appreciates the support of University College Utrecht.
Entanglement transfer via Chiral Quantum Walk on a Triangular Chain<|sep|>We explored the transfer of spatial entanglement of a single spin excitation (which we call particle) undergoing either CQW or CTQW on a triangular chain. We found that particle transfer to the end of the chain is more successful if the particle is injected simultaneously from the leftmost pair of sites in a speciﬁc Bell-type superposition state. The success, measured by the rightmost site’s occupation probability, depends on the relative phase φ between the site states in the initial quantum superposition. Using the Bures distance between the forward and backward time evolved states, we examined the dynamics of PTS breaking at different φ. We conclude that PTS breaking and the success of entangled state transfer via CTQW vary with φ. We explained the physical mechanism in terms of the role of the relative phase φ in the initial state played in the path interference in the triangular chain which eventually determines the quality of state transfer via
Infrared-suppressed QCD coupling and the hadronic contribution to muon g-2<|sep|>A QCD coupling A(Q2) was constructed by dispersive methods, in the lattice MiniMOM scheme (rescaled to the usual ΛMS scale convention). Mathematica programs for the evaluation of the coupling is available online [22]. This coupling deﬁnes a version of (A)QCD which has several attractive features: (a) At high momenta |Q2| > 1 GeV2, the coupling A(Q2) reproduces the pQCD results, because there it practically coincides with the underlying pQCD coupling a(Q2) ≡ αs(Q2)/π. (b) At very low momenta |Q2| ≲ 0.1 GeV2, A(Q2) goes to zero as ∼ Q2, as suggested by high-volume lattice results. (c) At intermediate momenta |Q2| ∼ 1 GeV2, A(Q2) reproduces the well measured physics of semihadronic τ-lepton decay. (d) As a byproduct of the construction, A(Q2) possesses an attractive holomorphic behaviour in the complex Q2-plane, the behaviour qualitatively shared by QCD spacelike physical quantities. (e) Several successful applications of A(Q2) were made in low-|Q2| phenomenology, including the correct reproduction of the value of muon (g − 2)had(1) exp . Other holomorphic couplings have been introduced and applied in QCD phenomenology by various authors, among them [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]. Further, spacelike QCD observables can be evaluated also by applying dispersive methods directly to them [37, 38, 39, 40, 41, 42, 43, 44]. Further details will be presented in the extended version of this work, Ref. [14]. We present here the expressions for the branching ratio r(D=0) τ of the τ-decay into nonstrange hadrons, ﬁrst in pQCD and then in AQCD. In r(D=0) τ , the contributions of nonzero quark masses and higher-twist terms (D > 0) are subtracted, and it is normalized in the canonical way: r(D=0) τ,pt = a + O(a2). This quantity can be expressed theoretically in terms of the Adler function d(Q2)D=0 (see also Sec. 3) where d(Q2)D=0 = −1 − 2π2dΠ(Q2)D=0/d ln Q2 is the massless Adler function. If we replace in Eq. (A.1) m2 τ by σmax (≤ m2 τ), we obtain the quantity r(D=0,σmax) τ,th . The perturbation expansion of r(D=0) τ,th is known up to O(a4). Since a(m2 τ) is rather large (≈ 0.312/π, 0.342/π in MS and LMM, respectively), one would wish to improve this approximation. Recently, by applying a renormalon-motivated model [13], we succeeded to extend the perturbation expansion formally to all orders in a and �dIR 2,1 = −1.831, �dIR 3,2 = 11.05, �dUV 1,2 = 0.005885, �α = −0.14; �K = −0.7704: these are the renormalon-motivated parameters appearing in the Borel transform B[�d](u) of an “Adler”-related auxiliary quantity �d(Q2; µ2). It turns out that the correct resummation in AQCD (“Ares”) is obtained from the pQCD resummation Eq. (A.3) by simply replacing a �→ A in all the integrands: By inserting d(Q2)D=0;Ares into Eq. (A.1) and setting r(D=0) τ,th = 0.200, we obtain the seventh condition for A(Q2) and for the respective parameters.
Automatic Active-Region Identification and Azimuth Disambiguation of the SOLIS/VSM Full-Disk Vector Magnetograms<|sep|>Although VSM full-disk vector magnetograms at the Fe I 630.2 nm photospheric line are obtained since August 2003, these data have not yet been released to the solar and space physics communities. This is due to work currently underway on the last remaining issues that, however, must be addressed prior to releasing
Inferring population history with DIYABC: a user-friendly approach to Approximate Bayesian Computation<|sep|>So far, the ABC approach has remained inaccessible to most biologists because of the complex computations involved. With DIY ABC, non specialists can now perform ABC-based inference on various and complex population evolutionary scenarios, without reducing them to simple standard situations, and hence making a better use of their data. In addition, this programs also allows them to compare competing scenarios and quantify their relative support by the data. Eventually, it provides a way to evaluate the amount of conﬁdence that can be put into the various estimations. The main limitations of the current version of DIY ABC are the assumed absence of migration among populations after they have diverged and the mutation models which mostly refer to microsatellite loci. Next developments will aim at progressively removing these limitations. The development of DIY ABC has been supported by a grant from the French Research National Agency (project MISGEPOP) and an EU grant awarded to JMC as an EIF Marie-Curie Fellowship (project StatInfPopGen) that allowed him to spend two years in D.J.B.’s Epidemiology and Public Health department at Imperial College (London, UK) where he wrote a major part of this program.
IRAC Photometric Analysis and the Mid-IR Photometric Properties of Lyman Break Galaxies<|sep|>Through the photometric analysis of 6 ﬁelds covered by all four IRAC bands on board Spitzer our conclusions are as follows: (i) The excellent agreement of the number counts between all the ﬁelds in the bright end and between observations of equal exposure time in the faint end, shows that our photometric technique and source extraction has treated all ﬁelds in the same manner. (ii) Out of ∼ 700 LBGs that were covered by our data, 443, 448, 137 and 152 LBGs were identiﬁed at 3.6µm, 4.5µm, 5.8µm, 8.0µm IRAC bands respectively, creating the largest existing rest-near-infrared sample of high-redshift galaxies. (iii) The SED of the LBGs were expanded to NIR and show that the near-infrared colours of the population spans over 6 magnitudes. The addition of IRAC bands reveals for the ﬁrst time that LBGs display a variety of colours and their rest-near-infrared properties are rather inhomogeneous, ranging from : • Those that are bright in IRAC bands and exhibit R − [3.6] > 1.5 colours with steeply rising SEDs towards longer wavelengths to • Those that are faint or not detected at all in IRAC bands with R − [3.6] < 1.5 colour whose SEDs are rather ﬂat from the far-UV to the NIR with marginal IRAC detections. (iv) Out of the whole sample, ∼ 20% of the LBGs are detected at 8.0µm. We refer to them as the 8.0µm sample of LBGs (It is equivelent to a rest frame K–selected sample). Those LBGs tend to have redder R−[3.6] colours when compared to the rest population with median values of 1.81 (±0.16) and 1.09 (±0.26) respectively. (v) The infrared colours of LBGs are consistent with those of z∼ 3 galaxies and indicating that their SEDs are can be ﬁtted with various stellar synthesis population models. The mid-infrared properties of the LBG (i.e., masses, dust, age, link to other z∼ 3 galaxy populations) will be presented in detail in a forthcoming paper as well as the full photometric catalogues. (vi) Based on on results for a few z∼3 optically identiﬁed AGN, IRAC 8µm band can be used as a diagnostic tool to separate luminous, high z, AGN dominated objects from star–forming galaxies with AGNs being brighter in [8.0] band when compared to the LBG population. This work is based on observations made with the Spitzer Space Telescope, which is operated by the Jet Propulsion Laboratory, California Institute of Technology under a contract with NASA. Support for this work was provided by NASA through an award issued by JPL/Caltech.
Valence quark and meson cloud contributions for the gamma* Lambda -> Lambda* and gamma* Sigma0 -> Lambda* reactions<|sep|>In this study we have analyzed the contributions from the valence quark and meson cloud eﬀects for the γ∗B → B∗ reactions with B = N, Λ, Σ0 and B∗ = N(1535), Λ(1670). While the valence quark contributions are estimated using a constituent quark model [23, 27, 28], those of the meson cloud are estimated using the chiral unitary model [17, 25]. In the chiral unitary model, the N(1535) has some components other than mesonbaryon dynamics as discussed in Ref. [13], but for the calculation of the transition form factors we take only coupling of the photon current to the meson component and do not take into account of photon couplings to genuine quark components. In this approach the Λ(1670) is almost composed of meson-baryon components [17]. Since the valence and meson cloud eﬀects are calculated by the diﬀerent formalisms we cannot simply combine the both contributions to obtain the ﬁnal, total results for the transition form factors. Nevertheless, the magnitude and signs of the individual contributions presented here are suﬃcient to conclude that it is possible to have a cancellation from the two eﬀects, the valence quark and meson cloud eﬀects, in a consistent, uniﬁed approach including the both eﬀects. For the γ∗N → N ∗(1535) reaction, we have found difference in signs for the two contributions for the Paulitype form factor F ∗ 2 , which can be the main reason for the experimental observation, F ∗ 2 ≃ 0 for Q2 > 2 GeV2. As for the reactions γ∗Y → Λ(1670) (Y = Λ, Σ0), we conclude that generally the valence quark contributions dominate for the Y = Λ case, but the two contributions are similar for the reaction with Y = Σ0. A particularly interesting case is the form factor F Σ 2 . Namely, if we assume the same sign for the Λ and Σ radial wave function normalization constants (ηΛΣ0 = 1) and ηΛ∗ = 1,
Spectra of absolute instruments from the WKB approximation<|sep|>In this paper we have developed a method for calculating frequency spectra of absolute instruments using the WKB approximation. Our method is based on a the key relation (15) between the optical path S and the semiclassical phase change Φ between the turning points, and on the fact that in AIs of the ﬁrst type the optical path S is independent of the angular momentum. The method proved to be very eﬃcient for describing the spectrum accurately even for the lowest levels. The results conﬁrmed the previously derived properties of the spectra, in particular that the frequencies are strongly degenerate in AIs and that they form almost equidistantly spaced groups. Since these properties of the spectrum have key importance for focusing of waves by AIs [13], the WKB method shows that sharp focusing of rays in fact implies that waves will be focused as well. This way the WKB method provides a nice and important bridge between geometrical and wave optics of absolute instruments. We applied our method also to AIs that contain mirrors; there, the Airy functions turned out to be very helpful in describing the wave in both the oscillatory and evanescent regions, and to calculate the eigenfrequency shifts caused by the mirror. When the mirror is added into the evanescent region, it turns out that even though light rays are not inﬂuenced at all, the spectrum can be inﬂuenced strongly. In particular, the spectrum becomes less regular, which can somewhat degrade imaging by the absolute instrument. On the other hand, placing the mirror in the oscillatory region leads to a constant shift in the spectrum. It may be somewhat surprising, but certainly very satisfactory, how accurate results the WKB method gives for the spectra of absolute instruments. The situation is similar to quantum mechanics where the WKB method also gives very precise values for the energy spectrum in many situations. An interesting question for further research could be how the WKB method can be applied to AIs of the second type where not all points have their full images, or to even more general devices. I thank Michal Lenc for his comments and acknowledge support from grant no. P201/12/G028 of the Grant Agency of the Czech Republic, and from the QUEST programme grant of the Engineering and Physical Sciences Research Council.
Network as a computer: ranking paths to find flows<|sep|>When the Web is viewed as a global data store, the problem of its semantics is the problem of determining a uniform meaning for the data published by its various participants. The search engines are dealing with
Artificial Intelligence Enabled Traffic Monitoring System<|sep|>augmented the scope of video-based traffic monitoring systems. In this study, an automatic traffic  monitoring system is developed that builds up on robust deep learning models and facilitates traffic  monitoring using a graphical user interface. Deep learning algorithms, such as Mask R-CNN, Faster  R-CNN, YOLO and CenterNet were implemented alongside two different object tracking systems  viz. IOU and Feature Tracker. Mask R-CNN was used to detect traffic queues from real-time traffic  CCTVs whereas YOLO and Faster R-CNN were deployed to predict objects which later coupled with  object trackers were used to detect stationary vehicles. Mask R-CNN predicted traffic queues with  92.8% accuracy while the highest accuracy attained by YOLO was 95.5%. The discrepancy in correctly  detecting queues was mainly due to the poor image quality, queues being distant from the camera  and glaring effects. These issues significantly affected the accuracies of the proposed models.  Similarly, the F1, RMSE and S3 scores for detecting stationary vehicles were 0.8333, 154.7741, and  0.4034 respectively. It was observed that the model correctly detected stranded vehicles which  remained closer to the camera but faced difficulties while detecting distant stationary vehicles. Part  of the problem for lower S3 scores was also due to issues such as video pixelation, and the presence  of traffic intersections. Regardless, procedures such as anomaly suppression and video pixelation  corrections were useful at improving the efficacy of the proposed model. It is worthwhile to note that  these corrections led to an effective stationary vehicle prediction system. Lastly, the performance of  vehicle counting framework was satisfactory for both CenterNet and YOLO’ combinations with  Feature Tracker. However, the vehicle counting framework could be further explored and the  existing models be further fine-tuned to generate a near to perfect counting framework. This could  in fact be ideal for most transportation agencies as they rely heavily on turning movement counts to  optimize traffic signals at intersections. system obtained superior results and could be useful at attaining some level of automation at Traffic  Management Centers. It is worth mentioning that since, most software suites sold by transportation  vendor companies cost over hundreds of thousands of dollars, their functionalities are limited; and  offer just a few additional traffic surveillance capabilities than our proposed framework. In that case,  the system proposed in this paper could be a cheaper and reliable alternative to bringing in some  level of traffic automation by supplementing it with some additional low-cost back-up software  suites.
PatchPerPix for Instance Segmentation<|sep|>In this work we present a novel generic method for instance segmentation that comprises a CNN to predict dense local shape descriptors and a one-pass instance assembly pipeline. The method is able to handle objects of sophisticated shapes that appear in dense clusters with overlaps, including crossovers. It is the ﬁrst to assemble all instances from learnt shape patches, simultaneously in one pass. We successfully applied our method to a range of domains, showing that it (1) outperforms the state of the art on the heavily contested ISBI 2012 challenge on neuron segmentation in electron microscopy, (2) outperforms the state of the art on the challenging BBBC010 C. elegans worm data by a large margin, (3) outperforms the state of the art on 2d Mask R-CNN[27] 0.594 0.832 0.773 0.684 0.489 0.189 StarDist[27] 0.584 0.864 0.804 0.685 0.450 0.119 PatchPerPix 0.693 0.919 0.919 0.915 0.898 0.868 0.827 0.755 0.635 0.379 MALA [8] 0.381 0.895 0.887 0.859 0.803 0.699 0.605 0.424 0.166 0.012 StarDist 3D[29] 0.406 0.936 0.926 0.905 0.855 0.765 0.647 0.460 0.154 0.004 3-label+cpv[12] 0.425 0.937 0.930 0.907 0.848 0.750 0.641 0.473 0.224 0.035 PatchPerPix 0.436 0.926 0.918 0.900 0.853 0.766 0.668 0.493 0.228 0.027 Figure 4: Qualitative results on 3d neuron light microscopy examples. (a) Maximum intensity projection of raw images. Orange circles indicate overlapping areas in 3d. Ground truth data (b) were generated by manual segmentation using VVD Viewer. PatchPerPix (c) shows promising results on this challenging dataset. and 3d ﬂuorescence microscopy data of densely clustered cell nuclei (on par in terms of cell detection performance, better in terms of pixel accuracy), showing that our method performs well also for simple (blob-like) instance shapes, and (4) can be applied to extreme cases of instance shapes, like neurons in 3d ﬂuorescence microscopy. Future work will tackle a performance bottleneck that becomes relevant on 3d data, where we’re currently restricted to patch sizes that are most probably sub-optimally small. Acknowledgments. We wish to thank Constantin Pape for his invaluable help in reproducing the training- and prediction setup from [30], Carolina Waehlby for help with the BBBC010 data, Stephan Saalfeld and Carsten Rother for inspiring discussions, the FlyLight Project Team5 at Janelia Research Campus for providing unpublished data, and Claire Managan and Ramya Kappagantula (Janelia Project Technical Resources) for their conscientious manual neuron segmentations. P.H., L.M. and D.K. were funded by the Berlin Institute of Health and the Max Delbrueck Center for Molecular Medicine. P.H. was funded by HFSP grant RGP0021/2018-102. P.H., L.M. and D.K. were supported by the HHMI Janelia Visiting Scientist Program. VVD Viewer6 is an open-source software funded by NIH grant R01-GM098151-01.
Intrinsic bottom and its impact on heavy new physics at the LHC<|sep|>This contribution presented a new method to generate a matched IC/IB distributions for any PDF set without the need for a complete global re-analysis. This renders easy to carry out a consistent analysis including intrinsic heavy quark eﬀects. In addition, because the evolution equation for the intrinsic heavy quarks decouples, one can freely adjust the normalization of the IC/IB PDFs. Figure 2. Ratio of c¯c luminosities (left) and b¯b luminosities (right) at the LHC14 for charm(bottom)-quark PDF sets with and without an intrinsic component as a function of √τ = mH/ √ S . The ratio for the c¯c (b¯b) luminosity (solid, green line) in the left (right) ﬁgure reaches values of 50 (17) at √τ = 0.5. In addition to the curves with 1% normalization (red, dashed lines) we include the results for the 3.5% normalization (green, solid lines) which was found to be still compatible with the current data [11]. We showed that our approximation holds to a very good precision for the IB. For the IC, the error is larger (because the IC increases), yet our method is still useful. Indeed, for an IC normalization of 1-2%, the error is less than the PDF uncertainties at the large-x where the IC is relevant. If the normalization is larger, although the error may be the same order as the PDF uncertainties, the IC eﬀects also grow and can be separately distinguished from the case without IC. In any case, the IC/IB represents a non-perturbative systematic eﬀect which should be accounted for. The method presented here greatly simpliﬁes our ability to estimate the impact of the intrinsic heavy quark eﬀects on the new physics searches. It can also be very useful in searching for and constraining the intrinsic charm and bottom components of the nucleon by itself. In particular in the future facilities such as an Electron Ion Collider (EIC), the Large Hadron-Electron collider (LHeC), or AFTER@LHC. The PDF sets for intrinsic charm and intrinsic bottom discussed in this analysis (1% IC, 3.5% IC, 1% IB, 3.5% IB) are available from the authors upon request.
Towards the construction of a model to describe the inter-ELM evolution of the pedestal on MAST<|sep|>Pedestal proﬁles of electron temperature and electron density, obtained as a function of time during the MAST ELM cycle, have been used to test whether the pressure pedestal gradient in MAST is limited by the onset of Kinetic Ballooning Modes (KBMs). In this set of discharges, the electron density and electron pressure pedestal heights and widths increase during the inter-ELM period, while their gradients remain approximately constant. Finite-n stability analyses using the ELITE code show that the ﬁnite-n stability limited normalised pressure gradient, αc, lies above the experimental value, αexp, initially just after an ELM, and then falls towards it just before the next ELM: i.e. rather than the experimental pressure gradient increasing towards a ﬁxed stability boundary it is the pressure gradient stability limit that moves towards the experimental pressure gradient due to the increasing pedestal width. The region over which inﬁnite-n modes are unstable also broadens during the ELM cycle. Ideal MHD stability boundary calculations track the evolution of the pedestal height and width well, provided the limiting criterion used is that 100% of the pedestal width be unstable to inﬁnite-n modes. The gyrokinetic code, GS2, was used to test whether the MAST pedestal region unstable to KBMs corresponds closely to the region that is unstable to inﬁnite-n ideal ballooning modes. KBM modes with twisting parity were found to be the dominant microinstabilities in the steep pedestal region, with a transition to tearing parity modes in the shallower pressure gradient core region immediately inside the pedestal top. The region over which KBMs dominate increases during the ELM cycle, and closely corresponds to the region unstable to inﬁnite-n ideal ballooning modes. The impact of sheared ﬂow on stability has not been considered here, and an assessment of its possible impact is underway. This work, part-funded by the European Communities under the contract of Association between EURATOM and CCFE, was carried out within the framework of the European Fusion Development Agreement. The views and opinions expressed herein do not necessarily reﬂect those of the European Commission. This work was also part-funded by the RCUK Energy Programme under grant EP/I501045. Gyrokinetic simulations were performed on the supercomputer, HECToR, which was made available through EPSRC grant EP/H002081/1.
The Consequences of Gamma-ray Burst Jet Opening Angle Evolution on the Inferred Star Formation Rate<|sep|>Observations suggest that the jet opening angles of lGRBs evolve over cosmic time, with lGRBs at higher redshifts more narrowly beamed than those at lower redshifts. In this paper we have: 1) estimated the star formation rate from the gamma-ray burst formation rate, accounting for the evolution of the distribution of GRB jet opening angles (and given an assumption about the fraction of stars that make long gamma-ray bursts), 2) estimated the fraction of stars that make long gamma-ray bursts under the assumption that lGRBs trace the global star formation rate as parameterized by MD14. • When accounting for beaming angle evolution - with lGRBs more narrowly beamed at higher redshifts - we ﬁnd a higher relative star formation rate at high redshifts. Depending on the strength of the beaming angle evolution and the normalization of the inferred SFR, the SFR can be up to an order of magnitude higher than the canoncial MD14 estimate. Our inferred SFRs from the GRB rate may be indicating a speciﬁc metallicity dependent SFR (see, e.g., Björnsson (2019); Chruslinska et al. (2020)), given the low-metallicity requirements for succesfully launching a GRB jet in a massive star. • There appears to be an excess in our SFR estimates at low redshifts relative to the MD14 rate (again, depending on the normalization we choose). Accounting for beaming angle evolution lessens this excess, which may suggest the importance of accounting for the evolution. Alternatively, this could be a reﬂection of the breakdown of a one-to-one correspondence between lGRBs and massive star progenitor systems at low redshifts. In other words, if multiple systems (including binary merger systems) contribute signiﬁcantly to the GRB rate at low redshifts this may lead to such an excess at low redshifts. • Under the assumption that GRBs trace the MD14 star formation rate, we estimate the fraction of stars that produce lGRBs (in order to be consistent with the observed GRB rate), once again accounting for beaming angle evolution. Although the overall normalization of this curve is uncertain, we ﬁnd that this approach implies a higher fraction of stars in the early universe produce GRBs. This result is plausible in light of the fact that low metallicity conditions are conducive to launching a successful GRB. We also ﬁnd, using this approach, that a higher fraction of stars produce GRBs at lower redshifts than at the peak of star formation (although less so when beaming angle evolution is accounted for). As discussed above, this somewhat unexpected result could reﬂect the breakdown of a oneto-one correspondence between lGRBs and massive star progenitors at low redshifts, and may also indicate the implausibility of assuming that the lGRB rate density follows the SFR as parameterized by MD14. Because of the extreme luminosity of long gamma-ray bursts, they remain powerful probes of the early universe, and potentially important tools with which to measure the star formation rate at redshifts that are inaccessible by other methods. That the jet opening angle of lGRBs may evolve over cosmic time, with jets in the early universe being more narrowly beamed than those at lower redshifts, has important implications on estimates of the star formation rate from the lGRB rate - implying it has perhaps up until now been largely underestimated. As the next generation of telescopes is launched - including deep space optical and infrared probes such as the James Webb Space Telescope and Nancy Grace Roman Telescope, as well as transient detectors such as Theseus and the Space Variable Objects Monitor - we will get a more extensive probe into the early universe. In addition, new methods employing measurements of the neutrino ﬂux (Riya & Rentala 2020), for example, could enable us to more securely ascertain star formation during these epochs, allowing us to test our predictions of the SFR at high redshift, and gain a better understanding of the history of star formation throughout our Universe.
Context-Aware Neural Machine Translation Learns Anaphora Resolution<|sep|>We introduced a context-aware NMT system which is based on the Transformer architecture. When evaluated on an En-Ru parallel corpus, it outperforms both the context-agnostic baselines and a simple context-aware baseline. We observe that improvements are especially prominent for sentences containing ambiguous pronouns. We also show that the model induces anaphora relations. We believe that further improvements in handling anaphora, and by proxy translation, can be achieved by incorporating specialized features in the attention model. Our analysis has focused on the effect of context information on pronoun translation. Future work could also investigate whether context-aware NMT systems learn other discourse phenomena, for example whether they improve the translation of elliptical constructions, and markers of discourse relations and information structure. comments. The authors also thank David Talbot and Yandex Machine Translation team for helpful discussions and inspiration. Ivan Titov acknowledges support of the European Research Council (ERC StG BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518). Rico Sennrich has received funding from the Swiss National Science Foundation (105212 169888).
Extending the baseline: Spitzer Mid-Infrared Photometry of Globular Cluster Systems in the Centaurus A and Sombrero Galaxies<|sep|>In this work, a catalogue of 260 GC candidates with good optical and Spitzer [3.6] photometry is presented, enabling, for the ﬁrst time, an examination of the optical to midIR properties of a large sample of GCs. Following previous examples in the literature using K-band photometry, the age-substructure of these GC systems is examined with colour-colour diagrams including the [3.6]–band. A spread of ages in the NGC 5128 metal-rich GC subpopulation might have been reported from photometry alone. However, the youngest GCs found in the large spectroscopic study of Beasley et al. (2008) do not occupy a young region in colourcolour space and actually show photometric properties that are consistent with the old NGC 5128 GCs. This observation likely demonstrates that the age-metallicity degeneracy is indeed diﬃcult to overcome when using only typical photometric observations. Spectroscopic age conﬁrmation is required for any age-substructure to be interpreted from old stellar population in such colour-colour diagrams. For the NGC 4594 GC system, for which no large sample of GCs with spectroscopic ages exists, the metal-rich GCs generally fall onto the the 5–7 Gyr SSP model predictions in B−R versus R−[3.6] . However, an empirical comparison between data and SSP models (see below) suggests the models likely under-predict the B−band ﬂux relative to the data at these metallicities. Until this discrepancy is understood, the apparent subpopulation of intermediate-aged NGC 4594 metal-rich GCs will remain unconﬁrmed. Another notable feature in the NGC 4594 GC colourcolour diagram (Fig. 11) is an apparent non-linearity traversal of the parameter space, which resemble the SSP model predictions of S. Yoon (2006, priv. comm.; Y06). Driving this resemblance are a subpopulation of very luminous and extended GC candidates ﬁrst noted by Spitler et al. (2006). These objects have B−R colours similar to two Galactic GCs that host very extended HB morphologies (Rich et al. 1997; Busso et al. 2007). These Galactic and the luminous NGC 4594 GCs are bluer in B−R than expected for their metallicities, perhaps suggesting they share similar HB morphologies (see Fig. 12). Another possibility is that the massive NGC 4594 GCs are ∼ 5 Gyr younger than the bulk of the GC system. This would explain their high luminosities, but not the extended sizes they tend to show (Spitler et al. 2006). A dwarf nuclei with a young stellar component that accreted onto NGC 4594 might match these characteristics, although testing this scenario with simulations is required. Using the excellent mass proxy, [3.6] , and the largest colour dynamical range studied thus far for subpopulation colour-magnitude trends, the present work demonstrates the NGC 5128 blue GC subpopulation shows redder colours at higher luminosities. The NGC 4594 sample does not span a large enough range of the GC luminosity function to constrain such trends. The NGC 5128 blue GCs show a massmetallicity proportionality of Z ∝ M 0.19; noticeably weaker compared to other galaxies. This conﬁrms initial suspicions of Spitler et al. (2006) that the strength of the blue tilt decreases with the host galaxy luminosity. No such trend is found among the red GCs in NGC 5128. The Beasley et al. (2008) NGC 5128 GC spectroscopic ages allow, for the ﬁrst time, the blue tilt to be studied independent of any young to intermediate-aged GC. It is concluded that age substructure in not likely playing a role in the blue tilt. The scenario of Yoon et al. (2006) is tested with the R−[3.6] GC data. Such colours are insensitive to hot HB stars (see the Y06 models in Fig 7), hence can determine whether GC colour bimodality implies metallicity bimodality without eﬀects from a rapidly changing HB morphology. While the NGC 4594 sample is too small to constrain this scenario, the old GCs in NGC 5128 provide strong evidence for R−[3.6] colour bimodality, which can safely be interpreted as metallicity bimodality. This conﬁrms the spectroscopic analysis of NGC 5128 GCs by Beasley et al. (2008). Strader et al. (2007) and Kundu & Zepf (2007) provide further evidence for GC system metallicity bimodality in massive ellipticals, suggesting the Yoon et al. (2006) scenario does not apply to extragalactic (nor Galactic) GC systems. To conclude, despite a low IRAC pixel resolution and relatively short exposure times (e.g. 240s – NGC 4594, 72s – NGC 5128), the present work demonstrates that interesting science in the ﬁeld of extragalactic GC system astronomy is indeed possible with Spitzer IRAC imaging. The largest prerequisite for such work is a good optical GC catalogue, because Spitzer photometry alone cannot be used for contamination removal for such faint objects. The authors appreciate useful discussions with R. Proctor and the enthusiasm and assistance provided by the stellar population modellers: M. Cantiello, S. Charlot, C. Maraston, and S. Yoon. The referee provided a number of very useful comments that helped improve the discussion. DF thanks the Australian Research Council for ﬁnancial support. This work is based on observations made with the Spitzer Space Telescope, which is operated by the Jet Propulsion Laboratory, California Institute of Technology under a contract with NASA.
A Survey on Federated Learning and its Applications for Accelerating Industrial Internet of Things<|sep|>4.0 emphasizing its application in advancing intelligent  manufacturing. To facilitate a common understanding of the FL  paradigm, we elaborate and update relevant concepts of the  roles, algorithms, tools used in FL, such as learner, organizer,  local model, federal model, etc. With the comprehensive survey,  the state of the art of FL on fundamental FL research is analyzed  from eight topics and further work and challenges are presented.   Before reviewing the FL applications in advancing more than  thirteen economic sectors, we present the paradigm of FLtransformed manufacturing. Clearly, more attention should be  paid on the investigation of integrating FL into Industry 4.0.  Meanwhile, we list some industrial areas for IIoT researchers  and practitioners into which FL could be seamlessly and  immediately integrated.  Our other findings are summarized  as follows：    Recently, the attention and research on FL have increased exponentially. However, there is not much research on  Industry 4.0 and smart manufacturing. This deserves more  attention from the industrial academia and practice on FL. applications is distributed in the eight areas, and most of  them focus on data distribution, model optimization, and  privacy protection. However, privacy protection lacks a  measurement standard and the suitable quantitative  evaluation is missed. We initially present and define the  problem in this paper. On the other hand, there are few  benchmarks and tool platforms. It can be seen that FL is still  in its infancy stage. of them are based on HFL. Few are based on VFL and FTL,  which needs more attentions and efforts in the future. prediction, device failure detection, cloud robotic system, etc.  However, there is huge potential space for FL to accelerate  PLCM in the context of IIoT. Other applications mainly fall  into categories of healthcare & medical and recommendation  systems. Medical care focuses on drug discovery, medical  image processing, privacy preservation of electronic health  records, and activity recognition. Recommendations include  entertainment, news, videos, and automatic text input on the  browser.
RankGAN: A Maximum Margin Ranking GAN for Generating Faces<|sep|>In this work, we introduced a new loss function to train GANs - the margin loss, that leads to a better discriminator and in turn a better generator. We then extended the margin loss to a margin-based ranking loss and evolved a new multi-stage GAN training paradigm that progressively strengthens both the discriminator and the generator. We also proposed a new way of measuring GAN quality based on image completion tasks. We have seen both visual and quanti We detail some of the face generation experimental results (in Section A) and face completion experimental results (in Section B) in the appendix. These results demonstrate the eﬀectiveness of the proposed RankGAN method. Further, we mention sample aware vs. sample agnostic RankGAN in Section C. Finally, we discuss a connection between margin loss and f-divergence in Section D. In Figure 10, we show face generation results from all the three stages of the RankGAN and the WGAN and LSGAN baselines. These images were generated by passing the input images through the encoder E and using the obtained latent vectors z as input to the generators. Since WGAN and LSGAN were trained without an encoder, image generation experiments don’t preserve identity for them. One can observe that, the Stage-3 generated images look much more aesthetically appealing and realistic than both WGAN and LSGAN images. Database In addition to the CelebA dataset we used in the main paper, we also collect a single-sample dataset containing 50K frontal face images from 50K individuals, which we call the SSFF dataset. They are sourced from several frontal face datasets including the FRGC v2.0 dataset [32], the MPIE dataset [8], the ND-Twin dataset [31], and mugshot dataset from Pinellas County Sheriﬀ’s Oﬃce (PCSO). Training and testing split is 9-1, with the image completion results being based only on the open-set. This dataset is single-sample, which means there is only one single image of a particular subject throughout the entire dataset. Images are aligned using two anchor points on the eyes, and cropped to 64 × 64. Occlusion Masks We carried out face completion experiments on four types of facial masks, which are termed as: ‘Center Small’, ‘Center Large’, ‘Periocular Small’, and ‘Periocular Large’. Examples can be seen from Figure 11. Face Completion Results on Large Number of Iterations Some preliminary image completion progression visual results are shown in Figure 12 with 8000 iterations for optimizing for the ˆz using Stage-3 RankGAN. As can be seen, after 8000 iterations, the RankGAN is able to achieve decent image completion results. However, such an algorithm is still slow because it requires optimization Fig. 10. Visualization of face generation results with RankGAN, Wasserstein GAN (WGAN) and Least Squares GAN (LSGAN). Latent vectors z’s are obtained by passing the input faces through the encoder E. Since WGAN and LSGAN have not been trained with an encoder, the face identities are not preserved. over ˆz for any query image. Therefore, for the large-scale experiments to be carried out, we limit the algorithm to optimize for only 2000 iterations for the sake of time. Results can be improved if we allow further iterations. As discussed above, we used Face Completion task to quantify the performance of our proposed approach. We used four diﬀerent mask types to perform image completion and showed quantitatively that reconstruction improves with each stage of RankGAN. The four masks represent varying diﬃculties of image inpainting depending on the amount and type of visible image region. We use a large and a small square mask to occlude the central facial region. Another
Lepton Flavour Violation in minimal grand-unified type II seesaw models<|sep|>We have revisited a class of SU(5) GUT models with minimal ﬁeld contents (see Table 1) that allow for successful uniﬁcation of the gauge couplings and account for the origin of neutrino masses via type II seesaw. In Section 2, we classiﬁed our models based on how realistic fermion masses are achieved, studied their spectrum compatible with uniﬁcation and p-decay constraints in Section 3, and ﬁnally discussed in detail their observable consequences in terms of LFV decays in Section 4. The main ﬁndings of our study can be summarised as follows. • The minimal SU(5) setup with non-renormalisable interactions (“Model 1”) is excluded by proton decay searches, barring the case of ﬁne cancellations triggered by a very peculiar ﬂavour structure of the Yukawa couplings, hence it is strongly disfavoured, see Section 3.1. • For what concerns models featuring vector-like matter (“Model 3”), we separately considered the case of a single 5 ⊕ 5 fermionic representation and that with a single 10 ⊕ 10. The former case is also strongly disfavoured by proton decay but it may become viable if multiple generations — at least 5 — of vector-like fermions are introduced. The latter case is instead viable in its simplest form. However, the model is not predictive as the constraints on its spectrum are very loose and, in particular, no ﬁeld is required to be light for the sake of uniﬁcation and proton decay, cf. Section 3.3. • The model with an additional scalar 45 and renormalisable interactions (“Model 2”) can successfully achieve uniﬁcation with a long enough proton lifetime and, especially in its minimal realisations, features very interesting predictions, as discussed at length in Section 3.2. Several ﬁelds are required to be light (that is, not much above the TeV scale), in particular the type II seesaw ﬁelds in the 15 representation that mediate LFV interactions. • The couplings of these ﬁelds (the seesaw triplet ∆ and its SU(5) partner, the scalar leptoquark � R2) to SM fermions are linked to one another by the SU(5) structure and thus their LFV eﬀects are related. From this it follows that measuring BR(µ → eee)/CR(µ N → e N) would pinpoint the mass ratio M∆/MLQ, see Figure 5. Such a measurement (or constraint, in case only one of the two LFV processes is observed) could be then confronted with uniﬁcation and p-decay constraints (as well as information from collider searches) in order to see if a consistent picture emerge. • Instead, ratios of processes mediated by the same ﬁeld (the most promising being µ → e conversion in nuclei and KL → µe, both due to the leptoquark) provide information on the ﬂavour structure of the couplings and thus directly on the neutrino mass matrix (in the charged-lepton mass basis), as both matrices Y∆ and YLQ are proportional to mν. We showed that ratios of diﬀerent LFV branching ratios can be particularly sensitive to the neutrino parameters that can not be measured through oscillation experiments, namely the Majorana phases and the absolute mass (see Figures 8-12). • While some of our results apply to more general extensions of the SM featuring the triplet ∆ (e.g. to a generic type II seesaw) or the leptoquark � R2, the connection between the processes induced by these two ﬁelds obviously requires the presence of a GUT. Our results show that measuring the rates of several LFV modes may allow to collect enough evidence of such a connection and thus of an underlying GUT structure. The renormalisation factor appearing in Eqs. (21-25) is given by A = ALDASD, where ALD and ASD account for the long-distance and short-distance running of the baryon-number-violating operators, respectively, see e.g. [41]. The former one corresponds to the QCD running from mt to the proton mass scale: The short-distance contribution encodes the renormalisation of the operators from the GUT scale down to mt. This can be given in terms of the running of the gauge couplings, that is, in terms of the SM β-function coeﬃcients plus the contribution of the extra ﬁelds: where we considered the extra matter at a single intermediate scale MI (with ∆bi ≡ � I bI i ), the generalization to multiple thresholds being straightforward. To obtain an estimate of the typical value of ASD, one can consider only the contributions from bsm i in the above equation and take αgut = 1/25, which gives ASD ≈ 1.3. A recent lattice QCD evaluation of the hadronic matrix elements in Eqs. (21-25) gives [82]:
Electronic band crossing in sliding bilayer graphene: Tight-binding calculations and symmetry group representation analysis<|sep|>The engineering of stacked layered materials to have desired electronic properties is currently an intensely developed ﬁeld. One such actively researched materials is bilayer graphene, which is a ﬂexible 2D system consisting of two graphene monolayers that are weakly bound together. We investigated the electronic structure features of a special class of bilayer graphene conﬁgurations: the sliding bilayer graphene systems. We systematically studied the geometrical and topological properties of the energy surfaces of SBG conﬁgurations while varying the values of the sliding vector. Using a tight-binding model that takes into account only the atomic pz orbitals, we showed that the electronic structure of SBGs is formed by the merging of the energy surfaces of two individual graphene layers. Sliding two graphene layers causes two things. First, sliding along an axis that is free of constraints due to the lattice symmetries only shifts the energy surfaces of the individual graphene layers. Second, sliding of the bilayers breaks the symmetry in the interlayer coupling of the electronic states for the two graphene layers. Both result in the crossing of the dispersion curves along the symmetrical axes and the shrinking of the energy surfaces in a narrow range of energies around the Fermi level. The emergence of the Dirac points in the vicinity of the K points is shown to be the result of the deformation of the energy surfaces under the constraints of the existing symmetries. These observations were validated using analysis of the group representation theory. We prove that the band crossings at generic k points are guaranteed by the compatibility relations between the symmetries of the eigenstates at the high symmetry points in the Brillouin zone. The emergence of Dirac points deﬁne the geometrical and topological features of the energy surfaces, i.e., the local maximal, minimal and saddle points. They manifest in the electronic properties through the shape of the Fermi energy surface and the density of states. The author ackowledges Dominik Domin, Dario Bercioux and Miguel ´Angel Jim´enez Herrera for fruitful discussions and, especially, reading carefully the manuscript before it was submitted. where M, N are the sizes of the submatrices (blocks). If the block DMM is invertible, the original block matrix can be transformed into a triangular matrix by the following matrix multiplication: The determinant of the second matrix in Eq. (A2) is trivially equal to one, hence one can combine Eqs.(A2) and (A3) to get the following expression: An electronic system is said to possess chiral symmetry if it is decomposible into subsystems and there exists a local unitary Hermitian operator, C, that anticommutative with the Hamiltonian of the system. This means that CH(λ) = −H(λ)C, where λ refers to a set of parameters which deﬁne the Hamiltonian H, but not C (i.e., C is independent of λ). These basic properties of the C operator allow one to write: Here Eq. (B1a) expresses the unitary and Hermitian properties of the chiral operator; the locality is expressed by Eq. (B1b) as the decomposition into a set of unitary operators Cn that act only on subsystem n. The hexagonal lattice of monolayer graphene can be seen as the composed of two sublattices A and B. A chiral operator C can be deﬁned in terms of projection operators (PA and PB) in the A and B sublattices: where PA +PB = 1 and PAPB = 0. The system’s Hamiltonian H is therefore decomposable into four terms describing the A and B sublattices and the coupling between them, i.e., H = HAA + HBB + HAB + HBA. The terms are deﬁned as HAA = PAHPA, HBB = PBHPB, HAB = PAHPB and HBA = PBHPA. If the system possesses chiral symmetry, from Eq. (B1c) it can be deduced that the terms HAA and HBB must vanish. This is the case for a commonly used tight-binding BlochHamiltonian matrix for non-interacting electrons in the nearest neighbor approximation: Sliding bilayer graphene systems do not always possess chiral symmetry. In high symmetry conﬁgurations, such as AA- and AB-stacked, some hopping terms between the four sublattices, A1, B1, A2 and B2, can be approximately ignored since they are much smaller than other terms. This may result in a Bloch-Hamiltonian matrix possessing the chiral symmetry. In particular, the Bloch-Hamiltonian matrix of the AA-stacked conﬁguration given by Eq. (17) takes the form of oﬀ-diagonal block matrices. Chiral symmetry is present in this model. The bilayer lattice is decomposable into two sub-systems, one consisting of the sub-lattices A1 and B2 and the other of the sub-lattices B1 and A2. The zero-energy eigenstates of hAA(k) given by Eqs. (22a) and (22b) are clearly the linear combinations of only two atomic orbitals A1 and B2 or B1 and A2. These results reﬂect the localization of the zero-energy eigenstates on either the lattice nodes (A1, B2) or on the lattice nodes (B1, A2). In the main text, Tg is a unitary linear operator representing a symmetry operation g of a symmetry group G(k) for a k vector within the Brillouin zone. The operators Tg, ∀ g ∈ G(k), act on the Hilbert space spanned by four orthogonal Bloch vectors {|A1, pz, k⟩, |B1, pz, k⟩, |A2, pz, k⟩, |B2, pz, k⟩}. Using Eq. (6) we can represent Tg operators in terms of matrices as follows: where α′ is the result of the transformation of the sublattice index α under the operation of g, i.e., α′ = gα. The other terms in the expression are the sign sg of the orbital pz under the operation g and the coeﬃcients, �U g α′α, for a particular linear combination of the Bloch vectors. Since k is a symmetrical point in the Brillouin zone, the action of the symmetry operation g ∈ G(k) results in another point k′ = gk that belongs to the star of k. This means that gk − k = Gg is a reciprocal lattice vector. Now, using Eq. (5) for the deﬁnition of the Bloch vector and adding the reciprocal lattice vector to the k-vector: Th elements of the Ug(k) matrix that represens the Tg operator can be obtained by inserting the basis vector from Eq. (C2) into Eq. (C1) and simplifying the expression to: Since G(k) is a sub-group of a symmetry group of the system’s Hamiltonian H, the invariant relation TgHT † g = H allows one to deduce an identical relation for the Bloch-Hamiltonian matrix h(k): This equation implies that the Bloch-Hamiltonian matrix is invariant under the transformation matrix Ug(k). We therefore check the symmetry of the electronic bands using the following procedure. For every symmetrical k point in the Brillouin zone we: (1) ﬁnd the Ug(k) matrices that represent the symmetry operations of the small symmetrical group G(k); (2) diagonalize the BlochHamiltonian matrix, h(k), to obtain all the eigenvalues, En(k), and their corresponding eigenvectors, ψn,pz(k); a matrix, W(k), is constructed from these eigenvectors ordered by their eigenvalues En(k); (3) check the validity of Eq. (C5); (4) construct the matrix U g(k) = W †(k)Ug(k)W(k); and (5) determine the traces of each diagonal block U g(k) and compare them to the character values of the symmetry group G(k). The electronic energy bands at high-symmetry k-points in Fig. 2 where labeled using this 5-step procedure. ∗ nam.dovan@phenikaa-uni.edu.vn 1 I. M. Lifshitz, Sov. Phys. JETP 11 (1960). 2 Y.-W. Son, S.-M. Choi, Y. P. Hong, S. Woo, and S.-H. Jhi, Phys. Rev. B 84, 155410 (2011). 3 A. Varlet, M. Mucha-Kruczy´nski, D. Bischoﬀ, P. Simonet, T. Taniguchi, K. Watanabe, V. Fal’ko, T. Ihn, and K. Ensslin, Synth. Met. 210, 19 (2015). 4 S. B. Dugdale, Phys. Scr. 91, 053009 (2016). 5 S. Park and B.-J. Yang, Phys. Rev. B 96, 125127 (2017). 6 D. Shao, H. Wang, T. Chen, P. Lu, Q. Gu, L. Sheng, D. Xing, and J. Sun, npj Comput. Mater. 5, 53 (2019). 7 Y.-H. Chan, B. Kilic, M. M. Hirschmann, C.-K. Chiu, L. M. Schoop, D. G. Joshi, and A. P. Schnyder, Phys. Rev. Materials 3, 124204 (2019).
Chemical abundance analysis of symbiotic giants - III. Metallicity and CNO abundance patterns in 24 southern systems<|sep|>Analysis of the photospheric abundances of CNO and elements around the iron peak (Fe, Ti, Ni, and Sc) was performed for the giant stars in a sample of 24 southern S-type symbiotic systems. Our analysis resulted in metallicities distributed in a wide range from signiﬁcantly subsolar ([Fe/H]∼ −0.8 dex) to slightly supersolar ([Fe/H]∼ +0.35 dex), with largest representation around slightly subsolar ([Fe/H]∼ −0.4 to −0.3 dex) and near-solar metallicity. The enrichment in 14N isotope, found in all cases, indicates that the giants have experienced the ﬁrst dredge-up. This is conﬁrmed by the low 12C/13C ratio (5–23) that was measured in a subset of the sample. Comparison with abundances from nebular lines shows that the nebulae are contaminated by activity on the white dwarf and do not provide reliable abundances for the red giant. We found that the enhanced [Ti/Fe] abundances previously found for yellow symbiotic systems are also typically enhanced in red symbiotic giants. This suggests that enhanced [Ti/Fe] abundance could be a characteristic of the giants in S-type symbiotic systems.
Spatial-Temporal Mitosis Detection in Phase-Contrast Microscopy via Likelihood Map Estimation by 3DCNN<|sep|>In this paper, we proposed an effective method for spatialtemporal mitosis detection by estimating the cell mitosis likelihood map. The method can detect multiple mitosis events and mitigate the annotation gap from ground-truth. In experiments, we conﬁrmed that our proposed method performs better than other methods. In addition, we demonstrated the effectiveness of our method for spatial-temporal localization. The proposed method won second place in the contest of CVPR workshop cell mitosis detection [11].
Universal properties of 3d O(4) symmetric models: The scaling function of the free energy density and its derivatives<|sep|>The availability of high accuracy numerical data on the order parameter and its susceptibility in a three dimensional, O(4) symmetric spin model allowed us to extract the underlying scaling function of the free energy density and its ﬁrst three derivatives. As the speciﬁc heat exponent α is negative in the 3d O(4) universality class, it is only the third derivative with respect to temperature, which diverges at the critical point. The corresponding scaling function f ′′′ f (z) has two extrema; a rather shallow minimum in the symmetry broken phase and a pronounced maximum in the symmetric phase. The latter is located at z3,0 p ≃ 1.45. This happens to be close to the location of the peak in the susceptibility of the order parameter, z0,2 p = 1.374(3). The higher order derivatives of the scaling function of the free energy density play a central role in the discussion of ﬂuctuations of conserved charges in QCD, e.g. the singular behavior of the 2n-th order cumulant of net baryon number ﬂuctuations is related to the n-th derivative of f f (z). The change of sign of f ′′′ f (z) and its pronounced maximum characterize the QCD transition. In fact, the change of sign of f ′′′ f (z) suggests that 6th order cumulants of net baryon number are negative in the vicinity of the QCD transition line. This may be detectable in a heavy ion collision, if the production of hadrons (freeze-out) occurs at temperatures and baryon chemical potentials that are close to the QCD crossover transition line.
State Identification for Labeled Transition Systems with Inputs and Outputs<|sep|>We studied the state identiﬁcation problem for suspension automata, generalizing results from [11]. We presented algorithms to construct test cases that distinguish all incompatible state pairs, if possible, or many, if not. Experiments suggest that this approach is quite eﬀective. We see several directions for future research. First, though we did apply our algorithms to instances of an industrial benchmark, we would like to apply it to diﬀerent case studies as well, to further explore the applicability of our approach. We note however that there are not that many (large) LTS benchmarks available. An open problem is to give a bound on the depth of the distinguishing graph that our algorithms constructs. For FSMs, a quadratic bound is known [11], with examples to show it is tight [23,11]. These examples extend to our setting, as we generalize from the FSM setting, but the proof for the quadratic bound on adaptive distinguishing sequences from [11] does not. If our algorithm returns an adaptive distinguishing graph that does not distinguish all incompatible state pairs, the question remains how to eﬃciently distinguish these remaining states. Graphs distinguishing pairs of states can be obtained directly from our splitting graph, or by computing them as in [4], but distinguishing all remaining pairs results in a large overhead compared to the small size of the distinguishing graph we obtained in our experiments. On the one hand, we can optimize the obtained distinguishing graph by improving the splitting graph’s quality by applying heuristics that optimize the choice of labels for splitting leaves. On the other hand, we can use causes for states not being distinguished to construct a distinguishing graph that distinguishes all or at least many of the not distinguished states. Though our distinguishing graphs signiﬁcantly improve the size of an ncomplete test suite, the problem to compute good access sequences for such a test suite requires further research as well [4]. Due to the output nondeterminism of suspension automata, we need an input-fairness assumption, to ensure that all outputs enabled from a state may eventually be observed. However, for access sequences we rather have a more adaptive strategy, in the spirit of [5], that reacts on the outputs as produced by the tested system rightaway. Adaptively choosing access sequences means that for reaching the same state, diﬀerent access sequences may be used. However, the proof of n-completeness of a test suite depends on using one unique access sequence for accessing the same state. It remains an open problem whether using diﬀerent access sequences breaks ncompleteness or not.
Franck-Condon Effect in Central Spin System<|sep|>We studied the inﬂuence of the environmental spins on absorption spectrum of the central spin. It is found that there exists similar FC eﬀect in the central spin model as that in conventional electron-phonon model. In the zero temperature case, the original Lorentz absorption spectrum of the bare central spin is shifted and split into few small peaks. And the most probable transitions, which make the largest contribution to the absorption spectrum, are governed by the “vertical transition” mechanism. If the system is at ﬁnite temperature, the peak of the absorption spectrum is markedly depressed and broadened. Especially, when the hyperﬁne coupling is strong enough, the excitation of the central spin is intensively suppressed, which leads to the spin F-C blockade. We thank Da Zhi Xu and Cheng-Yun Cai for helpful discussion. This work is supported by National Natural Science Foundation of China under Grants No.11121403, No. 10935010 and No. 11074261. where ǫijk is the totally antisymmetric Levi-Civita tensor, with ǫxyz = +1. With the help of these operators, the collective spins may be characterized by the simultaneous eigenstates |J, M⟩ of J2 and Jz with [34, 35] It is proofed that all of these states |J, M⟩ (Dicke states) are symmetric under permutations of the nucleus and all the symmetric states are in the subspace which is spanned by the states with maximal angular momentum J = N/2. Now we order the totally symmetric eigenstates as ���� N 2 − 1 � = J− |1, 1, . . . , 1⟩ = Sn |0, 1, . . ., 1⟩ ���� N 2 − 2 � = J2 − |1, 1, . . . , 1⟩ = Sn |0, 0, 1, . . ., 1⟩ . . . . . . . . . ���� N where Sn is the symmetrization operator and |N/2, −N/2⟩ the ground state of the atomic ensemble. Since all the operations are proceeding in the subspace of J = N/2, we abbreviate the eigenfunction {|N/2, M = −N/2 + m⟩ , m = 0, 1, 2, . . ., N} of the operators {J2, Jz} as It should be noted that we re-marked the eigenstate of the total angular momentum, with the excitation number m of the nuclear spins. The rotating operator corresponding to a rotation about the y axis reads as Since J2 commutes with the rotation operators (i.e. [J2, Jy] = 0) and hence the subspace of J = N/2 is an [4] M. Lax, J. Chem. Phys. 20, 1792 (1952). [5] K. Huang and A. Rhys, Proc. Roy. Soc. A 204, 406 (1950). [9] Y. M. Hu, W. L. Yang, Y. Y. Xu, F. Zhou, L. Chen, K. L. Gao, M. Feng, and C. Lee, New J. Phys. 13, 053037 (2011). [17] T. A. Kennedy, J. S. Colton, J. E. Butler, R. C. Linares, and P. J. Doering, Appl. Phys. Lett. 83, 4190 (2003). [18] L. Childress et al., Science 314, 281 (2006). [19] R. Hanson, V. V. Dobrovitski, A. E. Feiguin, O. Gywat, and D. D. Awschalom, Science 320, 352 (2008).
mGNN: Generalizing the Graph Neural Networks to the Multilayer Case<|sep|>In this paper we present an innovative way of employing existing Graph Convolutional Networks on multi-layer networks. Compared to other works, our proposal is problem agnostic and works for any multi-layer network. Moreover, it is transparent to the training (i.e., any type of training is supported), the node feature propagation happens in both the intra- and inter- layer independently and multiple layers can be stacked to capture information from the topology and the features farther in the network. We validate our proposal on three different tasks: multiplex node-classiﬁcation, intra-layer link prediction and network classiﬁcation. The results show that the approach is general and performs well in different settings, without any computational over-head which allows the application of the method to large multi-layer networks. The model we use to classify the nodes has 6 supra-layers with identical sub-layers (i.e., GAT layers with 60 output features and 5 heads each, negative slope 0.2), followed by a Multi-Layer Perceptron with 6 outputs (the classiﬁer) that takes as input the features of the replicas of each node and predicts its class. The model is trained for 2500 epochs (with 100 epochs patience) with learning rate 5 · 10−4, weight decay 10−3 and 0.3 dropout probability. We use the Cross Entropy loss function with a rescaling weight to account for the different distribution of classes. The model we use to perform link prediction has 3 identical supra-layers (i.e., GAT sub-layers with 30 output features and 5 heads each, negative slope 0.2). We train the model for 2500 epochs (with 400 epochs patience) with learning rate 10−3, weight decay 10−5 and 0.3 dropout probability.
Microscopic pairing theory of a binary Bose mixture with interspecies attractions: bosonic BEC-BCS crossover and ultradilute low-dimensional quantum droplets<|sep|>In summary, we have presented a systematic investigation of bulk properties of utradilute quantum droplets in a Bose-Bose mixture, by using the recently developed pairing theory [34]. We have focused on the lowdimensional droplets, and have found that the bosonic pairing plays an increasingly important role in low dimensions, particularly near the threshold at which the self-bound droplets start to emerge or disappear, as listed in Table I. We have also considered a strongly interacting quantum droplet in three dimensions. In one dimension, we have shown that the energy per particle predicted by our pairing theory agrees excellent well with the numerically accurate diﬀusion Monte Carlo data [28], at all the interaction strengths where the simulation data are available (which also nearly cover the phase window where one-dimensional quantum droplets exist). Our pairing theory also predicts a critical interspecies attraction for the emergence of droplets, i.e., (g12/g)crit ∼ −0.35, which is consistent with the DMC prediction (g12/g)crit ∼ −0.47(2) [28] and with the zerocrossing point (g12/g)0 ∼ −0.45 where the eﬀective dimer-dimer interaction changes from repulsive to attractive [52]. In two dimensions, quantum droplets form for an arbitrary small interspecies attraction. We have found our pairing theory becomes less eﬃcient, due to the weak interspecies attraction for pairing and the logarithmically small controlling parameters that disfavors the development of accurate perturbation theories. Yet, our pairing theory still provides an improvement compared with the prototype theory of two-dimensional quantum droplets developed earlier [19]. With increasing interspecies attractions, the pairing theory seems to become more useful. We have predicted a threshold [ln(a12/a)]crit ∼ 1.9, below which the droplet may turn into a many-particle bound state predicted earlier by Hammer and Son [54]. Interestingly, such a threshold is close to the zero-crossing [ln(a12/a)]0 ≃ 2.3 of the eﬀective dimer-dimer interaction in two dimensions found through few-body calculations [45]. In three dimensions, we have shown an exciting possibility of realizing the so-called bosonic BEC-BCS crossover, by tuning the interspecies scattering length a12 to be inﬁnitely large near a Feshbach resonance. The superﬂuid properties of the resulting strongly interacting quantum droplet are to be explored. We anticipate that it may have some universal behaviors in collective dynamics and thermodynamics, analogous to its fermionic counterpart. Across the Feshbach resonance, we have found that the strongly interacting quantum droplet disappears at about a12 ≃ 3.6a. In future studies, it would be interesting to use our microscopic pairing theory to directly investigate the proﬁle and the collective excitations of quantum droplets, without the use of the local density approximation or density functional theories. These fundamental properties are important for characterizing ultradilute quantum droplets in ultracold atomic laboratories. trakharchik, and Stefano Giorgini for sharing their DMC data. This research was supported by the Australian Research Council’s (ARC) Discovery Program, Grant No. DP170104008 (H.H.), Grants No. DE180100592 and No. DP190100815 (J.W.), and Grant No. DP180102018 (X.J.L). In Fig. 13, we show the numerical and analytical results of our pairing theory for the one-dimensional energy per particle at the interspecies interaction strength g12 = −0.75g. The analytical expression Eq. (71) does not provide a good approximation to the numerical result, since unlike in the three-dimensional case the assumption |µ| ≪ C, ∆0 is not satisﬁed so well. [1] F. Dalfovo, S. Giorgini, L. P. Pitaevskii, and S. Stringari, Theory of Bose-Einstein condensation in trapped gases, Rev. Mod. Phys. 71, 463 (1999). [2] E. A. Donley, N. R. Claussen, S. L. Cornish, J. L. Roberts, E. A. Cornell, and C. E. Wieman, Dynamics of collapsing and exploding Bose–Einstein condensates, Nature (London) 412, 295 (2001). [3] D. S. Petrov, Quantum Mechanical Stabilization of a Collapsing Bose-Bose Mixture, Phys. Rev. Lett. 115, 155302 (2015). [4] T. D. Lee, K. Huang, and C. N. Yang, Eigenvalues and Eigenfunctions of a Bose System of Hard Spheres and Its Low-Temperature Properties, Phys. Rev. 106, 1135 (1957). [5] D. S. Petrov, Liquid beyond the van der Waals paradigm, Nat. Phys. 14, 211 (2018). [6] I. Ferrier-Barbut, Ultradilute Quantum Droplets, Phys. Today 72, 46 (2019). [7] Y. Kartashov, G. Astrakharchik, B. Malomed, and L. Torner, Frontiers in multidimensional self-trapping of nonlinear ﬁelds and matter, Nat. Rev. Phys. 1, 185 (2019). [8] I. Ferrier-Barbut, H. Kadau, M. Schmitt, M. Wenzel, and T. Pfau, Observation of Quantum Droplets in a Strongly Dipolar Bose Gas, Phys. Rev. Lett. 116, 215301 (2016). [9] M. Schmitt, M. Wenzel, F. B¨ottcher, I. Ferrier-Barbut, and T. Pfau, Self-bound droplets of a dilute magnetic quantum liquid, Nature (London) 539, 259 (2016). [10] L. Chomaz, S. Baier, D. Petter, M. J. Mark, F. W¨achtler, L. Santos, and F. Ferlaino, Quantum-Fluctuation-Driven Crossover from a Dilute Bose-Einstein Condensate to a Macrodroplet in a Dipolar Quantum Fluid, Phys. Rev. X 6, 041039 (2016). [11] F. B¨ottcher, M. Wenzel, J.-N. Schmidt, M. Guo, T. Langen, I. Ferrier-Barbut, T. Pfau, R. Bomb´ın, J. S´anchez Baena, J. Boronat, and F. Mazzanti, Dilute dipolar quantum droplets beyond the extended Gross-Pitaevskii equation, Phys. Rev. Research 1, 033088 (2019). [12] F. B¨ottcher, J.-N. Schmidt, M. Wenzel, J. Hertkorn, M. Guo, T. Langen and T. Pfau, Transient Supersolid Properties in an Array of Dipolar Quantum Droplets, Phys. Rev. X 9, 011051 (2019). [13] C. Cabrera, L. Tanzi, J. Sanz, B. Naylor, P. Thomas, P. Cheiney, and L. Tarruell, Quantum liquid droplets in a mixture of Bose-Einstein condensates, Science 359, 301 (2018). [14] P. Cheiney, C. R. Cabrera, J. Sanz, B. Naylor, L. Tanzi, and L. Tarruell, Bright Soliton to Quantum Droplet Transition in a Mixture of Bose-Einstein Condensates, Phys. Rev. Lett. 120, 135301 (2018). [15] G. Semeghini, G. Ferioli, L. Masi, C. Mazzinghi, L. Wolswijk, F. Minardi, M. Modugno, G. Modugno, M. Inguscio, and M. Fattori, Self-Bound Quantum Droplets of Atomic Mixtures in Free Space, Phys. Rev. Lett. 120, 235301 (2018). [16] G. Ferioli, G. Semeghini, L. Masi, G. Giusti, G. Modugno, M. Inguscio, A. Gallemi, A. Recati, and M. Fattori, Collisions of Self-Bound Quantum Droplets, Phys. Rev. Lett. 122, 090401 (2019). [17] C. D’Errico, A. Burchianti, M. Prevedelli, L. Salasnich, F. Ancilotto, M. Modugno, F. Minardi, and C. Fort, Observation of quantum droplets in a heteronuclear bosonic mixture, Phys. Rev. Research 1, 033155 (2019). [18] For a recent review, see, for example, F. B¨ottcher, J.-N. Schmidt, J. Hertkorn, K. S. H. Ng, S. D. Graham, M. Guo, T. Langen, and T. Pfau, New states of matter with ﬁne-tuned interactions: quantum droplets and dipolar supersolids, arXiv:2007.06391 (2020). [19] D. S. Petrov and G. E. Astrakharchik, Ultradilute LowDimensional Liquids, Phys. Rev. Lett. 117, 100401
Constraining the Bulk Composition of Disintegrating Exoplanets Using Combined Transmission Spectra from JWST and SPICA<|sep|>We have investigated the feasibility of constraining the composition of the dust tail with combined spectroscopy with JWST MIRI and SPICA SMI. We have constructed a simple model of the dust tail that calculates transit light curves and transmission spectra of disintegrating planets. We have found that the combination of JWST (5–12 µm) and SPICA (17–36 µm) wavelength regions can provide stronger constraints on the composition of disintegrating planets than JWST alone (Table 2 and Figure 5). Although crystalline and amorphous silicates, SiC, and Al2O3 have a feature at similar wavelengths in the JWST range, crystalline silicates have additional features in the SPICA range that are absent for SiC and Al2O3. Moreover, crystalline Mg2SiO4 and crystalline Fe2SiO4 have peaks at diﬀerent wavelengths in the SPICA range. Thus, observations with SPICA in addition to JWST would greatly help to disentangle the degeneracy of the candidate minerals of disintegrating planets. We have also discussed how to infer the composition and formation process of disintegrating planets from the tail transmission spectra. According to previous studies, Mg-bearing minerals, Fe-bearing minerals, and SiC are potentially representative condensates of mantels of terrestrial planets, coreless planets, and carbon planets, respectively. One would be able to distinguish between these planets from tail transmission spectra in the JWST and SPICA bands because diﬀerent types of disintegrating planets produce dust tails of diﬀerent spectral features. Based on the expected observational noise of the lowresolution spectrographs of JWST MIRI and SPICA SMI, we have quantiﬁed the detectability of absorption features of the candidate minerals in the tail transmission spectra at R = 10. For a disintegrating planet located within the distance of 100 pc from the Earth with an optical depth deeper than ∼ 0.5%, tail transmission spectra in the JWST band show signiﬁcant absorption features of minerals with feature-to-noise ratio of ≳ 3, in a single transit (Fig 8). On the other hand, one can easily identify the mineral features in a single transit observation with SPICA if a disintegrating planet is located within 100 pc with a transit depth deeper than 2%. Multiple transit observations K2-22b’s dust tail with JWST will allow us to constrain the minerals composition of the tail, although detection of mineral features with SPICA would be challenging for this particular object. Current and upcoming dedicated missions for transit searches, such as TESS and PLATO, would be able to provide ideal targets for future transmission spectroscopy with JWST and SPICA. In this study, we have evaluated the detectability assuming the dust tails com posed of a single mineral species, as a ﬁrst step to examine the feasibility of such spectroscopic observations. The detectability in the case of the dust tails composed of multiple mineral species is left for future work. We thank Akemi Tamanai for providing optical constant data, and Aki Takigawa, Shota Notsu, and Shigeru Ida for useful discussions. This work was supported by JSPS KAKENHI Grant Numbers JP18J14557, JP19K14783, JP18H05438, and JP19K03926.
The 2006 November outburst of EG Aquarii: the SU UMa nature revealed<|sep|>We established the SU UMa nature of EG Aqr for the ﬁrst time by the detection of the superhumps with a mean period of 0.078828(6) days. The observed superoutburst showed a precursor, during which a hint of superhumps was seen at the last stage of the precursor. Extensive observations enabled us to examine detailed changes of the superhump period. It turned out that the superhump period kept almost constant near the bright maximum, after which the superhump period decreased normally at the rate of Pdot = ˙P/P = −8.2×10−5 all over the plateau stage. Although the origin is unknown, we detected a change of the period near the bright maximum. Concerning the observed period change and the presence of the precursor, the maximum radius of the accretion disk was not large during the superoutburst. The obtained light curves were typical of those of SU UMa-type dwarf novae. This is deﬁnitely peculiar when taking into account that EG Aqr showed only one recorded eruption until the 2006 superoutburst. Despite the inactivity of the variable, we conclude that EG Aqr is a new member of SU UMa-type dwarf novae. In future, quiescent studies both from photometry and spectroscopy are imperative in order to further understand the enigmatic object. We would like to thank Dr. Steve B. Howell for helpful comments on the paper. We acknowledge with thanks the variable star observations from the AAVSO and VSNET International Database contributed by observers worldwide and used in this research. We would also express our gratitude to G. Pojmanski for provid 6 Howell et al. (1993) determined a likely orbital period of EF Peg as 2.05 hours based on the observed superhump period. It should be noted that this is not a direct measurement of the orbital period. ing invaluable data of ASAS-3 observations. This work is supported by a Grant-in-Aid for the 21st Century COE “Center for Diversity and Universality in Physics” from the Ministry of Education, Culture, Sports, Science and Technology (MEXT). GM acknowledges the support of the Planetary Society and Software Bisque. This work is partly supported by a grant-in aid from the Ministry of Education, Culture, Sports, Science and Technology (No. 14079206, 16340057, 17684004, 17340054, 17740105, 18740153, 18840032). Part of this work is supported by a Research Fellowship of the Japan Society for the Promotion of Science for Young Scientists (RI, AI, KK, ON).
Multi-probe study of excited states in $\mathrm{^{12}C}$: disentangling the sources of monopole strength between the Hoyle state and $E_{x} = 13$ MeV<|sep|>Knowledge of the low-lying monopole strength in 12C—the Hoyle state in particular—is crucial for our understanding of both the astrophysically important 3α reaction and of α-particle clustering. Recent theoretical calculations predict a breathing-mode excitation of the Hoyle state at Ex ≈ 9 MeV with a width of Γ ≈ 1.5 MeV. The observation of this breathing-mode excitation is hindered by the presence of multiple broad states and potential interference eﬀects. The 12C(α, α′)12C and 14C(p, t)12C reactions were employed to populate states in 12C. A self-consistent, simultaneous analysis of the inclusive spectra with lineshapes accounting for experimental eﬀects and distortion due to nuclear dynamics yielded clear evidence for excess monopole strength at Ex ≈ 9 MeV, particularly for 12C(α, α′)12C at θlab = 0◦ and the data is not well reproduced by the previously established 0+ 2 and 0+ 3 states. The analysis of coincident charged-particle decay data supports this conclusion. An additional monopole state at Ex ≈ 9 MeV, denoted 0+ ∆, signiﬁcantly improved the description of both inclusive and charged-particle-gated data. This new monopole state is the leading candidate for the breathing-mode excitation of the Hoyle state and two parametrizations for the sources of monopole strength between Ex = 7 and 13 MeV were employed. The ﬁrst being submodel M 02 +, whereby the three sources of monopole strength (0+ 2 , 0+ ∆ and 0+ 3 ) were permitted to interfere according to Equation 2. For this submodel, the 0+ ∆ was optimized at Ex = 9.566 +− 0.018(stat) +− 0.104(syst) MeV with Γ(Er) = 3.203 +− 0.061(stat) +− 0.599(syst) MeV (see Table V for details). The second parametrization of the monopole strengths corresponded to submodel M 02 +, whereby the additional 0+ ∆ state was treated as an isolated resonance whilst the previously established 0+ 2 and 0+ 3 states were permitted to interfere. This parametrization may be appropriate if the 0+ ∆ state corresponds to a dynamic, collective monopole response which does not interfere with the neighboring 0+ states which are understood to be statically deformed. This submodel yielded Ex = 9.379 +− 0.013(stat) +− 0.050(syst) MeV with Γ(Er) = 4.565 +− 0.022(stat) +− 0.107(syst) MeV with ΓFWHM = 2.066 +− 0.005(stat) +− 0.098(syst) MeV. For the Γ(Er) width of the 0+ ∆ state, better agreement may be achieved with theoretical predictions by using more sophisticated models for the excitation-energy dependence compared to the R-matrix parametrization used in this work. In contrast, the recommended ΓFWHM width may be more model independent than Γ(Er) and is in good agreement with the predictions of Refs. [18, 19]. The highly collective nature of this additional 0+ ∆ state is supported by the fact that a signiﬁcant excess of monopole strength is not observed at Ex ≈ 9 MeV in the measurement of 14C(p, t)12C at θlab = 0◦—a reaction that is suggested to suppress collective isoscalar monopole excitations relative to 12C(α, α′)12C, because of the diﬀerent characters of the pairing and radial operators responsible for the two reactions, respectively. The interpretation of the 0+ ∆ state as a breathing-mode ex citation, composed of coherent 1p-1h excitations, is further supported by its weak population through β-decay [4], which is understood to be less selective towards 1p1h components of collective excitations in comparison to other nuclear/electromagnetic probes [67]. An alternative explanation, which must be considered, is that the excess monopole strength is symptomatic of a requirement for more sophisticated theoretical descriptions of the properties of the Hoyle state, which may inﬂuence the temperature-dependence of the 3α rate at T9 ≳ 2. This work is based on the research supported in part by the National Research Foundation of South Africa (Grant Numbers: 85509, 86052, 118846, 90741). The authors acknowledge the accelerator staﬀ of iThemba LABS for providing excellent beams. The authors would like to thank E. Khan, A. Tamii, B. Zhou, K. Masaaki, H.O.U. Fynbo, M. Itoh and J. Carter for useful discussions. PA acknowledges support from the Claude Leon Foundation in the form of a postdoctoral fellowship. The computations were performed on resources provided by UNINETT Sigma2 - the National Infrastructure for High Performance Computing and Data Storage in Norway. The authors are grateful to A.C. Larsen, F. Zeiser and F. Pogliano for their assistance with UNINETT Sigma2. In Section IV, the presented results correspond to the penetrability prescription of Equation 5. In this section, a set of analogous results are presented using the alternative penetrability prescription of Equation 6. The ﬁt results presented in this section are highly similar to those in Section IV and lead to the same conclusions. Consequently, the explanations for the ﬁt results in Section IV are not repeated in this section. The optimal ﬁts for submodels M 02 +(+α0,−α1) are presented in Figs. 19, 20, 21 and 22, respectively. The corresponding ﬁt results are summarized in Table VII. The recommended observable parameters for submodel M 02 +(+α0,−α1) are summarized in Tables IX with the corresponding decomposition in Fig. 24. It is observed that the observable parameters extracted using the penetrability prescription of Equation 6 agree well with the results in Section IV which correspond to the penetrability prescription of Equation 5.
Pure Gauge Configurations and Solutions to Fermionic Superstring Field Theories Equations of Motion<|sep|>In this article a singular limit of the pure gauge solution is discussed. We propose a simple recept to deal with a singularity problem and on the example of cubic SSFT show that it gives the same answer as the requirement to get a desirable value of the action [33] (see the discussion of the same question for the case with GSO(−) sector in [41] ) The equivalence of the solutions of the equation of motion in the cubic fermionic string ﬁeld theory [27] and that of the non-polynomial fermionic string ﬁeld theory [30] including the GSO(−) sectors is discussed using the matrix representations of both theories. However the singularity problem recall that a formal gauge equivalence of two theories needs a rather delicate studies. The work is supported in part by RFBR grant 08-01-00798 and NS-795.2008.1.
A Graph Model for Imperative Computation<|sep|>We have shown that a simple amendment of Scott’s Pω graph-model gives rise to a model of imperative computation, in the event-based style of Reddy’s object-spaces model and later models based on game semantics. Moreover we have shown that this model contains a universal type, thus yielding a very cheap proof of full abstraction for the language SCImk,ran. With some additional work we have established full abstraction for the original SCI language via conservativity results; this was not known prior to our work. We believe that the general approach of constructing models in this way is of interest and has the potential to give rise to a range of interesting concrete models and some useful insights at a more abstract level. We intend to develop an axiomatic presentation of our constructions, expanding on the work of Hyland et al. [7]. At present it is not clear whether the more reﬁned game-based models can be presented in this style; this remains a topic for further investigation.
Alpenglow - A Signature for Chameleons in Axion-Like Particle Search Experiments<|sep|>In this paper we have proposed a hitherto unnoted signature of chameleon ﬁeld theories — an afterglow eﬀect — which could be easily exploited in conventional axion-like particle search experiments. In strong magnetic ﬁelds, laser photons can convert to chameleon particles which can be trapped inside a vacuum cavity. We have shown that these particles are expected to form a gas inside the cavity, and that they might reveal themselves as an afterglow after the laser is switched oﬀ. In general, for long enough loading times, a stationary state is achieved, where the chameleon production in the laser beam and the loss via backconversion to photons is balanced. In this case, the afterglow rate only depends on the initial production rate and not on the details of the experiment. This kind of experiment is in principle sensitive to couplings below 1/M ∼ 10−8 GeV−1, which is orders of magnitude smaller than the sensitivity of typical laser polarization experiments. Its sensitivity also compares favorably with Casimir force experiments. The main constraints on afterglow experiments come from the required loading times which can easily become orders of magnitudes larger than ∼ O(100 s). We have also sketched the concept of a Fabry-P´erot cavity for the chameleons. With an appropriate tuning of phase shifts the sensitivity of the experiment compared to the case of a chameleon gas might be further increased by a factor ∆t/4ℓ in the case where the loading time ∆t is much smaller than the inverse loading rate Γ. In the (academic) case ∆tΓ ≫ 1 the cavity will act as a mirror for the incoming photon beam and the rate of afterglow photons in the unloading phase of the cavity reaches the intensity of the initial laser beam. However, in an experimental realization of this concept the time scale ∆t will be limited by the eﬀective ﬁnesse of the chameleon cavity, i.e. the maximal number of cycles achievable. Open questions concern the possible thermalization of the chameleon gas, and the question under which circumstances the particle picture breaks down due to strong coupling eﬀects. The answers will most probably depend strongly on the speciﬁc potential V (φ) and might lead to strong constraints on chameleon theories. We will consider this elsewhere [26].
Linear quantum systems: a tutorial<|sep|>In this tutorial, we have given a concise introduction to linear quantum systems, for example, their mathematical models, relation between their control-theoretic properties and physical properties, Gaussian states, quantum Kalman ﬁlter, Kalman canonical form, and response to continuous-mode single-photon states. Several simple examples are designed to demonstrate some fundamental properties of linear quantum systems. Pointers to more detailed discussions are given in various places. It is hoped that this tutorial is helpful to researchers in the control community who are interested in quantum control of dynamical systems. Finally, an information-theoretic uncertainty relation has been recorded in this tutorial, which describes uncertainties of mixed quantum Gaussian states better than the Heisenberg uncertainty relation. It is an open question whether this uncertainty relation is useful for mixed quantum Gaussian state engineering.
Constraining extra dimensions on cosmological scales with LISA future gravitational wave siren data<|sep|>In this paper we have investigated the potential capability of the LISA satellite to place constraints on higher-dimensional cosmological theories with non-compact spacetime dimensions, by using GW standard sirens observed with an EM counterpart. In the absence of a complete, unique GW model for these theories, we used a phenomenological ansatz for the GW amplitude, which is based on the physics of the DGP model with positive branch solutions and includes a screening scale Rc beyond which the GWs leak into the higher dimensions, leading to a reduction in the amplitude of the observed GWs and hence a systematic error in the inferred distance to the source. Considering various plausible cosmological scenarios (number of dimensions D = 5, 6 or 7; steepness parameter n = 1 or 10 and screening scale Rc = RH ∼ 4Gpc or 4RH) and three models for the MBHB formation (“heavy seeds without delays"; “heavy seeds with delays" and “popIII stars") we have investigated catalogues of MBHB events for which an optical counterpart may plausibly be expected, so that each event may be assigned a GW luminosity distance, the redshift of corresponding EM counterpart and an estimate of the error on measured distance. These catalogues are based on those previously presented in [16, 17] where they were used to investigate LISA constraints on other cosmological models. We have found that, in general, the heavy seeds with no delays model (where the number of detectable standard sirens amounts to a four-year average of ∼ 27) gives systematically better model constraints than the other two scenarios, where the four-year average only amounts to about 12 and 14 sirens for the popIII and Q3nod respectively. Thus, we conclude that the ability of LISA to place meaningful constraints on the number of spacetime dimensions and screening scale will strongly depend on the actual number and redshift distribution of MBHB merger events, and the corresponding eﬃciency in identifying a host galaxy redshift. Furthermore, by considering two diﬀerent scenarios, denoted ‘optimistic’ and ‘realistic’, for the total error on the luminosity distance – derived from a combination of the GW observations, the impact of peculiar velocities and weak lensing, and the Figure 8: Joint posterior PDF for the full seven-dimensional parameter space – comprising the four DGP parameters augmented by D, Rc and n – analysing Pantheon and LISA data combined. The analysis is for a representative catalogue in the Q3d formation scenario and assuming optimistic errors. True parameter values D = 5,Rc = 1.2RH,n = 1 were adopted and a value of M = −19.3 was adopted for the Pantheon sample. uncertainty on the redshift of the counterpart – we concluded that the constraints on the parameters will strongly depend on the error with which we can measure the redshift of the sources, the constraints being substantially weaker for the realistic error scenario. We also found that LISA’s ability to constrain higher-dimensional theories, as deﬁned by the scaling relation in (3.6), will depend on the cosmological parameters deﬁning the theory – namely the number of dimensions, screening scale and transition steepness – with the (D = 7, Rc = RH, n = 10) scenario, where modiﬁcation from GR is the most pronounced, being much better constrained than the other cases. In the model where the modiﬁcation to gravity is the least pronounced, namely (Rc = 4RH, n = 1), we found that LISA is not sensitive enough to provide meaningful constraints on the parameters. This is the case for D = 5, 6 and 7 and all MBHB formation models, although the popIII and Q3d scenarios give slightly worse results. On the other hand, for any of the other cosmological models and error scenarios considered, we found that LISA will be able to place meaningful constraints on the number of spacetime dimensions. Moreover, this constraining power considerably improves not only for greater numbers of standard sirens observed but also the steeper the transition and the smaller the screening scale – reaching a precision at the level of 0.86% for the most favorable cosmological scenario and formation model. We found that the screening scale Rc is not as well constrained as D, but that unlike the constraints on D, the constraints on Rc are very sensitive to the true number of spacetime dimensions. Indeed, LISA is unable to place meaningful constraints on Rc when D = 5, unless (n = 10, Rc = RH) and the Q3nod or Q3 model is considered, but for D ≥ 6, except for the least favourable cosmological scenario, LISA can place meaningful constraints on Rc in all cosmological models and MBHB formation scenarios reaching a precision of 7.2% for (D = 7, n = 10, Rc = RH) within the Q3nod model for the realistic error scenario. Finally, we investigated how strongly the siren data would favour the true model of modiﬁed gravity from which it was simulated, when compared with the DGP model. To that end we calculated the Bayesian evidence in favour of each higher-dimensional model considered, and found that for the case of optimistic siren errors the log evidence favored the true NGR model in every case. For realistic siren errors the evidence generally also favored the true NGR model – although the log Bayes factor values were substantially smaller and with larger scatter. In summary, we have shown that standard sirens observed with LISA in the redshift range 1 < z < 8 have the potential to test higher-dimensional theories with non compact extra-dimensions in a completely diﬀerent way than current EM probes. However, our analysis is a phenomenological one, modelling the GW damping by considering a very general type of leakage for large extra dimensions. Our results do not hold for higher-dimensional theories with compact extra dimensions such as string theory. Additionally, even though the GW amplitude scaling used applies to the DGP model, the so-called ‘infrared transparency’ eﬀect can be shown to result in a distance at which GW damping is manifested in the DGP model much beyond the distances to sources observable with frequencies relevant to LISA [32]. Nevertheless, our analysis still provides a useful measure of the constraints we can place on extra-dimensional theories with large (≥ 100km) extra dimensions. CE-R is supported by the Royal Astronomical Society as FRAS 10147, PAPIIT-UNAM Project IA100220. M. H. is supported by the Science and Technology Facilities Council (Ref. ST/L000946/1). M.C. is supported by the Perimeter Institute for Theoretical Physics. The authors gratefully acknowledge the help of Alberto Sesana and Nicola Tamanini, and their co-authors, for providing access to the mock LISA siren catalogues used in this paper.
Attention-Based Models for Speech Recognition<|sep|>We proposed and evaluated a novel end-to-end trainable speech recognition architecture based on a hybrid attention mechanism which combines both content and location information in order to select the next position in the input sequence for decoding. One desirable property of the proposed model is that it can recognize utterances much longer than the ones it was trained on. In the future, we expect this model to be used to directly recognize text from speech [10, 17], in which case it may become important to incorporate a monolingual language model to the ARSG architecture [26]. This work has contributed two novel ideas for attention mechanisms: a better normalization approach yielding smoother alignments and a generic principle for extracting and using features from the previous alignments. Both of these can potentially be applied beyond speech recognition. For instance, the proposed attention can be used without modiﬁcation in neural Turing machines, or by using 2–D convolution instead of 1–D, for improving image caption generation [3]. The authors would like to acknowledge the support of the following agencies for research funding and computing support: National Science Center (Poland), NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs and CIFAR. Bahdanau also thanks Planet Intelligent Systems GmbH and Yandex.
Fast and Accurate Optical Fiber Channel Modeling Using Generative Adversarial Network<|sep|>Fig. 19.  Optical spectra of channel output (200 km, 0 dBm): (a) SSFM-based channel output (b) GAN-based channel output without Ly loss, (c) GAN-based  channel output with Ly loss. Fig. 20.  Amplitudes of optical waveforms of channel output modeled by SSFM, GAN without Ly loss, and GAN with Ly loss at (200 km, 0 dBm). running time. The multiplication number is only 2% of SSFM  at 1000-km fiber transmission. The normalized MSE is 0.00925  at (1000 km, 0 dBm), which is much lower than the upper bound  of the normalized MSE. GAN also has good generalization  abilities for launch powers, modulation formats, and data  distributions, which provides flexibilities and versatility for the  application of channel modeling. GAN also has the capabilities  to model wavelength- and polarization-division multiplexed  optical channel, because the structure of GAN proposed in this  article has no limitation on the input signals. The new condition  vector structure and parameters of GAN may be required since  the complicated inter channel interferences, which would be  further investigated in the future. GAN also has the potential to  model more devices and even the whole physical optical  communication system. As a low complexity and DL-based  modeling tool, GAN is compatible to be embedded in other  neural networks for better signal design, such as new  modulation format, channel coding, and shaping filter.
Limits of the Stokes and Navier-Stokes equations in a punctured periodic domain<|sep|>We have analysed three models in a simple but unusual geometry, the ‘punctured periodic domain’, showing that the inﬂuence of the obstacle, a disc of radius r, evaporates in the limit as r → 0. Some interesting open problems remain. While the lack of a bound on the average of the solution ur over Ω (in both the Poisson and Stokes problems) that is uniform in r appears initially to be only a mathematical curiosity, such a bound is central to tackling the stationary Navier–Stokes problem in this geometry. The fact that there is no ‘uniform elliptic regularity’ for the Laplacian or Stokes operator in this geometry means that the important ‘vanishing tracer’ problem (cf. [3, 26]) also remains open. Very recently, Lacave & Takahasi [17] obtained a partial result in the two-dimensional case assuming that the density of the solid is independent of r. They employed some optimal Lp−Lq decay estimates of the semigroup associated to the ﬂuid-rigid body system. We plan to return to this in a future paper. This article was written during a part time employment of MC at the S. M. Nikolskii Mathematical Institute of RUDN University, 6 MiklukhoMaklay St, Moscow, 117198. The publication was supported by the Ministry of Education and Science of the Russian Federation (Agreement number 02.a03.21.0008). MC also thanks Monash University for an invitation during which a part of this article was completed. GP thanks the Mathematics Institute of the University of Warwick for their kind hospitality during her visit there. GP was partially supported by FAPESP, grant 2013/00048-3 and CNPq, grant 306646/2015-3, Brazil. JCR was partially supported by an EPSRC Leadership Fellowship EP/G007470/1, and would like to thank MC and WM for their hospitality in Zurich.
Robust Estimators in Generalized Pareto Models<|sep|>We have derived optimally robust estimators MBRE, OMSE, and RMXE for scale and shape parameters ξ and β of the GPD on ideal and contaminated data. Their computation has largely been accelerated by interpolation techniques. Among the potential starting estimators, clearly MedkMAD in its variant Hybr excels and comes closest to the aforementioned group. For the same purpose, PE is also robust, but not really advisably due to its low breakdown point and non-convincing efﬁciencies; the only reason for using PE is its ease of computation, which should not be so decisive. Even worse is the popular SMLE without bias correction, which does provide some, but much too little protection against outliers. Asymptotic theory and empirical simulations show that Hybr, MedkMAD, MDE, MBRE, OMSE, and RMXE estimators can withstand relatively high outlier rates as expressed by an (E)FSBP of roughly 1/3 (compare R.& H. [46, 47]). SMLE in the variant without bias correction as used in this paper, but with shrinking skipping rate, and MLE have minimal FSBP of 1/n, hence should be avoided. High failure rates for MMed and MedkMAD for small n, and under contamination limit their usability considerably, while Hybr works reliably. Looking at the inﬂuence functions, we see that, except for MLE, all estimators have bounded IFs, so ﬁnite GES, but do differ in how they use the information contained in an observation. This is reﬂected in asymptotic values, as well as in (simulated) ﬁnite sample values: for known radius we can recommend OMSE with Hybr as initialization. It has best statistical properties in the simulations, is computationally fast, efﬁcient for contamination of known radius. MBRE, and MDE come close to OMSE. For unknown radius RMXE is recommendable with again OMSE, MBRE, Hybr and MDE (in this order) as close For each of the estimators discussed in Section 4, we determine its IF, its asymptotic variance asVar, its maximal asymptotic bias asBias, and its FSBP where possible. All estimators considered in this appendix are deﬁned in the original (β-)scale and equivariant in the sense of (2.4). ξ 2 �−(ξ 2 + ξ)log(v)+ (2ξ 2 + 3ξ + 1)vξ − (ξ 2 + 3ξ + 1) ξ log(v)− (2ξ 2 + 3ξ + 1)vξ + (3ξ + 1) asBias Both components of the joint IF are unbounded—althoughonly growing in absolute value at rate log(x). FSBP The FSBP of MLE is minimal, i.e.; 1/n. As we have seen, SMLE in fact does not estimate θ but d(θ) = θ + Bθ, for bias Bθ already present in the ideal model.
On the Geometry of Supersymmetric Quantum Mechanical Systems<|sep|>By reformulating the supersymmetric systems we started out from in terms of geometric algebra, we could easily identify them as special cases of the following: H = −∆ + |f|2 + � divf + ∇ ∧ f − 2f ∧ ∇ � K, K, Q = −ˆı∇ + fI Not only do we have a clear geometric interpretation of every constituent of these systems, but relations between them are also simple to derive in this language. It is also interesting to note the similarities between these higher-dimensional systems and the well-studied one-dimensional toy model (82). Furthermore, by relaxing the requirement of a canonical complex structure and hermiticity of the supercharges, we have seen that we can also ﬁnd purely real analogues of these systems in arbitrary dimensions. I thank Jens Hoppe and Lars Svensson for useful discussions and valuable comments on the manuscript. I would also like to thank Volker Bach and Hubert Kalf for discussions and hospitality at Mainz University and King’s College London, respectively, as well as Gian Michele Graf for discussions.
Structure and Magnetic Fields in the Precessing Jet System SS433 III. Evolution of the Intrinsic Brightness of the Jets from a Deep Multi-Epoch VLA Campaign<|sep|>We have presented analysis of a sequence of ﬁve deep observations of SS 433 made over the summer of 2007 using the VLA at 5 and 8 GHz. The main results are as follows. 3Begelman et al. (1980) ﬁnd that the disk wind travels with a velocity of 1500 km/s, which is only ∼ 2% of the jet speed (∼ 8× 105 km/s). Thus we assume that the external environment is stationary relative to the jets. 1. We measure the spectral index to be 0.74 ± 0.06, consistent with previously reported values, and ﬁnd no signiﬁcant variations either across the source or with precession phase. 2. All the results of Paper II (based on a single very deep 5 GHz observation made in 2003) are conﬁrmed, but with the sequence of ﬁve closely spaced observations, the possibilities for analysis are greatly increased. 3. The jet intrinsic brightness proﬁles in 2007 are signiﬁcantly diﬀerent from those in 2003, even though the precession phases are similar. We attribute this to variability at the core, injecting varying power at the base of the jets. 4. As in Paper II, in every image the proﬁles of the east and west jets are remarkably similar if projection and Doppler beaming are taken into account. 5. The sequence of ﬁve images allows us to disentangle the evolution of individual pieces of jet from variations of jet power originating at the core. We ﬁnd that the brightness of individual pieces of jet fades as an exponential function of age (or distance from the core) rather than a power law. The exponential time constant is 55.9 ± 1.7 days. Our results are consistent with the brightness of the two jets evolving in the same way. 6. There is also signiﬁcant variation (by a factor of at least ﬁve) in jet power with birth epoch, with the east and west jets varying in synchrony. The variation is also consistent between 5 and 8.5 GHz. 7. The lack of deceleration between the scale of the optical Balmer line emission (1015 cm) and that of the radio emission (1017 cm) requires that the jet material is much denser than its surroundings. We ﬁnd that the density ratio must exceed 300:1. This material is based upon work supported by the National Science Foundation under Grants Nos. 0307531 and 0607453 and prior grants. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of the National Science Foundation. D.H.R. gratefully acknowledges the support of the William R. Kenan, Jr. Charitable Trust. The National Radio Astronomy Observatory is a facility of the National Science Foundation, operated under cooperative agreement by Associated Universities, Inc. For plotting and image analysis, we have made use of the CFITSIO and MFITSIO packages made available by the NASA Goddard SFC and Damian Eads of LANL, respectively. Facilities: VLA/EVLA (A array) 2008 June 6 260 16/4 16/9 0.72 2007 July 1 283 17/5 17/8 0.86 2007 July 18 300 16/6 15/9 0.96 2007 Aug. 5 318 14/7 16/9 0.07 2007 Aug. 24 337 15/5 16/7 0.19 2007 June 8 0.404 0.340 −68 329 0.12 0.04 2007 July 1 0.381 0.335 −22 359 0.10 0.03 2007 July 18 0.410 0.348 −69 261 0.11 0.04 2007 Aug. 5 0.414 0.345 −88 225 0.14 0.05 2007 Aug. 24 0.356 0.330 35 467 0.19 0.05 2007 June 8 0.219 0.208 53 211 0.08 0.03 2007 July 1 0.219 0.201 −31 221 0.08 0.03 2007 July 18 0.224 0.209 63 141 0.09 0.03 2007 Aug. 5 0.238 0.205 −73 129 0.07 0.02 2007 Aug. 24 0.240 0.213 −49 361 0.11 0.04 2007 June 8 91, 142, 253, 304 41, 231, 420 110, 267 121, 291 30, 173, 315 2007 July 1 114, 165, 276, 327 67, 257, 447 133, 289 146, 315 52, 192, 335 2007 July 18 20, 131, 182, 293, 344 88, 278 150, 305 163, 333 66, 208, 350 2007 Aug. 5 38, 149, 200, 311, 362 107,298 12, 168, 333 12, 182, 352 81, 223 2207 Aug. 24 57, 168, 219, 330, 380 131, 320 30, 185, 341 32, 202, 370 98, 240
K\"ahler-driven Tribrid Inflation<|sep|>A Consistency of Assumptions 22 A.1 Smallness of Slow-roll Parameters . . . . . . . . . . . . . . . . . . . . . . . 22 A.2 Validity of Semiclassical Treatment . . . . . . . . . . . . . . . . . . . . . . 23 A.3 Eﬀects of Higher-Dimensional Operators . . . . . . . . . . . . . . . . . . . 23
NIR jets from a clustered region of massive star formation: Morphology and composition in the IRAS 18264-1152 region<|sep|>We have studied the morphology and composition of the IRAS 18264−1152 massive star-forming complex via integral ﬁeld unit observations from SINFONI and KMOS in the K-band, as well as CO (2−1) data from SMA. Our results can be summarised as follows: 1. In the SINFONI ﬁeld of view we observed three point sources (from S5 to S7) emitting Brγ, which is consistent with YSO activity. Furthermore, we report another source (S4) that displays a rising K-band continuum and that has shown other YSO characteristics in previous studies (Rosero et al. 2016; Issac et al. 2020). Further away from the central cluster, through the KMOS maps, we found two more Brγ emitting sources (S8 and S9), but it is undetermined whether they belong to the IRAS 18264−1152 central cluster. We thus proposed ﬁve new YSOs for this region (S5 to S9) based on our work, and presented further evidence to justify the youth of S4. 2. We focussed our near-infrared analysis on the H2 jet knots found in the region. We identiﬁed seven H2 jet knots in the KMOS data (labelled K1 to K7) and three jet knots in the SINFONI data (K2*, K3, and K8), where two of which are the same as those resolved in KMOS. Large-scale outﬂow lobes are found in the directions of NE (K1, K2, K3?) and NW (K5, K7), with two smaller jet knots to the north (K4) and south-west (K6). Our observations are in good agreement with the previous NIR works that found a large-scale east-west outﬂow with other non-aligned knots (e.g. Varricatt et al. 2010; Lee et al. 2012; Issac et al. 2020, although with no kinematic information in the NIR). 3. We found that the H2 jet emission is shock driven by computing the ﬂux ratio of the H2 transitions 1 − 0 S(1) and 2 − 1 S(1). 4. Upon inspection of the radio velocity maps, we found that the large-scale jets detected are blue-shifted, as well as the northern knot, whereas the south-west knot is red-shifted, and there is a red-shifted bow shock knot at the west edge of the KMOS ﬁeld of view. It seems that the blue lobes belong to diﬀerent outﬂows, and their red counterparts are most likely obscured as they are not clearly detected in our K-band data. 5. We computed visual extinction estimates for the region, ranging from ∼ 10 to ∼ 20 magnitudes, and measured the line ﬂuxes of all H2 transitions present in each knot. With this information, we constructed and ﬁtted ro-vibrational diagrams to yield estimates of H2 temperatures and H2 column densities of 2100 K to 2800 K and 1018 to 1021 cm−2, respectively. The areas of the H2 knots vary between 0.2′′and 60′′, and the lower limits for the H2 masses (due only to H2 warm component) range from 5 × 10−4 M⊙ to 5 × 10−3 M⊙. 6. The CO (2−1) emission traces two clear bipolar outﬂows in the NE - SW and SE - NW directions, which is consistent with the works of Beuther et al. (2002a); Issac et al. (2020). In general, the blue lobes of the CO outﬂows are in good agreement with the blue lobes of the H2 jets. The red-shifted counterpart of the SE - NW CO outﬂow (K10) lies in a region that was not covered by the NIR observations or that has very low signal-to-noise, thus we cannot detect a corresponding H2 jet. On the other hand, the red lobe of the NE - SW CO outﬂow overlaps with a small H2 red-shifted knot (K6), suggesting that it might be the counterpart of the blue lobe to the north-east. Additionally, a red-shifted CO knot was found near the central region, just oﬀ the west of the SINFONI ﬁeld of view. This knot is consistent with the C18O (1−0) and C17O (3−2) observations of Issac et al. (2020) where a collimated SE - NW CO outﬂow was found, having the redshifted emission on the NW side. Moreover, this knot is radially aligned with the H2 red-shifted bow shock in K7, possibly unveiling another outﬂow. 7. We created the spectral energy distribution plot for IRAS 18264−1152 with archival data from Spitzer, Herschel, and WISE, and ﬁtted it using the Zhang & Tan (2018) radiative transfer models based on the Turbulent Core Accretion model (McKee & Tan 2003). Though this complex likely harbours a protocluster, we assume the scenario where one massive driving source dominates the luminosity of the region. The ﬁve best ﬁt models provided estimates for the mass of the driving source, bolometric luminosity, predicted initial mass of the core, extinction, and mass accretion rate of 4 - 8 M⊙, ∼ 0.9 − 1.2 × 104L⊙, 100 - 480 M⊙, > 150 mag, and ∼ 2.0 − 6.5 × 10−4 M⊙ yr−1, respectively. We also explored the scenario where two sources dominate the SED equally. We then ﬁtted an SED with 50% ﬂux at each wavelength to represent the individual contribution of each source. Consequently, we obtained smaller values for the central mass of the protostar that ranges from 2 to 4 M⊙forming in a core with initial mass of 60-200 M⊙. Our main conclusion of this study regards the outﬂow geometry in the IRAS 18264−1152 region, for which we report at least two, and up to four diﬀerent outﬂows. Both the H2 and CO (2−1) observations trace two large-scale bipolar outﬂows in the SE - NW and NE - SW direction (outﬂows I and II), and a possible red-shifted outﬂow to the west (outﬂow IV), whereas the possible blue-shifted north outﬂow (outﬂow III) is only revealed in the H2 maps. We compare our outﬂows with the locations of IRAS 18264−1152 massive sources and thermal jets listed in the literature. The main driving sources of the cluster are believed to be sources R2/MYSO/A/S4 (RA(J2000) = 18:29:14.69, Dec(J2000) = 11:50:23.6) and R1/MM2/b/F (RA(J2000) = 18:29:14.36, Dec(J2000) = -11:50:22.5). Further observations at high angular resolution in the MIR (e.g. with SOFIA) are required to ﬁnd the counterparts of the outﬂows presented in this study. Moreover, such data will help us to clearly assess the origin of the red-shifted bow shock, and to check whether point sources S8 and S9 truly belong to the IRAS 18264−1152 cluster. In conclusion, our study strongly supports the scenario of several outﬂows driven by multiple sources and demonstrates that NIR observations have high diagnostic power in probing high-mass star-forming regions. The outﬂows show high degrees of collimation on large scales suggesting that massive stars can form in a relatively ordered manner, consistent with core accretion models, even in a clustered region. Acknowledgements. We thank the referee for their comments and suggestions which improved the clarity of the manuscript. We would like to thank Prof. Dr. Henrik Beuther for assistance with the SMA observations. A.R.C.S. acknowledges funding from Chalmers Astrophysics and Space Sciences Summer (CASSUM) research programme. R.F. acknowledges funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 101032092. A.C.G. has been supported by PRIN-INAF-MAIN-STREAM 2017 “Protoplanetary disks seen through the eyes of new-generation instruments” and by PRIN-INAF 2019 “Spectroscopically tracing the disk dispersal evolution (STRADE)”. J.C.T. acknowledges support from ERC grant MSTAR, VR grant 2017-04522, and NSF grant 1910675. G.C and P.G acknowledge support from Chalmers Initiative Cosmic Origins (CICO) postdoctoral fellowships. This research made use of Photutils, version 1.0.0, an Astropy (Astropy Collaboration et al. 2013, 2018) package for detection and photometry of astronomical sources (Bradley et al. 2020).
Feedback-Controlled Sequential Lasso Screening<|sep|>We have shown analytically and empirically that feedback controlled sequential screening yields improved performance. Given a cross-validated target λt, the DASS rule tailors an individualized sequence {λk}N k=1 on the ﬂy which is optimized for each particular problem instance (D, y, λt). This has demonstrated signiﬁcantly greater robustness with respect to the inherent variability in the lasso problem. yields a feedback controlled version of sequential DPP [29]. This uses a looser bound than R0 k (see Fig. 1). Our results also suggest that it may be possible to use feedback to control sequential screening in more targeted ways. For example, by varying the parameter R in response to previously solved instances it may be possible to regulate the size of the dictionary after screening. This would be useful in situations of a large number of features and limited memory.
Back to Futures<|sep|>We have presented a concurrent shared-memory semantics based on a semiaxiomatic [10] presentation of adjoint logic [30,23,24,28], for which we have usual variants of progress and preservation, as well as conﬂuence. We then demonstrate that by adding a limited form of atomic writes, we can model sequential computation. Taking advantage of this, we reconstruct several patterns that provide limited access to concurrency in a sequential language, such as fork/join, futures, and monadic concurrency in the style of SILL. The uniform nature of these reconstructions means that they are all mutually compatible, and so we can freely work with any set of these concurrency primitives within the same language. There are several potential directions that future work in this space could take. In our reconstruction of futures, we incidentally also provide a deﬁnition of linear futures, which have been used in designing pipelines [2], but to our knowledge have not been examined formally or implemented. One item of future work, then, would be to further explore linear futures, now aided by a formal deﬁnition which is also amenable to implementation. We also believe that it would be interesting to explore an implementation of our language as a whole, and to investigate what other concurrency patterns arise naturally when working in it. Additionally, the stratiﬁcation of the language into layers connected with adjoint operators strongly suggests that some properties of a language instance as a whole can be obtained modularly from properties of the sublanguages at each mode. Although based on diﬀerent primitives, research on monads and comonads to capture eﬀects and coeﬀects, respectively, [8,11] also points in this direction. In particular, we would like to explore a modular theory of (observational) equivalence using this approach. Some work on observational equivalence in a substructural setting already exists, [21] but works in a message-passing setting and does not seem to translate directly to the shared-memory setting of Seax.
Ensemble Learning Applied to Classify GPS Trajectories of Birds into Male or Female<|sep|>We introduced an ensemble learning approach to predict gender of shearwater based on trajectory and associated metadata. The ensemble learning approach was able to model trajectories to output the predictions. One potential limitation of our approach is not considering cluster based information, which might be valuable to increase accuracy. As a future direction, we want to make cluster of grids from the trajectories and analyze the problem. One more interesting feature to see how 3D (latitude, longitude, and elevation) clustering might help increase accuracy. In aligned with this, it would be good to have date of the birds’s trajectories, so that we might beneﬁt from seasonal pattern of water level (elevation).
Simplifying Multiloop Integrands and Ultraviolet Divergences of Gauge Theory and Gravity Amplitudes<|sep|>In this paper, we recomputed the four-loop four-point amplitudes of N = 4 sYM theory and N = 8 supergravity, ﬁrst obtained in refs. [10, 11]. By exploiting the conjectured duality between color and kinematics [29, 30] we found greatly simpliﬁed representations. It also allowed us to ﬁnd the form of the complete amplitude, including nonplanar contributions, using only planar cut information as input. We conﬁrmed the correctness of the construction by comparing the unitarity cuts of the new expressions to the cuts of the earlier forms [10, 11]. This provides new nontrivial evidence in favor of the duality conjecture and the associated gravity double-copy property. An important advantage of the current construction is that once the sYM amplitude has been arranged into a duality-satisfying form, the construction of the corresponding supergravity integrand is trivial: one simply replaces the color factors with kinematic numerator factors in each graph. The new form of the four-loop four-point amplitude of N = 8 has an important advantage over the previous one [10], because no integral displays a worse UV power counting than the complete amplitude. This feature greatly simpliﬁes the extraction of the UV divergence of the four-loop N = 8 supergravity amplitude in the critical dimension Dc = 11/2, corresponding to the lowest dimension where both N = 4 sYM theory and N = 8 supergravity ﬁrst diverge at four loops. To carry out the required integration we used techniques similar to those described in refs. [9–11]. Our results prove that the four-loop four-point amplitude of N = 8 supergravity does indeed diverge in the same critical dimension as the corresponding amplitude of N = 4 super-Yang-Mills theory. Thus, the N = 8 supergravity ﬁniteness bound [10] is, in fact, saturated at four loops. The amplitude divergence in Dc = 11/2 means that the D8R4 supergravity counterterm has a nonzero divergent coeﬃcient, in much the same way as the Tr(D2F 4) counterterm of N = 4 sYM has a nonvanishing divergent coeﬃcient in this dimension. Moreover, we found that the four-loop ﬁniteness bound [11, 64], D < 6, for the double-color-trace terms of N = 4 sYM theory is also saturated. In other words, the corresponding D = 6 double-trace counterterms Tr(D4−kF 2) Tr(DkF 2) are also present with non-vanishing coeﬃcients. More generally, the duality between color and kinematics oﬀers the promise of carrying advances from the planar sector of gauge theory to the nonplanar sector and then to gravity theories. Its underlying origin is, however, still poorly understood; recent progress suggests that, at least in the self-dual case [46], underlying it is an inﬁnite-dimensional Lie algebra of area-preserving diﬀeomorphisms. Progress has also been made in ﬁnding explicit representations of tree amplitudes that manifestly satisfy the duality [47]. It would be interesting and very useful to devise eﬀective rules that would generate directly duality-satisfying representations for loop amplitudes, thus eliminating the need to solve the system of duality constraints on a case-by-case basis. A step towards ﬁnding a Lagrangian with the desired properties has been given in ref. [45]. It would also be interesting to explore whether the color-kinematic duality extends beyond weak-coupling perturbation theory as well as whether the existence of such a duality has practical consequences after carrying out the loop-momentum integrations. Explicit calculations often lead to surprises. The results described here are no diﬀerent. In particular, in the critical dimension D = 11/2 we found that, after reducing the integrals containing UV divergences to a basis of vacuum integrals encoding the numerical factors in front of the divergent operator, the UV divergence is given by exactly the same combination of basis integrals as found in the single-trace 1/N2 c -suppressed terms of N = 4 sYM theory. It seems unlikely that this is accidental because similar behavior is found at lower loops. It would obviously be important to understand the origin of this curious connection and implications it may have at higher loops on UV divergences. Another interesting property is the existence of strong cancellations between the contributions of various graphs to the UV divergence in the critical dimension. This suggests that diﬀerent integral contributions may be related to each other by a hidden symmetry. In summary, the duality between color and kinematics oﬀers a powerful means for streamlining the construction of multiloop amplitudes, carrying advances in the planar sector to the nonplanar sector. It allowed us to express the numerators of the four-loop four-point amplitudes of N = 4 sYM theory and N = 8 supergravity in terms of the numerators of two planar graphs. Using this simpliﬁed form, in the critical dimension Dc = 11/2, we found a surprising coincidence between the UV divergences of N = 8 supergravity and those of subleading color single-trace terms of N = 4 sYM theory. This hints at further new relations between gauge and gravity theories to be unraveled and that further surprises await us at ﬁve and higher loops. We look forward to using the tools described in this paper to further explore the multiloop structure of gauge and gravity amplitudes and to unravel their UV properties. We thank Nima Arkani-Hamed, Scott Davies, Tristan Dennen, Michael Green, Yu-tin Huang, Harald Ita, David Kosower, Kelly Stelle and Pierre Vanhove for many stimulating discussions. We thank Academic Technology Services at UCLA for computer support. This research was supported by the US Department of Energy under contracts DE–AC02– 76SF00515, DE–FG03–91ER40662 and DE–FG02–90ER40577 (OJI), and by the US National Science Foundation under grants PHY–0756174, PHY–0855356 and PHY05–51164. R. R. acknowledges support from the A. P. Sloan Foundation. H. J.’s research is supported by the European Research Council under Advanced Investigator Grant ERC-AdG-228301. In this appendix we list a set of numerator equations that determine the four-loop fourpoint N = 4 sYM amplitude (up to snail contributions), starting from the two planar master graphs 18 and 28 in ﬁg. 4. These equations follow directly from the dual Jacobi relations. However, to make the equations more convenient for generating numerator factors from the our two planar master numerators, we performed various simpliﬁcations which follow from the N = 4 sYM auxiliary constraints described in section II B. In particular, we use the two-term relations (see eq. (A5)) which rely on the no one-loop triangle subgraph constraint to eliminate numerators appearing in other dual Jacobi relations. We also simpliﬁed the functional arguments of the numerators using the auxiliary constraint that numerators are independent of the loop momenta of one-loop box subgraphs. For example, instead of the dual relation, N50(k1, k2, k3, l5, l6, l7, l8) = N28(k2, k1, k4, l5, k3 − l7, k2 − l6, l8) − N28(k1, k2, k3, l6, k4 − l8, k1 − l5, l7) , (A1) N50(k1, k2, k3, l5, l6, l7, l8) = N28(k2, k1, k4, l5, k3 − l7, l7, l8) − N28(k1, k2, k3, l6, k4 − l8, l7, l8) , (A2) using the fact that N28 is, in fact, independent of the values of the last two arguments since these momenta are those of one-loop box subgraphs (see ﬁg. 4). In this sense, the last two arguments of N28 are eﬀectively placeholders, and can be assigned any value without altering the numerators. These simpliﬁcations, however, imply that the equations given below are speciﬁc to N = 4 sYM theory and will not hold for corresponding numerators of amplitudes of theories with fewer supersymmetries. They are also not in direct correspondence with the color-Jacobi equations, because numerators of graphs with triangle subgraphs are set to zero, although corresponding color factors are nonvanishing. On the left-hand side of each duality equation, for simplicity, we will suppress the canonical arguments, which are the three external momenta and the four independent loop momenta following the graph labels in ﬁgs. 5–11, i.e. and we take k4 ≡ −k1 − k2 − k3 throughout. We have ordered the equations so that the substitutions that are required to express the given numerators in terms of the two master numerators always come from previous equations in the list. With the above notation, the N58 = N18(k1, k2, k3, k2 − l6, l5, l7, l8) − N18(k2, k1, k3, k1 − l6, l5, l7, l8) , N33 = N28(k4, k3, k2, k3 − l5, k2 − l6 + l7, l7, l8) − N18(k1, k2, k3, k2 − l6, k3 − l5, l7, l8) , N50 = N28(k2, k1, k4, l5, k3 − l7, l7, l8) − N28(k1, k2, k3, l6, k4 − l8, l7, l8) , N6 = −N33(k1, k2, k4, l7, l5 − l6, k1 − l6, l8) − N33(k2, k1, k4, l7, l6, k2 − l5 + l6, l8) , N14 = −N33(k3, k2, k1, l5, −l5 − l7, k3 − l7 + l8, l6) − N33(k3, k2, k1, l5, k2 + l7, l7 − l8, l8) , N24 = −N28(k1, k2, k3, l5 − l7, −l6, l7, l8) − N33(k1, k2, k4, −l6, −l7, −l5, l8) , N32 = −N28(k4, k2, k1, l7, k3 − l5, l7, l8) − N33(k2, k1, k3, l5, l6, k2 + l5 + l6 − l7, l8) , N48 = N28(k3, k4, k1, l8, k2 − l5, l7, l8) − N33(k1, k2, k3, k3 − l6, k2 − l5, l7, l8) , N49 = −N33(k1, k2, k3, k3 − l8, k2 − l5, −l7, l8) − N33(k4, k1, k2, l5, −l7, l6, l8) , N66 = N58(k1, k2, k4, l5 − k3 − l6, l6, l7, l8) − N58(k1, k2, k3, k3 + l6, l6, l7, l8) , N1 = −N6(k1, k2, k3, l6, l5, l7, l8) − N6(k1, k2, k4, l6, l5, l7, l8) , N68 = N14(k1, k2, k3, k1 − l5, −l6, −l7, −l8) − N14(k1, k2, k4, l5 − k2, −l7, −l6, l8) , N21 = −N14(k2, k1, k3, l5, l6, l7, l8) − N18(k2, k1, k3, −l5, k1 + k3 + l5 − l6, l7, l8) , N26 = N24(k2, k1, k3, −l5, −k4 − l6 − l7, l8, l6) − N24(k2, k1, k4, −l5, l7 − k3, l6 − k1 − l5 − l8, l6) , N27 = −N18(k2, k1, k4, −l5, l7, l7, l8) − N24(k1, k2, k4, l5, −k3 − l7 − l8, k3 − l6 + l7 + l8, l8) , N37 = −N28(k2, k1, k3, k1 − l5, k4 + l8, l7, l6) − N49(k2, k1, k3, k1 − l5, −l8, l7 − k2, l6) , N39 = N28(k2, k1, k3, −l5 − l7, k4 + l6 + l8, l5, l6) − N48(k1, k2, k3, l7, l8, −l5 − l7, −l6 − l8) , N45 = N49(k1, k2, k3, l5 − l6 − l7 − l8, k4 − l6, l5, l7) + N49(k1, k2, k4, k2 + l6 + l7 + l8, l7, l5, k4 − l6) , N38 = N49(k2, k1, k4, l6, k3 + l5 + l7, −l5 + l6, k4 − l8) − N49(k1, k2, k4, l5 − l6, k3 + l5 + l7, −l6, l7 + l8) , N53 = N58(k1, k2, k3, k3 − l8, l6, l7, l8) + N66(k1, k2, k4, l8, −k4 − l5, l7, l8) , N12 = N18(k4, k3, k2, l6, k2 + l8, l5, l7) + N26(k3, k4, k1, −l6, l8, −l5, l8) , N51 = N18(k3, k2, k1, k1 + k2 − l5, −l6, l7, l8) − N21(k2, k3, k1, l5 − k1 − k2, −l6, l7, l8) , N63 = N21(k1, k2, k3, k2 − l5, k1 + k2 − l5 − l6, l7, l8) − N21(k2, k1, k3, k1 − l5, k1 + k2 − l5 − l6, l7, l8) , N79 = N45(k1, k2, k3, k2 − l5, k4 − l7, l6, −l6 − l8) − N45(k1, k2, k3, l5 − k1, l7, k3 − l6, k4 + l5 − l7 − l8) , N80 = N53(k1, k2, k3, k3 − l7, l6, l7, l8) + N53(k1, k2, k3, l7 − k4, l5, l6, l8) , N55 = N51(k1, k2, k3, k1 + l5, l6, l7, l8) − N51(k1, k3, k2, k1 + l5, l6, l7, l8) , N83 = −N55(k3, k1, k2, k1 + k2 − l5, l8, l6, l7) − N55(k3, k1, k2, l5 − k3, l6, l7, l8) . (A4) N5 = N4 = N3 = N2 = N1 , N11 = N10 = N9 = N8 = N7 = N6 , N40 = N13 = −N12 , N41 = −N17 = −N16 = −N15 = N14 , N42 = N20 = −N19 = N18 , N43 = −N23 = −N22 = −N21 , N25 = N24 , N44 = −N26 , N31 = −N30 = N29 = N28 , N46 = N34 = N32 , N36 = −N35 = −N33 , N47 = N38 , N72 = N52 = N51 , N74 = −N54 = −N53 , N73 = N57 = N56 = N55 , N76 = −N62 = −N61 = −N60 = −N59 = −N58 , N77 = −N65 = N64 = N63 , N78 = −N67 = N66 , N75 = N71 = N70 = N69 = N68 , N82 = N81 = N80 , N85 = N84 = N83 . (A5) Plain-text, computer-readable versions of both the original duality relations (using only the no one-loop triangle subgraph property) and the simpliﬁed ones presented above may be found online [38]. Many other functional equations can be obtained from the dual Jacobi relations, which we will not list here. Although important, as they provide additional independent constraints to the full system, they are not needed to specify the solution once the system and master graph numerators have been solved. However, we have conﬁrmed that the numerators presented in appendix B automatically satisfy all these remaining dual Jacobi relations. In this appendix we give the explicit values Ni of the distinct graph numerators in the N = 4 sYM amplitude. (The remaining ones are given directly in terms of these via eq. (A5).) These values are obtained by taking the numerators of the master graphs (3.14) and substituting their values into eq. (A4). We have performed some algebraic simpliﬁcations to obtain the results collected here. The N = 8 supergravity numerators are squares of the Ni, as in eqs. (3.2) and (4.16). 2s(s(τ16 − τ26 − τ35 + τ45 + 2τ56 + 2t) − 2(4τ16τ25 + 4τ15τ26 + τ45(τ36 − 3τ46) + τ35(τ46 − 3τ36))) , 4(s(9τ 2 15 + 9τ 2 25 + 4t2 + 8tτ35 + 2τ 2 35 + 2τ 2 45) + 8τ25(u2 − s2) 4(6u2τ25 + u(2s(5τ25 + 2τ26) − τ15(7τ16 + 6t)) + t(τ15τ26 − τ25(τ16 + 7τ26)) + s(4τ15(t − τ26) + 6τ36(τ35 − τ45) 4(t(12τ 2 15 − 7τ15τ16 + τ25(τ16 − 10τ35)) + u(τ25(12τ25 − 8t − 7τ26) + τ15(8t + τ26 − 10τ35)) + 4l2 5(u2 − st) − s(2τ15(6τ16 + 5τ25) + 4u(τ16 − t + τ26 + 2τ45) + τ35(τ36 − 12τ35 − 10τ46) − 2s2(2τ15 − 2τ25 + τ26 − τ36 − τ37 + τ47 − τ56) + u(11τ16τ25 + τ15(7τ16 + τ26)) + tτ25(12τ16 + 7τ26)) , 4(uτ15(7τ16 + 12τ26) + t(11τ15τ26 + τ25(τ16 + 7τ26)) + s(16τ15τ17 − 4u(2τ15 + τ16 − 2τ25 + τ26) + τ25(16τ27 − 11τ16) + τ35(6τ36 − 4τ37 + τ46 − 20τ47) + τ45(2τ36 − 20τ37 + 13τ46 − 4τ47)) + 2s2(τ17 − 2τ15 + 2τ25 − τ26 − τ27 + τ36 + τ56 + 2τ57)) , 4(uτ18(τ25 − 7τ15) + τ28(t(τ15 − 11τ25) − 4uτ25) + s(4τ15(3τ17 + τ18 − τ27) + 4u(2τ15 − 2τ25 + τ28) + τ45(5τ18 + 5τ28 − 16τ37 − 5τ38) + τ35(2τ38 − 16τ47 − 9τ48) − 4τ18t − 4τ25(τ17 − 3τ27)) + 2s2(2τ15 + 2τ17 − 2τ25 + τ28 − τ36 + 2τ37 + τ38 + τ46 + 2τ57 + τ58)) , 4(t(τ25(τ16 − 11τ25 − 12τ26) + τ25τ35 − 6τ 2 35 − τ15(τ26 − 4τ45)) − u(5τ25τ26 + τ15(7τ16 − 5τ25 + 5τ35)) + s(τ15(5τ16 − 4t) + τ16(8t + τ25) + τ35τ36 + 5τ45(τ25 − τ35 − 2τ36) − τ46(11τ35 + 6τ45) + 2u(6τ25 + 4τ26 − 4τ35 − 2l2 5)) + 2s2(3τ25 − τ35 − τ36 − 3τ46 + τ56)) , 4(s2(4τ17 − 2(4τ26 + τ35 + 2τ36)) − 6u2τ35 + u((4τ16 + 5τ26)τ45 − τ17(11τ25 + 7τ35 + 6τ45) + τ35(11τ37 − 5τ46)) − t(5τ17τ25 + 6τ15τ26 + (6τ26 − 5τ17 − 4τ27)τ45 + τ35(7τ26 − 11τ36 + 5τ47)) + s(τ15(5τ26 + 4τ46) − 5τ35(τ16 + τ27) + 4τ25τ47 + 2u(5τ17 + 2τ25 − 5(τ26 + τ35) + τ56) + 2t(τ16 − τ15 − τ27 + τ57))) , 4(u2(4τ15 − 2τ27) − 2s2(2τ15 + 3τ27 + 4τ36) + t(6τ26τ27 + 5τ27τ35 − 6τ27τ36 + 6τ36τ37 − τ25(5τ36 + 4τ46) − τ15(6τ26 + 5τ46) + 4τ35τ47) + u(5τ27τ46 − 6τ35τ36 + (5τ27 + 4τ37)τ45 + τ17(5τ36 + 4τ46 − 2t) + τ15(12τ36 + 5τ37 + 6τ46 − 5τ47) + 2t(τ57 − τ25)) + s(6τ25τ27 − (4τ26 + 5τ36)τ45 − 5(τ26 + τ46)τ47 − τ16(4t − 5τ27 + τ47) − τ15(11τ26 + 12τ27 + 6τ37 + 11τ47) + 2t(τ35 − τ56) + 2u(t + 2τ26 − 5τ27 − 8τ36 + 2τ37 + τ67))) , 4(t(τ16τ25 + 6τ 2 25 − 11τ25τ26 − τ25τ35 + 11τ 2 35 + 7τ35τ37 + τ37τ45 + τ15(τ45 − 4τ16) − τ35τ47) − s(5τ 2 15 − 6τ17τ25 + 20τ17τ26 + 6τ25τ27 + 4τ26τ27 + 4τ16(τ17 + 5τ27) − 9τ25τ36 − 11τ35τ36 − 16τ36τ37 − 5τ37τ45 + 10τ35τ46 + 5τ45τ46 − 4(τ35 + 4τ46)τ47 + τ15(τ37 + τ47 − 4τ27 + 10τ35 − 9τ36)) + u(τ45(7τ47 − 5(2τ35 + τ45)) − 4τ25τ26 + τ15(τ26 − 11τ16)) + 2s(4tτ37 + u(4(τ16 + τ25 − τ26 − τ35 + τ47) + 2l2 5) + s(2τ16 − 2τ17 + 2τ25 − 2τ26 − 2τ27 − 2τ35 + τ56 − τ57 + 2τ67))) , 4(t(τ15(12τ16 + 6τ26 + τ36) − 10τ38τ45 + τ27(5τ26 + 12(τ36 + τ46)) + τ35(τ48 − 7τ38)) − u(τ17(7τ16 + τ26) + 11τ16τ27 − τ25(4τ36 + 5τ46) + τ45(9τ38 + 6τ46 + 7τ48)) + s(4τ17τ18 + 11τ17τ26 + 20τ18τ27 + 20τ17τ28 + 4τ27τ28 + τ15(5τ16 − 11τ18 + 10τ26 + 5τ28) − 4τ18τ35 + 4τ26τ35 − 6τ35τ38 − 16τ37τ38 + 2t(τ25 + τ36 + 4τ38) + 4τ28τ45 − 5τ25τ46 − τ37τ46 − 11τ45τ46 − 13τ46τ47 − 2τ36(3τ37 + τ47) + 2(3τ35 − τ45 − 8τ47)τ48 + 2u(3τ15 + 4τ17 − 4τ27 + 3τ46 + 4τ48 + τ56)) − 2s2(u − τ15 − 2τ17 + 2τ18 + τ26 + 2τ27 + 2τ28 + τ35 − τ46 + τ58 + τ67 + 2τ78)) , 4(8u2τ15 + t(τ15(τ16 + 7τ17 + τ26 + τ27 − 5τ35 + 5τ45) − (τ16 + τ17 + 8t)τ25) + s(τ15(12τ17 − 18τ25 + 11τ26 + τ27) + 2(5τ16τ25 − 2τ17t − 2t(6τ25 + τ26) + 3τ25(τ26 + 2τ27)) + 11τ 2 35 + τ45(6(τ45 − 2τ46 − τ47) − τ36 − 11τ37) + τ35(τ37 − 5τ36 + 13τ45 − 10τ47)) + u(4s(τ16 − t + τ27 + 2τ35) − 6τ15τ16 + τ25(7τ26 + 7τ27 − 5τ35 + 5τ45) + 4tl2 5) + 2s2(τ16 + τ27 + 5τ35 + τ36 + τ37 + τ38 + 3τ45 − τ48 − τ56 + τ57)) , 4(s(τ18(5τ25 + 4τ35) + τ36(τ15 − 5τ27 − 4τ45) − (τ17 − 5τ25)τ46 + 5τ35τ48) − 2sut − 5τ15τ18t − t(5τ17τ26 + 7τ35τ36 + τ25(11τ26 − 5τ38) + τ15(5τ28 + τ38) + 4τ37τ46 + τ36τ47 − 4τ16(τ45 + τ47)) + u(4τ15τ26 + 6τ25τ28 + τ16(7τ17 + τ25 − 4τ35) + τ36(11τ37 + τ45) − 4τ26(τ17 + τ47) + 6(τ45 − τ25)τ48) + 2(s2(τ25 − τ35 + 2τ36) + 3ut(τ17 + τ48) + s(u(3τ25 + τ26 + 4τ36 − τ56) + t(τ15 − τ16 − 2τ17 − τ18 − τ47 − 2τ48 + τ58 + τ67)))) , 4(2s2t − t(11τ35τ38 + 2τ15(2τ36 + 5τ46) + τ25(11τ28 − 4τ37 + 4τ38 + τ47) + 4τ27τ48 + 5τ17(τ35 + τ48)) + s(τ15(11τ17 + 5τ38) − 5τ17τ28 − 2τ16(5τ25 + 2τ35) + τ45(5τ46 − 4(τ18 + τ37)) + 5τ38(τ27 − τ47) − 4τ28τ47 + τ25(7τ27 + 5τ48 − 5τ26)) + u(τ27(11τ38 + 4τ45) − τ25(5τ16 − 5τ18 + 11τ26 + τ37) − τ17(6τ18 − 12τ38 + τ45) + (7τ25 + 6τ35)τ46 + τ38(5τ45 + 6τ47) + τ15(τ46 − 4(τ47 + τ48))) + 2(s2(τ25 − 4τ17 − τ35 + 2τ38) + u(4uτ17 + t(2τ27 − 2τ15 − 2τ25 − 3τ46 + τ57)) + s(τ16t + u(τ28 + 4τ38 − τ58) − t(τ18 + 2τ37 − 2τ46 + τ56 + τ78)))) , 4(t(τ25(5τ37 + 4τ47) − τ38(4τ16 + 5τ26) − 5τ15τ27) + s(τ45(4τ27 + 5τ37) − τ18(5τ26 + 4τ36) − 5τ15τ47 + 5(τ16 − τ36)τ48) + u(6τ35τ37 − 6τ26τ28 + τ15(11τ17 − τ37) + (11τ16 + 6(2τ26 + τ36))τ48) + 2t(3u(τ15 − τ26 + τ37 − τ48) + s(τ18 − 2τ15 + 2τ26 − τ27 + τ36 − 2τ37 − τ45 + 2τ48 + τ57 − τ68))) , + 5τ15(sτ25 + uτ35) + 14s2τ45 − s(6uτ15 + t(τ15 + 6τ25 + 13τ35 − 2l2 5)) − 2u2l2 5) , N53 = 8s(tτ35 + uτ45 − sτ25) , 2t(t(τ25 − 8τ15 + 5τ45) + u(9τ45 − 17τ15)) , N58 = s(2u(τ45 − 3τ35) − s(u − t + 4τ25 + 5τ35 + τ45)) , 2s(5tτ35 + u(12τ36 + 5τ45 − 4τ46) + s(2t − 2u + 2τ25 + 2τ46 + 8τ26 + 10τ36 − 7τ15)) , N66 = s(4t(τ35 − 2τ36) + 2u(τ35 + 3τ45 − 4τ46) − s(6u + τ15 − 6t + 5τ25 − 8τ26)) , 2s(5tτ35 + u(5τ45 + 4τ46 − 4τ37 + 12τ47 − 12τ36) + s(2(τ16 + τ25 − τ27 + 3τ17 − 3τ26 + 4τ47 − 4τ36) − 7τ15)) , The other numerators are given directly in terms of these in eq. (A5). For N83 it should be understood that k2 4 → 0 only after canceling the 1/k2 4 propagator. Alternatively, we can rewrite the snail contributions in terms of the numerators of the graphs in ﬁg. 12, we have Plain-text, computer-readable versions of these expressions may be found online [38]. It is interesting to note that N33 can be used as a non-planar master graph numerator, as discussed in section III B. This implies that the single numerator N33 contains the same amplitude-speciﬁc information as the two planar master numerators N18 and N28 combined. In the ﬁrst column of table I we give the numerators of the vacuum integrals in ﬁgs. 18–20, as they appear in the expression for �V(4) deﬁned in eqs. (4.20) and (4.21). Each integral can be reduced to a linear combination of the three master integrals V1, V2 and V8. The second, third and fourth columns of the table provide the coeﬃcients of V1, V2 and V8, respectively, after this reduction. To obtain the coeﬃcient of each vacuum integral Vi in the ﬁnal formula for the four-loop UV divergence in eq. (4.37), we simply sum the numbers in each column labeled by a Vi, to obtain �V(4) = 23 TABLE I: Eﬀective numerators for the vacuum integrals Iv i entering the UV pole of the fourloop N = 8 supergravity amplitude, and the coeﬃcients arising from writing them as a linear combination of basis vacuum integrals V1, V2, and V8.
The High-$z$ Universe Confronts Warm Dark Matter: Galaxy Counts, Reionization and the Nature of Dark Matter<|sep|>We have shown that the Lyman-break technique for galaxy surveys at high redshift can provide a direct method for constraining the nature of dark matter and its clustering at small scales, with sensitivity to the structure formation suppression present in WDM models. We have analyzed CDM and WDM cosmological simulations in order to test WDM models using the luminosity function observations at high-z as well as a new analysis of cosmological reionization limits. Given the assumptions that the luminosity function of a ΛCDM universe is modeled by a Schechter function down to faint magnitudes and that the mass-luminosity relation of galaxies is independent of the dark matter model employed, we have modeled the luminosity function for several dark matter models to analyze the sensitivities to WDM dark matter models. Using an approximate χ2 test of the faint end of the luminosity function, direct number counts of galaxies signiﬁcantly disfavors a 0.8 keV WDM model at greater than 10σ, and a 1.3 keV model is disfavored at approximately 98.6% C.L. (2.2σ). Further, with highly optimistic values for the parameters that translate high redshift galaxy luminosity to ionizing ﬂux, the 0.8 keV and 1.3 keV model are inconsistent with the CMB optical depth at greater than 68% C.L. Furthermore, for the conservative case of a limiting luminosity of MAB = −13, a 2.6 keV WDM model is only marginally consistent with the 68% conﬁdence region of the optical depth from Planck. Wherever possible, we have used conservative values on parameters, making WDM behave more like cold dark matter. For this reason we feel conﬁdent concluding that neither the 0.8 keV or 1.3 keV models are consistent at more than 68% C.L. with reionization, even with the large uncertainty on the reionization process. We expect upcoming deep surveys with JWST (and possibly HST via the Frontier Fields) to be able to reach luminosities and redshifts that can fully discern between a CDM model and a 1.3 keV model by direct number counts. Even 2.6 keV WDM might prove discernible if the observations are deep enough. Additionally, if the constraints on reionization parameters are improved, a 2.6 keV WDM model can be distinguished from cold dark matter by its different reionization history. The study of galaxy formation and reionization in the high-z universe adds a complementary and competitive probe to the nature of dark matter. We thank useful discussions with John Beacom and Richard Ellis. We would also like to thank the referee Massimo Ricotti for helpful feedback on the paper. KNA is partially supported by NSF CAREER Grant No. PHY-11-59224. JO and JSB were supported by grants from the NSF and NASA. Herpich et al. (2013) found that the star formation in low-z MilkyWay-like galaxies is slightly suppressed in WDM cosmologies. This seems to ﬁt well with what one would expect: the small-scale cut-off in the WDM transfer function postpones the formation of dwarf galaxy halos, and therefore the potential wells that act as seeds for the ﬁrst galaxies are shallower. The effect is relatively small, only a factor of 2 for their most extreme WDM model at z = 0. The star formation efﬁciency for different dark matter models can readily be inferred from abundance matching. Figure A1 shows the halo mass-luminosity relation by using the different dark matter halo catalogues. For a ﬁxed mass, a WDM halo is seen to be more luminous than a CDM halo, and thus WDM would, unsurprisingly, need to have an enhanced star formation efﬁciency Figure A1. Abundance matching utilising the different halo catalogues. Abundance matching with the CDM catalogue gives a power law down to faint magnitudes. The dashed lines are power law extrapolations to the faint end. Clearly, the faint end in the WDM models diverge from the CDM power law towards more efﬁcient star formation: lower mass halos have a larger luminosity relative to CDM. However, the WDM models must have roughly the same or a sligtly lower star formation rate than CDM, hence our assumption of the CDM power law behaviour is a very conservative estimate. The circle indicate the current HUDF magnitude limit, the asterisk is the expected JWST limits. relative to CDM in order to match observations. This is counterintuitive, and more importantly contradicts the low-z results of Herpich et al. (2013). A realistic WDM halo mass-luminosity mapping would give a slightly lower star formation efﬁciency: that is, a ﬂatter slope than CDM in ﬁgure A1 instead of a steeper slope. In our analysis we therefore conservatively assume that the halo massluminosity is a power law. Therefore, a halo mass can uniquely be mapped to the same luminosity independent of WDM model.
Scaling Properties of Ge-SixGe1-x Core-Shell Nanowire Field Effect Transistors<|sep|>We demonstrate high-performance Ge-SixGe1-x core-shell  NW FETs with highly doped S/D and systematically  investigated their scaling properties.  Our data allow us to  extract key device parameters, such as intrinsic channel  resistance, carrier mobility, effective channel length, and  external contact resistance, as well as to benchmark the device  switching speed and ON/OFF current ratio.
Synergies between Vera C. Rubin Observatory, Nancy Grace Roman Space Telescope, and Euclid Mission: Constraining Dark Energy with Type Ia Supernovae<|sep|>Type Ia supernovae (SNe Ia) are a key cosmological probe, and the supernovae that the three great observatories - Euclid, Nancy Grace Roman Space Telescope and the Vera C. Rubin Observatory - will discover promise to revolutionize our understanding of the nature of dark energy. These transients are already key science drivers for two of the three large surveys of the 2020s: LSST and Roman, which have large and active communities involved in all aspects of these missions. Combining data sets allows increases in redshift range and wavelength range, boosting the statistical precision and the systematic control. We discussed improvements to both statistical and systematic precision that combinations of data sets will enable such as improved photometric calibration, SN Ia standardization, and redshift measurements. Other science cases will also beneﬁt from these synergies. In particular, the photometric redshift improvements will beneﬁt cosmological measurements from weak lensing. Also the discovery and conﬁrmation of strong-lensing systems, galaxy clusters, and gravitational wave counterparts. While the community will beneﬁt from combining data from these observatories, orchestrating them to act in concert with one another will yield the greatest scientiﬁc beneﬁts. Particularly due to the time-dependent nature of SN surveys, planning must be done both prior to and during the surveys. We have discussed what teams and plans should be put in place now to start preparing for these combined sets by setting up cross-agency task forces. These task forces will allow for the needed communication to improve survey operations, calibration, spectroscopic follow-up, joint pixel-processing, and analysis. By working together,
Investigating light neutralinos at neutrino telescopes<|sep|>In this section we give our results for the muon ﬂuxes expected at a neutrino telescope with a threshold muon energy of 1.6 GeV, generated by annihilation of light neutralino pair-annihilation in the Earth and in the Sun.
Multi-Region Neural Representation: A novel model for decoding visual stimuli in human brains<|sep|>As a conjunction between neuroscience and computer science, Multivariate Pattern (MVP) is mostly used for analyzing task-based fMRI data set. There is a wide range of challenges in the MVP techniques, i.e. decreasing noise and sparsity, deﬁning eﬀective regions of interest (ROIs), visualizing results, and the cost of brain studies. In overcoming these challenges, this paper proposes Multi-Region Neural Representation as a novel feature space for decoding visual stimuli in the human brain. The proposed method is applied in three stages: ﬁrstly, snapshots of brain image (each snapshot represents neural activities for a unique stimulus) are selected by ﬁnding local maximums in the smoothed version of the design matrix. Then, features are generated in three steps, including normalizing to standard space, segmenting the snapshots in the form of automatically detected anatomical regions, and removing noise by Gaussian smoothing in the level of ROIs. Experimental studies on 4 visual categories (words, objects, consonants and nonsense photos) clearly show the superiority of our proposed method in comparison with state-of-the-art methods. In addition, the time complexity of the proposed method is naturally lower than the previous methods because it employs a snapshot of brain image for each stimulus rather than using the whole of time series. In future, we plan to apply the proposed method to diﬀerent brain tasks such as risk, emotion and etc. We thank the anonymous reviewers for comments. This work was supported in part by the National Natural Science Foundation of China (61422204 and 61473149), Jiangsu Natural Science Foundation (BK20130034) and NUAA Fundamental Research Funds (NE2013105).
Power-balancing dual-port grid-forming power converter control for renewable integration and hybrid AC/DC power systems<|sep|>In this paper, we proposed a novel grid-forming (GFM) control paradigm for dc/ac voltage source converters that simultaneously imposes the converter ac voltage and controls the dc voltage (dual-port GFM). Conceptually, dual-port GFM control uniﬁes standard functions of GFM and GFL converters used to interface, e.g., renewable generation and HVDC transmission. We developed a graph representation of power system combining ac and dc subgrids and reducedorder linear dynamical models of converters, machines, and power sources that, in abstraction, model a wide range of devices. For this complex class of power systems, we obtained stability conditions that only require partial knowledge of the systems topology. Finally, we used a high-ﬁdelity case study to illustrate the main features uncovered by our theoretical analysis. While these results are encouraging, there is a need for more detailed studies to understand how to leverage the results in applications such as wind turbine control. Moreover, converter current limiting is a crucial aspect that is well understood for ac-GFL control, but requires further study for dual-port GFM control. Proof of Proposition 1: Note that Wac, M, C, Kg, Tg and ˜Kθ are positive deﬁnite matrices. By construction IT g,acIg,ac + IT g,dcIg,dc is an identity matrix and, because [IT g,ac, IT g,dc]T has full column rank, IT g,acIg,ac + IT g,dc ˜KθIg,dc ≻ 0. Therefore, V is positive-deﬁnite. Moreover, the time derivative of Vη, Vω, Vv along the trajectories of (9) restricted to ¯P = 0| ¯ Ng| are Adding the derivatives of functions Vη, Vω, Vv and VP , and using ˜KθIT cdc = IT cdcKθ to cancel cross terms results in (10). Next, we show that d dtV is negative semi-deﬁnite. Because Mp ≻ 0, WacBT acIT cacMpIcacBacWac ⪰ 0 holds. Moreover, by deﬁnition D ⪰ 0, and G ⪰ 0, and ˜KθG = G ˜Kθ. Next, note that 1 and for all i ∈ N[1,Ndc], by deﬁnition, ˜Ki θ = diag{ki θ}. Hence, ˜Ki θLi dc+Li dc ˜Ki θ = 2ki θLi dc ⪰ 0. Moreover, IT g,acIg,ac + IT g,dc ˜KθIg,dc ≻ 0 is a diagonal matrix and we can conclude that d dtV ≤ 0 holds. To prove Proposition 2, we introduce vectors ηi, ωi, vi cac that correspond to the angle differences, frequencies, and the dc voltages of the devices connected to the ith ac subgrid. Analogously to Iac and Icac (cf. Sec. IV), we deﬁne matrices Ii ac ∈ {0, 1}|N i ac|×(|N i cac∪N i ac|) and Ii cac ∈ {0, 1}|N i cac|×(|N i cac∪N i ac|) that extract the machine and converter variables of the ith ac subgrid, i.e., Ii acθi is the vector of machine angles. Moreover, Ii acg, Ii acl and Ii aco extract variables of machines with power sources with kg,i > 0, losses di > 0, and the remaining machines. Similarly, Ii cacg, Ii cacl and Ii caco extract variables of converters with power sources with kg,i > 0, losses gi > 0, and the remaining converters. Next, the following Lemma is needed for the proof of Proposition 2. Lemma 1 Let Xi 0 = Ii cacLi acIi ac if |N i c | ≥ |N i ac| + µN and if |N i c |+µN < |N i ac|. If, for all i ∈ N[1,Nac] with N i ac ̸= ∅, there exists Ki ∈ N such that Algorithm 1 terminates with ¯Ci Ki = ∅, then Xi 0 has full column rank for all systems obtained by deleting up to µN nodes and µE edges from (9). Proof: For any ac subgrid i ∈ N[1,Nac], the Laplacian matrix Li ac of the graph Gi ac can be partitioned according to the node partition in Deﬁnition 1 to obtain At each iteration k, Algorithm 1 removes a node ξk ∈ ¯Ci k if it is connected to µmax + 1 single-edge nodes lk ∈ Di. Thus, at k = 0, the only non-zero element in the ﬁrst µmax +1 rows is the element (l0, ξρ), where ρ ∈ N[1,µmax+1]. Using elementary row and column operations, we obtain with j = 1. Moreover, the matrix XDi j,Ci j is obtained by removing the lth j−1 row and ξth j−1 column from XDi j−1,Ci j−1, the lth j−1 row from XDi j−1,Fi, and the ξth j−1 column from XFi j−1,Ci j−1. Moreover, if at k = 1, Algorithm 1 removes a node ξ1 ∈ ¯Ci 1, then the only non-zero element in the lth 1 row of (11) is the element (l1, ξ1) and we can again apply elementary row and column operations to obtain (11) with j = 2. Induction over j until j = K = |Ci| results in Finally, the matrix XFi,Fi is obtained by removing |Di|+|Ci| rows and columns from the Laplacian matrix Li ac of the graph Gi ac. Because the graph Gi ac is connected, deleting rows and columns of Li ac results in a loopy-Laplacian with at least one diagonal element that is larger than the sum of the absolute values of the other elements in its row. Thus, XFi,Fi is a loopy Laplacian with full rank (cf. [36]) and it follows that rank{Xi} = rank{Xi K} = |Ci| + |Fi|. Proof of Proposition 2: To characterize the largest invariant set contained in S, we ﬁrst separate the time derivative of V (see (10)) into terms corresponding to the dc and ac subgrids: Here, Bi ac denotes the incidence matrix of the ac subgrid with index i ∈ N[1,Nac]. Moreover, Li dc and vi correspond to the Laplacian matrix and dc voltages of the dc subgrid with index i ∈ N[1,Ndc]. Next, we distinguish the frequencies of the machines with losses ωi acl ∈ R|N i acl|, generation ωi acg ∈ R|N i acg| and the remaining frequencies ωi aco ∈ R|N i aco|. We use vi cac ∈ R|N i ac|, vj cdc ∈ R|N j cdc|, and vj dc ∈ R|N j dc| to denote the dc voltages of the converters in the ith ac subgrid, the converter dc voltages in the jth dc subgrid, and the dc node voltages in the jth dc subgrid. Like the machine frequencies, we separate these vectors and, e.g., use vi cacg ∈ R|N i cacg| to denote the dc voltages of the converters with generation. Moreover, d dtV (x) = 0 holds for x ∈ S := � ∪Nac i=1 Ai� ∪ � ∪Ndc j=1 Vj� ∪ P, with Next, we characterize the largest invariant set M ⊆ S to show that M = {0n}. To this end, we note that S can be rewritten as S = {x ∈ Rn|Sx = 0n} and use the fact that dk and combining (16), (14b), and (12), we have (ωi, Ki θvi cac) ∈ null(Ii cacLi ac) ∩ null(Ii acLi ac) = null(Li ac). Because Li ac is the Laplacian of a connected graph, it follows that all entries of (ωi, Ki θvi cac) are identical. On the other hand, combining (13), (14), (15), and (ηi, ωi, vi, P) ∈ Ai ∪ Vi ∪ P, we have It follows that (ωi aco, kθvi caco) = 0|N i aco∪N i caco| (i.e., all entries are identical) if Xi 0 has full column rank. Under the conditions of the Theorem, Lemma 1 ensures that either Ii cacLi acIi ac or Xi 0 has full column rank and, for all i ∈ N[1,Nac], all entries of (ωi, Ki θvi cac) are identical when x ∈ M. Next, we assume that Ndc > 0 and Nac > 0 and pick any j ∈ N[1,Ndc]. Next, x ∈ Vj implies that vj = v⋆1|N j dc| where v⋆ ∈ R. Using condition 1, this results in ωi = kj θv⋆1|N iac| for each N i cac ∩ N j cdc ̸= ∅. Moreover, if Ndc > 1 there exists l ∈ N[1,Ndc] such that N i cac ∩N l cdc ̸= ∅ and vl = v⋆kj θ/kl θ1|N lcdc∪N l dc|. Because the graph G is connected, it follows by induction over the dc and ac subgrids that all the frequency and dc voltage deviations are proportional to v⋆. By Assumption 2, at least one dc voltage deviation or frequency deviation will be zero if x ∈ M, and therefore v⋆ = 0 and M = {0n}. If Ndc = 0, it follows that Nac = 1 and Ii cac is an empty matrix. Thus, we require Xi 0 to have full column rank, and using Assumption 2.1 it directly follows that M = {0n}. Moreover, if Nac = 0, it follows that Ndc = 1 and it trivially follows from Assumption 2 and x ∈ Vi that M = {0n}. Note that if D ≻ 0 and G ≻ 0 the proof simpliﬁes and only d dtω = 0|Nac| and d dtv = 0|Nc∪Ndc| are needed to characterize the invariant set S.
Private Information Retrieval from Non-Replicated Databases<|sep|>In this paper, we investigated the PIR problem from non-replicated and non-colluding databases. We studied the (K, R, 2, N) storage systems, where every database stores M = 2 messages. This system is uniquely described by an R-regular graph. We proved a general upper bound, which depends on the spread of the graph. We derived the capacity of two classes of graphs, namely: cyclic graphs and fully-connected graphs. For these two classes of graphs, we proposed novel achievable schemes, whose retrieval rate matches the developed upper bound. Our results showed that non-replication signiﬁcantly hurts the retrieval rate.
Extragalactic Point Source Search in WMAP 61 and 94 GHz Data<|sep|>We present a CMB independent method for point source detection and apply it to the WMAP ﬁrst-year and three-year maps. The main result of this search is a point source catalog of 64 members, out of which 21 are newly detected in WMAP data. Two sources remain unidentiﬁed. We are not sure whether they are variable or extended sources. More years WMAP data are needed to further identify them. We demonstrate that our method is both competitive with and complementary to other currently adopted methods of point source searching in the WMAP data, with a low detection threshold as well as a good positional accuracy (∼ 15% of the W-band FWHM). And our source list can be a good supplement to the existing WMAP point source catalog and the point source mask. Contamination from unresolved point sources is at a negligible level of (2.4±0.8)×10−3 µK2 sr at WMAP V-band, which implies a correction value A = 0.012 ± 0.004 µK2 sr. For most sources, with Fν ∝ ν0, the K, Ka and Q bands have a higher signal to noise ratio and therefore can yield more source detections as proven in the search results from Hinshaw et al. (2007), Nie & Zhang (2007) and L´opez-Caniego et al. (2007). However, part of the intention of this work was to look for interesting sources that might peak around the V and W bands. Nevertheless, among all the identiﬁed sources in our catalog, we ﬁnd 39 quasars, 10 galaxies, 9 HII regions and 4 other radio sources. There is no sign of a signiﬁcant diﬀerent class of new objects. We are planning to extend this work by including the WMAP Q-band data. We anticipate having more source detections so that a more accurately model of the source count distribution and furthermore a better estimate of the contribution from unresolved point sources to the CMB power spectrum can be reached. We acknowledge the use of the Legacy Archive for Microwave Background Data Analysis (LAMBDA). Support for LAMBDA is provided by the NASA Oﬃce of Space Science. We also want to acknowledge the use of the VLA, CARMA and ATCA. We want to especially thank Philip Edwards for making the ATCA observation possible. PMN and GB6 maps in this paper are retrieved from the SkyView Virtual Observatory, which is a service of the Astrophysics Science Division at NASA/ GSFC and the High Energy Astrophysics Division of the Smithsonian Astrophysical Observatory (SAO).
Reactive Liquid: Optimized Liquid Architecture for Elastic and Resilient Distributed Data Processing<|sep|>We presented Reactive Liquid, a highly scalable and resilient distributed architecture based on the Liquid architecture. The Reactive Liquid delivers the reactive manifesto promises which are responsiveness, resiliency, elasticity, and message-driven communication. The Reactive Liquid detects the failed components as quickly as possible and regenerates them to heal the system. Moreover, The Reactive Liquid adjusts the usage of the resources to ﬁt the computation power to input workload. In other words, the Reactive Liquid is elastic which means scalable on demand. Our work leaves an open research problem. The completion time of the Reactive Liquid architecture is generally more than the Liquid architecture, which contradicts the reactive manifesto. Nevertheless, the Reactive Liquid architecture satisﬁes the requirements of a reactive system. It seems that the need for a message distribution scheduler algorithm which distributes the messages among the tasks is crucial to minimize the completion time of the messages.
$f(R)$ Dual Theories of Quintessence : Expansion-Collapse Duality<|sep|>Quintessence ﬁelds are viable alternatives to the ΛCDM models, which can provide an explanation for the recent accelerated expansion of the universe. It is known that an f(R) theory of gravity in the Jordan frame acts as a dual to a quintessence ﬁeld in Einstein gravity, in a conformally connected spacetime, known as the Einstein frame. In this work, we reconstruct f(R) functions in Jordan frame which reproduce quintessence models with time-independent and time-dependent equation of state parameter (w) in the Einstein frame. As an example of time-dependent equation of state parameter, we choose the logarithmic parameterization and reconstruct f(R) function corresponding to the quintessence model. This solution is obtained in two parts. A perturbative solution of F(R) is obtained which is valid in the small curvature limit of the Jordan frame. We show that this perturbative solution may be applicable in the near future of the Einstein frame universe. However, we ﬁnd that this ansatz becomes ill-deﬁned in the late time limit of Einstein frame. We obtain an asymptotic solution for f(R), which is valid in large ﬁeld and small curvature limit of the Jordan frame. This asymptotic solution is applicable in the late time limit of the Einstein frame. We also show that the Jordan frame scale factor has a ﬁnite maximum value determined by the quintessence model parameters, after which it keeps on decreasing. In the late time limit of the Einstein frame, the Jordan frame universe collapses while the expansion in the Einstein frame universe continues. We generalize this result and obtain the condition for expansion-collapse duality in terms of a simple inequality. This condition can predict the possibility of Jordan frame collapse for quintessence models with arbitrary equation of state parameters, even in the presence of other components in the universe such as dust, radiation or spatial curvature. Given an equation of state parameter, such prediction can be made only from the knowledge of energy density of the quintessence model. Using this condition we show that the example quintessence model does not necessarily lead to the Jordan frame collapse in the presence of dust, however the expansion-collapse duality may be recovered by introducing a positive spatial curvature component in the universe. The general condition for expansion-collapse duality can be readily applied to other physically viable quintessence models. This opens a possibility of further studies of the collapse of the Jordan frame. The mapping between expanding and collapsing geometries may have implications on growth of perturbations which is a subject of further exploration. Research of K.L. is partially supported by the DST, Government of India through the DST INSPIRE Faculty fellowship (04/2016/000571). The authors acknowledge the useful discussions with Jasjeet S. Bagla. Here we present the solutions of the ansatz (3.21), i.e., we write the constants ϵ0, ϵ1, ϵ2 in terms of the quintessence parameters w0, w′. We use the ansatz (3.21) in the RHS of (3.15)
Nuclear effects in neutral current quasi-elastic neutrino interactions<|sep|>Our work indicates that the theoretical analysis of the MiniBooNE NCE data sample involves the same diﬃculties already emerged in the studies of CCQE interactions [14]. The results discussed in Section 3, showing that it is impossible to describe both the CCQE and NCE data sets using the same value of the axial mass, conﬁrm that nuclear eﬀects not included in the oversimpliﬁed RFG model cannot be taken into account through a modiﬁcation of MA. In this context, it has to be pointed out that the need of a larger MA to reproduce the measured NCE Q2-distribution is not likely to be ascribable to diﬀerent nuclear eﬀects in the CCQE and NCE channels. In fact, the ratio between the Q2-distributions obtained from the RFG model and the spectral function approach, providing a measure of the eﬀects of nuclear dynamics, turns out to be nearly identical for CCQE and NCE. The diﬀerence does not exceed 2% over the whole Q2 range. Our analysis also shows that the strange quark contribution to the cross section of nuclei with equal number of protons and neutrons is vanishingly small. As a consequence, the possibility of improving the agreement between MC simulations and Carbon data adjusting the value of ∆s appears to be ruled out. The authors of Ref.[14] argued that the disagreement between theory and MiniBooNE CCQE data may be due to the uncertainties associated with the ﬂux average procedure, as the resulting cross section at ﬁxed energy and scattering angle of the outgoing muon picks up contributions from diﬀerent kinematical regions, where diﬀerent reaction mechanisms are known to be dominant. This uncertainty also aﬀects the ﬂux averaged NCE diﬀerential cross section, which is given in bins of reconstructed Q2 [2], deﬁned as where M is the nucleon mass and T is the sum of the kinetic energies of the ﬁnal state nucleons. In order to provide results that can be compared to data in a meaningful fashion, theoretical models of neutrino-nucleus interactions must be based on a consistent description of the broad kinematical range corresponding to the relevant neutrino energies. In the quasi elastic sector, this amounts to taking into account, besides single-nucleon knock out, multi-nucleon knock out as well as processes involving the nuclear two-body currents, whose contribution is expected to be signiﬁcant [29].
Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic Experiences for Robot Action Execution<|sep|>We proposed a deep neural network implementing an episodic memory. Given a set of training data, the proposed network ﬁrst generates subsymbolic representation of action episodes. Such a latent encoding can be used to distinguish actions, reconstruct memorized episodes, and predict future frames based on the spatio-temporal features extracted by the deep architecture. We show that conceptual similarity of videos is reﬂected by the proximity of their vector representation in the latent space. Using this property of the latent space, we introduce a matching and retrieval mechanism, which enables the recollection of previously experienced visual episodes. Benchmarking our proposed mechanism against a variety of action descriptors, we show that our model outperforms other state-of-the-art approaches in terms of matching precision. We conduct various experiments showing that the proposed framework can help extending the cognitive abilities of a humanoid robot such as action encoding, storing, memorizing, and predicting. Since the memory instances in our approach are purely visual, the adaptation of stored actions on a robot to new scenario is not straightforward. For instance, our robot manipulation pilot study requires an auxiliary object detector for trajectory extraction. In future work it would be desirable to embed relevant action information such as trajectories into the latent representation and directly use them for action adaptation and execution. Another limitation of our approach is the observed background bias. To make the model less sensitive to different backgrounds it might be promising to avoid pixel-level predictions and instead employ segmentation masks as reconstruction targets. To the best of our knowledge, this is the ﬁrst comprehensive study that attempts to encode visual experiences not only for matching and retrieving purposes but also for prediction and reconstruction in a longer time scale. Comparable work such as [11], [14] can only achieve unsupervised action matching without reconstructing the memorized video episodes. Video prediction models, introduced in [21], [24], can only predict a single future frame at a time. Thus, the mentioned approaches lack key features to resemble an episodic memory. Our model overcomes the architectural drawback of [14] and comprises the full episodic-memory capabilities described above. Also, we are not aware of any previous work that extensively applies an episodic memorylike framework to such large and complex datasets.
The Generalised Colouring Numbers on Classes of Bounded Expansion<|sep|>In this work we gave several new applications of the generalised colouring numbers on classes of bounded expansion. In particular, we have shown that whenever a graph class C excludes some ﬁxed topological minor, then any graph from C admits one ordering of vertices that certiﬁes the boundedness of the generalised colouring numbers for all radii r at once. It is tempting to conjecture that such an ordering exists for any graph class of bounded expansion. Our construction of the uniform ordering proved to be useful in showing that model-checking successor-invariant FO is FPT on any graph class that excludes a ﬁxed topological minor. We believe that our construction may be helpful in extending this result to any graph class of bounded expansion, since both the construction of the order, and the reasoning of Section 5, are oblivious to the fact that the graph class excludes some topological minor. The only place where we used this assumption is the analysis of the constructed order.
Automated and Explainable Ontology Extension Based on Deep Learning: A Case Study in the Chemical Domain<|sep|>We have presented a novel approach to the problem of ontology extension, applied to the chemical domain. Instead of extending the ontology using external resources, we created a model using the ontology’s own structured annotations. This Transformer-based model can not only classify previously unseen chemical entities (such as molecules) into the appropriate classes, but also provides information about relevant aspects of its internal structure on which the decision is based. At the same time, it was able to outperform previously existing approaches to ontology-based chemical classification in terms of predictive performance. However, the trained model still struggles with several chemical classes that depend on specific structural features. E.g, classes that exhibit cyclic structures are often found in the lower quantile of classification quality. This behaviour can be traced back to the way molecules are encoded into the SMILES notation. This weakness might be addressed by using architectures that operate directly on the molecular structures, such as Graph Neural Networks [42]. We have illustrated our approach by applying it to the chemical domain, but as we discussed in Section 5, the approach is applicable to any ontology that contains classes that are annotated with information that is relevant to their position in the class hierarchy. While our approach supports an automatic extension of an ontology, it can also be used in a semi-automated fashion to help ontology developers in their manual curation of the ontology. Since the model is trained based on the content of a manually curated ontology, improving and extending this ontology will lead to better quality training data and, thus, enable better predictions. Hence, there is a potential for a positive feedback loop between manual development and the AI-based extension. One limitation of our current approach is that it does not use most of the logical axioms of the ontology during the learning process. The logical axioms within the ontology could be used to detect possible inconsistencies between the predicted classes and the ontology’s axioms. A logic-based framework could then be employed to detect those results that were most likely mis-classifications. Another strategy to address this gap would be to represent the axioms in the form of Logical Neural Networks [43] in order to detect possible inconsistencies already in the learning process and to penalise them accordingly. Overall, there is still a pressing need for research in the field of (semi-)automatic ontology extension. Here, the growing field of neuro-symbolic integration can serve as the interface between formal ontologies and the power of deep learning. The possibility of incorporating explanations may further the understanding of the inner workings of artificial intelligence systems and, therefore, raise trust in these systems.
Beam Designs for Millimeter-Wave Backhaul with Dual-Polarized Uniform Planar Arrays<|sep|>In this paper, we proposed the beamformer designs for mmWave MIMO backhaul systems with dual-polarization UPAs. The SE and MIP criteria are considered as speciﬁc examples for beam design optimizations. For each criterion, the reformulated optimization problem is solved with constraints regarding the dual-polarization UPA structure. The resulting beamformers depend on the partial channel information, and we also proposed the use of pilot sequences to ﬁgure out the required channel information. While solving the optimization problems, we have shown that, for both the Tx and Rx beamformings, the optimal dual-polarization UPA beamformers can be directly built from the optimal beamformers of single-polarization UPA sharing the same optimality. This ﬁnding shows that any previously designed beamformers for the single-polarization can be used to efﬁciently build dualpolarization beamformers satisfying the same performance metric by simple matrix operations.
Modeling Transitivity in Complex Networks<|sep|>In this paper, we proposed a new model, called the η model, for describing transitivity relations in complex networks. We theoretically analyzed the model and calculated a lower bound on the clustering coefﬁcient of the model which is independent of the network size and depends only on the model’s parameters (η and m). We proved that the model satisﬁes important properties such as power-law degree distribution and the small-world property. We also evaluated the model empirically and showed that it can precisely simulate real-world networks from different domains with different speciﬁcations.
Using Duality in Circuit Complexity<|sep|>We have presented a method applicable to arbitrary classes of languages, to describe circuit classes by equations. The tools and techniques used originate from algebra and topology and have previously been used on regular language classes. Due to recent developments in generalizing these methods to non-regular classes, they are now powerful enough to describe circuit classes. But the knowledge that they are powerful enough itself is not suﬃcient, as we require a constructive mechanism behind these descriptions. Since non-uniform circuit classes are by deﬁnition not ﬁnitely presentable, this seemed to be impossible. Nevertheless, we were able to ﬁnd a description of small but natural circuit classes via equations. This description seems helpful as it easily allows to prove non-membership of a language to some circuit class. Another advantage is the possibility of using Zorn’s Lemma for the extension of ﬁlter bases to ultraﬁlters, which prevents us from having to use probabilistic arguments in many places. Also in Lemma 34 we use purely topological arguments of convergence, for which it is unclear how this could be achieved purely combinatorially. The results we acquired are not so diﬀerent from the results about equations for varieties of regular languages by Almeida and Weil [2]. This gives hope that their results can be used as a roadmap for further research. In [7] it was shown that a certain restricted version of the block product of our constant size circuit classes would actually yield linear size circuit classes (over the same base). Here having equations for all languages captured by this circuit class, not just the regular ones, would pay oﬀ greatly. By showing that a padded version of a language is not in a linear circuit class we could already prove that PARITY is not in a polynomial size circuit class. Equations for non-regular language classes could be used to overcome previous bounds. The separation results in the corollary can easily be extended to show that a padded version of those languages is not contained in these circuit classes. A diﬀerent approach would be to examine the way the block product was used here. The evaluation of a circuit is equivalent to a program over ﬁnite monoids. While the program itself has little computational power, it allows non-uniform operations like our N-transducers. The ﬁnite monoid itself corresponds loosely speaking to the computational power of the gates of the circuit, which was handled by our variety V. For general circuit classes one would need to consider larger varieties containing also non-commutative monoids. While the methods here seem to be extendable to non-commutative varieties, the more complicating problem remaining is to ﬁnd an extension of the block product that corresponds to polynomial programs over these monoids.
The large deviations of the whitening process in random constraint satisfaction problems<|sep|>A. Technical details on the cavity treatment 35 1. Simplifying the Belief Propagation equations 35 2. Finite time horizon 38 3. Explicit time clipping 40 4. The regular graph case 40 B. The large k limit of the T = 1 results 42 1. The large k limit of the entropy function 42 2. The tipping point 43 3. The asymptotic behavior of the threshold l1(k) 44 C. The ﬁxed points of the whitening (large T limit) 44 1. A more compact equivalent form of the RS equations 44 2. A dynamical system point of view 45 3. The unfrozen solutions for l > lr 45 4. The frozen solutions of the ﬁrst kind 47 5. The frozen solutions of the second kind 48 6. The large k expansion of the entropy s∞(θ = 0) and of the threshold l∞(k) 49
A linear-time algorithm for semitotal domination in strongly chordal graphs<|sep|>In this paper, we resolved the complexity status of MINIMUM SEMITOTAL DOMINATION problem in strongly chordal graph, which is an important subclass of chordal graphs. The complexity status of the problem in dually chordal graphs and tolerance graphs is still unknown. It would be interesting to investigate the complexity of the problem in these graph classes. Further, as the problem is NP-complete in planar graphs, designing approximation algorithms for the problem in planar graphs is a good research direction.
Elastic Coulomb breakup of $^{34}$Na<|sep|>In the present work, we try to analyse the halo character of 34Na while calculating diﬀerent reaction observables if it undergoes elastic Coulomb breakup when bombarded on 208Pb at 100 MeV/u and in doing so, try and get a better understanding of its one neutron separation energy and ground state conﬁguration. We apply the Coulomb dissociation method under the patronage of the ﬁnite range distorted wave Born approximation extended to include the eﬀects of deformation in an approximate way. This theory requires only the ground or the bound state wavefunction of the projectile and includes the entire non-resonant continuum. We are able to factorize the reduced transition amplitude into a structure part and a dynamics part. The theory, with and without the inclusion of deformation, has been used in the past to study various nuclei as well as their radiative capture reactions [32, 34, 58–60]. Our results, combined with the patterns detected in the medium mass region of ‘island of inversion’ with N = 20 28, augment the speculation that 34Na has a dominant p-state conﬁguration, which also support the suggestions by
Resource-efficient adaptive Bayesian tracking of magnetic fields with a quantum sensor<|sep|>In this article, we have simulated and tracked a ﬂuctuating magnetic ﬁeld via Ramsey experiments on an NV centre. Ramsey measurements were optimised through an adaptive Bayesian update protocol, with Gaussian approximation of all probability distributions. A comparison of the Gaussian-approximated with the original protocol revealed that the approximation yielded potentially signiﬁcant increases in computation speed, which ranged from 1.3 to 13.5 times faster, depending on the experiment parameters. In the cases of T ∗ 2 = 100 µs, 10 µs, the approximation performed slightly poorer at tracking than the non-approximated protocol. However, these cases yielded larger increases in speed – around an order of magnitude faster than the original protocol. In the slower cases (where T ∗ 2 = 1 µs), this lesser increase in speed was compensated for by the Gaussian method having the better tracking performance. This protocol could ﬁnd applications in sensing settings where one needs to track a ﬂuctuating signal with statistics that are, at least approximately, known in advance. For example, one could use an NV centre in a nanodiamond to monitor temperature drift inside a living cell [24,25]. Measuring temperature is an integral part of studying energy metabolism [41] or developmental processes [42, 43]. One other issue with nanodiamonds, is that while moving in a ﬂuid medium, they rotate considerably, often very rapidly. Our protocol could be extended to track this rotation with minimal resource consumption. Another possible application is in experiments with levitated nanodiamond, where our technique could be used for fast tracking of rotation. In these experiments, the nanodiamond containing NV centres is held in place translationally using ion traps [44,45], optical traps [46] or magnetic traps [47]. The librational (rotational) frequencies of trapped nanodiamond vary from 100s [48] of Hz to 1GHz [49]. For the lower frequencies, in which tracking resolutions of 1ms per data point are suitable, our method could be directly applied. However, the method could also be used in conjunction with ac magnetomety [50, 51], using spin-echo instead of Ramsey measurements, to achieve tracking of faster, periodic librations. Tracking provides the information required for realigning the nanodiamond orientation, for whichever feedback mechanism the traps use. This feedback mechanism could potentially be 3D Helmholtz coils in the case of ion traps or optical traps. For a magnetic trap, it has been proposed [52] that the diamond orientation be conﬁned with an electrode using the dielectric force on the non-spherical diamond. Though this method does not conventionally require feedback, tracking may nonetheless prove useful in testing the conﬁnement. Finally, the Gaussian-approximation described here could be applied to track a quantum signal, such as the magnetic ﬁeld arising from a bath of nuclear spins surrounding a central electron spin. Previous theoretical work has shown that, by adaptively tracking the ﬂuctuating nuclear magnetic ﬁeld and narrowing its distribution through the back-action of the quantum measurement process, one can considerably extended the coherence time of the central spin [53]. The protocol described here can reduce the computational complexity of this task, enabling faster and more precise tracking. The authors would like to thank Eleanor Scerri for helpful discussions. This project is supported by the Engineering and Physical Sciences Research council through grants EP/S000550/1 and EP/T01377X/1, and by a Weizmann-UK Joint Research Program grant. K.C. acknowledges studentship funding from EPSRC under grant no. EP/L015110/1. Y.A. is supported by the Royal Academy of Engineering under the Research Fellowship scheme RF201617/16/31.
Structural and excited-state properties of oligoacene crystals from first principles<|sep|>In summary, we have studied the structure and excited state properties of the series of acene-based crystals, from benzene to hexacene, from ﬁrst principles using vdWcorrected-DFT and MBPT. Both vdW-DF and pair-wise correction methods were found to predict lattice parameters in excellent agreement with experimental data. We ﬁnd that DF1 overestimates volumes but DF2 improves over DF1, consistent with the general trends for these functionals. DF-cx further improves lattice parameters, with a residual discrepancy of < 1%). Furthermore, the relatively simple TS pair-wise approach performs as well as the best DF methods. For acenes in the solid-state, charged excitations are generally well-described by the G0W0 method, but partial self-consistency – in the form of the evGW method – is needed for hexacene, likely owing to the PBE starting point employed in this study. The results are found to be sensitive to the geometry used owing to a combination of inter-molecular hybridization and polarizationinduced level renormalization. Neutral low-lying singlet and triplet excitation energies are generally welldescribed using the G0W0-BSE method. They are generally less sensitive to structure, except for the important case of singlet excitations in larger acenes. There, large structural sensitivity is found owing to signiﬁcant delocalization of the singlet state. Our study reveals the importance of an accurate account of dispersive interactions as a prerequisite to predictive calculations of excited states properties in the acene crystals. Furthermore, it suggests routes for predictive calculations, in which both structures and excited states are calculated entirely from ﬁrst-principles, for broader classes of molecular solids. T. Rangel thanks Marc Torrent and Muriel Delaveau for addressing technical issues in ABINIT, related to the calculation of a large number of bands needed for GW calculations. This research was supported by the SciDAC Program on Excited State Phenomena in En Cohesive energy [eV] LDA PBE D2 TS∗ DF1 DF2 DF-cx Exp. Benzene 0.59 0.12 0.73 0.69 0.64 0.60 0.61 0.52 Naphthalene 0.76 0.15 1.16 1.04 0.93 0.86 0.92 0.82 Anthracene 0.97 0.19 1.61 1.39 1.24 1.16 1.23 1.13 Tetracene 1.21 0.25 2.10 1.56 1.42 1.56 Pentacene P1 1.46 0.30 2.61 1.88 1.76 1.87 Pentacene P2 1.48 0.30 2.63 1.88 1.76 1.92 Pentacene P3 1.42 0.31 2.61 1.88 1.79 1.87 Hexacene 1.82 0.36 2.18 2.21 2.09 2.30 TABLE VI. Cohesive energies of the acenes. Calculated (Ecoh.) and experimental (EExp. coh. ) cohesive energies are tabulated. Experimental cohesive energies are taken from Ref. 41. MAE and MA%E are shown for all functionals: MAE=�Nm i |EExp. coh.,i − Ecoh.,i|/Nm and ergy Materials funded by the U.S. Department of Energy, Oﬃce of Basic Energy Sciences and of Advanced Scientiﬁc Computing Research, under Contract No. DEAC02-05CH11231 at Lawrence Berkeley National Laboratory. Work at the Molecular Foundry was supported by the Oﬃce of Science, Oﬃce of Basic Energy Sciences, of the U.S. Department of Energy under Contract No. DEAC02-05CH11231. This research used resources of the National Energy Research Scientiﬁc Computing Center, which is supported by the Oﬃce of Science of the U.S. Department of Energy. Work in Sweden supported by the Swedish Research Council and the Chalmers Nanoscience Area of Advance. Work in Israel was supported by the US-Israel Binational Science Foundation, the molecular foundry, and the computational resources of the National Energy Research Scientiﬁc Computing center. In this Appendix we provide detailed information on the structural data obtained with diﬀerent methods for the acene family of crystals. As in the main text, we consider standard DFT methods (LDA and PBE) and diﬀerent vdW methods: D2, TS, DF1, DF2 and DF-cx. Throughout, we make use of CSD107 data to benchmark our results. For the smallest acenes, low temperature data (T ≤ 14 K) is available in the CSD under the entries BENZEN14, NAPHTA31, and ANTCEN16, from Refs. 95, 103, and 104. Ref. 163 also reports low temperature data for benzene, consistent with the data of Ref. 103. For tetracene P1 and pentacene-P2, we extrapolate experimental data from Refs. 113 and 162 to zero Kelvin, as shown in Fig. 7. Note that we assign the tetracene structures of Ref. 162 to its P1 polymorph.164 For other pentacene polymorphs and hexacene, in the absence of suﬃcient low-temperature data that would allow for extrapolation to 0 K, we compare to the lowest-temperature experimental data available from Refs. 94, 97, and 98, also found in the CSD as PENCEN, PENCEN10, and ZZZDKE01. We emphasize that only by extrapolating experimental data to 0 K do we observe consistent trends in the comparison of our relaxed geometries for the various DFT methods used here. In the main text, we have also compared our data to experimental cohesive energies. These are taken from Ref. 41, in which temperature contributions have been removed. A complete set of experimental and calculated lattice parameters and cohesive energies is given in Tables VII and VI, respectively. Lattice parameters are usually found in literature following old conventions. However, recent data use the so called Niggli165 (or reduced-) lattice parameters. For completeness, we present both conventions in Table VII. Finally, in Fig. 8 we present a comparison of theory and experiment for the angles that characterize the herringbone structure in the three pentacene polymorphs. Here, all DF approximations predict angles in good agreement with experiment. At the experimental resolution and temperature, we cannot conclude deﬁnitively which DF version performs best for angle prediction, but see no reason for trends diﬀerent from those reported in the main text.
Interface-resolved direct numerical simulations of sediment transport in a turbulent oscillatory boundary layer<|sep|>New and interesting information of the sediment transport generated by sea waves is obtained by means of the DNSs which allow to evaluate the hydrodynamics within the oscillatory boundary layer generated by surface waves close to the bottom and to determine the dynamics of idealised sediment particles dragged by the ﬂowing ﬂuid. The values of the ﬂow Reynolds number fall in the intermittently turbulent regime, such that turbulence is signiﬁcant only during part of the ﬂow cycle. The other parameters are typical of medium sand. Hence, the results are useful to quantify the bedload sediment transport outside the breaking and surf regions where higher values of the Figure 24: Dimensionless sediment ﬂow rate qs as function of the absolute value of the Shields parameter for simulations at Rδ = 72, 128 and d ≃ 0.25 (solid blue and red lines) (non turbulent, Mazzuoli et al., 2019) and for the non-turbulent cycles of run 1at Rδ = 450 and d = 0.335 (black line). The blue dashed-dotted blue line represents the formula by Wong and Parker (2006). Arrows indicate the orbital direction. Reynolds number are usually found such that the DNSs of the turbulent ﬂow ﬁeld within the bottom boundary layer are presently unaﬀordable. The main result of the investigation is the description of sediment dynamics under the action of the turbulent eddies which are generated within the boundary layer. The pressure ﬂuctuations induced by the turbulent eddies penetrate within the porous bed and generate lift forces which superimpose to those due to the pressure diﬀerence between the bottom and the top of the sediment particles that, in turn, is associated with the shear ﬂow close to the bed surface. On average, the lift force due to the turbulent pressure ﬂuctuations is upward directed and the sediment grains tend to be picked-up from the bed and then transported by the external ﬂow in the saltation mode. On the other hand, when the ﬂow re-laminarises but the bed shear stress is large enough to induce sediment transport, the sediment grains tend to roll and slide one over the top of the others. This particle dynamics is typical of a laminar ﬂow and it gives rise to sediment transport rates quite diﬀerent from those observed when turbulence is present. The diﬀerences between the values of qs generated by a laminar and a turbulent oscillatory boundary layer can be easily appreciated if the results of ﬁgure 23 are compared with those obtained by Mazzuoli et al. (2019) which, for reader’s convenience, are plotted in ﬁgure 24. Mazzuoli et al. (2019) investigated the formation of sea ripples by means of DNSs and computed the sediment transport rate for values of the parameters as those of some of the laboratory experiments of Blondeaux et al. (1988). In particular, the experiments characterised by Rδ = 72 and Rδ = 128 and by d ≃ 0.25 were considered by Mazzuoli et al. (2019). For such values of the Reynolds number, the ﬂow regime is laminar. In ﬁgure 24, the results obtained for Rδ = 450 and d = 0.335 (run 1) during the half-cycles characterised by weak turbulence are also plotted. As already pointed out, in these cases, the values of qs during the accelerating phases are diﬀerent from those computed during the decelerating phases, even if the Shields parameter θ is the same, because particle dynamics is aﬀected not only by the bottom shear stress but also by the
Constraints on decaying dark matter from weak lensing and cluster counts<|sep|>We have investigated the nonlinear structure formation in the DDM model, in particular focussing on the halo abundance. For this purpose, extending our previous study [2], we performed N-body simulations in the DDM model. We have shown that DDM suppresses the halo abundance in these simulations. This suppression is predominantly due to the mass loss of the formed clusters originating from the decay of DDM, while the relaxation of the gravitational instability caused by the DM decay also contributes. Adopting the ﬁtting function for the halo abundance based on the simulation, we derived cosmological constraints on the DDM from the Planck 2015 SZ cluster count combined with the Planck 2015 CMB power spectrum and the KiDS450 cosmic shear power spectrum. We have found the cosmological observations are consistent with CDM and obtained a lower bound on the lifetime of DM as Γ−1 ≥ 175 Gyr from the combination of all the data above. We note that our simulation is based on the collisionless N-body simulations and hence baryonic eﬀects are not taken into account. We expect that baryonic eﬀects will not aﬀect the mass function of cluster-sized massive haloes very signiﬁcantly. Moreover, since the baryonic eﬀect in general decreases the mass of haloes and hence further suppresses the mass function for given mass [43, 44], its eﬀect is more or less degenerate with the decay rate of DDM. Therefore, one can regard our lower bound on Γ−1 as conservative in respect to the baryonic eﬀect. This research was supported by JSPS KAKENHI Grant Numbers 15K05084 (TT), 17H01131 (TT), 15H02082 (TS), 18H04339 (TS), 18K03640 (TS), MEXT KAKENHI Grant Number 15H05888 (TT) and UK Space Agency grant ST/N00180X/1. We thank the CSC - IT Center for Science (Finland) for computational resources. We here present the ﬁtting formula for the halo mass function in DDM model. Given the DDM decay rate Γ, the suppression in the mass function from the CDM case (i.e. Γ = 0) at halo initial mass Mi and redshift z can be approximated by We note that our ﬁtting formula is calibrated with the best-ﬁt parameters of the Planck 2015 TT+TE+EE results. Our ﬁtting formula can reproduce the suppression ϵ as function of Mi with accuracy of ∼ 20% for 1014 ≤ Mi/(h∅M⊙) ≤ 1015, 0 ≤ z ≤ 1 for Γ−1 = 31 Gyr as is shown in the ﬁgure 4. For larger Γ−1, the suppression in the mass function becomes less prominent compared to the statistical error, which makes it harder to assess the accuracy of our ﬁtting formula in terms of the suppression factor ϵ. Still, our ﬁtting formula shows reasonable agreement with the simulation results. Moreover, as shown in ﬁgure 1, the mass function in terms of actual halo mass M = Mi{(1 − rdm) + rdme−Γt} exhibits more prominent suppression from the CDM model than in the initial halo mass Mi. This is because halo mass function is the decreasing function of Mi, and Mi in the DDM model should necessarily be larger than that in the CDM model. Even if the abundance of halos with Mi were the same in these two models, given a ﬁxed actual halo mass M, the mass function of M in DDM would be suppressed compared to CDM. In reality, the halo mass function of Mi in DDM model is suppressed relative to the one in the CDM model as shown in ﬁgure 4. Figure 5 shows the concentration parameter c as function of the halo mass. We have ﬁtted the density proﬁle of halo assuming the NFW halo proﬁle [45]:
The effect of Galactic foreground subtraction on redshifted 21-cm observations of quasar HII regions<|sep|>One of the primary goals of future low-frequency telescopes is the detection of large quasar-generated HII regions in redshifted 21-cm emission during the epoch of reionisation. Like all 21-cm signatures of the reionisation era, the detection of a quasar HII region will need to overcome the diﬃculties associated with the removal of bright Galactic and extragalactic foregrounds. In this paper we have assessed the impact of removing a spectrally smooth foreground on the observable properties of high-redshift quasar HII regions. We have assumed prior removal of extragalactic point sources and considered only the unpolarised Galactic synchrotron foreground. The primary eﬀect of continuum foreground removal is to erase contrast in the image. In particular, contributions to the 21-cm intensity ﬂuctuations that have a scale length comparable to the frequency bandpass of the observation are removed by foreground subtraction. On the other hand, we ﬁnd that this loss of contrast does not aﬀect the ability of 21-cm observations to measure the size and shape of the HII region. We model the HII region shape as a spheroid described by six parameters and show that the shape recovered following foreground removal agrees well with the shape derived directly from ﬁtting using the input model. Using the recovered best-ﬁt shape of the HII region, the global neutral fraction of hydrogen in the IGM could be measured directly from the contrast in intensity between regions that are within and beyond the HII region. However, we ﬁnd that since foreground removal lowers the observed contrast between the HII region and the IGM, such a measurement of the neutral fraction would require a correction factor. Our results suggest that the value of this correction factor depends on the reionisation history. This correction factor would therefore need to be modeled using uncertain astrophysics. In addition, the measured contrast of the HII region, and therefore the inferred neutral fraction, is sensitive to the degree of the polynomial used for the foreground removal. Thus, measurement of the neutral fraction from quasar HII regions will require a detailed knowledge of the continuum foreground spectrum. Finally, we have considered both cases where the quasar is observed in an IGM which evolves slowly relative to the light crossing time of the HII region, and where the IGM evolves rapidly. The latter case is likely to be more relevant for observations around high-redshift quasars which are observed to be very rare at z > 6. Our results indicate that the evolution of the IGM will not impact negatively on the ability of 21-cm observations to measure the size and shape of quasar HII regions. Acknowledgments PMG acknowledges the support of an Australian Postgraduate Award. The research was supported by the Australian Research Council (JSBW). NP and SPO acknowledge support from NSF grant AST-0407084 and NASA grant NNG06H95G.
Boosting Deep Neural Networks with Geometrical Prior Knowledge: A Survey<|sep|>In this section, we ﬁrst summarize the various methods presented and their different advantages and disadvantages. Afterwards, we draw a conclusion and give a short outlook on possible future work. Data augmentation is easy to implement and highly adaptable towards a variety of transformations. Additionally, it can easily be used in combination with other approaches. On the downside, it does not provide any mathematical guarantees, augmenting the data with all possible transformations is computationally inefﬁcient, the solution space is not restricted effectively and the equivariance is only learned for the network as a whole, not at each layer. Restricted ﬁlters provide a provable way to obtain in- or equivariant representations in neural networks and reduce the space of learnable parameters. However, the restrictions might be too strict and prevent the network from learning informative representations, especially when the ﬁlters are ﬁxed rather than learnable. Enforcing equivariance properties using group convolutions provides a mathematically guaranteed way to incorporate geometrical prior knowledge to DNNs and leads to state-of-the-art results in a variety of tasks beneﬁting from in- or equivariance properties. Up to now, group convolutions are restricted to fairly simple and often ﬁnite transformation groups and suffer from a computational overhead. Moreover, important symmetries occurring in tasks are not expressible using the concept of groups, e.g. viewpoint changes in images. Incorporating in- or equivariance to multiple transformation groups at once is also an area which needs further investigation. Learned Feature Transformations provide a way to learn invariance from data rather than deﬁning and incorporating it manually. Feature transformation layers are easy to include to existing neural network architectures. The transformations are learned from data and not restricted to groups. Nonetheless, learned feature transformations usually lack mathematical guarantees. Capsules disentangle learned object representations into invariant object information and equivariant pose parameters. This includes sophisticated transformations such as viewpoint-changes or lighting which can not be modeled as groups. However, capsules do not guarantee the desired properties. Adaptations leveraging group symmetry exist but either suffer from a reduced transformation efﬁciency or restrictions regarding the equivariant pose information. Capsule networks are non-straightforward to train because they need a carefully-designed routing algorithm to assign activations to other layers as well as for backpropagation. Invariant Integration is a method to guarantee invariance while increasing the separability of distinct patterns. However, it suffers from computational complexity, is restricted to group transformations, the monomials are hard to choose in a principled manner and disposes the information about the symmetry group. Consequently, it is best used in combination with equivariant G-CNNs to improve the separability of learned invariant representations while exploiting the properties of equivariant learnable ﬁlters. In this survey, we showed how utilizing prior knowledge can enhance state-of-the-art deep learning systems and used 3D object detection as an example. Foremost, deﬁning problem-speciﬁc symmetry groups and introducing in- or equivariance to them can greatly improve the performance of neural networks, especially when the amount of training data is scarce. This has been proven in a broad variety of interesting applications from medical imaging to 3D shape recognition. Moreover, this approach improves the interpretability of neural network layers which is important for validation. We expect future work to investigate further generalizations to more complex input domains and symmetry groups as well as to more complicated, non-invertible transformations, e.g. occlusions. Moreover, DNNs incorporating multiple symmetries at once could be of interest.
Random phase approximation with exchange for an accurate description of crystalline polymorphism<|sep|>The RPA is becoming an important tool in materials science for calculating ground-state properties of solids. There are, however, some inherent limitations of the RPA which can cause signiﬁcant errors in certain systems or situations. In this work we have analyzed some of these cases and demonstrated how exact-exchange in the response function provides a theoretically well-deﬁned, accurate and reliable improvement. In general, the exchange term increases atomic polarizabilities leading to, e.g., improved vdW coeﬃcients. Here Table II. Lattice energies (meV) of ice XI, II, IX and VIII with the optimized hybrid functional HYBopt (35% exchange), the same hybrid functional with a TS-vdW correction, RPA, RPAx, SOSEX, CCSD(T) [82], QMC [91] and experiment [92]. The relative energies with respect to XI are also presented. For CCSD(T) and QMC XI should be replaced by Ih. XI (Cmc21) 614 676 551 609 634 601 615 610 II 559 671 542 609 639 601 613 609 IX 570 673 538 598 625 606 VIII 471 606 516 584 613 574 594 595 II—XI 55 5 9 0 -5 0 2 1 IX—XI 44 3 13 11 9 4 VIII—XI 143 70 35 25 21 27 21 15 we have shown that for the cohesive energy of purely vdW bonded solids, such as Ar and Kr, RPAx produces very accurate results, improving the RPA by more than 20%. vdW forces also play an important role in the SiO2 and BN polymorphs. We have shown that the interlayer binding energy of r-BN is enhanced with RPAx. However, with a smaller correction than for purely vdW bonded systems. The α-quartz-cristobalite energy diﬀerence is also enhanced with RPAx, and now agrees with reference QMC calculations. Earlier work have shown that the RPAx corrects the overestimated RPA correlation energy. As often pointed out, this error cancels to a large extent when looking at energy diﬀerences of similar systems. Which systems are similar enough is, however, not well-deﬁned. In this work we have highlighted that when the coordination number changes, as between high- and low-pressure phases, errors do not cancel to a satisfactory degree. This has been demonstrated for the r-BN-c-BN and the α-quartzstishovite energy diﬀerences. In the ﬁrst case, RPA and RPAx predict diﬀerent energy ordering, and in the second case, RPAx strongly enhances the energy diﬀerence. In both cases RPAx produces results in line with highly accurate methods such as CC or QMC. We have also studied the (H2O)2 potential energy surface and ice polymorphism. The errors of RPA (and hybrid functionals) in describing the lattice energies of ice IX, II, XI and VIII were analysed in terms of the polarizability of the water molecule and the 10 SPs. The RPA clearly underestimates both magnitude and anisotropy of the polarizability, but captures rather well the energy diﬀerences between diﬀerent H2O dimer conﬁgurations thanks to a good error cancellation. This implies under estimated lattice energies of ice but good relative energies. The RPAx, which includes higher order exchange effects, captures the correct strength of the hydrogen bond and is able to describe the full anisotropic interaction between H2O monomers. Not only relative energies but also lattice energies are, thereby, in very good agreement with experiment and more sophisticated methods. Regarding the computational cost and feasibility of the RPAx the main constraint is the N 3 k scaling, which can make k-point demanding systems several times more expensive than RPA. On the other hand, the convergence with respect to eigenvalues in the response function is in many cases faster with RPAx. Other parameters such as the total number of k-points or frequency points have a similar behaviour in RPA and RPAx. For the systems studied here, the cost of the RPAx calculation is 3-10 times that of the RPA calculation. All calculations were done non-self-consistently, on top of a PBE ground-state. For the water dimers and ice we showed that the use of hybrid orbitals, which can be considered as a step towards self-consistency, increases the energy diﬀerences by a few meV only. Further developments in this direction are, however, interesting for eventually calculating forces and lattice vibrations within the same framework. To conclude, we have shown that the RPAx opens up for highly accurate density functional calculations on solids that can address structural phase transitions involving distinct but nearly degenerate phases, and provide more reliable reference results than RPA when experimental data are missing or diﬃcult to access. [3] T. Bj¨orkman, A. Gulans, A. V. Krasheninnikov, and R. M. Nieminen, “van der waals bonding in layered compounds from advanced density-functional ﬁrst-principles [4] Michael J. Gillan, Dario Alf`e, and Angelos Michaelides, “Perspective: How good is DFT for water?” The Journal of Chemical Physics 144, 130901 (2016). [5] Henri Hay, Guillaume Ferlat, Michele Casula, Ari Paavo Seitsonen, and Francesco Mauri, “Dispersion eﬀects in sio2 polymorphs: An ab initio study,” Phys. Rev. B 92, 144111 (2015). [6] Huy-Viet Nguyen and Stefano de Gironcoli, “Eﬃcient calculation of exact exchange and RPA correlation energies in the adiabatic-connection ﬂuctuation-dissipation theory,” Phys. Rev. B 79, 205114 (2009). [8] Xinguo Ren, Patrick Rinke, Christian Joas, and Matthias Scheﬄer, “Random-phase approximation and its applications in computational chemistry and materials science,” Journal of Materials Science 47, 7447 (2012). [9] D.C. Langreth and J.P. Perdew, “The exchangecorrelation energy of a metallic surface,” Solid State Communications 17, 1425 – 1429 (1975). [10] Marie D. Str¨omsheim, Naveen Kumar, Sonia Coriani, Espen Sagvolden, Andrew M. Teale, and Trygve Helgaker, “Dispersion interactions in density-functional theory: An adiabatic-connection analysis,” The Journal of Chemical Physics 135, 194109 (2011). [11] Bing Xiao, Jianwei Sun, Adrienn Ruzsinszky, Jing Feng, and John P. Perdew, “Structural phase transitions in Si and SiO2 crystals via the random phase approximation,” Phys. Rev. B 86, 094109 (2012). [12] Niladri Sengupta, Jeﬀerson E. Bates, and Adrienn Ruzsinszky, “From semilocal density functionals to random phase approximation renormalized perturbation theory: A methodological assessment of structural phase transitions,” Phys. Rev. B 97, 235136 (2018). [13] Wuming Zhu, Julien Toulouse, Andreas Savin, and J´anos G. ´Angy´an, “Range-separated density-functional theory with random phase approximation applied to noncovalent intermolecular interactions,” The Journal of Chemical Physics 132, 244108 (2010). [14] Anant Dixit, Julien Claudot, S´ebastien Leb`egue, and Dario Rocca, “Improving the eﬃciency of beyond-RPA methods within the dielectric matrix formulation: Algorithms and applications to the a24 and s22 test sets,” Journal of Chemical Theory and Computation 13, 5432– 5442 (2017). [15] Maria Hellgren, Nicola Colonna, and Stefano de Gironcoli, “Beyond the random phase approximation with a local exchange vertex,” Phys. Rev. B 98, 045117 (2018). [16] Manfred Lein, E. K. U. Gross, and John P. Perdew, “Electron correlation energies from scaled exchangecorrelation kernels: Importance of spatial versus temporal nonlocality,” Phys. Rev. B 61, 13431–13437 (2000). [17] Miguel Marques, Angel Rubio, E. K. U. Gross, Kieron Burke, Fernando Nogueira, and Carsten A Ullrich, Timedependent density functional theory, Vol. 706 (Springer, Berlin, Heidelberg, 2006). [18] E. K. U. Gross and Walter Kohn, “Local densityfunctional theory of frequency-dependent linear response,” Phys. Rev. Lett. 55, 2850–2852 (1985). [20] Ulf von Barth, Nils Erik Dahlen, Robert van Leeuwen, and Gianluca Stefanucci, “Conserving approximations in time-dependent density functional theory,” Phys. Rev. B 72, 235109 (2005). [22] Thomas Olsen and Kristian S. Thygesen, “Extending the random-phase approximation for electronic correlation energies: The renormalized adiabatic local density approximation,” Phys. Rev. B 86, 081103(R) (2012). [23] Thomas Olsen and Kristian S. Thygesen, “Accurate ground-state energies of solids and molecules from timedependent density-functional theory,” Phys. Rev. Lett. 112, 203001 (2014). [24] Thomas Olsen, Christopher E Patrick, Jeﬀerson E Bates, Adrienn Ruzsinszky, and Kristian S Thygesen, “Beyond the RPA and GW methods with adiabatic xc-kernels for accurate ground state and quasiparticle energies,” npj Computational Materials 5, 1–23 (2019). [26] Andreas G¨orling, “Exact exchange-correlation kernel for dynamic response properties and excitation energies in density-functional theory,” Phys. Rev. A 57, 3433–3436 (1998). [27] Maria Hellgren and Ulf von Barth, “Linear density response function within the time-dependent exactexchange approximation,” Phys. Rev. B 78, 115107 (2008). [28] Maria Hellgren and Ulf von Barth, “Correlation energy functional and potential from time-dependent exactexchange theory,” The Journal of Chemical Physics 132, 044101 (2010). [30] David L. Freeman, “Coupled-cluster expansion applied to the electron gas: Inclusion of ring and exchange eﬀects,” Phys. Rev. B 15, 5512–5521 (1977). [31] Andreas Gr¨uneis, Martijn Marsman, Judith Harl, Laurids Schimka, and Georg Kresse, “Making the random phase approximation to electronic correlation accurate,” The Journal of Chemical Physics 131, 154115 (2009). [32] Georg Jansen, Ru-Fen Liu, and J´anos G. ´Angy´an, “On the equivalence of ring-coupled cluster and adiabatic connection ﬂuctuation-dissipation theorem random phase approximation correlation energy expressions,” The Journal of Chemical Physics 133, 154106 (2010). [33] Xinguo Ren, Patrick Rinke, Gustavo E. Scuseria, and Matthias Scheﬄer, “Renormalized second-order perturbation theory for the electron correlation energy: Concept, implementation, and benchmarks,” Phys. Rev. B 88, 035120 (2013). [34] Nicola Colonna, Maria Hellgren, and Stefano de Gironcoli, “Correlation energy within exact-exchange adiabatic connection ﬂuctuation-dissipation theory: Systematic development and simple approximations,” Phys. Rev. B 90, 125150 (2014). [35] Nicola Colonna, Maria Hellgren, and Stefano de Gironcoli, “Molecular bonding with the RPAx: From weak dispersion forces to strong correlation,” Phys. Rev. B 93, 195108 (2016). [36] Patrick Bleiziﬀer, Marcel Krug, and Andreas G¨orling, “Self-consistent Kohn-Sham method based on the adiabatic-connection ﬂuctuation-dissipation theorem and the exact-exchange kernel,” The Journal of Chemical Physics 142, 244108 (2015). [38] Thomas Gruber and Andreas Gr¨uneis, “Ab initio calculations of carbon and boron nitride allotropes and their structural phase transitions using periodic coupled cluster theory,” Phys. Rev. B 98, 134108 (2018). [39] Andrea Marini, P. Garc´ıa-Gonz´alez, and Angel Rubio, “First-principles description of correlation eﬀects in layered materials,” Phys. Rev. Lett. 96, 136404 (2006). [40] Bastien Mussard, Dario Rocca, Georg Jansen, and J´anos G. ´Angy´an, “Dielectric matrix formulation of correlation energies in the random phase approximation: Inclusion of exchange eﬀects,” Journal of Chemical Theory and Computation 12, 2191–2202 (2016). [41] Julien Toulouse, Iann C. Gerber, Georg Jansen, Andreas Savin, and J´anos G. ´Angy´an, “Adiabatic-connection ﬂuctuation-dissipation density-functional theory based on range separation,” Phys. Rev. Lett. 102, 096404 (2009). [42] Krzysztof Ro´sciszewski, Beate Paulus, Peter Fulde, and Hermann Stoll, “Ab initio calculation of ground-state properties of rare-gas crystals,” Phys. Rev. B 60, 7905– 7910 (1999). [46] Martin Schlipf and Francois Gygi, “Optimization algorithm for the generation of oncv pseudopotentials,” Computer Physics Communications 196, 36 – 44 (2015). [47] E. Zaremba and W. Kohn, “Van der waals interaction between an atom and a solid surface,” Phys. Rev. B 13, 2270–2285 (1976). [48] J.F Dobson, “Topics in condensed matter physics,” (Nova, 1994) Chap. Quasi-local-density approximation for a van der Waals energy functional, p. 121. [49] Julien Toulouse, Wuming Zhu, J´anos G. ´Angy´an, and Andreas Savin, “Range-separated density-functional theory with the random-phase approximation: Detailed formalism and illustrative applications,” Phys. Rev. A 82, 032502 (2010). [50] Tim Gould, “Communication: Beyond the random phase approximation on the cheap: Improved correlation energies with the eﬃcient radial exchange hole kernel,” The Journal of Chemical Physics 137, 111101 (2012). [51] Thomas Olsen and Kristian S. Thygesen, “Beyond the random phase approximation: Improved description of short-range correlation by a renormalized adiabatic local density approximation,” Phys. Rev. B 88, 115131 (2013). [52] Julien Toulouse, Elisa Rebolini, Tim Gould, John F. Dobson, Prasenjit Seal, and J´anos G. ´Angy´an, “Assessment of range-separated time-dependent densityfunctional theory for calculating c6 dispersion coeﬃcients,” The Journal of Chemical Physics 138, 194106 (2013). [53] Tim Gould, Julien Toulouse, J´anos G. ´Angy´an, and John F. Dobson, “Casimir?polder size consistency: A constraint violated by some dispersion theories,” Jour [54] Judith Harl and Georg Kresse, “Cohesive energy curves for noble gas solids calculated by adiabatic connection ﬂuctuation-dissipation theory,” Phys. Rev. B 77, 045136 (2008). [55] L. A. Schwalbe, R. K. Crawford, H. H. Chen, and R. A. Aziz, “Thermodynamic consistency of vapor pressure and calorimetric data for argon, krypton, and xenon,” The Journal of Chemical Physics 66, 4493–4502 (1977). [56] Tim Gould and Tomas Bucko, “C6 coeﬃcients and dipole polarizabilities for all atoms and many ions in rows 1?6 of the periodic table,” Journal of Chemical Theory and Computation 12, 3603–3613 (2016). [57] Andreas Heßelmann, “Chapter 3 - intermolecular interaction energies from kohn-sham random phase approximation correlation methods,” in Non-Covalent Interactions in Quantum Chemistry and Physics, edited by Alberto Otero de la Roza and Gino A. DiLabio (Elsevier, 2017) pp. 65–136. [58] Torbj¨orn Bj¨orkman, “Testing several recent van der waals density functionals for layered structures,” The Journal of Chemical Physics 141, 074708 (2014). [59] W. Paszkowicz, J. B. Pelka, M. Knapp, T. Szyszko, and S. Podsiadlo, “Lattice parameters and anisotropic thermal expansion of hexagonal boron nitride in the 10-297.5 k temperature range,” Applied Physics A 75, 431 (2002). [60] V. L. Solozhenko, “The thermodynamic aspect of boron nitride polymorphism and the bn phase diagram,” High Pressure Research 7, 201–203 (1991). [62] Jeﬀerson E. Bates, Savio Laricchia, and Adrienn Ruzsinszky, “Nonlocal energy-optimized kernel: Recovering second-order exchange in the homogeneous electron gas,” Phys. Rev. B 93, 045119 (2016). [63] K. P. Driver, R. E. Cohen, Zhigang Wu, B. Militzer, P. Lopez Rios, M. D. Towler, R. J. Needs, and J. W. Wilkins, “Quantum monte carlo computations of phase stability, equations of state, and elasticity of highpressure silica,” Proceedings of the National Academy of Sciences 107, 9519 (2010). [64] P. Richet, Y. Bottinga, L. Denielou, J.P. Petitet, and C. Tequi, “Thermodynamic properties of quartz, cristobalite and amorphous SiO2: drop calorimetry measurements between 1000 and 1800 k and a review from 0 to 2000 k,” Geochimica et Cosmochimica Acta 46, 2639 – 2658 (1982). [65] Patrick M. Piccione, Christel Laberty, Sanyuan Yang, Miguel A. Camblor, Alexandra Navrotsky, and Mark E. Davis, “Thermochemistry of pure-silica zeolites,” The Journal of Physical Chemistry B 104, 10001–10011 (2000). [66] Guillaume Ferlat, Maria Hellgren, Francois-Xavier Coudert, Henri Hay, Francesco Mauri, and Michele Casula, “van der waals forces stabilize low-energy polymorphism in b2o3: Implications for the crystallization anomaly,” Phys. Rev. Materials 3, 063603 (2019). [68] J.L Holm, O.J Kleppa, and Edgar F Westrum, “Thermodynamics of polymorphic transformations in silica. thermal properties from 5 to 1070 k and pressure-temperature [69] Masaki Akaogi and Alexandra Navrotsky, “The quartzcoesite-stishovite transformations: new calorimetric measurements and calculation of phase diagrams,” Physics of the Earth and Planetary Interiors 36, 124 (1984). [70] Alexandre Tkatchenko and Matthias Scheﬄer, “Accurate molecular van der waals interactions from ground-state electron density and free-atom reference data,” Phys. Rev. Lett. 102, 073005 (2009). [71] Kevin F. Garrity, Joseph W. Bennett, Karin M. Rabe, and David Vanderbilt, “Pseudopotentials for highthroughput DFT calculations,” Computational Materials Science 81, 446 (2014). [72] Gregory S. Tschumper, Matthew L. Leininger, Brian C. Hoﬀman, Edward F. Valeev, Henry F. Schaefer, and Martin Quack, “Anchoring the water dimer potential energy surface with explicitly correlated computations and focal point analyses,” The Journal of Chemical Physics 116 (2002). [73] Dennis M. Elking, Lalith Perera, Robert Duke, Thomas Darden, and Lee G. Pedersen, “A ﬁnite ﬁeld method for calculating molecular polarizability tensors for arbitrary multipole rank,” Journal of Computational Chemistry 32, 3283–3295 (2011). [74] Oleksandr Loboda, Francesca Ingrosso, Manuel F. RuizL`opez, Heribert Reis, and Claude Millot, “Dipole and quadrupole polarizabilities of the water molecule as a function of geometry,” Journal of Computational Chemistry 37, 2125–2132 (2016). [75] I-Chun Lin, Ari P. Seitsonen, Maur´ıcio D. CoutinhoNeto, Ivano Tavernelli, and Ursula Rothlisberger, “Importance of van der waals interactions in liquid water,” The Journal of Physical Chemistry B 113, 1127–1131 (2009). [76] Romain Jonchiere, Ari P. Seitsonen, Guillaume Ferlat, A. Marco Saitta, and Rodolphe Vuilleumier, “Van der waals eﬀects in ab initio water at ambient and supercritical conditions,” The Journal of Chemical Physics 135, 154503 (2011). [77] Biswajit Santra, J`ıˇr`ı Klimeˇs, Dario Alf`e, Alexandre Tkatchenko, Ben Slater, Angelos Michaelides, Roberto Car, and Matthias Scheﬄer, “Hydrogen bonds and van der waals forces in ice at ambient and high pressures,” Phys. Rev. Lett. 107, 185701 (2011). [78] Biswajit Santra, J`ıˇr`ı Klimeˇs, Alexandre Tkatchenko, Dario Alf`e, Ben Slater, Angelos Michaelides, Roberto Car, and Matthias Scheﬄer, “On the accuracy of van der waals inclusive density-functional theory exchangecorrelation functionals for ice at ambient and high pressures,” The Journal of Chemical Physics 139, 154702 (2013). [79] Jianwei Sun, Adrienn Ruzsinszky, and John P. Perdew, “Strongly constrained and appropriately normed semilocal density functional,” Phys. Rev. Lett. 115, 036402 (2015). [80] Jianwei Sun, Richard C Remsing, Yubo Zhang, Zhaoru Sun, Adrienn Ruzsinszky, Haowei Peng, Zenghui Yang, Arpita Paul, Umesh Waghmare, Xifan Wu, et al., “Accurate ﬁrst-principles structures and energies of diversely bonded systems from an eﬃcient density functional,” Nature chemistry 8, 831 (2016). [82] M. J. Gillan, D. Alf`e, P. J. Bygrave, C. R. Taylor, and F. R. Manby, “Energy benchmarks for water clusters and ice structures from an embedded many-body expansion,” The Journal of Chemical Physics 139, 114101 (2013). [83] S. J. A. van Gisbergen, F. Kootstra, P. R. T. Schipper, O. V. Gritsenko, J. G. Snijders, and E. J. Baerends, “Density-functional-theory response-property calculations with accurate exchange-correlation potentials,” Phys. Rev. A 57, 2556–2571 (1998). [84] Maria Hellgren, Lucas Baguet, Matteo Calandra, Francesco Mauri, and Ludger Wirtz, “Electronic structure of tise2 from a quasi-self-consistent G0W0 approach,” Phys. Rev. B 103, 075101 (2021). [85] Ngoc Linh Nguyen, Nicola Colonna, and Stefano de Gironcoli, “Ab initio self-consistent total-energy calculations within the exx/rpa formalism,” Phys. Rev. B 90, 045138 (2014). [86] Brian J. Smith, David J. Swanton, John A. Pople, Henry F. Schaefer, and Leo Radom, “Transition structures for the interchange of hydrogen atoms within the water dimer,” The Journal of Chemical Physics 92, 1240– 1247 (1990). [87] Biswajit Santra, Angelos Michaelides, and Matthias Scheﬄer, “Coupled cluster benchmarks of water monomers and dimers extracted from density-functional theory liquid water: The importance of monomer deformations,” The Journal of Chemical Physics 131, 124509 (2009). [88] M. J. Gillan, F. R. Manby, M. D. Towler, and D. Alf`e, “Assessing the accuracy of quantum monte carlo and density functional theory for energetics of small water clusters,” The Journal of Chemical Physics 136, 244105 (2012). [90] Tomas K. Hirsch and Lars Ojamae, “Quantum-chemical and force-ﬁeld investigations of ice ih: Computation of proton-ordered structures and prediction of their lattice energies,” The Journal of Physical Chemistry B 108, 15856–15864 (2004). [91] Andrea Zen, Jan Gerit Brandenburg, Jiri Klimes, Alexandre Tkatchenko, Dario Alfe, and Angelos Michaelides, “Fast and accurate quantum monte carlo for molecular crystals,” Proceedings of the National Academy of Sciences 115, 1724–1729 (2018). [92] E. Whalley, “Energies of the phases of ice at zero temperature and pressure,” The Journal of Chemical Physics 81, 4087 (1984). [93] A. J. Leadbetter, R. C. Ward, J. W. Clark, P. A. Tucker, T. Matsuo, and H. Suga, “The equilibrium lowtemperature structure of ice,” The Journal of Chemical Physics 82, 424 (1985). [94] Ph. Pruzan, J. C. Chervin, and B. Canny, “Stability domain of the ice viii proton-ordered phase at very high pressure and low temperature,” The Journal of Chemical Physics 99, 9842–9846 (1993). [95] M. Song, H. Yamawaki, H. Fujihisa, M. Sakashita, and K. Aoki, “Infrared investigation on ice viii and the phase diagram of dense ices,” Phys. Rev. B 68, 014106 (2003). The authors would like to thank Lorenzo Paulatto, Guillaume Ferlat and Michele Casula for discussions. The work was performed using HPC resources from GENCI-TGCC/CINES/IDRIS (Grant No. A0050907625). Financial support from Emergence-Ville de Paris is acknowledged.
Criticality of Large Delay Tolerant Networks via Directed Continuum Percolation in Space-Time<|sep|>DTN is designed to operate in settings where connectivity is intermittent at best. In this paper, we have analyzed necessary conditions for DTN type communication. To this end, we assumed an elementary mobility model where nodes join the network for a constant time t and then depart. During the sojourn time, they move a distance ℓ to a random direction. We assume a large network, where the ability to sustain a message indeﬁnitely means that the message will eventually also reach its ﬁnal destination(s). The stochastic model of DTN is amenable to analysis by means of the percolation theory. In particular, we studied the so-called directed continuum percolation in space-time, where the objects are cylinders with height t and the diameter equal to the transmission range d sheared (tilted) around the time axis by an amount corresponding to the movement ℓ. We showed that the critical percolation threshold in terms of the critical reduced number density ηc depends only on the tilt ratio γ = ℓ/d, ηc = ηc(γ), where ℓ denotes the distance nodes move and d is the transmission range. Numerical values for ηc(γ) were obtained by means of Monte Carlo simulations, where universal exponents of the undirected model were utilized. Some evidence to support the hypothesis that the universal exponents are the same for the directed model was also given. Moreover, the asymptotic behavior of ηc(γ) when γ tends to inﬁnity was derived. In terms of the mean node degree ν, our main result states that ν > 4 ηc(γ) in order for a large DTN network to be operational. A general observation is that increased mobility improves the transport capacity of DTN and allows a lower node density, as expected. The elementary mobility model, as well as, the symmetric Gilbert’s disc communication model, represent, in some sense, a strict lower bound for more realistic scenarios. That is, if, e.g., the sojourn time t or the distance ℓ were random variables, or, if directional antennas were used, then the shapes and the orientation of the objects would vary more, increasing “disorder” and yielding a lower critical percolation threshold. Such studies are left as a future work.
Spatial Interactions of Peers and Performance of File Sharing Systems<|sep|>The following general law quantifying P2P super-scalability was identiﬁed: in a P2P system with rate function g and range R, according to our model, the stationary latency is of the form with γ = 2π � R 0 g(r)dr and with M(x) a function which is larger than 1 and tends to 1 when x tends to inﬁnity (if there is no range, (59) can still be used with the typical range ˜R deﬁned in VII). Both in the TCP case, i.e. for g(r) = C r , and in the UDP case, i.e. for g(r) = C, the function x → M(x) is decreasing (and has an explicit approximation). With a decreasing M, Equation (59) exhibits two causes of super-scalability. First, there is the 1 √ F λγ . This is the same type of super–scalability that was observed in the toy example. But there is also a super-scalability that comes from M, which expresses the surprising fact that increasing the arrival rate reduces the slow-down due to the repulsion phenomenon identiﬁed in the paper. For Nf large enough, the main cause of scalability is Wf, but otherwise, the effect of M on super– scalability is not to be neglected. The conditions for the super-scalability formula (59) to hold were also identiﬁed: First, the network should have the capacity to cope with the P2P trafﬁc. This translates into the requirement
MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning<|sep|>In this paper, we have introduced the open-source Python library MIScnn: A framework for medical image segmentation with convolutional neural networks and deep learning. The intuitive API allows fast building medical image segmentation pipelines including data I/O, preprocessing, data augmentation, patch-wise analysis, metrics, a library with state-of-the-art deep learning models and model utilization like training, prediction, as well as fully automatic evaluation (e.g. crossvalidation). High configurability and multiple open interfaces allow users to fully customize the pipeline. This framework enables researchers to rapidly set up a complete medical image segmentation pipeline by using just a few lines of code. We proved the MIScnn functionality by running an automatic cross-validation on the Kidney Tumor Segmentation Challenge 2019 CT data set resulting into a powerful predictor. We hope that it will help migrating medical image segmentation from the research labs into practical applications. We want to thank Bernhard Bauer and Fabian Rabe for sharing their GPU hardware (2x Nvidia Quadro P6000) with us which was used for this work. We also want to thank Dennis Klonnek, Florian Auer, Barbara Forastefano, Edmund Müller and Iñaki Soto Rey for their useful comments.
Chance-Constrained Two-Stage Unit Commitment under Uncertain Load and Wind Power Output Using Bilinear Benders Decomposition<|sep|>commitment (UC) model considering the uncertainty of load  demand and wind power output. In this study, chance constraint guarantees the power balance being satisfied with a predefined high probability. Then, this UC model is reformulated  as the bilinear mixed integer programming problem and then  linearized by McCormick linearization method. Finally, to deal  with the large number of random scenarios, a bilinear variant  of Benders decomposition algorithm is developed to achieve  fast computation for chance-constrained UC problems. widely adopted Big-M based formulation. In particular, the bilinear Benders decomposition algorithm is an exact algorithm  without any concern of Big-M issues and generally performs  an order of magnitude faster than using a professional solver to  directly compute both linear and bilinear chance-constrained  UC models in terms of CPU time. In the future, more efforts  would be made to develop appropriate enhancement techniques  of bilinear Benders decomposition algorithm for further improving its computational efficiency.
Optimally fuzzy temporal memory<|sep|>Signals with long-range temporal correlations are found throughout the natural world. Such signals present a distinct challenge to machine learners that rely on a shift-register representation of the time series. Here we have described a method for constructing a self-suﬃcient scale-free representation of temporal history. The nodes are chosen in a way that minimizes information redundancy and information loss while equally distributing them over all time scales. Although the temporal accuracy of the signal is sacriﬁced, predictively relevant information from exponentially long timescales is available in the fuzzy memory system when compared to a shift register with the same number of nodes. This could be an extremely useful way to represent time series with long-range correlations for use in machine learning applications. The information redundancy in the memory representation can be quantiﬁed by deriving expressions for mutual information shared between neighboring nodes. When the input signal is uncorrelated or has scale-free long range correlations, it will be shown that the information redundancy is equally spread over all nodes only when the ∗τ values of the nodes are given by eq. 16. Taking f(τ) to be a stochastic signal and the current moment to be τ = 0, the activity of a ∗τ node in the T column is (see eq. 15) The expectation value of this node can be calculated by simply averaging over f(τ ′) inside the integral, which should be a constant if it is generated by a stationary process. By deﬁning z = τ ′/ ∗τ, we ﬁnd that the expectation of T is proportional to the expectation of f.
The unusually large population of Blazhko variables in the globular cluster NGC 5024 (M53)<|sep|>Precise, dense, and long time-span time-series photometry is required to detect amplitude and phase modulations in RR Lyrae variables. If this is achieved, then one may ﬁnd that the Blazhko effect is more common than generally realized, particularly among the ﬁrst overtone pulsators or RRc stars. The signiﬁcant increase in the detections of Blazhko variables in NGC 5024 is mainly due to the high quality of the CCD photometry achieved using the DIA technique which enables one to detect small amplitude and phase modulations. The accuracy of our V photometry at the brightness of the RR Lyrae stars is about 0.008 mag per data point, which is, in most of the conﬁrmed Blazhko variables, signiﬁcantly smaller than the amplitude of the modulations in the light curves. Our data reveal that amplitude and phase modulations are present in the majority of the light curves of the RRc stars (23 of 31) and in about half of the RRab stars (11 of 24) in NGC 5024, i.e. 66% and 37% respectively. We also ﬁnd a lower limit of 52% on the overall incidence rate of the Blazhko effect among the RR Figure 9. Phase and maximum brightness modulations in for three selected RRab stars, V11, V57 and V60 and ﬁve RRc stars V17, V18, V23, V44, V62. Table 6. Phase and magnitude at the time of maximum brightness in the V -band for a group of stars with Blazhko modulations. The phase in column 2 was calculated using the ephemerides given in Table 2. This is an extract from the full table, which is available with the electronic version of the article (see Supporting Information). V2 +0.000 16.656 4940.34212 V2 -0.039 16.690 5249.24358 ... ... ... ... V4 +0.000 16.681 4939.27817 V4 -0.004 16.688 4941.20439 ... ... ... ... V11 +0.014 16.181 4940.20614 V11 +0.004 16.243 4941.45969 ... ... ... ... It is likely times of maximum O-C residuals observed in Blazhko variables are caused by the phase modulations. Given the short time base of our observations we cannot say whether or not these O-C differences are at all connected with secular variations in the pulsation period. The RRab and RRc stars are cleanly separated in the CMD which is the expected situation if the evolution of the RR Lyrae stars is progressing towards the red. In the prevailing hysteresis paradigm, the current pulsation mode of an RR Lyrae star is deﬁned by its previous evolutionary history (e.g., van Albada & Baker 1973; Caputo et al. 1978). As schematically shown in Fig. 3 of Caputo et al. (1978), one only expects to ﬁnd a clean separation between RRab and RRc stars if horizontal branch evolution is pre dominantly towards the red in the instability strip. At the same time, the color distribution of the Blazhko RRc variables is redder than that of the stable counterparts, and populate the hysteresis region where ﬁrst overtone and fundamental pulsators may coexist. This may suggest that the Blazhko effect manifests in stars undergoing a pulsation mode change. Our limited time coverage of the Blazhko cycle prevented us from estimating the modulation periods. Despite of this the light curve modulations could be detected with conﬁdence. A continued monitoring of NGC 5024 should prove useful in characterizing the modulation properties of the conﬁrmed and suspected Blazhko stars, in particular the determination of the modulation frequencies. The large sample of Blazhko variables in NGC 5024 will eventually be used to conﬁrm the existance of the correlation between the main pulsation frequency f0 and the modulation frequency fm, sketched
Probabilistic aspects of ZM-groups<|sep|>The probabilistic aspects of ﬁnite groups theory constitute a fruitful research topic and the connections between them may establish new relevant results. We end our paper by indicating the following three open problems. Problem 5.1. Consider the sets {sd(G) | G = finite group} and {csd(G) | G = finite group}. Study their density in [0, 1]. Problem 5.2. For a ﬁxed α ∈ (0, 1), describe the structure of ﬁnite groups G satisfying sd(G) = (⩽, ⩾)α or csd(G) = (⩽, ⩾)α.
The Dynamical State of The Serpens South Filamentary Infrared Dark Cloud<|sep|>1. From the Herschel and 1.1 mm dust continuum data, we identiﬁed three dense clumps along the main ﬁlament: northern (V-shap), central (cluster-forming), and southern clumps. The clump masses are estimated to be around 40 − 230 M⊙. 2. Applying a hyperﬁne ﬁtting to the N2H+ (J = 1 − 0) data, we derived the spatial distributions of the N2H+ column density, excitation temperature, fractional abundance, optical depth, and line width in Serpens South. We found that the cluster-forming clump has larger line widths of about 1 km s−1, whereas the regions with no signs of active star formation have smaller line widths of about 0.6 km s−1. The fractional abundance of N2H+ is estimated to be XN2H+ ≃ 2.5×10−10 over the whole area and tends to decrease with increasing column density. We interpret that the overall dependence of XN2H+ on the column density reﬂects the ionization degree in the dense molecular gas, which is determined by the balance between the cosmic-ray ionization and the recombination. 3. Applying the virial analysis, we found that all the three clumps have small virial ratios of 0.1 − 0.3. This indicates that the internal turbulent motions play only a minor role in the clump support. The northern clump has a mass and radius comparable to the central cluster-forming clump, although there is no sign of active cluster formation. Thus, it is a likely candidate of pre-protocluster clump, where active cluster formation will be in the future. 4. Although the clumps show the observational signs of the global infall motions, the infall speed appears to be much slower than the free-fall velocities of a few km s−1. We propose that the slow global infall is due to the fact that the Serpens South ﬁlament is supported by the large-scale ordered magnetic ﬁeld that was discovered by Sugitani et al. (2011). According to our estimation, the parent cloud was magnetically critical and either the ambipolar diﬀusion or mass accretion along the magnetic ﬁeld lines may have triggerd the global infall and the active cluster formation that is observed in the central clump. 5. The physical properties of the pre-protocluster clump are (1) the column density higher than 1023 cm−2, (2) a lower temperature compared to the ambient gas, (3) a small internal velocity dispersion, (4) a small virial ratio, and (5) the magnetic ﬁeld that is close to the critical. These properties contradict the accretion-driven turbulence scenario, which predicts that in the pre-protocluster clump, the turbulent energy should balance with the gravitational energy due to the momentum injection driven by the global accretion ﬂow. 6. Applying the clumpﬁnd to the 3D data cube of the N2H+ emission taken with the Nobeyama 45-m telescope, we identiﬁed 70 dense cores. The cores with larger LTE masses tend to have smaller virial ratios. The majority of the cores have virial ratios smaller than 2, indicating that for most of the cores, the total energy (the gravitational plus internal kinetic energies) is negative. Therefore, the self-gravity appears to be important for most of the cores. A caveat is that the physical properties of the identiﬁed cores depend strongly on the telescope beam size because the substructures smaller than the telescope beam size (≈ 20′′, corresponding to 0.04 pc) cannot be resolved. Higher spatial resolution and higher sensitivity observations will be necessary to uncover the basic units of star formation. This work is supported in part by a Grant-in-Aid for Scientiﬁc Research of Japan (20540228). We thank Thushara Pillai, Jens Kauﬀmann, Alvaro Hacar, Philippe Andr´e, and Huei-Ru Chen for valuable comments and suggestions. We are grateful to the staﬀs at the Nobeyama Radio Observatory (NRO) for both operating the 45-m and helping us with the data reduction. NRO is a branch of the National Astronomical Observatory, National Institutes of Natural Sciences, Japan. We also thank the staﬀs at the Caltech Submillimeter Observatory for giving us an opportunity to use the telescope.
Van der Waals-like phase transition from holographic entanglement entropy in Lorentz breaking massive gravity<|sep|>In this paper, we have investigated the VDP phase transition with the use of HEE as a probe. Firstly, we investigated the phase diagrams of the BHE in the T − S phase plane and ﬁnd that the phase structure depends on the scalar charge Q and the parameter γ of the AdS black holes in this massive gravity. For the case that γ = 1 or Q = 0, we ﬁnd there always exists the Hawking-Page like phase transition in this thermodynamic system. While for the case γ = −1, we found for the small scalar charge Q, there is always an unstable black hole thermodynamic system interpolating between the small stable black hole system and large stable black hole system. The thermodynamic transition for the small hole to the large hole is ﬁrst order transition and the Maxwell’s equal area law is valid. As the scalar charge Q increases to the critical value Qc in this space time, the unstable black hole merges into an inﬂection point. We found there is a SPT at the critical point. When the scalar charge is larger than the critical charge, the black hole is stable always. That is to say, we found that there exists the VDP gas-ﬂuid phase transition in the T − S phase plane of the AdS black hole in massive gravity. The more interesting thing is that we found the HEE also exhibits the VDP phase structure in the T − δS plane when γ = −1 and the scalar charge Q ̸= 0. In order to conﬁrm this results, we further showed that the Maxwell’s equal area law is satisﬁed and the critical exponent of the speciﬁc heat capacity is consistent with that of the mean ﬁeld theory of the VDP gas-ﬂuid system for the HEE system. These results show that the phase structure of HEE is similar to that of BHE and the HEE is really a good probe to the phase transition of AdS black holes in Lorentz breaking massive gravity. This also implies that HEE and BHE exhibit some potential underlying relationship. We are grateful to De-Fu Hou and Hong-Bao Zhang for their instructive discussions. This work is supported by the National Natural Science Foundation of China (Grant No.11365008, Grant No.11465007, Grant No.61364030).
On the frequency correlations of low-frequency QPOs with kilohertz QPOs in accreting millisecond X-ray pulsars<|sep|>We study the correlations of the centroid frequencies of the low frequency (LF) and hump (h) QPOs with that of the kHz QPO in a sample of 6 AMXPs (SAX J1808, XTE J1807, HETE J1900, IGRJ 17480, IGR J17511 and SAX J1748), a burst oscillation source (SAX J1810.8) and a source with unknown spin (4U 1820–30) as observed with RXTE and compare them to those in non-pulsating atoll sources as reported in DK17. For two AMXPs (IGR J17511 and SAX J1748) the relevant QPOs did not occur simultaneously. The correlations in SAX J1808 and XTE J1807 have signiﬁcantly lower power law indices than in the non-pulsating sources, and are compatible with the relativistic precession model (RPM) prediction of 2.0. The QPO frequencies are ∼4 times higher than predicted by the RPM for NSs with realistic equations of state, however, and we do not ﬁnd evidence that the best-ﬁt normalizations depend on spin frequency. We can interpret the frequencies as being due to solidbody like precession of a toroidal hot inner ﬂow, where an
Exploration of dynamical regimes of irradiated small protonated water clusters<|sep|>We have explored in this paper the dynamical response of small water clusters (in particular (H2O)H3O+) to a short and intense laser pulse for various frequencies and intensities covering the regime of stable vibrations up to Coulomb break-up of a highly ionized cluster. The ﬁrst stages of response of the system are, as is well known from
Sensitive survey for 13CO, CN, H2CO, and SO in the disks of T Tauri and Herbig Ae stars II: Stars in $\rho$ Oph and upper Scorpius<|sep|>We have performed a sensitive survey of 29 young stars in the ρ Oph and upper Scorpius regions in CN, ortho-H2CO, SO, 13CO, and C17O rotational lines near 206 - 228 GHz with the IRAM 30 m telescope. Compared to a similar study performed for the isolated star formation region Taurus-Auriga, the detection rate of CN is much lower in the ρ Oph region. This result may indicate that disks in the ρ Oph region are on average smaller than those in the Taurus-Auriga complex, perhaps because they have not spread out suﬃciently by viscous diﬀusion, or as a result of initial truncation of their parental protostellar condensation. However, the CN chemistry is shown to be sensitive to disk temperature, so that a direct comparison of the disk properties would require resolving the disks out instead of relying on the good correlation between CN ﬂux and disk radii previously found in the Taurus region. Interferometric observations are required for this purpose. Finally, these results show that even large single-dish telescopes are severely limited in identifying disks around embedded young stars because of contamination, but also because of sensitivity. Disks smaller than about 300 au, which represent 50 % of the disks in the sample studied by Guilloteau et al. (2013) in the Taurus region, are beyond the sensitivity limit of even the largest telescope operating at 1.3 mm, the IRAM 30 m. Only ALMA is sensitive enough in the southern hemisphere to detect a substantial fraction of the gas disk population. Acknowledgements. We acknowledge all the 30 m IRAM staﬀ for their help during the observations. This work was supported by “Programme National de Physique Stellaire” (PNPS) and “Programme National de Physique Chimie du Milieu Interstellaire” (PCMI) from INSU/CNRS. The work of MS was supported in part by NSF grant AST 09-07745. This research has made use of the SIMBAD database, operated at CDS, Strasbourg, France. VW’s research work is funded by the ERC Starting Grant (3DICE, grant agreement 336474).
Quantum error correction with only two extra qubits<|sep|>In order to get a sense for the practicality of the twoqubit error-correction schemes, we simulate error correction using a standard depolarizing noise error model on the one- and two-qubit operations [18].1 Figure 6 shows the results from simulating at least 106 consecutive errorcorrection rounds for each value of the CNOT failure rate p. Observe that for the [[15, 7, 3]] code, Steane-style error correction, which extracts all stabilizer syndromes at once, performs better than either the Shor-style or twoqubit procedures. For the [[5, 1, 3]] and [[7, 1, 3]] codes, the diﬀerent error-correction methods are all very close. Note that we do not introduce memory errors on resting qubits, and nor do we add errors for moving qubits into position to apply the gates; we leave more detailed simulations and optimizations to future work. We have focused on extracting syndromes using two ancilla qubits in order to minimize the qubit overhead. With just one more qubit, however, fault-tolerant syndrome extraction becomes considerably simpler. Consider for example the circuit in Fig. 7 for extracting a ZZZZ syndrome.
Trusted Multi-View Classification<|sep|>In this work, we propose a novel trusted multi-view classiﬁcation (TMC) algorithm which, based on the Dempster-Shafer evidence theory, can produce trusted classiﬁcation decisions on multi-view data. Our algorithm focuses on decision-making by fusing the uncertainty of multiple views, which is essential for making trusted decisions. The TMC model can accurately identify the views which are risky for decision making, and exploits informative views in the ﬁnal decision. Furthermore, our model can produce the uncertainty of a current decision while making the ﬁnal classiﬁcation, providing intepretability. The empirical results validate the effectiveness of the proposed algorithm in classiﬁcation accuracy and out-of-distribution identiﬁcation. This work was supported in part by National Natural Science Foundation of China (No. 61976151, No. 61732011), and the Natural Science Foundation of Tianjin of China (No. 19JCYBJC15200).
A detailed view of the gas shell around R Sculptoris with ALMA<|sep|>We present the full set of ALMA Cycle 0 observations of 12CO towards the carbon AGB star R Scl. The observations clearly resolve the shell around R Scl and reveal a spiral structure
The Hagedorn temperature Revisited<|sep|>In this paper we have made a new analysis of the number of hadronic resonances using the latest information from the Particle Data Group 12. This leads to a tem
Deciphering a novel image cipher based on mixed transformed Logistic maps<|sep|>This paper studied the security of an image cipher based on a variant of Logistic map. Observing its essential structure, we found that the previous chosen-plaintext attack can be further improved in terms of reducing the number of chosen plain-images from eight-seven to two and decreasing the computational complexity a little. Beside this, an eﬀective known-plaintext attack can break the cipher in the sense that only two known plain-image are needed. In addition, some other security ﬂaws, insensitivity to change of plaintext/secret, weak randomness of used PRNG, were identiﬁed and brieﬂy discussed. This research was supported by the Distinguished Young Scholar Program, Hunan Provincial Natural Science Foundation of China (No. 2015JJ1013), the Natural Science Foundation of China (No. 61202398). References ´Alvarez, G. & Li, S. [2006] “Some basic cryptographic requirements for chaos-based cryptosystems,” International Journal of Bifurcation and Chaos 16, 2129–2151. ´Alvarez, G., Montoya, F., Romera, M. & Pastor, G. [2004] “Cryptanalysis of dynamic look-up table based chaotic cryptosystems,” Physics Letters A 326, 211–218. Baptista, M. [1998] “Cryptography with chaos,” Physics Letters A 240, 50–54. Bose, R. [2005] “Novel public key encryption technique based on multiple chaotic systems,” Physical Review Letters 95, art. no. 098702. Chen, G., Mao, Y. & Chui, C. K. [2004] “A symmetric image encryption scheme based on 3D chaotic cat maps,” Chaos, Solitons & Fractals 21, 749–761. Chen, Y., Liao, X. & Wong, K.-W. [2006] “Chosen plaintext attack on a cryptosystem with discretized skew tent map,” IEEE Transactions on Circuits and Systems II: Express Briefs 53, 527–529. Jakimoski, G. & Kocarev, L. [2001] “Chaos and cryptography: block encryption ciphers based on chaotic maps,” IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications 48, 163–169. Li, C., Li, S., ´Alvarez, G., Chen, G. & Lo, K.-T. [2007] “Cryptanalysis of two chaotic encryption schemes based on circular bit shift and XOR operations,” Physics Letters A 369, 23–30. Li, C., Liu, Y., Xie, T. & Chen, M. Z. Q. [2013a] “Breaking a novel image encryption scheme based on improved hyperchaotic sequences,” Nonlinear Dynamics 73, 2083–2089, doi:10.1007/s11071-013-0924-6. Li, C., Liu, Y., Zhang, L. Y. & Chen, M. Z. Q. [2013b] “Breaking a chaotic image encryption algorithm based on modulo addition and XOR operation,” International Journal of Bifurcation and Chaos 23, Article number 1350075, doi:10.1142/S0218127413500752. Li, C., Xie, T., Liu, Q. & Cheng, G. [2014] “Cryptanalyzing image encryption using chaotic logistic map,” Nonlinear Dynamics 78, 1545–1551. Li, C.-Y., Chen, Y.-H., Chang, T.-Y., Deng, L.-Y. & To, K. [2012] “Period extension and randomness enhancement using high-throughput reseeding-mixing PRNG,” IEEE Transactions on Very Large Scale Integration (VLSI) Systems 20, 385–389. Li, J., Li, X., Yang, B. & Sun, X. [2015] “Segmentation-based image copy-move forgery detection scheme,” IEEE Transactions on Information Forensics and Security 10, 507–518. Li, S., Li, C., Chen, G. & Lo, K.-T. [2008] “Cryptanalysis of the RCES/RSES image encryption scheme,” Journal of Systems and Software 81, 1130–1143. Mao, Y., Chen, G. & Lian, S. [2004] “A novel fast image encryption scheme based on 3D chaotic baker maps,” International Journal of Bifurcation and Chaos 14, 3613–3624. MAY, R. M. [1976] “Simple mathematical-models with very complicated dynamics,” Nature 261, 459–467. Persohn, K. & Povinelli, R. [2012] “Analyzing logistic map pseudorandom number generators for periodicity induced by ﬁnite precision ﬂoating-point representation,” Chaos Solitons & Fractals 45, 238–245. Rukhin, A. & et al. [2010] “A statistical test suite for random and pseudorandom number generators for cryptographic applications,” NIST Special Publication 800-22rev1a, available online at http://csrc. nist.gov/groups/ST/toolkit/rng/documentation software.html. Sam, I. S., Devaraj, P. & Bhuvaneswaran, R. S. [2012] “A novel image cipher based on mixed transformed logistic maps,” Multimedia Tools and Applications 56, 315–330. Schneier, B. [2007] Applied cryptography: protocols, algorithms, and source code in C (John Wiley & Sons). Solak, E., C¸okal, C., Yildiz, O. T. & Biyiko˘glu, T. [2010] “Cryptanalysis of fridrich’s chaotic image encryption,” International Journal of Bifurcation and Chaos 20, 1405–1413. Ulam, S. M. & von Neumann, J. [1947] “On combination of stochastic and deterministic processes,” Bulletin of the American Mathematical Society 53, 1120. Zhang, Y., Xiao, D., Wen, W. & Li, M. [2014] “Cryptanalyzing a novel image cipher based on mixed transformed logistic maps,” Multimedia Tools and Applications 73, 1885–1896. Zheng, H., Yu, S. & Lu, J. [2014] “Design and ARM platform-based realization of digital color image encryption and decryption via single state variable feedback control,” International Journal of Bifurcation and Chaos 24, art. no. 1450049. Zhou, Y., Hua, Z., Pun, C.-M. & Chen, C. L. P. [2015] “Cascade chaotic system with applications,” IEEE Transactions on Cybernetics 45, 2001–2012, doi:10.1109/TCYB.2014.2363168. Zhu, K., Li, C., Asari, V. & Saupe, D. [2015] “No-reference video quality assessment based on artifact measurement and statistical analysis,” IEEE Transactions on Circuits and Systems for Video Technology 25, 533–546.
Comparative study of the discrete velocity and lattice Boltzmann methods for rarefied gas flows through irregular channels<|sep|>In summary, we provided a systematic assessment of the accuracy of the MRT-LBM method in simulating rareﬁed gas ﬂows in rough microchannels and complex porous media, over a wide range of the Knudsen number. By solving the linearized Bhatnagar-Gross-Krook kinetic equation accurately via the discrete velocity method, we have shown that:
Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models<|sep|>In this paper, we presented our approach to the CMCL 2022 Shared Task on eye-tracking data prediction. Our models use the fusion model that involve using the multilingual contextualized token representations using transformer architecture and attaching input features that we created that aid the model performance in predicting eye tracking features . This approach helped us become language agnostic which essentially helped the model to perform well in the zero shot cross-lingual setting in Subtask-2 . Our best model based on XLMRoBERTa outperforms the baseline and is also competitive with other systems submitted to the shared for both SubTask-1 and SubTask-2. In future , we aim to use more etymological features based on shared language history and also use the cross language lexical similarity index when predicting in cross lingual setting .
Alternating Heisenberg Spin-1/2 Chains in a Transverse Magnetic Field<|sep|>In this paper, we have investigated the ground state phase diagram of the antiferromagnetic dominant (JAF > JF ) spin-1/2 AF-F chain with anisotropic ferromagnetic coupling in a transverse magnetic ﬁeld h. We have implemented the modiﬁed Lanczos method to diagonalize numerically ﬁnite chains. Using the exact diagonalization results, we have calculated the various order
Transitivity of codimension one Anosov actions of R^k on closed manifolds<|sep|>As we already mentioned in the introduction, codimension one Anosov ﬂows has been extensively studied, from the 60’s until nowadays. It is reasonable to expect that all these results admit natural extensions to (irreducible) Anosov actions of Rk, but most work still has to be done. A symmetric ﬂow is the ﬂow deﬁned by a one-parameter subgroup gt of a Lie group G by right translations on a quotient manifold Γ\G/K, where Γ is a lattice of G and K a compact subgroup commuting with gt. In [32] P. Tomter classiﬁed Anosov symmetric ﬂows up to ﬁnite coverings and conjugacy when G is semisimple or solvable. He proved that in the former case, the symmetric ﬂow is (commensurable to) the geodesic ﬂow of a rank 1 symmetric space, and in the former case, the ﬂow is (commensurable to) the suspension of hyperbolic automorphisms of a compact infranilmanifold. He further pursued his study to the more general case ([33]). This deﬁnition of symmetric ﬂows extends naturally to the notion of symmetric actions of Rk. It is natural to ask about the classiﬁcation of these actions for k > 1, at least in the case of irreducible actions. But the case k = 1 is already quite intricate. In a forecoming paper, we will classify irreducible symmetric actions of Rk of codimension one: either they are Anosov symmetric ﬂows, or suspensions of hyperbolic automorphisms of tori (cf. examples 2, 3).
Outlier-Robust Filtering For Nonlinear Systems With Selective Observations Rejection<|sep|>Considering scenarios where a set of independent sensors provide observations for dynamical systems, we propose to model the outliers independently in each dimension for such cases. We devise an outlier-robust ﬁlter, resulting in selective rejection of corrupted measurements during inference. In addition, we propose modiﬁcations to the existing tractable learning-based outlier robust ﬁlters to deal outliers selectively. Also a modiﬁcation to the proposed method is presented which yield lower computational complexity. Simulations reveal that the techniques which treat outliers selectively exhibit comparative estimation quality which is better as compared to the other methods. Moreover, the theoretical computational overhead is veriﬁed during simulations. Lastly, experimentation in various indoor localization scenarios, using UWB modules, suggests the practical efﬁcacy of the proposed method. The gains obtained in terms of computational overhead can be critical where 1) the data is obtained from a large number of sensors and data acquisition rates are very high 2) the processing power is limited 3) energy savings are of prime concern for example in battery operated devices. ACKNOWLEDGEMENT The authors thank Mr. Arslan Majal (serving as a research assistant in the Smart Data Systems & Applications Laboratory LUMS) for his efforts in the experimental campaign.
Upper critical field of high quality single crystals of KFe$_2$As$_2$<|sep|>Measurements of the in-plane electrical resistivity as a function of the magnetic ﬁeld applied parallel and perpendicular to the tetragonal c-axis of the crystal allow us to extrapolate residual resistivity of the samples in the zero ﬁeld in 100 to 200 nΩ.cm range and residual resistivity ratio in the 2500 to 3000 range. These high values are in reasonable agreement with resistivity measurements in the normal state achieved by application of the magnetic ﬁeld 1.5 T H ∥ c at base temperature of 1.8 K, rrr ∼2000. The upper critical ﬁelds in our samples with Tc=3.8 K signiﬁcantly increased, compared to those for samples with Tc=2.8 K, but Hc2 for two sets of samples can be matched well by a simple scaling of Tc. This unusual linear relation is not expected for the orbital limiting mechanism of the upper critical ﬁeld.
Online Dynamic Motion Planning and Control for Wheeled Biped Robots<|sep|>of motion such as rolling, walking and hybrid motions can be generated from our proposed framework. It is realised through the composition of decoupled rolling motion and walking motion. The Cart-LIPM and the under-actuated LIPM are proposed to model these two motions. Due to the model linearity, the motion planning can be executed in real time which enables fast online MPC implementation, signiﬁcantly increasing the robustness of the motion. At the end, we have demonstrated the usefulness of the enabled hybrid locomotion. However, the proposed composition method has limitations introduced by the template models such as the requirement of constraint CoM height. This prohibits more dynamic motion, such as jumping, which could be very effective in certain circumstances. The motion composition mentioned in this paper only composes the decoupled motions in different directions. Actually, the composition can happen in a single direction. For example in the forward rolling direction, rolling and walking can be also combined. Another issue that has not been mentioned in the paper is the steering control which could be potentially explored, especially in the hybrid locomotion scenario. (EP/R026092/1). The authors also would like to thank Traiko Dinev, Vladimir Ivan, Matt Timmons-Brown and Daniel Gordon for their suggestions on the writing of the paper.
Unconventional quantum Hall effects in two-dimensional massive spin-1 fermion systems<|sep|>as an inﬁnite ladder of ﬁne staircase after it crosses the zero plateau when the chemical potential is moved toward zero energy and then suddenly reverses with its sign being ﬂipped when the chemical potential is moved across zero, in sharp contrast to the conventional QHE in normal materials or the unconventional QHE for Dirac fermions. Although the sudden jump of the Hall conductance also happens in lattice models, e.g. graphene, at high energy, our results show that this phenomenon occurs in a continuous massive spin-1 fermion model around zero energy. Moreover, we have explored the peculiar QHEs in a gapped dice model and found the same phenomena as the continuous spin-1 fermion model. The dice model may be realized in cold atom systems with an appropriate potential being generated by lasers31 and an artiﬁcial magnetic ﬁelds being engineered by the laser-assisted tunneling technology that has implemented strong magnetic ﬁelds in square optical lattices55,56. We thank S.-T Wang, Y.-H. Zhang, F. Zhang, D.-W. Zhang, S. A. Yang and T. Biswas for helpful discussions. This work was supported by the ARL, the IARPA LogiQ program, and the AFOSR MURI program. ∗ yongxuph@umich.edu 1 D. J. Thouless, M. Kohmoto, M. P. Nightingale, and M. den Nijs, Phys. Rev. Lett. 49, 405 (1982).
On the Null Space Constant for $l_p$ Minimization<|sep|>In characterizing the performance of ℓp minimization in sparse recovery, null space constant γ(ℓp, A, k) can be served as a necessary and suﬃcient condition for the perfect recovery of all k-sparse signals. This letter derives some basic properties of γ(ℓp, A, k) in k and p. In particular, we show that γ(ℓp, A, k) is strictly increasing in k and is continuous in p, meanwhile for random A, the constant is strictly increasing in p with probability 1. Possible future works include the properties of γ(ℓp, A, k) in A, for example, the requirement of number of measurements M to guarantee γ(ℓp, A, k) < 1 with high probability when A is randomly generated.
Automatic morphological classification of galaxy images<|sep|>Here we described an algorithm that can automatically classify between images of spiral, elliptical, and edge-on galaxies. The galaxy dataset features a random collection of galaxy images. Since luminosity, size and distance was found highly important for the automatic classiﬁcation of galaxies (Bamford et al. 2009), it can be assumed that the classiﬁcation accuracy can be improved when using datasets of nearby, large or bright galaxies. Since the described supervised machine learning method can be used for general purpose image classiﬁcation, it is reasonable to assume that the same utility can be used for other problems in morphological analysis of celestial objects. The native format of Sloan images is FITS. Since the conversion from FITS format to lossy JPEG requires the sacriﬁce of image information, it can be assumed that direct access to the raw Sloan image ﬁles can potentially lead to a better performance, especially in cases of subtle differences of pixel intensity. Researchers are therefore advised to take this issue under consideration when applying WNDCHARM to problems in automatic galaxy morphology in which the diﬀerences between the galaxies are more diﬃcult to notice by the unaided eye. The dataset used for the described experiments consists of galaxy images manually classiﬁed by the author. Since supervised learning is used, the classiﬁer can be biased by the intuition of the person(s) who prepare the gold standard training data. Therefore, training data for an image classiﬁer that can be used for practical galaxy morphology classiﬁcation should be selected and reviewed carefully. Even if the selection of the data follows a diﬀerent intuition than the author’s, as long as the classiﬁcation criteria are consistent for all images the supervised learning is expected to provide performance ﬁgures that are comparable to the results reported in this paper. Interestingly, many of the challenges of automatic morphology analysis of galaxies appear to be quite similar to automatic analysis of cell morphology. For instance, the interest in automatic detection of binucleate galaxies, which indicate that the two galaxies are being merged, coincides with the interest in automatic detection of binucleate cells, which means that the cell failed to complete the process of mitosis (e.g., G1 arrest). Another example is the interest in peculiar galaxies, which coincides with the interest in affected cells or unexpected phenotypes that are found among very many regular cells. One of the major advantages of the algorithm is that its full source code is available for free download as a compilable software package (Shamir et al. 2008a) that has been tested for robustness and correctness, and researchers who have basic computer skills can easily use the application as a command line utility. Therefore, in cases where there is a need for computer-based morphological analysis we encourage scientists to try WND-CHARM before taking the labour-intensive challenge of designing, developing and testing new task-speciﬁc image classiﬁers. Applications of this method to galaxy classiﬁcation include fully automatic analysis of galaxies, but it can also be used as a decision-supporting tool for datasets that are classiﬁed manually such as Galaxy Zoo.
Decaying Dark Atom constituents and cosmic positron excess<|sep|>Dark matter can potentially be in the form of neutral OHe dark atoms made of stable heavy doubly charged particles and primordial He nuclei bound by ordinary Coulomb interactions. This scenario sheds new light on the nature of dark matter and oﬀers a nontrivial solution for the puzzles of direct dark matter searches. It can be realized in the framework of Minimal Walking Technicolor, in which an exact relation between the dark matter density and baryon asymmetry can be naturally obtained predicting also the ratio of leptons over baryons in the Universe. In the context of this scenario a sparse component of WIMP-like dark atoms of charged techniparticles can also appear. Direct searches for WIMPs put severe constraints on the presence of this component. However we demonstrated in this paper that the existence of a metastable positively doubly charged techniparticle, forming this tiny subdominant WIMP-like dark atom component and satisfying the direct WIMP searches constraints, can play an important role in the indirect eﬀects of dark matter. We found that decays of such positively charged constituents of WIMP-like dark atoms to the leptons e+e+, µ+µ+, τ +τ + can explain the observed excess of high energy cosmic ray positrons, while being compatible with the observed gamma ray background. These decays are naturally facilitated by GUT scale interactions. This scenario makes a prediction about the ratio of leptons over baryons in the Universe to be close to −3. The best ﬁt of the data takes place for a mass of this doubly charged particle of 1 TeV or below making it accessible in the next run of LHC. Acknowledgements C.K. is supported by the Danish National Research Foundation, Grant No. DNRF90.
Correlation effects in superconducting quantum dot systems<|sep|>We studied a 0−π quantum phase transition in a singlelevel quantum dot connected to two superconducting BCS leads using the continuous-time hybridization-expansion quantum Monte-Carlo method. We used the U and ∆ parameters from experiment described in Ref. [9] in order to stay in a realistic region of the parameter space. Performing an electron-hole transformation in the spindown sector we mapped the system on a model that can be solved using CT-HYB method as implemented in the TRIQS package. We presented results as functions of the gate voltage ε as this parameter is easily tunable in the experiment. We showed how the 0 − π quantum phase transition point can be extracted from the behavior of the induced gap and presented the ﬁnite-temperature spectral function as well as the phase diagram in the ε − Γ plane that can be used to determine the value of the tunneling rate Γ. In summary, we showed that CT-HYB is an eﬀective method for studying superconducting quantum dot systems, where the interaction strength is the dominant energy scale. The present formulation is sign problem free and one can access the low-temperature region using reasonable amount of computational resources. We also showed how the spectral function can be obtained using analytic continuation based on the Mishchenko’s stochastic sampling method in order to study the behavior of the subgap Andreev bound states. Comparing the position of the subgap maxima with ABS frequencies from the NRG calcula
Achieving Large Sum Rate and Good Fairness in MISO Broadcast Communication<|sep|>In this paper we ﬁrst investigated the tradeoff between sum rate and fairness for MISO BC. The achievable sum rates were based on DPC or ZFDPC, subject to an individual power constraint. Several qualitative approaches for fairness, such as max sum rate, proportional fairness, harmonic mean and max-min, were also discussed. For the quantitive approaches, we showed that the widely used Jain’s index could become insensitive at high SNR regime and hence proposed an ℓ1norm based fairness measure that can compare the fairness levels achieved by various design objectives at a much ﬁner resolution. We also introduced a new tri-stage design objective that is based on a new concept of statistical power allocation, in sharp contrast to the ﬁxed, deterministic method used in all existing wireless/wired communication systems. The new scheme randomly allocates powers to users based on an optimal probability distribution derived from the tradeoff between sum rate and fairness. We also remark that the proposed tristage design objective can be easily extended to MIMO BC with successive zero forcing DPC [15] as well as zero-forcing beamforming methods. Simulation results showed that the proposed approach can simultaneously achieve a larger sum rate and better fairness than the reputable proportional fairness. A performance upper bound was also given in the paper to show that there might still be rooms for further improvement, especially in the low SNR regime. Finally, it is worth to note that the ordering of users also has some impact on fairness and sum rate. For traditional DPC, all user-orderings have the same sum rate but possibly different fairness values. For ZFDPC, both sum rate and fairness value could change as the ordering of users varies. How to use the ordering of users to improve sum rate and fairness still calls for further research.
Design and Analysis of E2RC Codes<|sep|>The E2RC codes were proposed in [6] as a promising class of rate-compatible codes. In this work we introduced semistructured E2RC-like codes and protograph E2RC codes. We developed EXIT chart based methods for the design of semistructured E2RC-like codes that allow us to determine nearoptimal degree distributions for the systematic part of the code while taking into account the structure of the deterministic parity part. We presented a novel method for ﬁnding EXIT functions for structured code components that have a succinct protograph representation that is applicable in other scenarios as well. This allows us to analyze the puncturing performance of these codes and obtain codes that are better than the original construction. Using our approach we are able to jointly optimize the code performance across the range of rates for our rate-compatible punctured codes. Finally we consider E2RC-like codes that have a protograph structure (called protograph E2RC codes) and propose design rules for ratecompatible protographs with low thresholds. These codes are useful in applications since the protograph structure facilitates implementation. For both the semi-structured and protograph E2RC families we obtain codes with small gaps to capacity across the range of rates.
Non-degenerate Bound State Solitons in Multi-component Bose-Einstein Condensates<|sep|>In summary, we derive and investigate double-hump and tripe-hump bound state solitons in multi-component BECs. The analysis indicates that bright solitons with nodes correspond to the excited bound eigen-states in the self-induced eﬀective quantum wells. Particularly, we reveal that the incoherent interactions between solitons in diﬀerent components is the generation mechanism of the bound state solitons. Furthermore, we demonstrate collisions of non-degenerate bound state solitons are inelastic in general case, which are induced by incoherent interactions and coherent interactions. Similar studies can be extended to more than two components cases, and more abundant bound state solitons are expected. These NDBSSs can be used to investigate much richer nonlinear dynamics and interactions in multi-component BEC systems, such as beating eﬀects, tunneling dynamics, spin-orbital coupling eﬀects, quantum ﬂuctuations. This work is supported by National Natural Science Foundation of China (Contact No. 11775176), Basic Research Program of Natural Science of Shaanxi Province (Grant No. 2018KJXX-094), The Key Innovative Research Team of Quantum Many-Body Theory and Quantum Control in Shaanxi Province (Grant No. 2017KCT12), and the Major Basic Research Program of Natural Science of Shaanxi Province (Grant No. 2017ZDJC-32). Note added: Recently, we noticed nondegenerate solitons were discussed in nonlinear optical ﬁbers by the Hirota bilinear method [33]. In this paper, we perform Darboux transformation method to derive NDBSS solutions. Moreover, the discussions on the mechanism and the node properties could be helpful for our understanding on the NDBSS. The star denotes the complex conjugate. With the trivial seed solutions q1[0] = 0, q2[0] = 0 and spectral parameter λ = λj = aj + bji (j = 1, 2, ..., N), the vector eigenfunctions of the linear system Eqs.(5) can be writed as: where c1j, c2j are the coeﬃcients of eigenfunctions, and they are complex parameters. The fundamental one bright solitons can be obtained by the following Darboux transformation ger denotes the matrix transpose and complex conjugate, and (P[j])1j represent the entry of matrix P[j] in the ﬁrst row and j column. To obtain double-hump one soliton, we need to do the second step of transformation. We employ Φ2 which is mapped to Φ2[1] = T[1]|λ=λ2Φ2, one double-hump soliton solution can be obtained with spectral parameter λ2 = a1 + ib2: Φ2[1]†Φ2[1]. For this case, we choose the coeﬃcients of eigenfunctions Φ1,2 (7) as the following way: (i) c21 = c12 = 0, c11, c22 are nonzero complex parameters, or (ii) c11 = c22 = 0 and c21, c12 are nonzero complex parameters. The corresponding simpliﬁed solution has been present in (2). Examples of the relevant intensity proﬁles have been exhibit in Fig.1. To study the interaction between non-degenerate solitons, it needs to do multiple step transition. For example, by performing the third-step of transition, we employ Φ3 which is mapped to Φ3[2] = (T[2]Φ3[1])|λ=λ3 with Φ3[1] = (T[1]Φ3)|λ=λ3, then the collision between a double-hump soliton and a single-hump soliton can be obtained with spectral parameter λ3 = a2 + ib3: Φ3[2]†Φ3[2]. For this case, we choose the coeﬃcients of eigenfunctions Φ3 (7) as the following way: (i) c13, c23 are nonzero complex parameters, or (ii) c13 = 0 or (iii) c23 = 0, and the coeﬃcients of eigenfunctions Φ1,2 are same as the (9). Typical example for this case has been shown in Fig.3 (a1) and (a2). Naturally, by performing the fourth-step transformation, one can investigate the interaction between two double-hump solitons. We employ Φ4 which is mapped to Φ4[3] = (T[3]Φ4[2])|λ=λ4 = (T[3]T[2]T[1]Φ4)|λ=λ4, then two double-hump solitons solutions can be obtained as follows with spectral parameter λ4 = a2 + ib4 : Φ4[3]†Φ4[3]. For this case, the coeﬃcients of vector eigenfunctions Φ3,4 are analogous to Φ1,2, i.e., (i) c23 = c14 = 0, c13, c24 are nonzero complex parameters, or (ii) c13 = c24 = 0 and c23, c14 are nonzero complex parameters. One typical case has been shown Fig.3 (b1) and (b2). TN = I − XNM−1 N (λI − DN)−1X† N, XN = [Φ1, Φ2, · · · , ΦN] , DN = diag (λ∗ 1, λ∗ 2, · · · , λ∗ N) , XN,i represents the i-th row of matrix XN. For the two double-hump soliton, we choose the parameters as the following way: λ1 = a1 + b1i, λ2 = a1 + b2i, c21 = 0, c12 = 0, c11, c22 are non zero complex parameters, which determine the ﬁrst double-hump soliton; and λ3 = a2 + b3i, λ4 = a2 + b4i, c23 = 0, c14 = 0, c13, c24 are non zero complex parameters, which determine the second one. The oscillator for the two solitons is governed by the factors sin[(a1 − a2)x + (a2 1 − a2 2 + b2 3 − b2 1)t], cos[(a1 − a2)x + (a2 1 − a2 2 + b2 3 − b2 1)t] and sin[(a1 − a2)x + (a2 1 −a2 2 −b2 2 +b2 4)t], cos[(a1 −a2)x+(a2 1 −a2 2 −b2 2 +b2 4)t]. The velocity of solitons is controlled by x+2ajt = const, j = 1, 2, respectively, i.e. the velocity of soliton equals to −2aj. Assume that bj > 0, j = 1, 2, 3, 4, and a1 > a2, ﬁxed the parameters of the ﬁrst double-hump soliton x + 2a1t = const, then x + 2a2t = x + 2a1t + 2(a2 − a1)t. If t → +∞, we have x + 2a2t → −∞. Then we see that
Weighted Matching in the Semi-Streaming Model<|sep|>We presented a semi-streaming algorithm calculating a weighted matching in a graph G. Our algorithm achieves an approximation ratio of 5.585 and therefore surpasses all previous algorithms for the maximum weighted matching problem in the semi-streaming model. In addition to the edges of an actual matching M the algorithm memorizes some more edges of G, the so called shadow-edges. For each input edge e, the subgraph S made up of e and of shadow-edges and edges of M in the vicinity of e is examined. If a certain gain in the weight of M can be made, matching and non-matching edges in S are exchanged. The subgraph S investigated by our algorithm for each input edge consists of at most seven edges. It is reasonable to assume that by examining bigger subgraphs the approximation ratio can be enhanced further. Therefore we believe that extending our approach will lead to improved semi-streaming algorithms computing a weighted matching.
Just DIAL: DomaIn Alignment Layers for Unsupervised Domain Adaptation<|sep|>In this paper we presented DIAL, a general framework for unsupervised, deep domain adaptation. Our main contribution is the introduction of novel, domain-alignment lay
A sharp-front moving boundary model for malignant invasion<|sep|>equilibria: (i) (1, 0) is a saddle for all c; and (ii) (0, 0) is a stable node if c ≥ 2 √
Cosmological Constraints from the double source plane lens SDSSJ0946+1006<|sep|>We have derived the ﬁrst cosmological constraints from a galaxy-scale double-source-plane lens. Our measurement of β is completely independent of other cosmological probes, and can hence easily be combined with other datasets to produce tighter cosmological parameter estimates, lift parameter degeneracies, and test for the presence of unknown systematics. Because of the complementarity of DSPLs with the CMB, our measurement with just a single DSPL improves the precision of the inference on w by approximately one or deﬁned as a lens strength in some other way, but when we apply this deﬁnition they are all in close agreement third. More precise inferences have been made by combining the CMB with, e.g., baryon acoustic oscillation (BAO) measurements (combining Planck with the BAO results from Percival et al. 2010 yields w = −1.12+0.10 −0.11), but we note that the number of DSPLs that will be useful for cosmological inference will increase by orders of magnitude with Euclid (Collett et al., in prep), dramatically improving the precision but also helping to uncover systematic biases. For example, combining Planck with either J0946 or BAO measurements causes the inference on w to shift closer to −1 by around 1σ. The lens model that we have presented is robustly constrained, but our inference on β depends on the assumption that the observed lensing is entirely due to an elliptical power-law mass distribution at z = 0.222 and an isothermal mass distribution at z = 0.609. The latter point is not a signiﬁcant concern here, since the highest redshift source has an impact parameter with respect to the z = 0.609 source that is more than three times larger than the Einstein radius θE s1. Similarly, the power-law description for the central total mass density distribution is motivated by the absence of any correlation between the power-law indices and radii of strong lenses (e.g., Koopmans et al. 2006, 2009; Auger et al. 2010), as well as the power-law behaviour of the total mass distribution over a large range of scales from the ensemble weak lensing mass proﬁle of lenses (Gavazzi et al. 2007) and mass proﬁles of massive X-ray-bright galaxies (Humphrey & Buote 2010). Furthermore, Suyu et al. (2013) have explicitly modelled one time delay gravitational lens, RX J1131-1231, with both a power-law and a stars-plus-dark matter model and ﬁnd no signiﬁcant diﬀerence in the cosmographic inference between the two models when stellar kinematics are included in the modelling; a similar analysis for J0946 is under way. The mass-sheet degeneracy (Falco, Gorenstein, & Shapiro 1985) will also impact our inference on β. Although multiple-source-plane lenses largely break this degeneracy for true mass sheets, Liesenborgs et al. (2008) have shown that a ring of mass (in addition to the mass from the power-law model) between the Einstein radii of the two sources can mimic the masssheet degeneracy even for multiple-source-plane lenses. However, it is not clear what physical process would be responsible for signiﬁcant ring-like projected over- (or under-) densities and we therefore neglect this possibility. Lensing by line-of-sight structures is also not included in our model, and if these objects introduce a positive external convergence then our estimate of β will be low. Treu et al. (2009) found no evidence that this line of sight is atypical, but even in the absence of a bias the unmodelled external convergence will lead to an artiﬁcially low uncertainty. We estimate that ignoring the external convergence results in a ∼ 1 per cent systematic uncertainty on β (i.e., comparable to the statistical uncertainty) which would degrade the precision of our inference on w by ≈ 25 per cent. However, directly modelling the line of sight using the existing SDSS and HST imaging (e.g., Wong et al. 2012; Collett et al. 2013) and including the velocity dispersion proﬁle from Sonnenfeld et al. (2012) will signiﬁcantly decrease this systematic uncertainty. Furthermore, modelling the strong lensing with all of the available HST data will reduce our statistical uncertainty while allowing us to further test for residual systematics by comparing our inference on β between the diﬀerent HST ﬁlters. Although there is still room for improvement of our measurement of β for J0946, the most signiﬁcant obstacle for DSPL cosmological constraints is the scarcity of simple multiple-source-plane lenses. Gavazzi et al. (2008) suggest that one in 40 − 80 galaxy-scale strong lenses should be a DSPL, and tentative Euclid forecasts of ∼ 100000 galaxy-galaxy strong lenses5 imply ∼ 2000 DSPLs and ∼ 40 triple-source systems. Although it is not clear how many of these systems will be useful for cosmography, including favourable (and well-measured) lens and source redshifts (e.g., Collett et al. 2012), our analysis of J0946 demonstrates the signiﬁcant degeneracy-breaking power of even a single DSPL. We are greatful to the referee, Dr Prasanjit Saha, for comments on the original manuscript. TEC thanks Vasily Belokurov for supervision, guidance and suggestions. TEC acknowledges support from STFC in the form of a research studentship. MWA acknowledges support from the STFC in the form of an Ernest Rutherford Fellowship.
The Hyades Cluster: Identification of a Planetary System and Escaping White Dwarfs<|sep|>Tremblay et al (2012 and references cited therein) consider the white dwarf members of the Hyades cluster and suggest 7 white dwarfs that may currently be escaping from the cluster. We measured the radial velocities of the 7 single classical white dwarf members and the 7 candidate escapees. The measured velocities suggest that three of the candidates are indeed leaving the cluster, while one was never a cluster member. The jury is still out on the other three candidate escapees; one is probably not a member and the other two are magnetic. None of the classical white dwarf members have cooling ages >340 Myr and neither do any of the likely escaping members we have identiﬁed. However, if GD31 is an escaping member, which appears possible, albeit rather unlikely, then its cooling age is not much less than the ∼625 Myr age of the cluster. The DBA (now DBAZ) white dwarf LP 475-242 displays photospheric calcium lines indicative of a surrounding planetary system. The spectrum of the DA white dwarf GD 31 displays narrow Ca II K- and H-lines. These might be circumstellar, but are more likely interstellar in which case an usually substantial interstellar cloud exists in the ∼30 pc interval between Earth and GD 31. Also noteworthy is the presence of molecular hydrogen in the ultraviolet spectrum of this high-gravity ∼13,700 K white dwarf. This is the hottest among the three white dwarf atmospheres that are known to contain H2 (Xu et al 2013b). We are grateful to Siegfried R¨oser, Elena Schilbach, and Pier-Emmanuel Tremblay for numerous helpful communications, Seth Redﬁeld and Barry Welsh for guidance on interstellar calcium lines, Detlev Koester for his estimation of the gravity and temperature of GD 31, Patrick Dufour for his calculations of line splittings in magnetic white dwarf GD 77, Adela Kawka for communication of unpublished Ca II K-line EWs, and Boris G¨ansicke for helping to set us straight on the temperature of GD 31. This research was funded in part by NASA and NSF grants to UCLA. Observations of GD 31 made with the NASA/ESA Hubble Space Telescope, were obtained from the Data Archive at the Space Telescope Science Institute, which is operated by the Association of Universities for Research in Astronomy, Inc., under NASA contract NAS 5-26555. These HST observations are associated with program #12169 (B. G¨ansicke PI). Most data presented herein were obtained at the W.M. Keck Observatory, which is operated as a scientiﬁc partnership among the California Institute of Technology, the University of California, and the National Aeronautics and Space Administration. The Observatory was made possible by the generous ﬁnancial support of the W.M. Keck Foundation. We thank the Keck Observatory support staﬀ for their assistance. We recognize and acknowledge the very signiﬁcant cultural role and reverence that the summit of Mauna Kea has always had within the indigenous Hawaiian community. We are most fortunate to have the opportunity to conduct observations from this mountain.
Flux tubes at finite temperature<|sep|>In this paper we studied the color ﬁeld distribution between a static quark-antiquark pair in the SU(3) pure gauge theory at ﬁnite temperatures. To our knowledge, this kind of investigation within the dual superconductivity approach is so far unique, after the pioneer study of Ref. [6], except for the preliminary analyses of Refs. [88–92]. For the chromoelectric sector we adopted the connected correlator built with Polyakov lines, while for the chromomagnetic sector we used the connected correlator built with Wilson loops. We have made use of the publicly available MILC code [62], which has been suitably modiﬁed by us in order to introduce the relevant observables. Indeed, the use of the MILC code permits to carry out simulations on lattices with considerable spatial and temporal extensions. From previous studies, it is known that, at zero temperature, the chromoelectric ﬁeld generated by a static quark-antiquark pair can be described within the dual superconductor mechanism for conﬁnement. In particular, it has been shown that the transverse proﬁle of the longitudinal chromoelectric ﬁeld can be accurately accounted for by the phenomenological functional form given in Eq. (2.8). Remarkably, in the present study we found that this last result extends also at ﬁnite temperatures. Moreover, we found that the ﬂux tube structure survives to the deconﬁnement transition. However, the behavior of the ﬂux-tube chromoelectric ﬁeld across the deconﬁnement transition does not match the dual version of the eﬀective Ginzburg-Landau description of ordinary type-II superconductors. In particular, the Ginzburg-Landau parameter κ is seen to be κ ≪ 1 at zero temperature, while κ ≃ 1 near the deconﬁnement critical temperature. Indeed, we found that as the temperature is increased towards and above the deconﬁnement temperature Tc, the amplitude of the ﬁeld inside the ﬂux tube gets smaller, while the shape of the ﬂux tube does not vary appreciably across the deconﬁnement temperature, thus leading to a scenario which resembles an “evaporation” of the ﬂux tube. Since our results are quite surprising, some comments are in order here: • To exclude the possibility of contamination of our Monte Carlo ensembles above Tc from conﬁgurations belonging to the conﬁned phase (and vice versa), we looked at histograms of the measured values for the longitudinal chromoelectric ﬁeld, conﬁguration by conﬁguration, at ﬁxed distance xt = 0 (i.e. on the axis connecting the static sources) and at zero smearing steps; the distribution of our measurements showed a Gaussian shape in all temperature regimes (T = 0.8 Tc, T = Tc, T = 1.2 Tc). • In our continuum scaling analysis we performed simulations, at ﬁxed temperature, on lattices of spatial size 24, 32, 40, 48, 64 (results for Ls = 40 and 64 are explicitly shown in the manuscript), thus providing with a check of the stability of our results in the thermodynamic limit. • Our investigation relies to a large extent on the smearing procedure and it would be advisable to check the stability of our results under changes of the smoothing procedure, which we plan to do in future studies. We stress, however, that the behavior under smearing of the parameter φ, shown in Fig. 4, is just as expected: after a number of smearing steps which scales in accordance with the diﬀusive nature of the process, φ reaches a broad maximum or stabilizes, thus signalling the complete washing out of ﬂuctuations at the level of lattice spacing, before ﬂuctuations at physical length scales are aﬀected. Moreover, even admitting that the smearing procedure introduces some alteration in the shape of the ﬂux tube, it would be quite unlikely that it would do that in such a clever way to get proﬁles of the chromoelectric ﬁeld in so nice an agreement with the function given in Eq. 2.8 (see Fig. 6) and with the continuum scaling (see Fig. 5). As a matter of fact, we have monitored the shape of the chromoelectric ﬁeld during the whole smearing procedure, and always found that the ﬁeld proﬁle changes very mildly, in an interval of smearing steps ranging from 10 up to numbers of order 100. • The attenuation of the ﬂux we observe above Tc could be explained by screening eﬀects; we have ﬁxed the physical distance between the two static sources at ∆ = 0.76 fm and, according to Ref. [93], such distance is compatible with the screening length from the lattice at the deconﬁning temperature. • A direct comparison between our results and those of Refs. [91, 92] is not possible, since in these works the distance between the sources is not speciﬁed. There is, however, a reasonable qualitative agreement. Indeed, both works agree on the persistence beyond the critical temperature of the longitudinal component of the chromoelectric ﬁeld. Namely, the rightmost panels in Figs. 3 and 4 of Ref. [91] show that the chromoelectric ﬁeld, though attenuated in amplitude, survives at values of β as large as 6.30, corresponding, on a 243 × 6 lattice, to temperatures of about 1.93 Tc, much larger than those considered in our work. We also investigated the chromomagnetic sector which is relevant for the QCD eﬀective theory at high temperatures. We focused on the chromomagnetic ﬂux tube which is responsible for the nonzero spatial string tension. Even in the chromomagnetic sector we found that the ﬂux tube is built mainly from the longitudinal chromomagnetic ﬁeld. Our results showed that the strength and the size of the chromomagnetic ﬂux tube increase with the temperature, consistently with the temperature behavior of the spatial string tension. Our ﬁndings conﬁrm the importance of long-range chromomagnetic correlations in hightemperature QCD. Finally, it is worthwhile to stress that our results could have important phenomenological applications in hadron physics. In particular, we believe that they are relevant to clarify the nature of the initial state of the quark-gluon plasma in heavy-ion collisions. However, before attempting phenomenological applications, it is important to extend the present study to full QCD, i.e. to the SU(3) lattice gauge theory with improved gauge action and dynamical quarks with masses at (almost) the physical point. This work was in part based on the MILC collaboration’s public lattice gauge theory code. See http://physics.utah.edu/~detar/milc. This work has been partially supported by the INFN SUMA project. Simulations have been performed on BlueGene/Q at CINECA (Project INF14_npqcd), on the BC2S cluster in Bari, and on the CSNIV Zeﬁro cluster in Pisa.
Water in Comet 2/2003 K4 (LINEAR) with Spitzer<|sep|>We have observed the ν2 vibrational band of water in comet C/2003 K4 (LINEAR) within 5.5 to 7.6 µm Spitzer IRS spectra, deriving a water production rate of (2.43±0.25)× 1029 molec. s−1 when the comet was at a pre-perihelion heliocentric distance of 1.760 AU. Although the IRS spectra are of moderate resolution, modeling of the observed emission in the 5.7 to 6.8 µm region constrained the water spin temperature to be 28.5+6.5 −3.5 K. The measured Tspin is comparable to that of other Oort cloud comets and suggestive of a common formation zone for the precometary water ices that eventually agglomerated into the nuclei, though the precise interpretation of the OPR as a probe of the primordial formation zones of comets in the protosolar nebula remains controversial (Crovisier 2007). The observed decrease (at 3-σ conﬁdence level) of the water rotational temperature with cometocentric distance is compatible with evolution from thermal to ﬂuorescence equilibrium and constrains somewhat the role of electron collisions in water excitation. The kinetic temperature of the gas is poorly constrained. Neither emission from carbonates nor PAHs was necessary to account for any emission in excess of the continuum at wavelengths between 5 to 7 µm, suggesting that these species are not present in the coma of C/2003 K4 (LINEAR) at the abundance levels measured by Lisse et al. (2006) in comet 9P/Tempel 1. However, an emission feature at ∼ 7.3 µm is observed that remains unidentiﬁed, as potential emission candidates, the SO2 ν3 band or CH3 deformation modes, can be discounted. This work is based on observations made with the Spitzer Space Telescope, which is operated by the Jet Propulsion Laboratory, California Institute of Technology under a contract with NASA. Support for this work was provided by NASA through an award issued by JPL/Caltech. Support for this work also was provided by NASA through contracts 1263741, 1256406, and 1215746 issued by JPL/Caltech to the University of Minnesota. C.E.W. and M.S.K. acknowledge support from the National Science Foundation grant AST-037446. M.S.K. acknowledges support from the University of Minnesota Doctoral Dissertation Fellowship. The authors also wish to thank E. F. Polomski with initial assistance in planning PID 131 activities, J. Crovisier for useful discussions, E. Peeters for providing her PAH spectra in digital form, and the referee who helped improve the discussion presented in the manuscript.
Deconfinement transition in two-flavour lattice QCD with dynamical overlap fermions<|sep|>The preliminary data presented here certainly rules out the phase transition (or crossover) at β ≤ 7.6 and β ≥ 8.1, thus the transition point in our simulations with dynamical overlap fermions should be somewhere in the range 7.6 < β < 8.1. Within this range of β, however, the precision of our measurements is insufﬁcient to determine the transition point. The fact that for β = 7.7 and β = 7.8 all our observables deviate from the smooth behavior suggests that autocorrelation times might be larger for these two points, which can in turn hint at the proximity to the phase transition. We therefore plan to investigate this region of β values using larger numbers of conﬁgurations. We should also note that at present we perform simulations at ﬁxed bare quark mass, thus changing the temperature by varying the inverse gauge coupling β results also in some variation of the pion mass. Therefore in our simulations the physical pion mass is not ﬁxed. Possible improvement of our simulation strategy would be then either to keep the physical pion mass constant, or to vary the temperature by changing the temporal size of the lattice.
Diffusion of active chiral particles<|sep|>The diﬀusion of chiral, active Brownian particles in free, three-dimensional space has been considered. Particular attention was conceded to the probability density, P(x, t), of ﬁnding a particle at position x at time t independently of its swimming direction, quantity that is susceptible of experimental sampling by the use of singleparticle tracking techniques. A systematic method, based on the multipole expansion of the complete probability density P(x, ˆv, t), where v denotes the particle’s direction of motion, allows to ﬁnd Smoluchowski-like equa tions for P(x, t) that includes the eﬀects of chirality for diﬀerent time regimes. For the rotationally invariant motion, i.e. in the absence of chirality, diﬀusion is described by the standard telegrapher’s equation which emerge from the method in the long-time regimen when the hierarchy can be cut up to the dipole terms. Notwithstanding the nature of the approximation, the telegrapher’s equation provides the exact time dependence, of the mean squared displacement for arbitrary values of the P´eclet number as was veriﬁed by numerical simulations using the corresponding Langevin equation for active Brownian particles. We found that such is the case even when the eﬀects of chirality are taken into account, in that instance, the telegrapher’s equation is modiﬁed by an extra term that carries the information about the anisotropy due to the rotational component of the motion. Previous reported expressions for the eﬀective diﬀusion coeﬃcient were recovered from our theoretical framework. The fourth moment of P(x, t) was also calculated and the kurtosis, that measures the “shape” of the probability density, analyzed. For this, the quadrupole terms of the expansion were included in the analysis, which resulted into a generalization of the telegrapher’s equation from which analytical expressions for the fourth moment, and therefore for the kurtosis, were obtained in the rotationally invariant case. Numerical simulations were performed to verify the exactness of the time dependence of the kurtosis. In the isotropic case (τ0 = 0) κ(t) is bounded from below by 9, value that corresponds to a spherical shell distribution, and from above by invariant value for a Gaussian distribution, 15, and exhibits a nonmonotonic behavior for ﬁnite values of the P´eclet number in the form of a global minimum which is related to the persistence eﬀects. On the other hand, the particles trace stochastic helical trajectories along the ˆz direction as chirality breaks rotational invariance, making diﬀusion anisotropic. For large enough P´eclet numbers the transient of the probability density shows an interesting oscillating behavior between a Gaussian shape and a spherical shell one. No analytical expressions were obtained in this case, however it is possible to obtain analytical expression of the kurtosis of the marginal distribution of the particles position in the plane orthogonal to the axis of rotation. The case for which a chiral active particle moves rotating along an axis of rotation uniformly distributed on the sphere is presented. A statistical analysis of the trajectories obtained from numerical simulations of the Langevin equations, indicates that the asymptotic regime presents normal diﬀusion described by non Gaussian distribution, revealing an instance where “anomalous, yet Brownian, diﬀusion” is exhibited. The results presented in this paper has proven that the method employed to obtain analytical expressions of the exact time dependence for standard experimental data, namely, the mean squared displacement and the kurtosis of the particles position distribution, is valuable and com plements the common approach based only on Langevin equations, particularly for the description of the combined eﬀects of chirality and active motion, a situation that is of interest in biological and man-made systems. Though we have restricted our analysis to the case of free diﬀusion it is of interest to extend the method presented in this paper to the case when particles diﬀuse under the action of position/velocity dependent forces.
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?<|sep|>In this paper, we conducted extensive experiments, pretraining and ﬁnetuning of up to 100 models ranging from 10 well-established Transformer and non-Transformer architectures. We showed that different model architectures can have different scaling behaviours and models performing well in one compute region (or model size) may not do identically well in another compute region. We also showed that model architectures may do well on upstream perplexity but fail to transfer to downstream tasks. Hence, practitioners should be cautious about developing architectures that not only scale well with respect to the upstream perplexity, but also based on downstream performance. While we certainly do not expect researchers to always report model performance across all scales (especially large-scale), we believe that it is good to keep in mind that architectures can perform quite differently at different compute regions. Hence, this might be a good dimension to consider when designing new inductive biases. As such, performing evaluation at a certain compute region may be insufﬁcient to capture the full picture. It is also good to consider if different inductive biases will result in different extends of emergent capabilities (Wei et al., 2022; Abnar et al., 2020). We also showed that different model architectures may react differently to different scaling protocols, which further expands on the narrative that Table 2: Slope of a ﬁtted linear line for each model, when we compare FLOPs vs. upstream performance (F, U), FLOPs vs. downstream performance (F, D), parameter size vs. upstream performance (F, U), parameter size vs. downstream performance (P, D), and ﬁnally upstream performance vs. downstream performance (U, D). Transformer 0.54 0.28 0.47 0.24 0.49 GLU-Trans. 0.49 0.24 0.42 0.22 0.46 LConv 0.32 0.13 0.29 0.11 0.48 Funnel 0.47 0.22 0.38 0.18 0.46 Switch 0.23 0.14 0.13 0.08 0.58 Universal 0.50 0.20 0.56 0.22 0.35 ALBERT 0.08 -0.12 0.13 -0.21 -1.67 Evolved 0.44 0.22 0.42 0.21 0.47 Performer 0.25 0.05 0.24 0.05 0.24 MoS-Trans. 0.43 0.21 0.43 0.20 0.47 MLP-Mixer 0.32 -0.03 0.26 0.65 -0.02 comparing and benchmarking these models can be very challenging (Dehghani et al., 2021b). When it comes to scaling large models, this paper shows that novel inductive biases can be indeed quite risky which might explain why most state-of-theart large language models (Rae et al., 2021; Chowdhery et al., 2022; Tay et al., 2022) are based on relatively vanilla architectures. Our advice is to be cautious when staking an expensive run on a Transformer architecture that drastically modiﬁes the attention mechanism (e.g., Mixers and Performers are generally high risk options as seen in our experiment results). Finally, we acknowledge that not every practitioner or researcher would require models that are able to scale to billion of parameters. In that case, inductive biases that are tailored to small or low compute will be sufﬁcient.
Federated Learning on Adaptively Weighted Nodes by Bilevel Optimization<|sep|>We propose a FL approach on a network with weighted nodes and develop a federated bilevel optimization algorithm to optimize the weights based on the model’s performance on a validation set. We analyze the generalization performance of the resulting model and identify the scenarios where our method theoretically outperforms training with local data and FL with even weights.
Unified Picture of Electromagnetic and Gravitational Forces in Two-Spinor Language<|sep|>We have seen how the two-spinor formalism via the generalization of equations (6) to curved spacetime provides a unifying picture of both gravitational and electromagnetic interactions including spin. In the special case of radial trajectories, gravitational and electric forces are approached in a new integrating picture in which mass, charge and spin are accounted for. It is shown that while Coulomb law is valid at all distances, Newton gravitational inverse square law remains valid only for weak ﬁelds or long distances (i.e. for r ≫ 2GM), increasing and reaching a singularity when r = 2GM. When there is a magnetic ﬁeld in the radial direction, a test charged particle with spin one-half precesses internally with a frequency that previously was only accounted for within the contest of relativistic quantum mechanics.
Topological Insulators and Superconductors from String Theory<|sep|>The tenfold classiﬁcation of TIs/SCs was derived previously by several diﬀerent approaches. One of the reasonings in Ref. [15] that leads to the classiﬁcation is the socalled boundary-to-bulk approach; i.e,. exhaustive study of non-Anderson localized state at boundaries which signals the existence of the topological bulk theory. On the other hand, Kitaev proposed K-theory approach for the classiﬁcation. Our string theory realization of TIs/TSCs can be thought of yet another derivation of the tenfold classiﬁcation. It is quite compelling that all diﬀerent approaches give the identical table. It is worth while mentioning that the construction of TIs/TSCs from Dp-Dq systems is highly constrained in the following sense: Dp-Dq systems which do not correspond to a bulk TI/TSC or a boundary of a TI/TSC (i.e., an intersecting Dp-Dq system) suﬀer from the existence of tachyons in the open string spectrum or are just multiple copies of the minimal fermion systems for the ten classes. It is also interesting to note that, for the Dirac representative of TIs/TSCs which are constructed systematically in Ref. [15], the momentum dependence of the projection operator of Bloch wavefunctions, which is one of key ingredients in the classiﬁcation of TIs/TSCs, looks quite similar to the spatial proﬁle of the tachyon ﬁeld in string theory in real space [24, 25]. One of the implications of our D-brane construction of TIs/TSCs is the underlying ﬁeld theory description. When a charge of some sort is conserved, one can describe the topological phase in terms of eﬀective ﬁeld theory for the linear responses, but this is not always the case. For example, the d = 2-dimensional p-wave SC is one of representative and most important topological phases in d = 2, but its description in terms of ﬁeld theory is not yet clearly understood, partly because there is no conserved quantum number. An eﬀective (topological) ﬁeld theory description, if exists, suggests the topological phase in question is robust against several perturbations, in particular against interactions. D-branes not only carry a K-theory charge which is necessary to describe topological phases, but they also come with the WZ term. This sheds a light on ﬁeld theory description of TIs/TSCs in all symmetry classes. Finally, to put our D-brane construction in a perspective, we can consider holographic descriptions of these systems by extending the constructions in [33, 34] in principle, though it is not possible to take the large-N limit of Z2 charged D-branes. For a closely related holographic construction of fractional topological insulators, refer to the recent paper [48]. It would be interesting to holographically calculate the entanglement entropy [49] as it is expected to distinguish various topological phases, including topological insulators/superconductors [50]. We acknowledge “Quantum Criticality and the AdS/CFT correspondence” miniprogram at KITP, “Quantum Theory and Symmetries 6” conference at University of Kentucky. We would like to thank K. Hori, A. Karch, A. Kitaev, P. Kraus, A. Ludwig, J. Moore, M. Oshikawa, V. Schomerus, and S. Sugimoto for useful discussion, and A. Furusaki for his clear lecture at “De velopment of Quantum Field Theory and String Theory” (YITP-W-09-04) at Kyoto University. SR thanks Center for Condensed Matter Theory at University of California, Berkeley for its support. TT is supported in part by JSPS Grant-in-Aid for Scientiﬁc Research No. 20740132, and by JSPS Grant-in-Aid for Creative Scientiﬁc Research No. 19GS0219. In this appendix, we give an overview on how to calculate the fermion spectrum (the number of fermion species) arising from open strings between Dp- and Dqbranes. We neglect all stringy modes and focus on the low energy limit. For more details, refer to textbooks, e.g., Ref. [31].
Dissipation and spontaneous emission in quantum electrodynamical density functional theory based on optimized effective potential: A proof of concept study<|sep|>In conclusion, we demonstrate the possibility of describing quantum dissipation in the framework of QEDFT with xc potential approximated within the OEP formalism. This opens a way for the ﬁrst principle modeling of non-relativistic electron systems interacting with cavity photons of realistic lossy cavities, as well as with other types of Caldeira-Leggett dissipative environments relevant in condensed matter and chemical physics.
Non-Abelian two-form gauge transformation and gauge theories in six dimensions<|sep|>We have introduced a toy model which contains a two-form and a Dirac spinor. The ambiguity to deﬁne the Wilson surface operator is explored and we deﬁned a ﬁxed map from the spacetime to the loop space to solve this obstacle. This map determined a speciﬁc coordinates to the surface which connects the loops which are speciﬁed from the ﬁxed map in the diﬀerent points of the spacetime. With the aim of this map, we have derived a non-Abelian gauge transformation of two-forms. We also deﬁned a covariant derivative with respect to the two-forms and constructed an interacting Abelian gauge theory in six-dimensions. By integrating over all maps we have obtained an eﬀective ﬁeld theory.
Zero-bias anomaly in nano-scale hole-doped Mott insulators on a triangular silicon surface<|sep|>insulating nanoscale domains on heavily-doped p-type Si(111) substrates. STS data at  77 K show that the hole-doping concentration is largest near the edges, as well as for  the smallest nanoscale domains. STS spectra were recorded for different domain sizes,  different doping levels, and different temperatures. For the 3-Sn surface on the  most-heavily-doped substrate, small domains exhibit a strong ZBA, which evolves into  a hard gap for the smallest 44 nm2 domain. Two potential mechanisms for the ZBA are  considered: the chiral 𝑑 +  i𝑑  wave superconductivity proposed in a recent  theoretical work, and a DCB effect due to a charging effect in the 3-Sn domains. The  latter results from the imperfect electrical coupling between the hole-doped Mott  insulating domains and the Si substrate. Both scenarios fit well for weak ZBAs.  However, the DCB theory provides a better fit when the ZBA becomes fully gapped. It  also provides a better fit to the temperature-dependent tunneling data. A qualitatively  different ZBA develops for 3-Sn domains grown on lesser-doped Si substrates (e.g.  the p-0.004 and p-0.008 substrates). These spectra are interpreted in terms of a  classical Coulomb blockade effect, where the resistances of the T-junction and Sjunction are comparable. doped B3 substrate is predominantly a DCB effect. Whether a secondary d-wave  superconductivity-induced spectral feature exists in these spectra remains an open  question. It is possible that superconductivity does not yet appear at this doping level  or at this temperature. Comparing STS with and without a sufficiently high magnetic  field is likely to clarify this issue. Alternatively, one could potentially use quasiparticle  interference imaging to explore sign changes in the superconducting order parameter  [48-50]. While sufficient small sizes are always expected to suppress Cooper pairing,  superconductivity may ultimately emerge if one could increase the hole-doping level  of large-scale domains well beyond the currently achievable maximum of about 12%.  Such efforts are currently underway in our laboratory.
Analyzing transverse momentum spectra by a new method in high-energy collisions<|sep|>We summarize here our main observations and conclusions. (a) We have used a new method to analyze the pT spectra of identiﬁed particles produced in central AA collisions. The particle’s pT is regarded as the joint contribution of two participant partons which obey the modiﬁed Tsallislike transverse momentum distribution and have random azimuths in superposition. The Monte Carlo method is performed to calculate and ﬁt the experimental pT spectra of π±, K±, p(¯p), and φ produced in central Au-Au and Pb-Pb collisions over an energy range from 2.16 to 2760 GeV measured by international collaborations. Three free parameters, the eﬀective temperature T , entropy index q, and revised index a0 are obtained. (b) Our results show that, with the increase of √ and K± spectra. The boundary is around 5 GeV. This energy is possibly the critical energy of a possible de-conﬁned phase transition from hadron matter to QGP. The values of q are close to 1 and have a slight increase with increasing √ sNN. This result shows that the system is in approximate equilibrium in the considered energy range and closer to the equilibrium at lower energy. Generally, the values of a0 are mass dependent and not energy dependent. The resonance generation of π± and the constraints of other particles in a low-pT region are reﬂected by the values of a0. Data Availability The data used to support the ﬁndings of this study are included within the article and are cited at relevant places within the text as references. The funding agencies have no role in the design of the study; in the collection, analysis, or interpretation of the data; in the writing of the manuscript; or in the decision to publish the results. Acknowledgments The work was supported by the Shanxi Agricultural University Ph.D. Research Startup Project under Grant No. 2021BQ103, and by the Fund for Shanxi “1331 Project” Key Subjects Construction.
Adjoint approach to calculating shape gradients for 3D magnetic confinement equilibria<|sep|>We have obtained a relationship between 3D perturbations of MHD equilibria that is a consequence of the self-adjoint property of the MHD force operator. The relation allows for the eﬃcient computation of shape gradients for either the outer plasma surface using the ﬁxed boundary adjoint relation (2.20) or for coil shapes using the free boundary adjoint relation (2.18). The computation of the shape gradient of several stellarator ﬁgures of merit has been demonstrated with both the adjoint and direct approach. The application of the adjoint relation provides an O(N) reduction in CPU hours required in comparison with the direct method of computing the shape gradient, where N is the number of parameters used to describe the shape of the outer boundary or the coils. For fully 3D geometry, N can be 102 − 103. Thus, the application of adjoint methods Figure 3: The coil shape gradient for fι (3.11) computed using the adjoint solution (3.18) for each of the 3 unique coil shapes (black). The arrows indicate the direction of Sk, and their length indicates the local magnitude relative to the reference arrow shown. The arrows are not visible on this scale on the outboard side. can signiﬁcantly reduce the cost of computing the shape gradient for gradient-based optimization or local sensitivity analysis. To compute the adjoint equilibria in this work, the full non-linear MHD equilibrium equations are solved using VMEC with the addition of a small perturbation to the current proﬁle, characterized by I∆, or a small perturbation to the pressure proﬁle, characterized by ∆. These parameters must be tuned carefully such that the perturbation is large enough that the result is not dominated by round-oﬀ error, but small enough that nonlinear eﬀects do not become important. If a perturbed equilibrium code were instead used, these diﬃculties could be avoided. However, it is convenient to use the same code for both the unperturbed and adjoint equilibrium. It should be noted that the adjoint approach we have outlined can not yield an exact analytic shape gradient. Throughout we have assumed the existence of magnetic surfaces as the 3D equilibrium is perturbed. Therefore a code such as VMEC, which minimizes an energy subject to the constraint that surfaces exist, is suitable. Generally VMEC solutions do not satisfy (2.3) exactly, (N¨uhrenberg et al. 2009), as they do not account for the formation of islands or current singularities associated with rational surfaces. Furthermore, the the parameters ∆ and I∆ introduce additional numerical noise. We have demonstrated that the typical diﬀerence between the shape gradient obtained with the adjoint method and that computed directly from numerical derivatives is ≲ 5%. These errors should not be signiﬁcant for applying the shape gradient to an analysis of engineering tolerances. The discrepancy between the true shape gradient and that obtained numerically, with the adjoint approach or with ﬁnite diﬀerence derivatives, may become problematic as one nears a local minimum during gradient-based optimization, as the resulting shape gradient may not provide a true descent direction. For the ﬁgure of merit considered in section 3.1, the additional force (δF2 in (2.14)) applied to the adjoint problem can be expressed as a gradient of a scalar pressure; thus an existing equilibrium code could be utilized without modiﬁcation. This approach could be applied to other quantities of interest. For example, consider a ﬁgure of merit which
Invariant Jet differentials and Asymptotic Serre duality<|sep|>An invariant version of the main theorem of Demailly [2] on the existence of global sections of the twisted k-jet bundle is proved using the Morse estimates on the curvature of an invariant metric on the Demailly-Semple bundle. Furthermore, we present a speciﬁc Serre duality for a bundle of k-jets on a projective variety. The proof is based on the existence of the relative pluri-canonical sheaf on jet ﬁbers. We also provide an algebraic proof that the ﬁber ring on the regular locus of moduli of k-jets is differentially ﬁnitely generated.
Random lasing in an Anderson localizing optical fiber<|sep|>The ﬂexible ﬁber based random laser demonstrated in this paper clearly operates in the regime of Anderson localization leading to a spectrally stable and highly directional random laser. Disorder induced localized states form several isolated channels compactly located across the transverse dimension of g-ALOF. Upon excitation of one of these channels by a narrow input pump, the system starts lasing by the feedback provided through the 4% reﬂections at each air-ﬁber interface. In this implementation, a point to point correspondence between the transverse position of pump and output laser is achieved. Transversely localized laser signal is associated with a more stable frequency response as long as the localization properties are unchanged. The stability of the laser spectrum is attributed to the strong mode conﬁnement provided by the localized states in g-ALOF.
Hadronic Light by Light Contributions to the Muon Anomalous Magnetic Moment With Physical Pions<|sep|>We have described two sampling strategies for our choice of stochastic integration points. We have also presented a new lattice-based numerical method for reducing the QED power-law, ﬁnite volume error by introducing a second, larger “QED volume” in which the photon and muon parts of the path integral are evaluated. This method has been illustrated by comparing HLbL calculated on combinations of 163, 243 volumes. Finally we have shown our current result using this method on a physical-pion-mass, 483, 5.5fm lattice. We plan to a) address the discretization errors by computing on our ﬁner, physical-pion 643 lattice with similar physical volume. b) address the ﬁnite volume effect by using the 483 QCD box inside a larger QED box and c) compute disconnected diagrams within the framework of this newly-developed evaluation strategy. We would like to thank our RBC and UKQCD collaborators for helpful discussions and support. We would also like to thank RBRC for BG/Q computer time. The 483 computation is performed on Mira with ALCC allocation using BAGEL [10] library. T.B is supported by U.S. DOE grant #DE-FG02-92ER41989. N.H.C and L.C.J are supported by U.S. DOE grant #DESC0011941. M.H is supported by Grants-in-Aid for Scientiﬁc Research #25610053. T.I and C.L are supported by U.S. DOE Contract #AC-02-98CH10996(BNL). T.I is also supported by Grantsin-Aid for Scientiﬁc Research #26400261. This research used resources of the Argonne Leadership Computing Facility, which is a DOE Ofﬁce of Science User Facility supported under Contract DEAC02-06CH11357.
How Scale Affects Structure in Java Programs<|sep|>We have described a quantitative study designed to answer the question: does the scale of a software system affect the internal structure of its modules? We have made an important step into answering this question by performing a statistical analysis of a very large and varied collection of Java projects. The statistical signiﬁcant results in this dataset are strong: there are, indeed, superlinear effects on some aspects of the modules’ internal structure and composition with other modules. This reinforces the widely accepted idea that programming-in-the-large carries with it different concerns that aren’t as strongly present for programming-in-thesmall. More importantly, it has tremendous consequences for software metrics in general. Many of the metrics proposed in the literature, and that are used widely in IDEs, have suffered from poor information content for prediction models because they correlate with the much simpler size metrics. Our paper shows how this can be corrected. This work was supported by National Science Foundation grants nos. CCF-0725370 and CCF-1018374, and by the DARPA MUSE program. We would like to thank Pedro Martins for his assistance in the production of the artifact, and the anonymous reviewers, who made this a better paper.
Dynamical symmetrization of the state of identical particles<|sep|>In summary, we have proposed a self-consistent model for the dynamics of identical particles produced independently by spatially separated sources and therefore described initially by a product state. The symmetrization of the twoparticle state occurs dynamically as the particles collide with each other which erases their distinguishability. Our simple two-particle 1D model provides an intuitively plausible symmetrization picture. We show that, for Gaussian wavepackets, the collision probabilities can be accurately described via a quasi-classical approach employing Wigner functions of the particles. Quantum mechanically, the transition between the product and symmetric states can be formally regarded as a consequence of spontaneous measurement of the (non-self-adjoint) collision time operator. Semiclassical extension of the model to 3D collisions of short-range interacting particles is straightforward, as discussed in Appendix F, but a fully quantum generalization of the collision time operator to 3D space is a known problem and will require further research. Our model is consistent with the bulk of experimental observations of the (anti)symmetrization eﬀects, such as, e.g., the Pauli principle or Bose-Einstein condensation, which consider identical particles that had suﬃcient time to interact and collide with each other. For such particles, we can safely assume that the symmetrization transition had already taken place before the system became subject to interrogation. The main purpose of our model is to provide a clear physical picture of the transient regime between the particles being very far (when their state is still a tensor product) and suﬃciently close (when their state is already symmetrized), by employing several physically plausible assumptions. It is worth emphasizing that our results are not interpretational—they can be tested experimentally. Once such tests are preformed, the hypothesis that independently generated, spatially separated identical particles can be considered distinguishable can be supported or laid to rest. We thank Janet Anders for interesting discussions on identical particles, and Roger Balian and Theo Neuwenhuizen for useful remarks. A.E.A. thanks Theo Neuwenhuizen for his role in motivating this research. This work was supported by the SCS of Armenia, grants No. 18T-1C090 and No. 20TTAT-QTa003. Reduced (sometimes also called “marginal”) states of multipartite quantum systems are deﬁned through local observables. Consider a compound system in a Hilbert space H = H1⊗H2 consisting of two distinguishable subsystems 1 and 2 living, respectively, in H1 and H2. Given a state of the total system, ˆσ, the reduced state of, say, the ﬁrst subsystem is deﬁned as a positive-semideﬁnite operator ˆσ1 with tr ˆσ1 = 1 such that where L(H1) is the algebra of observables ˆO on H1 and I2 is the identity operator in H2. For identical particles, the set of observables is restricted to permutation-symmetric operators, i.e., only such operators can be measured. This in turn implies that tr(ˆσ ˆO ⊗ I2) is devoid of any physical meaning since any apparatus measuring a single-particle observable ˆO in fact measures ˆO ⊗ I + I ⊗ ˆO; in other words, it measures ˆO for both particles. Hence, the single-particle states are deﬁned through tr(ˆσ1 ˆO) + tr(ˆσ2 ˆO) = tr(ˆσ [ ˆO ⊗ I + I ⊗ ˆO]) ∀ ˆO ∈ L(H1) ∼= L(H2), (A2) where H1 ∼= H2 for identical particles (the symbol ∼= means isomorphic). When ˆσ is permutation-symmetric, the standard approach is to require that ˆσ1 = ˆσ2, in which case Eq. (A2) uniquely determines the reduced state. For example, if the joint state is ∝ |L⟩ ⊗ |R⟩ + η|R⟩ ⊗ |L⟩, with some ⟨L|R⟩ = 0 and η = ±1, then the reduced states are Note that the operational deﬁnition (A2) only necessitates that ˆσ1 + ˆσ2 = |L⟩⟨L|+|R⟩⟨R|, which leaves some freedom in choosing the reduced states, e.g., as ˆσ1 ̸= ˆσ2. Motivated by the intuitive notion of particles, Ref. [31] used this freedom to devise an interpretation of the quantum mechanics of indistinguishable particles in which ˆσ1 = |L⟩⟨L| and ˆσ2 = |R⟩⟨R|. This interpretation, however, breaks down for bosons whenever ⟨L|R⟩ ̸= 0 (for fermions, the interpretation requires amendments but does not fall apart overall). Moreover, speciﬁc assignments for reduced states do not entail any observational consequences, since, due to their symmetry, measurements cannot verify whether ˆσ(sym) 1 = ˆσ(sym) 2 as in Eq. (A3) or ˆσ1 = |L⟩⟨L| and ˆσ2 = |R⟩⟨R|. When the joint state is not permutation-symmetric, there is no reason to assume that ˆσ1 = ˆσ2. For example, when the joint state is |L⟩ ⊗ |R⟩, Eq. (A2) implies that ˆσ1 + ˆσ2 = |L⟩⟨L| + |R⟩⟨R| for arbitrary states |L⟩ and |R⟩, not necessarily orthogonal. Now it is most sensible to assign ˆσ1 = |L⟩⟨L| and ˆσ2 = |R⟩⟨R| which is a prescription that we follow. Operationally, this prescription is justiﬁed by the fact that product states such as |L⟩ ⊗ |R⟩ produce independent probabilities with respect to measurements of the form ˆO ⊗ ˆO; such an example is provided by the position measurement in Eq. (12). To ﬁnd the eigenstates of operator ˆt, we look for the solutions of the equation ˆt|φ⟩ = tc|φ⟩, where tc denotes the eigenvalue of ˆt. In the u-representation, according to Eq. (40), this equation takes the form We seek the solutions among the continuous, piecewise diﬀerentiable functions φ(u) = ⟨u|φ⟩. Solving Eq. (B1) for u < 0 and u > 0 gives where C− and C+ are some constants [cf. Eq. (41)]. The solutions of Eq. (B1) are unique both for u < 0 and u > 0. Therefore, if Eq. (B1), along with the continuity and piecewise diﬀerentiability of φ(u), provides a way to connect C− and C+, the eigenstate will be unique. Otherwise, we shall extend the solutions (B2) to (resp.) u < 0 and u > 0 as in Eq. (41), and the eigenstate will be double degenerate. The only point at which the two solutions meet is u = 0. We see that φ(0−) = φ(0+) = 0, so the continuity condition does not provide suﬃcient information on C− and C+, since φ(0) = 0 for any C− and C+. As for φ′(u), the eigenvalue equation (B1) leads to which again does not connect C− to C+ in any conceivable way. Indeed, if we, e.g., impose φ′(0−) = φ′(0+), we see that C− = C+ when ϵ1 = ϵ2 → 0, C− = 2 C+ when ϵ1 = 4ϵ2 → 0, and so on, which means that C− and C+ are independent. In a sense, the singularity of ˆt at u = 0 is too strong to allow to uniquely connect the eigensolutions from both sides. The independence of C− and C+ means double degeneracy, and the functions in Eq. (41) constitute an orthogonal basis in the eigensubspace of the eigenvalue tc. The physical reason for non-existence of a self-adjoint time operator is that the possible energetic states of a system (the spectrum of the Hamiltonian) has to be limited from below: if there were a self-adjoint time operator, it could be used to translate energy (as momentum is used to translate coordinate), yielding physical states with arbitrarily low energies [22, 29]. To illustrate this statement, let us assume that there exists a collision time observable ˆtc, and consider its expectation value obtained from measurements on the two-particle system in some initial state |Ψ⟩: ⟨Ψ|ˆtc|Ψ⟩. If we start observing the system not at time t = 0, but at a later time t > 0, the collision time should be reduced by t. We therefore impose the time-translation rule ⟨Ψ(t)|ˆtc|Ψ(t)⟩ = ⟨Ψ|ˆtc|Ψ⟩ − t, which is equivalent to which, as mentioned above, is incompatible with H ⊗ I + I ⊗ H having a spectrum bounded from below. Therefore, the collision time operator ˆtc cannot be a self-adjoint operator that satisﬁes the Born’s rule. POVMs represent (generalized) quantum measurements and determine the probabilities of measurement outcomes through the Born rule [4]. This observation is used in the literature [24] to postulate that the outcome probabilities of measuring ˆt deﬁned in Eq. (40) are determined by the POVM comprised of the operators |tc, s⟩⟨tc, s| (s = ±), which are Hermitian, positive-semideﬁnite, and sum up to the identity: � s � dtc|tc, s⟩⟨tc, s| = I(u). Hence, in view of Eq. (39), the POVM describing the measurement of ˆtc will be comprised of operators A POVM, while being able to determine the probabilities of measurement outcomes, does not give a unique prescription (operator) to identify the post-measurement states. More speciﬁcally, if I ≥ ˆ Mx ≥ 0 is an element of a POVM, then the most general reduction rule for it can be written as where ˆσ is a density operator and the otherwise arbitrary operator ˆAx satisﬁes ˆA† x ˆAx = ˆ Mx [32]. In our case [Eq. (D1)], the most general such decomposition is |tc, s⟩⟨tc, s| ◦ I(v) = � |tc, s⟩⟨φs,tc| ◦ I(v)� ˆV † s,tc · ˆVs,tc � |φs,tc⟩⟨tc, s| ◦ I(v)� , (D3) where |φs,tc⟩ are arbitrary normalized states, ⟨φs,tc|φs,tc⟩ = 1, and ˆVs,tc are arbitrary unitary operators ( ˆV † s,tc ˆVs,tc = I(u) ◦ I(v)). Note that, in view of their normalization, states |φs,tc⟩ cannot coincide with |tc, s⟩ because of Eq. (46). Thus, the general state-reduction rule described by Eq. (D2) for POVM elements given by Eq. (D1) is the state |ς⟩ is normalized and does not depend on the choice of the basis {|n⟩}n. Since ˆVs,tc is arbitrary, |ξs,tc⟩ can be an arbitrary pure state that does not depend on s. In particular, it can be chosen to be |Ψs⟩ of Eq. (7), so the quantum jump in Eq. (24) can be made consistent with the post-measurement state change (D6) induced by the collision time operator ˆtc. Note that Eq. (D5) does not violate the repeatability principle: if the same measurement is carried out twice, then the second measurement should conﬁrm the result of the ﬁrst one. This is because there exists an actual projective measurement in a larger Hilbert space the reduction of which to the Hilbert space of the two particles induces the POVM in Eq. (D1) and the transition in Eq. (D5) [4] We assume the Gaussian wavepackets of Eqs. (29), (30), (31), (32) with d ≡ cR − cL > 0, a ≡ aR = aL, and b ≡ bL = −bR. The relevant quantities in Eq. (50) are then given by the following explicit expressions:
Modeling IoT-aware Business Processes - A State of the Art Report<|sep|>The IoT-A project introduced the first IoT-aware business process modelling (IAPM). The IAPM  proposes seven new elements that extend the possibilities of BPMN 2.0 for IoT-ware business  processes. However, the IAPM is not thoroughly evaluated aat the time of writing this summary and  it might not fulfill all the needs to model IoT-aware business process. We need additional research  on the evaluating the application of IAPM by to different domains and needs. Other research proposes alternations to the new elements, such as including a resource role for  activity and sensing task. Some created additional elements or decided that some of the elements of  the IoT-A project were unnecessary. For example, Appel et al. [34] chose to model IoT-aware  processes using event streams in order to use the value of the real-world data that was collected. This report reviews the state of the art on modelling IoT-aware business processes in BPMN. The  extensions to BPMN are elaborated and show a solution for the main issues that were of concern to  IoT-aware business processes. However the new modelling elements have not been evaluated  thoroughly yet and the question remains if they are sufficient when modelling the processes. Furthermore, in the current research the thing is treated as a black box. The assumption is made  that a physical entity cannot have its own process and therefore the representation as a black box is  considered sufficient. However, this assumption is questionable and even if it would be true, a  physical entity can have its own lifecycle. The lifecycle contains states in which the thing can be. In  the case of mobile things, a state can also be a location. The states in which a thing stays can change  due to influences from the process, such as actuating activities. Representing the states can be  important for both clarification of the business process and the accuracy of the remainder of the  process. The current modelling extensions do not enable to make distinctions between different  states of the things. In future research it would be interesting to see if this is necessary and how this  can be modelled in a business process. Lastly, there is not much work on spatial and temporal synchronization of things. The use cases,  analysis and research shows that special and temporal coordination is necessary for IoT-aware  business processes, no extension for BPMN is found in this field at the time of writing to the best of  our knowledge. This is a promising area of future work.
Red and Blueshifts in Multi-stranded Coronal Loops: A New Temperature Diagnostic<|sep|>We have deﬁned a new temperature diagnostic tool relying on the measure of the avearge Dopplershift for a broad range of observed spectral lines. First, we have complemented the work of Sarkar & Walsh (2008, 2009) regarding the thermodynamic properties of coronal loops deﬁned a collection of strands. We have computed the physical properties of a 100 Mm loop with two diﬀerent energy inputs with a particular focus on the Dopplershifts distribution for diﬀerent spectral lines. The recent Hinode/EIS observations have put forward the existence of both blueshift and redshift velocities in active region loops (del Zanna 2008; Warren et al. 2011), the amount of Dopplershifts depending on the peak temperature of the observed spectral line. The multistranded coronal loop model has reproduce the main observed features: (1) red and blueshifts exist in active region loops of realistic length (100 Mm); (2) the amount of red and blueshifts depends on the peak temperature of the observed spectral line: cooler lines being redder, hotter lines being bluer; (3) the amount of red and blueshifts depends on the mean/average temperature of the loop (as an amalgamation of strands). These properties are now well-known for coronal loops and have been reproduced with other numerical experiments (see, for instance, Taroyan and Bradshaw 2014). Second, the 1D multistranded hydrodynamic model oﬀers more information on the spatial and temporal evolution of heated coronal loops, and thus on the physical processes at play. The comparison between the multistranded model and modelled observations leads to the following physical interpretations: Figure 13. Loop i: (a) average Dopplershift along the loop (solid line) and at the footpoint (dash line), and (b) average redshifts (positive) and blueshifts (negative) along the loop (solid line) and at the footpoints (dash line). (c) and (d): same as (a) and (b) respectively for Loop ii Figure 14. Temperature diagnostic tool. (Left) Average Dopplershifts at the footpoint of Loop i (blue) and Loop ii (red) for fp heating, and the average Dopplershift values (black curve) obtained by Tripathi et al. (2009); (Right) Percentage of blueshift at the footpoint (solid) and along the loop (dash) for Loop i (blue) and Loop ii (red) cases. The dashed black lines indicate the crossing of the curves with a vanishing average Dopplershift or a 50% level.
On profitability of selfish mining<|sep|>Selﬁsh mining is a trick that slows down the network and reduces the mining diﬃculty. The attack diminishes the proﬁtability of honest miners and the one of selﬁsh miners before a diﬃculty adjustment. Selﬁsh mining only becomes proﬁtable after lowering the diﬃculty. Another way to achieve that would be to withdraw from the network and start mining another cryptocurrency with the same hashing function. The existence of other cryptocurrencies with the same validation algorithm that allows to switch mining without cost is a vector of attack in itself. When the attacker withdraws and comes back the mining diﬃculty will increase again after 2016 blocks unless the miner executes a selﬁsh mining strategy. Then after a ﬁrst diﬃculty’s adjustment, the diﬃculty mining will stay constant on average. Selﬁsh mining is an attack on the Bitcoin protocol, but the arguments present in the literature do not properly justify the attack. T hey lack of a proper analysis of proﬁt and loss per unit of time. To compare the proﬁtability of diﬀerent mining strategies one needs to compute the average length of their cycles and their revenue ratio, that is a new notion introduced in this article. The attack exploits a ﬂaw in the diﬃculty adjustment formula. The parameter used to update the mining diﬃculty is supposed to measure the actual hashing power of the network. In the presence of a selﬁsh miner, this is no longer true. We have proposed a formula that corrects this anomaly by taking into account the production of orphan blocks. We propose to reinforce the protocol which states that the oﬃcial blockchain is the chain which contains the most proof-of-work by requiring peers to give priority to those containing the most proof-of-work (with ”uncles”). The proposed formula, if adopted, would not eliminate the possibility of selﬁsh mining but it would make it non-proﬁtable compared to honest mining even after a diﬃculty adjustment. So this will keep the individual incentives properly aligned in the protocol rules, as intended in the original inception of Bitcoin [9]. Remark 1. There are some selﬁsh miners simulators available where the reader can test numerically the ﬁndings in this article (see [7] and [8]). Remark 2. The new theory developed in this article has been applied to Stubborn and Trail Mining strategies presented in [10]. The authors have solved completely the proﬁtability problem for these strategies in [5] and [6], where we give close-form formulas. These formulas are used to rigorously compare the proﬁtability of all these strategies and conﬁrm and correct previous numerical studies ([10]). Catalan numbers and the Catalan generating function appear naturally in these other problems.
Spectral classification of the mass donors in the high-mass X-ray binaries EXO 1722-363 and OAO 1657-415<|sep|>Within this paper we have presented the analysis and results of observations performed at ESO/VLT with the ISAAC spectrometer on the eclipsing high-mass X-ray binaries systems EXO 1722-363 and OAO 1657-415. Using NIR spectrometry we have constrained the previous spectral classiﬁcation of the mass donor in the EXO 1722-363 system from B0-B5 I to B0-BI Ia and determined its distance to be 8.0+2.5 −2.0 kpc. Examination of the OAO 1657-415 system has allowed us to determine that the donor in this system is more evolved than the typical OB supergiants found in other eclipsing high mass X-ray binaries. We have classiﬁed the donor within OAO 1657-415 as type Ofpe/WN9, a transitionary object between OB main sequence stars and hydrogen depleted Wolf-Rayet stars. Due to the large range in luminosity exibited by these types of stars it is diﬃcult to precisely perform distance calculations. Adopting a luminosity range of log(L/L⊙) ∼ 5.3 - 6.2 we determined a distance range of 4.4 ≤ d ≤ 12 kpc. The anomolous position of OAO 1657-415 on the Corbet diagram is explained by the more evolved (than typical OB supergiant XRBs) mass donor having a stronger, slower stellar wind, enabling the NS to increase its accretion rate and its rate of spin. This result reinforces our belief that the circumstellar environment from which a HMXB accretes from plays a crucial role in determining its X-ray properties. Results from the INTEGRAL γ-ray telescope (Walter et al. 2006) have lead to the discovery of two new distinct classes of SG XRB within the past decade. Supergiant Fast X-ray Transients (SFXT) (Negueruela et al. 2006) are believed to stem from periods of high accretion due to wind clumping, and obscured systems such as sgB[e] stars (Filliatre & Chaty 2004) have a very high X-ray extinction due to the density of the circumstellar environment. In addition to examining the spin and orbital periods of HMXB systems, it is of vital importance to also consider the properties of the donor’s wind. It is increasingly apparent that the X-ray properties of HMXBs are inﬂuenced greatly by the circumstellar environment in tandem with the mass-loss properties of the mass donor. We plan to present radial velocity studies of both systems in a future paper, where we shall attempt to determine the masses of the two components in each case. Acknowledgements. ABM acknowledges support from an STFC studentship. JSC acknowledges support from an RCUK fellowship. This research is partially supported by grants AYA2008-06166-C03-03 and Consolider-GTC CSD-200600070 from the Spanish Ministerio de Ciencia e Innovaci´on (MICINN). Based on observations carried out at the European Southern Observatory, Chile through programme ID 081.D-0073(A). We also thank the anonymous referee for their useful comments.
Decay Replay Mining to Predict Next Process Events<|sep|>In this paper, we introduced a novel approach to predict next events in running process cases called DREAM-NAP. Speciﬁcally, we extended the places of PN process models with decay functions to obtain timed state samples when replaying an event log. These timed samples are used to train a deep neural network which accurately predicts the next event in a running process case. Our results surpass many state-of-the-art techniques. We obtain cross-validated accuracies above 90% and show robust, precise performances across a diverse set of real-world event logs. This underscores the feasibility and usefulness of our proposed approach. We have shown that decay functions are a suitable tool to express a traditionally discrete PN state as a continuous representation during process runtime. In this way, we can incorporate timing information of processes directly into the process model. This is important for predictive tasks such as predicting the next event since the duration between two event instances might be correlated with a subsequent occurring event. While most recent techniques model processes implicitly, our approach is based on explicit process models. Therefore, our method is easier to interpret than algorithms which are based exclusively on deep learning. While decision making of neural networks is naturally hard to understand and explain [67]–[69], we are retaining an interpretable process model in combination with a simple deep learning architecture. Therefore, organizations will still be able to debug their processes using graphical representations of PNs while taking advantage of the predictive capabilities. A sensitivity analysis can be performed to interpret the decision making of the neural network which performs on top of the decay function extended PN process model. This paper introduced a promising novel approach with many potential real-world applications. High quality next event predictions are beneﬁcial for the efﬁcient control of real-time processes. Predicting future events helps organizations to improve scheduling, model ﬂexible demand, and reduce system waste which are high impact problems to tackle issues like climate change [70]. Other applications can be found in atypical process mining disciplines such as in black box controller logic estimation [71] that has the potential to release huge amounts of engineers from heavy-duty of repeated controller design work. The proposed approach can further be applied in the novel process mining discipline of Human-Computer Interaction [72]. Predicting upcoming user interactions with a given computer system might unveil error-prone or inefﬁcient interfaces and can be used to create accessible and interactive devices to overcome e.g. user impairments [73]. Finally, DREAM-NAP might have potential applications in Healthcare to understand patient’s medical records with the ultimate goal to optimize treatments and diagnoses. Further research can be conducted in the following three directions. First, the predictive quality might be able to be improved by incorporating quality performance measures of process discovery algorithms apart from ﬁtness scores to investigate the impact of the process model quality on the proposed approach. Additionally, repair methods might be beneﬁcial to increase the process quality and may have a positive impact on the predictive performance [74]–[76]. The outcome of such studies might further increase the metric scores on the next event prediction that we have reported. Second, we have applied the simplest kind of decay function. A comprehensive study of different decay function types might improve the predictive performance of our approach. Moreover, we proposed two deep learning architectures that have shown satisfying results on the evaluated benchmark datasets. Further optimized architectures might exist that increase the quality of predicting next events and that might overcome the low precision scores reported in Section VE. Finally, the presented approach has been applied to next event prediction only, but might apply to further predictive process management tasks such as remaining case time prediction, next event timestamp prediction, or anomalous process state predictions. C(τ) token counting vector from time 0 to τ, each element represents the number of tokens which entered a speciﬁc place
Investigating the Origins of Spiral Structure in Disk Galaxies through a Multiwavelength Study<|sep|>Our approach to testing density-wave theory is to look at the entire logarithmic spiral arm for differences in pitch angle. A different approach to investigating the same issue is to look for tracers of star formation and older stars in individual patches of the spiral arm. Recent studies of this type, such as Foyle et al. 2011 and Ferreras et al. (2012), report no consistent trend in positional offsets between these tracers. Thus, unlike us, they do not ﬁnd evidence for this prediction of the modal density wave theory, and report their results as favoring the picture of density waves as transient structures that do not persist long enough to produce consistent offsets. Similar conclusions are drawn by those who use the radial Tremaine–Weinberg method to ﬁnd results in support of the claim that the pattern speed of the spiral arm is actually radial-dependent (Merriﬁeld et al. 2006; Meidt et al. 2009; Speights & Westpfahl 2011). It is true that comparing B-band with NIR images ,the pitch-angle measurements, including the work of several different groups as in Figure 7, show no overall pitch angle difference. This is certainly a point in favor of the transient picture. However, our results are not consistent with this picture regarding the consistent difference we see between pitch angles in the starformation-tracing wavebands versus the stellar wavebands. Of course, we must recall that it has been proposed that different mechanisms for the formation of spiral arms operate in different galaxies. It is certainly possible that our sample favors grand-design spirals, with clearly logarithmic spirals and that as our sample is broadened, differences in behavior between individual galaxies will become more apparent. It may NIR spiral arms coincide and have a common origin, since their  amplitudes are equal. Admittedly, other observers have claimed that the amplitudes are not equal. Since we wish to point out the possibility  that the light from new stars is responsible for the spiral arm in  the NIR as well as the optical, let us discuss how this might be.  Figure 3 (Figure 5 on page 35 of Eufrasio 2015) gives the  spectrum of a cluster formed by a single burst of star formation  at different stages of its evolution. The three relevant curves to  note are the blue curve showing the spectrum of light from the  cluster at age 10 Myr, the green curve for its spectrum at  100 Myr, and the yellow curve for one at a billion years. At any  given position the luminosity at a given wavelength is  essentially a sum of the new stars (10 Myr or less for the star  formation pitch angle, and up to 100 Myr for the stellar pitch  angle) plus the background of old disk stars (the 1 Gyr line  or more). At 151 nm (FUV) we see that there is a very high  contribution from the new stars at 10m years and an essentially  zero contribution from background stars. There is a sharp drop  in the contribution from stars 100 Myr stars, so it is not  surprising that at 151 nm we see the stars in the position of the  star formation region. By the time 100 Myr has passed the  luminosity has greatly decreased. Note that the speed at which  stars move out of the star-forming region is slow because it is  the relative speed of the star to the pattern speed that counts,  which is typically some 20 or 30 pc/Myr. Looking at the u  band, which is at 365 nm, we see that the gap from 10 Myr to  100 Myr is a little less, but actually only a little. The background contribution is more signiﬁcant at the 1000  Myr line but it still signiﬁcantly decreased in luminosity from  the younger stars. Certainly, this suggests that the u-band spiral  arm might not be too dissimilar from the FUV arm and this is  what we see. Now, looking at the B band (445 nm) we see that  there is a noticeably smaller gap between the 10 Myr and 100  Myr curves, because of the bump in the 100 Myr curve that  falls between the u band and B band. The wavelength difference between the u band and B band is  not great, but there is still a difference in the reduction from the  10 Myr curve to the 100 Myr curve. Of course, it is not a huge  difference, which is not surprising given how close the two  wavelengths are. Therefore, one might also expect that there  would not be a big difference between the u band and B band  pitch angles and this is again what we see. Since the u band is  close to both the FUV and B band, but these two are  distinguishable from each other, this suggests that, on average,  u band pitch angles are a little tighter than the FUV but a little  looser than the B band, as suggested by Figure 4. Since the B  band is part of the optical complex of pitch angles and FUV is  part of the star-forming complex, it is in the u band that we see a  wavelength that falls between these two. One is tempted to see  an evolution here (which would require further scrutiny and  more data) from the FUV to the u band to the B band to the NIR,  with each step being a small decrease in pitch angle. Each step  in this sequence is too small for the difference in pitch angle to  be greater than our average measurement error (between two  and three degrees for pitch angles in this sample), but a double  step typically shows a large enough difference to be greater than  our measurement errors (as shown by the histograms in  Figures 4 and 5). Thus, there is a decisive difference between  the FUV and B band and between the u band and 3.6 μm. This  tends to strengthen our belief that what we are seeing is a loose Berlind, A. A., Quillen, A. C., Pogge, R. W., & Sellgren, K. 1997, AJ, 114, 107 Bertin, G., & Lin, C. C. 1995, Spiral structure in galaxies a density-wave theory (Cambridge, MA: MIT Press) Choi, Y., Dalcanton, J. J., Williams, B. F., et al. 2015, ApJ, 810, 9 Davis, B. L., Berrier, J. C., Shields, D. W., et al. 2012, ApJS, 199, 33 Davis, B. L., Berrier, J. C., Shields, D. W., et al. 2016, 2DFFT: Measuring Galactic Spiral Arm Pitch Angle, Astrophysics Source Code Library, ascl:1608.015 Elmegreen, B. G., Seiden, P. E., & Elmegreen, D. M. 1989, ApJ, 343, 602 Elmegreen, D. M., Elmegreen, B. G., Yau, A., et al. 2011, ApJ, 737, 32 Eufrasio, R. T. 2015, PhD thesis, The Catholic Univ. of America Fazio, G. G., Hora, J. L., Allen, L. E., et al. 2004, ApJS, 154, 10 Ferreras, I., Cropper, M., Kawata, D., Page, M., & Hoversten, E. A. 2012, Gittins, D. M., & Clarke, C. J. 2004, MNRAS, 349, 909 Goldreich, P., & Lynden-Bell, D. 1965, MNRAS, 130, 125 Graham, A. W., Soria, R., & Davis, B. L. 2019, MNRAS, 484, 814 Grand, R. J. J., Kawata, D., & Cropper, M. 2012, MNRAS, 421, 1529 Grosbol, P. J., & Patsis, P. A. 1998, A&A, 336, 840 James, P. A., & Seigar, M. S. 1999, A&A, 350, 791 Julian, W. H., & Toomre, A. 1966, ApJ, 146, 810 Kendall, S., Kennicutt, R. C., Clarke, C., & Thornley, M. D. 2008, MNRAS, Kennicutt, R. C., Jr. 1981, AJ, 86, 1847 Kim, Y., & Kim, W.-T. 2013, MNRAS, 440, 208 Lin, C. C., & Shu, F. H. 1964, ApJ, 140, 646 Louie, M., Koda, J., & Egusa, F. 2013, ApJ, 763, 94 Martínez-García, E. E. 2012, ApJL, 744, 92 Martínez-García, E. E., Puerari, I., Rosales-Ortega, F. F., et al. 2014, ApJL, Meidt, S. E., Rand, R. J., & Merriﬁeld, M. R. 2009, ApJ, 702, 277 Merriﬁeld, M. R., Rand, R. J., & Meidt, S. E. 2006, MNRAS, 366, L17 Peterken, T., Merriﬁeld, M., Aragón-Salamanca, A., et al. 2019, NatAs, 3, 178 Pour-Imani, H., Kenneﬁck, D., Kenneﬁck, J., et al. 2016, ApJL, 827, L12 Rix, H.-W., & Rieke, M. J. 1993, ApJ, 418, 123 Rix, H.-W. R. 1991, PhD thesis, Univ. Arizona Roberts, W. W. 1969, ApJ, 158, 123 Salo, H., Laurikainen, E., Laine, J., et al. 2015, ApJS, 219, 4 Savchenko, S. S., & Reshetnikov, V. P. 2011, AstL, 37, 817 Seigar, M. S., Bullock, J. S., Barth, A. J., & Ho, L. C. 2006, ApJ, 645, 1012 Seigar, M. S., Kenneﬁck, D., Kenneﬁck, J., & Lacy, C. H. S. 2008, ApJL, Sellwood, J. A., & Binney, J. J. 2002, MNRAS, 336, 785 Sellwood, J. A., & Carlberg, R. G. 1984, ApJ, 282, 61 Sellwood, J. A., & Carlberg, R. G. 2011, ApJ, 785, 137 Shields, D. W., Boe, B., Pfountz, C., et al. 2015a, arXiv:1511.06365 Shields, D. W., Boe, B., Pfountz, C., et al. 2015b, Spirality: Spiral arm pitch angle measurement, Astrophysics Source Code Library, ascl:1512.015 Shu, F. H. 2016, ARA&A, 54, 667 Speights, J. C., & Westpfahl, D. J. 2011, ApJ, 736, 70 Wright, G. S., Casali, M. M., Walther, D. M., & McLean, I. S. 1991, in ASP Conf. Ser. 14, Astrophysics with Infrared Arrays, ed. R. Elston (San Francisco, CA: ASP), 44 be true that some galaxies have transient spirals and others have  more long-lived ones. We have presented clear evidence in favor of a prediction of  the density-wave theory that there is a tighter pitch angle in the  B band then is found for wavelengths associated with the starforming region. We have built upon the result in our earlier paper (PourImani et al. 2016) by adding one more star-forming  wavelength, the H-α, which ﬁts the pattern established by the  8.0 μm and FUV wavelengths from the earlier paper. We have  also found that the u-band light seems to show pitch angles  tending to fall between those of the star-forming region and the  B band, consistent with the standard interpretation of the  density-wave theory. We regard our results as strongly in favor  of the density-wave theory, though more work is certainly  needed to reconcile our results with those reported from other  methods. We do ﬁnd evidence to support those earlier works  which see only small differences in pitch angle between optical  and NIR wavelengths. Broadly, we see two different systems, a  star-forming region visible in the 8.0 μm, H-α, and FUV and  stellar light downstream visible in the B band and the NIR. The  u-band light, predictably, falls between these two systems.  Thus we argue that we are seeing the star-forming region and  then light from recently born stars moving downstream from  that position. This is consistent with the long-lived densitywave theory of spiral structure. The authors thank Bret Lehmer and Jerry Sellwood for  valuable suggestions that guided our research. This research  has made use of the NASA/IPAC Extragalactic Database,  which is operated by the Jet Propulsion Laboratory, California  Institute of Technology, under contract with the National  Aeronautics and Space Administration.
A Simple Yao-Yao-Based Spanner of Bounded Degree<|sep|>We have shown that the Yao-Yao graph is a spanner for UDGs of bounded aspect ratio. We have also proposed an extension of the Yao-Sink method, called Yao-Sparse-Sink, that enables an eﬃcient local computation of sparse sink trees. The Yao-Sparse-Sink method is preferable to the Yao-Sink method for topology control in highly dynamic wireless environments. Our analysis of the Yao-Sparse-Sink method provides additional insight into the properties of the Yao-Yao structure. However, the main question of whether the Yao-Yao graph for arbitrary UDGs is a length spanner or not remains open.
A Unified, Hardware-Fitted, Cross-GPU Performance Model<|sep|>Being able to reason and make predictions about the wall time cost of a given computation is a foundational capability for a large number of activities in high-performance computing: • In algorithm design, it can provide guidance on which aspects of the workload under consideration are the biggest contributors to computational cost. Addition/Subtraction 6.81e-13 Multiplication 5.68e-13 Exponentiation 3.91e-13 Other Ops (rsqrt) 1.61e-12 Local F32 Loads -1.76e-12 F32 Stride-1 Loads 8.27e-12 F32 Stride-2 (100%) Loads 9.82e-13 F32 Stride-3 ( 33%) Loads 2.89e-11 F32 Stride-3 (100%) Loads 9.30e-13 F32 Uncoalesced (100%) Loads 2.67e-12 F32 Stride-1 Stores 6.52e-12 F32 Uncoalesced (100%) Stores 3.55e-10 Min(Stride-1 Loads, Stride-1 Stores) -6.63e-12 Barriers 4.26e-11 Thread Groups 3.75e-09 Const(1) 1.29e-04 TABLE 2. EXAMPLE SET OF PROPERTY WEIGHTS (IN UNITS OF SECONDS PER OPERATION) PRODUCED FOR THE AMD RADEON R9 FURY. PERCENTAGES REPRESENT THE UTILIZATION RATIO DISCUSSED IN SECTION 2.1 • In load balancing, accurate predictions of workload run times enable better scheduling decisions, thereby facilitating the reduction of idle time and making better use of available computational resources. This need is particularly salient when a workload is to be moved across heterogeneous compute resources. • In machine bringup and qualiﬁcation, our measurement procedure can expose bottlenecks as well as unexpected interactions and help enable comparisons between different processor architectures. Our work permits a number of immediate extensions. Perhaps the most obvious one of these would be to investigate how much information on potentially overlapped operations can be obtained through a ﬁtted model. A prominent example of this would be overlapping arithmetic with data motion. Another possible extension would handle resource limitations on, say, the number of registers, the amount of local memory, and their respective effects on performance. Other aspects of GPU execution cost may be simpler to account for. For example, bank conﬂicts in local memory may be handled by binning the stride of local memory access. Another interesting extension would be to study our model’s ability to select the optimal set of kernel conﬁgurations (i.e., the set that produces the fastest kernel) from a collection of potential optimizations. This ability, combined with the rapid evaluation speed of our model, would enable runtime performance tuning of GPU kernels. Another immediate extension of this work would be to examine its applicability on CPU-type architectures. For a. 0.32 0.41 0.44 0.40 0.70 0.70 0.16 0.22 b. 1.03 1.39 1.35 1.21 2.37 2.42 0.27 0.48 c. 4.27 5.32 4.98 4.46 9.17 9.31 0.89 1.55 d. 15.33 21.05 19.55 17.43 37.34 36.87 3.23 5.81 a. 0.18 0.14 0.28 0.18 0.27 0.14 0.25 0.14 b. 0.56 0.55 0.58 0.51 0.41 0.28 0.41 0.18 c. 3.52 3.81 3.35 3.16 1.65 1.33 0.87 0.55 d. 27.23 29.73 23.26 24.23 9.62 9.71 3.23 3.44 a. 0.48 0.16 1.06 0.48 0.99 0.24 0.39 0.14 b. 0.90 0.38 2.67 1.51 1.99 0.59 0.64 0.15 c. 1.83 1.29 7.41 5.66 4.26 2.01 1.31 0.22 d. 4.49 4.90 24.58 22.26 10.90 7.66 2.32 0.49 a. 0.49 0.47 0.34 0.25 0.43 0.33 0.28 0.19 b. 1.54 1.64 0.62 0.60 1.08 0.96 0.43 0.35 c. 5.73 6.32 1.73 2.01 3.49 3.48 1.13 1.02 d. 19.32 25.04 6.19 7.65 13.30 13.56 6.75 3.69 these types of machines, data motion cost would necessarily need to include a model of cache and data reuse. It would also be interesting to investigate to what extent a version of this model can apply to current and future wide-vector manycore accelerators of the Xeon Phi and related families. • It describes a procedure for the automatic extraction of symbolic counts from the internal representation of our transformation engine, based on the polyhedral model. The representation of these counts as piecewise quasi-polynomials offers both efﬁcient evaluation and considerable generality. • It describes a set of measurements as well as a ﬁtting procedure to, once again in a black box and unassisted fashion, determine hardware- speciﬁc weights for each of the properties determined above. We have demonstrated an alternative to previous GPU performance models that can be easily ﬁtted to new hardware and allows rapid, runtime performance prediction. This speed and versatility turns out to require minor if any sacriﬁces in prediction accuracy compared to models in the literature. To our knowledge, this is the ﬁrst GPU The authors’ work was supported in part by US Navy ONR grant numbers N00014-14-1-0117, and by the National Science Foundation under grant numbers DMS1418961 and CCF-1524433. AK also gratefully acknowledges a hardware gift from Nvidia Corporation. Opinions expressed herein are those of the authors and in no way reﬂect the ofﬁcial position of any of the funding agencies.
Electronic structure of GdN, and the influence of exact exchange<|sep|>It was demonstrated that Gaussian type orbitals are technically capable for performing calculations on GdN bulk with f electrons explicitly treated. The LDA results are in very good agreement with previous calculations and give a half-metallic solution, with the majority bands being conducting and the minority bands insulating. The HartreeFock solution is insulating with a large gap of ∼ 5 eV, which is a typical overestimation of the gap at the Hartree-Fock level due to the lack of screening. On the B3LYP level, three solutions were found. The lowest one has a gap for majority spin; and for minority spin, the valence and conduction bands only touch at certain points of the Brillouin zone. The corresponding density of states is thus very small around the Fermi energy. The second solution is very close in energy (∼ 0.1 eV higher), and the majority bands cross the Fermi energy. A third solution was found to be insulating, but this third solution is about 2.4 eV higher in energy than the lowest solution. The fact that there are two nearly degenerate solutions makes the comparison with the experiment very diﬃcult, and it is diﬃcult to judge the performance of the hybrid functional B3LYP in this case. Figure 6. B3LYP density of states at the computed equilibrium lattice constant of 5.10 ˚A, for the energetically most favorable solution. Besides the total density of states, the projected density of states is shown for projections on N, Gd d, Gd f states. The Fermi energy is positioned at 0. It is however also very interesting that two diﬀerent solutions with very diﬀerent Fermi surfaces are obtained in the calculations. As the experimental situation is not fully clear, experiments such as photoemission on this system might be very interesting to further elucidate the electronic structure of GdN (indeed, spin and angle resolved inverse photoemission spectroscopy was also suggested in another recent theoretical work [37]).
Alkali vapor pressure modulation on the 100ms scale in a single-cell vacuum system for cold atom experiments<|sep|>The pulsed dispenser device demonstrated here provides alkali pressure modulation by more than an order of magnitude on the 100 ms timescale in both directions. While fast pressure rise with pulsed dispensers has been demonstrated before, the important new feature of our device is fast pressure decay after the pulse. Although the plateau pressure reached after this decay is above the base pressure, it is lower than the cw pressure in a similarly-sized nonpulsed MOT, while loading is at least an order of magnitude faster. This performance is achieved in a single-cell setup with only marginal added complexity over a standard MOT setup. These features make our device attractive for many applications of ultracold atoms, including single-cell Bose-Einstein condensation experiments with sub-second repetition rate and cold atom sensors for ﬁeld use. Several further improvements are possible. Our test setup had small beam 1/e2 beam diameters of 1.2 cm. Increasing this beam size would dramatically increase the number of trapped atoms25 for the same alkali pressure, without any changes to the dispenser device itself. Using current pulses with an initial “boost” instead of square pulses would reduce the delay before alkali emission, further reducing the loading time. Total loading times below 300 ms are realistic with this improvement alone (cf. Fig. 3). As discussed in sec. IVA, shortening the copper holding bars should signiﬁcantly reduce their temperature rise and the associated transient behavior. Furthermore, having shown that adsorption dynamics is an important factor for the plateau pressure, we expect that this pressure can be further reduced by improving the cell geometry and possibly by a suitable choice of cell materials. Our pulsed dispenser can also be combined with a nozzle such as described in18 to direct atoms more eﬃciently towards the capture region. This may reduce the overall amount of alkali vapor released in every pulse. Finally, the pulsed dispenser can be combined with lightinduced desorption (LIAD) to further increase the peak pressure and reduce the surface coverage during the lowpressure intervals. In a ﬁrst test, we have observed a reduction of the 87Rb pressure in the science phase by roughly 1/3 when applying LIAD pulses simultaneously with dispenser pulses.
The Design and Fabrication of Platform Device for Dna Amplification<|sep|>A simple apparatus for rapid DNA amplification was  developed in this research. The major components of the  PCR platform included DC motor, tappet, heater plates,  PMMA frame, and the temperature control system. The  DNA chamber of 300μm depth was fabricated using the  wet anisotropic KOH solution to make the side wall of  54.74°. An excellent thermal transition rate of chamber  was obtained due to the high thermal conductivity of  silicon. Finally, a 100 bp segment of E. coli K12 was  demonstrated successfully for amplification. The total reaction time took 36minutes with 1µl volume for 30  cycles. However, the PCR platform was an easy and lowcost apparatus to be fabricated. It could be used in  common Lab.
On Neural Network Equivalence Checking using SMT Solvers<|sep|>In this work, we formally deﬁned the equivalence checking problem for neural networks and we introduced a series of equivalence criteria that might be more appropriate than others for speciﬁc applications and veriﬁcation requirements. Furthermore, we provided a ﬁrst SMT-based encoding of the equivalence checking problem, as well as experimental results that demonstrate its sanity and give insight into its current scalability limitations. In our future research plans, we aim to explore whether the equivalence checking problem (and our equivalence criteria) can be encoded in state-of-the-art veriﬁcation tools for neural networks (e.g. Reluplex [10], ERAN [17], α − β Crown [21, 20, 19], VNN competition [2]) through the parallel composition of the two networks that are to be compared. As additional research priorities, we also intend to explore the scalability margins of alternative solution encodings, including an optimized version of our current encoding (by elimination of the internal SMT variables) and a mixed-integer linear programming encoding. Lastly, it may be also worth to explore the practical eﬀectiveness of technical solutions to similar problems from other ﬁelds, like for example the equivalence checking of digital circuits [7, 14, 6]. In this context, we may need to rely on novel ideas towards the layer-by-layer checking of equivalence between two neural networks.
Chromoelectric Dipole Moments of Quarks in MSSM Extensions<|sep|>supersymmetric loop contributions. In the non-supersymmetric sector we have contributions arising from the exchanges of the W and Z bosons in the loops, while in the supersymmetric it is found that there exists strong interference eﬀects between the MSSM sector and the vectorlike quark sector which can drastically change both the sign and the magnitude of improve the current limits up to two orders of magnitude and thus the quark EDMs provide an important window to new physics beyond the standard model.
Measurement of light charged particles in the decay channels of medium-mass excited compound nuclei<|sep|>The decay of 88Mo at 1.4 and 3.0 MeV/u excitation energy is under study. Proton emission in the FE channel is well reproduced by the Gemini++ statistical code run with standard input values. An excess of α particles has been found at both bombarding energies also associated to a slightly diﬀerent CM-energy spectrum with respect to the predicted one. Work is in progress to better quantify this excess and to understand if it could be accounted for by suitable variations of model parameters, in particular to better describe the shape deformation at high spin as pointed out in [8], or it comes for other sources. It is also in progress the study of the ﬁssion channel, whose description is important for shape transitions as it is associated to the highest spins where such shape changes are expected.
The MAGIC project. III. Radial and azimuthal Galactic abundance gradients using classical Cepheids<|sep|>We report the chemical composition (25 elements) of 105 classical Cepheids. Abundances have been derived from 122 high-resolution SALT spectra observed in the context of the MAGIC project. The Galactocentric distances of the Cepheids are taken from the literature, they are based on mid-infrared photometry. Our new sample contains 25 Cepheids with RG <7 kpc (14 with RG < 6.5 kpc), and 14 Cepheids with RG > 11 kpc, thus signiﬁcantly improving the sampling in the inner and outer disc. Our results are in line with Cepheids literature studies concerning radial abundance gradients of iron, α, or neutron-capture elements. We focus on the azimuthal variations of the oxygen abundance [O/H]. Since such variations are found to be negligible (typically < 0.05 dex) in external spirals, the Milky Way seems to be one of the few galaxies with noticeable [O/H] azimuthal inhomogeneities. We ﬁnd that they are the largest in the inner Galaxy, where they are usually attrib uted to perturbations induced by the rotating bar, and in the outer disc. In these outer regions, our results support similar results obtained in a few nearby spiral galaxies as well as the outcome of chemo-dynamical evolution models by e.g., Spitoni et al. (2019); Moll´a et al. (2019).
Spectrum of the Anomalous Microwave Emission in the North Celestial Pole with WMAP 7-Year data<|sep|>We have studied the spectrum of the diﬀuse AME with WMAP 7-yr and ancillary data in the North Celestial Pole (NCP) region of the sky. In this region the AME dominates the low frequency emission, as both synchrotron and free-free are faint. Previous template-ﬁtting analysis by [6] found that the AME spectrum in this region is consistent with a power-law; the same would apply to most diﬀuse AME at intermediate latitudes ([2], [6], [4], [9], [25], [15]). This favors a low peak frequency (νp < 23 GHz). For our analysis we rely on the CCA component separation method, which exploits the data auto- and cross-spectra to estimate the frequency spectra of the components in terms of a set of spectral parameters. Our method models the AME as a peaked spectrum and ﬁts for the peak frequency, νp, and the slope at 60 GHz, m60. We veriﬁed with simulations that we are able to correctly recover the AME spectrum and more speciﬁcally the peak frequency, even when it is below 23 GHz. We get νp = 21.7 ± 0.8 GHz, which is both a conﬁrmation and an improvement of the previous results. This result relies on the assumption that our parametric model for the AME is a good representation of the true spectrum, which has been veriﬁed for a wide range of theoretical spinning dust models. Using the SpDust code, we linked the estimated spectrum to the local physical conditions with the hypothesis of spinning dust emission. We investigated in particular the hydrogen density (nH [cm−3]), which is sensitive to the position of the peak, and its degeneracy with the gas temperature T [K]. The densities that we get are those typical of WIM/WNM conditions (0.2–0.4 cm−3). For a low hydrogen ionization fraction (nH ≤ 10−2), densities up to a few cm−3 are allowed by the nH–T degeneracy. Lower radiation ﬁelds (χ) require slightly lower nH. By considering 100 µm data for our region we obtain a hydrogen column density NH ∼ 1020 cm−2. Overall, the recovered AME spectrum is found to be consistent with that predicted by spinning dust models for plausible physical conditions.
DeepType: Multilingual Entity Linking by Neural Type System Evolution<|sep|>In this work we introduce DeepType, a method for integrating symbolic knowledge into the reasoning process of a neural network. We’ve proposed a mixed integer reformulation for jointly designing type systems and training a classiﬁer for a target task, and empirically validated that when this technique is applied to EL it is effective at integrating symbolic information in the neural network reasoning process. When pre-training with DeepType for NER, we observe improved performance over baselines and a new state of the art on the OntoNotes dev set, suggesting there is cross-domain transfer: symbolic information is incorporated in the neural network’s distributed representation. Furthermore we ﬁnd that type systems designed by machines outperform those designed by humans on three benchmark datasets, which is attributable to incorporating learnability and target task performance goals within the design process. Our approach naturally enables multilingual training, and our experiments show that bilingual training improves over monolingual, and type systems optimized for English operate at similar accuracies in French, German, and Spanish, supporting the claim that the type system optimization leads to the discovery of high level cross-lingual concepts useful for knowledge representation. We compare different search techniques, and observe that stochastic optimization has comparable performance to heuristic search, but with orders of magnitude less objective function evaluations. The main contributions of this work are a joint formulation for designing and integrating symbolic information into neural networks, that enable us to constrain the out puts to obey symbolic structure, and an approach to EL that uses type constraints. Our approach reduces EL resolution complexity from O(N 2) to O(N), while allowing new entities to be incorporated without retraining, and we ﬁnd on three standard datasets (WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) that our approach outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system (Ling, Singh, and Weld 2015) and the more recent work by Yamada et al. for embedding words and entities (Yamada et al. 2016), or document and entities (Yamada et al. 2017). As a result of our experiments, we observe that disambiguation accuracy using Oracles reaches 99.0% on CoNLL (YAGO) and 98.6% on TAC KBP 2010, suggesting that EL would be almost solved if we can close the gap between type classiﬁers and the Oracle. The results presented in this work suggest many directions for future research: we may test how DeepType can be applied to other problems where incorporating symbolic structure is beneﬁcial, whether making type system design more expressive by allowing hierarchies can help close the gap between model and Oracle accuracy, and seeing if additional gains can be obtained by relaxing the classiﬁer’s conditional independence assumption. Acknowledgments We would like to thank the anonymous reviewers for their valuable feedback. In addition, we thank John Miller, Andrew Gibiansky, and Szymon Sidor for thoughtful comments and fruitful discussion. A Training details and hyperparameters Optimization Our models are implemented in Tensorﬂow and optimized with Adam with a learning rate of 10−4, β1 = 0.9, β2 = 0.999, ϵ = 10−8, annealed by 0.99 every 10,000 iterations. To reduce over-ﬁtting and make our system more robust to spelling changes we apply Dropout to input embeddings and augment our data with noise: swap input words with a special <UNK> word, remove capitalization or a trailing “s.” In our NER experiments we add Gaussian noise during training to the LSTM weights with σ = 10−6. We use early stopping in our NER experiments when validation F1 score stops increasing. Type classiﬁcation model selection is different as the models did not overﬁt, thus we instead stop training when no more improvements in F1 are observed on held-out type-training data (∼ 3 days on one Titan X Pascal). Architecture Character representation Our character-convolutions have character ﬁlters with (width, channels): {(1, 50), (2, 75), (3, 75), (4, 100), (5, 200), (6, 200), (7, 200)}, a maximum word length of 40, and 15-dimensional character embeddings followed by 2 highway layers. We learn 6-dimensional embeddings for 2 and 3 character preﬁxes and sufﬁxes. Text Window Classiﬁer The text window classiﬁers have 5-dimensional word embeddings, and use Dropout of 0.5. Empirically we ﬁnd that two passes through the dataset with a batch size of 128 is sufﬁcient for the window classiﬁers to converge. Additionally we train multiple type axes in a single batch, reaching a training speed of 2.5 type axes/second. Link statistics collected on large corpuses of entity mentions are extensively used in entity linking. These statistics provide a noisy estimate of the conditional probability of an entity e for a mention m P(e|m). Intra-wiki links in Wikipedia provide a multilingual and broad coverage source of links, however annotators often create link anaphoras: “king” → Charles I of England. This behavior increases polysemy (“king” mention has 974 associated entities) and distorts link frequencies (“queen” links to the band Queen 4920 times, Elizabeth II 1430 times, and monarch only 32 times). Problems with link sparsity or anaphora were previously identiﬁed, however present solutions rely on pruning rare links and thus lose track of the original statistics (Ferragina and Scaiella 2010; Hasibi, Balog, and Bratsberg 2016; Ling, Singh, and Weld 2015). We propose instead to detect anaphoras and recover the generic meaning through the Wikidata property graph: if a mention points to entities A and B, with A being more linked than B, and A is B’s parent in the Wikidata property graph, then replace B with A. We deﬁne A to be the parent of B if they connect through a sequence of Wikidata properties {instance of, subclass of, is a list of}, or through a single edge in {occupation, position held, series16}. The simpliﬁcation process is repeated until no more updates occur. This transformation reduces the number of associated entities for each mention (“king” senses drop from 974 to 143) and ensures that the semantics 15The choice of pstart affects the system size at the ﬁrst step of the CEM search: setting it too low leads to poor search space exploration, while too high increase the cost of the objective function evaluation. Empirically we know that for a given λ the solution will have an expected size s. Setting pstart = s |R| leads to sufﬁcient exploration to reach the performance of larger pstart. 16e.g. Return of the Jedi → series Star Wars
Chemical tracers of high-metallicity environments<|sep|>The main results of this theoretical study are contained in Section 4 which shows how the chemistry evolves with respect to the changes in various parameters likely to be appropriate for high-metallicity environments such as ETGs: FUV radiation ﬁeld, density, cosmic-ray ionisation rate, metallicity (increase up to 3 times solar), and an enhancement in the α−elements. The impact of the last two parameters on the chemistry have never been investigated so far in such detail. Table 4 and Figs. 2-6 are thus of particular interest, especially when considering the fact that at high metallicity the inﬂuence of the FUV radiation ﬁeld, density and cosmic-ray ionisation rate on the chemistry does not differ signiﬁcantly from what has been previously shown by e.g. Bayet et al. (2009b); Papadopoulos (2010); Meijerink et al. (2011); Bayet et al. (2011). From our study, it appears that the best observable tracers of metallicity are C+, C2H, CN, HCN, HNC, OCS and CO whereas the most likely observable chemical tracers for probing enhancement in α−elements are C+, C, CN, HCN, HNC, SO, SO2, H2O and CS. Fractional abundances and line brightness ratios with respect to CS (insensitive to metallicity changes) and to CO (insensitive to α−element changes) provide more quantitative values, useful for future follow-up observational programmes. Powerful tracers of metallicity changes have been identiﬁed (from the most sensitive to the least): the HCN/CS and HNC/CS line brightness ratios at all frequency for a gas with Av either smaller than 3 mag or of about 8 mag, line brightness OI/CS(2-1) ratios for a gas with Av ⩽ 4 mag and OCS/CS line brightness ratios at all frequencies for gas with Av ⩽ 4 mag. The ideal line brightness ratio for probing the enhancement of α−elements is the SO/CO whatever the optical extinction and the frequency of the lines involved. One notes that ratios involving two CO lines are not the best probes of either metallicity or α−element enhancement. However when CO(6-5)/CO(1-0) line brightness ratio is used and the error on the ratio is small, some reasonable estimate of the metallicity can be obtained. This however may be diﬃcult without a good treatment of beam dilution eﬀects. To conclude, if CS data are obtained in sources like ETGs, HCN/CS line brightness ratios can be calculated and from the results displayed here, we may be able to estimate the molecular gas metallicity in these sources which could put strong constraints on the origin of the gas in ETGs. This work can also be useful in predicting metallicity independently from previously used (i.e. mainly optical) methods in other high metallicity environments such as galaxy centres, etc. Telescopes such as ALMA and the IRAM-30m telescope are amongst the most privileged instruments to use for this study since large backends are available, hence observing simultaneously several molecular lines of CS, HCN, etc is possible. Table 4. Fractional abundance ratios which are likely the best tracers of high-metallicity environments for various models (see characteristics in Tables 1 to 3). Av = 1 mag Model 0 1.40(1) 4.97(-1) 3.98 5.57 3.44 4.10(-1) 2.68(5) 5.23(-5) 3.70(-1) 1.49(-5) 2.08(-5) 1.29(-5) 5.79(-7) 9.76(-10) 1.41(-5) Model 8 2.87 2.10(-1) 1.31 9.69 4.49 1.53 3.50(5) 8.21(-6) 2.50(-1) 3.75(-6) 2.77(-5) 1.28(-5) 2.58(-7) 1.20(-9) 6.47(-6) Model 12 3.15 3.00(-1) 1.26 1.11(1) 4.81 1.37 2.85(5) 1.11(-5) 4.20(-1) 4.42(-6) 3.91(-5) 1.69(-5) 1.96(-7) 1.07(-8) 5.17(-6) Model 19 2.76 1.00(-2) 6.50(-1) 1.29 5.50(-1) 7.30(-1) 2.32(5) 1.19(-5) 2.30(-1) 2.81(-6) 5.54(-6) 2.39(-6) 1.32(-6) 9.98(-9) 3.05(-5) Av = 3 mag Model 0 2.1(-2) 1.02(-2) 4.26(-2) 3.14(-1) 2.06(-1) 2.06(-1) 2.81(3) 7.40(-6) 1.23(-1) 1.52(-5) 1.12(-4) 7.34(-5) 4.29(-6) 4.30(-7) 9.56(-4) Model 8 5.07(-3) 2.06(-4) 9.33(-3) 7.19(-1) 3.29(-1) 1.05 4.98(3) 1.15(-6) 5.23(-2) 1.87(-6) 1.44(-4) 6.61(-5) 1.67(-6) 4.56(-7) 5.42(-4) Model 12 4.71(-3) 1.48(-4) 7.50(-4) 8.36(-1) 3.52(-1) 1.61 6.96(3) 6.76(-7) 4.19(-2) 1.08(-6) 1.20(-4) 5.06(-5) 1.29(-6) 3.77(-7) 3.55(-4) Model 19 1.37(-3) 1.33(-6) 6.20(-3) 1.16(-2) 5.23(-3) 1.22(-1) 7.55(2) 1.81(-6) 3.62(-2) 8.21(-6) 1.54(-5) 6.92(-6) 5.37(-5) 2.91(-5) 2.99(-3) Av = 5 mag Model 0 1.32(-3) 5.83(-5) 4.10(-3) 4.52(-2) 3.17(-2) 8.68(-2) 2.25(2) 5.89(-6) 9.54(-2) 1.82(-5) 2.01(-4) 1.41(-4) 8.38(-6) 4.55(-6) 7.31(-3) Model 8 3.54(-4) 1.34(-5) 5.62(-4) 1.17(-1) 5.71(-2) 5.07(-1) 3.26(2) 1.09(-6) 5.27(-2) 1.72(-6) 3.58(-4) 1.75(-4) 9.56(-7) 2.03(-6) 5.19(-3) Model 12 2.15(-4) 7.23(-6) 2.62(-4) 1.19(-1) 5.37(-2) 7.21(-1) 3.18(2) 6.78(-7) 4.61(-2) 8.23(-7) 3.72(-4) 1.69(-4) 2.35(-7) 7.73(-7) 3.88(-3) Model 19 6.27(-5) 2.53(-9) 6.60(-4) 1.00(-3) 8.01(-4) 4.27(-2) 5.39(1) 1.16(-6) 5.46(-3) 1.23(-5) 1.86(-5) 1.49(-5) 6.40(-4) 2.13(-3) 2.94(-2) Av = 8 mag Model 0 7.25(-4) 2.50(-5) 2.83(-3) 2.23(-2) 1.56(-2) 4.75(-2) 1.39(2) 5.19(-6) 7.28(-2) 2.03(-5) 1.60(-4) 1.12(-4) 1.58(-5) 7.68(-6) 8.66(-3) Model 8 2.58(-4) 8.83(-6) 5.92(-4) 7.57(-2) 3.64(-2) 3.41(-1) 2.56(2) 1.01(-6) 4.61(-2) 2.31(-6) 2.96(-4) 1.42(-4) 2.60(-6) 4.68(-6) 5.48(-3) Model 12 1.71(-4) 5.80(-6) 2.71(-4) 9.20(-2) 4.13(-2) 5.50(-1) 2.61(2) 6.53(-7) 4.33(-2) 1.03(-6) 3.51(-4) 1.58(-4) 7.71(-7) 2.24(-6) 4.68(-3) Model 19 3.59(-5) 5.03(-11) 7.78(-5) 2.62(-4) 1.93(-4) 1.63(-2) 3.48(1) 1.03(-6) 7.75(-4) 2.23(-6) 7.52(-6) 5.53(-6) 6.02(-3) 1.68(-2) 3.61(-2) Av = 20 mag Model 0 6.74(-4) 2.05(-5) 2.63(-3) 2.11(-2) 1.47(-2) 3.61(-2) 1.38(2) 4.88(-6) 6.20(-2) 1.90(-5) 1.55(-4) 1.06(-4) 1.89(-5) 7.20(-6) 8.53(-3) Model 8 1.65(-4) 4.10(-6) 6.02(-4) 3.80(-2) 1.76(-2) 1.82(-1) 1.91(2) 8.64(-7) 3.38(-2) 3.14(-6) 1.99(-4) 9.21(-5) 7.12(-6) 9.41(-6) 5.42(-3) Model 12 1.16(-4) 2.97(-6) 3.68(-4) 4.83(-2) 2.07(-2) 3.03(-1) 2.12(2) 5.49(-7) 3.17(-2) 1.74(-6) 2.27(-4) 9.77(-5) 3.93(-6) 7.84(-6) 4.63(-3) Model 19 3.37(-5) 3.72(-11) 4.15(-5) 1.27(-4) 4.91(-5) 1.11(-2) 3.27(1) 1.03(-6) 6.54(-4) 1.27(-6) 3.88(-6) 1.50(-6) 7.59(-3) 1.63(-3) 3.57(-2)
Opinions within Media, Power and Gossip<|sep|>In this paper, the dynamics of two relatively independent opinions in a simulated network is observed. Plunged into the network, agents characterized as
Weyl locally integrable conformal gravity, rotation curves and cosmic filaments<|sep|>century and in the past years. One of the reasons for this interest relies on the open issues in astrophysics and in cosmology, which gave rise to alternative theories of gravitation. In this paper we have considered the case of locally integrable (non-exact) Weyl conformal geometry, perhaps the simplest extension of Einstein’s theory of general relativity. The ﬁrst point relates to the topological constraints which exclude simply connected spacetime manifolds and imply linear singularities. The main result is the Weyl-Einstein tensor which includes a function in place of the cosmological constant Λ. The toy model of last section is based on a Schwarzschild metric and therefore it is far from addressing main issues of astrophysics. However this simple example suggests that the Weyl locally integrable conformal gravity provides a slightly diﬀerent starting point for both rotations curves of spiral galaxies and suggests a topological basis for cosmic ﬁlament. I would like to thank E. Scholz for fruitful discussions, for his support and his encouragements.
Nonlocal Exchange Interactions in Strongly Correlated Electron Systems<|sep|>In this work, we investigated the properties of a Hubbard-Heisenberg model, which interpolates between many mechanisms for magnetism, i.e. the Slater-, Stoner- and Heisenberg-picture. For the realistic SU(2)case, we presented the ﬁrst U-J phase diagram for a half-ﬁlled square lattice which goes beyond static meanﬁeld theory, by employing a Hubbard model with broken spin symmetry as an eﬀective Hamiltonian through FIG. 6. Double occupancy pf the Hubbard-Heisenberg model as function of nearest-neighbor Heisenberg exchange J at a ﬁxed U/t = 3. The shaded, grey area is meant to mark uncertainties resulting from ﬁnite size eﬀects in the DQMC data. the use of the Feynman-Peierls-Bogoliubov variational principle. While both interactions present in the system lead to the formation of local magnetic moments, the interplay between the two (e.g. the competition between Slater-type antiferromagnetism and Heisenberg-type ferromagnetism) can lead to non-monotonous behaviour in properties such as the double occupancy. Compared to the discrete transitions between areas with dominant antiferromagnetic and ferromagnetic correlations, obtained within the Hartree-Fock mean-ﬁeld treatment, the variational approach leads to continuous transitions. This work has been performed within the research program of the DFG Research Training Group Quantum Mechanical Materials Modeling (QM3) (Project P3). We thank Erik van Loon for many insightful discussions. The authors furthermore acknowledge the North-German Supercomputing Alliance (HLRN) for providing resources and the computing time necessary for carrying out the DQMC simulations. The mean-ﬁeld solutions for the 4-site model and the square lattice can in principle be obtained by the decoupling of the interaction terms in the original Hamiltonian (Eq.(2)). Here, however, we use the variational principle (which leads to a completely analogous solution) by employing a non-interacting, eﬀective Hamiltonian ˜H which allows both for ferro- and antiferromagnetism through two eﬀective ﬁelds: Noteably, the staggered magnetic ﬁeld with the magnitude ˜B2 breaks translational symmetry due to the induced Nel-order. Hence, the original square lattice is divided into two sublattices, leading to two distinct fermionic operators for the respective sublattices. The parameters ( ˜B1, ˜B2) are chosen variationally for each set of original (U,J). We can solve the eﬀective, non-interacting Hamiltonian analytically through simple fourier transform of the fermionic operators and ﬁnd (with the lattice constant set to a = 1) the following four bands: From this, all relevant expectation values can be computed exactly, either directly through the derivatives of the grand potential or through Wick factorization. If we then write out the variational equation (Eq.(4)) explicitly, we obtain the following expression which needs to be minimized with respect to ( ˜B1, ˜B2): As mentioned above, employing a non-interacting eﬀective ˜H within the variational framework is completely analogous to performing a decoupling of H which allows for ferro- and antiferromagnetic solutions. Fig.(7) shows, again, the spin-spin correlation and double occupancy, for a greater parameter range than in Sec.(III). The transition between Nel- and ferromagnetic order, which is analytically expected at the J = 4t2/U-line in the strong-U regime, can be seen clearly. FIG. 7. Mean-ﬁeld solutions for a half ﬁlled square lattice. (a): Next-neighbor spin-spin correlation with the 4t2/U line. (b): Double occupancy. only two data points (∆τ = 0.1 and 0.2). The error is known to scale with O(∆τ 2). Consider again the exactly solvable four-site model which was treated in Sec.(III A), for the speciﬁc case of J = 0, i.e. a four-site Hubbard model at half ﬁlling. In order to compare to the exact results obtained from exact diagonalization, we simulated the model within DQMC with the aforementioned Trotter discretizations. The small size of the system allows for much longer samplings with 10000 warmup sweeps and 150000 measurement sweeps. The simulations were performed with U = 0 − 6, at 100 equidistant data points. Furthermore, the obtained data is smoothed with a Savitzky-Golay ﬁlter using third order polynomials and a window length of w = 7 data points. FIG. 8. (a): Double occupancy of a four-site Hubbard model at half ﬁlling, obtained from ED, DQMC for two diﬀerent Trotter steps and after extrapolation ∆τ → 0. (b): Total error in the double occupancy. Fig.(8a) shows the double occupancy of the model depending on U, obtained from ED, DQMC for both Trotter steps and the result from the extrapolation ∆τ → 0. It is visible that the extrapolation brings the DQMC results very close to the exact solution. In order to better quantify this, Fig.(8b) shows the error in the double occupancy for both discretizations. After extrapolating, the systematic error is diminished, and only the statistical noise remains, which is visible as the extrapolated curve oscillates slightly around 0. The calculations in this work are prone to a variety of diﬀerent error sources, which need to be adressed separately. First, we discuss the variational principle itself, where the exact benchmarking data provides some insight. Secondly, we turn our attention to the results obtained from DQMC, i.e. the statistical error, the ﬁnite size extrapolation and the integration errors which occur when computing the free energy.
Multiple stellar systems under photometric and astrometric analysis<|sep|>The binary stars are crucial for our knowledge about the universe. Especially eclipsing binaries provide us an unique insight to the basic physical parameters of the stars, stellar clusters, interstellar medium and galaxies. They are excellent distance indicators. We are able to learn more about the matter composition of the stars, about their evolution status, or the presence of planets or other components in these systems. The very ﬁrst task is the data acquisition, because only with precise input data is one able to get precise results. In last few decades mainly due to excellent satellite observatories (and not only in the visible part of the spectrum) our knowledge of them has rapidly grown. Regarding the astrometry, there is still decreasing the number of observations of the wide pairs. On the other hand, due to the new interferometers, which could resolve the milli- and micro- arcsecond angular distances, the observable semimajor axes of the astrometric binaries are still decreasing. Unfortunately, most of the systems analyzed below have the angular size of the astrometric orbit from 1 arcsec down to 100 mas, which is beyond the limits for the modern multi-aperture interferometers. And the lack of recent observations lead to the low accuracy of the results. Another approach is photometry and the classical observation of minimum light. Due to the large ”baseline” of observers in our country and the interest of amateur astronomers, the number of these observations is growing very rapidly and the cooperation between professional and amateur astronomers is very intensive. Many of the observations of minimum timings used in this study came from amateur astronomers and these measurements are as accurate as from the professional observatories. Thanks to the large minimum times data set we are able to analyze many of the eclipsing binary systems for their long-term period variations. The whole thesis is divided into several parts. In the ﬁrst one is presented the theory needed for the analysis of multiple stellar systems by photometric and astrometric techniques, description of such systems and some limitations which have to be considered. In the second part are introduced several systems which show apparent period changes in their O − C diagrams. And in the third one are the systems analyzed by photometry and astrometry simultaneously. Also the catalogue of other suggested systems for simultaneous analysis is included in this chapter. This is the crucial part of this thesis. The method itself is introduced in the chapter 2 and the results are in the chapter 4.
Robust Domain Decomposition Preconditioners for Abstract Symmetric Positive Definite Bilinear Forms<|sep|>The theory developed above introduces a method for constructing stable decompositions with respect to symmetric positive deﬁnite operators. The robustness with respect to problem and mesh parameters is proved under rather general assumptions. We have furthermore applied this abstract framework to several important cases, i.e., the scalar elliptic equation in Galerkin and mixed formulation, Stokes’ equations, and Brinkman’s equations. For the scalar elliptic equation in Galerkin formulation, we have additionally presented a strategy of reducing the dimension of the coarse space in the stable decomposition. To verify our analytical results, we have performed several numerical experiments, which are in coherence with our theory and show the usefulness of the method.
Does Bankruptcy Protection Affect Asset Prices? Evidence from changes in Homestead Exemptions<|sep|>A large body of literature studies the impact of bankruptcy protection on bankruptcy ﬁlings. These studies do not capture a lot of strategic behavior since the majority of defaulting consumers do not ﬁle for bankruptcy, and most debt collection takes place outside of the courtroom (Dawsey et al. (2013)). In contrast, we use house prices to quantify demand for bankruptcy protection. We ﬁnd that the average rise in homestead exemptions raises real house prices 0.73%, an eﬀect which is positive, statistically signiﬁcant, and small. However, when the sample is restricted to Pre-BAPCPA observations (t ≤ 2005 when bankruptcy was cheaper and easier), the treatment eﬀect rises to 1.07%, and Post-BAPCPA (t > 2005 when bankruptcy was more expensive and less beneﬁcial), the eﬀect is no longer statistically signiﬁcant. Next, when the sample is restricted to “large” changes in homestead exemptions (deﬁned to be changes greater than or equal to $50, 000) the eﬀect rises to 1.82% and is not statistically signiﬁcant for small changes, mostly due to inﬂation adjustment. Big changes Pre-BAPCPA raise house prices 3.04%, whereas small changes Pre-BAPCPA and all changes Post-BAPCPA have no statistically signiﬁcant eﬀect. Together these estimates reveal evidence of strategic behavior by households to protect their assets before BAPCPA (when bankruptcy was cheaper, easier, and more ﬁnancially beneﬁcial). These estimates also indicate that BAPCPA achieved its stated goal of reducing bankruptcy abuse.
Improving Contextual Coherence in Variational Personalized and Empathetic Dialogue Agents<|sep|>We have proposed two UA-CVAE frameworks: UA-CVAE(M) and UA-CVAE(C), which improved coherence on both ConvAI2 (60% and 121%) and EMPATHETICDIALOGS (11% and 17%). Additionally, we introduced the UE-score, which correlated positively with human judgement in terms of contextual coherence. Future work could involve exploring the possibility of enhancing the dialogue quality by leveraging the epistemic or homoscedastic aleatoric uncertainty.
Suppressing Lepton Flavour Violation in a Soft-Wall Extra Dimension<|sep|>In this paper we have studied the lepton sector of the SM in a soft-wall extra dimension, applying ﬂavour dependent fermion locations to accommodate the observed lepton ﬂavour structure. The Higgs is a bulk ﬁeld, with a VEV that increases near the soft wall. We have in particular considered the inclusion of small Dirac neutrino masses and investigated the constraints on the model from lepton ﬂavour violation mediated by the Z boson and its KK states. In order to do so we ﬁrst developed solutions for a massive gauge boson in the soft-wall background and found the proﬁle is independent of the AdS curvature scale. In order to generate the masses of the charged leptons whilst keeping the fermions located in an area of almost universal gauge couplings we ﬁnd that we need to increase the hierarchy of scales in the model to around k/µ = 107. When incorporating sub-eV neutrino masses we need a much larger hierarchy, and we choose k/µ = 1015, similar to the hierarchy between the Planck and the electroweak scales. To incorporate three generations of leptons into our model we solve the fermion equations of motion numerically, including an order one ﬂavour diagonal Yukawa coupling and use these solutions as a basis of states from which we treat oﬀ-diagonal Yukawa couplings, connecting diﬀerent generations, as perturbations. The mass term related to the diagonal Yukawa coupling is necessary to generate a normalisable wavefunction and cannot be treated as a perturbation. We can construct the full lepton mass matrices, including KK states and diagonalise them to ﬁnd the fermion masses and mixings. However, to our level of precision we can neglect the fermionic KK states. The locations of the left-handed fermions are dictated by the fact that we require large mixings in the neutrino sector. We take a large number of random Yukawa couplings and choose the locations of the right-handed fermions so that the averaged zero mode masses reproduce the SM charged lepton masses. With the inclusion of oﬀ-diagonal Yukawa couplings, the transformation to mass eigenstates produces ﬂavour violating couplings. We calculated the expected rates for various ﬂavour changing processes for a number of diﬀerent scenarios. We found that the soft-wall model is in fact mildly constrained when we consider a scenario with a low hierarchy of scales such as k/µ = 107. The most stringent constraint comes from µ → e conversion in a muonic atom where we ﬁnd that only the scenario where all the left-handed leptons have degenerate locations well toward the UV brane would occur at acceptable rates with a KK scale of 2 TeV. This is a considerable suppression of lepton ﬂavour violation compared to hard-wall models, such as the one studied in Ref. [5]. Including a larger hierarchy of scales (k/µ = 1015), it is also possible to generate sub-eV Dirac neutrino masses. In this case the model is even less constrained and most of the FCNC processes would occur at rates well below the experimental bounds. The most stringent bounds are coming from radiative decays, such as µ → eγ. Again a KK scale of 2 TeV seems suﬃcient to keep the rate below the experimental bound. Our estimate for this rate does not include contributions from KK gauge bosons, and it would be interesting to include these in a more detailed analysis. Another obvious direction of research would be to extend the present setup to the quark sector, similar to an analysis that was performed recently in much detail for the hard-wall model in Ref. [30]. The soft-wall extra dimension continues to oﬀer a valid model for electroweak physics, with constraints from precision data relaxed compared to the hard-wall model. Having said this, we have found that with a (gauge boson) KK scale of 2 TeV the complete lepton ﬂavour structure can be accommodated while keeping rare processes below experimental bounds. In our setup the KK states of fermions have masses around 1.5 TeV, within reach of the LHC experiment. Thus the softwall framework seems to oﬀer an alternative when it comes to suppressing ﬂavour violation to models relying on ﬂavour symmetries [29, 31, 32], a bulk Higgs [33] or to utilising non-minimal representations under the SU(2)R bulk gauge symmetry [34]. The parameter range with a large hierarchy k/µ = 1015 is both attractive to further suppress ﬂavour violation and necessary to accommodate neutrino masses. This rises the important question whether such a hierarchy can be stabilised, like in the way proposed in Ref. [13]. It would be very interesting to extend our analysis to such a framework.
Non-linear quantum critical dynamics and fluctuation-dissipation ratios far from equilibrium<|sep|>We have reviewed the non-thermal steady state dynamics of two models of quantum criticality, the spin-density wave theory and the pseudogap Kondo impurity model, a model of local quantum criticality. In both cases, voltage and temperature are relevant perturbations and in both cases only the case of simple relaxational dynamics was considered. An analysis of the relation between correlation and response functions in terms of eﬀective termperatures, valid in the scaling regime accompanying these quantum critical points, has been presented. For the SDW, an eﬀective temperature exists for the order parameter susceptibility in the limit of large bias voltage. For the pseudogap Kondo model we ﬁnd that several scaling functions, including that of a fourpoint correlator, reproduce their equilibrium behavior when rescaled in terms of a uniquely deﬁned Teff . This Teff has the additional property that it shows scaling collapse of T/Teff in terms of V/T. It is important to note that the results for the two models are based on diﬀerent methodologies. The SDW results were obtained from a perturbative RG scaling approach on the Keldysh contour [22, 23] while the results for the critical Kondo destruction were obtained within a dynamical large-N limit [26, 27, 20] Interestingly, there are indications that the ﬂuctuation spectrum of current-carrying steady states is that of a thermal state within the ADS/CFT correspondence [30, 31]. We complemented our analysis by results for Teff obtained for the pseudogap resonant level model, an non-interacting relative of the pseudogap Kondo model. The pseudogap resonant level model is a quantum impurity model that emerges e.g. in a meanﬁeld treatment of the pseudogap Kondo model. As a consequence of its noninteracting nature its steady-state properties are derived from an action of the form of Eq. 5. Although we ﬁnd that (χ−1)K of the pseudogap resonant level model approaches a V dependent constant so that a Teff in the large-V limit can be deﬁned a scaling collapse of T/Teff in terms of V/T was not found for T < Γ and V < Γ. This suggests that the behavior near the Gaussian ﬁxed point diﬀers from that obtained near the critical ﬁxed points of the pseudogap Kondo model reported in Ref. [20]. Acknowledgments. We thank Q. Si for useful discussions. P. Ribeiro acknowledges support by FCT through the Investigador FCT contract IF/00347/2014. S. Kirchner acknowledges partial support by the National Science Foundation of China, grant No.11474250.
Acquisition-invariant brain MRI segmentation with informative uncertainties<|sep|>in the literature for neuroimaging studies. This is a twofold boon, as it not only lends credence to the model by showcasing
Equilibrium-charge diagram of single quantum dot in an axial magnetic field<|sep|>Usually electron transport properties of quantum dots is studied by adjusting gate voltages, which corresponds to the ω0 in Eq. (5). In this paper, the chemical potentials of the dot are modulated by means of adjusting external magnetic ﬁelds, which corresponds to the A in Eq. (5). The equilibrium-charge diagram of the dot is obtained in the phase space determined by the magnetic ﬁeld and the lead voltage. The dependence of the number of electrons on the magnetic ﬁelds and the lead voltage is clear at a glance in the equilibrium-charge diagram. The result shows external magnetic ﬁeld can be used as a tool to modulate the number of electrons of single quantum dot with the aid of the voltage of the leads. On the other hand, the result suggests that the number of electron can be changed unexpectedly by external magnetic ﬁeld. Moreover, the result shows that the electron transport through the dot can be controlled by adjusting the magnetic ﬁelds with the aid of the voltage of the leads. The equilibrium-charge diagram is a compact tool to show clearly the condition of the electron transport, which helps to use single quantum dot as a magnetic electron-transport switch.
Studying stellar halos with future facilities<|sep|>Based on the results of simulations, we conclude that future telescopes will allow ample and detailed studies of stellar halos with suﬃcent accuracy to constrain models of galaxy formation. In particular it will be possible to: • catch young stars up to distances of hundreds of Mpcs and witness current stars formation in large samples of interacting galaxies; • costruct the color-magnitude diagram of stars in tidal features down to the red clump for galaxies up to ∼ 10 Mpc away, and derive the metallicity distribution and its gradient in the halo of galaxies within 20 Mpc; • map the stellar density in galaxy halos up to a distance of 50 Mpc; overdensities will be traceable at very faint limits of surface brightness (µV ∼ 33 mag/arcsec2); • analyze the color-magnitude diagram of resolved stars in globular clusters down to the old turn-oﬀ region in the whole Local Group; • measure the luminosity, mass and color distribution of the globular cluster members of galaxies up to a redshift z ≃ 0.1.
Sharing pattern submodels for prediction with missing values<|sep|>We have presented sharing pattern submodels (SPSM) for prediction with missing values. The method exploits recurring pattern structure in missingness, ﬁtting a shared model for diﬀerent patterns while allowing limited pattern-speciﬁc specialization. We have described settings where sharing is provably optimal even when the prediction target depends on both missing values and on the missingness pattern itself. Experimental results, based on real-world and synthetic data, conﬁrm that SPSM performs the classiﬁcation and regression tasks comparably or slightly better than diverse baseline models across all data sets, while being independent of imputation and allowing eﬀective use of the available data across patterns. Notably, the proposed method never performs worse than non-sharing pattern submodels as these do not use the available data eﬃciently. By enforcing sparsity in pattern specialization, we ensure that the resulting subset of features is reduced to relevant diﬀerences which will foster interpretability for domain experts; SPSM allows for more straight-forward reasoning about the similarity between submodels and the eﬀects of missingness. Lipton et al. (2016) provides qualitative design criteria to address model properties and techniques thought to confer interpretability. We will show that SPSM satisﬁes some form of transparency by asking, i.e., how does the model work?. As stated in (Lipton et al., 2016), transparency is the absence of opacity or black-boxness meaning that the mechanism by which the model works is understood by a human in some way. We evaluate transparency at the level of the entire model (simulatability), at the level of the individual components (e.g., parameters) (decomposability), and at the level of the training algorithm (algorithmic transparency). First, simulatability refers to contemplate the entire model at once and is satisﬁed in SPSM by it’s nature of a sparse linear model, as produced by lasso regression Tibshirani (1996). Moreover, we claim that SPSM is small and simple (Rudin, 2019) , in that we allow a human to take the input data along with the parameters of the model and perform in a reasonable amount of time all the computations necessary to make a prediction in order to fully understand a model. The aspect of decomposabilty (Lipton et al., 2016)can be satisﬁed by using tabular data where features are intuitively meaningful. To that end, we use two real-world tabular data sets in the experiments and present the coefﬁcient values for input features in Table 3. Moreover, one can choose to display the coefﬁcients in a standardized or non-standardized way to provide even better insights. The comprehension of the coefﬁcients depends also on domain knowledge. Finally, algorithmic transparency is given in SPSM, since in linear models, we understand the shape of the error surface and have some conﬁdence that training will converge to a unique solution, even for previously unseen test data. Additionally, Henelius et al. (2017) claims that knowing interactions between two or more attributes makes a model more interpretable. SPSM shows in θ + ∆ the coefﬁcient adaptation between the shared model and the pattern speciﬁc model and therefore reveals associations between attributes. to learning from linear data generating processes. In fact, there are also many cases where the goal is to consider the best linear approximation to a nonlinear function given the risk of hold-out. We have presented a way to do this given patterns of missingness at test time. The restriction to linear models can be overcome by considering other classes of parametric models for which the class is closed under parameter addition. Although this technically includes neural network estimators (with ﬁxed architecture), we do not expect linear combinations of model parameters to perform as well for them. An interesting direction for future work is to identify other classes of models developed with interpretability that might beneﬁt from this type of sparsity (parameter sharing). Future work could include different regularization methods, such as adaptive LASSO or ﬁtting separate regularization terms for each pattern, as well as quantitative analysis of variance reduction implied by parameter sharing. Data collection and sharing was funded by ADNI (NIH #U01 AG024904) and DOD ADNI (#W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoﬀmann-La Roche Ltd and its aﬃliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pﬁzer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California. LS and FJ are supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.
Slipping magnetic reconnections with multiple flare ribbons during an X-class solar flare<|sep|>The apparent slipping motion of the FLs and the hook-like structures of FRs are consistent with the 3D reconnection in QSLs, as predicted by the 3D standard ﬂare model (Aulanier et al. 2012 and Janvier et al. 2013). The 3D model is generically with two polarities and two J-shape FRs. This kind of slippage of twin FRs have been reported in previous studies (Dud´ık et al. 2014; Li & Zhang 2014, 2015). Here, the slipping motions of the ﬂare involved at least two ARs and three distinct FRs, which may be a consequence of the more complex ﬂux distribution in the photosphere. The semi-circular SFR contributes to complex magnetic conﬁguration, and conﬁrms the presence of a QSL (Aulanier et al. 2007). The SFR was not connected by FLs, and disappeared following the slippage. Does it mean that SFR is a diﬀerent structure, e.g. footpoints of coronal loops impulsively heated by the ﬂare (Zheng et al. 2015)? Note that the eruption was launched between FR1 and FR3 towards southwest (better seen in 193 ˚A of the attached animation), and it was closely associated with a fast partial-halo CME in the southwest 1. Following the slippage, SFR widened, and coronal dimmings formed within the widened part of SFR (white arrows in right column of Figure 3), which is in accordance with the predictions of the 3D standard model for eruptive ﬂares (Figure 11a of Janvier et al. 2015). Hence, SFR is the extended hook of FR3, and represents footpoints of erupting structures as predicted in the 3D standard model for eruptive ﬂares. The presence of dimmings within the hook is due to the stretching of the ﬁeld lines opened by the eruption. According to 3D slipping reconnection in QSLs predicted by the 3D ﬂare model, we suggest the scenario of the slippage of SFR in a sketch (Figure 5), which is based on an HMI magnetogram showing the associated magnetic polarities (P1-P2, N1-N3). FR3 and SFR are at a single continuous bright lane (orange lines) before the ﬂare (Figure 2a1). The erupting SL (the dashed light-blue line) induced the ﬂare and FLs (yellow lines). Due to the successive reconnections, FR3 extended clockwise and formed a semi-circular SFR. The bright features ﬁrst moved along the outer edge of SFR toward its easternmost point, and continually slipped anticlockwise along the inner edge of SFR, followed by coronal dimmings within the hook (the shadow in panel b). The slipping directions of SFR is indicated by orange arrows, and the tight narrow hook is similar to that reported by Dud´ık et al. (2014). The presence of dimmings reveals the open of the erupting structure (the pink lines). Note that the erupting structure is just a schematic representation of the eruption, and can itself be fragmented, since all of the FR1-FR3 have hook structures. In addition, the survived warm coronal loops (L1; red lines) connected the inner edge of SFR and FR2. Due to the eruption direction, the dimmings only occurred to the southwest of the closed L1. The inner edge of SFR is then simply the topological boundary between closed and open ﬁeld lines. On the other hand, the hot coronal loops L2 (green lines) are impulsively heated, expanded, and reconnected with the magnetic ﬁeld lines (black lines) overlying F3 (the sparkle), which resulted in the formation of L3 (deep-blue lines). As the ﬁndings in Dud´ık et al. (2014), the tight narrow hook of SFR fade and disappear, due to the moving footpoints of erupting structures. For the fading and disappearance of the hook of FR2, it is possibly because of the successive reconnections between L2 and the overlying loops of F3. It suggests that the eruption with complex magnetic conﬁgurations involves at least two All the FRs underwent slipping motions in most of AIA passbands, especially for FR1 and SFR. FR1 extended fast with a speed of 384 km s−1, which is faster than that of previous results; the anticlockwise propagation velocity of SFR is about 130 km s−1, which is similar to that of the previous results (Dud´ık et al. 2014; Li & Zhang 2014, 2015). The fast slippage of FR1 reveals the strong-to-weak shear transition of FLs, which is likely associated with the untwisting of SL during the eruption. According to the deﬁnitions in Aulanier et al. (2006), the slipping-running and slipping reconnection regimes respectively correspond to super- and sub-Alfv´enic ﬁeld line fast slippage. Due to the average Alfv´en speed for the decaying AR is 500 km s−1 (Aulanier et al. 2012), all the slipping speeds here are in the range of sub-Alfv´enic speed. We suggest that the extension and propagation in multiple FRs are as a result of slipping magnetic reconnections. Beneﬁting from the high-resolution observations of AIA/SDO, we capture the slipping magnetic reconnections in multiple FRs during the X1.2 ﬂare on 2014 January 7. As the result of slipping motion, multiple FRs extended with hook structures. Due to the moving footpoints of erupting structures and another magnetic reconnection, hooks disappeared, and/or was replaced by coronal dimmings. Our results are accordant with the slipping magnetic reconnection regime in 3D standard model for eruptive ﬂares (Aulanier et al. 2006; Aulanier et al. 2012; Janvier et al. 2013; Janvier et al. 2015). More studies of the slipping reconnection will be helpful in understanding the 3D standard ﬂare model; further observations and theoretical work will be necessary. Many thanks to Wei Liu, Ting Li, Xiaoli Yan, and Hongqiang Song for the constructive discussion. SDO is a mission of NASA’s Living With a Star Program. The authors thank the SDO team for providing the data. This work is supported by grants NSBRSF 2012CB825601, NNSFC 41274175, and 41331068, and Yunnan Province Natural Science Foundation 2013FB085.
Robust Unit Commitment Considering Strategic Wind Generation Curtailment<|sep|>proposed to cope with the volatile and non-dispatchable wind  generation. WGCR is introduced to control the proportion of  committed wind generation. Compared with traditional RUC  models, the proposed RUC with strategic WGC has three  advantages. Economically, strategic WGC would benefit  power systems by reducing ramping requirement, increasing  ramping up capability as well as improving the utilization ratio  of transmission lines, all of which would contribute to  operational cost decrement of power systems. From solvability  aspect, traditional RUC may fail to generate a robust dispatch  strategy under critical operational reliability requirement,  however, the solvability of RUC with strategic WGC could  always be guaranteed. In terms of computational efficiency, the proposed model somewhat increases the dispatchbility of  the non-dispatchable wind generation by introducing WGCR  to the first stage of RUC, which results in better computational  efficiency than traditional RUC. Mathematically, the RUC  with strategic WGC is still a two-stage robust optimization  problem, which can be efficiently solved by the proposed  C&CG-based iterative algorithm. Simulations are carried out  on the modified IEEE 39-bus system to illustrate the  effectiveness of the proposed model and algorithm. The  proposed methodology is also applied to IEEE 118-bus system  and the real Guangdong Power Grid, demonstrating the  practicality of our methodology.
Insight into bias in time-stratified case-crossover studies<|sep|>Using the decomposition of the ozone time series, we scrutinized the control ability of the time-stratiﬁed reference selecting schema. This schema provides effective control on yearly and monthly time trends but not on weekly trend. We believe that a lack of control on weekly trend is the main source of systematic bias in case-crossover studies. Without controlling weekly time trend, it will enter into the errors of our logistic regression model, leading to the so-called overlap bias and a possible bias in point estimate. Doing model adjustment for weekly time trend can be an effective way of reducing bias from it. Further model adjustment and calibration could be necessary if there are other potential time-varying confounders. The typical case-crossover study design, i.e., combining the time-stratiﬁed reference-select schema with a traditional logistic regression, leads to an estimate of weekly time trend effect, instead of the transient effect under investigation. The difference in results between the traditional approach and the proposed method could be signiﬁcant. We strongly recommend researchers to do controlling (adjustment) for weekly time trend of exposures and covariates in their case-crossover application studies. We found, through the empirical study for AMI hospitalization, that higher concentration of CO, NO, O3 and PM2.5 are associated with higher risk of AMI hospitalization, while higher concentration of NO2 is associated with lower risk of AMI hospitalization. A limitation of the empirical study for AMI hospitalization is that we only did ‘univatiate’ analyses for the ﬁve pollutants without controlling for impact from other potential time-varying factors. Considering also none of the candidate association has p value less than the Bonferroni criterion, we believe further conﬁrmation for the reported “ﬁndings" is still needed from other studies. Ethical approval for the study was granted by the University of Alberta’s Health Research Ethics Board-Health Panel (IREB Pro00010852). Patient records/information was anonymized and de-identiﬁed with a unique scrambled ID and released to us in this form by the ministry of health prior to analysis. Air pollution data of daily average records in urban Edmonton from April 1, 2000 to March 31, 2010 for carbon monoxide (CO), nitrogen monoxide (NO), nitrogen dioxide (NO2), ozone (O3), and particulate matter with an aerodynamic diameter ≤ 2.5 (PM2.5) is available online. An Excel ﬁle reporting modeling results of the 1375 models for the empirical study is available online. R code ﬁles for the simulation and the empirical study are available upon request.
Formulating an $n$-person noncooperative game as a tensor complementarity problem<|sep|>In this paper, we reformulated the multilinear game as a tensor complementarity problem and showed that ﬁnding a Nash equilibrium point of the multilinear game is equivalent to ﬁnding a solution of the resulted tensor complementarity problem. Especially, we provided a one to one correspondence between the solutions of the multilinear game and the tensor complementarity problem, which built a bridge between these two classes of problems so that one can investigate one problem by using the theory and methods for another problem. We also applied a smoothing-type algorithm to solve the resulted tensor complementarity problem and reported some preliminary numerical results for solving the multilinear games. Hopefully some more eﬀective algorithms can be designed to solve the tensor complementarity problem by using the structure of the tensors and properties of the homogeneous polynomials.
V1460 Her: A fast spinning white dwarf accreting from an evolved donor star<|sep|>We ﬁnd that V1460 Her belongs to a select group of CVs in which the donor star is signiﬁcantly over-luminous and over-sized (by 50 per cent) for its mass. This indicates that it is probably the remnant of a phase of high rate mass transfer. It is moreover eclipsing and shows phenomena in its spectra associated with both the eclipse of its disc by the donor star and the eclipse of the donor star by the disc. Most remarkably of all, V1460 Her shows strong pulsations on a period of 39 s in HST ultraviolet data. This is a clear sign of a rapidly spinning white dwarf, and amongst the fastest known amongst CVs such as AE Aqr and the recently reported CTCV J2056-3014. AE Aqr also hosts an evolved donor star, and these two systems may share a history of white dwarf spin-up through high rate accretion. We speculate that this may only occur if accretion at a rate high enough to suppress classical nova eruptions has taken place. TRM, BTG & OT acknowledge support from the Science and Technology Facilities Council (STFC) grant numbers ST/P000495/1 and ST/T000406/1 and EB from STFC grant number ST/S000623/1. This paper makes use of data from the ﬁrst public release of the WASP data (Butters et al. 2010) as provided by the WASP consortium and services at the NASA Exoplanet Archive, which is operated by the California Institute of Technology, under contract with the National Aeronautics and Space Administration under the Exoplanet Exploration Program. The research leading to these results has received funding from the European Research Council under the European Union’s Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement n.320964 (WDTracer). We acknowledge the ongoing support of the Instituto de Astrof´ısica de Canarias that has enabled our work. This research has made use of the SIMBAD database, operated at CDS, Strasbourg, France. The authors would like to thank the astronomers of the AAVSO network who provided excellent photometry of V1460 Her during the outburst and in particular Paul Benni, Ivaldo Cervini and Thomas Wikander whose support during the HST observations helped to ensure the safety of the spacecraft. We would also like to thank Ian Skillen and Ovidiu Vaduvescu at the Isaac Newton Group in La Palma for their invaluable WHT ISIS spectra taken during four nights of service time. Jorge Casares very kindly sent us his list of template spectra of K-type stars that helped in identifying the spectral type of the secondary and allowed us to calculate the rotational broadening. We would also like to thank the anonymous referee for their helpful, insightful and constructive feedback.
Future-Aware Diverse Trends Framework for Recommendation<|sep|>In this paper, we propose a novel Future-Aware Diverse Trend(FAT) framework to capture diverse trends of user preference dynamically. Our frame work leverages a neighbor behavior extractor to generate relative future interactions from similar users implicitly and utilizes diverse trends module to capture intrinsic varying dynamics of user preferences. To improve the expressive ability of trend representation, we utilize time-aware attention layer to make the duration between prediction time and target item interaction time choose which trend is more relative. Experimental results demonstrate that our models can achieve significant improvements over state-of-the-art models on three challenging datasets. For the future, we plan to leverage multi-hop user-item graphs to address limited interaction issues and incorporate multi-behavior data into neighbors extraction to better model potential trends.
Learning for Detecting Norm Violation in Online Communities<|sep|>The proposed framework, combining machine learning (Logistic Model Trees and K-Means) and taxonomy exploration, is an initial approach on how to detect norm violations. In this paper, we focused on the issue of norm violation assuming violations may occur due to misunderstandings of norms originated by the diverse ways people interpret norms in an online community. To study norm violation, our work used a dataset from Wikipedia’s vandalism edition, which contains data about Wikipedia article edits that were considered vandalism. The framework described in this work is a ﬁrst step towards detecting vandalism, and it provides relevant information about the problems (features) of the action that led to vandalism. Further investigation is still needed to get a measure of how our system would improve the interactions in an online community. The experiments conducted in our work show that our ML model has a precision of 78,1% and a recall of 63,8% when classifying data describing vandalism. Future work is going to focus on the use of feedback from the community members to continuously train our ML model, as explained in Section 3. The idea is to apply an online training approach to our framework, so when a community behavior changes, that would be taken to indicate a new view on the rules deﬁning the norm, and our ML model should adapt to this new view. Throughout this investigation, we have noticed that the literature mostly deals with norm violation that focus either on hate speech or cyberbullying. We aim that our approach can be applied to other domains (not only textual), thus we are planning to explore domains with diﬀerent actions to analyze how our framework deals with a diﬀerent context (since these domains would have a diﬀerent set of actions to be executed in an online community). This research has received funding from the European Union’s Horizon 2020 FET Proactive project “WeNet – The Internet of us”, grant agreement No 823783, as well as the RecerCaixa 2017 funded “AppPhil” project.
Few-shot Generation of Personalized Neural Surrogates for Cardiac Simulation via Bayesian Meta-Learning<|sep|>In this work, we demonstrated the promising potential of metaPNS – a novel framework for obtaining a personalized neural surrogate by simple feed-forward embedding of a small and ﬂexible number of data available from a subject. Future work will evaluate use of metaPNS for higher-ﬁdelity cardiac simulations, extend its ability to personalize using diﬀerent types of observational data, and improve its generalization ability to more complex real-data applications. Acknowledgement: This work is supported by the National Institutes of Health (NIH) under Award Numbers R01HL145590, R01NR018301, and F30HL149327; the NIH NIGMS Center for Integrative Biomedical Computing (www.sci.utah.edu/cibc), NIH NIGMS grants P41GM103545 and R24 GM136986; the NSF GRFP; the Utah Graduate Research Fellowship; and the Nora Eccles Harrison Foundation for Cardiovascular Research..
Fragmentation of star-forming filaments in the X-shape Nebula of the California molecular cloud<|sep|>We performed a detailed study of ﬁlaments and cores in the X-shape Nebula of the California MC using a high-resolution (18.2′′) column density map constructed from Herschel data, along with 12CO(2−1) and 13CO(2−1) data from the SMT 10m telescope. Our main ﬁndings may be summarized as follows: 1. We selected 10 robust ﬁlaments with aspect ratios AR > 4 and column density contrasts C > 0.5 from a skeleton network obtained with the getﬁlaments algorithm. The dust temperatures of the ﬁlaments are anti-correlated with their column densities (NH2). The deconvolved FWHM widths (Wdec) of the 10 ﬁlaments range from 0.09 to 0.18 pc and are uncorrelated with their column densities (NH2). The derived median ﬁlament width, 0.12 ± 0.03 pc, is consistent with the common inner width of ∼ 0.1 pc measured by Arzoumanian et al. (2011, 2019) for Herschel ﬁlaments in nearby molecular clouds. 2. We identiﬁed two thermally supercritical ﬁlaments: ﬁlaments 8 and 10, which both exhibit quasi-periodic chains of dense cores. The typical projected core spacing is ∼ 0.15 pc, close or only ∼ 30–40% higher than the ﬁlament inner width. Five prestellar cores form the chain structure of ﬁlament 8. There is a transverse velocity gradient across ﬁlament 8, suggesting that this ﬁlament is accreting gas from a surrounding gas reservoir with an accretion rate ˙M ∼ 40±10 M⊙ Myr−1 pc−1. Filament 10 is ∼ 0.5 pc away from ﬁlament 8 and at a later fragmentation stage than ﬁlament 8. Two prestellar cores and two protostellar cores form the chain structure of this ﬁlament. 3. We emphasize that classical cylinder fragmentation theory cannot account for the observed fragmentation properties of ﬁlaments 8 and 10. We suggest that three key factors may explain why the observed core spacing is shorter than the standard fragmentation lengthscale of equilibrium ﬁlaments. First, ﬁlaments 8 and 10 are not straight cylinder structures and feature bends along their crests which likely aﬀect the fragmentation process. Second, if a longitudinal magnetic ﬁeld of ∼100 µG is present at the center of ﬁlaments 8 and 10, the characteristic fragmentation lengthscale may become comparable to the ﬁlament diameter as observed. Third, at least in the case of ﬁlament 8, the presence of external accretion from the ambient cloud may enhance initial density perturbations, leading to a shorter core spacing compared to an isolated ﬁlament. Acknowledgements. We would like to thank the anonymous referee for valuable comments which improved the quality of the paper. The SMT data presented in this paper are based on the ESO-ARO programme ID 196.C-0999(A). This work was carried out in the HGBS group of the Astrophysics Department (DAp/AIM) at CEA Paris-Saclay. Guoyin ZHANG acknowledges support from a Chinese Government Scholarship (No. 201804910583). We also acknowledge support from the French national programs of CNRS/INSU on stellar and ISM physics (PNPS and PCMI). Ke Wang acknowledges support by the National Key Research and Development Program of China (2017YFA0402702, 2019YFA0405100), the National Science Foundation of China (11973013, 11721303), and a starting grant at the Kavli Institute for Astronomy and Astrophysics, Peking University (7101502287).
Unit Disk Cover Problem<|sep|>In this paper, we have proposed a PTAS for LSDUDC problem, improving previous 2-factor approximation result. Using this PTAS, we proposed an (9+ǫ)-factor approximation algorithm for DUDC problem, improving previous 15-factor approximation result for the same problem. The running time of our proposed algorithm for ǫ = 3 (i.e., approximation factor of DUDC problem is 12)is same as the running time of 15-factor approximation algorithm. We have also proposed an (9 + ǫ)-factor approximation algorithm for RRC problem, which runs in O(max(m8, m4(1+3/ǫ)) time. In the reduce radius setup, we proposed an 2.25factor approximation algorithm. The previous best known approximation factor was 4 [11]. The running time of our proposed algorithm for RRC problem in reduce radius setup is less than that of 4-factor approximation algorithm proposed in [11] for reasonably small values of δ(= ν 2), where δ is the radius reduction parameter. Since the number of disks participating in the solution of 4×4 square is constant for ﬁxed value of δ, the number of disks participating in the solution of L × L square is constant. Therefore, using the shifting strategy proposed by Hochbaum and W. Maass [16], we can design a PTAS for the RRC problem in reduce radius setup.
Cavity approach to sphere packing in Hamming space<|sep|>In summary, we have written the BP and SP equations to study the hard sphere packing problem in the Hamming space. Within these approximations we obtained a maximum rate of packing that is asymptotically the same as the lower bound of Gilbert and Varshamov. In the RS approximation we also found a crystalline phase where for even values of d the spheres prefer to be in one sector of the Hamming space. The BP solutions were stable with respect to continuous glass transitions as long as δ < 1/2. This suggests that phase transitions, if any, would be of discontinues type. We have also introduced two new algorithms. First a message passing algorithm based on the BP equations which ﬁnds dense packings of hard spheres in ﬁnite dimensions. An approximate scheme is used to reduce the time and memory complexity of the algorithm, which can still be improved and hopefully lead to new packing results. An almost identical algorithm can be used in continuous spaces. As a proof of concept we used it to ﬁnd some known local dense packings in two-dimensional Euclidean space. Second, we introduced an iterative algorithm to ﬁnd packings of hard spheres starting from small diameters and dimensions. For even diameters the algorithm generates packings with all spheres in one subspace of the Hamming space, as expected from the crystalline solution of the BP equations. There are still some points that need more eﬀort to be clariﬁed. It is of extreme relevance to establish the relation between the exact maximum rate of packing and the one provided by the BP equations. This means, for instance, that we need to study some interpolating functions between the exact and the BP entropy. Preliminary results are discussed in Appendix C. We expect the BP bound to be asymptotically exact in the binary Hamming space. Finally, it would be nice if one could obtain the algebraic-geometry lower bound for the maximum rate of packing in the q-ary Hamming spaces with some physical arguments. These are probably crystalline solutions that cannot be captured within the RSB formalism as the RS entropy is expected to be larger than that of the glassy solutions in the RSB phase. As a concluding remark, we should mention that the 1RSB study presented here is not complete; the SP equations only consider the hard or frozen part of the cavity messages. A more complete study of the 1RSB approximation also takes the soft part of the messages into account and asks for the stability of these solutions [33–35]. We would like to thank C. Baldassi, A. Braunstein, H. Cohn, S. Franz, S. Torquato, and F. Zamponi for their help and useful discussions. AR and RZ acknowledge the ERC grant OPTINF 267915. The liquid solution of the BP equations would be stable as long as the ferromagnetic (linear) and spin glass (nonlinear) susceptibilities are ﬁnite [33–35]. These conditions can be expressed in terms of the maximum eigenvalue of the response matrix M: where λmax is the maximum eigenvalue (in absolute value) of M. More precisely, for N < N BP L 1 and N < N BP L 2 we would have no continuous phase transition to an ordered and spin glass phase, respectively. Notice that these conditions do not exclude discontinuous phase transitions. The response matrix M is symmetric with real eigenvalues and orthogonal eigenvectors. Using the translational symmetry by even vectors we write the eigenvectors as where σ is the binary vector representing a point in the Hamming space and k is a binary wave vector. The eigenvalues are obtained by plugging the above expression in the eigenvalue equation The above expression is independent of σ and for simplicity we choose σ = (0, 0, . . . , 0). Consider the maximum wave vector k = (1, 1, . . . , 1). The corresponding eigenvalue is Obviously, the maximum eigenvalue is bounded by 2vd. Let us approximate the maximum eigenvalue by the largest contribution at l = n/2, Notice that for large n this is still asymptotically equivalent to the trivial bound 2vd. Using the Sterling approximation for large n we get ησ1,...,σm i→j = 1 − Prob �� σ∈V f Hσ i→j = 1 .OR. � σ∈Λ\V f Hσ i→j = 0 � Here Prob �� σ=σ1,...,σm Hσ i→j = 0 � is the probability of having Hσ1 i→j = 0, Hσ2 i→j = 0, . . . , Hσm i→j = 0. The probability in the numerator can be rewritten in the same way as Notice that the last term in the ﬁrst bracket is canceled with the ﬁrst term in the second bracket. This cancellation indeed happens for any two subsequent brackets. Simplifying the above expression we ﬁnd where V∩(σ′ 1, . . . , σ′ m′) is the intersection of sets Vd(σ′ 1), . . . , Vd(σ′ m′). This is the probability that no variable in V (i)\j sends a survey that forbids state σ1 for variable i. Here I(C) is an indicator function for condition C. And in general σ′ 1<···<σ′ m′ I({σ1, . . . , σm} ∩ V∩(σ′ 1, . . . , σ′ m′) ̸= ∅). (B10) �2n−|V f| m′=0 (−1)m′ � σ′ 1<···<σ′ m′ ∈Λ\V f � k∈V (i)\j (1 − γk→i(σ′ 1, . . . , σ′ m′, σ1, . . . , σm)) �2n m′=1(−1)m′+1 � σ′ 1<···<σ′ m′ � k∈V (i)\j (1 − γk→i(σ′ 1, . . . , σ′ m′)) , (B11) In any approximation it is important to know how well the method approximates the correct behavior. Considering the BP approximation, we know that it is exact at least on tree interaction graphs. On loopy graphs, the BP approximation may overestimate or underestimate the correct entropy. This in general depends on the nature of the interactions and conﬁguration space.
Agent Based Negotiation using Cloud - an Approach in E-Commerce<|sep|>Cloud computing provides various features such as security, scalability, reliability  and low maintenance which are beneficial to negotiation process in E-Commerce. In  this paper, we propose an agent based negotiation system, in which the agent uses  cloud for storage of data and product information. Agents negotiate on some issues  using the product information and seller’s or buyer’s requirement. After completing  negotiation process, agents give feedback to user about whether the negotiation is  successful or not. This negotiation system is dynamic, if number of users is  increased, then number of agents also increases automatically. Our future work is to  make a faster, secure and flexible E-negotiation agent using rule based reasoning and  case based reasoning [10].
Generalized conditional symmetries of evolution equations<|sep|>1. The property of an operator Q to be a conditional symmetry of a diﬀerential equation L is equivalent to the fact that the corresponding invariant surface equation Q[u] = 0 is formally compatible (in a certain sense) with L, i.e., the joint system of the above two equations has no nontrivial diﬀerential consequences. This determines what diﬀerential consequences of these equations should be involved in the criterion of the conditional invariance of the equation L with respect to the operator Q. In the property of formal compatibility conditional symmetries diﬀer from purely weak symmetries [45, 52] for which the calculation of integrability conditions (resp. “the reduction to passive form”) of the corresponding joint systems has to be carried out in each case. In fact, weak symmetries of L are associated with diﬀerential constraints whose solution sets at least intersect the solution set of L. 2. Therefore, the criterion of conditional invariance in fact is nothing but the criterion of formal compatibility for a system associated with the pair of equations L and Q[u] = 0. This has two consequences: There does not exist a universal explicit criterion of conditional invariance similar to the criterion of Lie invariance, which would contain a priori the complete information which diﬀerential consequences to take into account and would be appropriate for any system of diﬀerential equations and any set of generalized vector ﬁelds. At the same time, for any ﬁxed pair of a system of diﬀerential equations and a set of generalized vector ﬁelds the criterion can be formulated in diﬀerent forms. 3. Single generalized conditional symmetries are assumed equivalent if they diﬀer by multipliers being nonvanishing diﬀerential functions. Therefore it suﬃces to consider only symmetries with characteristic containing some isolated (e.g., highest-order) derivative of the unknown function. 4. In order to be usable, a conditional symmetry should correspond to an integrable diﬀerential constraint which admits a simple representation of its general solution. Such a representation is considered as an ansatz for the solution of the initial equation L. The formal compatibility of the diﬀerential constraint with L should imply a (strong) reduction of L by the ansatz. In other words, after the substitution of the ansatz into L we should obtain a system of diﬀerential equations of a simpler structure, e.g., with a smaller number of independent variables. Symmetries equivalent as vector ﬁelds induce the same set of ansatzes and equivalent reductions. In fact there does not exist a universal precise deﬁnition of reduction which does not involve splitting with respect to parametric variables and covers all possible representations of solutions. In view of the above problems of integrability and deﬁning reduction, it is still unclear in the general case what diﬀerential constraints formally compatible with the initial equation should be considered associated with reduction operators. This question becomes trivial and has a positive answer in the situation considered in the paper (single evolution equations and diﬀerential constraints depending only on derivatives with respect to x). Probably, in the general case it would be more natural to assume that the notion of reduction operator is narrower than the notion of conditional symmetry, cf. [43]. 5. If the characteristics of operators coincide on the manifold determined by the initial equation L and one of the operators corresponds to a diﬀerential constraint formally compatible with L then the other operators have the same property. Such conditional symmetry operators can be considered equivalent in a weak sense since they are associated with the same set of invariant solutions of L. At the same time, they are inequivalent, in general, from the point of view of their usefulness for ﬁnding solutions. In particular, they may give inequivalent ansatzes and reduced systems. These general principles can be applied in other situations as well. We plan to complete soon our study on basic properties of usual (i.e., ﬁrst-order quasilinear) conditional symmetries of systems of diﬀerential equations. In spite of the no-go results presented in the paper, generalized conditional symmetries can be eﬀectively applied to the construction of exact solutions of evolution equations. As it is impossible to exhaustively describe generalized conditional symmetries of a ﬁxed evolution equation, they should be looked for under additional constraints or in special classes of diﬀerential functions, e.g., with separated variables. In this way, usual and generalized conditional symmetries were studied for a number of particular subclasses of evolution equations, cf. the discussion in the end of Section 7. Note that only in [43] generalized conditional symmetries which are not in reduced form were considered. Generalized conditional symmetries were also used for the exact solution of initial-value problems for evolution equations [4, 68]. Another relevant direction of research is the related inverse problem, namely, the description of evolution equations possessing certain generalized conditional symmetries, see [31, 58, 59, 61] and references therein. A systematic investigation of generalized conditional symmetries of non-evolution equations in fact is not available in the literature at the moment. An exception is the paper [43] of Olver mentioned in the introduction, where the connection between the reduction of a partial diﬀerential equation by a generalized ansatz within the higher-order direct method of Galaktionov [20] and the compatibility of the associated diﬀerential constraint with this equation was discovered. At the same time, there exist a number of examples on the application of generalized ansatzes to ﬁnding exact solutions of non-evolution equations, which are collected, e.g., in [15, 25]. It is obvious that all such examples can be interpreted within the framework proposed in [43]. Ansatzes of another kind with new unknown functions depending on diﬀerent arguments arise under generalized separation of variables [1, 66]. Theoretical aspects of this subject should certainly be further investigated. The authors are grateful to Vyacheslav Boyko, Artur Sergyeyev, Christodoulos Sophocleous and Olena Vaneeva for useful discussions and interesting comments. We are thankful to the referees for helpful suggestions that have led to improvements of the paper. MK was supported by START-project Y237 of the Austrian Science Fund (FWF). The research of ROP was supported by project P20632 of FWF.
A surface moving mesh method based on equidistribution and alignment<|sep|>We have proposed a direct approach for surface mesh movement and adaptation that can be applied to a general surface with or without analytical expressions. We did so by ﬁrst proving the relation (4) between the area of a surface element in a Riemannian metric and the Jacobian matrix of the aﬃne mapping between the reference element and any simplicial surface element. From this we formulated the equidistribution and alignment conditions as given in (7) and (9), respectively. These two conditions enabled us to formulate a surface meshing function that is similar to a discrete version of Huang’s functional for bulk meshes [13]. The surface function satisiﬁes the coercivity condition (37) for θ ∈ (0, 1/2] and p > 1. We deﬁned the surface MMPDE (32) as the gradient system of the meshing function, which utilizes surface normal vectors to inherently ensure that the mesh vertices remain on the surface during movement. Equations (26) and (27) give explicit, compact formulas for the mesh velocities making the time integration of the surface MMPDE (33) relatively easy to implement. Moreover, we showed that this surface MMPDE satisﬁes the energy decreasing property, which is one of the keys to proving Theorem 4.1. This theorem is an important theoretical result as it states that the surface mesh remains nonsingular for all time if it is so initially. Finally, we proved Theorem 4.2 that states the mesh has limiting meshes, all of which are nonsingular. A point of emphasis is that the new method is developed directly on surface meshes thus, making no use of any information on surface parameterization. As mentioned, the MMPDE (32) only depends on surface normal vectors which can be computed even when the surface has a numerical representation. This allows the new method to be applied to general surfaces with or without explicit parameterization. The numerical results presented in this work demonstrated that this new approach to surface mesh movement is successful. In all of the examples, the ﬁnal mesh was seen to be much more uniform with respect to both cases of the metric tensor MK = I and MK = (kK + ϵ)I which was supported by the mesh quality measures. Moreover, the theoretical properties were numerically veriﬁed in Ex. 5.3 and Ex 5.7 as we showed that Ih is decreasing and |K| is bounded below. The future goal is to develop this algorithm for any surface with or without analytical expression. Even though we only presented examples which have analytical expressions, we should emphasize that the MMPDE (33) uses only the normal direction of the surface. Since these derivatives can be obtained numerically from the initial mesh or a background mesh, the method developed in this paper should work in principle for surfaces without explicit expressions. A practical diﬃculty is that the initial mesh or a background mesh typically does not represent the underlying surface accurately and approximate partial derivatives of Φ obtained directly from the mesh may not lead to acceptable results. Our next step in the research is to investigate the use of spline approximations of surfaces for this purpose. Moreover, the monitor functions we used in the examples are limited to simple scalar metric tensors. It will be interesting to see how an anisotropic metric tensor such as one based on the shape map aﬀects mesh movement and quality. Acknowledgement. We would like to thank Dr. Lei Wang at the University of Wisconsin-Madison for providing us her code to generate initial icosahedral meshes for Example 5.8. Let t be an entry of [xK 1 , xK 2 , . . . , xK d ]. Using the chain rule we have To begin, consider (50). When t is an entry of [xK 2 , . . . , xK d ], recalling that EK = [xK 2 −xK 1 , . . . , xK d − xK 1 ], we have ∂GK −1 −1 · · · −1 0 0 · · · 0 ... ... ... 0 0 · · · 0 We can have similar expressions for � xK 1 �(j) for j = 2, . . . , d. This gives where eT = [1, . . . , 1] ∈ R1×(d−1). For (51), we assume that M = M(x) is a piecewise linear function with the vertex xK j for all j = 1, . . . , d. Denote the ith components of x and xK by x(i) and x(i) K , respectively. Then, for any entry t of [xK 1 , xK 2 , . . . , xK d ], we have ∂t is a dot product. From this and the identity xK = (xK 1 + · · · + xK d )/d, we get
The ALICE experiment -- A journey through QCD<|sep|>Event selection. The study of event and multiplicity selection in small collision systems has been proven to be crucial for a proper understanding of the physics observables across different colliding nuclei. These studies indicate that further measurements in the ﬁeld of relativistic heavy-ion collisions must take proper care to understand such selections carefully, especially in high-multiplicity proton– proton and proton-nucleus collisions. Identiﬁed particle production. A comprehensive set of identiﬁed particle measurements from lowmultiplicity proton–proton to highest multiplicity Pb–Pb collisions has conclusively demonstrated that identiﬁed particle ratios such as Ξ/π and Ω/π evolve continuously as a function of charged-particle multiplicity density, regardless of collision system or beam energy. Furthermore, speciﬁc ﬁnal-state effects such as the depletion / excess of baryons with respect to mesons at low-/ intermediate- transverse momentum have been observed to be present throughout all collisions systems in different magnitudes. Recent measurements have even extended this pattern to the Λc/D0 ratio. A successful description of all these phenomena for all collision systems remains a theoretical challenge of the ﬁeld. Collective behaviour. Measurements of azimuthal correlations and anisotropic ﬂow in small collision systems exhibit features of a collectively expanding system, similar to those observed in heavy-ion collisions, where they are believed to originate from the presence of QGP medium. The origin of the collective effects depends on particle multiplicity of a collision. While hydrodynamic-like description seems to be favored by data especially at high multiplicities, the effects of initial state effects from initial gluon momentum correlations may play an important role at low-Nch. Charmonium and Bottomonium. Charmonium measurements in p–Pb collisions indicate the ψ(2S) suppression in the backward rapidity region (Pb-going) to be larger with respect to that of the J/ψ. This is not expected from cold nuclear matter effects and is suggestive of ﬁnal state interactions between the weakly bound cc and a strongly interacting partonic or hadronic medium, leading to its dissociation. Similar suppression patterns are observed also in the bottomonium sector for ϒ(2S) and ϒ(3S). Searches for jet quenching. Measurements in small systems have not observed signiﬁcant jet quenching effects thus far within the sensitivity of the measurements. These measurements however provide the ﬁrst quantitative limit on jet quenching in small systems, corresponding to medium-induced energy transport out of a jet cone with radius 0.4 less than 400 MeV at 90% conﬁdence level in high-event-activity p–Pb collisions.
Dependent Inductive and Coinductive Types are Fibrational Dialgebras<|sep|>We have seen how dependent inductive and coinductive types with type constructors, in the style of Agda, can be given semantics in terms of data type closed categories (DTCC), with the restriction that destructors of coinductive types are not allowed to refer to each other. This situation is summed up in the following table. Cloven ﬁbration Deﬁnition of signatures and data types Data type completeness Construction of types indexed by objects in base (e.g., vectors for N ∈ B) and types agnostic of indices (e.g., initial and ﬁnal objects, sums and products) Data type closedness Constructed types as index; Full interpretation of data types Moreover, we have shown that a large part of these data types can be constructed as ﬁxed points of polynomial functors. Let us ﬁnish by discussing directions for future work. First, a full interpretation of syntactic data types has also still to be carried out. Here one has to be careful with type equality, which is usually dealt with using split ﬁbrations and a Beck-Chevalley condition. The latter can be deﬁned generally for the data types of this work, in needs to be checked, however, whether this condition is sufﬁcient for giving a sound interpretation. Finally, the idea of using dialgebras has found its way into the syntax of higher inductive types [7], though in that work the used format of dialgebras is likely to be too liberal to
Separation delay via hydro-acoustic control of a NACA4412 airfoil in pre-stalled conditions<|sep|>Passive ﬂow control on a NACA 4412 airfoil was investigated by means of acoustic resonance of modeled Helmholtz resonators. We manage to achieve successful hydrodyanmic control of the trailing edge ﬂow separation, when appropriate impedance panels are located just upstream the separated region. Although the control set up is fully passive, it operates at the natural resonator frequency, and may be tuned diﬀerently to target diﬀerent conditions of operation. We show that a signiﬁcant (+13%) increase in lift is obtained if the resonant frequency is appropriately chosen regarding the time scales of the detached shear layer: frequencies below the shedding frequency F + res < F + shed used in combination with relatively high permeability (R = 0.2) generate large coherent Kelvin Helmholtz rollers, suﬃcient to drain momentum from the farﬁeld and periodically reattach the ﬂow ﬁeld. This passive ﬂow control strategy is very attractive as it may address diﬀerent ranges of operation, hence oﬀering passive set up advantages of reduced maintenance and higher reliability, while providing additional ﬂexibility in comparison with traditionnal passive set up.
