Climate & BCG: Effects on COVID-19 Death Growth Rates<|sep|>The COVID-19 pandemic has triggered extensive efforts to predict the severity of COVID-19 to aid in decision making around interventions such as lockdown and the closure of schools 1 [1]. Regions hit hard by the pandemic, such as Wuhan, Lombardy and New York, where doubling times of 2-3 days and high crude mortality rates stand in stark contrast to other countries only mildly aﬀected such as Hong Kong, South Korea and New Zealand. Potential explanations for the apparent diﬀerences in the transmissivity (encoded by the time-dependent reproductive number, Rt) and lethality (encoded by the Infection Fatality Rate, IFR), currently fall into four broad categories. The ﬁrst posits that diﬀerences are largely ﬁctitious, driven by the heterogeneity of testing and reporting of cases and deaths; a known issue and one that we are particularly concerned with in this paper. The next three categories posit that the diﬀerences are primarily real and are driven by (1) cultural and policy factors (swift lockdown, eﬃcient contact tracing and quarantining, use of masks, obedient populations or social structures that are naturally distant or isolated), (2) local environmental factors (such as temperature and humidity variations, population density, comorbid factors, vaccinations, vitamin D levels, blood type etc...) or (3) existence of multiple strains with diﬀerent transmissibility or lethality [4]. Our primary interest lies in disentangling the ﬁrst category (testing) from a subset of potential factors in the second category. Finding the relative contributions of each of these four categories is key in understanding and optimally ﬁghting the pandemic. The widely diﬀerent testing capabilities between countries, particularly between the developed and developing world, imply that if not correctly treated, testing variability will create spurious evidence that can lead to false hope and sub-optimal interventions. In the wake of the spreading pandemic there have been a host of studies that have examined the possible impact of climate [8]-[19], blood type [20, 21], haplogroup [31], pollution [26]-[30] and BCG vaccination prevalence [22, 23, 24, 25] on the spread of COVID-19. Our main conclusion is that testing issues are signiﬁcant and the factors above are likely not the main causes of variability in growth rates of deaths worldwide. We note that both the basic reproductive number, R0, and the IFR will be aﬀected by the four categories above in general. However, since we do not currently have access to the true number of infections we cannot address the potential eﬀect of environmental factors on the IFR. Similarly the Case Fatality Rate cannot be used for this purpose due to testing diﬀerences around the world. We therefore focus on their potential eﬀect on R0, which we quantify through the daily growth rate of deaths of countries around the world for which we have suﬃciently good data.
Provable Self-Representation Based Outlier Detection in a Union of Subspaces<|sep|>In many applications in computer vision, including motion estimation and segmentation [19] and face recognition [2], high-dimensional datasets can be well approximated by a union of low-dimensional subspaces. Such applications have motivated a lot of research on the problems of learning one or more subspaces from data, a.k.a. subspace learning and subspace clustering, respectively. In practice, datasets are often contaminated by points that do not lie in the subspaces, i.e. outliers. In such situations, it is often essential to detect and reject these outliers before any subsequent processing/analysis is performed. Prior work. We address the problem of outlier detection in the setting when the inlier data are assumed to lie close to a union of unknown low-dimensional subspaces (low relative to the dimension of the ambient space). A traditional method for solving this problem is RANSAC [13], which is based on randomly selecting a subset of points, ﬁtting a subspace to them, and counting the number of points that are well ﬁt by this subspace; this process is repeated for sufﬁciently many trials and the best ﬁt is chosen. RANSAC is intrinsically combinatorial and the number of trials needed to ﬁnd a good estimate of the subspace grows exponentially with the subspace dimension. Consequently, the methods of choice have been to robustly learn the subspaces by penalizing the sum of unsquared distances (in lieu of squared distances used in classical methods such as PCA) of points to the closest subspace [9, 22, 62, 61]. Such a penalty is robust to outliers because it reduces the contributions from large residuals arising from outliers. However, the optimization problem is usually nonconvex and a good initialization is extremely important for ﬁnding the optimal solution. The groundbreaking work of Wright et al. [54] and Cand`es et al. [4] on using convex optimization techniques to solve the PCA problem with robustness to corrupted entries has led to many recent methods for PCA with robustness to outliers [55, 32, 25, 60, 21]. For example, Outlier Pursuit [55] uses the nuclear norm ∥ · ∥∗ to seek low-rank solutions by solving the problem minL ∥X −L∥2,1+λ∥L∥∗ for some λ > 0. A prominent advantage of convex optimization techniques is that they are guaranteed to correctly identify outliers under certain conditions. Very recently, several nonconvex outlier detection methods have also been developed with guaranteed correctness [20, 6]. Nonetheless, these methods typically model a unique inlier subspace, e.g., by a low rank matrix L in Outlier Pursuit, and therefore cannot deal with multiple inlier subspaces since the union of multiple subspaces could be high-dimensional. Another class of methods with theoretical guarantees for correctness utilizes the fact that outliers are expected to have low similarities with other data points. In [5, 1], a multiway similarity is introduced that is deﬁned from the polar curvature, which has the advantage of exploiting the subspace structure. However, the number of combinations in multi-way similarity can be prohibitively large. Some recent works have explored using inner products between data points for outlier detection [17, 40]. Although computationally very efﬁcient, these methods require the inliers to be well distributed and densely sampled within the subspaces. Figure 1. An illustration of a self-representation matrix R in the presence of outliers. The ﬁrst 32 columns of the data matrix X correspond to 32 images of one individual under different illuminations from the Extended Yale B database, and the next 32 images are randomly chosen from all other individuals; three examples from each category are shown near the top of 1(a). We also show a typical representation vector for an inlier and an outlier image in 1(a), and the complete representation matrix R in 1(b), where white and black denote rij ̸= 0 and rij = 0. Notice that inliers use only other inliers in their representation, while outliers use both inliers and outliers in their representations. Overview of our method and contributions. In this work, we address the problem of outlier detection by using data self-representation. The proposed approach builds on the self-expressiveness property of data in a union of lowdimensional subspaces, originally introduced in [11], which states that a point in a subspace can always be expressed as a linear combination of other points in the subspace. In particular, if the columns of X = [x1, · · · , xN] lie in multiple subspaces, then for all j = 1, . . . , N, there exists a vector rj ∈ IRN such that xj = Xrj and the nonzero entries of rj correspond to points in the same subspace as xj. If the subspace dimensions are small, rj can be taken to be sparse and be computed by solving the ℓ1 minimization problem for some γ > 0. In [11], an undirected graph is constructed from R = [r1, · · · , rN] in which each vertex corresponds to a data point, and vertices corresponding to xi and xj are connected if either rij or rji is nonzero. Such a graph can be used to segment the data into their respective subspaces by applying spectral clustering [48] to the graph’s Laplacian. Consider now the case where X contains outliers to the subspaces. Figure 1 illustrates an example representation matrix R computed from (1) for data drawn from a single subspace (face images from one individual) plus outliers (other images). In this case, the representation R is such that inliers express themselves as linear combinations of a few other inliers, while outliers express themselves as linear combinations of both inliers and outliers. Motivated by this observation, we use a directed graph to model data relations: a directed edge from xj to xi indicates that xj uses xi in its representation (i.e. rij ̸= 0). Then a random walk on the representation graph initialized at an outlier will not return to the set of outliers since once the random walk reaches an inlier it cannot return to the outliers. Therefore, we design a random walk process and identify outliers as those whose probabilities tend to zero. Our work makes the following contributions with respect to the state of the art: 1. Our method can detect outliers using the probability distribution of a random walk on a graph constructed from data self-representation. 2. Our data self-representation allows our method to handle multiple inlier subspaces. Knowledge of the number of subspaces and their dimensions is not required, and the subspaces may have a nontrivial intersection. 3. Our method can explore contextual information by using a random walk, i.e., the “outlierness” of a particular point depends on the “outlierness” of its neighbors. 4. Our analysis shows that our method correctly identiﬁes outliers under suitable assumptions on the data distribution and connectivity of the representation graph.
Full Diversity Space-Time Block Codes with Low-Complexity Partial Interference Cancellation Group Decoding<|sep|>Multiple-input multiple-out (MIMO) wireless communications have been witnessed to offer large gains in spectral efﬁciency and reliability [1], [2]. Efﬁcient designs of signal transmission schemes include space-time (ST) codes over MIMO systems have been active areas of research over the past decade [3]. Orthogonal ST block code (OSTBC) is one of the most powerful ST code designs due to its simple low-complexity maximum-likelihood (ML) decoding while achieving maximum diversity gain [4]–[7]. However, it is found that OSTBC has a low code rate that cannot be above 3/4 symbols per channel use for more than two transmit antennas [8]. To improve the code rate of the STBC, numerous code designs have been developed including quasi-orthogonal STBC [9]–[19] and STBC based on algebraic number theory [20]–[30]. Two typical designs of those codes are threaded algebraic ST (TAST) codes [21], [23] and cyclic division algebra based ST codes [24]–[30] which have been shown to obtain full rate and full diversity. The full rate means M symbols per channel use for M transmit antennas. Note that the OSTBC for two transmit antennas, also namely Alamouti W. Zhang and L. Shi are with School of Electrical Engineering and Telecommunications, The University of New South Wales, Sydney, Australia (e-mail: {w.zhang; long.shi}@unsw.edu.au). Their work was supported in part by the Australian Research Council Discovery Project DP1094194. X.-G. Xia is with Department of Electrical and Computer Engineering, University of Delaware, DE 19716, USA (e-mail: xxia@ee.udel.edu). His work was supported in part by the Air Force Ofﬁce of Scientiﬁc Research (AFOSR) under Grant No. FA9550-08-1-0219. code [4], has a rate of 1 symbol per channel use only, i.e., two independent information symbols are sent through a codeword occupying two symbol intervals. Since most of the highrate STBC are designed based on the rank criterion which was derived from the pairwise error probability of the ST codes with ML decoding [5], they have to rely on the ML decoding to collect the full diversity. Considering that the ML decoding complexity grows exponentially with the number of information symbols embedded in the codeword, the high-rate STBC obtain the full diversity at a price of the large decoding complexity. Recently, several fast decodable STBC have been proposed to reduce the high decoding complexity while not compromising too much performance gains [31]–[33]. MIMO systems with linear receivers have also received a lot of research attention and information-theoretic analysis has been done in [34]–[38]. Efﬁcient designs of ST codes for transmission over MIMO systems with linear receivers have also been studied in [39]–[43]. Linear receiver based STBC designs are attractive because they can exploit both gains of efﬁciency and reliability of the signal transmission over MIMO systems with a lowcomplexity receiver such as zero-forcing (ZF) or minimum mean square error (MMSE) receiver. Similar to the OSTBC, the STBC designs in [39]–[43] can also obtain full diversity with linear receivers. However, it is found that the rate of the linear receiver based STBC is upper bounded by one [40], though it is larger than that of OSTBC. To address the complexity and rate tradeoff, a partial interference cancelation (PIC) group decoding for MIMO systems was proposed and the design criterion of STBC with PIC group decoding was also derived in [44]. In fact, the PIC group decoding can be viewed as an intermediate decoding approach between the ML receiver and the ZF receiver by trading a simple single-symbol decoding complexity for a higher code rate more than one symbol per channel use. Very recently, a systematic design of STBC achieving full diversity with PIC group decoding was proposed in [45]. However, the decoding complexity of the STBC design in [45] is still equivalent to a joint ML decoding of M symbols. In order to further reduce the decoding complexity, in this paper we propose a new design of STBC with PIC group decoding which can obtain both full diversity and lowcomplexity decoding, i.e., only half complexity of the STBC in [45]. Our proposed STBC is featured as an Alamouti block matrix, i.e., every element of the 2 × 2 Alamouti code matrix is replaced by an elementary matrix and each elementary matrix is designed from multiple diagonal layers. It should be mentioned that in [43] the similar Alamouti block matrix was used where each entry of the Alamouti matrix was replaced by a Toeplitz STBC. The major difference between the STBC in [43] and our proposed STBC lie in the construction of elementary matrix, i.e., the Toeplitz matrix used in [43] and the multiple diagonal layers used in our codes. While the STBC in [43] achieves the full diversity with linear receivers but the code rate is not more than 1. It will be shown that our proposed STBC can achieve full diversity under both ML and PIC group decoding and the code rate can be up to 2 when full diversity is obtained. Our simulation results demonstrate that the codes can obtain similar good performance to the codes in [45] but a half decoding complexity is reduced. This paper is organized as follows. In Section II, the system model is described and the PIC group decoding algorithm is reviewed. In Section III, a design of STBC achieving full diversity with a reduced-complexity PIC group decoding is proposed. The full diversity is proved when PIC group decoding is applied. In Section IV, a few code design examples are given. In Section V, simulation results are presented. Finally, we conclude the paper in Section VI. Notation: Throughout this paper we use the following notations. Column vectors (matrices) are denoted by boldface lower (upper) case letters. Superscripts t and H stand for transpose and conjugate transpose, respectively. C denotes the ﬁeld of complex numbers. In denotes the n×n identity matrix, and 0m×n denotes the m×n matrix whose elements are all 0. det(X) represents the determinant of the matrix X. ⊗ denotes the Kronecker product. ||X|| denotes the Frobenius norm of matrix (vector) X.
Skeleton-based Gait Index Estimation with LSTMs<|sep|>Vision-based health-care systems are nowadays becoming popular because of the fast development of related research ﬁelds such as computer vision and machine learning. The task of gait analysis is one application example that has been focused on with such systems. Many researchers attempted to deal with this problem using various input data types such as subject silhouette [2, 3], skeleton [9, 16, 7], depth map [10], typical color image [1], and a combination of types [8]. These inputs usually require different pre-processing operations to have a reasonable representation. In our work, we select the skeleton as the input of our system since it is already represented by a collection of 3D body joints and thus does not need any complicated pre-processing as the others. This stage is typically followed by a feature extraction, in which the gait features are deﬁned under spatial and/or temporal aspects and are thus usually interpretable. Differently from such studies, we perform this task automatically by structuring our index estimator as auto-encoders, where the features of interest are salient inside the network without any supervision. An auto-encoder consists of an encoder, that converts the input into an appropriate representation in a latent space, and a decoder that attempts to reconstruct the input based on the salient features. The difference between an input and its output is thus a reasonable choice as a (weak) gait index. Concretely, by ﬁtting an auto-encoder using patterns of speciﬁc gait types, the loss between an input and its reconstruction should be useful to indicate a (normalized or non-normalized) probability that the input belongs to such known gait types. An auto-encoder is a special type of neural networks, where the input and output are pair-wise deﬁned. Instead of using a feed-forward network, we design our model as a Recurrent Neural Network (RNN) with LSTM cells. There are several reasons for this selection. First, a RNN can embed the temporal factor into its weights. This is an important property because many studies on gait analysis (e.g. [3, 9, 16, 7]) demonstrated that temporal gait assessment provided higher accuracies than assessing individually each frame. Second, a RNN does not require a large capacity for storing parameters compared with non-cyclic neural networks that can provide the same accuracy. Third, a RNN can deal with variable-length inputs while a feed-forward network only works on inputs of Published as a conference paper at 2018 IEEE/ACIS 17th Int. Conf. on Computer and Information Science. The ﬁnal version is available at: https://doi.org/10.1109/ICIS.2018.8466522 Figure 1: An illustration of our joint selection. The blue circles indicate the 17 selected joints and the green circles correspond to the 8 discarded joints. a ﬁxed size. Although we performed our experiments (see Section 3) on inputs of the same size, this property is also important in practical applications where the skeletons may be acquired under different walking speeds or camera frame rates. The remaining of this paper is organized as follows: Section 2 describes the structure details as well as the workﬂow of our system; the employed dataset and our experimental results with the proposed method together with some related works are presented in Section 3; and Section 4 gives the conclusion.
Aggregates of clusters in the Gaia data<|sep|>Assuming that the stellar members of a given open cluster are born from a molecular cloud which is chemically almost homogeneous, one can determine the distance, reddening, age, and metallicity for these stars with the help of statistical methods (for example, isochrone ﬁtting techniques). The latter two quantities are especially diﬃcult to determine for single stars. This shows that galactic star clusters represent an important tool for studying the evolution and properties of stars and the Milky Way itself. Binary clusters are especially interesting for studying the formation and evolution on a global scale. The pairs could either be formed at the same time (Priyatikanto et al. 2016) or sequentially caused by stellar winds or supernova shocks generated by one cluster inducing the collapse of a nearby cloud, thus triggering the formation of a companion cluster (Goodwin 1997). Furthermore, they can be formed completely separately and then captured either by tidal forces (van den Bergh 1996) or resonant trapping (Dehnen & Binney 1998). Since its ﬁrst data release, the Gaia satellite presented an improvement in the measured parallaxes of stars and expanded the list of known stars by several orders of magnitude. With the use of Gaia’s most recent accurate astrometric and photometric measurements (Gaia Collaboration et al. 2018), it is not only possible to study clusters at larger distances but also to reﬁne the list of nearby clusters. Cantat-Gaudin et al. (2018, CG18 from now on) present a status report for 1229 open cluster based on the Gaia Data Release 2 (Gaia DR2). They establish the parallaxes, the proper motions, and the most likely distances of the clusters, together with the membership probabilities of the individual stars (based on UPMASK). These data were later expanded and currently describe 1481 clusters (Cantat-Gaudin & Anders 2020, CG20). Soubiran et al. (2018) examined the kinematical properties in the 6D phase space of open clusters derived from Gaia DR2 data. They have conﬁrmed that the velocity distribution of clusters coincides well with the velocity distribution of stars at given Galactic locations. Furthermore, they have shown that several clusters seem to be conﬁned to the same (small) volume of the phase space. This suggests that there may be a physical connection between the clusters. Any procedure used for detecting clusters and deriving cluster membership probabilities is expected to have some limits. Using only astrometric measurements (stellar coordinates, proper motion, and parallax) necessarily leads to the assignment of non-member stars to the clusters, although Bayesian statistics may help to constrain the number of such false identiﬁcations. For the parallax space, this is discussed in Luri et al. (2018), for example. Additional information can be extracted from the colour-magnitude diagrams of the clusters, which may further constrain the membership probabilities. Overall, the ﬁnal list of cluster members will never completely resemble the true host cluster – the quality of such a representation will depend on the number of detected cluster members (signal), the number density of the surrounding ﬁeld stars (noise), and instrumental errors (random and systematic). Based on the arguments above, it is possible for an automated procedure to fail under special circumstances. An example would be a case of two apparently (or physically) neighbouring clusters which may overlap in the position, parallax, and proper motions space – this was already predicted by the creators of the UPMASK method (Krone-Martins & Moitinho 2014). The only assumption used by this procedure is that the distribution of stars in the phase space is more tight than in the case of a random (uniform) distribution. The procedure is composed of two main steps – the identiﬁcation of clusters in the Table 1: Aggregates used for the detailed analysis. The presented values are the mean values for the whole clusters taken from CG20. data (based on the k-means clustering method) and the comparison of the groupings with random distributions (returning a binary value which states whether the grouping is a cluster or not). The process is applied several times, and the membership probability of each star is determined based on the ratio number of positive assignments total number of iterations . If some of the cluster members are assigned to more than one cluster, one must be cautious with the calculation of the cluster parameters. We ﬁnd it important to re-examine the data presented by CG20. Our goal is to investigate the number of duplicates appearing in the data set and determine their impact on the derived cluster parameters. However, we do not aim to search for new cluster groups.
3C273 variability at 7 mm: Evidences of shocks and precession in the jet<|sep|>Even though the launch of the Fermi Gamma-ray Space Telescope resulted in an unprecedented amount of data, which complemented the already known lower frequency spectral energy distribution (SED) of active galactic nuclei (AGNs), the actual emission process at each frequency is still under debate. There is no doubt that the radio emission has synchrotron origin, but the high energy X- and γ-rays can be attributed to diﬀerent processes, like inverse Compton up scattering of low frequency photons, either of synchrotron origin (Synchrotron Self Compton, SSC), or external (External Compton, EC), or even hadronic processes initiated by relativistic protons (see review by B¨ottcher 2010). Besides a quiescent or slowly varying emission, blazars present short duration high energy γ-ray ﬂares with intensities that can diﬀer in several orders of magnitude, even for ﬂares in the same object. Similar but longer lasting ﬂares are observed at infrared and radio frequencies, the latter associated with the appearance of relativistically beamed super luminal components in the parsec scale jets (Jorstad et al. 2001). As the radio emission, the γ-ray ﬂux must be also relativistically beamed, to account for the short variability timescales and for the small optical depth for pair production (Mattox et al. 1993; Wehrle et al. 1998). The superluminal components do not have the same apparent velocity and position angle in the plane of the sky (eg. Cotton et al. 1979), and in the case of 3C273, the systematic variation of these quantities were interpreted as due to jet precession, assuming ballistic motion for the components (Abraham & Romero 1999). The period detected was 16 years, which together with the black hole mass, suggested that the Bardeen-Peterson eﬀect could be the origin of this precession (Caproni et al. 2004). Considering only single dish observations, the radio emission of 3C273 was extensively monitored at diﬀerent frequencies (T¨urler et al. 1999; Soldi et al. 2008). This long time coverage (almost 40 years) allowed statistical studies that revealed the existence of several periodicities, in
Material Dependence of the Wire-Particle Casimir Interaction<|sep|>Casimir forces contribute signiﬁcantly to the eﬀective interaction of micro- and nanometer sized structures [1]. For identical objects or mirror symmetric conﬁgurations, this type of interaction is attractive [2] and can cause stiction in micro-motors and other similar structures [3]. More generally, if the permittivities of the objects are higher or lower than those of the surrounding medium, any equilibrium position of the objects is unstable due to the Casimir interactions [4]. Therefore, a good quantitative understanding of such forces is a key parameter in the design and manufacturing of micro-mechanical devices. It is important to study the Casimir forces for diﬀerent shapes as they strongly depend on the geometry and material properties [1, 5, 6]. Technically, investigating the interplay between the shape and material eﬀects is quite involved. The scattering formalism provides a powerful tool to calculate the Casimir interaction between objects of general shape and material properties [7]. There is much recent research activity based on the scattering formalism, e.g., for edges and tips [8, 9], anisotropic particles [10], wires and plates [11–14], and spheres and plates [15]. An important geometry which has not yet been investigated in detail, consists of a wire and a particle (atomic or macroscopic). In the plane-particle geometry this force is known as Casimir-Polder (CP) interaction [16]. Our study of the wire-particle case is motivated by theories [17] and experiments [18, 19] on the two-dimensional quantum scattering of neutral atoms or molecules at wires or nanotubes. In an early work, the interaction between a ﬁlament and an isotropic atom has been studied for perfectly and non-perfectly conducting metals [20]. Later, Eberlein and Zietal studied the interaction between a neutral atom and a perfect metal cylinder, using perturbation theory [21, 22]. Recently, the Casimir energy for a polarizable micro-particle and an ideal metal cylindrical shell has been computed using the Green’s function technique [23]. The focus of previous studies were mainly on the interaction between a metal wire and a perfect metal particle or an atom. Therefore, the inﬂuence of material properties of the spherical particles on the energy remains to be studied in detail. In this work, we study the Casimir interaction between a metallic spherical particle and cylindrical metallic wire where the latter is described by the Drude, plasma or perfect metal model. Using the scattering formalism, we derive a general expression for the Casimir interaction between the particle and cylinder. From this general expression we determine the behavior of the Casimir energy in various limiting cases (separation regimes) analytically, and numerically over a wide range of separations. Interestingly, we ﬁnd ranges of distances in which the Casimir interaction does not depend on the material properties of the metallic wire. In contrast, we ﬁnd that the interaction depends in general on the material properties of the metallic particle at all separations. An exception is the plasma sphere with a plasma wavelength that is much smaller than the size of the sphere for which the Casimir interaction is universal at asymptotically large distances. At short separations, we compute the exact Casimir interaction numerically and compare it both with the asymptotic results and the prediction of proximity force approximation (PFA). In both limits we obtain good agreement. The structure of the rest of the paper is as follows: In Sec. II, we review the scattering approach and derive the elements that are needed for computing the interaction between a wire and a particle. In Sec. III, the large– distance asymptotic interaction between a metallic wire and a particle (metal sphere and isotropic atom) is derived for the perfect metal, plasma and Drude models. In Sec. IV, the exact Casimir interaction is computed numerically and compared with the asymptotics expan
The luminosities of cool supergiants in the Magellanic Clouds, and the Humphreys-Davidson limit revisited<|sep|>Models of stellar evolution predict that stars with initial masses Minit ≥ 8M⊙ should swell up to become cool supergiants (SGs) when they leave the main-sequence. However, it has long since been established that there is an upper luminosity limit Lmax above which no cool SGs are observed (Stothers 1969; Sandage & Tammann 1974), now commonly referred to as the HumphreysDavidson (H-D) limit (Humphreys & Davidson 1979). The existence of this limit implies that the highest mass stars do not evolve to the cool side of the Hertzsprung-Russell (H-R) diagram and instead remain more compact, ending their lives as either blue hypergiants or Wolf-Rayet stars. The common interpretation of this luminosity limit is that it is caused by mass-loss, either via a smooth wind, or by episodic Luminous Blue Variable (LBV) type eruptions: the more massive the star, the stronger the mass-loss, resulting in a larger fraction of the star’s initial mass being lost prior to core-collapse supernova (ccSN). Above some initial mass threshold, the entire H-rich envelope can be lost before the star can evolve to the cool side of the H-R diagram, causing it to evolve directly to the Wolf-Rayet (WR) phase. Just below this mass limit, stars are expected to have a brief cool SG phase before becoming a WR (e.g. Stothers & Chin 1979; Chiosi & Maeder 1986). Therefore, Lmax is sensitive to the mass-loss rates of stars integrated over their lifetimes. The most luminous Red Supergiants (RSGs) identiﬁed by Humphreys & Davidson (1979) in the Milky Way and Large Magellanic Cloud (LMC) were inferred to have log L/L⊙ = 5.74 and 5.66, respectively, interpreted as reﬂecting a genuine limit at log L/L⊙ = 5.8±0.1. These measurements of Lmax relied upon assumed optical bolometric corrections for RSGs, uncertain distances to the Galactic cool supergiants, an outdated distance modulus to the LMC, and a selective sample of optically-bright stars. Hence, dust-enshrouded cool hypergiants (e.g. van Loon et al. 2005a) would have been missed from their optical study, while those with moderate circumstellar extinction may have had their luminosities
Physical Parameters of Asteroids Estimated from the WISE 3 Band Data and NEOWISE Post-Cryogenic Survey<|sep|>NASA’s Wide-ﬁeld Infrared Survey Explorer mission (WISE; Wright et al. 2010; Cutri et al. 2012) launched on 14 December 2009 and operated until placed into hibernation on 17 February 2011. WISE surveyed the entire sky near 90◦ solar elongation in four infrared wavelengths: 3.4, 4.6, 12 and 22 µm (denoted W1, W2, W3, and W4 respectively). This scan pattern resulted in an average of 12 exposures on the ecliptic, rising to hundreds of exposures at the ecliptic poles with 6.5 arcsecond spatial resolution at 12 µm. Cooling for all four detectors was provided by dual solid hydrogen tanks. Survey operations began on 7 January, 2010, and the ﬁrst pass on the entire sky was completed six months later. Coverage of the solar system was incomplete at that time owing to the long synodic periods of nearEarth objects (NEOs) and many Main Belt asteroids (MBAs). The outer hydrogen tank was exhausted on 5 August, 2010, resulting in the loss of the 22 µm channel that day. As the remaining hydrogen ice in the inner tank sublimated, both the detectors and the telescope temperature rose. During this period, the 12 µm channel continued to operate (albeit with reduced sensitivity) until 29 September, 2010. The inner tank’s hydrogen supply was then ﬁnally exhausted, and the telescope temperature rose, resulting in the loss of this channel. Before launch, the WISE baseline data processing pipeline was enhanced with the WISE Moving Object Pipeline System (WMOPS) to enable the independent discovery of new minor planets in near real-time and archive individual exposures. These augmentations to the WISE pipeline, collectively known as “NEOWISE,” resulted in the discovery of ∼34,000 new asteroids, including 135 new NEOs. The survey has reported observations of >158,000 minor planets (Mainzer et al. 2011a). Beginning 1 October, 2010, NASA’s Planetary Science Directorate funded a four month extension, known as the NEOWISE Post-Cryogenic Survey, to search for new asteroids using WMOPS and to ﬁll in the gap in coverage of the inner Main Asteroid Belt (see Figure 1 of Mainzer et al. 2011a). The focal plane assemblies, optics and telescope temperature warmed to 73.5 K, cold enough to permit observations in the two shortest wavelengths. As discussed in Cutri et al. (2012), the W1 and W2 arrays operated with minimal performance degradation. Data collection was halted on 1 February, 2011. To date, the NEOWISE Post-Cryogenic Survey data have been processed using a ﬁrstpass version of the reduction pipeline. The image data were calibrated using early versions of dark and ﬂat ﬁeld images and bad pixel masks that were largely derived from the fully cryogenic (FC) mission phase. The current processing version does not beneﬁt from the improvements implemented in the second-pass processing that have been applied to the WISE All-Sky and 3-Band Cryogenic Data Release products. Furthermore, the precise calibrations needed to compensate optimally for the eﬀects of the increasing temperature were not applied to the Post-Cryogenic Survey data. For example, the W1 and W2 system throughput is known to have changed with time as the focal plane arrays and telescope warmed. Comparison of photometry of inertial calibration sources (primarily stars) between the four band data and the Post-Cryogenic Survey data indicates that objects may appear up to 2% brighter in W1 and up to 11% brighter in W2 during diﬀerent times throughout the Post-Cryogenic Survey (Cutri et al. 2012). The NEOWISE Post-Cryogenic Survey preliminary data were delivered to NASA’s public Infrared Science Archive on 31 July, 2012. These preliminary release data do not have the same degree of quality assurance as the ﬁnal release products delivered for the fully cryogenic WISE All-Sky Data Release. Consequently, users are strongly advised to consult the cautionary notes in Section VIII.1.d of the Post-Cryo Preliminary Data Release Explanatory Supplement (Cutri et al. 2012, http://wise2.ipac.caltech.edu/docs/release/allsky/expsup/). The NEOWISE project is now reprocessing the Post-Cryogenic Survey data with a secondpass version of the pipeline, using the much improved calibrations optimized for the warm telescope. Here, we characterize the performance of the preliminary version of the Post-Cryogenic Survey data with regard to small bodies. To date, ∼6500 objects were detected during the NEOWISE Post-Cryogenic Survey, including 88 NEOs. NEOWISE discovered ∼1000 new asteroids during the Post-Cryogenic Survey, including 12 new NEOs. We present preliminary thermal model results for NEOs detected during the 3-Band Cryogenic and Post-Cryogenic Survey phases. NEOWISE discovered two unusual NEOs during the Post-Cryogenic Survey: 2010 TK7, the ﬁrst known Earth Trojan (Connors et al. 2011), and 2010 SO16, an NEO in a so-called “horseshoe” orbit (Christou & Asher 2011). The ﬁts for these and other objects are given.
Projection-based reduced order models for a cut finite element method in parametrized domains<|sep|>A wide variety of numerical methods and computational libraries for the solution of problems governed by partial diﬀerential equations is nowadays available. However, there are still many cases in which either the solution of the governing equations or the solution of associated inverse problems becomes impractical or unfeasible using standard discretization techniques, such as the Finite Element Method (FEM). For instance, complicated topology of the problem or a complex geometry may pose a challenge in the discretization of complex phenomena, and ultimately aﬀect the quality of the resulting simulation. Moreover, repeated queries to the underlying solver in the context of an iterative solution of inverse problems may result in unbearably large computational times. Such situations occur, for example, when a large number of diﬀerent conﬁgurations are in need of being tested, such as in uncertainty quantiﬁcation, optimal control and shape optimization. The overall objective of this manuscript is to investigate how the recently introduced unﬁtted mesh ﬁnite element methods may be used combined with reduced order modeling techniques for parametrized partial diﬀerential problems. Indeed, unﬁtted methods are very useful in cases characterized by complex geometrical conﬁgurations; however, as their Finite Element counterparts, they usually require large computational eﬀorts. A combination with reduced order methods, able to widely decrease the overall computational time, would result in a very compelling methodology to be possibly applied in several diﬀerent ﬁelds. Classical embedded/immersed methods provide simple, eﬃcient, and robust numerical schemes for solving PDE in general domains [1, 2, 3]. Since these early works, several improvements have been made, for instance for what concerns the rate of spatial accuracy near embedded boundaries. Recent improvements go under the names of Ghost-Cell ﬁnite diﬀerence methods, Cut-Cell ﬁnite volume approach, Immersed Interface, Ghost Fluid, Volume Penalty methods, for which we refer to the review paper [1] and references within. In particular, for what concerns incompressible ﬂows in arbitrary smooth domains, the Immersed Boundary Smooth Extension method has shown high-order convergence for the incompressible Navier-Stokes equations [4]. More in detail, extended mesh ﬁnite element methods using cut elements are examined in [5, 6] for stationary Stokes ﬂow systems, as well as for Navier-Stokes. An analysis for high Reynolds numbers, independent of the local Reynolds, has been carried out in [7, 8, 9]. XFEM approaches in 2D and 3D Navier-Stokes are reported in [10]. Higher Reynolds number aerodynamics problems in unbounded domains, thin vortex ring and Lattice Green function Immersed Boundary methods are studied in [11, 12]. Furthermore, embedded and immersed methods have been used in solving ﬂuid structure 1SISSA, Mathematics Area, mathLab, Via Bonomea 265, Trieste, 34136, Italy. 2Department of Mathematics, School of Applied Mathematical and Physical Sciences, National Technical University of Athens, Zografou 15780, Greece. E-mail addresses: efthymios.karatzas@sissa.it, francesco.ballarin@sissa.it, gianluigi.rozza@sissa.it. Date: August 2, 2019. *Corresponding author.
Accelerating PageRank using Partition-Centric Processing<|sep|>Graphs are the preferred choice of data representation in many ﬁelds such as web and social network analysis [9, 3, 29, 10], biology [17], transportation [15, 4] etc. The growing scale of problems in these areas has generated substantial research interest in high performance graph analytics. A large fraction of this research is focused on shared memory platforms because of their low communication overhead compared to distributed systems [26]. High DRAM capacity in modern systems fur ther allows in-memory processing of large graphs on a single server [35, 33, 37]. However, efﬁcient utilization of compute power is challenging even on a single node because of the (1) low computation-to-communication ratio and, (2) irregular memory access patterns of graph algorithms. The growing disparity between CPU speed and DRAM bandwidth, termed memory wall [42], has become a key issue in high performance graph analytics. PageRank is a quintessential algorithm that exempliﬁes the performance challenges posed by graph computations. It iteratively performs Sparse MatrixVector (SpMV) multiplication over the adjacency matrix of the target graph and the current PageRank vector −→ PR to generate new PageRank values. The irregularity in adjacency matrices leads to random accesses to −→ PR with poor spatial and temporal locality. The resulting cache misses and communication volume become the performance bottleneck for PageRank computation. Since many graph algorithms can be similarly modeled as a series of SpMV operations [37], optimizations on PageRank can be easily generalized to other algorithms. Recent works have proposed the use of Gather-ApplyScatter (GAS) model to improve locality and reduce communication for SpMV and PageRank [43, 11, 5]. This model splits computation into two phases: scatter current source node values on edges and gather propagated values on edges to compute new values for destination nodes. The 2-phased approach restricts access to either the current −→ PR or new −→ PR at a time. This provides opportunities for cache-efﬁcient and lock-free parallelization of the algorithm. We observe that although this approach exhibits several attractive features, it also has some drawbacks leading to inefﬁcient memory accesses, both quantitative as well as qualitative. First, we note that while scattering, a vertex repeatedly writes its value on all outgoing edges, resulting in large number of reads and writes. We also observe that the Vertex-centric graph traversal in [11, 5] results in random DRAM accesses and the Edge-centric traversal in [34, 43] scans edge list in coordinate format which increases the number of reads. Our premise is that by changing the focus of computation from a single vertex or edge to a cacheable group of vertices (partition), we can effectively identify and reduce redundant edge traversals as well as avoid random accesses to DRAM, while still retaining the beneﬁts of GAS model. Based on these insights, we develop a new Partition-Centric approach to compute PageRank. The major contributions of our work are: 1. We propose a Partition-Centric Processing Methodology (PCPM) that propagates updates from nodes to partitions and reduces the redundancy associated with GAS model. 2. By carefully evaluating how a PCPM based implementation impacts algorithm behavior, we develop several system optimizations that substantially accelerate the computation, namely, (a) a new data layout that drastically reduces communication and random memory accesses, (b) branch avoidance mechanisms to remove unpredictable branches. 3. We demonstrate that PCPM can take advantage of intelligent node labeling to further reduce the communication volume. Thus, PCPM is suitable even for high locality graphs. 4. We conduct extensive analytical and experimental evaluation of our approach using 6 large datasets. On a 16-core shared memory system, PCPM achieves 2.1×−3.8× speedup in execution time and 1.3×−2.5× reduction in main memory communication over state-of-the-art. 5. We show that PCPM can be easily extended to weighted graphs and generic SpMV computation (section 3.5) even though it is described in the context of PageRank algorithm in this paper.
A two-stage formalism for common-envelope phases of massive stars<|sep|>Common-envelope (CE) evolution remains one of the most uncertain phases of binary evolution (see Ivanova et al. 2013, 2020, for an overview on CE evolution). In this phase, one of the stars in a binary system evolves to initiate unstable mass transfer, engulﬁng the companion star. The companion star will spiral in to orbit the core of the primary star inside a shared envelope, releasing a large amount of orbital energy. Part of the released energy can be transferred to the envelope to eject it. As a result, the binary is left with the companion star and the core of the primary in a very tight orbit. The CE phase was initially introduced to explain the origin of cataclysmic variables (Paczynski 1976). Today, the idea is extended to higher-mass stars as well, and is considered responsible for explaining the origin of many other systems such as type Ia and stripped-envelope supernova progenitors, X-ray binaries and gravitationalwave sources, to name just a few. The main goal of CE studies is to establish a way to predict the outcome, namely the post-CE orbital separation, provided the pre-CE binary properties. Despite the extensive efforts made via hydrodynamical modelling in 1D (e.g. Ivanova et al. 2015; Clayton et al. 2017; Fragos et al. 2019; Klencki et al. 2021; Marchant et al. 2021) and 3D (e.g. Rasio & Livio 1996; Ricker & Taam 2008; Passy et al. 2012; Ivanova & Nandez 2016; Iaconi et al. 2017; Law-Smith et al. 2020; Glanz & Perets 2021; Moreno et al. 2021; Lau et al. 2022a,b; Gonzalez-Bolivar et al. 2022), there is not yet a suﬃcient understanding of the physics to be able to provide accurate predictions. Many binary population synthesis studies instead adopt a simple parameterization to CE phases based on energy balance (the so-called α-formalism). This parameterization enables us to predict the outcome given an appropriate choice for the value of the parameter αCE. However, there is growing evidence both observationally and theoretically that the value for αCE may not be universal across diﬀerent systems (Politano 2004; Iaconi & De Marco 2019), questioning the suitability of the parameterization itself. In particular, there seems to be a qualitative discrepancy between low-mass and high-mass donors (e.g. Wilson & Nordhaus 2022), necessitating the use of diﬀerent values of αCE for diﬀerent mass regimes. There is an alternative formalism based on angular momentum conservation, often called the γ-formalism (Nelemans et al. 2000; Nelemans & Tout 2005). However, various studies point out that the ratio of the initial and ﬁnal orbital angular momenta are so large that even a small change in model parameters will predict wildly diﬀerent ﬁnal separations (Webbink 2008; Ivanova et al. 2013, 2020). Therefore, there seems to be little advantage of the γ formalism over the α formalism as an outcome predictor. In this letter, we propose a new framework to parameterize the CE phase, with particular emphasis on massive star donors. In Section 2, we review the classical α-formalism and then explain our new proposed framework. We demonstrate some examples in Section 3 and conclude our study in Section 4.
Multiclass Common Spatial Pattern for EEG based Brain Computer Interface with Adaptive Learning Classifier<|sep|>There is on-going research to connect brain signals with computers/machines in order to create a comfortable communication pathway for disabled people with external environment. Brain Computer Interface (BCI) aims to provide non-muscular communication between brain and computer/machine by interpreting the thoughts in the brain. BCI can be deﬁned as: ”a system that acquires brain signal activity and translates it into an output that can replace, restore, enhance, supplement, or improve the existing brain signal, which can, in turn, modify or change ongoing interactions between the brain and its internal or external environment”. Or in simple words, it is a system that translates brain signals into new kinds of outputs as mentioned in Daly and Wolpaw (2008). In intelligent devices, which rely hugely upon the user experience, BCI is used for monitoring purposes, for example monitoring attention level of driver on a long drive. Apart from clinical application for disabled people for controlling devices, BCI is used in virtual reality, gaming industry and entertainment industry. It is also used for meditation and rehabilitation purposes. BCI is employed in boosting learning abilities of children who have diﬃculty with this. There are three broad categories of BCIs namely, implantable/invasive, non-invasive and partial/semiinvasive, distinguished by invasively and noninvasively acquired brain signals, respectively. Invasive BCI have a requirement of placing the sensors inside the brain, for which operation is necessary. Although this type of BCI provide one of the best signal-to-noise ratio (SNR), they are not preferred due to high economic cost, short sensor duration and surgical risks. Non-invasive BCI can be realized using brain’s electric or magnetic ﬁeld, out of which electric ﬁeld is preferred choice as it is mobile and set of apparatus needed for this is easily available and not bulky. Electroencephalography (EEG) has been used to detect the electric signals in a non-invasive manner. BCI using EEG can be further catergorized into event related potential P300 proposed by Farwell and Donchin (1988), steady state visual evoked potential (SSVEP) mentioned in Vidal (1973) and event related desynchronization/synchronization (ERD/S). In this paper, we concentrate on motor imagery system based BCI which is a type of ERD/S. Motor Imagery based BCI were invented to help people with disabilities to communicate with the outer world. EEG has proven to be eﬀective in motor imagery based BCI due to its very light equipment, low cost and high temporal resolution Curran and Stokes (2003). However, BCI using EEG suﬀers from challenges such as extracting features which are useful for speciﬁc task due to low speciﬁcity, vulnerable to volume conduction eﬀects, non-stationarity, and prone to noise Wolpaw et al. (2000). Another problem posed by EEG signals is that they vary from person to person and session to session Krauledat et al. (2007). Spatial ﬁltering has been introduced to discriminate between the motor imagery signals using multichannel EEG. The objective behind this ﬁltering is to transform the multichannel EEG signals into small set of channels which are useful to distinguish between the diﬀerent brain activities Vidaurre et al. (2011); Tangwiriyasakul et al. (2013).
"I have no idea what they're trying to accomplish:" Enthusiastic and Casual Signal Users' Understanding of Signal PINs<|sep|>Signal is an encrypted messaging application that is dedicated to preserving the privacy of its users and enacts features along those lines, such as not centrally storing users’ contact lists, messages, or location histories unencrypted. Signal has historically relied only on users’ telephone numbers for identiﬁcation, authentication (via SMS), and contact discovery. Unfortunately, these methods are insufﬁcient against attacks, including SIM-swapping [2, 18, 22]. In addition, these have some usability issues such as users who lose access to their telephone numbers also lose their Signal contact lists. Finally, they hamper additional features requiring additional metadata, like user proﬁles. To improve the app in terms of these shortcomings, Signal released two new features: Secure Value Recovery (SVR) [23] and registration lock [33]. Both features require the user to establish a PIN, which can be a sequence of numbers, like a traditional PIN, but also include letters and symbols. This key is used to recover encrypted backups of contacts and settings stored on Signal servers. The registration lock aims to prevent anyone but the original user from creating a Signal account for a phone number without the associated PIN. Signal’s choice of naming the credential a “PIN” (as in, personal identiﬁcation number) may not clearly indicate to the user the importance of the PIN in the Signal ecosystem. Unlike device or screen lock which is familiar to users, the in-app use of the Signal PIN is meant to achieve an app-speciﬁc purpose not satisﬁed by the device or operating system’s features. A banking app for example might mostly be using in-app authentication to protect access to an OAuth token, while Signal has a different goal. As Signal represents one of the ﬁrst, large-scale usages of in-app PINs, in this paper we investigate to what extent do participants, both the security-/privacy-savvy and the average ones, understand the PIN feature and what effect does this have on their choice and usage? Additionally, we also investigate how participants react to Signal’s PIN veriﬁcation reminders that encourage users to not only select a complex PIN but regularly remind users to reenter it for veriﬁcation. This feature may have been implemented because the PIN is not meant for daily use, but instead only needed in acute moments of setting up a new device with the Signal app. Finally, we examine the way participants select and compose their Signal PINs and the effect of their general understanding of the underlying Signal features to make these decisions. To this end, we consider the following research questions: RQ3 How do participants choose and compose a PIN for Signal, and does their understanding of how these PINs are used affect that choice? We surveyed Signal users (n = 235), asking about their understanding, usage of the Signal PIN feature, and response to Signal PIN veriﬁcation. For example, we asked participants to explain the purpose of Signal PINs, in their own words. We additionally asked participants about the composition of their PIN (e.g., length, character set), if they reuse the PIN in other contexts (e.g., phone lock, in another messenger app), if they have opted out of selecting a PIN, and their response to periodic PIN veriﬁcation. We ﬁnd that only 14 % (n = 33) of respondents opted out of setting a Signal PIN, and also we ﬁnd a large disparity between the practices of participants who can explain the purpose of the in-app PIN authentication (who we term Signal enthusiasts; n = 132; 56 %) and those who cannot (dubbed casual Signal users; n = 103; 44 %). Many enthusiasts set PINs because they thought it was required — initial communication from Signal indicated that it was, although it is not in current versions of the app. Many enthusiasts also speciﬁcally mentioned registration locking and cloud backups. Interestingly, when enthusiasts did not set a PIN, 44 % cited anti-cloud storage sentiments, indicating that they are aware of the features Signal PIN provides (e.g., cloud backups of proﬁles) but felt that this metadata storage did not sufﬁciently guard their privacy. Among casual users, 25 % set a PIN for generalized security reasons although they are not able to articulate those. Moreover, 13 % set a PIN simply because they were prompted by Signal or do not know why they actually set a PIN (16 %). If casual users did not set a PIN, they typically indicate that it was inconvenient (18 %) or they did not see the necessity (18 %). Their inaccurate understanding also affects this decision: 24 % state that they do not need an additional safeguard to secure access to their Signal app although the PIN is not used for this purpose. Very few participants who set a PIN indicated that they had difﬁculty remembering their PIN; only 12 % said they occasionally, frequently or very frequently have difﬁculty remembering. When interacting with the periodic reminders to verify their PIN, 59 % conﬁrm their PIN frequently or very frequently. Only 24 % of all participants conﬁrm their PIN rarely, very rarely, or never when prompted, yet, here the behavior of enthusiasts and casuals diverges: 16 % of the latter tend to ignore the reminder prompt compared to 28 % of the enthusiasts. In addition, 45 or 24 % of the participants who currently use a PIN disabled these reminders. When asked why, 67 % of the enthusiasts mention that they use a password manager while casuals are mostly annoyed (42 %) or do not feel it is necessary to be reminded (33 %). We also ﬁnd that enthusiasts’ PINs are more password-like, often containing numbers, letters and symbols. Compared to casuals, enthusiasts on average choose PINs with an additional 1.3 digits, 3.0 letters, and 1.3 special characters. Moreover, many participants, particularly enthusiasts, use a password manager to store their Signal PIN, which additionally increased the complexity of their PIN: password manager users selected PINs with an additional 2.1 digits, 5.3 letters, and 3.1 special characters compared to non-password manager users. A number of participants, both enthusiasts and casuals, noted the reuse of their Signal PIN in other contexts, apps, and as their screen lock, yet, 76 % of the participants who use a PIN within Signal said they do not reuse it. In short, it appears Signal’s core audience of privacyconscious enthusiasts is using the PIN effectively, however, this roll-out may have been affected by inconsistent communication. Some earlier versions of the app made PIN creation a requirement. In addition, Signal PINs can contain letters and special characters. Weak Signal PIN choices can have consequences for those that choose secure PINs as secure communication requires both parties to be secure. We would recommend that Signal consider adding features to encourage better choices, like an improved blocklist, or even re-branding Signal PINs to more accurately depict their use, like “Account Recovery Passwords,” which could help users apply the right context during selection and storage of this credential. Though our focus is on Signal, our results may inform communication strategies of other app developers, since account recovery and registration lock features are common in secure messaging. All our ﬁndings were shared with the Signal developers.
Open quantum systems with delayed coherent feedback<|sep|>While the theory of Markovian open quantum systems is well-understood [1, 2], simulating the dynamics of non-Markovian open quantum systems is considerably more diﬃcult [3]. A signature non-Markovian open quantum system is a qubit emitting into a discrete feedback reservoir: an environment that ‘remembers’ the state of the system and feeds this information back coherently – as a quantum ﬁeld – after one or more discrete time delays. One example of such a system is sketched schematically in Fig. 1. This kind of feedback has previously been studied in work on “atomic” emission in front of a mirror [4–7], and also in solid state systems with signiﬁcant propagation delays [8]. Such systems are all the more interesting due to the fact that an environmental memory with a continuous kernel, where the evolution of the system depends most generally on its state at all previous times, may be approximated as a sequence of coherent feedback loops with discrete delays. As such, a tool that proves capable of simulating multiple discrete delays may shed light on the more general problem of dealing with a continuous memory. Discrete propagation delays also appear in the standard theory of cascaded open quantum systems, which describes the situation where the retarded output from one open quantum system drives a second system [9–11]. In the standard treatment of cascaded systems there is no backscatter from the second system, so the coupling between the systems is one-way. In this case the propagation delay between the systems is an arbitrary parameter that can be removed by way of a simple transformation of the time variable, which leads to an irreversible, Markovian coupling between subsystems. The cascaded systems formalism is easily generalised to describe irreversible coupling between the systems in both directions, so long as the propagation delay associated with the coupling is suﬃciently small that it may be neglected [12]. We cannot, however, transform away a non-negligible delay in both directions. A treatment of open quantum systems with delayed feedback is therefore necessary to extend the theory of cascaded open quantum systems to encompass cascaded systems with backscatter where there is a nontrivial propagation delay. There are conditions under which delayed feedback and cascaded systems are well known to be connected. In fact, in a certain limit a coherent feedback loop leads to exactly the same dynamics as a chain of cascaded systems. This equivalence is employed in, for example, work by Menicucci et al. [13]. While it is not the case that cascaded systems and delayed feedback are equivalent in general, it turns out that there is suﬃcient similarity between these two set-ups that results from the theory of cascaded systems can be exploited to help simulate feedback. Recently, Grimsmo [14] employed tensor network methods to show how to simulate a nonlinear quantum system (such as a two-state ‘atom’) interacting with a discrete feedback reservoir and indeed, the resulting equations demonstrate a close connection to the master equation for cascaded systems. Pichler and Zoller [15] subsequently published a diﬀerent technique, based on matrix product states, for simulating quantum circuits in the regime where time delays are signiﬁcant. We present here an elementary derivation and generalisation of Grimsmo’s method, which permits the simulation of multiple delays, as well as cascaded systems with delayed backscatter where the delay may diﬀer in each direction. Our derivation is based on the fact that the evolution of a generic open quantum system may be decomposed into a nested sequence of evolutions over distinct time intervals—what we refer to as a decomposition into intervals. This decomposition is outlined in Sec. 2. In Sec. 3, we apply this decomposition to open quantum systems interacting with the environment such that the system experiences delayed coherent feedback, with discrete delays of various lengths applying to interactions between diﬀerent pairs of subsystems. We go on in Sec. 4 to present a series of examples of the use of this algorithm, and demonstrate in Sec. 5 how the algorithm may be extended to enable the calculation of two-time (and more generally multi-time) correlation functions, presenting an example calculation of the second-order photon correlation for the output ﬁeld of a system interacting with a delayed coherent feedback loop. We then show in Sec. 6 that our algorithm for simulating open quantum systems with delayed coherent feedback may be interpreted as a quantum teleportation protocol. We summarise our main results and Figure 1. A bipartite open quantum system, depicted here as a pair of ring cavities, with delayed coherent feedback. The propagation delay from subsystem A to subsystem B is greater than that in the reverse direction. Black arrows denote system ﬁelds, while grey arrows denote ﬁelds propagating in the environment.
The nature of the SDSS galaxies in various classes based on morphology, colour and spectral features - II. Multi-wavelength properties<|sep|>One of the fundamental issues of the observational cosmology is the evolutionary connection between various classes of galaxies. Since galaxies show a large range of physical properties and activities, they are classiﬁed using various criteria: morphology (e.g. Park & Choi 2005), colour (e.g. Martin et al 2007), spectral features (e.g. Mateus et al 2006), and so on. Knowledge about those various kinds of galaxies has been accumulated for a long time, motivating many eﬀorts to ﬁnd the connections between diﬀerent galaxy classes and their implication on galaxy evolution. However, since the galaxy classiﬁcations in most previous studies were limited to only one or two properties, some detailed aspects in galaxy evolution have not been suﬃciently inspected. For example, galaxies with unusual features, such as blue earlytype galaxies (Ferreras et al. 2005; Lee et al. 2006) or passive spiral galaxies (Yamauchi & Goto 2004; Ishigaki et al. 2007) are diﬃcult to understand well in studies using simple galaxy classiﬁcations. The necessity of ﬁne classiﬁcations of galaxies was also pointed out in Lee et al. (2007), who
A $Gaia$ Data Release 2 catalogue of white dwarfs and a comparison with SDSS<|sep|>All stars with main sequence masses ≲ 8−10 M⊙ (Iben et al. 1997; Dobbie et al. 2006a) share the same common fate: they will one day evolve into white dwarfs, dense stellar embers destined to cool over billions of years (Fontaine et al. 2001; Althaus et al. 2010). This broad mass range includes over 90 per cent of all stars in the Galaxy. This makes white dwarfs signiﬁcant contributors to the global stellar population and, thanks to their well deﬁned cooling rates, accurate tracers of the formation and evolution of the Milky Way (e.g., Winget et al. 1987; Torres et al. 2005; Tremblay et al. 2014). The diagnostic potential of the Galactic white dwarf population can only be fully exploited once we have large, homogeneous, and well-deﬁned samples of white dwarfs. Given the intrinsic low luminosities and relatively high proper motions of stellar remnants, these samples have been historically challenging to assemble. The fundamental properties of white dwarfs (mass, cooling age, atmospheric and internal composition) can be determined from spectroscopic, photometric or asteroseismic analyses (Bergeron et al. 1992, 2001; Koester et al. 2009; Bergeron et al. 2011; Tremblay et al. 2013; Romero et al. 2017; Giammichele et al. 2018). These parameters are
A search for ultrahigh-energy neutrinos associated with astrophysical sources using the third flight of ANITA<|sep|>The Antarctic Impulsive Transient Antenna (ANITA) experiment [1] deploys a balloon-borne radio interferometer to search for the impulsive Askaryan radio emission [2, 3] expected to be produced by the interactions of ultrahigh-energy (UHE) neutrinos (E > 1018 eV) interacting in polar ice. ANITA has previously reported constraints on diﬀuse UHE neutrinos [4–7] as well as neutrinos in time-coincidence with gamma-ray bursts (GRBs) [8]. No candidate events have been observed above background expectations so far in the Askaryan channel, but ANITA sets the most stringent limits on diﬀuse UHE neutrino ﬂux above 1019.5 eV. Cosmogenic UHE neutrinos are expected to be produced in the interactions of the UHE cosmic-rays (UHECR) with the CMB (i.e. the GZK process) [9–11]. The sources of the UHECR have not yet been identiﬁed, and it is unknown if the sources are transient in nature or steady-state. Typical GZK interaction lengths of a few hundred Mpc imply cosmogenic neutrinos will retain the source direction over cosmological distances, but any time association with potential astrophysical transients is likely lost due to deﬂections of UHECR by intergalactic magnetic ﬁelds. Astrophysical neutrinos, believed to be produced directly in astrophysical sources, have been detected at TeV-PeV energies by IceCube [12]. IceCube has identiﬁed evidence for some particular astrophysical neutrino sources, including TXS 0506+056 [13, 14] and NGC 1068 [15]. Astrophysical neutrinos may also exist at UHE energies, either as a continuation of the same ﬂux that IceCube has detected, or from other sources, such as ﬂat-spectrum radio quasars (FSRQs) [16–18] or GRBs [19–23]. Compared to a diﬀuse UHE neutrino search, a search for UHE neutrinos associated with particular sources can narrow the detection phase space in direction and, for transient objects, in time. This in general allows a reduction in backgrounds and/or an improvement in analysis eﬃciency, therefore increasing the sensitivity compared to diﬀuse ﬂuxes. In this paper, we build on an ANITA-III diﬀuse search to develop a methodology to search for UHE neutrinos in spatial and time coincidence with astrophysical source classes. We deﬁne a source class as a speciﬁcation of the time-dependent neutrino ﬂux from one or more sources, F(E, t) = � sources Fi(E, t). This methodology is applied to the ANITA-III ﬂight for ﬁve source classes: TXS 0506+056, NGC 1068, blazars ﬂaring in UHE gamma-rays as identiﬁed by the Fermi All-sky Variability Analysis (FAVA [24]), GRBs, and supernovae (SN).
A jigsaw puzzle framework for homogenization of high porosity foams<|sep|>Highly porous metallic foams possess an extensive application potential. These materials feature high energy absorption, strength, and stiﬀness at very low weight, which makes them appealing for the automotive, aircraft, and defense industry, to name a few [1, 2, 3]. In order to foster new application areas, a qualiﬁed understanding of the foam behavior and relevant predictive tools are required. Computer models, in particular, are regarded as a key ingredient to optimization of either/both the microstructure geometry or/and ﬁnal products made of these materials [4]. A consensus regarding the approximation of the behavior of porous solids seems to exist. Open-cell foams1 are usually represented with the threedimensional beam models, while their closed-cell counterparts require an addition of membrane elements acting as the cell walls [5]. Nevertheless, in the case of very thin walls even the behavior of closed-cell foams can be approximated with beams [2, 6, 1, 7]. Following this assumption, Ashby and Gibson presented a three-dimensional beam model of the unit cell and derived solutions to the overall thermo-mechanical parameters based purely on material porosity [5]. Our goal is to explore the adequacy of a twodimensional wired model discretized with beam elements. We build on the recent outcomes by Nˇemeˇcek et al. [8], who developed a planar beam model based on the approximation of foam ligament geometry by the Voronoi tessellation. The objectives of the paper are threefold: $The author’s post-print manuscript of the article published in Computers & Structures, DOI: 10.1016/j.compstruc.2016.01.003. ∗Corresponding author. Tel.: +420-224-354-606 Email addresses: martin.doskar@fsv.cvut.cz (Martin Doˇsk´aˇr), novakj@cml.fsv.cvut.cz (Jan Nov´ak) 1As Banhart [2] pointed out, sponge would be an appropriate term for open-cell highly porous solids. ii) to provide the upper and lower bounds on the eﬀective stiﬀness coeﬃcients for the two-dimensional beam model via the ﬁrst-order homogenization procedure; iii) to validate obtained results against the experimental and numerical results from [8] and to question the assumption of the vanishing Poisson ratio made therein. The ﬁrst objective is addressed with the help of Wang tilings, a concept recently introduced to Materials Engineering community [9]. In particular, the foam microstructure is compressed within a set of smaller domains, called Wang tiles. The morphology of the tiles is designed such that the tiles are microstructurally compatible across the corresponding edges. As a result the reconstructed material microstructure remains continuous across the gridline of the regular lattice to which the tiles are accommodated during synthesis of a computational model, see Fig. 3(c). The method is extremely eﬃcient in producing arbitrarily diverse ensembles of arbitrarily sized and geometrically consistent microstructure realizations in a fully stochastic setting [10]. Moreover, creating the ﬁnite element mesh on the level of tiles avoids expensive mesh generation of each microstructure realization. Altogether, we are able to reach for a proper size of the computational model, which is expected to be relatively large because of the inﬁnite contrast in constituent properties [11]. As regards the second objective, we reformulate the ﬁrstorder homogenization procedure for the wire-frame ﬁnite element models by means of macroscopic degrees of freedom [12]. The upper bound on the apparent stiﬀness is obtained from the ensemble of microstructures exposed to the kinematic uniform boundary conditions (KUBC). The lower bound is rendered by applying the minimal kinematic boundary conditions (mKBC) [13]. In order to compare our results with those reported in [1, 2, 14, 8], the bulk and shear moduli are derived from the homogenized stiﬀness matrices by assuming material isotropy. The paper is structured as follows. In the next chapter Alporas R ⃝ aluminum foam, the material of interest, is characterized in brief. Section 3 covers the adopted modeling strategy addressing the ﬁrst objective. In particular, Wang tilings are presented ﬁrst together with their use in the context of numerical homogenization. Description of microstructure discretization into planar beams is also provided in this section. The ﬁrst order homogenization is reviewed in Section 4 with emphasis on the beam model and the boundary conditions connected to the upper and lower bounds on the eﬀective stiﬀness. Numerical results are provided in Section 5 and discussed in the last section. We use the following nomenclature within the text. Scalar quantities are denoted by plain letters, e.g. a or A, matrices by bold sans-serif font, e.g. a or A, and tensorial quantities by bold serif letters, e.g. a or A. In addition, we adopt the standard Voigt matrix representation of symmetric second- and fourth-order tensors.
Geometrical approach to SU(2) navigation with Fibonacci anyons<|sep|>Topological Quantum Computation (TQC) [1, 2, 3, 4] makes use of the subtle properties of topological phases of matter to provide an original implementation for quantum computation, better immune to decoherence. Its main ingredients are anyonic excitations displaying non-abelian braiding statistics. Although no direct experimental proof exists that such characteristics occurs in real physical systems, there are some evidence that, for instance, the 12/5 Fractional Quantum Hall Eﬀect states should be good candidates to display the expected properties. Up to now, contributions to the TQC ﬁeld are mainly splitted into two parts, a ”hardware” part whose main purpose is to ﬁnd microscopic models, and possible experimental realizations displaying these topological features in their spectral properties, and a ”software” part, which starts from a formal (non-abelian) anyon model, and deﬁnes, out of it, qubit states, quantum gates and algorithms. Notice that this splitting is already present in more ”standard” qantum computation, with on one hand the large eﬀort devoted to built experimental implementations of sets of coupled qubits, and the quantum algorithm part, which in fact started ﬁrst, and most often do not discriminate between the very diﬀerent microscopic realizations for the qubits, supposing that a large amount of them are already available. In the present paper, we analyse a model with three Fibonacci anyons (irrespective of their implementation), and ask how their manipulation (upon braiding) can appropriately approximate the action of generic SU(2) unitay transformations. As is well known ([2]), this is in principle possible to any desired accuracy, thank to the fact that the associated non-abelian braid group representation is dense in SU(2). To make this system interesting, it is also important that this can be done eﬃciently. Such a task has yet been fulﬃlled [5, 6] by splitting the braid search into two distinct parts : ﬁrst, a brute force search among all braids up to a given length to generate the closest matrix to the target one; then, a reﬁnement step done by iteratively implementing the Solovay-Kitaev algorithm [7, 8]. With additional Fibonacci anyons, it is possible to deﬁne more qubits, whose interaction results from appropriate braiding. For example, a universal set of quantum gates has been derived [5, 6] , with six anyons forming a two-qubit system, proving that it can in principle allows for quantum computation. Here an alternative approach is proposed, of rather diﬀerent nature, in order to generate the SU(2) elements. Instead of ﬁrst insisting on the dense SU(2) covering generated by the Fibonacci braid group generators, we start by analysing how good the latter can approximate the generators of binary polyhedral SU(2) ﬁnite subgroups. It comes out that the subgroup of higher order, the binary icosahedral group Y with 120 elements, can indeed be very eﬃciently approached. Recalling the isomorphism between SU(2) and the 3-dimensional sphere S3, this already allows a ﬁne grained description of SU(2). Indeed, to the group Y corresponds the regular polytope {3, 3, 5} [9, 10], whose full symmetry group G (discrete subgroup of O(4)), has order 14400. This already leads to an eﬃcient way of generating 14400 SU(2) unitary transformations, related by symmetry. We further show how to iteratively gets ﬁner and ﬁner meshes in SU(2) by generating the so-called ”geodesic hyperdomes”, the analogues with one dimension more, of the celebrated families of geodesic domes which provide ﬁne discrete approximations of the usual sphere S2. In a ﬁnal part, a more ”disordered” version of the latter step is described, which already provides an eﬃcient speedup for ”brute-like” search.
Star Formation and Gas Phase History of the Cosmic Web<|sep|>The large scale structure (LSS) is composed of dark matter, gas and stars strung together in a system of clusters, ﬁlaments, sheets with vast voids spanning the regions between them. From numerical simulations, we know that the ﬁlaments and sheets act as conduits vacating matter from low density regions (e.g. voids) and accreting it onto high density regions (e.g. clusters) (Klypin & Shandarin 1983; Davis et al. 1985; Bertschinger & Gelb 1991). These intricately woven sheets and ﬁlaments connecting high density regions together has been dubbed the “cosmic web” (Bond, Kofman & Pogosyan 1996). The structural features of the cosmic web are observed in large galaxy redshift surveys (J˜oeveer, Einasto & Tago 1978; de Lapparent, Geller & Huchra 1986; Geller & Huchra 1989; Colless et al. 2001; Gott et al. 2005). These surveys provide information on the distribution and morphology of galaxies within each structure. Initial detections of the underlying dark matter structure (e.g. ﬁlaments) (Massey et al. 2007; Heymans et al. 2008; Jauzac et al. 2012) were challenging due to the requirement for precise (∼ 1%) measurements of weak lensing distortions of background galaxies by foreground large scale structure. In recent years, however, individual dark matter ﬁlaments have been conﬁrmed and studied through weak lensing experiments (Dietrich et al. 2005; Jauzac et al. 2012). Motivated by measuring the matter power spectrum in a regime not affected by baryonic physics, this weak lensing technique has also been used to measure the cosmic shear due to the underlying dark matter structure (Bacon, Refregier & Ellis 2000; Kaiser, Wilson & Luppino 2000; Van Waerbeke et al. 2000; Wittman et al. 2000). More expansive surveys measuring cosmic shear are proposed (LSST, JDEM). These surveys will, in the near future, provide a more complete map of the underlying dark matter structure (e.g. 3-D mass tomography) and constrain the time evolution of dark energy (Ivezic et al. 2008; Albrecht et al. 2009). The characterization of the baryonic matter in the IGM near density peaks is more complete. Hot gas in clusters and in some higher density ﬁlaments has been directly detected through X-ray
Learn to Forget: Machine Unlearning via Neuron Masking<|sep|>Nowadays, machine learning models, especially neural networks, become prevalent in many real-world applications including medical diagnosis, credit-risk assessment, autopilot and so on. These models are trained from user data that may contain sensitive information. Recent research, like federated learning [1], [2] and cryptography-based machine learning [3], [4], enables the training process to be done in a way without seeing the training data. However, the trained model itself may still “memorize” the training data [5], and there is no way for users to withdraw. Recently released data protection regulations, e.g., the California Consumer Privacy Act [6] and the General Data Protection Regulation in the European Union [7], clearly state that users should have the right to withdraw their private data. To this end, machine unlearning (MU) becomes a popular research topic. It allows users to eliminate memorization of their private data from a trained machine learning model. At ﬁrst glance, differential privacy (DP) [8] can naturally achieve machine unlearning. It guarantees that by looking at the model, one cannot tell whether a sample is in the training data or not. However, DP focuses on protecting the privacy of all samples and the protection is only to some extent. More speciﬁcally, DP ensures a subtle bound on the contribution of each sample to the ﬁnal model, but the contribution cannot be constrained to zero; otherwise, the model would learn nothing from the training data. In contrast, the aim of MU is to cancel the contribution of a target sample completely. Therefore, MU is orthogonal to DP. Existing unlearning methods can be roughly classiﬁed into two categories: summation-based [9] and retrainingbased [10], [11]. Summation-based unlearning trains a model based on a small number of summations, each of which is the sum of some efﬁciently computable transformation (e.g., gradient descent) of the training samples [9]. To forget a sample, one can simply subtract that sample from its corresponding summations and update the model. This method works well for non-adaptive machine learning models (later training does not depend on earlier training), such as Na¨ıve Bayes [12] and C4.5 [13]. However, for adaptive models such as neural networks, subtracting a sample from a summation can easily cause excessive unlearning of unrelated memorization, hence signiﬁcantly decrease the utility. As its name implies, retraining-based unlearning retrains the model after removing the samples to be forgotten [14]. Given that retraining from scratch incurs an overhead that is usually unaffordable (it may take several days to retrain a model), Bourtoule et al. [10] propose SISA, which divides the training set into slices and trains a model via incrementally learning, s.t. each intermediate model (after one slice is added) is recorded. To forget a sample, retraining starts from the ﬁrst intermediate model that contains the contribution of that sample. However, this method is simply trading storage (for recording the intermediate models) for retraining time, instead of truly reducing the overhead of retraining. Our contributions. In this paper, we propose a novel unlearning method called Forsaken. Compared with summationbased unlearning, Forsaken is more friendly to adaptive models such as neural networks, introducing a much smaller utility loss. Compared with retraining-based unlearning, Forsaken has a much more efﬁcient unlearning phase in both storage and time usage. The core idea of Forsaken is neuron masking. We introduce a mask gradient generator that continuously generates mask gradients, and apply them to the neurons of the neural network and stimulate them to unlearn the memorization of the given samples. Based on the state of the updated model, the mask gradient generator adjusts the magnitude of the mask gradients to avoid unexpected unlearning. Especially, Forsaken can be applied to unlearn any training data, including out-of-distribution (OOD) and indistribution (ID) data, but focus more on OOD data unlearning. This is because the OOD but sensitive data inadvertently uploaded by users can form unintended memorization, which is hard to avoid and can increase the possibility of the adversary to extract private user information [15]. Furthermore, we propose a uniform metric called forgetting rate to evaluate the effectiveness of an unlearning method. It is based on the membership oracle [16], [17], which is to infer whether a given sample is a member of the training set. It describes the transformation rate of the eliminated data from “memorized” to “unknown” after conducting memorization elimination. To the best of our knowledge, this is the ﬁrst indicator that can be directly used for machine unlearning evaluation. The contributions of this paper are summarized below. • We propose the ﬁrst uniform metric called forgetting rate to measure the effectiveness of a machine unlearning method. (Section III) • We propose a novel machine unlearning method called Forsaken. It is superior to previous work in either utility or efﬁciency (when achieving the same forgetting rate). (Section IV) • We benchmark Forsaken with eight standard datasets to evaluate its performance. The experimental results show that it can achieve more than 90% forgetting rate on average and only cause less than 5% accuracy loss. (Section V)
Mean-square convergence of a semi-discrete scheme for stochastic nonlinear Maxwell equations<|sep|>Stochastic Maxwell equations play an important role in stochastic electromagnetism and statistical radiophysics ﬁelds. Some articles (see, e.g., [1, 2, 6]) introduced randomness into Maxwell equations in order to strengthen the correspondence between theoretical results and the real-life situations. In [7], problems about how to account, rigorously, for uncertainties in classical macroscopic electromagnetic interactions between ﬁelds and systems of linear material were discussed. [15] considered the problem about how to use the spectral representation to describe the random electromagnetic ﬁelds, which are coupled by Maxwell’s equations with a random source term. [3] dealt with the mathematical analysis of stochastic problems arising in the theory of electromagnetic in complex media, including well-posedness, controllability and homogenization. Assuming the existence of magnetic charges or monopoles, consider the following generalized symmetrized stochastic nonlinear Maxwell equations driven by multiplicative Itˆo noise,      ε∂tE−∇×H = −Je(t,x,E,H)−Jr e(t,x,E,H)· ˙W, (t,x) ∈ (0, T]×D, µ∂tH+∇×E = −Jm(t,x,E,H)−Jr m(t,x,E,H)· ˙W, (t,x) ∈ (0, T]×D, E(0,x) = E0(x), H(0,x) = H0(x), x ∈ D, n×E = 0, (t,x) ∈ (0, T]×∂D, where D ⊂ Rd with d = 3 is a bounded domain, T ∈ (0, ∞), and the function J : [0,T]×D×Rd × Rd → Rd is a smooth nonlinear function satisfying The research of C. Chen and J. Hong were supported by the NNSFC (NOs. 91130003, 11021101, 11290142, and 91630312), the research of L. Ji was supported by the NNSFC (NOs. 11601032, and 11471310). 1
Efficient Bayesian phase estimation using mixed priors<|sep|>Phase estimation is an important building block in quantum computing, with applications ranging from ground-state determination in quantum chemistry, to prime factorization and quantum sampling [2, 15, 18]. In the ideal setting we assume that the quantum system can be initialized to an eigenstate |φ⟩ of a known unitary U, such that U|φ⟩ = eiφ|φ⟩. The goal of quantum phase estimation (QPE) is then to estimate the phase φ. Some of the existing approaches include quantum Fourier based phase estimation [1, 6], iterative phase estimation [11, 17], and other methods including robust-phase estimation, time-series analysis, and integral kernels [10, 14, 16]. In practice, there are several factors that complicate the problem. First, it may not be possible to initialize the state exactly to a single eigenstate. This could be because the state is perturbed by noise, or simply because the eigenstate is unknown. The latter case arises, for instance, in the ground state determination of molecules in quantum chemistry where the desired eigenstate can only be approximated. Regardless of the cause, phase estimation may need to deal with an initial state that is a superposition of eigenstates: Second, practical phase-estimation algorithms may also need to deal diﬀerent sources of noise present in current and near-term quantum devices. Bayesian phase estimation [17] has been shown to be particularly well suited for dealing with noise [19] and the presence of multiple eigenstates [13]. In this paper we describe an eﬃcient implementation of Bayesian phase estimation. In Section 2 we describe the Bayesian approach to quantum phase estimation along with an explanation of techniques used to implement it. In Section 3 we review some of the diﬃculties encountered in existing techniques and provide a detailed description of the proposed algorithm. This is followed by numerical experiments in Section 4, and conclusions in Section 5.
The new semianalytic code GalICS 2.0 - Reproducing the galaxy stellar mass function and the Tully-Fisher relation simultaneously<|sep|>Semianalytic models (SAMs) are a technique to model the formation and evolution of galaxies in a cosmological context. Pioneered by White & Frenk (1991) and Lacey & Cole (1993), this technique is based on the notion that galaxy formation is a two-stage process (White & Rees 1978). The gravitational instability of primordial density ﬂuctuations in the dark matter (DM) forms haloes. The dissipative infall of gas within haloes forms luminous galaxies. SAMs follow these two stages separately. First, one constructs merger trees for the haloes in a representative cosmic volume. Then, the evolution of baryons within haloes is broken down into a number of elementary processes, which are modelled analytically. This article introduces the new SAM GalICS 2.0. An early version had already been presented in a comparison of all the main SAMs (Knebe et al. 2015). The models that participated to this comparison are those by Bower et al. (2006), Font et al. (2008), Gonzalez-Perez et al. (2014), Croton et al. (2006), De Lucia & Blaizot (2007), Henriques et al. (2013), Benson (2012), Monaco et al. (2007) , Gargiulo et al. (2015), Somerville et al. (2008) and Lee & Yi (2013). GalICS 2.0 builds on our previous experience with GalICS (Hatton et al. 2003; Cattaneo et al. 2006, 2008, 2013) but is more than a new version. The entire code has been re-written from scratch. One of the reasons is to enable a more extensive use of the cosmological N-body simulation used to contruct the merger trees. In GalICS 2.0, we use the information on DM substructures (merger rates are more
Towards Ultra-Reliable Low-Latency Communications: Typical Scenarios, Possible Solutions, and Open Issues<|sep|>The Fifth Generation (5G) New Radio (NR) considers three new application scenarios, namely enhanced Mobile Broadband (eMBB), massive Machine-Type Communications (mMTC), and Ultra-reliable Low-latency Communications (URLLC) [1]. URLLC is crucial for enabling mission-critical services, such as factory automation, automation vehicles, remote control and virtual/augmented reality (VR/AR). There are many open technical hurdles ahead in achieving URLLC, and thus it has attracted signiﬁcant attention from both the academic and industrial communities. In the current Long Term Evolution (LTE) systems, the transmission time interval (TTI) is 1 ms, which cannot satisfy the end-to-end (E2E) delay requirement of URLLC. To reduce the latency, short frame structure with short channel codes should be considered. With short codes, it is very difﬁcult to achieve the ultra-high reliability requirement. Analyzing and optimizing the transmission delay and the decoding error probability in the short blocklength regime are also very challenging [2]. Aside from transmission delay and decoding error probability, other delay components and delay bound violation probabilities in scheduling procedure and queueing systems also have signiﬁcant impacts on the E2E performance. For example, in LTE systems, the control signaling for uplink (UL) scheduling leads to a high latency that is much longer than 1 ms in the control plane [3]. Thus, how to design grant-free access techniques for URLLC deserves further study. Besides, with the ﬁrst-come-ﬁrst-serve (FCFS) scheduling policy, the short packets of URLLC services may need to wait for the processing of long packets of eMBB services. Thus, FCFS policy may not be the optimal policy for short packets in URLLC services, and other policies should be considered to minimize the E2E delay. The current techniques in the 5G NR [4] mainly focus on achieving the target E2E performance in local area communications, where all the user equipment (UE) lies in one or few adjacent cells. For different communication scenarios, the network architectures are different. In factory automation, the communication area is limited in a smart factory, while for remote control, the controller and slave can be located on different continents. As a result, the latency in radio access network only contributes a small portion of the E2E delay, and other delay components such as core network delay over a long distance large scale network and processing delay in the computing systems may be the dominant components [5]. Therefore, how to improve the E2E performance with different network architectures is still a challenging issue. In this paper, we focus on how to guarantee the E2E delay and overall packet loss probability in different communication scenarios, including local area communications, mobile edge computing (MEC) systems, and the long distance large scale networks. The rest of this paper is organized as follows: • We elaborate possible components of the E2E delay and overall packet loss probability in typical communication scenarios, and provide a general way to formulate the quality-of-service (QoS) constraints of URLLC. • We summarize possible solutions and techniques in physical layer, link layer, network layer, and cross-layer design aspects for URLLC, such as 5G NR physical layer technologies, different packet scheduling policies, and network slicing.
Alterations And Rearrangements Of A Non-Autonomous Dynamical System<|sep|>Dynamical systems have been long used to investigate various physical processes occurring in nature. The theory has been applied eﬀectively across various disciplines of sciences and engineering and has helped providing solutions to a variety of modern day problems. To name a few, the theory has been applied to address problems like ”determining chemical dynamics of a system”, ”estimating population growth of a species” and ”controlling dynamics of various electrical and mechanical systems”[1, 7, 12]. The theory developed pertains to determining the dynamics of a general dynamical system and hence is extremely beneﬁcial for addressing problems across a variety of disciplines. Although most of the problems addressed are modelled using autonomous systems, it is intuitive to believe that better estimates can be obtained for a system when the system is modelled in a non-autonomous setting. As any general model approximating any natural or physical process is non-autonomous in
Green-aware Mobile Edge Computing for IoT: Challenges, Solutions and Future Directions<|sep|>wireless communication and mobile technologies [1]. IoT has been regarded as a  global network consisting of connected smart devices, which contributes to the  arising and evolving of various novel mobile applications. Furthermore, with fast  development, IoT has the potential to promote many more possible applications and  scenarios, such as smart cities, smart home, smart health-care, smart agriculture,  and so on. However, since IoT devices have the inherent features, including  constrained power capacity, low computation capacity, and storage, provisioning  limited resources for a great amount of computation-intensive applications on  devices is a significant challenge [2] [3] [4] [5].
The Fun is Finite: Douglas-Rachford and Sudoku Puzzle -- Finite Termination and Local Linear Convergence<|sep|>Given two non-empty sets C and S whose intersection is also non-empty, the feasibility problem aims to ﬁnd a common point in the intersection C ∩ S. In the literature, popular numerical schemes for solving feasibility problems are developed based on projection, among them alternating projection is the fundamental one. The method of alternating projection was ﬁrst introduced by von Neumann for the case of two linear subspaces [29], then was extended to closed convex sets by Bregman [11]. Relaxation is a standard approach to speed up alternating projection and related work can be found in [12, 22]. Proximal splitting methods, such as Forward–Backward [21] splitting and Backward–Backward splitting [13], and Peaceman–Rachford/Douglas–Rachford splitting [14, 25], can also be applied to solve feasibility problem either directly or up to reformulation. Moreover, equivalence between projection based methods and proximal splitting methods can be established, such as alternating projection is equivalent to Backward–Backward splitting while relaxed alternating relaxed projection covers Peaceman– Rachford/Douglas–Rachford splitting as special cases [13]. Our focus in this paper is Douglas-Rachford splitting method, which has shown to be eﬀective for solving feasibility problem, particularly in the non-convex setting [7]. However, the convergence property is rather less understood than its convex counter part. One reason for this is that Douglas–Rachford splitting method is not symmetric and non-descent, when compared to (proximal) gradient descent whose non-convex case is much better studied [4]. Research on non-convex Douglas–Rachford either focuses on speciﬁc cases or imposing stronger assumptions (e.g. smoothness) and proposes modiﬁcations to the original iteration. For instance [1] considers Douglas–Rachford splitting for solving feasibility problem of a line intersecting with a circle, and conditions for convergence are provided. In [19], the authors proposed a damped Douglas–Rachford splitting method for general non-convex optimization problem under the condition that one function has a Lipschitz continuous gradient. The study of this paper is motivated by applying Douglas–Rachford to solve Sudoku puzzle1, for which three diﬀerent convergence behaviors are observed • Globally, the method converges sub-linearly. • Locally, two regimes occur: ﬁnite termination and linear convergence. Finite termination and local linear convergence are reported in the literature [7, 10], however, conditions in respective work either are designed for convex setting or cannot be satisﬁed by Sudoku puzzle. Therefore, a new analysis is needed for Douglas–Rachford splitting which is the aim of this paper: 1. Finite termination Under a non-degeneracy condition, see (4.1), we show in Section 4 that one sequence generated by Douglas–Rachford splitting has the ﬁnite termination property. All sequences terminate in a ﬁnite number of iterations if the problem satisﬁes certain assumptions (e.g. polyhedrality, see Assumptions (A.1)-(A.3)). 2. Local linear convergence We also provide a precise characterization for the local linear convergence of Douglas–Rachford splitting method. Particularly, for Sudoku puzzle, we prove that locally the linear rate of convergence of Douglas–Rachford splitting method is precisely √ 5 5 . Moreover, such a rate is independent of puzzle size. For the damped Douglas–Rachford splitting method, we also provide an exact estimation of the local linear rate which depends on the damping coeﬃcient. Relation to Prior Work There are several existing work studying the ﬁnite termination property of the standard Douglas–Rachford splitting method. In [10], the authors established ﬁnite convergence of Douglas–Rachford in the presence of Slater’s condition, for solving convex feasibility problems where one set is an aﬃne subspace and the other is a polyhedron, or one set is an epigraph and the other one is a hyperplane. The result was extended to general convex optimization problems in [20] under the notion of partial smoothness [18]. In [23], ﬁnite termination is proved for ﬁnding a point which is guaranteed to be in the interior of one set whose interior is assumed to be non-empty. The result of [10] was later extended to the non-convex case in [7], where one of the two sets can be ﬁnite. For local linear convergence, results can be found in for instance [26] where linear convergence of Douglas–Rachford splitting method is established under a regularity condition. Similar results can be found in [15, 16]. Under a constraint qualiﬁcation condition, [19] also discussed the local linear convergence property of the damped Douglas–Rachford splitting method. Paper Organization The rest of the paper is organized as follows. Some preliminaries are collected in Section 2. Section 3 states our main assumptions on problem (3.1) and introduces the standard and damped Douglas–Rachford algorithms, global convergence is also discussed. Our main result on local convergence of Douglas–Rachford is presented in Section 4. In Section 5, we report numerical experiments on Sudoku puzzle and s-queens puzzle to support our theoretical ﬁndings.
Radiative heat transfer in nonlinear Kerr media<|sep|>The radiative properties of bodies play a fundamental role on the physics of many naturally occurring processes and emerging nanotechnologies1,2. Central to the theoretical understanding of these electromagnetic ﬂuctuation effects is the ﬂuctuation-dissipation theorem of electromagnetic ﬁelds, developed decades ago by Rytov and others3–5 in order to describe radiative transport in macroscopic media. The same formalism has been recently employed in combination with new theoretical techniques6,7 to demonstrate strong modiﬁcations of the thermal properties of nanostructured bodies, including designable selective emitters8 and greater than blackbody heat transport between bodies in the near-ﬁeld9. To date, these studies have focused primarily on linear media, where emission depends only on the linear response functions of the underlying materials. A cubic (χ(3)) nonlinearity, however, can convert light from one frequency to another or alter the dissipation rate10 and hence the ﬂuctuation statistics. We show that these phenomena lead to a variety of interesting effects in nonlinear radiators, such as lineshape alterations, temperature-dependent emission, and even radiation exceeding the black-body limit in nonequilibrium systems. In this work, we obtain a nonlinear ﬂuctuation–dissipation theorem (FDT) that describes radiative thermal effects in nonlinear χ(3) media, extending previous work on nonlinear oscillators11. Since nonlinear optical effects are generally weak in bulk materials, we focus on nanostructured resonant systems with strong effective nonlinear interactions10,12. Such systems are susceptible to universal descriptions based on the coupled-mode theory framework13,14, which we exploit to investigate the ways in which nonlinearities can enable interesting/designable radiative effects. In particular, we show that self-phase modulation (SPM) and two-photon absorption (TPA) effects lead to strong modiﬁcations of their emissivity, including thermal broadening and non-Lorentzian, asymmetric lineshapes. These nonlinear effects pave the way for additional material tunability, including designable, temperaturedependent selective emitters and absorbers. We also consider nonequilibrium situations and show that TPA results in se lective heat transfer exceeding the black-body limit, a phenomenon that has only been observed in situations involving multiple bodies in the near-ﬁeld9. Finally, we show that recently proposed, wavelength-scale cavities with ultra-large dimensionless lifetimes Q ≲ 108 and small mode volumes V ∼ (λ/n)3 can be designed to display these strongly nonlinear effects at infrared wavelengths and near room temperature. Fluctuation–dissipation relations in nonlinear media have been a subject of much interest in recent decades, starting with the early work of Bernard and Callen15, Stratonovich16, and Klimontovich17. The effects of nonlinearities of both conservative and dissipative nature on the Brownian motion of resonant systems have been studied in the context of Van der Pol oscillators17, optomechanical systems18, and mechanical Dufﬁng oscillators11,19. Despite the relatively large body of work involving noise in nonlinear systems, the role and consequences of nonlinear damping in mechanical oscillators have only recently begun to be explored19,20, and there remains much to be known about the underlying physical mechanisms in such systems. The effects of nonlinear noise are also nonnegligible and of great importance in a variety of applications, e.g. MEMS sensors21, frequency stabilization22, frequency mixing23, and ﬁltering24. While there is increased interest in studying nonlinear effects in micro and nano-mechanical oscillators, studies of nonlinear effects on thermal radiation remain scarce and are largely restricted to driven systems with conservative nonlinearities e.g. resonators based on RF-driven Josephson junctions25 or optomechanical oscillators26. (The situation is different in the quantum regime, where the effects of SPM on the tunneling rate and quantum statistics of photons have been well studied.)27,28 Following an approach analogous to the treatment of nonlinear friction in mechanical oscillators11, we extend previous work on Dufﬁng oscillators to the case of nonlinear photonic cavities coupled to external baths/channels, a situation of direct relevance to currentgeneration experiments on radiative thermal transport in photonic media29. Interestingly, we ﬁnd that effects arising from the interference of radiation reﬂected and emitted from the cavity into the external bath are crucial in order to observe thermal radiation enhancements in realistic situations, such as in cases where the external-bath temperature is at or near room temperature. We believe that these photonic systems not only offer new opportunities for understanding the role of nonlinear damping on ﬂuctuations, but also greatly extend the functionality and tunability of devices based on thermal radiation. As we argue below, while these effects require very strong optical nonlinearities, the increasing accessibility of ultra-high Q resonators with small modal volumes12,30–33, such as the nanobeam cavity explored below, offers hope that they may soon be within the reach of experiments.
Redshift Space Distortion of 21cm line at 1<z<5 with Cosmological Hydrodynamic Simulations<|sep|>The acceleration of the Universe has been one of the greatest mysteries since it was ﬁrst discovered by the observations of type Ia supernovae (Perlmutter et al. 1999). One of the most natural explanations of the accelerated expansion is the dark energy in the regime of general relativity or modiﬁed theory of gravity (e.g. Clifton et al. 2012, for review). Because the acceleration only becomes eﬀective at the late epoch of z ≲ 1, the most promising probe of dark energy or modiﬁed gravity is the large-scale structure of the Universe. Baryon acoustic oscillation (BAO) is recognized as a useful technique which is least aﬀected by the systematics to constrain the dark energy models (e.g. Albrecht et al. 2006). After the ﬁrst detection of BAO by the clustering of luminous red galaxies (LRG) in the Sloan Digital Sky Survey (SDSS) (Eisenstein et al. 2005), signiﬁcant attention has been paid to constrain the dark energy using BAO in the power spectrum and correlation function (e.g. Ross et al. 2015; Beutler et al. 2011). As the BAO is a measurement of the oscillation peak scales, an accurate prediction of the peak scales is required. It is well known that the oscillation peak scale is readily changed by the non-linear clustering of matter (Nishimichi et al. 2007) or the non-trivial couplings among diﬀerent ﬂuctuation modes due to galaxy bias (e.g. Cole et al. 2005; Dalal et al. 2008; McDonald & Roy 2009). Another important aspect of the BAO is the combination of parallel and perpendicular components to the line of sight (Alcock & Paczynski 1979). Although the APtest makes the BAO a more powerful tool to constrain cosmological parameters, the systematic eﬀect due to redshift space distortion (RSD) has to be taken into account as it is degenerate with the AP eﬀect. As we can observe galaxies only in redshift space, the distance to the galaxies are contaminated by the peculiar velocities of galaxies; on large scales, galaxies are coherently attracted toward the overdensity regions which makes the anisotropic two dimensional correlation function squashed, while on small scales, nonlinear random motion makes correlation function elongated along the line of sight (e.g. Matsubara 2004). The RSD is important not only for correctly understanding the distortion of the correlation function to utilize the AP eﬀects in the BAO, but also to gain an independent cosmological information from the BAO. Since the RSD
Stellar models in Brane Worlds<|sep|>Stellar astrophysics is one of the most characteristic topics studied by General Relativity (GR), which has helped to describe the dynamic and evolution of stars with unprecedented success[1]. In addition, the matter inside a star may be in some cases in extreme conditions generating complicated high energy phenomena, principally in white dwarfs, neutron stars, and others, and then a complete description of the stellar properties requires the introduction of a particular equation of state (EoS) like in the case of polytropes[2], or even Bose-Einstein Condensates (BEC)[3]. Another interesting possibility in recent times is to consider alternative theories of gravity and to look for their particular signatures in stellar models, specially for some of the extreme situations mentioned above. For instance, the authors in[4] considered the corrections induced by a Galileon Lagrangian in stars of constant density. Another example is given by the so-called models of Brane-Worlds (see[5, 6] for a good review) whose main characteristic is the existence of branes (four dimensional manifolds) embedded in a ﬁve dimensional bulk[7]. This particular geometry allows a natural extension of Einstein’s equations[8], and introduces new degrees of freedom through quadratic terms of the energy momentum tensor, the non-local Weyl terms, and other ﬁelds that could live in the bulk. This framework has been used for stars with a constant density in[9], and also for polytropic matter with a given relationship between the quantities arising from the non-local Weyl terms in[10]. It has also been shown that the exterior solutions of these branestars is not the Schwarzschild one[9, 11], and then the Weyl ﬂuids in the exterior of the stars can have a nonnegligible inﬂuence in the internal pressure and compactness of stellar objects. More recently, the conditions for stellar stability in brane-stars were revisited in[12] for a set of hypotheses called the minimal setup, which are consistent with a Schwarzschild exterior. Also see[13] for a study on the gravitational collapse of brane stars. With the previous background, this paper is dedicated to the study of the stellar equations of motion that arise from the formalism of Brane-World theory, and the role of the Weyl functions in the regular behavior of a stellar distribution. It is important to remark that our main objective is to consider models of stars as realistic as possible, and for this reason we will follow conventional wisdom in this regard: a Schwarzschild exterior, and regularity of all functions involved. Based on these premises, we perform numerical studies of the so-called extended GM solution with constant density, and of a polytropic ﬂuid. The organization of the paper is as follows. In Sec. II, we describe the equations of stellar dynamics with branes, emphasizing the high and low energy limits, boundary conditions, and the role played by the Weyl functions in providing consistent and regular solutions. Subsequently in Sec. III, we study the case of constant density and the extended GM solution. Also, in Sec. IV we study polytropic brane-stars. Finally, in Sec. V we
Two-parton Light-cone Distribution Amplitudes of Tensor Mesons<|sep|>In the past few years, BaBar and Belle have measured several charmless B decay modes involving light tensor mesons in the ﬁnal states [1]. These decays play a complementary role, compared with e.g., B → V V, V A, AA channels (V is a vector and A is an axialvector meson) [2, 3], since the tensor meson T can be produced neither from the local (axial-)vector current nor from the local tensor current which is relevant only to new physics. The polarization studies for B → TV, TA, TT decays can further shed light on the underlying helicity structure of the decay mechanism, recalling that the longitudinal polarization dominance observed in the decay B+ → φK∗ 2(1430)+ is quite diﬀerent from the polarization measurement in B → φK∗ which indicates a large fraction of transverse polarization [4]. In the quark model, the JP C = 2++ tensor meson can be modeled as a constituent quark-antiquark pair with the angular momentum L = 1 and total spin S = 1. The observed tensor mesons f2(1270), f ′ 2(1525), a2(1320) and K∗ 2(1430) form an SU(3) 1 3P2 nonet. The q¯q content for isodoublet and isovector tensor resonances are obvious. ∗ Nevertheless, in full QCD ﬁeld theory, the tensor meson is represented by a set of Fock states, each of which has the same quantum number as the meson. In this work, we present the study for two-parton asymptotic light-cone distribution amplitudes (LCDAs) of lowest-lying tensor mesons with quantum numbers JP C = 2++ because, in the treatment of exclusive B decay processes in QCD, the Fock states of the energetic meson can be further represented in terms of LCDAs. The LCDAs are governed by the special collinear subgroup SL(2, R) of the conformal group [6, 7] and can be expanded as a series of partial waves, where the rotational invariance is characterized by the conformal spin j and the concept of “collinear twist” is equivalent to the “eigen-energy” in quantum mechanics. Due to the G-parity of the tensor meson, according to our deﬁnition, both the chiraleven and chiral-odd two-parton LCDAs of the tensor meson are antisymmetric under the interchange of momentum fractions of the quark and anti-quark in the SU(3) limit. The asymptotic LCDAs are relevant to the ﬁrst Gegenbauer moment of the leading twist distribution amplitudes, φ∥ and φ⊥. In analogy to the cases of axial-vector mesons [3, 8], the sizable Gegenbauer term containing the ﬁrst Gegenbauer moment could have a large impact on B decays involving a tensor meson. The present paper is organized as follows. In Sec. 2 we deﬁne the LCDAs for the tensor mesons. A slightly diﬀerent deﬁnition for chiral-even LCDAs is given in [14]. The detailed properties of LCDAs are given in Sec. 3. Results for the decay constants are presented in Sec. 4. Sec. 5 comes to our conclusion. ∗Just as the η-η′ mixing in the pseudoscalar case, the isoscalar tensor states f2(1270) and f ′ 2(1525) also have a mixing, and their wave functions are deﬁned by 2(f u 2 + f d 2 ) cos θf2 + f s 2 sin θf2 , f ′ 2(1525) = 1 with f q 2 ≡ q¯q. Since ππ is the dominant decay mode of f2(1270) whereas f ′ 2(1525) decays predominantly into K ¯K (see Ref. [1]), it is obvious that this mixing angle should be small. More precisely, it is found that θf2 = 7.8◦ [5] and (9±1)◦ [1]. Therefore, f2(1270) is primarily a (u¯u+d ¯d)/ √
Towards precision distances and 3D dust maps using broadband Period--Magnitude relations of RR Lyrae stars<|sep|>RR Lyrae stars are old (age ≳ 10 × 109 yr) Population II pulsating stars that exist throughout the Milky Way Bulge, Disc, and Halo. At optical wavebands they are variable with peak-to-peak amplitudes up to about 1 mag. This amplitude generally diminishes with increasing wavelength to around 0.3 mag in the mid-infrared (λ ∼ 4µm). The heat generation and gravitational support of RR Lyrae stars comes from the fusion of helium in the core and hydrogen in a shell surrounding the core. RR Lyrae stars have speciﬁc values of temperature, luminosity, and radius such that they exist in the instability strip of the Hertzsprung–Russell diagram. In this slice of stellar parameter-space stars are unstable to radial oscillation, and RR Lyrae stars oscillate with peri
UV Direct-Writing of Metals on Polyimide<|sep|>laser or ink-jet printing, are additive processes which  eliminate the traditional resist moulding and mask-based  photo-patterning steps.  Such techniques reduce the amount  of wasted materials, the turnaround time and manpower cost.   Laser, electron-beam and X-ray direct-writing have been  employed for a long time in an enclosed pressurized  chamber [1].  In search of low cost direct-writing processes  without using any vacuum facilities, new types of processes  have emerged which can be carried out through the coating  of liquid solution or solid over-layer.  The chart in Fig. 1  shows a review of the recent approaches employed by  researchers worldwide.  To minimize the effect of diffraction  of light through liquid, direct-writing through a thin solid  over-layer coating on the substrate is favored in order to  achieve high resolution of sub-micron features. We propose  a novel technique of direct laser writing, at atmospheric  pressure, of metallic patterns onto a polyimide. Polyimide is a high performance polymer widely used in  microsystems technology owing to its many desirable  properties such as low dielectric constant (low-k), high  mechanical strength, high resistance to moisture and heat (Tg  = 365oC), chemical inactivity as well as bio-compatibility. conducting substrates. The work described in this article is highlighted. The ability to direct-write metals on polyimide substrates in  air without using the conventional photolithography method  can thus motivate developments of innovative applications  such as advanced interconnections, chip packaging in  microelectronics, integration of electrical functions in BioMEMS implants and micro-fluidic devices.
Tunable scattering cancellation of light using anisotropic cylindrical cavities<|sep|>Cloaking and invisibility are optical techniques with considerable advances recently due to the advent of metamaterials. Designs for cloaking where a shadow region prevents the light-matter interaction with a tailored target placed therein are largely based on transformation optics [1,2,3] providing extensive theoretical studies and physical analysis without drawing on numerical simulations [4,5]. On the other hand, invisibility relies on the scattering cancellation of a given object by using for instance a metallic coating and even complex nanostructured coverings [6,7]; the negative polarizability of the carpet layer might severely drop the scattering cross section of the particle making it undetectable [8]. The ﬁrst experimental realization was performed 1 Department of Optics and Optometry and Vision Science, University of Valencia, Dr. Moliner 50, Burjassot 46100, Spain ∗ Corresponding author: E-mail: carlos.zapata@uv.es
The shortest period detached white dwarf + main-sequence binary<|sep|>Mass-radius relations are of fundamental importance in a wide range of astrophysical circumstances. They are routinely used to infer accurate masses and radii of transiting exoplanets, calibrating stellar evolutionary models and understanding the late evolution of mass transferring binaries such as cataclysmic variables (Littlefair et al. 2008; Savoury et al. 2011). Additionally, the mass-radius relation for white dwarfs has played an important role in estimating the distance to globular clusters (Renzini et al. 1996) and the determination of the age of the Galactic disk (Wood 1992). Mass-radius relations play a crucial role in determining stellar parameters of single, isolated stars as well as in non-eclipsing binaries, where direct measurements of masses and radii are not possible. Despite their importance, the mass-radius relations for both white dwarfs and low-mass stars remain largely untested. Provencal et al. (1998) tested the white dwarf massradius relation using Hipparcos parallaxes to determine the radii for white dwarfs in visual binaries, common propermotion (CPM) systems and ﬁeld white dwarfs. However, the radius measurements for all of these systems still rely to some extent on model atmosphere calculations. For ﬁeld white dwarfs the mass determinations are also indirect. Barstow et al. (2005) used Hubble Space Telescope/STIS spectra to measure the mass of Sirius B to high precision, however, their radius constraint still relied on model atmosphere calculations and is therefore less direct when it comes to testing white dwarf mass-radius relations. To date, only two white dwarfs have had their masses and radii modelindependently measured, V471 Tau (O’Brien et al. 2001)
High-z dusty star-forming galaxies: a top-heavy initial mass function?<|sep|>The opening of the sub-millimeter (sub-mm) window on galaxy formation and evolution, thanks to the 850 µm surveys with the Submillimeter Common-User Bolometer Array (SCUBA) on the James Clerk Maxwell Telescope (JCMT; e.g., Smail et al. 1997; Hughes et al. 1998; Barger et al. 1998), has revolutionized the ﬁeld. SCUBA blank-ﬁeld pointings demonstrated that the most active star formation phases of high-z galaxies are heavily dust-enshrouded and therefore largely missed even by the deepest optical surveys (for a review see Casey et al. 2014). The results of these surveys have strongly shaken up the leading galaxy formation paradigm of the time which under-predicted the sub-mm counts by one order of magnitude or more (e.g., Kaviani et al. 2003; Baugh et al. 2005). Part of the discrepancy was later shown to be attributable to an overestimate of the observed counts (Coppin et al. 2006; Geach et al. 2017), mainly due to source blending within the relatively large SCUBA beam and to insufﬁcient corrections for ﬂux boosting (Eddington bias; Eddington 1913). Nevertheless, sub-mm data are still challenging for semi-analytic galaxy formation models (SAMs). In particular, SAMs generally under-predict the bright end of the star formation rate (SFR) function at z ∼> 2 (Niemi et al. 2012; Gruppioni et al. 2015). A further challenge came from the ﬁrst searches for submm selected z ∼> 4 galaxies (Dowell et al. 2014; Asboth et al. 2016; Ivison et al. 2016): the inferred surface densities at ﬂux densities larger than several tens mJy at 500 µm were found to be in excess of model predictions by about one order of magnitude. More recent studies (Donevski et al. 2018; Duivenvoorden et al. 2018) suggest that the discrepancy with models may be accounted for by ﬂux boosting due to instrumental noise and confusion (including the contribution from clustering), plus source blending. However, the issue is not settled yet (see Section 2). On the other hand, a key ingredient to understand the star formation process is the initial mass function (IMF), i.e., the relative number of newly formed stars as a function of their mass. It is still being debated whether or not the IMF is universal or varies with conditions in star-forming regions. Bastian et al. (2010), in their comprehensive review, concluded that the data available at the time did not provide
Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels<|sep|>A natural way to consider two parallel sentences in different languages is that each language expresses the same underlying meaning from a different viewpoint. Each language can be thought of as a transformation that maps an underlying concept into a view that we collectively agree is determined as ‘English’ or ‘French’. Similarly, an image of a cat and the word ‘cat’ are expressing two views of the same underlying concept. In this case, the image corresponds to a high bandwidth channel and the word ‘cat’ to a low bandwidth channel. This way of conceptualizing parallel viewpoints naturally leads to the formulation of a fully generative model over each instance, where the transformation corresponds to a particular generation of the underlying view. We deﬁne each of these views as a channel. As a concrete example, given a parallel corpus of English and French sentences, English and French become two channels, and the corresponding generative model becomes p(English, French). One key advantage of this formulation is that a single model can be trained to capture the full expressivity of the underlying concept, allowing us to compute conditionals and marginals along with the joint. In parallel sentences, the conditionals correspond to translations from one channel to another while the marginals correspond to standard monolingual language models. In this work, we present a general framework for modeling the joint distribution p(x1, ..., xk) over k channels by marginalizing over all possible factorizations across the channels and within each channel. This formulation allows our framework to perform: 1) unconditional generation, 2) fully conditional generation (source channels are fully observed and ﬁxed), and 3) partial conditional generation (source channels contain incomplete sequences). The key contributions in this work are: 1. We present MGLM, a multichannel generative modeling framework. MGLM models the joint distribution p(x1, . . . , xk) over k channels by marginalizing over all possible factorization across and within sequences. 2. Since MGLM is trained over all possible factorizations, MGLM can perform both conditional generation (e.g., machine translation with fully observed source channel), and partially observed conditional generation across different channels (e.g., seeding each channel "Žába je zelená" "The green" "grenouille" "Der ist" MGLM "The frog is green" "La grenouille est verte" "Der Frosch ist grün" Figure 1: Multichannel Generative Language Models (MGLM) marginalize over all possible factorizations of the joint distribution within and across all channels (e.g., languages). MGLM is trained to predict the tokens to be inserted (in bold), given partially observed inputs. At inference, MGLM can take full, partial, or empty sequence from each channel and generate the full sequence for each channel. 3. In the case of conditional generation over multiple target languages, we show that we are competitive in BLEU and have signiﬁcant advantages in inference time and model memory savings.
Asymmetry and structural information in preferential attachment graphs<|sep|>Study of the asymptotic behavior of the symmetries of graphs, originally motivated by enumerative combinatorial problems, has recently found diverse applications in problems ranging from graph compression to discovering interesting motifs to understanding dynamics of growing graphs. Let us explore some of these applications in more detail. The basic problem of structural (unlabeled graph) compression can be formulated as follows: given a probability distribution on labeled graphs, determine a binary encoding of samples from the induced unlabeled graph distribution so as to minimize expected description length. In [6] the authors studied this problem in the setting of Erd˝os-R´enyi graphs. They showed that, under any distribution giving equal probability to isomorphic graphs, the entropy of the induced distribution on graph structures (i.e., isomorphism classes of graphs) is less than the entropy of the original distribution by an amount proportional to the expected logarithm of the number of automorphisms. Thus the solution to the above problem is intimately connected with the symmetries of the random graph model under consideration. We explore this topic in some detail in Lemma 1 of Section 2. We mention also a few representative algorithmic motivations for the study of graph symmetries. The ﬁrst involves the problem of motif discovery: given a graph G and a pattern graph H, the problem is to ﬁnd all subgraphs of G that are isomorphic to H. It has been observed (see, e.g., [19]) that taking into account the symmetries of H can signiﬁcantly decrease the time and space complexity of the task. The same is true for G if it has nontrivial symmetries. In the area of Markov chains, the paper [5] studies the following problem: given a graph G, the task is to assign weights to edges of G so as to minimize the mixing time of the resulting Markov chain. The authors show that symmetries in G may be exploited to signiﬁcantly reduce the size of a semideﬁnite program formulation which solves the problem. Moreover, they point out several references to the literature in which symmetry plays a key role in reducing complexity for various problems. Study of symmetries is further motivated by their connection to various measures of information contained in a graph structure. For instance, the topological entropy of a graph, studied in [20] and [23], is a function on graphs that measures the uncertainty in the orbit class (i.e., the set of nodes having the same long-term neighborhood structure) of a node chosen uniformly at random from the node set of the graph. Note that, unlike the labeled and unlabeled graph entropies that we study throughout this paper, the topological entropy is a function of a particular graph, rather than of a probability distribution on graphs. If the graph is asymmetric, then the topological entropy is maximized: if n is the size of the graph, then the topological entropy is, to leading order, log n. In general, if the symmetries of the graph can be characterized precisely, then so can the topological entropy. The present paper is a step in the direction of understanding symmetries of complex networks and toward extending graph structure compression results to random graph models other than Erd˝os-R´enyi. In particular, many real-world graphs exhibit a power-law degree distribution (see [9]). A commonly studied model for real-world networks is the preferential attachment mechanism introduced in [1], in which a graph is built one vertex at a time, and each new vertex t makes m choices of neighbors in the current graph, where it attaches to a given old vertex v with probability proportional to the current degree of v. We study here a simple variant of the preferential attachment model (see [9] and the conclusion section for other models), and in the conclusion of this paper we suggest that the symmetry behavior of other preferential models can be studied using the approach developed here. Our main result is the following: for the variant of the preferential attachment model under consideration, when each vertex added to the graph chooses a ﬁxed number m ≥ 3 neighbors, with high probability, there are no nontrivial symmetries. This is in stark contrast to the many symmetries observed in real-world networks [15]. As we remark below our statement of Theorem 1, the asymmetry threshold (as well as the degree sequence power law exponent) changes with other parameters in variants of preferential attachment, such as aﬃne and nonlinear models, which may explain this discrepancy. The problem of establishing asymmetry in preferential attachment graphs appears to be diﬃcult, and literature on it seems to be scarce. We are aware only of [17], which proved that such graphs are symmetric for m = 1 and (with asymptotically positive probability) also for m = 2. The authors of [17] conjectured that preferential attachment graphs are indeed asymmetric for m ≥ 3. In this paper we ﬁrst settle this conjecture in the aﬃrmative using diﬀerent methods than the one applied in [12] and [17]. Namely, instead of relying on the graph defect (a measure of asymmetry deﬁned in [12] which is necessarily bounded in preferential attachment graphs, and hence has poor concentration properties), we shall observe that symmetry would imply that certain vertices make the same choices with regard to an initial set of vertices uniquely identiﬁable by their degrees, which we prove is unlikely to happen for preferential attachment graphs whenever m ≥ 3. After settling the asymmetry question for preferential attachment graphs, we use it to address the issue of structural entropy. We ﬁrst review an estimate of the labeled graph entropy given in [22], and then estimate the unlabeled graph entropy (also known as the structural entropy). In Lemma 1 we relate both entropies. Then, using our asymmetry result from Theorem 1, we estimate the structural entropy. To derive the structural entropy estimate, we study the characteristics of the directed, acyclic graph version of the preferential attachment process (culminating in Proposition 1, which may be of independent interest). In particular, we estimate the number of ways that a given graph could have arisen according to the preferential attachment mechanism. We additionally estimate the typical height (i.e., the length of the longest directed path) of this directed version of the graph, which, being a natural structural quantity, may be of independent interest. We emphasize that the labeled and unlabeled graph entropies that we study are fundamental, as they give the minimum achievable expected length of any preﬁx source code (i.e., compression code) for these graphs. Now we review some of the literature on symmetries of random graphs. The study of the asymptotic behavior of the automorphism group of a random graph started with a paper of Erd˝os and R´enyi [10], where they showed that G(n, M) (i.e., the uniformly random graph on n vertices with M edges) with constant density (i.e. when M = Θ(n2)) is asymmetric with high probability, a result motivated by the combinatorial question of determining the asymptotics of the number of unlabeled graphs on n vertices for n → ∞. Then Wright [25] proved that G(n, M) whp becomes asymmetric as soon as the number of isolated vertices in it drops under 1. His result was later strengthened by Bollob´as [3], who also proved asymmetry for r-regular graphs with r ≥ 3. The asymptotic size of the automorphism group of G(n, M) for small M, where the graph is not connected, was given by �Luczak [13]. As a similar question motivated the investigation of symmetry properties of random regular graphs, Bollob´as improved his result from [3] by showing in [2] that unlabelled regular graphs with degree r ≥ 3 are whp asymmetric as well. Let us note that it is a substantially stronger theorem (see the discussion below after Theorem 1). For general models, asymmetry results can be nontrivial to prove, due in part to the fact that asymmetry is a global property. Furthermore, the particular models considered here present diﬃculties not seen in the Erd˝os-R´enyi case: there is signiﬁcant dependence between edge events, and graph sparseness makes derivation of concentration results diﬃcult. However, settling the symmetry/asymmetry question opens the door to several other lines of investigation, including, e.g., the design of optimal structural compression schemes and the precise characterization of the limits of inference problems (see, for example, [16]) for preferential attachment graphs. These applications crucially depend on our precise understanding of graph symmetry. The paper is organized as follows. In Section 2 we present our main results regarding the graph asymmetry and structural entropy. In Section 3, we state and prove several results on the degree sequence which will be useful in subsequent proofs. We prove the graph asymmetry result in Section 4 and the entropy result, along with the necessary structural results on the directed version of the model, in Section 5.
Disease Normalization with Graph Embeddings<|sep|>In biomedical search applications, e.g. clinical search applications, it is of key importance to query not simply by keywords but rather by concept, viz., resolve names into their synonyms and return all matching documents. This is particularly true regarding diseases [4]. When we issue a query about “ovarian cancer”, we expect the system to return hits mentioning standard synonyms such as “ovarian neoplasms”. To achieve this it is necessary not only to perform named entity recognition (NER) to detect all entities mentioned in a document, but also to disambiguate them against databases of canonical names and synonyms, a task known as entity normalization or linking (EL). Detecting and normalizing disease mentions are challenging tasks due to linguistic variability (e.g. abbreviations, morphological and orthographic variation, word order) [15,23]. Interest in these tasks led to the creation of disease knowledge bases and annotated corpora. The Medical Subject Headings (MeSH®) taxonomy [17] is a repository of clinically-relevant terms covering (among others) a large range of standardized disease names, where standard names and known synonyms are organised hierarchically. The NCBI disease corpus [5] is a collection of PubMed® abstracts with disease mentions manually resolved to MeSH® or OMIM® concepts. Annotated resources allow the training of supervised learning algorithms. The ﬁrst such system was [15], which used a learning-to-rank approach. More recently,
Photometric observation of HAT-P-16b in the near-UV<|sep|>HAT-P-16b, one of �3171 conﬁrmed transiting exoplanets, is a hot Jupiter orbiting a F8-type star with a short orbital period of 2:77596 � 0:000003 d, a mass of 4:193 � 0:094 MJup, and a radius of 1:289 � 0:066 RJup (Buchhave et al., 2010). Several follow-up spectroscopic studies have constrained additional stellar and planetary parameters, Husnoo et al. (2012) conﬁrmed that the planet had the smallest reliably measured eccentricity of 0:034 � 0:003. There have been no additional photometric measurements or secondary transit measurements of this planet yet. The transit method for detecting and observing exoplanets is the only method that allows direct measurements of the planetary radius and can also be used to determine planetary parameters such as surface gravity, average density, atmospheric composition, semi-major axis and eccentricity (Charbonneau et al., 2007). We can obtain information about certain absorption features of the planet as light gets ﬁltered through a planet’s atmosphere during the primary transit. Probing the optical and near-UV wavelengths where the opacity is dominated by Rayleigh scattering can give us insight to information about clouds at high-altitudes (Pont et al., 2008), atmospheric circulation patterns (Knutson et al., 2007) and possible interior structures and compositions (Charbonneau et al., 2002). In addition, Lazio et al. (2010a) theorized that the transit method can be used to detect exoplanet magnetic ﬁelds. The gas giant planets in our Solar System possess magnetic ﬁelds (Russell and Dougherty, 2010) and it has been predicted through interior structure models that extrasolar gas giants should have magnetic ﬁelds (Sánchez-Lavega, 2004). Detecting and studying the magnetic ﬁelds of exoplanets will allow for the investigation of more properties of exoplanets, including interior structure and rotation period (Lazio et al., 2010a), atmospheric retention (Lazio et al., 2010a; Cohen et al., 2011), and the presence of extrasolar moons (e.g., the variability in Jupiter’s magnetic ﬁeld can be attributed to the presence of Io (Lazio et al., 2010a)). Griemeier et al. (2005) suggest that the magnetic ﬁeld of Earth helps contribute to its habitability by deﬂecting cosmic rays and stellar wind particles; exoplanets could also exhibit this characteristic (Lazio et al., 2010a). Studying the magnetic ﬁelds of hot Jupiters will help lay the foundation for the characterization of magnetic ﬁelds around Earth-like and other types of planets, and therefore, it will aid in the search for life outside our Solar System. There are several methods that can be used in attempts to detect magnetic ﬁelds of exoplanets. Farrell et al. (1999), Zarka et al. (2001), and Lazio et al. (2010b) suggest that the most direct method for detecting the magnetic ﬁeld of an exoplanet is through radio emission from the planet generated by electron-cyclotron maser interactions. This electron-cyclotron maser radio emission is generated as new material is injected into the magnetosphere due to interactions between the solar wind and magnetosphere. Many studies conducted to ﬁnd exoplanet radio emissions have resulted in non-detections (e.g., Yantis et al., 1977; Winglee et al., 1986; Ryabov et al., 2004; George and Stevens, 2007; Lazio and Farrell, 2007; Smith et al., 2009; Lecavelier Des Etangs et al., 2009; Lecavelier Des Etangs et al., 2011; Lazio et al., 2010b; Lazio et al., 2010c) and one possible detection by Lecavelier des Etangs et al. (2013) of the hot Neptune, HAT-P-11b. Alternatively, Cuntz et al. (2000), Saar and Cuntz (2001), and Ip et al. (2004) proposed that the interaction of an exoplanet’s magnetic ﬁeld with that of its host star could produce detectable changes in the star’s outer layers and corona in phase with the planet’s orbit. This indirect method of detecting the magnetic ﬁeld of an exoplanet was validated through observations of Ca II H K emission by Shkolnik et al. (2003, 2005, 2008), and Gurdemir et al. (2012). There is also a non-detection by Miller et al. (2012) while observing WASP-18b. However, it is possible an observational bias can lead to spurious trends of star-planet-interaction signatures due to planet detection methods and should be taken into consideration (Poppenhaeger and Schmitt, 2011). In this paper we use another method to attempt to detect interference in our light curve due to the magnetic ﬁeld of an exoplanet, described by Vidotto et al. (2010, 2011a,b,c), Lai et al. (2010), and Llama et al. (2011). They proposed that if a transiting exoplanet harbors a magnetic ﬁeld it will show an early transit ingress in the near-ultraviolet (near-UV) wavelengths but not at longer wavelengths, while the transit egress will remain the same. The authors explained this effect by the presence of a bow shock in front of the planet formed by the interactions between the stellar coronal material and the planet’s magnetosphere. Moreover, if the shocked material is sufﬁciently optically thick, it will absorb starlight and cause an early ingress in the near-UV light curve (Vidotto et al., 2011b, see Fig. 6). An early near-UV ingress has been observed in one transiting exoplanet, WASP-12b (Fossati et al., 2010 hereafter FHF10). Observations by FHF10 of WASP-12b using the Hubble Space Telescope with the NUVA (253.9–258.0 nm) nearUV ﬁlter indicate that the near-UV transit started approximately 25–30 min earlier than its optical transit. Using these observations, Vidotto et al. (2010) determined an upper limit for the magnetic ﬁeld of WASP-12b to be �24 G. We can constrain the properties of the planet’s magnetic ﬁeld by analyzing the difference in ingress times in different wavelength bands. However, VJH11a do not go into detail about whether this effect can only be seen in narrow-band spectroscopy or broad-band near-UV photometry. Additionally, Vidotto et al. (2011c) predicted that bow shock variations should be common and are caused by eccentric planetary orbits, azimuthal variations in coronal material (unless the planet is in the corotation radius of the star), and time-dependent stellar magnetic ﬁelds (e.g., coronal mass ejections, magnetic cycles, stellar wind changes). Consequently the near-UV light curve of exoplanets predicted by VJH11a will exhibit temporal variations. We chose HAT-P-16b for our study because it is listed as one of the possible candidates predicted by VJH11a to exhibit near-UV asymmetries. In addition, the WASP-12 and HAT-P-16 systems have very different physical characteristics (see Table 1 for a summary). We also include information on TrES-3b since its magnetic ﬁeld was previously constrained by Turner et al. (2013) to be between 0.013 to 1.3 G and HAT-P-11b has a possible upper limit on its magnetic ﬁeld to be 50 G (Lecavelier des Etangs et al., 2013). Therefore, since FHF10 observed near-UV asymmetries in WASP-12b and all of these planets are eccentric perhaps a more diverse planet(ie. one with different properties), HAT-P-16b, could exhibit this effect. If observed, a difference in timing between the near-UV and optical light curves of HAT-P-16b can be used to determine the planetary magnetic ﬁeld, Bp, with the following equation derived from Vidotto et al. (2011b):
Dust dynamics and evolution in expanding HII regions. I. Radiative drift of neutral and charged grains<|sep|>Theoretical and observational studies of dust evolution and survival in H ii regions have a long history. The ﬁrst evidence for dust within an H ii region was found from observations of scattered light in the Orion Nebula (O’Dell & Hubbard 1965) and several other H ii regions (O’Dell, Hubbard & Peimbert 1966). These ﬁrst observations hinted that dust content in H ii regions decreases towards ionizing stars. Development of infrared (IR) detectors has allowed the observation of not only light scattering by dust in H ii regions but also proper dust emission (e.g., Ney & Allen 1969; Stein & Gillett 1969; Harper & Low 1971). These observations have generally conﬁrmed the presence of inner cavities with low dust density in compact and extended H ii regions (e.g., Gillett et al. 1975; Aitken, Griﬃths & Jones 1977; Nakano et al. 1983; Chini, Kruegel & Wargau 1987; Aannestad & Emery 2001). On the other hand, evidence has also been presented in favour of dust overabundance inside H ii regions (Panagia 1974; Tenorio-Tagle 1974). The interest in the dust in H ii regions was renewed by observations of Spitzer Space Telescope1, Herschel Space Observatory2 and WISE. One of the great Spitzer results was a discovery of numerous IR emission bubbles in our Galaxy. Churchwell et al. (2006) argued that these bubbles are primarily formed by hot young stars in massive star forming regions. Detailed study of the bubbles observed by Spitzer showed that 86 per cent of these objects indeed enclose H ii regions (Deharveng et al. 2010). The IR bubbles look like partial or closed rings on 8 µm maps (Churchwell et al. 2006; Simpson et al. 2012). A similar ring-like morphology is seen in longer wavelength images obtained with Herschel (Anderson et al. 2012), in the sense that in the far-IR the bubbles look like rings coinciding with 8 µm emission rings or residing somewhat further away from the ionizing source (Paladini et al. 2012). An IR bubble morphology at 24 µm is signiﬁcantly diﬀerent. Emission at this wavelength is observed not only towards the outer ring 1 Spitzer Space Telescope is operated by the Jet Propulsion Laboratory, California Institute of Technology, for the National Aeronautics and Space Administration. 2 Herschel is an ESA space observatory with science instruments provided by European-led Principal Investigator consortia and with important participation from NASA.
Dissecting Energy Consumption of NB-IoT Devices Empirically<|sep|>The recent explosion in the number of IoT devices has been supported by a few proprietary low power wide area systems, which rely on unlicensed spectrum. Their popularity caused Third Generation Partnership Project (3GPP) to investigate cellular IoT technologies, resulting in the development of Long Term Evolution (LTE)-M and Narrowband-IoT (NB-IoT) standards. A main focus area of these technologies is high energy efﬁciency, enabling devices operated by tiny batteries to operate for prolonged periods of time. The main advantages of the 3GPP standards are the use of licensed spectrum and the fact that they build upon existing 3GPP technologies, allowing for more stable and predictable performance, and reuse of infrastructure. LTE-M and NB-IoT are critical in F. Michelinakis, A. Saeed Al-selwi and A. Elmokashﬁ are with Simula Metropolitan, Oslo, 0167 Norway (e-mail: foivos, anasal, ahmed@simula.no). M. Capuzzo and A. Zanella are with University of Padova Italy (e-mail: capuzzom, zanella@dei.unipd.it). K. Mahmood is with Telenor Research Norway (e-mail: Kashif.Mahmood@telenor.com). enabling future 5G networks to support the density and latency requirements of massive machine type communications [1]. They can also seamlessly coexist with the upcoming New Radio (NR) access technology, since the latest standards allow the reservation of NR time-frequency resources for LTEM and NB-IoT transmissions. In this work, we focus on NB-IoT, which provides lower throughput but more robust connectivity than LTE-M, and is hence geared towards massive deployments of IoT devices. Energy efﬁciency is certainly a major concern for typical IoT deployment scenarios, since batteries of IoT devices are not meant to be recharged or replaced, tying the lifetime of the battery to the lifetime of the device itself. Our analysis aims to quantify the impact of several parameters to energy consumption and reveals that network conﬁgurations may greatly affect the device lifetime, without offering performance gains. For example, we show that setting a ﬂag may reduce the energy needed to transmit an Uplink packet and receive a response under good signal conditions from 0.82 J to 0.12 J, with no performance penalty. In a scenario, where 6 messages per day are transmitted, the device’s lifetime is extended from 8.5 years to 30. NB-IoT users though, are naturally inclined to believe that, in analogy with broadband cellular services, NBIoT services can also be accessed in a plug–and–play fashion, without or with minimal set-up of the end devices. In the same fashion, application developers should not rely on default settings and, instead, carefully pick parameter values that best match the tradeoff between delay and device lifetime of their use-case. The purpose of this paper is to go beyond the early studies of empirical NB-IoT performance characterization, most notably [2], [3], [4], whose ﬁndings and limitations are discussed in detail in Section IX. Comparatively, our experiments are more comprehensive: we 1) test more operators and / or more modules, thus revealing inefﬁciencies of speciﬁc moduleoperator combinations 2) use the latest NB-IoT features and 3) study a bigger variety of scenarios. In particular, we analyze the intricacies of operator conﬁguration and strategies that greatly affect key metrics and battery life, while also deep diving into the performance of Release-13 enhancements. We conduct the ﬁrst exhaustive experimental study of its kind, exploring the NB-IoT ecosystem, under various power management conﬁgurations. The experiments involved two different NB-IoT boards and two main telecommunication operators in a western European country, so as to appreciate the impact of implementation choices on the system energy efﬁciency. Since we focus on parameter tuning the main ﬁndings can be generalized to other networks and devices. Frequency Licensed LTE frequency bands Bandwidth 180 kHz Theoretical peak data rate at the physical layer 226.7 kbps (DL); 250 kbps (UL) Our measurements are spread across several months in the period between October 2018 and October 2019. Our main contributions are: 1) a thorough presentation of the power-saving mechanisms supported by the latest commercially available NB-IoT release; 2) experimental study of the different conﬁgurations and operator strategies, where we quantify their impact on energy consumption and network Key Performance Indicators (KPIs) at the Radio Resource Control (RRC) state level and, when applicable, within a state; 3) analysis of which metadata metrics better reﬂect the device behavior and 4) an algorithm for extracting the state of a device directly from current time series logs. In the sequel, we elaborate on some rather surprising ﬁndings. The energy consumption of NB-IoT devices does not seem to be strongly affected by channel conditions, except in extremely harsh conditions. Furthermore, under default parameters setting, the packet size has negligible impact on the overall device power consumption, while its effect becomes more signiﬁcant when energy saving mechanisms are used. Based on such empirical observations, we provide indication for device-side and network-side parameter conﬁgurations that yield similar application-level performance, while preserving the device battery. We have communicated the ﬁndings to the measured operators, and they reconﬁgured their networks accordingly resulting in a boost in energy efﬁciency for end users. The remainder of this paper is structured as follows: Section II is an introduction to the speciﬁcs and mechanisms of NB-IoT and Section III describes our experiment workﬂow. Sections V and VI break down the parameters that affect energy consumption, Section VII discusses network KPIs and Section VIII summarizes our ﬁndings. Section IX looks into how the above affect typical NB-IoT usecases, Section X condenses the existing literature and Section XI concludes this article. Finally, we include two technical appendices, where we discuss how we isolate device states and prove the relationship of two metadata metrics.
Measurement of the muon-induced neutron seasonal modulation with LVD<|sep|>As it is known, there is seasonal variation of the muon ﬂux at the sea level and underground [1]. The variation is caused by the temperature and barometric eﬀects associated with the increase of the atmosphere altitude in summer and decrease of it in winter. Temperature eﬀect infuences the muon production in the π/K- decays, barometric eﬀect inﬂuences the muon decays. For high-energy muons, which we detect at 3650 m w.e. underground there is a positive temperature eﬀect. It is associated with the fact that far underground penetrate, mainly, muons from pions of the ﬁrst generation, the number of which increases when the atmosphere density of the upper layers (at the altitude ∼ 20 km) falls due to expansion of the atmosphere. Measurements of the muon intensity variation deep underground were performed by experiments MACRO [2], MINOS [3], AMANDA [4], LVD [5], IceCube [6], Borexino [7]. In the LVD papers [5], [8] the seasonal modulation of the muon intensity and the variation of the neutrons produced by muons in the LVD material were presented. The variation amplitude of the muon ﬂux intensity Iµ = 1.5% and phase φ = 185±15 were measured during 8 years of observations. Moreover, it was obtained that the amplitude of the neutron number variation is higher than the muon intensity modulation amplitude almost 10 times as much in Ref.[9]. The purpose of this work is to determine more accurately the magnitude of amplitude and phase of the muon-induced neutron seasonal modulation.
Analytical properties of Einasto dark matter haloes<|sep|>The Λ cold dark matter (CDM) model, with (Ωm, ΩΛ) = (0.3, 0.7), has become the standard theory of cosmological structure formation. ΛCDM seems to agree with the observations on cluster-sized scales (Primack 2003); however, on galaxy/sub-galaxy scales there appears to be a discrepancy between observations and numerical simulations. Highresolution observations of rotation curves, in particular of low surface brightness (LSB) and dark matter dominated dwarf galaxies (de Blok et al. 2001; van den Bosch & Swaters 2001; Swaters et al. 2003; Weldrake et al. 2003; Donato et al. 2004; Gentile et al. 2005; Simon et al. 2005; Gentile et al. 2007; Banerjee et al. 2010) favour density proﬁles with a ﬂat central core (e.g. Burkert 1995; Salucci & Burkert 2000; Gentile et al. 2004; Li & Chen 2009). In contrast, N-body (dark matter only) CDM simulations predict galactic density proﬁles that are too high in the centre (e.g. Navarro, Frenk, & White (1996, 1997) (NFW); Moore et al. 1999). This discrepancy is called the cuspcore problem; a complete review can be found in de Blok (2010). Gravitational lensing is one of the most powerful tools in observational cosmology for probing the matter distribution of galaxies and clusters in the strong regime (Kochanek et al. 1989; Wambsganss & Paczynski 1994; Bartelmann 1996; Chae et al. 1998; Kochanek et al. 2000; Keeton & Madau 2001; Sand et al. 2002; Keeton 2002, 2003; Keeton & Zabludoﬀ 2004; Limousin et al. 2008; Anguita et al. 2009; Zitrin et al. 2011a,b) and the weak regime (Kaiser & Squires 1993; Mellier 1999; Bartelmann & Schneider 2001; Hoekstra et al. 2004; Clowe et al. 2006; Mahdavi et al. 2007; Jee et al. 2009; Huang et al. 2011). Comparing these observations to theoretical models provides key information to help resolve the cusp-core problem. Evidently, one must use the most accurate density pro ﬁle to obtain the best ﬁt to observational data from strong- and weak-lensing studies. Recently, N-body CDM simulations (Navarro et al. 2004; Merritt et al. 2006; Gao et al. 2008; Hayashi & White 2008; Stadel et al. 2009; Navarro et al. 2010) have found that certain three-parameter proﬁles provide an excellent ﬁt to a wide range of dark matter haloes. One of these is the Einasto (1965) proﬁle, a three-dimensional version of the two-dimensional S´ersic (1968) proﬁle used to describe the surface brightness of early-type galaxies and the bulges of spiral galaxies (e.g. Davies et al. 1988; Caon et al. 1993; D’Onofrio et al. 1994; Cellone et al. 1994; Andredakis et al. 1995; Prugniel & Simien 1997; M¨ollenhoﬀ & Heidt 2001; Graham & Guzm´an 2003; Graham et al. 2006; Gadotti 2009). The S´ersic proﬁle can be written as: where R is the distance in the sky plane, m the S´ersic index, Υ is the mass-to-light ratio, Ie is the luminosity density at the eﬀective radius Re, and bm is a dimensionless function of m that can be determined from the condition that the luminosity inside Re equals half of the total luminosity. Numerical solutions for bm are given by Ciotti (1991), Moriondo et al. (1998), Prugniel & Simien (1997) and an asymptotic expansion bm = 2m − 1/3 + 4/405m + O � m2� using analytical methods was obtained by Ciotti & Bertin (1999). The Einasto proﬁle (model) is characterised by a power-law logarithmic slope, with n, which we call the Einasto index, a positive number deﬁning the steepness of the power law. Integrating leads to the gen where rs represents the radius of the sphere that contains half of the total mass, ρs is the mass density at r = rs, and dn is a numerical constant that ensures that rs is indeed the half-mass radius. In the context of dark matter haloes, the density can also be expressed as where ρ−2 and r−2 are the density and radius at which ρ (r) ∝ r−2. In the remainder of this paper, we will use yet another, equivalent version, If a model is to describe real galactic systems, several conditions must be imposed on the model description functions, as discussed by Einasto (1969a). When a model is constructed, an initial descriptive function is chosen; the most practical choice is the density proﬁle, because the main descriptive functions (cumulative mass proﬁle, gravitational potential, surface mass density) are integrals of the density proﬁle. Furthermore, a physical model has to satisfy several conditions: i) the density proﬁle must be non-negative and ﬁnite; ii) the density must be a smoothly decreasing function that approaches zero at large radii; iii) some moments of the mass function must be ﬁnite, in particular moments that deﬁne the central gravitational potential, the total mass, and the eﬀective radius of the system; and iv) the descriptive functions must not have jump discontinuities. Einasto (1969a) presented several families of valid descriptive functions, among which the so-called Einasto proﬁle is a special case that agrees best with observations. The Einasto proﬁle was used by Einasto (1969b) to obtain a model of M31. Later, this model was applied by Einasto (1974) to several nearby galaxies, including M32, M87, Fornax and Sculptor dwarfs, and also M31 and the Milky Way. These models were multi-component ones, and each component has its own parameter set { ρ0, h, n } representing certain physically homogeneous stellar populations. Navarro et al. (2004) found that for haloes with masses from dwarfs to clusters, 4.54 ≲ n ≲ 8.33 with an average value of n = 5.88. Hayashi & White (2008) and Gao et al. (2008) found that n tends to decrease with mass and redshift, with n ∼ 5.88 for galaxy-sized and n ∼ 4.35 for clustersized haloes in the Millennium Run (MR) (Springel et al. 2005). Navarro et al. (2010) obtained similar results for galaxy-sized haloes in the Aquarius simulation (Springel et al. 2008). Also, Gao et al. (2008) showed that n ∼ 3.33 for the most massive haloes of MS. Chemin et al. (2011) modelled the rotation curves of a spiral galaxies subsample from THINGS (The HI Nearby Galaxy Survey, (de Blok et al. 2008)), using the Einasto proﬁle, and found that n tends to have lower values by a factor of two or more compared to those predicted by N-body simulations. Dhar & Williams (2011) ﬁtted the surface brightness proﬁles of a large sample of elliptical galaxies in the Virgo cluster, using a multi-component Einasto proﬁle consisting of two or three components for each galaxy with diﬀerent n for each individual component. For the central component, they found values of n ≲ 1 for the most massive and shallow-cusp galaxies and n < 2 for steep-cusp galaxies, while 5 ≲ n ≲ 8 for the outer component of massive galaxies. These values for the outer component are consistent with the results from N-body simulations for Einasto dark matter haloes, and the authors argued that Einasto components with an n in this range could be dark matter dominated. In the light of its increasing popularity to describe the density of simulated dark matter haloes, a detailed investigation of the properties of the Einasto model is of paramount importance. Some aspects of the Einasto model have been presented by several authors (Mamon & Łokas 2005; Cardone et al. 2005; Merritt et al. 2006; Dhar & Williams 2010). The most complete study of the properties of the Einasto model is the work by Cardone et al. (2005), who provided a set of analytical expressions for quantities such as the mass proﬁle and gravitational potential and discussed the dynamical structure for both isotropic and anisotropic cases. Nevertheless, the Einasto model has not been studied analytically as extensively as the S´ersic models, and several properties still have to be further investigated in more detail. One area where progress is still to be made is on the value of the dimensionless parameter dn. The most important lacuna, however, concerns the surface density on the plane of the sky, an important quantity that deﬁnes the lensing properties of a dark matter halo. The surface mass density has an importance in theoretical predictions and observations. On the observational side, based mostly on the mass decomposition of rotation curves using cored haloes, various studies (Kormendy & Freeman 2004; Spano et al. 2008; Donato et al. 2009; Gentile et al. 2009) have shown that the product of the central density ρ0 and the core radius r0 is consistent with a universal value, independent of galaxy mass. The product ρ0r0 is directly proportional to the average surface density within r0, and to the gravitational acceleration of dark matter at r0. In view of a detailed comparison of these results with the outcome of the most recent dark matter simulations, it is therefore important to study the analytical properties of the surface density distribution of Einasto haloes. Recent works studying theoretical and observational aspects of dark matter surface densities include Boyarsky et al. (2010), Walker et al. (2010), Napolitano et al. (2010), and Cardone et al. (2011): eﬀorts are being made to conﬁrm or call into question the universality of the dark matter surface density within one dark halo scale length. Cardone et al. (2005) showed that the surface density of the Einasto model cannot be expressed in terms of elementary functions and discussed the general properties using numerical integration. Dhar & Williams (2010) presented an analytical approximation for the Einasto surface brightness proﬁle and demonstrated that this surface density proﬁle is not S´ersic-like. A general analytical formula, which would enable an arbitraryprecision calculation and an analytical study of the asymptotic behaviour, is still lacking to date. The most recent studies in strong and weak lensing (e.g. Donnarumma et al. 2011; Sereno & Umetsu 2011; Morandi et al. 2011; Umetsu et al. 2011; Oguri et al. 2011) use the NFW proﬁle or its generalization (Zhao 1996; Jing & Suto 2000) to model the dark matter halo instead of the Einasto proﬁle. One of the factors that limits its adoption is the absence of analytical formulas for its lensing properties. Therefore, its application in lensing studies is not as wide as for other proﬁles. A complete general set of analytical formulas for the lensing properties of the Einasto proﬁle would help to increase its use in lensing studies. In this paper, we extend the analytical study of the Einasto model using some of the techniques also employed in the extensive literature on the analytical properties of the S´ersic model (e.g. Ciotti 1991; Ciotti & Lanzoni 1997; Ciotti & Bertin 1999; Trujillo et al. 2001; Mazure & Capelato 2002; Cardone 2004; Graham & Driver 2005; El´ıasd´ottir & M¨oller 2007; Baes & Gentile 2011; Baes & van Hese 2011). In Section 2, we discuss an analytical expansion for the dimensionless parameter dn. In Section 3, we derive an analytical expression for the Einasto surface mass density in terms of the Fox H function, using the Mellin transform-method. We then use the result for the projected surface mass density to calculate the cumulative surface mass, lens equation, deﬂection angle, deﬂection potential, magniﬁcation, shear and the critical curves for a spherically symmetric lens described by the Einasto proﬁle in terms of this function. We also calculate explicit series expansions for the surface mass density and all the lensing properties. In Section 4, we derive some special cases for the lensing properties for integer and half-integer values of n in terms of the Meijer G function. Furthermore, we compare the Einasto and S´ersic surface mass densities for the same values of their respective indices. Finally, in Section 5 we discuss the asymptotic behaviour of the surface mass density and cumulative surface mass at small and large radii. We present our conclusions in Section 6.
The cavity method: from exact solutions to algorithms<|sep|>The quest of an analytic solution for the simplest mean-ﬁeld spin-glass model (the Sherrington-Kirkpatrick (SK) one [Sherrington and Kirkpatrick (1975)]) led Giorgio Parisi to the invention of the replica method [Parisi (1980)]. This method is able to describe and handle the complicated structure of the conﬁguration space of the SK model, with a hierarchical division of the conﬁgurations into nested pure states, through the analytical parametrization of matrices of size n×n, in the limit where n → 0, which is, to say the least, a questionable mathematical construction (its predictions have been nevertheless conﬁrmed rigorously later on [Guerra and Toninelli (2002); Talagrand (2006); Panchenko (2013)]). In the physics literature an alternative method to solve the SK model was proposed in [M´ezard et al. (1986)], and subsequently dubbed the cavity method. In a nutshell the idea is to consider the eﬀect of the addition of one spin in a large SK model, or equivalently to create a “cavity” by isolating one spin and modeling the inﬂuence that the rest of the system has on it in a self-consistent way. The replica and the cavity methods yield the same predictions for the SK model,
Complex-valued information entropy measure for networks with directed links (digraphs). Application to citations by community agents with opposite opinions<|sep|>”Complicated systems” are usually, but abusively [1], called complex systems. Recall that complex numbers are found in physics to describe various macroscopic properties: the dielectric permittivity, the electric impedance, the amplitudes and phase angles of modal vibrations, Magnetic Resonance images, etc.. Complex eigenvalues (EVs) do naturally occur in a Hamiltonian formalism: the imaginary part of some self-energy which turns out to be the ”density of states”, localization in superconductors [5], dissipation and scattering in Quantum Chaos [6, 7] or Quantum Chromodynamics, with a non-vanishing chemical potential [8], fractional Quantum Hall Eﬀect [9], two-dimensional plasma of charged particles [10, 11]. See also the imaginary part of the ”free energy” measuring the quantum decay rate of a pure state or the imaginary part of power law exponents describing oscillations in the speciﬁc heat or in the electrical resistivity temperature derivative at (magnetic, for example) transitions [3, 4]. It will be argued below that one modern case of interest is the entropy of directed networks: it can have a real and an imaginary part. For coherence, the deﬁnitions of the thermodynamic and the information entropy measure are recalled in Sect. 2. Subsequently the complex information entropy measure (CIE) is deﬁned in order to take into account complex algebra, when or if necessary, - in particular when discrete ”states” are considered. In Sect. 3, a brief outline of some illustrating case is presented. It concerns a citation network, - see Sect. 3.1. The entropy of such digraphs can be complex, following some illustrative calculation. A preliminary interpretation follows in Sect. 3.2. Some conclusion is found in Sect. 4. A discussion on normalization is found in Appendix A. An extension to Tsallis entropy measure is suggested in Appendix B. A long but useful discussion on the origin of complex EV for citation and similar asymmetric networks is found in Appendix C, where the various cases of the most simple asymmetric networks, i.e. triads, described by 3x3 matrices, are used to point to transitivity causes.
Evidence of Particle Acceleration in the Superbubble 30 Doradus C with NuSTAR<|sep|>OB associations typically have tens of massive stars, and the collective eﬀect of their fast stellar winds and supernovae (SNe) create superbubbles (SBs; e.g., Mac Low & McCray 1988; Oey 1996; Yadav et al. 2017). SBs are large (∼100 pc) shell-like structures that sweep up material from the surrounding interstellar medium (ISM), producing tenuous cavities ﬁlled with hot (∼106 K), shock-heated gas (e.g., Castor et al. 1975; Weaver et al. 1977; Chu & Mac Low 1990; Rogers & Pittard 2014). Due to the low densities within these bubbles (nISM ∼ 0.01 cm−3), shock waves travel large distances before substantial deceleration, and thus the timescale of eﬃcient particle acceleration is longer than in the case of individual/isolated supernova remnants (SNRs). Since SBs also have large energy reservoirs, SBs are plausible candidates for sites of cosmic-ray acceleration (e.g., Bykov & Fleishman 1992; Parizot et al. 2004; Butt & Bykov 2008; Ferrand & Marcowith 2010; Bykov 2014). Observational evidence of particle acceleration in SBs is growing. GeV gamma-rays have been detected by the Fermi Gamma-ray Space Telescope toward some SBs (e.g., Abdo et al. 2010), and possible detection of nonthermal X-rays from SBs have been reported from multiple sources (e.g., 30 Doradus C: Bamba et al. 2004; N51D: Cooper et al. 2004; N11: Maddox et al. 2009; IC 131: T¨ullmann et al. 2009; see also the recent review by Kavanagh 2020). However, in some cases, followup work failed to ﬁnd diﬀuse non-thermal X-rays in these sources, suggesting that the previous ﬁndings may be due to inadequate background subtraction or unresolved point sources (e.g., Yamaguchi et al. 2010). Consequently, the detection of diﬀuse, non-thermal X-rays in SBs remains controversial, and additional observational constraints are necessary to elucidate the role of SBs in the particle acceleration process. In this paper, we present hard (>3 keV) X-ray images and spectra from NuSTAR observations of the superbubble 30 Doradus C (hereafter, 30 Dor C) in the Large Magellanic Cloud (LMC). 30 Dor C is a ≈95 pc across SB (Dunne et al. 2001) powered by the OB star association LH 90 (Lucke & Hodge 1970), with 26 O stars and 7 Wolf-Rayet (WR) stars with ages of 3–7 Myr (Testor et al. 1993).
Logistic map with memory from economic model<|sep|>2. Memory effect in economic process  The concept of memory is actively used in econometrics [7, 8]. We consider the concept of
Molecular line mapping of the giant molecular cloud associated with RCW 106 - II. Column density and dynamical state of the clumps<|sep|>Molecular clouds must be supported by turbulent motions in order to resist gravitational collapse and thereby explain the relatively long time-scale for star formation in the Galaxy (Zuckerman & Palmer 1974). Observations of clumpy and ﬁlamentary structures in these clouds, in qualitative agreement with numerical simulations of turbulence, provide further evidence for turbulent support. However, the simulations also indicate that turbulence dissipates quickly (within a few crossing times, e.g. Stone et al. 1998), and must therefore be continually renewed by energy injection. Given that several physical processes, acting on different scales and contributing different amounts of kinetic energy, can contribute to turbulence, the nature of the principal driving source of turbulence in molecular clouds remains a subject of active debate. When studying the turbulent properties of molecular clouds and their possible universality, statistics which distinguish among different spatial scales are generally preferred: these include Fourier power spectra of 2-D velocity slices (Lazarian & Pogosyan 2000), the delta variance (Stutzki et al. 1998), and principal component analysis (Heyer & Schloerb 1997). These techniques take advantage of the self-similar nature of turbulence to reduce its characterization to a small number of quantities, e.g. power-law indices of the energy spectrum or of the correlation between spatial scale and velocity dispersion. A number of studies in recent years have ap
An estimation of the Moon radius by counting craters: a generalization of Monte-Carlo calculation of $\pi $ to spherical geometry<|sep|>The Moon is always a subject of intense debate, for instance, it is the cause of many natural phenomena, the most common of which are solar eclipses and ocean tides. In turn, it is a source for testing theories of gravitation and to investigate geophysical phenomenas ([1] and [2]). Moreover, there are several relational properties between the Moon and the Earth, such as the instantaneous distance between them or the relative rotation, as well as intrinsic properties of the Moon, such as the Moon diameter, its excentricity or the more perplexing long-standing puzzle of its origin, that requires sophisticated and non-sophisticated experimental procedures in order to obtain approximate values or conceptual explanations (see for instance [3], [4], [5],[6], [7] and [8]). In turn, the Moon is suitable to desing simple experimental procedures to obtain accurate information of diﬀerent parameters of it ([9]), or for example by measuring the Moon’s orbit by using a hand-held camera (see [10]). In the same line of thought, this work introduces a novel procedure to estimate the Moon’s radius without using any sophisticated measuring device. Brieﬂy, the method consists in using the Monte-Carlo method to obtain π using random points (see [11] and [12]) generalized to spherical geometry.1 In this manuscript, the sphere will be the Moon and the random points will be the craters on it. The quarter of circle inscribed in a square on the plane, where the random points are located and which is used in the typical Monte-Carlo ∗email: jsardenghi@gmail.com, fax number: +54-291-4595142 1In [12], page 302 there is a plot of π in a sphere, where the value is computed with the ratio of the cimcurference of a circle to its diameter, measured on the curved surface of the sphere. method, is generalized to a quarter of circle inscribed in a spherical square over the surface of a sphere. This mathematical generalization of the Monte-Carlo method to spherical geometry is very simple and implies a derivation of areas of circles and squares drawn over a spherical surface. Should be expected that when the radius of the spherical surface tends to inﬁnity, the Monte-Carlo method gives π because spherical geometry in this limit tends to ﬂat geometry. This leads to the following conclusion: by counting random points over quarter of circles and squares in spherical geometry we obtain deviations from π by computing the areas ratio and these deviations are a function of the radius of the sphere. Interestingly, we can apply the Monte-Carlo method over a quarter of circle in the spherical surface without knowing the sphere radius but it can be obtained by counting random points on it. Of course, to do so we must know the function that relates the deviation from π with the spherical surface radius. Using the craters of the Moon as a random points in a particular spherical square inscribed on it, we can obtain the deviation from π and by using the theoretical function that relates the deviation from π with the Moon’s radius, we can deduce the Moon’s radius with an accuracy related to the number of craters considered. Should be stressed that altough the conceptual procedure is simple, spherical geometry introduce subtetlies concerned with circle and squares inscribed over the surface of a sphere that must be taken into account in order to obtain the correct limit when the ﬂat geometry is recovered, which can be implemented easily by taking the inﬁnite limit of the sphere radius. Then, in order to be consise and self-contained, this mansuscript will be organized as follows: In Section II, the function that relates the deviation from π in the Monte-Carlo method applied to spherical geometry and the spherical surface radius is obtained. In Section III we used the results obtained in Section II to determine the Moon’s radius with the respective error by considering random craters in a particular zone of the Moon surface. In last section, the conclusions are presented and in Appendix some mathematical details are shown.
Utilizing UNet for the future traffic map prediction task Traffic4cast challenge 2020<|sep|>challenge [1] following Traffic4cast 2019’ [2, 3]. Challenge’s task is to predict future  traffic flow volume, heading and speed on a high resolution map of three large cities  worldwide. Contrary to last year’s challenge task which was to predict the next fifteen  minutes at maximum time distance, this year’s task gets more challenging because we  need to predict more distant future traffic maps up to one hour later. task, we tried to experiment with a more diverse set of neural net structures and combined them to improve performance beyond any single model can achieve.
Interference Effects in Quantum Belief Networks<|sep|>The problem of violations of the axioms of probability go back to the early 60s. [32] published a work that inﬂuenced modern psychology by showing that humans violate the laws of probability theory when making decisions under risk. The principle that humans were constantly violating is deﬁned by The Sure Thing Principle. It is a concept widely used in game theory and was originally introduced by [64]. This principle is fundamental in Bayesian probability theory and states that if one prefers action A over B under state of the world X, and if one also prefers A over B under the complementary state of the world X, then one should always prefer action A over B even when the state of the world is unspeciﬁed. Cognitive psychologists A. Tversky and D. Khamenman also explored more situations where classical probability theory could not be accommodated in human decisions. In their pioneering work, [69] realised that the beliefs expressed by humans could not follow the rules of Boolean logic or classical probability theory, because humans cannot process large amounts of data in order to make estimations or judgements. Consequently, the inferences performed are based on limited data coupled with several heuristics, leading to a violation on one of the most important laws in bayesian theory: the law of total probability. One of the key differences between classical and quantum theories is the way how information is processed. According to classical decision making, a person changes beliefs at each moment in time, but it can only be in one precise state with respect to some judgement. So, at each moment, a person is favouring a speciﬁc belief. The process of human inference deterministically either jumps between deﬁnite states or stays in a single deﬁnite state across time [17]. Most computer science, cognitive and decision systems are modelled according to this single path trajectory principle. Figure 1 illustrates this idea. Figure 1: Example of how information is processed in a classical setting. At each time, beliefs can only be in one deﬁnite state. In quantum information processing, on the other hand, information (and consequently beliefs) are modelled via wave functions and therefore they cannot be in deﬁnite states. Instead, they are in an indeﬁnite quantum state called the superposition state. That is, all beliefs are occurring on the human mind at the same time. According to cognitive scientists, this effect is responsible for making people experience uncertainties, ambiguities or even confusion before making a decision. At each moment, one belief can be more favoured than another, but all beliefs are available at the same time. In this sense, quantum theory enables the modelling of the cognitive system as it was a wave moving across time over a state space until a ﬁnal decision is made. From this superposed state, uncertainty can produce different waves coming from opposite directions that can crash into each other, causing an interference distribution. This phenomena can never be obtained in a classical setting. Figure 2 exempliﬁes this. When the ﬁnal decision is made, then there is no more uncertainty. The wave collapses into a deﬁnite state. Thus, quantum information processing deals with both deﬁnite and indeﬁnite states [17]. Figure 2: In human decision making, believes occur in the human mind at the same time, leading to uncertainty feelings and ambiguity. Beliefs can be represented in superposition states that can generate interferences between them.
A Lagrangian model of copepod dynamics: Clustering by escape jumps in turbulence<|sep|>The study of swimming microorganisms and their interaction with ﬂuid ﬂows has attracted enormous attention in the last decade. A line of research has focused on characterizing individual swimming strategies by means of experiments [1–3] as well as by theoretical and numerical modelling [4, 5]. A second direction of study devoted to the consequences of swimming on population dynamics, e.g., by focusing on encounter rates and other collective behaviours [6–10]. A third direction focused on the mutual interactions of microorganisms with the ﬂuid ﬂow environment, in particular bio-induced ﬂow ﬂuctuations, sometimes dubbed as bacterial turbulence [11– 13], or, vice-versa, on active matter clustering induced by non homogeneous ﬂows or ﬂuid turbulence [14–22]. The present study will focus on this latter aspect, in particular on copepod’s dynamics in turbulent ﬂow. Copepods are the most diversiﬁed crustaceans in the aquatic environment whose length ranges from 0.1 mm to few millimetres. They are important to global ecology and to the carbon cycle [23] (see also [24] & [25]). Although copepods are not at the top of the food web, they have a major role in the marine ecosystem because they are the secondary producers in the ecological food web linking phytoplankton cells (the primary producers) to ﬁsh larvae and even to large mammals such as whales. Copepods also consume the mosquito larvae, acting as control mechanism for malaria [26]. They are of great importance in ﬁshery industry. A central issue in breeding ﬁsh species, is the external food supply. Most ﬁshes prefer copepods to other zooplankton species (i.e. rotifers) and they grow bigger in shorter time when eating copepods [27, 28]. Living in a ﬂuid environment characterised by bodyscale Reynolds number up to 1000, they are subjected to the physics of the ﬂow ﬁeld both in viscous and inertial regime [29]. Copepods typically have a short, cylindrical body with antennas, few pairs of swimming legs and tales. Using their antennas, copepods can sense the disturbance, which is caused either by the presence of predators or by high turbulent regions in the ﬂow. Kiørboe et al. [30, 31] performed series of experiments, investigating the eﬀect of non uniform ﬂow motion on copepods. In order to ﬁnd the component of the ﬂow which copepods react the most to, the copepods were put into a time dependent siphon ﬂow (which ideally generates a pure longitudinal deformation rate), in an oscillating chamber where copepods experience only acceleration, in a couette device producing shear deformation, and ﬁnally in a rotating cylinder where acceleration and vorticity are both present. The conclusion of this study was that these small crustaceans react to the ﬂow deformation rate. Kiørboe also reported [32], that there are two threshold values of the deformation rate: the upper one, around 10 s−1, corresponds to either the presence of a predators or to a region where turbulence intensity is high, and the lower one, 1 s−1, corresponds to regions in the ﬂow where turbulence intensity is lower or food abundance is not enough for copepods. These tiny crustaceans ﬁnd themselves at ease in regions in between these two thresholds. To avoid uncomfortable regions, copepods exhibit a rapid escape in the ﬂow which is often dubbed a jump. Buskey et al. [33, 34] showed that copepod’s velocity can reach the rate of 500 body length per second (0.5 m/s) while jumping. The mechanical energy produced during their escape is reported to be very high (8 × 10−5 J/s) [35], which makes copepods, relative to their size, among the fastest and the strongest animals in the world. Buskey [33, 34] also reported that males and females respond diﬀerently to hydrodynamic stimulus in terms of response latency, jump speed, number of thrusts, distance jumped and many other parameters. According to their investigations, copepods jump in an unpredictable direction, but rarely in the backward direction of their motion. Other studies have considered the mating behaviour of copepods [36] and the eﬀect of salinity on copepod’s dynamics and copepod’s encounter rate [37, 38]. Copepods are also sensitive to light stimuli, being attracted by natural light sources [39]. In the last two decades many studies have been conducted to quantify the dynamics of copepods. Most of them focused on their behaviour in still water [36–38], while less studies have studied the dynamics in their natural living environment because of the diﬃculties of such experimental investigations. Few works have been devoted to the dynamics of copepods in turbulent ﬂows [40–44]. However, the densities of copepods used in these studies are often lower than the maximum densities that can be encountered in the ﬁeld. The numerical simulation can provide a tool that integrate our current knowledge on copepod dynamics and use high number of individuals. The objective of the present study is to simulate copepods numerically in turbulence to characterise their dynamics induced by a behaviour model. To achieve this goal, our strategy is two-fold: on one hand, new experimental measurements and observations available in the literature [45–51], along with the aforementioned copepods properties, should be considered in details in order to introduce a realistic and physical model. On the other hand, fundamental knowledge on simulation of particles in turbulent ﬂows, available in numerical and experimental studies on particles in turbulence [52–56], is needed to couple the physics and biology in the numerical model. The paper is organised as follows: section II describes the experimental framework used to stimulate copepods. We then analyse copepod’s trajectories to introduce a model equation describing copepods behaviour. Furthermore similarity analysis is performed to tune the LC model and its numerical implementation is explained at the end of this section. Section III details the single point statistics, fractal dimension and orientation dynamics of copepods. The paper ends with conclusion and outlook on future
Mitigation of Civilian-to-Military Interference in DSRC for Urban Operations<|sep|>Year 2014 marked the ﬁrst year in which over 50% of the world’s population lived in an urban area, and by 2050, the proportion is projected to approach 70% [1]. As these numbers climb, so does the probability that the U.S. military will be called to operate in a dense urban environment for operations ranging from disaster relief to counterinsurgency and beyond. The recent history of U.S. military humanitarian assistance/disaster relief (HA/DR) response is indicative of the Army’s likely operational tempo for the near future. From the disaster response to Haiti in 2010 to Puerto Rico in 2017, there has been no shortage of catastrophes requiring military assistance domestically and abroad. Technologies are rapidly changing and are essential for intelligence gathering and processing (e.g., command, control, communications, computers, intelligence, surveillance and reconnaissance (C4ISR) applications) [2]. Speciﬁcally, wireless communications among the military vehicles play a pivotal role in urban operations. The reason is that urban operations pose distinct challenges to effective utilization of American military power: a high density of social and organizational ties that are difﬁcult for outsiders to understand; complex, civilian-ﬁlled terrain that extends in three dimensions; opaque networks of formal and informal institutions; and an even more severe than usual overload of information and accompanying difﬁculties separating signal and noise.
Triple collinear emissions in parton showers<|sep|>Parton showers solve the leading-order DGLAP equations [1–4] using Markovian Monte-Carlo algorithms [5]. As such they work at much lower computational precision than many other calculational tools used in high-energy physics to date [6]. Due to their importance for both experimental analyses and phenomenological surveys, a limited set of the most important higher-order eﬀects has been included into parton showers over time, such as angular ordering [7], and soft-gluon enhancement [8]. The numerical size of the remaining theoretical uncertainties is unclear, especially since parton showers are tuned to match the most relevant experimental observables. The net eﬀect of this tuning is that their predictions are most often accurate, yet imprecise, and that the level of imprecision is diﬃcult to quantify numerically. As fully exclusive, high precision simulations are mandatory in order to perform reliable measurements of Standard Model parameters and/or searches for physics beyond the Standard Model, the extension of parton showers to higher formal accuracy would beneﬁt large parts of the high-energy physics community. The possibility of including next-to-leading order corrections into parton showers has been explored early on [9– 12] and was revisited recently [13, 14]. NLO splitting functions have been recomputed using a novel regularization scheme [15, 16]. The dependence of NLO matching terms on the parton-shower evolution scheme has been investigated in detail [17]. In addition, the ﬁrst solutions to incorporate eﬀects beyond the leading-color approximation into parton showers have been found [18, 19], and threshold logarithms have been included in a fully automated approach [20]. In this publication, we construct a framework for the simulation of triple-collinear parton splittings, which contribute to the next-to-leading order corrections to DGLAP evolution [21–24]. Triple-collinear splitting functions have been known since long [25], but they have not been included into parton showers to date1. We start with the simplest case of the ﬂavor-changing splitting kernels. We use these 1 → 3 kernels to recompute the timelike and spacelike NLO splitting functions Pqq′ in the MS scheme, and we show how the result can be implemented straightforwardly in its diﬀerential form in a Markovian Monte-Carlo simulation, such that the integral matches Pqq′ up to momentum conserving eﬀects. Our algorithm depends crucially on the usage of a weighted parton shower, a technique that was presented in [26, 27]. We see an opportunity to extend our new method to more complicated triple-collinear splitting functions, and to include virtual corrections, such that all NLO kernels may eventually be calculated on-the-ﬂy, similar to the computation of a ﬁxed-order result in the dipole subtraction method [28]. The outline of this publication is as follows: Sec. II highlights the correspondence between the formalisms for parton-shower evolution and DGLAP evolution. The main components of parton-showers are the splitting kernels and the kinematics mapping, which deﬁne the probability and kinematics in the transition from an n-parton ﬁnal state to an n+1-parton ﬁnal state. Section III therefore presents the recomputation of the timelike and spacelike NLO splitting kernels Pqq′ and, based on the individual terms identiﬁed in the analytical calculation, the construction of a formalism to include 1 → 3 branchings in the parton shower. We present a validation of our numerical implementation and a test of the numerical impact of q → q′ and q → ¯q splittings in Sec. IV. The kinematical mappings introduced to simulate 1 → 3 splittings are an integral part of the new algorithm, but their presentation is rather technical and has therefore been included in App. A. Section V contains some concluding remarks.
Anisotropic CR diffusion and gamma-ray production close to supernova remnants, with an application to W28<|sep|>Galactic Cosmic Rays (CRs) are mainly constituted by relativistic protons and are believed to be accelerated at SuperNova Remnant (SNR) shocks via ﬁrst order Fermi mechanism (Hillas 2005). Though very popular, this scenario still needs to be conclusively proven by observations. If CRs are indeed accelerated at SNRs, these objects must be gamma–ray sources. This is because the CRs accelerated at the shock undergo inelastic proton–proton interactions with the ambient medium and produce neutral pions which in turn decay into gamma rays (Drury et al. 1994; Naito & Takahara 1994). Several SNRs have been detected in gamma rays at both TeV (e.g. Hinton & Hofmann 2009) and GeV (e.g. Giordano 2011) energies, in agreement with such expectations. However, it is often diﬃcult to determine whether the origin of the gamma–ray emission is hadronic, and thus related to the acceleration of CRs, or due to leptonic mechanisms such as inverse Compton scattering. For this reason, multi-wavelength studies of SNRs have been extensively carried out in an attempt to solve this degeneracy. Though for some individual SNRs it has been possible to ascribe the gamma–ray emission exclusively and quite conﬁdently to hadronic (e.g. Acciari et al. 2011; Morlino & Caprioli 2012) or leptonic (e.g. Abdo et al 2011; Ellison et al 2010) processes, in other cases this ambiguity remains a problem. An alternative way to reveal the presence of a CR source is by searching for the radiation produced by CRs that escape the acceleration site (Aharonian & Atoyan 1996; Gabici & Aharonian 2007; Rodriguez Marrero et al. 2008; Gabici et al. 2009). At some stage of the dynamical evolution of the SNR, CRs are expected to leave the shock region and escape into the interstellar medium. The details of the escape mechanism are still not very well understood (see e.g. Gabici 2011, and references therein), but it is generally believed that the ability of a SNR in conﬁning particles decreases gradually with the shock speed, with higher
SelectVisAR: Selective Visualisation of Virtual Environments in Augmented Reality<|sep|>immerse themselves in a Virtual Environment (VE). A see-through AR device can overlay virtual content on top of the physical environment. VR and AR users can interact with each other through Collaborative VEs [23], and the interaction between a VR and AR user is considered a type of "Cross-Reality Interaction". Cross-Reality (CR) is a field of research that looks at how users of different realities can interact with each other. These realities can be described through Milgram’s Reality-Virtuality continuum [13, 16], which ranges from the real world to the virtual world and the spectrum of hybrid realities in between. This paper will focus on scenarios that involve interactions between two different points in this continuum: AR and VR. This type of scenario can be beneficial when the roles of the AR and VR user in the collaboration are asymmetrical. It is important to note the differences in how users perceive their VEs: VR users benefit from more immersion and AR users benefit from more nonverbal cues. In a CR context, nonverbal cues refer to the advantage AR users have over VR users when communicating with an external user — For instance, both VR and AR users can talk to an external user, but only AR users can see the physical body and gestures of an external user in real life. The VR user cannot see the external user, and at most can only perceive the external user’s virtual avatar. As such, AR retains most nonverbal cues lost to VR users. In contrast, while VR users retain high immersion, AR users will be less immersed in the VE due to their vision of the physical environment. For some users VR might be more desirable, such as for a training simulation where the user needs to have a sense of being at the place of the training. Other users can benefit from AR to enable them to see nonverbal cues of other co-located users. In this study, we aim to develop a CR scenario that exploits both the immersive benefits of VR and the nonverbal communication features of AR. We question how an AR user can spectate and interact with the VR user in their VE [6]. We propose a selective visualisation system that enables AR users to only see select virtual elements of the VE whilst VR users see the entire VE to maintain their immersion. We investigated two design factors in terms of visualising a VE: level of information and scale. We designed a framework where we presented the VE to the AR user at a 1:5 dollhouse-scale and at 1:1 room-scale with three levels of information: no selection, a dynamic selection following the VR user, and a predetermined static selection. This study was then repeated with two improved dynamic and static selection techniques implementing feedback from the main study. In our studies we found that participants felt they had a better overview of the VE at the small 1:5 dollhouse-scale; however, this had the drawback that it was more difficult to see smaller movements of the VR user. We found that our dynamic selection methods were preferred by fewer participants than the static selections. No significant differences were found in participant competence in recognising events in the VE.
Almost-Orthogonal Layers for Efficient General-Purpose Lipschitz Networks<|sep|>Deep networks are often the undisputed state of the art when it comes to solving computer vision tasks with high accuracy. However, the resulting systems tend to be not very robust, e.g., against small changes in the input data. This makes them untrustworthy for safety-critical high-stakes tasks, such as autonomous driving or medical diagnosis. A typical example of this phenomenon are adversarial examples [1]: imperceptibly small changes to an image can drastically change the outputs of a deep learning classiﬁer when chosen in an adversarial way. Since their discovery, numerous methods were developed to make networks more robust against adversarial examples. However, in response a comparable number of new attack forms were found, leading to an ongoing cat-and-mouse game. For surveys on the state of research, see, e.g., [2, 3, 4]. A more principled alternative is to create deep networks that are robust by design, for example, by restricting the class of functions they can represent. Speciﬁcally, if one can ensure that a network has a small Lipschitz constant, then one knows that small changes to the input data will not result in large changes to the output, even if the changes are chosen adversarially. A number of methods for designing such Lipschitz networks have been proposed in the literature, which we discuss in Section 3. However, all of them have individual limitations. In this work, we introduce the AOL (for almost-orthogonal Lipschitz) method. It is the ﬁrst method for constructing Lipschitz networks that simultaneously meets all of the following desirable criteria: Generality. AOL is applicable to a wide range of network architectures, in particular most kinds of fully-connected and convolutional layers. In contrast, many recent methods work only for a restricted set of layer types, such as only fully-connected layers or only convolutional layers with non-overlapping receptive ﬁelds. Formal guarantees. AOL provably guarantees a Lipschitz constant 1. This is in contrast to methods that only encourage small Lipschitz constants, e.g., by regularization. Efﬁciency. AOL causes only a small computational overhead at training time and none at all at prediction time. This is in contrast to methods that embed expensive iterative operations such as matrix orthogonalization or inversion steps into the network layers. Modularity. AOL can be treated as a black-box module and combined with arbitrary training objective functions and optimizers. This is in contrast to methods that achieve the Lipschitz property only when combined with, e.g., speciﬁc loss-rescaling or projection steps during training. AOL’s name stems from the fact that the weight matrices it learns are approximately orthogonal. In contrast to prior work, this property is not enforced explicitly, which would incur a computational cost. Instead, almost-orthogonal weight matrices emerge organically during network training. The reason is that AOL’s rescaling step relies on an upper bound to the Lipschitz constant that is tight for parameter matrices with orthogonal columns. During training, matrices without that property are put at the disadvantage of resulting in outputs of smaller dynamic range. As a consequence, orthogonal matrices are able to achieve smaller values of the loss and are therefore preferred by the optimizer.
Clustering of Galaxies with Dynamical Dark Energy<|sep|>Accelerated expansion of the Universe is established by the observations of the Type Ia Supernovae (SNeIa) (Riess et al. (2004)) and the Cosmic Microwave Background (CMB) radiation anisotropies (Jarosik et al. (2011)). This accelerated expansion of the Universe can be explained by dark energy. In that case, there are several models to describe dark energy like Λ-CDM which tells that the cosmological constant Λ plays the role of the dark energy (Bahcall et al. (1999)). However, this model has some famous ambiguities such as the ﬁne-tuning (Copeland et al. (2006)), and the cosmic coincidence problems (Nobbenhuis (2006)). Also, it is not a dynamical model of dark energy and cannot show evolution of dark side of the Universe. But, there are some dynamical model of dark energy like phantom (Caldwell (2002)) quintessence (Wetterich (1998)), K-essence (Armendariz-Picon et al. (2000)) and tachyonic models (Sen (2005)), which are based on scalar ﬁelds. Also, there is an exotic ﬂuid with a special equation of state called as Chaplygin gas (Kamenshchik et al. (2001); Bento et al. (2002)) and its generalizations (Bilic et al. (2002)), modiﬁcations (Debnath et al. (2004); Saadat & Pourhassan (2013)), and extensions (Kahya et al. (2015); Pourhassan (2016)) which play role of the dark energy and dark matter, and emerged initially in cosmology from string theory point of view (Barrow (1986); Barrow (1988)). There are also some proposals of the holographic dark energy (Li & Wang (2010); Elizalde et al. (2005)). f(R) theories of gravity are alternative to dark energy to explain accelerating expansion of the Universe (Capozziello (2002); Capozziello et al. (2003); Carroll et al. (2004); Khurshudyan et al. (2015)). In order to have a dynamical model of dark energy, it is also possible to consider varying Λ (Kahya et al. (2015); Khurshudyan et al. (2015); Jamil et al. (2009)). In this paper, we will analyze the eﬀect of time-dependent cosmological constant on the clustering of galaxies. It is known that the inter-galactic distances are many orders of magnitudes greater than the length scale of a galaxy. Thus, it possible to represent a system of galaxies as a system of point particles, and analyze this system using standard methods of statistical mechanics (Ahmad et al. (2002)). It
Min-Max Regret Problems with Ellipsoidal Uncertainty Sets<|sep|>where the objective vector c is unknown, and coming from a set U of possible realizations. To ﬁnd a solution x that still performs well under all possible outcomes of c, several robust optimization approaches have been developed (for an overview, we refer to [GS16, BBC11, CG16]). In this paper, we focus on the minmax regret approach, which is amongst the best-established methods in robust optimization [IS95, KY97, ABV09]. The basic idea is to ﬁnd a solution that minimizes the largest diﬀerence to the optimal objective value in each scenario. More formally, we use a robust objective function of the form ∗Eﬀort sponsored by the Air Force Oﬃce of Scientiﬁc Research, Air Force Material Command, USAF, under grant number FA8655-13-1-3066. The U.S Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright notation thereon. †Email: chassein@mathematik.uni-kl.de ‡Corresponding author. Email: m.goerigk@lancaster.ac.uk with opt(c) being the optimal objective value of the original problem with objective function c, and aim at solving the minmax regret problem This problem has been extensively analyzed for ﬁnite and hyperbox uncertainty sets. Most minmax regret problems of this kind are NP-hard, see., e.g., [AL05, ABV05, Ave01] and the overview in [ABV09]. Therefore, both approximation algorithms and heuristic algorithms without performance guarantees have been suggested. [KZ06] showed that solving the midpoint scenario of an interval uncertainty set gives a 2-approximation for minmax regret combinatorial optimization problems. This was further extended in [Con12] to symmetry points of general uncertainty sets. In [CG15], an a-posteriori bound for the midpoint solution was presented, which can be used in a branch-and-bound algorithm. [MG05] developed a branch-and-bound algorithm for robust spanning trees. For the same problem, also a scenario relaxation procedure was presented in [PGAMCVT14]. The basic idea of scenario relaxation is to begin with a ﬁnite subset of scenarios, instead of the whole interval set. Then, worst-case scenarios are iteratively added to the scenario set, until the objective value of this relaxation coincides with the actual objective value of the regret problem with intervals. Quite surprisingly, little attention has been paid to uncertainty sets that are not ﬁnite or hyperboxes. It seems that this is at odds with the development of other approaches to robust optimization, where the use of more sophisticated sets has been of primary importance. We mention ellipsoidal uncertainty sets (see [BTGN09]) and Γ-uncertainty sets (see [BS04]) as the most prominent examples. There are several reasons to use ellipsoidal uncertainty sets in robust optimization. First, they give good tractability results for other robust optimization approaches. So far, this question is open for minmax regret. Second, they are ﬂexible, as the generalized ∩-ellipsoidal uncertainty introduced in [BTN98] even incorporates ﬁnite (via their convex hull) and interval sets. Third, they are well-motivated from a stochastic setting, where they naturally occur when a normal distribution is cut oﬀ at a certain level of probability. In this paper we consider ellipsoidal uncertainty sets in minmax regret problems. To the best of our knowledge, there is only one previous paper that also considers this type of problem [TTT10]. There, the authors consider uncertain convex quadratic problems and present a relaxation heuristic with probability guarantees. In this paper, we focus on combinatorial problems, complexity results and exact solution algorithms. In Section 2 we present complexity results for the unconstrained combinatorial problem, and for the shortest path problem. While the unconstrained problem with ﬁnite sets is NP-hard to solve, and the regret objective value of a candidate solution can be computed in polynomial time, we ﬁnd the surprising result that the reverse holds true for axis-parallel ellipsoids: While it is NP-hard to compute the regret objective of one candidate solution, the optimal solution of the problem can be found in polynomial time. In Section 3, we discuss two diﬀerent ways to reformulate the minmax regret problem via a scenario relaxation procedure, resulting in exact, general solution approaches. These algorithms are compared in computational experiments in Section 4. Final conclusions are drawn and further research directions are posted in Section 5.
Multi-Tubal Rank of Third Order Tensor and Related Low Rank Tensor Completion Problem<|sep|>A tensor is a multidimensional array, and an Nth-order tensor is an element of the tensor product space of N vectors, which have their own dimensions [17]. Tensors, as higher order generalizations of vectors and matrices, have wide applications in various ﬁelds [4, 7, 8, 13, 19, 20, 22, 26, 30, 36]. Tensor decompositions, various generalizations of matrix singular value decomposition, have attracted more and more attentions, including CANDECOMP/PARAFAC (CP) decomposition [6, 14], Tucker decomposition [34] and tensor singular value decomposition (SVD) [9, 11, 15, 16, 28]. Corresponding to such tensor decompositions, ∗School of Mathematics, Tianjin University, Tianjin 300354, P.R. China. E-mail: QuanYu527@163.com. †School of Mathematics, Tianjin University, Tianjin 300354, P.R. China. E-mail: xzzhang@tju.edu.cn. Her work is supported by NSFC (11871369). ‡School of Mathematics, Tianjin University, Tianjin 300354, P.R. China. E-mail: huangzhenghai@tju.edu.cn. His work is supported by NSFC (11871051). Third order tensors are widely used in chemometrics [29, 32], psychometrics [18] and image inpainting [5, 23, 25, 42]. Unless otherwise specialized, tensors in this paper are of third order. For a third order (n1, n2, n3)-dimensional tensor X ∈ Rn1×n2×n3, the CP decomposition is to decompose X as a sum of some outer products of three vectors: where the symbol “◦” denotes the outer product and a(i) j ∈ Rnj is a vector (i ∈ {1, 2, . . . , r} and j ∈ {1, 2, 3}). The smallest r in CP decomposition is called CP rank of X. From [10], it is NP-hard to determine the CP rank. Compared with CP rank, Tucker rank is easy to compute, and hence most of low rank tensor completion and recovery models are based on Tucker rank. Precisely, Tucker rank is a vector of the matrix ranks where C(1) ∈ Rn1×(n2n3) (C(2) ∈ Rn2×(n1n3) and C(3) ∈ Rn3×(n1n2)) is mode-1 (mode-2 and mode-3, respectively) matricization of tensor. More recently, Kilmer et al. [15] introduced tensor-tensor product (t-product) and tensor singular value decomposition (t-SVD). Based on these deﬁnitions, tubal rank was introduced and studied in [15, 16, 28]. The low rank tensor completion problem is to ﬁnd a low rank tensor from observed incomplete data, which arises from various ﬁelds including internet traﬃc recovery [1, 2, 33, 41], image and video inpainting [12, 21, 22, 42]. Low rank tensor completion is modeled as where rank(·) is a tensor rank and Ω is an index set locating the observed data. PΩ is a linear operator that extracts the entries in Ω and ﬁlls the entries not in Ω with zeros, and M is a given tensor. Diﬀerent tensor ranks lead to diﬀerent low rank tensor completion models of (1) with diﬀerent methods. The following low Tucker rank tensor completion is considered Note that problem (2) is non-convex since matrix rank function is nonconvex. To solve (2), the convex optimization problem is considered as In general, SVD is needed in each iteration of numerical methods for (3), which leads to high computational cost. To lower the computational cost, a matrix factorization method was considered by Xu et al. This method has been widely used in various areas [24]. As pointed in [15, 16, 28], unfolding a tensor directly will destroy the original multi-way structure of the data, which leads to vital information loss and degraded performance. Note that the sizes of C(i), i = 1, 2, 3 in (4) are the same as C in principle, which makes it diﬃcult to lower the computational eﬀorts. where “∗” denotes the t-product. By analysis in [15, 16, 28, 42], the t-product can be computed by some block diagonal matrices of smaller sizes, which makes a signiﬁcant reduction of computational cost. Later, a corrected tensor nuclear norm minimization method was proposed in [39] for noisy observations. It is valuable to mention that only one mode is considered in tubal rank and the other two modes are ignored. That is, low rank structure on the other two modes is missing. Motivated by this, we introduce a vector of tubal ranks on three diﬀerent modes, called multi-tubal rank, which is similar to Tucker rank. Then a relationship between multi-tubal rank and Tucker rank is established. Based on the new introduced multi-tubal rank, a new tensor completion model is proposed. Similar to TCTF in [42], a tensor factorization based method is applied to solve the proposed model. In video and internet traﬃc tensor completion, spatio-temporal characteristics are intrinsic features. To make full use of such features, we improve the proposed low multi-tubal rank tensor completion model, and then apply tensor factorization based method for the improved model. To the best of authors’ knowledge, this paper is the ﬁrst one to introduce multi-tubal rank, to present the relationship between tubal rank and Tucker rank and to introduce the spatio-temporal characteristics to recover video data. The reported numerical examples show that our results have less relative error and higher peak signal-to-noise ratio (PSNR) within less computational time than those of some existing methods. That is, our models and methods outperform the existing methods. The paper is organized as follows. Section 2 introduces the multi-tubal rank of a third order tensor with motivation in both theory and application. In Section 3, a new model of tensor completion based on the multi-tubal rank is proposed and tensor factorization based method is applied with its corresponding convergence analysis. In Section 4, the tensor completion model is modiﬁed to tensor data with some characteristics when the involved data have spatio-temporal characteristics. For this improvement, tensor factorization based method is also modiﬁed. Finally, some numerical results on colorful image recovery, gray video recovery and internet traﬃc data recovery are reported, which show the eﬃciency of the proposed methods.
A parsec-scale outflow from the luminous YSO IRAS 17527-2439<|sep|>Low- and intermediate-mass stars are known to form by gravitational collapse and subsequent accretion of their parent molecular clouds, and driving collimated outﬂows. However, the main mechanism leading to the formation of massive stars is debated as to whether it is either disk accretion similar to that for lower mass stars (e.g. Yorke & Sonnhalter 2002) or a merger of lowermass stars (e.g. Bonnell, Bate & Zinnecker 1998). Many of the recent CO line surveys show outﬂows from massive YSOs (e.g. Zhang et al. 2005; Beuther et al. 2002). Several massive YSO outﬂows have been observed in the near-IR, where the spatial resolution is better than that in single-dish CO line observations. A recent near-IR imaging survey by Varricatt et al. (2010) shows that massive stars up to at least late-O spectral types form primarily by disk accretion. The case of a luminous YSO taking birth by accretion is presented in this paper. IRAS 17527-2439 (hereafter IRAS 17527) is a luminous YSO located in a dark cloud situated close to the Galactic plane (l = 4.8273◦, b = 0.2297◦) in the Ophiuchus region. It is associated with emission from dense gas and dust typical of massive YSOs. Molinari et al. (1996) detected NH3 emission lines from this region. From the radial velocity of the NH3 lines (VLS R=13.3 km s−1), they estimated a kinematic distance of 3.23 kpc, with the distance ambiguity resolved. Based on the IRAS colours they classiﬁed IRAS 17527 as a “high” source, which is possibly in a UCHii phase. (Note that at 12 µm, the IRAS catalogue gives only an upper limit ﬂux for this source). In their 97.981-GHz CS(2-1) survey of UCHii candidates, Bronfman, Nyman & May (1996) detected IRAS 17527 at VLS R=13.5 km s−1, similar to the velocity at which Molinari et al. (1996) detected NH3 emission. Massive star formation is often associated with H2O, CH3OH and OH maser emission. Palla et al. (1991) detected H2O maser emission from IRAS 17527. From the radial velocity of the maser (Vpeak=-1.81 km s−1) they estimated a kinematic distance of 17.9 kpc, which is quite diﬀerent from the distance estimated from radial velocity of the dense gas tracers NH3 and CS. In massive YSOs, H2O maser is often considered to be excited in jets (e.g. Felli, Palagi & Tofani 1992; Goddi et al. 2005). The blueshift of the wavelength of the maser emission in this region with respect to the velocity of the dense gas tracers indicates that H2O maser near IRAS 17527 also may be excited by a jet. Hence we adopt the distance estimate of 3.23 kpc (Molinari et al. 1996) for the calculations in this paper. Surveys by van der Walt, Gaylard & MacLeod (1995), Walsh et al. (1997) and Slysh et al. (1999) did not detect any 6.7 GHz Class-II methanol maser from this region. Edris, Fuller & Cohen (2007) detected faint (0.4 Jy) OH maser emission at Vpeak=11.53 km s−1, located ∼6′ NW of IRAS 17527. Nevertheless, this oﬀset is less than their beam size. Therefore it remains to be investigated at better spatial resolution. The faintness of the detected OH maser is consistent with their observation that the OH masers associated with younger sources are weaker than the ones associated with Hii or UCHii regions. The VLA survey of Hughes & MacLeod (1994) detected 6cm emission from IRAS 17527 with a peak ﬂux of 5.3 mJy. The source was however rejected as a UCHii candidate because the radio emission was diﬀuse.
Countering Misinformation on Social Networks Using Graph Alterations<|sep|>Be it a deliberate spread of controversy caused by a disinformation campaign, or benign misinformation content that cascades through the internet, the propagation of false news is a major issue in social media networks. The increasing public consumption of social media over the last decade has caused more and more people to rely on social media as a source of news[9, 17]. And the attempts to counter misinformation using manual content classiﬁcation and human moderators have failed to scale up the sheer amount of content [5] that propagates through modern social networks. Therefore over the last decade, automated means of countering false news have drawn great interest. Existing automated counters to false news mainly focus on the detection of misinformation content. The exact form of these detection algorithms depends on the type of content and the underlying social media network. In general, most misinformation detection methods rely on content classiﬁcation using some black-box machine learning algorithm that is trained on a dataset labeled by humans. These content classiﬁcation methods can yield high accuracy. However, these content classiﬁers still suffer from large biases caused by the biases in the training datasets [7]. And their use in social media platforms can lead to ethnic, religious or political discrimination within the platform. In this work, we deviate from the existing literature that mainly focuses on misinformation detection, and instead, we approach the problem of countering misinformation as an optimal control problem on a network. In this approach, we study the problem of altering the social network dynamics in a way that restricts misinformation spread while keeping the propagation of true content above some acceptable level. As such, we represent the problem of countering misinformation as an optimization problem on a social network model and then solve this optimization problem to ﬁnd real-world changes to the social network that reduce the misinformation spread over the network. In our social network model, we represent the propagation of news on a social media platform using a percolation process model. In this model, each news item propagates in a smallworld network of users in consecutive cascades. In each cascade, the users in the network that believe a news content is correct re-share the news content probabilistically. Then, each user that receives the shared news content, either believes in it or discards it based on a probability distribution determined by the polarization of the sharing and receiving users and the reﬂexivity of the receiving user. The evidence suggests that there are subtle yet detectable differences in the cascade behaviors of misinformation and true content. User polarization and reﬂexivity are thought to be the main drivers of this difference [3, 18]. Thus, the above model can capture different propagation patterns of misinformation and true content. Our approach to countering misinformation relies on this difference in propagation patterns to discriminate between different content types. We exploit this propagation difference to design alterations on the network of users that selectively hinders the spread of misinformation containing news while maintaining acceptable propagation of true content. The speciﬁc methods that can be used to control the content ﬂow over the network vary signiﬁcantly depending on the capabilities and the structure of the underlying online platform. We assume a social media environment that can acquire usage data from the users and can estimate the polarization of the user as well as the probability to reshare a particular news content. Under this assumption, we propose a method called the Dropout Method. This method relies on selectively limiting the content ﬂow over the network by the random omission of news items in a user’s news feed with a predetermined dropout probability. These probabilities must be set up to reduce misinformation spread while minimally affecting the spread of correct content. To achieve this discrimination between misinformation and true content, we let the dropout probabilities depend on the polarization of both the sharing and receiving users. As mentioned before, these quantities are known to affect misinformation and true content ﬂow differently, thus they can be used to identify news shares that are likely to contain misinformation. 1) In section 3, we develop an optimization-based approach to model the problem of countering misinformation through alterations in the social network structure. 2) In section 4, we seek approximate solitouns to the problem we develop in section 3, and ultimately develop an algorithm that can counter misinformation through alterations in the social network structure. 1) Misinformation Propagation: The existing works on misinformation propagation can be separated into two different categories. the ﬁrst of these categories focuses on ﬁnding mathematical models that describe the propagation of misinformation content. Most of these models describe the propagation of misinformation using established epidemiological models. The epidemiological models that have been most widely used in modeling viral content are, susceptible-infectedsusceptible (SIS) [8, 11], susceptible-infected-removed (SIR) [22, 25], and susceptible-exposed-infected-removed (SEIR) [15, 24]. All of these models describe misinformation propagation over a social network by classiﬁcation the users on the social network to different groups and modeling how these groups evolve over time. In their work Raponi et. al. provides a comprehensive analysis of these epidemiological models and their use in modeling misinformation spread [20]. In our work we use an SIR-based model for content propagation as it is a widely accepted model for modelling fake news and it is simple to analyze. The second category of works on misinformation propagation focuses on discriminating misinformation spread from the spread of other content. In their work Zhao et. al. statistically show that the propagation patterns of fake news differ predictably form other content [26]. Wu et. al. uses support vector machine classiﬁers to detect identify misinformation campaigns based on propagation patterns [23]. There are also mixed approaches to misinformation detection that uses both automated content classiﬁers and propagation patterns to detect misinformation. Varol et. al. uses a supervised learning approach based on k-nearest neighbors classiﬁers that uses sentiment values and propagation patterns to identify promoted campaigns on social media [21]. Our approach is similar to these works in the sense that we seek discrimination between misinformation and other content. However, our method does not attempt to explicitly identify misinformation. 2) Countering Misinformation: There are some existing works that attempt to limit the propagation of misinformation. In their work Fan et. al. proposes two models for multiple competing diffusion processes on network and investigates the problem of containing rumor spread on a competitive diffusion model [4]. Similarly Litou et. al. model the competition between misinformation and credible information on a network using a novel dynamic linear threshold model and investigate the problem of ﬁnding optimal set of users on a network to initiate the propagation of credible content [13, 14]. More recently there has been work on reﬁning this approach by considering location [27] or community [16] structures of the underlying social network. Unlike these works we do not focus on minimizing the inﬂuence of misinformation by maximizing the inﬂuence of a competing diffusion model. Instead we focus on altering the social media dynamics in a way that passively reduces misinformation spread without requiring any competing credible content.
Maximum Number of Modes of Gaussian Mixtures<|sep|>where µ ∈ Rd is the mean vector and the symmetric positive deﬁnite d × d matrix Σ is the covariance matrix. This density has a unique absolute maximum at x = µ. Now consider a mixture distribution X consisting of k Gaussian components Xi ∼ N(µi, Σi) and mixture weights αi for i = 1, . . . , k, so that: This is again a probability density function given that αi ≥ 0 and α1 + α2 + · · · + αk = 1. In other words, the density of a Gaussian mixture is a convex combination of Gaussian densities. Such mixtures can exhibit quite complex behavior even for a small number k of components. This is a feature that makes them attractive for modeling in applications. A fundamental property of a probability density function is the number of modes, i.e. local maxima, that it possesses. For Gaussian mixtures, this is especially relevant in applications such as clustering [9, p. 383]. For example, the mean shift algorithm converges if there are only ﬁnitely many critical points [17]. We will be interested in the maximal number m(d, k) of local maxima for d-dimensional Gaussian mixtures with k components. Shockingly, it is not known whether this maximal number is always ﬁnite for general Gaussian mixtures. On the other hand, we stress that the number of modes is a property of a Gaussian mixture density with ﬁxed parameters and no sample involved; it should not be confused with the number of local maxima of the likelihood function of a Gaussian mixture model (a relevant but diﬀerent question, see [2] and [10]).
Trail of the Higgs in the primordial spectrum<|sep|>The conﬁrmation of the Higgs with a mass 125.09±0.21±0.11 GeV at the Large Hadron Collider (LHC) [1] has marked the discovery of the last piece of the jigsaw for the standard model. At the same time, this raises signiﬁcant questions on the cosmological evolution of the Higgs. If we extend the standard model to higher energy scales, the quartic Higgs coupling becomes negative beyond a scale Λ ∼ 1010 GeV, mainly contributed by the loop corrections from the Yukawa coupling to the top quark [2]. At very large ﬁeld values of the Higgs lies the true minimum with negative Planckian energy density, if extrapolated up to the Planck scale, indicating that the electroweak vacuum is only metastable. This metastability is sensitive to the top quark mass: the current measurement of the top quark mass 173.34 ± 0.76 ± 0.3 GeV [3] excludes absolute stability at 99% conﬁdence level [4], while the lifetime of the electroweak vacuum is longer than the age of the universe [5]. That means, with a generic sub-Planckian initial value of the Higgs h ≲ mPl, it is very unlikely to ﬁnd ourselves in the metastable electroweak vacuum h ≲ Λ ∼ 10−8mPl unless the Higgs is forced to stay near the minimum. Even if the Higgs is initially placed at the origin, during inﬂation [6] the picture could become diﬀerent very easily if the energy scale of inﬂation is large enough. Indeed, with the current bound at 95% conﬁdence on the tensor-to-scalar ratio r0.05 ≲ 0.12 [7], it is not diﬃcult to construct inﬂationary models with the Hubble parameter H ≳ Λ [8]. If the Higgs is only coupled to the standard model particle species, it can be treated as an eﬀectively massless scalar ﬁeld in the inﬂationary era. Then the Higgs acquires quantum ﬂuctuations of O(H), so that it eventually settles down in the true minimum with large negative energy [9]. There are many ways to solve this problem by introducing some new physics beyond standard model. For instance, some new particle with a mass larger than top quark can change the running of the Higgs self-coupling λ, and prevent it from becoming negative [10]. Inﬂation with ﬁnite temperature caused by dissipative eﬀects can also change the eﬀective potential by thermal corrections and raise the vaccum with negative energy to a false one [11]. Besides, Higgs inﬂation with a non-minimal coupling to gravity can solve this problem similarly [12]. A simple way to make the Higgs safe from instability during inﬂation is to introduce a large mass by a direct coupling to the inﬂaton, which makes mh ≳ H [13]. It is shown that the Hawking-Moss decay rate is suppressed by e(mh/H)4, while the Coleman-de Luccia decay rate is still neglegible [14] so that the evolution of the Higgs towards the electroweak vacuum during inﬂation is very likely. However, this coupling can aﬀect the evolution of the curvature perturbation during inﬂation, thus may leave distinctive features primordial power spectrum by resonant oscillations [15, 16]. These potentially important eﬀects have not been studied closely. By treating the coupling term as a perturbation to the standard equation of motion [17], we can study in detail the impacts of the heavy Higgs during inﬂation. This article is outlined as follows. In Section 2, we ﬁrst consider the classical background evolution of the Higgs and the inﬂaton with a simple renormalizable coupling between them. Then we show this oscillating background gives rise to logarithmic oscillations in the primordial spectrum. In Section 3 we compute the contributions from the quantum ﬂuctuations to the power spectrum, which is subdominant compared to the classical features. After brieﬂy commenting on the prospects for collider search in Section 4, we conclude in Section 5.
Local Explanation of Dialogue Response Generation<|sep|>As we use machine learning models in daily tasks, such as medical diagnostics [6, 19], speech assistants [31] etc., being able to trust the predictions being made has become increasingly important. To understand the underlying reasoning process of complex machine learning models a sub-ﬁeld of explainable artiﬁcial intelligence (XAI) [2, 17, 36] called local explanations, has seen promising results [35]. Local explanation methods [27, 39] often approximate an underlying black box model by ﬁtting an interpretable proxy, such as a linear model or tree, around the neighborhood of individual predictions. These methods have the advantage of being model-agnostic and locally interpretable. Traditionally, off-the-shelf local explanation frameworks, such as the Shapley value in game theory [38] and the learning-based Local Interpretable Model-agnostic Explanation (LIME) [35] have been shown to work well on classiﬁcation tasks with a small number of classes. In particular, there has been work on image classiﬁcation [35], sentiment analysis [8], and evidence selection for question answering [32]. However, to the best of our knowledge, there has been less work studying explanations over models with sequential output and large class sizes at each time step. An attempt by [1] aims at explaining machine translation by aligning the sentences in source and target languages. Nonetheless, unlike translation, where it is possible to ﬁnd almost all word alignments of the input and output sentences, many text generation tasks are not alignment-based. We further explore explanations over sequences that contain implicit and indirect relations between the input and output utterances. In this paper, we study explanations over a set of representative conditional text generation models – dialogue response generation models [45, 55]. These models typically aim to produce an engaging and informative [3, 24] response to an input message. The open-ended sentences and multiple acceptable responses in dialogues pose two major challenges: (1) an exponentially large output space and (2) the implicit relations between the input and output texts. For example, the open-ended prompt “How are you today?” could lead to multiple responses depending on the users’ emotion, situation, social skills, expressions, etc. A simple answer such as “Good. Thank you for asking.” does not have an explicit alignment to words in the input prompt. Even though this alignment does not exist, it is clear that “good” is the key response to “how are you”. To ﬁnd such crucial corresponding parts in a dialogue, we propose to extract explanations that can answer the question: “Which parts of the response are inﬂuenced the most by parts of the prompt?” To obtain such explanations, we introduce LERG, a novel yet simple method that extracts the sorted importance scores of every input-output segment pair from a dialogue response generation model. We view this sequence prediction as the uncertainty estimation of one human response and ﬁnd a linear proxy that simulates the certainty caused from one input segment to an output segment. We further derive two optimization variations of LERG. One is learning-based [35] and another is the derived optimal similar to Shapley value [38]. To theoretically verify LERG, we propose that an ideal explanation of text generation should adhere to three properties: unbiased approximation, intra-response consistency, and causal cause identiﬁcation. To the best of our knowledge, our work is the ﬁrst to explore explanation over dialog response generation while maintaining all three properties. To verify if the explanations are both faithful (the explanation is fully dependent on the model being explained) [2] and interpretable (the explanation is understandable by humans) [14], we conduct comprehensive automatic evaluations and user study. We evaluate the necessity and sufﬁciency of the extracted explanation to the generation model by evaluating the perplexity change of removing salient input segments (necessity) and evaluating the perplexity of only salient segments remaining (sufﬁciency). In our user study, we present annotators with only the most salient parts in an input and ask them to select the most appropriate response from a set of candidates. Empirically, our proposed method consistently outperforms baselines on both automatic metrics and human evaluation. • We propose a uniﬁed formulation that generalizes local explanation methods towards sequence generation and show that our method adheres to the desired properties for explaining conditional text generation.
Population I Cepheids and star formation history of the Large Magellanic Cloud<|sep|>The Large Magellanic Cloud (LMC) is among one of the most studied galaxy in the Universe due to its close proximity to the Galaxy, favourable viewing angle, and star formation activities. In recent times, the distribution of stellar populations in the LMC has been studied with variety of objects, e.g. star clusters (Pietrzynski & Udalski 2000, Harris & Zaritsky 2009, Glat et al. 2010), Cepheid variables (Alcock et al. 1999, Nikoleav et al. 2004), RR Lyrae variables (Subramaniam & Subramanian 2009, Wagner-Kaiser & Sarajedini 2013), red clump stars (Koerwer 2009, Subramaniam & Subramanian 2013), among others. These studies imply that the episodic star formation events have taken place in the LMC, most likely due to repeated interaction between the Magellanic Clouds (MCs) and/or with the Galaxy. Population I Cepheids have been widely used to reconstruct the history of star formation because they are intrinsically bright, easily observable and ubiquitous in the LMC. They are very good candidates to understand the star formation activity during the last 30 – 600 Myrs as typical life of the Pop I Cepheids lie in this time span. There are two principal types of Pop I Cepheids, one those pulsating in the fundamental mode (FU) exhibit sawtooth-like light curves and other ﬁrst overtone (FO) generally show sinusoidal light curves, have shorter periods and typically have lower amplitudes than the FU Cepheids. The use of Cepheids as tracers of young stellar populations comes from the fact that they obey a period-luminosity and a mass-luminosity relation. There has been many surveys for Cepheids in the LMC, however, due to selection biases, most of them were not complete. In recent years the amount and quality of the photometric data has increased manifold, thanks mainly to microlensing surveys (e.g., Beaulieu et al. 1995, Alcock et al. 1997, Udalski et al. 1999). One such survey, the Optical Gravitational Lensing Experiment (OGLE), has revolutionized the ﬁeld by producing thousands of Cepheid light curves with high signal-to-noise (S/N) and determining very accurate parameters like periods, magnitudes and amplitudes (Soszy´nski et al. 2008). In this paper we aim to understand the Cepheids period and age distributions in the LMC and to study the spatial distribution of the Pop I Cepheids in order to examine the star formation scenarios in the LMC. The paper is organized as follows: details of the data are given in Section 2. The period and age distributions of Cepheids are studied in Sections 3 and 4 respectively. Their spatial distributions are given in Section 5. A comparative study of the spatial distribution of Cepheids with that of the star clusters is presented in Section 6 followed by the summary of the results in Section 7.
Maximum of N Independent Brownian Walkers till the First Exit From the Half Space<|sep|>The probability distribution of the maximum of a single one-dimensional Brownian motion (and its variants such as a Brownian bridge or an excursion) over a ﬁxed interval of time [0, t], has a long history in the probability literature [1–7]. The statistics of the maximum has diverse applications. One example is the Kolmogorov-Smirnov test in statistics that is used to compare, in a nonparametric way, two diﬀerent probability distributions [8, 9]. Similarly, the distribution of the global maximum of a discrete-time random ﬂights (including L´evy ﬂights) has also been studied in the probability literature [10, 11], with more recent applications in computer science [12], physics [13] and chemistry [14]. In statistical physics, there has been a recent revival of interest in related problems in the context of the distribution of the maximal height, measured with respect to a reference point, of (1 + 1)-dimensional ﬂuctuating interfaces [15– 17]. In the stationary state of a ﬁnite sample of size L, such ﬂuctuating interfaces are often described by a Brownian bridge in space over an interval [0, L], albeit with certain global constraints [16]. The statistics of maximum has also been computed for continuous-time subdiﬀusive processes [18, 19] and has been used to analyse single particle trajectories [20]. The distribution of the maximum for a single Brownian motion (or its variants such as bridge, excursion etc.) has been extended to many Brownian motions, including certain strongly interacting random walkers, e.g. nonintersecting, so-called vicious random walkers [21–24]. (The latter problem has an intriguing connection to the Gaussian ensembles of the random matrix theory [21, 24, 25].) For independent walkers, the results on the distribution of the maximum have recently been used to compute the mean perimeter and the mean area of the convex hull of N independent planar Brownian motions [26, 27]. These results on Brownian motion and its variants represent rare exact analytical results for the extreme value statistics of correlated random variables, a subject of increasing current interest [28]. However, all these results about the distribution of the maximum, for a single or multiple walkers, have been derived in the case when one considers the walkers over a ﬁxed interval of time [0, t]. An interesting variation of this problem, with several applications, arises when the interval [0, t] is not ﬁxed, but itself varies from realization to realization, i.e., one observes the walker (or walkers) over a time interval [0, ts] where the stopping time ts of the process itself is a random variable. For example, ts may represent the ﬁrst-passage time (through the origin) of a walker. To be more precise, consider ﬁrst a single Brownian walker that starts at time t = 0 at position x1 > 0. The position x1(t) of the walker evolves via the continuous-time stochastic equation, dx1/dt = η1(t) where η1(t) is a Gaussian white noise with mean ⟨η1(t)⟩ = 0 and a correlator ⟨η1(t)η1(t′)⟩ = δ(t − t′). The process stops at the stopping time ts when the walker hits the origin for the ﬁrst time (see Fig. 1). Let m be the maximum displacement of the particle till the stopping time ts. The statistics of the random variable m is interesting and it represents an example of the so called ﬁrst-passage Brownian functional [29]. The problem is a toy model of ‘random search’, where the origin represents a ﬁxed ‘target’ and the Brownian walker represents a random searcher. The search is called oﬀ when the searcher ﬁnds its target and m represents the maximum distance travelled by the searcher before it ﬁnds its target. For concreteness, we shall mostly used terminology related to random search, although there are several applications of this problem. For example, in the context of trapping [30–32] or predator-prey [33] models the origin may represent an immobile target (prey) and the Brownian walker may represent a diﬀusing chemical trap (predator). The stopping time ts is then the reaction time or the survival time of the prey and m denotes the maximum distance the predator travels before ﬁnding its prey. In the context of the directed FIG. 1: The trajectory (red line) of a single Brownian walker starting initially at x1 till the stopping time ts at which it hits the origin for the ﬁrst time. The maximum distance travelled by the particle till ts is denoted by m. Abelian sandpile model in (1 + 1)-dimensions [34], m represents the maximum lateral size of an avalanche [35]. The random variable m also plays an important role in characterizing the so called staircase polygons [36]. In the context of queueing theory, where the position of the walker represents the length of a queue, m represents the maximum length of a queue during the so called busy period [35, 36]. The probability density function (pdf) P1(m ��x1) of m (for ﬁxed x1) can be easily computed [35] and it turns out to be a pure power law: While this pdf is evidently normalized to unity, the average ⟨m⟩ and higher integer moments are inﬁnite! The cumulative distribution of the maximum is given by This distribution has a very simple interpretation: it just represents the exit probability of a Brownian particle [37], starting at 0 ≤ x1 ≤ L, from a box [0, L] through its left boundary at 0. In this paper, we study a generalization of this search problem where there is still one ﬁxed target at the origin, but there are N searchers who perform independent Brownian motions on the x > 0 axis, starting at the initial positions ⃗x ≡ (x1, x2, . . . , xN). The position xi(t) of the i-th walker evolves with time t via the Brownian evolution where ηi(t) is a Gaussian white noise satisfying ⟨ηi(t)⟩ = 0 and ⟨ηi(t)ηj(t′)⟩ = δ(t − t′)δi,j. Since the walkers are independent they can cross each other. The process stops at a stopping time ts when the origin is hit for the ﬁrst time by any one of the walkers (e.g., the second walker (red) in Fig. 2). Note that ts varies from one history of the process to another. In the context of chemical kinetics [31] where the problem is generally referred to as the ‘target annihilation’ problem, various generalizations of this problem have been investigated including e.g. the situation where the target itself diﬀuses [38–42]. In the following, we shall limit ourselves to the case of immobile target and focus on the statistics of the maximum distance m (from the target) travelled by any of the walkers till the stopping time ts when the target is found. Thus m denotes the distance of the farthest point on the x axis visited by any one of the walkers till ts. Clearly m is a random variable ﬂuctuating from one realization of the process to another. Our object of interest is the FIG. 2: The trajectories of N = 5 independent Brownian walkers starting at initial positions x1, x2, x3, x4 and x5 till the stopping time ts when one of the walkers (the second one (red) in this ﬁgure) hits the origin. The maximum displacement along the x direction till ts (undergone, e.g., by the third particle (green)) is denoted by m. The cumulative probability QN(L ��⃗x) = Prob [m ≤ L|⃗x] also represents the exit probability of the ﬁrst particle from a box [0, L] through its left boundary. probability density PN(m ��⃗x) of this maximum distance m, given the number N of walkers and their initial positions ⃗x. Thus m provides an estimate (worst-case) of the distance that needs to be covered by a team of N walkers to ﬁnd a ﬁxed target. As in the single searcher case, let QN(L ��⃗x) = Prob[m ≤ L ��⃗x] = � L 0 PN(m ��⃗x) dm be the cumulative probability that the maximum m till ts is less than or equal to L. This cumulative distribution of the maximum can be interpreted as the solution of a diﬀerent problem as in the N = 1 case. Consider, for instance, a slightly diﬀerent problem where again we have a set of N independent walkers, but now inside a box [0, L], starting at the initial positions ⃗x. Let us deﬁne the exit probability as the probability that the ﬁrst particle that exits the box [0, L] does so through 0 (and not through the upper boundary at L), see Fig. 2. As in the N = 1 case, this exit probability is precisely the cumulative distribution QN(L ��⃗x) of the maximum m till the stopping time in the semi-inﬁnite system, as it counts all those events where one of the trajectories hits the lower boundary 0 before hitting the upper boundary at L while all the others stay inside the box [0, L] till this event of ﬁrst-hitting the origin. We will see that for this seemingly simple one-dimensional model of independent walkers, the statistics of m has a rich and nontrivial dependence on the number N of walkers. This is partly due to the fact that the same stopping time ts for all the walkers eﬀectively introduces a correlation between the trajectories of the walkers, even though each executes an independent Brownian motion. While for N = 1 the solution is simple, it becomes rather nonrivial even for N = 2! Let us ﬁrst summarize our main results. We compute the pdf PN(m ��⃗x) exactly for all N ≥ 1 by a path counting (or path integral) method. We show that, for arbitrary N ≥ 1, the pdf of the maximum has an asymptotic power-law tail where the prefactor BN has a nontrivial N dependence which we compute explicitly. For N = 1, we have B1 = 1 and the asymptotic result in (4) is actually valid exactly for all m ≥ x1. For N = 2, we will see that Our asymptotic result (4) indicates that for N walkers, integer moments of m up to order (N − 1) are ﬁnite, while higher integer moments are inﬁnite. Evidently, as N increases, the distribution becomes narrower and narrower as expected but it does so in a nontrivial fashion. Recently, the cumulative distribution of the maximum m till the ﬁrst-passage time ts, or equivalently the exit probability Q(L|x) from the box [0, L] through the origin, was studied [43] for a generic self-aﬃne stochastic process x(t) starting at the initial position x. The process x(t) typically grows with time as x(t) ∼ tH where H is the Hurst exponent. This power law growth of distance with time makes the process self-aﬃne. An example is the ordinary Brownian motion where H = 1/2. For such a generic self-aﬃne process, it was argued [43] that the cumulative distribution of the maximum Q(L|x) = Prob(m ≤ L|x) ∼ 1 − A (x/L)φ in the limit x/L → 0 where A is a constant. The exponent φ was found to be related to the persistence exponent θ via the scaling relation φ = θ/H [43]. The persistence exponent θ characterizes the late time power law decay of the survival probability, i.e., the probability that the process stays on the positive half-axis up to time t [44]. Thus the pdf of the maximum decays for large m as, P(m|x) ∼ m−φ−1 with φ = θ/H. The exact result (4) shows that if we think of the assembly of N independent Brownian motions as a single self-aﬃne stochastic process in the N-dimensional space, then φ = N. We will see later that the persistence exponent for this collective process is θ = N/2 and the Hurst exponent H = 1/2. Thus our exact result for this model supports the general scaling relation φ = θ/H found in [43]. The paper is organized as follows. In Section 2, we provide a simple heuristic argument in favour of our main result (4). This argument is not suﬃcient to compute the prefactor BN exactly for all N. However, we show that this heuristic argument becomes asymptotically exact for large N and one can extract the limiting behavior of BN for large N using an extreme value argument. In Section 3, we set up the general method for computing the cumulative distribution QN(L ��⃗x) of the maximum m. This requires solving Laplace’s equation in an N-dimensional space with appropriate boundary conditions. We present explicit solutions for the cases N = 1 and N = 2. In Section 4, we present an alternative path counting method that is more general, physically transparent and provides explicit results for all N ≥ 1. In Section 5, we present numerical results to verify our analytical predictions. Finally in Section 6, we conclude with a summary and a list of interesting open problems. Some of the details of the computations are relegated to the appendices.
M2-branes Coupled to Antisymmetric Fluxes<|sep|>The dynamics of D-branes was well studied a decade ago [1, 2, 3, 4, 5], which has driven huge progress of string theory. In contrast, multiple M-branes are more un tamable. Recently, a 3-dimensional ﬁeld theory for multiple M2-branes was proposed by Bagger and Lambert [6, 7, 8] and Gustavsson [9, 10]. To check this theory, it is important to do the dimensional reduction from M2-branes to D2-branes. The reduction has been investigated from diﬀerent viewpoints in [9, 11, 12, 22]. By virtue of the Nambu-Poisson algebra, Ho and Matsuo et.al. have carefully worked over the relations between multiple M2-brans and a M5-brane [13, 14, 15, 22]. The construction of Bagger-Lambert-Gustavsson model relies on a 3-algebra with a positive-deﬁnite metric, namely the A4 algebra. It was conjectured by [13] and conﬁrmed by [16, 17] that the BLG theory is unique [10, 18], because all ﬁnite di mensional 3-algebras with a positive-deﬁnite metric are direct sums of A4 with trivial algebras [13, 17]. However, if we do not require the metric to be positive-deﬁnite, there are still a rich class of models [19]. In particular, very recently, a class of models based on 3-algebras with a Lorentzian metric3 were studied by several groups [20, 21, 22]. Besides considering Lorentzian metrics, various attempts to extending the BLG model were also made during the past few months [12, 13, 23, 24]. In [25, 26, 27], there are some interesting discussions relating BLG theory to M2branes on an obifold (or namely an “M-fold” [26]). As a partial list, other aspects of BLG theory were extensively studied in [30, 31, 32, 33, 34, 35, 36, 37]. Our interest in this paper is to consider the eﬀects of the background antisymmetric tensor ﬁelds on multiple M2-branes. It is well-known that in the world-volume theory of D-branes, when the background ﬂuxes are switched on, there will be additional Chern-Simons terms [2, 3], through which the background ﬁelds couple not only to the internal gauge ﬁelds but also to the world-volume scalar ﬁelds [5]. In string theory, the antisymmetric tensor ﬁelds are sourced by various dimensional D-branes or strings. In M-theory, these are 3-forms C(3) and 6-forms C(6), they are dual to each other and are coupled to M2-branes and M5-branes respectively. The Chern-Simons terms we want to discuss describe interactions between world-volume ﬁelds and C(3), C(6). To distinguish them from those Chern-Simons terms purely of internal gauge ﬁelds (that is, the terms appearing in Bagger and Lambert’s papers [7, 8]), we will +λ2ǫλµνCIJKLMNSTr([T d, T e, T f]T aT bT c)XI dXJ e XK f DλXL a DµXM b DνXN c � . When writing down this action, we only take the lowest order terms into consideration, although it is expectable that a full Myers-Chern-Simons action will involve higher order terms of C(3), C(6) and Aλab and their derivatives. Here ǫλµν (λ, µ, ν = 1, 2, 3) is the Levi-Civita symbol with ǫ123 = 1 along the world-volume directions. The coeﬃcients λ1 and λ2 depend on conventions, so we leave them as unﬁxed parameters at present. The covariant derivative with respect to the internal gauge ﬁeld Aλab is 3! � Tr([T a, T b, T c]T dT e) + Tr(T dT e[T a, T b, T c]) + Tr(T e[T a, T b, T c]T d) +Tr([T a, T b, T c]T eT d) + Tr(T eT d[T a, T b, T c]) + Tr(T d[T a, T b, T c]T e) � , (3) and “Tr” to denote an ordinary trace. But, in the present case of multiple M2branes, we do not know how to perform such a trace “Tr” unless one gives the correct representation of all generators {T a}. To circumambulate this diﬃculty, let The tensors gabc and dabcd will be very useful. They are completely symmetric under the permutation of indices as indicated. In our conventions, indices inside round brackets (a1, ..., an) will always be understood as symmetrized with unit weight, i.e. with a factor 1/n!. We will use the same unit weight convention to anti-symmetrize indices inside square brackets [a1, ..., an]. This form of action may be still correct when background ﬁelds C(3), C(6) are functionals of non-Abelian scalars XI, XJ, etc. But in our following investigation we will consider constant background ﬁelds for simplicity. Furthermore, to systematically neglect other terms induced by the metric of spacetime, we work in a ﬂat spacetime background. The remaining of this paper is organized as follows. In section 2 we derive the general conditions of gauge invariance for the above Myers-Chern-Simons action. After a brief review of Lie 2-algebras in section 3, we solve the gauge invariance conditions and the constraints arising from dimensional reduction, hence ﬁx almost all of the components appearing in (4). This is done in concrete examples, i.e., section 4 for A4, and section 5 for 3-algebras with a Lorentzian metric. Failing to reobtain the above results in terms of cubic matrices, in section 6, we present some tentative thoughts on cubic matrices as representations of 3-algebras. Section 7 is a short conclusion.
Magnetic complexity as an explanation for bimodal rotation populations among young stars<|sep|>Stellar rotation catalyzes magnetic dynamo activity in the interiors of late-type stars that is manifest at the surface in the form of magnetic ﬁelds, energetic photon and particle radiation, supersonic winds and coronal mass ejections. The magnetized winds carry away angular momentum, a process commonly referred to as “magnetic braking”. As stars age, their rotation rates, Ω, eventually converge to the empirical Skumanich (1972) spin down law Ω ∼ t−1/2. Weber & Davis (1967) and Mestel (1968) derived the ﬁrst analytical expression for stellar angular momentum loss, obtaining ˙J = 2 3Ω ˙MR2 A, where ˙J is the angular momentum loss rate, ˙M the mass loss rate, RA is the radial distance at which the wind speed exceeds the local Alfv´en speed (the “Alfv´en radius”), and where a constant radial ﬁeld was assumed at the surface of the star. This approach was later generalized to more realistic scenarios in a range of diﬀerent studies (e.g. Mestel & Spruit 1987; Kawaler 1988; Taam & Spruit 1989; Chaboyer et al. 1995), which have generally proven successful in explaining spin down on the Main Sequence, including the empirical Skumanich law. The early phase of rotation evolution has proven more diﬃcult to understand. Observations of young open clusters by (e.g. Stauﬀer & Hartmann 1987; Soderblom et al. 1993; Queloz et al. 1998; Terndrup et al. 2000, see Meibom et al. 2011 for a recent compilation) found a large spread in rotation rates at ages up to a few hundred Myrs with a bimodal aspect comprising two branches corresponding to fast and slow rotation and implying an extremely fast transition between the two. The currently favored explanation for this phenomenon is a core-envelope decoupling near the zero age main-sequence (e.g. Stauﬀer et al. 1984; Soderblom et al. 1993; Barnes 2003), after which the outer convection zone with lower moment of inertia is rapidly spun down, leaving a more rapidlyrotating core. More recently, Brown (2014) has proposed a “Metastable Dynamo Model” (MDM) in which coupling between the magnetic ﬁeld and wind is initially weak. Spontaneous strong coupling of the star to the wind then happens at a certain early age, initiating the rapid spin down. For ad hoc coupling constant changes by factors of 100 or more, the model is successful in reproducing the observed rotation distributions of young clusters. However, the mechanism behind such a change in coupling has not been identiﬁed. Except for a small handful of indirect detections (e.g. Wood 2004; Wood et al. 2014), observational progress is stymied by the winds themselves being generally weak, while surface magnetic ﬁelds can only be inferred indirectly and with very limited spatial resolution (Donati & Landstreet 2009). One key ingredient in stellar rotation evolution models that has received scant attention is the morphology of the magnetic ﬁeld. While the early treatments of Mestel & Paris (1984) and Kawaler (1988) that provided much of the basis for subsequent rotation evolution models considered the multipole order of the magnetic ﬁeld, this was limited to the eﬀect of the radial dependence of the ﬁeld strength. Moreover, spin down models have generally assumed dipolar ﬁelds. Instead, a growing database of Zeeman-Doppler imaging observations indicates that surface magnetic ﬁelds of young, active stars mainly consist of high-order multipole components, rather than a simple dipole such as characterizes the large-scale solar magnetic ﬁeld (e.g. Donati 2003; Donati & Landstreet 2009; Marsden et al. 2011; Waite et al. 2011, 2015). Linsky & Wood (2014) have also recently inferred mass loss rates for the active stars ξ Boo A and π1 UMa that are two orders of magnitude lower than expected based on extrapolation from lower activity stars, suggesting that magnetic topology could have a more profound eﬀect on angular momentum loss than simply through the radial ﬁeld strength dependence. Here, we investigate the role of magnetic ﬁeld complexity on stellar angular momentum loss using a detailed, self-consistent three-dimensional magnetohydrodynamic (MHD) wind model that has proven successful in matching observations of the solar wind (Oran et al. 2013). We explore a range of simple magnetic conﬁgurations with diﬀerent multipolar complexity. The numerical methods are described in Section 2, the results of model calculations in Section 3, and we discuss our main ﬁndings and their implications in Section 4. We conclude in Section 5 that magnetic complexity can provide the strong coupling switch sought in the MDM by Brown (2014).
Physics-based Shadow Image Decomposition for Shadow Removal<|sep|>Shadows are present in most natural images. They form as a result of complex physical interactions between light sources, geometry, and materials of the objects in the scene. Analyzing shadows [1], [2] gives cues about the physical properties of objects [3], illumination conditions [4], [5], [6], scene geometry [7], [8], and object motions [9], [10]. Realistic shadow manipulation is an integral part of media editing [11], [12] and can greatly improve performance on various computer vision tasks [13], [14], [15], [16], [17], [18], [19], [20], [21], [22]. Therefore, methods for detecting and removing shadows have been extensively studied in computer vision. They are getting increased attention in recent years, following the rapid development of deep learning and increasing interest in image generation [7], [23] and augmented virtual reality [24]. Early shadow removal work was based on physical shadow models [25]. A common approach is to formulate the shadow removal problem using an image formation model, in which the image is expressed in terms of material properties and a light source-occluder system that casts shadows. Hence, a shadow-free image can be obtained by estimating the parameters of the sourceoccluder system and then reversing the shadow effects on the image [26], [27], [28], [29]. These methods relight the shadows in a physically plausible manner. However, estimating the correct solution for such illumination models is non-trivial and requires considerable processing time or user assistance [11], [30]. Alternatively, recently published large-scale datasets [31], [32], [33] allow the use of deep learning methods for shadow removal. In these cases, a network is trained in an end-to-end fashion to map the input shadow image to a shadow-free image. The success of these approaches shows that deep networks can effectively learn transformations that relight shadowed pixels. • H. Le is with Amazon Robotics, North Reading, MA 01864. E-mail: hle@cs.stonybrook.edu. This work is done prior to joining Amazon. Fig. 1: Shadow Removal via Shadow Image Decomposition. A shadow-free image Ishadow-free can be expressed in terms of a shadow image Ishadow, a relit image Irelit and a shadow matte α. The relit image is a linear transformation of the shadow image. The two unknown factors of this system are the shadow parameters (w, b) and the shadow matte layer α. We use two deep networks to estimate these two unknown factors. However, the actual physical properties of shadows are ignored although they contain strong priors to localize and modify shadowed pixels. Without any shadow priors, there is no guarantee that the networks would learn physically plausible transformations. Moreover, there are still well known issues with images generated by deep networks: results tend to be blurry [34], [35] and/or contain artifacts [36]. How to improve the quality of generated images is an active research topic [7], [37]. deep learning. Following early shadow removal works, we propose to use a simpliﬁed physical illumination model to deﬁne the mapping between shadow pixels and their shadow-free counterparts. Our main idea is to use deep networks to estimate the parameters for such a physical illumination model. We show that this approach has multiple advantages. The framework removes shadows in a physically plausible manner, avoiding the blurring and undesired artifacts commonly introduced by deep-networks. Moreover, the shadow removal mapping is simpler and easier to constrain since it is strictly deﬁned by the simpliﬁed illumination model. The proposed framework achieves the state-of-the-art shadow removal performance via training on paired data, and also can be trained effectively using only unpaired shadow and shadow-free patches, obviating the need of shadow-free image supervision. Speciﬁcally, our proposed illumination model is a linear transformation consisting of a scaling factor and an additive constant per color channel - for the whole umbra area of the shadow. These scaling factors and additive constants are the parameters of the model. The illumination model plays a key role in our method: with correct parameter estimates, we can use the model to remove shadows from images. Since these parameters are used in a single common linear model to relight the shadows for all shadowed pixels in the input image, this approach ensures global consistency for our shadow removal method. We propose training a deep network (SP-Net) to automatically estimate the parameters of the shadow model. Through training, SP-Net learns a mapping function from input shadow images to illumination model parameters. Furthermore, we use a shadow matting technique [11], [28], [30] to handle the penumbra area of the shadows. We incorporate our illumination model into an image decomposition formulation [11], [38], where the shadow-free image is expressed as a combination of the shadow image, the parameters of the shadow model, and a shadow density matte. This image decomposition formulation allows us to reconstruct the shadow-free image, as illustrated in Fig. 1. The shadow parameters (w, b) represent the transformation from the shadowed pixels to the illuminated pixels. The shadow matte represents the per-pixel linear combination of the relit image and the shadow image, which results to the shadow-free image. Previous work often requires user assistance [39] or solving an optimization system [40] to obtain the shadow mattes. In contrast, we propose to train a second network (M-Net) to accurately predict shadow mattes in a fully automated manner. Lastly, we employ an inpainting network, I-Net, to handle shadow pixels that might not follow our simpliﬁed linear illumination model (see Sec. 4.3). For example, these cases are due to pixels with saturated colors or shadows with inconsistent intensities across the umbra areas. In essence, we propose a method that removes shadows by regressing a set of shadow parameters and a matte layer from the input shadow image. This mapping is simpler than a pixelwise image-to-image translation and is easier to constrain via regularization of the shadow parameters and matte layer. Thus we can train our proposed shadow removal method even without paired shadow and shadow-free images. In this weakly-supervised setting, we use only shadow and non-shadow patches cropped from the shadow images, which eliminates the need for any shadow-free images. This is an unpaired cross-domain image-toimage translation problem and we approximate this mapping via an adversarial framework. We trained and tested our model on the ISTD dataset [32], which is the largest and most challenging available dataset for shadow removal. Our framework achieves state-of-the-art shadow removal performance on both the fully-supervised and weaklysupervised settings. On the fully-supervised setting, the combination of SP-Net and M-Net alone outperforms the state-of-the-art in shadow removal [41] by 14% error reduction in terms of MAE on the shadow areas, from 7.6 to 6.5 MAE. The full framework with SP-Net, M-Net, and the inpainting network I-Net further improves the results by another 8%, which yields a MAE of 6.0. On the weakly-supervised setting, our weakly-supervised method outperforms the state-of-the-art weakly-supervised shadow removal method [42] by 22%, reducing the MAE on the shadow areas from 12.4 to 9.7. Note that our method is the ﬁrst deep-learning approach that can learn a shadow removal transformation without any shadow-free images. Last, we introduce a novel challenging testing set to evaluate shadow removal methods based on time-lapse videos. Note that all current shadow removal datasets [31], [32] are limited to simple shadows, simple scenes, and without the oclluders in the images due to the data acquisition scheme [31], [32]. With time-lapse videos where the only motions are due to illumination effects, we can obtain paired shadow and non-shadow pixels that move in and out of the shadows when they travel across the scene. This method allows us to collect paired shadow data that is impossible to obtain via the standard data acquisition method such as shadows of immobile occluders, self-cast shadows, and shadows on various types of backgrounds and materials. We introduce SBUTimelapse, a video dataset of 50 clips. Our methods perform better than other methods on this challenging test. In summary, the contributions of this work are: • We propose a new deep learning approach for shadow removal, grounded by a simpliﬁed physical illumination model and an image decomposition formulation. • We propose a method to obtain paired shadow data of complex shadows in complex scenes based on time-lapse videos. We introduced SBU-Timelapse, a video shadow removal dataset for evaluating shadow removal methods. The pre-trained model, shadow removal results, and more details can be found at: https://github.com/cvlab-stonybrook/SID
Removing Qualified Names in Modular Languages<|sep|>Modularity is the key technique for dealing with large programs. Most modern languages (including objectoriented ones) employ qualiﬁed names of the form m.f to access a method f in a module m. Although the notion of qualiﬁed names is easy to implement, it leads to unnecessarily long names and it runs counter to the core of knowledgebase representation, i.e., the conciseness. Thisl problem should be eliminated to preserve clean and concise codes. In order to do so, we consider here an alternative to qualiﬁed names. The key idea is to import the declarations in m to the current module before they are used. In this way, all the declarations can be accessed locally. To be speciﬁc, we propose to add the following: where f(t1, . . . , tn) = V is a query to a module m and V is a free variable not appearing in x1, . . . , xn. Manuscript received January 1, 2003. Manuscript revised January 1, 2003. Final manuscript received January 1, 2003. †The author is a professor of Computer Eng., DongA University. email:khkwon@dau.ac.kr ††The author is a professor of Electronics Eng., DongA University. This expression thus supports the idea of importing all the declarations of a module. The latter one has the following semantics: ﬁrst evaluate f(t1, . . . , tn) w.r.t. /m and set V to the resulting value w. This expression thus supports the idea of importing some ( logical consequence of) declarations of a module. This is called module weakening/module customization. Note that the notion of MQ declarations is a novel feature which is not present in traditional languages. For example, (fib(3) = V )mf is a querying declaration where the value of v is not known. Its value is later determined by evaluating fib(3) w.r.t. the “ﬁbonacci” module mf. Although our approach can be applied to other programming paradigms, this paper focus on functional languages. That is, we extend a functional language with MI/MQ declarations.
Bounding Box Disparity: 3D Metrics for Object Detection With Full Degree of Freedom<|sep|>As 3D object detection gets more popular and new datasets are published [1, 2, 3], evaluation metrics gain in importance. The most common one is Intersection over Union (IoU). It is well known from object detection on two-dimensional data such as images. Existing implementations of its threedimensional counterpart usually neglect one or more degrees of freedom. Examples for this are implementations that work with axis-aligned bounding boxes or only consider a rotation around the z-axis [4]. This oversimpliﬁes real world problems, as usually rotation of objects is possible in any given direction. To the best of our knowledge, although the strategy of computing IoU has already been mathematically generalized [5], we provide and derive the ﬁrst closed-form analytic solution for the case of 3D bounding boxes with full degree of freedom. We further derive an analytic solution for the volume-tovolume distance (v2v) of two 3D bounding boxes. The metric v2v is deﬁned as the shortest distance between the hull of one volume to the hull of another volume. Both metrics are visualized in Fig. 1. For both metrics we provide the ﬁrst open source implementation as a standalone python function, as well as an extension to the Open3D library [6] and a ROS-node [7]. This paper is structured as follows. First, we discuss related work. In Section 3, we mathematically deﬁne bounding boxes and review the existing metrics. Afterwards the solution for volumetric IoU is stated and shortly compared to its point based method. Consecutively v2v is presented and a combined positive continuous metric called Bounding Box Disparity (BBD) is proposed.
High order semi-Lagrangian methods for the BGK equation<|sep|>In the kinetic theory of gases, the dynamics of a monoatomic rareﬁed gas system is described by the Boltzmann equation [1]. The numerical approximation of this equation is not trivial due to the complex structure of the collision operator. The BGK equation, introduced by Bhatnagar, Gross and Krook [2] and independently by Welander [3] is a simpliﬁed model of the Boltzmann equation. In the BGK model the collision operator is substituted by a relaxation operator; the initial value problem reads as where d and N denote the dimension of the physical and velocity spaces respectively, and ε−1 is the collision frequency, that, throughout this paper, is assumed to be a ﬁxed constant for simplicity. M[f] denotes the local Maxwellian with the same macroscopic moments of the distribution function f(x, v, t), and is given by where R is the ideal gas constant and ρ(x, t) ∈ R+, u(x, t) ∈ RN and T(x, t) ∈ R+ denote the macroscopic moments of the distribution function f, that is: density, mean velocity and temperature, respectively. They are obtained in the following way (3) The physical quantity E(x, t) is the total energy that is related to the temperature T(x, t) by the underlying relation:
Stochastically excited oscillations on the upper main sequence<|sep|>There are several mechanisms driving oscillations in pulsating variables. The oldest known is the opacity (κ) mechanism acting like a heat engine, converting thermal energy into mechanical energy (e.g. Eddington 1919, Cox 1963). The layers where the thermal energy is stored are the zones connected to (partial) ionisation of abundant elements, which can take place only at speciﬁc temperatures. The zone where neutral hydrogen (H) and helium (He) are ionised is at about 14 000 K and close to the surface, whereas the second ionisation zone of He II is at ∼50 000 K. The driving in the He II ionisation zone is the main source of excitation in stars placed in the classical instability strip such as the δ Scuti (δ Sct) stars. The pulsations of the more massive β Cephei (β Cep) and Slowly Pulsating B stars (SPB) are triggered by the κ mechanism operating on the iron-group elements (located at ∼200 000 K). The oscillations in γ Doradus stars (γ Dor) are driven by a similar mechanism, in this case, however, it is the bottom of the outer convection zone blocking the ﬂux from the interior (Guzik et al. 2000) and is therefore called convective blocking (see Fig. 1 for exact location in the Hertzsprung-Russell diagram). Another important mechanism, especially for the present review, is stochastic driving. In stars like the Sun the modes are intrinsically stable, damped by the turbulent convection. Nevertheless the acoustic energy of the convective motion, which is comparable to the sound speed, is suﬃcient to cause resonance at a star’s natural frequencies where a part of the energy is transferred into global oscillation modes. Because of the large number of convective cells the excitation is random, hence stochastic. This is very diﬀerent from the κ mechanism, which excites pulsation coherently. This contrast is a very important way of distinguishing between these two types of driving mechanisms in the signal processing. Another important attribute of stochastic driving is that all the modes in a certain frequency range are excited to observable amplitudes, allowing mode
Planar wiggler as a tool for generating hard twisted photons<|sep|>A rigorous deﬁnition of twisted photons in QED is as follows: the twisted photons are the quanta of the electromagnetic ﬁeld with the deﬁnite energy k0, the momentum projection k3, the projection of the total angular momentum m on the same axis, and the helicity s. We will call the axis that appears in the deﬁnition of twisted photons as the detector axis. The mass-shell condition for such photons reads as k0 = (k2 3 +k2 ⊥)1/2. In the paraxial approximation, n⊥ := k⊥/k0 ≪ 1, the projection of the orbital angular momentum l is deﬁned. It is related to the helicity and the total angular momentum as l = m − s. By its deﬁnition, the projection of angular momentum changes under the shifts of the axis with respect to which it is deﬁned. In performing such a shift, a photon state with the deﬁnite projection of the total angular momentum m passes into a superposition of twisted states with all the possible projections m [1]. Due to peculiar properties of twisted photons stemming from the fact that they are quanta with the deﬁnite projection of angular momentum m, the twisted electromagnetic waves provide new instruments for study and solution of fundamental and technical problems. For example, the twisted photons were used in telecommunication to increase the capacity of a channel by employing the projection of angular momentum as an additional quantum number that carries information [2]. In microscopy, the use of twisted photons resulted in overcoming the diﬀraction limit [3]. In astrophysics, the twisted photons were used to perform a high-contrast coronagraph [4]. The optical tweezers based on twisted photons were employed to manipulate nanoparticles [5, 6]. The peculiarities of interaction of twisted photons with atoms were investigated in many papers (see, e.g., [7–10]). As for interaction of twisted photons with nuclei, we refer to the works [11, 12]. The simplest means to create twisted electromagnetic wave is to convert an ordinary plane-wave radiation to a twisted one by employing the holographic or phase plates [9]. However, this approach is unapplicable for production of hard twisted photons in the x-ray and gamma spectral ranges. The pioneering theoretical proposals for generation of hard twisted photons were based on the use of inverse Compton scattering [13– 15], its nonrelativistic limit [11, 16], and channeling [17, 18]. The charges moving along helical trajectories provide a pure source of twisted photons with deﬁnite projection of the total angular momentum [19–26]. This observation was conﬁrmed experimentally in the radiation of helical undulators [22, 27, 28]. There are other ways to produce hard twisted photons. For example, they can be generated in irradiating a plasma by intense laser beams [29] and in the transition and Vavilov-Cherenkov radiations [30]. Recently, modifying the well-known Baier-Katkov method [31, 32], the general theory of radiation of twisted photons with account for the quantum recoil was developed for both scalar and Dirac particles [26]. In particular, it was shown there that MeV twisted photons can be generated by 180 GeV electrons in the helical wiggler and by 51.1 MeV electrons evolving in the laser wave produced by the free-electron laser with photon energy 1 keV. Suﬃciently strong electromagnetic ﬁelds of CO2 and Ti:Sa lasers can be employed for production of keV twisted photons by 256 MeV electrons. Modern detectors of twisted photons have a rather compact form [33, 34] and are applicable in a quite wide spectral range. The impressive achievements were reached in the methods of sorting electromagnetic radiation by the states with deﬁnite projection of the total angular momentum. Nowadays, they allow one to discriminate the projections of angular momentum within the range |lmax| ⩽ 30 [35]. Such a wide resolution of the twisted photon detectors can be used for detailed analysis of the matter structure by twisted photons. It should be noted, however, that the design of twisted photon sorters in the x-ray and gamma ranges is still an open problem [36]. As for the x-ray photons with deﬁnite projection of the orbital angular momentum, a triangle aperture can be used as the simplest detector [37, 38].
Can we measure the neutrino mass hierarchy in the sky?<|sep|>In the past decade, there has been great progress in neutrino physics. It has been shown that atmospheric neutrinos exhibit a large up-down asymmetry in the SuperKamiokande (SK) experiment. This was the ﬁrst signiﬁcant evidence for a ﬁnite neutrino mass [1] and hence the incompleteness of the Standard Model of particle physics. Accelerator experiments [2, 3] have conﬁrmed this evidence and improved the determination of the neutrino mass splitting required to explain the observations. The Sudbury Neutrino Observatory (SNO) experiment has shown that the solar neutrinos change their ﬂavors from the electron type to other active types (muon and tau neutrinos)[4]. Finally, the KamLAND reactor anti-neutrino oscillation experiments reported a signiﬁcant deﬁcit in reactor anti-neutrino ﬂux over approximately 180 km of propagation [5]. Combining results from the pioneering Homestake experiment [6] and Gallium-based experiments [7], the decades-long solar neutrino problem [8] has been solved by the electron neutrinos produced at Sun’s core propagating adiabatically to a heavier mass eigenstate due to the matter eﬀect [9]. As a summary, two hierarchical neutrino mass splittings and two large mixing angles have been measured, while only a bound on a third mixing angle has been established. Furthermore the standard model has three neutrinos and the motivation for considering deviations from the standard model in the form of extra neutrino species has now disappeared [11, 12]. New neutrino experiments aim to determine the remaining parameters of the neutrino mass matrix and the nature of the neutrino mass. Meanwhile, relic neutrinos produced in the early universe are hardly detectable by weak interactions but new cosmological probes oﬀer the opportunity to detect relic neutrinos and determine neutrino mass parameters. It is very relevant that the maximal mixing of the solar mixing angle is excluded at a high signiﬁcance. The exclusion of the maximal mixing by SNO [4] has an important impact on a deep question in neutrino physics: “are neutrinos their own anti-particle?”. If the answer is yes, then neutrinos are Majorana fermions; if not, they are Dirac. If neutrinos and anti-neutrinos are identical, there could have been a process in the Early Universe that aﬀected the balance between particles and antiparticles, leading to the matter anti-matter asymmetry we need to exist [13]. This question can, in principle, be resolved if neutrinoless double beta decay is observed. Because such a phenomenon will violate the lepton number by two units, it cannot be caused if the neutrino is diﬀerent from the anti-neutrino (see [10] and references therein). Many experimental proposals exist that will increase the sensitivity to such a phenomenon dramatically over the next ten years (e.g., [14] and references therein). The crucial question we want to address is if a negative result from such experiments can lead to a deﬁnitive statement about the nature of neutrinos. Within three generations of neutrinos, and given all neutrino oscillation data, there are three possible mass spectra: a) degenerate, with mass splitting smaller than the neutrino masses, and two non-degenerate cases, b) normal hierarchy, with the larger mass splitting between the two more massive neutrinos and c) inverted hierarchy, with the smaller spitting between the two higher mass neutrinos. For the inverted hierarchy, a lower bound can be derived on the eﬀective neutrino mass [10]. The bound for the degenerate spectrum is stronger than for inverted hierarchy. Unfortunately, for the normal hierarchy, one cannot obtain a similar rigorous lower limit. Neutrino oscillation data have measured the neutrino squared mass diﬀerences, which are hierarchical. Given the smallness of neutrino masses and the hierarchy in mass splittings, we can characterize the impact of neutrino masses on cosmological observables and in particular on the the matter power spectrum by two parameters: the total mass Σ and the ratio of the largest mass splitting to the total mass, ∆. As we will show, one can safely neglect the impact of the solar mass splitting in cosmology. In this approach, two masses characterize the neutrino mass spectrum, the lightest one, m, and the heaviest one, M. Neutrino oscillation data are unable to resolve whether the mass spectrum consists in two light states with mass m and a heavy one with mass M, named normal hierarchy (NH) or two heavy states with mass M and a light one with mass m, named inverted hierarchy (IH). Near future neutrino oscillation data may resolve the neutrino mass hierarchy if one of the still unknown parameters that relates ﬂavor with mass states is not too small. On the contrary, if that mixing angle is too small, oscillation data may be unable to solve this issue. Analogously, a total neutrino mass determination from cosmology will be able to determine the hierarchy only if the underlying model is normal hierarchy and Σ < 0.1 eV (see e.g., Fig 1). If neutrinos exist in either an Figure 1. Left: constraints from neutrino oscillations and from cosmology in the m-Σ plane. Right: constraints from neutrino oscillations (shaded regions) and from cosmology in the Σ-∆ plane. In this parameterization the sign of ∆ speciﬁes the hierarchy. inverted hierarchy or are denegerate, (and if the neutrinoless double beta decay signal is not seen within the bounds determined by neutrino oscillation data), then the three light neutrino mass eigenstates (only) will be found to be Dirac particles. In this paper, we investigate whether cosmological data may positively establish the degenerate spectrum from the inverted hierarchy (or vice versa). Our approach is to take cosmic variance limited surveys, rather than speciﬁcally planned experiments, so that we can determine if (even in the ideal case) cosmology can make any impact on this question.
The feature of shadow images and observed luminosity of the Bardeen black hole surrounded by different accretions<|sep|>Since the concept of black hole was put forward, people have been trying to ﬁnd this mysterious physical object in the universe. Recently, the ﬁrst image of supermassive black holes at the center of M87∗ galaxy, which was captured by the Event Horizon Telescope Collaboration (EHT), is a strong proof of the existence of the black hole [1–6]. One can observed that a bright ring appears in the background of dark area in the image, where the dark area in the center is called the shadow of black hole and the bright area is photon sphere. It is commonly known that the strong gravity of black hole deﬂects the light and forms the shadow of black hole, which was also known as gravitational lensing eﬀect [7]. With the deepening of the study of black hole, people pay more attention to the observation of black hole shadow, because it will bring new insights and development to the study of black hole [8–40]. In our universe, the real astrophysical black holes in galaxies are surrounded a large amount of high energy radiation material, which makes it possible to indirectly observe the black hole [41]. The accretion ﬂow around a black hole is usually not spherically symmetric, but the simpliﬁed ideal spherical model can provide strong support for the characteristics of black hole observation. It is generally accepted that the shape of the black hole shadow is standard circular geometry in the static spherically symmetric spacetime [42]. In [43], Bardeen pointed out that the radius of shadow for Schwarzschild black hole is rs = 3M, where M is the mass of black hole. Meanwhile, Bardeen also mentioned that the shadow shape of rotating black hole is closely related to angular momentum. In 1979, the simulated image of a black hole is shown for the ﬁrst time, in which the black hole surrounded by optically thin disk was considered [44]. Taking into account the model in which the black hole surrounded by the optically thin spherical accretion, it is found that the shadow is geometric characteristics of spacetime, and has nothing to do with the details of the accretion process [45]. However, the observed luminosity is aﬀected by the accretion ﬂow model [46–50]. When an optically and geometrically thin disk accretion is located on the equatorial plane of black hole, the observational peculiarity of black hole shadow observed by the distant observer have been studied in [51]. Gralla et al. classiﬁed the trajectories of light rays near the black hole, whereby divided the rings outside the shadow area into the direct, photon ring, and lensing ring, and proposed that the observation intensity is dominated by the direct case. Furthermore, they also found that the size of the black center region depends on the speciﬁc emission model of accretion ﬂow. Using the diﬀerent accretion ﬂow models, there is a series interesting observable features about the shadow and rings of the black hole in other gravitational spacetime background, which could refer [52–62]. Most often, the black hole has a singularity inside the horizon. While in [63], the model of a black hole with regular non-singular geometry was proposed by Bardeen. In this system, an energymomentum tensor is introduced, which is interpreted as the gravitational ﬁeld of a nonlinear magnetic monopole charge. Since then, the physical properties of Bardeen black hole has aroused people’s interest, and involve a wide range of quasinormal modes, energy distribution, radiation, thermodynamic behavior and so on [64–77]. Indeed, the study of black hole shadow can explore the basic physical properties of spacetime. Therefore, it is necessary to study the shadow and observation characteristics in the context of Bardeen spacetime. In this paper, we focus on the shadow and observational luminosity of the Bardeen black hole surrounded by various accretion models, i.e., the static, infalling spherical accretion and the optically thin disk accretion, which were regarded as the only background light source. By comparing the results in this work with Schwarzschild spacetime, whereby we can study the inﬂuence of magnetic monopole charge on the shadow and observation characteristics, due to the charge parameter is a remarkable role of Bardeen spacetime. Therefore, it may provide a feasible method to distinguish the Bardeen spacetime from Schwarzschild spacetime. The paper is organized as follows. In section 2, we have studied the associated trajectories of the light ray near the Bardeen black hole, as well as the radius of photon sphere and the shadow of black hole when the state parameters changed. In section 3, we analyzed the shadow image and luminosity of the black hole surrounded by the diﬀerent spherical accretion models. In section 4, we considered thin disk emission near the black hole, and compared the observational appearance with the diﬀerent emission proﬁles. Finally, we discussed our results and conclusions in section 5. In this paper, we use the units G = c = 1.
Mild solutions to the dynamic programming equation for stochastic optimal control problems<|sep|>u σ(X) dW, for t ∈ (0, T ) X(0) = X0 (2) where U is the set of all {Ft}t≥0-adapted processes u : (0, T ) → R+ = [0, +∞] and W : R → R is an 1-D Wiener process in a probability space (Ω, F, P), provided the natural ﬁltration {Ft}t≥0. Here X0 ∈ R, while X : [0, T ] → R is the strong solution to (2). We would like to underline that the studied optimization problem is related to the so called stochastic volatility models, used in the ﬁnancial framework, whose relevance has raised exponentially during last years. In fact such models, contrarily to the constant volatility ones as, e.g., the standard Black and Scholes approach, the Vasicek interest rate model, or the Cox-Ross-Rubistein model, allow to consider the more realistic situation of volatility levels changing in time. As an example, the latter is the case of the Heston model, see [9], where the variance is assumed to be a stochastic process following a CoxIngersoll-Ross (CIR) dynamic, see [10] or [4] and references therein for more recent related techniques, as well as the case of the Constant Elasticity of Variance (CEV) model, see [5], where the volatility is expressed by a power of the underlying level, which is often referred as a local stochastic volatility model. Other interesting examples, which is the object of our ongoing research particularly from the numerical point of view, include the Stochastic Alpha, Beta, Rho (SABR) model, see, e.g., [8], and models which are used to estimate the stochastic volatility by exploiting directly markets data, as happens using the GARCH approach and its variants. Within latter frameworks and due to several macroeconomic crises that have aﬀected diﬀerent (type of) ﬁnancial markets worldwide, governments decided to become active players of the game, as, e.g., in the recent case of the Volatility Control Mechanism (VCM) established for the securities, resp. for the derivatives, market established in August 2016, resp. in January 2017, within the Hong Kong Stock Exchange (HKEX) framework, see, e.g., [12, 13] and 1. h : R → R is convex, continuous and h(u) ≥ α1 |u|2 + α2, ∀u ∈ R, for some α1 > 0, α2 ≥ 0.
Weak associativity and deformation quantization<|sep|>In the canonical formulation of quantum mechanics the physical observables are represented by the hermitian linear operators ˆf acting on the Hilbert space. The composition of quantum mechanical operators in general is non-commutative, ˆf ˆg ̸= ˆg ˆf, yielding the uncertainty principle, but should be necessarilly associative, ˆf(ˆgˆh) = ( ˆfˆg)ˆh. The later property implies two important identities involving the commutator of operators, the Leibniz rule, [ ˆfˆg, ˆh] = [ ˆf, ˆh]ˆg + ˆf[ˆg, ˆh], and the Jacobi Identity, where ˆH stands for the hamiltonian operator. The Leibniz rule and the Jacobi identity guaranty the consistency of the quantum dynamics, meaning that the time evolution will preserve the algebra of physical observables. In particular, if ˆf and ˆg are two integrals of motion, [ ˆH, ˆf] = [ ˆH, ˆg] = 0, the commutator [ ˆf, ˆg] is also an integral of motion, due to (1) and (2). So, the associativity is essential for the consistency of the canonical quantum mechanics. However, some quantum mechanical systems are formulated in terms of non-associative algebras of the canonical operators. The standard exemple of such a situation is the introduction of the magnetic charges through the commutator of the covariant momenta, [ˆxi, ˆxj] = 0, [ˆxi, ˆπj] = iδi j and [ˆπi, ˆπj] = ieεijkBk(ˆx), with div ⃗B ̸= 0, see e.g., [1, 2] for more details. For the Dirac monopole, in particular, one has ⃗B(⃗x) = g⃗x/x3, with g being the magnetic charge. The Jacobi identity is violated only in one point (the position of the magnetic charge). To overturn this diﬃculty one may impose the appropriate boundary condition for the wave function leading to the famous Dirac quantization rule: eg/2πℏ ∈ Z. For the linear magnetic ﬁeld, ⃗B = g⃗x/3, the jacobiator is constant, meaning that one cannot repeat the same trick to ﬁx the problem. Another source of the examples of non-associative structures is the string theory. Recent advances in understanding ﬂux compactiﬁcations of string theory have suggested that non-geometric frames are related to non-commutative and non-associative deformations of space-time geometry [3–7]. Since, these ﬂux deformations of geometry are probed by closed strings, they have a much better potential for providing an eﬀective target space description of quantum gravity than previous appearances of non-commutative geometry in string theory. To give an example of arising a non-geometric background let us consider the closed strings propagating in a three-torus endowed with a constant Neveu-Schwarz ﬂux H = dB. Applying consecutive T-duality transformations along all three directions one obtains the relation between geometric and non-geometric ﬂuxes: H → f → Q → R. The Q-ﬂux background is non-commutative but associative, while the purely non-geometric R-ﬂux background is not only non-commutative, but also non-associative. The presence of non-vanishing three-form H-ﬂux in string compactiﬁcations makes the closed strings coordinates non-commutative and non-associative in the analogy with the coordinates of the open string endpoints attached to a D-brane in a background B-ﬁeld [8–12]. Mainly motivated by the string theory arguments there is a growing interest to the theories on non-geometric backgrounds. In particular, the non-associative quantum mechanics was studied in [13–15]. For recent developments in the area of non-associative ﬁeld theory and non-associative gravity one may see [16–18] and references therein. The aim of this paper is to study the violation of the associativity in the framework of deformation quantization [19]. In this approach to quantum mechanics the physical observables are represented by the functions on smooth manifold, f ∈ C∞(M). To reﬂect the non-commutative nature of the composition low of quantum observables the ordinary point-wise multiplication of functions is substituted by the star multiplication, f · g → f ⋆ g, satisfying some natural restrictions described in the Sec. 2 and 3. In particular, star products representing diﬀerent quantization prescriptions of the same classical system should be related by the gauge transformation, see Sec. 3 for details. To deﬁne the non-associative star products we are looking for the condition which would be gauge invariant and include the associative star products as a particular case. We imply the requirement that the star associator of any three elements should vanish whenever two of them are equal. The star product satisfying this condition is called alternative. Any associative multiplication is automatically alternative. We show that the proposed condition is invariant under the gauge transformations in a sense of a deformation quantization. The alternative multiplications enjoy an important properties, like the Moufang identities, which can be used in physics. In particular, in non-associative quantum mechanics one may use the Moufang identities for the deﬁnition of states and uncertainty relations [14]. On the other hand the Moufang identities for the star product the star imply the Malcev identity on the commutator restricting the non-associative algebras which can be treated with the help of the alternative star product, see the discussion in the Sec. 4. On the classical level, the Malcev identity is equivalent to the Malcev-Poisson identity for the corresponding bracket. In Sec. 5 we discuss the classical dynamics on the MalcevPoisson manifold. We introduce the modiﬁed bracket and show that there is a weaker analogue of the classical Poisson theorem, the modiﬁed bracket {f, g}H of two integrals of motion f and g is an integral of motion once one of them is manifestly time independent, ∂f/∂t = 0. So, the Malcev-Poisson identity can be useful to construct the integrals of motion. The non-trivial example of the Malcev algebra is the algebra of the imaginary octonions, described in the Sec. 6. For this algebra we obtain an explicit formula for the alternative and non-associative star product. In the Sec. 7, we discuss the deformation quantization of the Malcev-Poisson structures of the general form. First, following our previous work [20] we describe the construction of non-associative weakly-Hermitian Weyl star product. Then we prove some important properties of the alternative Weyl star products and deriva the lower order formula for it. Finally, in the Sec. 8, we discuss the deﬁnition of the trace functional (integration) on the algebra of the alternative star product and show that the integrated associator vanishes.
Scintillation properties of pure and Ce$^{3+}$-doped SrF$_2$ crystals<|sep|>The interest in new scintillation materials is promoted by increasing number of new applications in medicine, science, and homeland security, which require ramp-up of material production. The most perspective scintillators are bromides and iodides doped with Ce3+ and Eu2+ ions, such as SrI2-Eu and LaBr3-Ce. These crystals have high light outputs (up to 100000 photons/MeV for SrI2-Eu), good energy resolution, and high proportionality (Dorenbos, 2010). Disadvantages of these scintillators are high hygroscopic and price. In addition, SrI2-Eu has temperature instability of light output observed by Alekhin et al., 2011. For the most applications a cheaper NaI-Tl scintillator has quite properties (light output about 45000 photons/MeV cited in Derenzo, 2012). Therefore, one of the way in development of new scintillators is to ﬁnd new materials with similar to NaITl properties but no hygroscopic. In this way advanced materials for new scintillators are alkali-earth ﬂuorides doped with rare earth ions. Theoretical limit of light output for them is up to 50000 photons/MeV (Dorenbos, 2010). If an eﬃcient energy transfer is provided then alkali-earth ﬂuorides will be promising scintillators. A real light output of CaF2-Eu is 1800022000 photons/MeV, but BaF2 and BaF2-Ce crystals demonstrate lower light output at about 10000 photons/MeV (Visser et al., 1991). Scintillation properties of SrF2 crystals are almost not investigated. Light output of SrF2 was estimated about 10000-12000 photons/MeV by Schotanus et al., 1987. However, potential light output of SrF2 will be higher. Also SrF2 crystals doped with Ce3+ and Pr3+ have a temperature stability of light output in wide range (20 ◦C to 200 ◦C) (Shendrik and Radzhabov, 2010). Therefore, SrF2 can be high-potential scintillator for well-logging. So, scintillator properties of strontium ﬂuoride crystals are among the least studied of ﬂuorides crystals, but these crystals have potential applications. Thus, the investigations of scintillation properties of strontium ﬂuorides are topical today. This paper describes the scintillation properties of pure and cerium doped strontium ﬂuorides crystals, a newly discovered inorganic scintillator.
Interplay between destructive quantum interference and symmetry-breaking phenomena in graphene quantum junctions<|sep|>Quantum-interference (QI) eﬀects in the electron transport in nanostructures are a direct evidence of the particle-wave duality of electrons, which is deeply rooted in the fundamentals of quantum mechanics. From a theoretical point of view, it is well established that ballistic electron transport in molecular junctions characterized by multiple transmission paths displays clear signatures of QI. The prototype of completely destructive QI is the meta-benzene molecular junction.1–3 In the classical interpretation, QI emerges when electrons propagating through two diﬀerent spatial paths along the shortand the long-arms of the ring acquire a phase diﬀerence ∆φ=π,2,3 yielding a complete cancellation of the transmitted wave amplitude. Interestingly, this view was recently challenged4 in favor of a diﬀerent interpretation, where the antiresonance is a consequence of interference in energy space between diﬀerent molecular orbitals. Independently of its origin, the presence of a QI antiresonance close to the Fermi level drastically inﬂuences the transport properties of quantum junctions and results in huge ON/OFF ratios, which can be exploited for the realization of transistors2,3 or spin ﬁlters,5–9 nanocircuitry,10 and to enhance the thermoelectric performance11 of nanoelectronic devices with organic functional units. Recently, experimental evidence of destructive QI was clearly observed in molecular junctions involving benzene,12 terphenyl,13 anthanthrene14, antraquinone,15, fullerenes and porphyrins,16 as well as several other molecules with an organic backbone.17 Sharp resonances in the diﬀerential conductance, the ﬁngerprint of destructive QI, has been clearly detected even at room temperature.18,19 In some cases, the agreement between experiments and density functional theory calculations,14,17,20 tiresonances can be regarded as robust features of quantum junctions, thus paving the way towards the realization of atomic-scale engineered quantum coherent devices. Poly-phenyl molecular systems, or, more generally, alternant hydrocarbons with delocalized π orbitals represent the natural platform for QI eﬀects. Remarkably, graphene nanostructures also fall into this category. Indeed, recent experiments reported QI patterns in graphene nanoconstrictions,21 or bridges,22 and break junctions,18,19,23 What is more important, quantum junctions with graphene functional blocks beneﬁt from all the extraordinary properties of graphene. Their chiral nature enables the manipulation of spin24 and valley25–29 degrees of freedom, while appropriate engineering of the substrate and gating oﬀer the possibility to realize superlattices30 and to tune the properties of the junction. Furthermore, the presence of edges and reduced dimensionality oﬀer the possibility to enhance correlation effects and to induce magnetic order, absent in pristine graphene,24,31–33 thus paving the way to a wide range of applications. Very recently, for instance, edge magnetism was stabilized in graphene nanoribbons functionalized with stable magnetic radical groups, demonstrating spin coherence times in the range of microseconds at room temperature.24 The present work is related to all these aspects. By means of numerical calculations and a detailed symmetry analysis, we show that QI eﬀects can be used to control spin and valley polarization of ballistic transport in graphene quantum junctions up to room temperature in the absence of external magnetic ﬁelds. In particular, we show that both spin ﬁltering and valley ﬁltering can be achieved in the same device by simply tuning the coupling with a substrate to switch the nature of the site-site correlations in the functional element between ionic and antiferromagnetic. The paper is organized as follows. In Sec. II we discuss the model and the methods used to tackle the problem of correlated transport and QI eﬀects in graphene nanostructures. In Sec. III we discuss the interplay between destructive QI and diﬀerent kinds of symmetrybreaking, and we provide a uniﬁed description of the phenomenon. In Sec. IV we explore the occurrence of the QI anti-resonances from a Green’s function perspective, which allows us to pinpoint their origin. Finally, Sec. V contains our conclusions and an outlook.
Darboux Transformation and Exact Solutions of the Myrzakulov-Lakshmanan-II Equation<|sep|>During the past decades, there has been an increasing interest in the investigation of integrable classical and quantum systems. The theory of integrable systems is an important branch of nonlinear science. Integrable systems describe various kinds of nonlinear phenomena, such as soliton signals, soliton waves and etc. At the same time, the soliton theory gives many methods of ﬁnding exact solutions of nonlinear ordinary and partial diﬀerential equations of modern mathematical physics. Constructing exact solutions of such nonlinear diﬀerential equations is a hard job. However, during the past decades, some methods to ﬁnd exact solutions are proposed, such as Hirota method, Backlund transformations, Darboux transformations and etc. In this paper, we will construct the Darboux transformation (DT) for the Myrzakulov-Lakshmanan-II equation (ML-II equation) and using the DT, some exact soliton solutions of this equation are found. Note that the DT for some other Heisenberg ferromagnetic equations (HFE) were constructed in [1]-[4] (see also Refs. [5]-[35]). The paper is organized as follows. In section 2, the ML-II equation and its Lax representation are introduced. In section 3, we derived the DT of the ML-II equation. Using the one-fold DT, some exact soliton solutions are derived in section 4. Section 5 is devoted to conclusion.
Radio polarimetry of compact steep spectrum sources at sub-arcsecond resolution<|sep|>The number of Compact Steep-Spectrum (CSS) sources with detailed polarimetric information available at subarcsecond resolution is still small. We have conducted a series of polarimetric observations of CSS sources using the Very Large Array (VLA). CSS objects are young radio sources, with ages < 103−5 yr (Fanti et al. 1990). They have linear sizes ≤ 20 kpc 1 and steep high-frequency radio spectra (α > 0.5; Sν ∝ ν−α). Being sub-galactic in size, CSS sources reside within their host galaxies. Therefore, Faraday rotation effects are to be expected when their polarised synchrotron emission is observed through the magneto-ionic interstellar medium (ISM) of the host galaxy. The comparison of polarised emission over a range of wavelengths is an important diagnostic of the physical conditions within and around these compact radio sources. Existing sub-arcsec polarimetry has provided evidence in favour of the interaction of components of CSSs with dense gas clouds, (for example, see Junor et al. 1999). Send oﬀprint requests to: Franco Mantovani e-mail: fmantovani@ira.inaf.it 1 H0 = 71 km s−1 Mpc−1, Ωm = 0.27, Ωvac = 0.73 Aiming at a deeper understanding of the CSS source phenomenon, with the VLA 2 A-Array we observed an “incomplete” sample of 29 sources selected from the list of Dallacasa (1990). The adopted selection criteria were: i) total ﬂux density at 5 GHz >1 Jy, ii) declination > −20◦, and iii) lack of observations at sub-arcsecond resolution (at the time of source selection). In this paper, we report multi-frequency VLA polarisation observations of our CSS sample at 8.1, 8.5, 15 and 23 GHz. In Section 2 we summarise the observations and data processing. Section 3 describes the new information obtained on the structural and polarisation properties. Discussion and conclusions are presented in Sections 4 and 5 respectively.
Rapid estimation of drifting parameters in continuously measured quantum systems<|sep|>The problem of accurately measuring unknown parameters in an experimental system is of both fundamental and practical importance. The process of determining the parameters of the experiment serves as a valuable calibration of the experiment, and also determines the limitations of experimental accuracy. The understanding of how to minimize parameter uncertainties given a variety of possible measurement strategies has been developed into the science of quantum metrology over the past several decades [1, 2]. The usual approach taken in the laboratory is to repeatedly perform the following sequence of operations: prepare a quantum state, let it evolve unitarily in a way that depends on the unknown parameter, and then perform a (potentially unsharp) measurement to extract information about the parameter of interest. The concept of the quantum Fisher information [3] is fundamental to this approach, since it sets the bound on the minimum variance for all possible estimation strategies based on this ﬁnal measurement, the so-called Cram´er-Rao bound. However, there are other methods for estimating such an unknown parameter that go beyond the prepare-evolvemeasure paradigm, which may be beneﬁcial under certain circumstances. One such scenario is when the parameter changes slowly in time such that this variation cannot be predicted in advance, as is common in experimental laboratories (e.g., from thermal ﬂuctuations). In this case, it is beneﬁcial to be able to continuously track the changing parameter as it evolves in time, to sense when and how it is changing. Given knowledge of how the parameters are changing, introducing feedback control to stabilize the parameter then becomes possible [4]. Such a situation brings into play the physics of open quantum systems and how they relate to metrology [5–9]. To continuously track the changes of a parameter in time, it is natural to consider measurements that are also continuous in time. In order to increase the speed of the estimation, the technique builds upon prior parameter information obtained from an initial broad system characterization. Assuming relatively slow drift of the parameter, the initial characterization narrows the search region of subsequent repeated estimations in real-time as a single noisy measurement record is monitored. Previously, the physics of parameter estimation using continuous measurements has been analyzed by Ralph, Jacobs, and Hill [10]. They numerically integrated a stochastic master equation to estimate the unknown frequency of a Hamiltonian drive for a qubit. Our work is closely related to theirs, and builds oﬀ of it. Klaus Mølmer and collaborators have also developed parameter estimation methods using continuous quantum measurements. These relate to how the parameter estimation can be carried out by Bayesian estimation in solving stochastic master equations [11, 12], and how Fisher information is degraded in the quantum Zeno regime [13]. The potential use of continuous measurements for quantum state tomography is also starting to be explored [14–16]. In this work, we consider a quantum system undergoing such a continuous measurement, which gives rise to a stochastic measurement record that can be monitored in time. We wish to analyze this data to extract the value of an unknown parameter in a way that is both computationally rapid and statistically eﬃcient to permit estimation on a short enough time scale where feedback control becomes possible to correct drift. For speciﬁcity, we focus on the determination of an unknown and drifting Rabi drive for the qubit. There are a number of open problems in this area which we now consider: How can one minimize the complexity of maximum likelihood algorithms so they are computationally fast? Is it possible to work only with raw measurement data, so numerical implementations of quantum ﬁlters that estimate the quantum state dynamically are not needed? Is it possible to incorporate numerically more eﬃcient methods to narrow down the parameter search space? Can existing methods be generalized to account for experimental nonidealities, such as additional dephasing, detector ineﬃciency and energy relaxation? In this paper we work toward solutions to these problems, and outline how they can be experimentally implemented in superconducting circuits, among other possibilities. Continuous measurement with superconducting circuits have a proven ability to accurately track the quantum state in time [17] with excellent agreement with predicted statistics [18]. Our basic insight is to speed up the estimation protocol by avoiding the numerical integration of the stochastic master equation. Rather, we construct eﬀective propagators directly from the observed measurement record that can be used in the maximum likelihood algorithm. These eﬀective propagators use measurement operators with the sequence of digitized measurement results from, e.g., a homodyne measurement, together with unitary matrices with unknown Rabi drive frequency. These can all be evaluated numerically using the particular realization of the stochastic measurement results. By simple multiplication of the composite matrices in the eﬀective propagator, a suitable likelihood function is straightforwardly constructed with the initial state, which can then be maximized. Such maximum likelihood estimation (MLE) saturates the Cram´er-Rao bound. Combining this MLE technique with an initial fast Fourier transform (FFT) technique, (which identiﬁes a range of prior Rabi frequencies) provides a signiﬁcant speed up by decreasing the number of trial frequencies needed for MLE. We further generalize this method to incorporate realistic non-idealities to prepare this method for experimental implementation in superconducting circuit architectures, where continuous homodyne and heterodyne measurement are now routinely carried out. We believe this method will be suitable to be directly programmed into a ﬁeld-programmable gate array (FPGA) for rapid nearreal-time implementation. Finally, we illustrate how this method can be applied to a time-varying, unknown Rabi drive, and show that we can accurately track even irregular motion in time. Notably, a projective measurement version of this task has been accomplished by Shulman, Harvey, Nichol, et al. using such an FPGA in a triplet/singlet spin qubit in order to detect how the surrounding nuclear magnetic ﬁeld was changing, and incorporate feedback to prolong the qubit dephasing time [19]. We thus present our own analysis of the projective equivalent with ﬁxed spacing between measurements in the Appendix as a comparison to the present work. The paper is organized as follows. In Sec. II, we outline our strategy for determining the value of a static Hamiltonian parameter by maximizing the likelihood of observing a particular stochastic measurement record. We focus on the example of a driven qubit, where the param eter to be estimated is an initially static Rabi frequency. We then compare the maximum likelihood approach to a simpler FFT, which can be used to help identify a suitable frequency range for subsequent maximum likelihood estimation, and comment on the relative computational eﬃciency of each method. In Sec. III we generalize the static estimation method to a dynamic estimation method that is able to track arbitrary time-dependent parameters. Setting a desired estimation precision then speciﬁes the time resolution for the tracking of drift. We demonstrate that we are able to accurately track dynamical parameters using this method. We conclude in Sec. IV. We also provide an Appendix that includes an analytic treatment of the maximum likelihood frequency estimation using periodic projective measurements, for handy comparison to the continuous case.
Ionised gas abundances in barred spiral galaxies<|sep|>With the exception of few primordial light elements, stellar nucleosynthesis is responsible of the secular metal enrichment in galaxies. The ﬁnal metallicity distribution within a galaxy is reasonably well reproduced by chemical evolution models considering the appropiate star formation history together with episodes of gaseous inﬂow and outﬂows (e.g. Portinari & Chiosi 1999; Chiappini et al. 2000; Prantzos 2008, Colavitti et al. 2009). However, the metallicity gradients can be futher modiﬁed by dynamical processes (e.g., Roskar et al. 2008, Sch¨onric & Binney 2009; S´anchez-Bl´azquez et al. 2009). It is, therefore, evident that observing how metals are distributed in a galaxy should highly constrain its evolution. However, the detailed dynamical processes responsible for the modiﬁcation of the metallicity distribution and their importance are not yet well understood. In particular, it is not known, from the observational point of view, the importance that bars might have in producing the metallicity radial mixing. Bars are believed to aﬀect the overall dynamics of the galaxy and are a well known mechanism to induce secular evolution (Athanassoula 2003; Pfenniger & Friedli 1991). For instance, in a previous work (P´erez & S´anchezBl´azquez 2011, hereinafter, Paper II) we hinted that the bulges of early-type barred galaxies show diﬀerent stellar enrichment his tories compared to the bulges of their unbarred counterparts (see also Ellison et al. 2001 for a similar conclusion for gas-phase metallicities). Gas and stars respond to the gravitational potential due to their own nature, being viscosity and magnetic ﬁelds only important in the former component. The highly asymmetric bar potential induces diﬀerential radial motions. While the gas component, being highly dissipative, suﬀers from the gravitational torque of the non-axisymmetric mass component, the stars are mainly aﬀected by orbital mixing. Furthermore, studies of the gas-phase abundances provide with present-day snap-shots of the interstellar medium abundance. On the other hand, the study of stellar abundances provide archeological clues as to the formation and evolution of the bar. Therefore, the study of stellar and gas metallicities are of crucial importance to interpret the processes dominating galaxy evolution. It is expected that, if gas is injected into the interstellar medium (ISM) from stellar outputs, it would generally be more metal-rich than if it has an external origin. Deriving the disk radial distribution of stellar abundances from spectroscopic measurements is diﬃcult due to the low surface brightness of the disk and the contamination of the emission lines from ionised gas. Few works have tried to overcome these diﬃculties and have derived the radial distribution for a small number of galaxies (e.g. Yoachim & Dalcanton 2008; MacArthur, Gonz´alez & Courteau 2009; S´anchez-Bl´azquez et al. 2011). The stellar metallicity gradients along the bars of the 20 early-type galaxies presented here were carried out in previous works (P´erez et al. 2009, hereinafter, Paper I) and provided interesting results regarding the radial distribution of the stellar parameters in the bar region. As for the nebular gas abundance distribution, for most spiral galaxies, both barred and unbarred, negative radial metallicity gradients along the disk have been obtained for the gas component (e.g. Bresolin et al. 2009). These gradients seem to be shallower in the case of barred galaxies than in unbarred galaxies, and larger for late-types than for early ones (Pagel & Edmunds 1981; Alloin et al. 1981; Vila-Costas and Edmunds 1992; Zaritsky et al. 1994). The issue is not closed yet, so work is being done to investigate the variation of the gradient with radius (Bresolin et al. 2009 obtain a higher gradient in the central part of M83, and ﬂatter in the outer disk, beyond 1.2 R25) and also in azimuth (Balser et al. 2011, for the Milky Way). Simulations explain the shallower gradient value in the outer part than in the inner side of a galaxy based on radial mixing processes associated to a strong bar (e.g. Considere et al. 2000; Zahid & Bresolin 2011; Friedli et al. 1994; Friedli 1999). In this work, we will focus on the analysis of the gas metallicity, in the bar region of the galaxies presented in Papers I and II. Only a few works have considered both, nebular and stellar metallicities. A pioneering paper in this study is that of StorchiBergmann et al. (1994). They found a good correlation between stellar and gas metallicity investigating through the correlation between the oxygen abundance in the nebular component, and the absorption-line equivalent width W(CIVλ1550) (as a tracer of stellar metallicity) in a sample of 44 star-forming galaxies. They concluded that galaxies with lower values of metallicity appear to experience instantaneous outbreaks of star formation, but those with higher metallicity are probably experimenting an ongoing stellar formation. Annibali et al. (2010) estimated gas abundance gradients in early-type galaxies, and they compared these results with their stellar metallicities (Annibali et al. 2007), concluding that the gas metallicity tends to be lower than the stellar one, beeing this eﬀect higher for higher metallicities. In this paper, we present an analysis of the radial oxygen abundance distribution in the bulge and bar region of a sample of 20 early type barred galaxies, with and without nuclear activity. The stellar component of this sample has been studied in Papers I and II. In Sec. 2 observations and data reduction are described. The method to analyse the spectral lines to estimate their ﬂux is presented in Sec. 3. In Sec. 4 we show the diagnostic diagrams and derive the abundance distributions with diﬀerent methods. A comparison of the nebular and stellar metallicity gradients is also presented in this section. The results are discussed in Sec. 5.
Hiding neutrino mass in modified gravity cosmologies<|sep|>Recent cosmological observations have brought upon us the era of precision cosmology. To challenge the current standard cosmological model seems to require very precise cosmological parameters determinations, as all current available observations are consistent with the simplest ΛCDM model [1]. However, the current model is just a convenient phenomenological description of the Universe as it gives no insight on the nature of the individual energy components like dark matter and dark energy. Most likely before major breakthroughs in our understanding of the Universe come through, precision cosmology should verify yet undetected, small eﬀects, corresponding to standard expectations. Among them, the eﬀect of neutrino masses on large-scale structure is the most promising candidate to verify cosmology at the sub-percent level. Is there any chance for surprise? This has been addressed in a series of works which involve a plethora of modiﬁed cosmological models (for a review, see [2, 3]) where some speciﬁc piece of the extended model mimics the impact of neutrino masses. Modiﬁed gravity models stand as the most promising alternative to the current paradigm (see e.g., [4–6]). The impact of deviations of Einstein gravity on the determination of neutrino masses has been studied and analysed both in the linear and nonlinear regime [7–9]. Most often, the extended gravity models are speciﬁc and simpliﬁed scenarios which avoid the exploration of large parameter spaces in time-consuming simulations and/or analysis. The outcome of these studies typically shows a qualitative understanding of the inﬂuence of the modiﬁed model’s parameters in the adopted neutrino mass bound. In this paper, we make a more general characterization of the inﬂuence of modiﬁed gravity models on the determination of the neutrino mass. We characterize and analyse fully general massive neutrino scalar-tensor (Horndeski) cosmologies, for the ﬁrst time, working with the eﬀective theory and observations in the linear regime [10]. The modiﬁed gravity models are very generally characterized by a minimal number of given functions, set by a limited number of parameters. The redshift dependence of these functions is driven by searching for the largest impact on the neutrino mass constraints. In this framework, we can address the questions: “where is the degeneracy between neutrino mass and a modiﬁed gravity model hidden?” and “how could it be partially resolved?”. Moreover, we can quantitatively characterize the knowledge of neutrino mass in the general models under scrutiny. Our results can be directly applied to theoretically motivated tensor-scalar gravity theories by matching the functions of the eﬀective theory to those used here (see §2).
Particle Acceleration in Collapsing Magnetic Traps with a Braking Plasma Jet<|sep|>Magnetic reconnection is thought to play an important role in releasing energy stored in magnetic ﬁelds in multiple environments, including solar ﬂares (e.g.
An experimental data-driven mass-spring model of flexible Calliphora wings<|sep|>The wings of an insect are hundreds of times lighter than its body, yet they sustain dynamic loads that exceed the body weight. Consequently, they deform signiﬁcantly during ﬂapping ﬂight. To deal with these large deformations, insects have evolved highly compliant wings from which they beneﬁt in many aspects: enhanced aerodynamic eﬃciency [1, 2], ﬂight stability [3], enhanced ﬂight control [4], damage resistance [5], robustness to collisions [6], to name a few. For understanding the aerodynamics of insect locomotion, the ﬂuid–structure interaction problem must be addressed by coupling ﬂuid with solid mechanics. Computational methods yield insight into the instantaneous ﬂow ﬁeld surrounding the studied insect and with access to all aerodynamic quantities, which are diﬃcult to obtain in experiments. Thus fundamental mechanisms behind the nonlinear dynamics of the ﬂow can be revealed. However, numerical studies of insect ﬂight are not trivial due to their high complexity. For simpliﬁcation, studies are usually employing either completely rigid wings (e.g., [7–9]), or prescribed time-varying deformation [2, 10]. Fully coupled ﬂuid-structure interaction simulations of ﬂapping insect wings are still challenging and give controversial results. While some studies found advantages of wing ﬂexibility on aerodynamic performance of insects [1, 11–14], others reported negative impact on lift production [15–17]. The anisotropy and inhomogeneity of the elastic properties of wings are clearly important factors in these studies. During ﬂight, the architecture and material properties of insect wings determine predominantly their deformations, which are mostly passive [18]. Therefore, determining wing stiﬀness is critical to the modeling of insect wing dynamics [19, 20]. In combination with other functional requirements such as wing folding, hemolymph transport, etc., evolution has led to complex designs with individual shapes and sizes of veins, diﬀerent types of hinges, resilin patches and varying thickness of the membrane. As a matter of fact, Young’s modulus of insect wings may change from tens to hundreds of MPa between species or even diﬀerent parts of the wing [21]. Measurement conditions play also a crucial role in determining the wing stiﬀness due to wing desiccation. Altogether, the distribution of ﬂexural rigidity in insect wings and its eﬀects on wing dynamics are still poorly known. In the past, only few numerical studies took into consideration these complex structures of wings. Combes and Daniel [22] measured the overall ﬂexural stiﬀness EI either in spanwise or chordwise directions by assuming that wings were homogeneous beams. The data were then used in a simpliﬁed ﬁnite element model of a Manduca wing. Nakata and Liu [1] and Tobing et al. [14] also set the parameters for their ﬂexible wing models based on the measurements of Combes and Daniel. Nakata and Liu proposed an anisotropic hawkmoth wing model. On the other hand, Tobing et al. considered a 3D ﬂexible wing model of bumblebees with uniform and reduced-tip stiﬀness. Ishihara et al. [23] employed a model composed of a rigid leading edge connected with a rigid plate through a plate spring. The torsional stiﬀness of the latter was deﬁned based on dynamic similarity. Nguyen et al. [24] modeled a fruit ﬂy wing where sharp variations in material properties of stiﬀ veins and soft adjacent membrane were taken into account. In our previous work [20,25], numerical simulations of bumblebees with ﬂexible wings using the open source framework FLUSI [26] have been carried out. We studied the impact of wing ﬂexibility on aerodynamic forces by varying the Young’s modulus E of the vein system in a chosen range. Even though these contributions succeeded to include the venation structure in their wing models, the identiﬁcation of the ﬂexural rigidity for both the veins and the membranes remains a daunting task. In the present paper, we propose a numerical method for estimating wing stiﬀness. It consists of a relatively simple mass-spring model based on the wing planform and venation pattern measured from photographs of the blowﬂy (Calliphora vomitoria). The stiﬀness parameters are now optimized considering acquired experimental data of real insect wings using a genetic algorithm with covariance matrix adaptation strategy. This approach ensures that the model has the same behavior as the real wing specimens in static bending tests. The wing model with the optimized stiﬀness is then used for three-dimensional unsteady FSI simulations of ﬂapping wings at high resolution on massively parallel computers. The remainder of the manuscript is organized as follows. In the material and methods section 2, we describe the wing model, the experimental as well as the mathematical methods that are used. The numerical results are then discussed in section 3, starting with the validation of the optimization. Finally, conclusions of the study are drawn in section 4 and some perspectives on future work are given.
Risk Measurement, Risk Entropy, and Autonomous Driving Risk Modeling<|sep|>According to the deﬁnition of Wikipedia, an autonomous vehicle (AV), also known as a self-driving car, connected and autonomous vehicle (CAV), is a vehicle that is capable of sensing its environment and moving safely with little or no human input. In May, 2021, PR Newswire reported “The global autonomous car market is expected to reach 1383.89 billion dollar in 2025 at a CAGR of 14%.” Since 2019, the newly launched car models of major automobile manufacturers have been AVs above Level 2 (see SAE J 3016-2018), and autonomous vehicles are becoming more and more popular. Many proponents of self-driving, including users, automakers, technicians and specialists, claim that autonomous vehicles can eliminate most crashes and promote traveling safety greatly, hence insurance companies should oﬀer special premium discount. However, in reality, most actuaries and insurers associate selfdriving with huge risks and uncertainties. AVs are not only ineligible for premium discount, but also charged
Multiple-isotope pellet cycles captured by turbulent transport modelling in the JET tokamak<|sep|>In present tokamaks, particle fuelling is mainly provided by neutral gas puﬃng from the plasma periphery and from Neutral Beam Injection (NBI). Gas fuelling may be rendered ineﬀective in future reactors due to increased neutral opacity, while the particle source from the NBI will be relatively small. A viable alternative as a primary fuelling technique is the injection of cryogenic pellets [1], with higher penetration and faster response times. Pellet mass, injection speed and frequency can be jointly adjusted to optimize the particle source and provide fuelling in the plasma core, where the pellet is ablated. In ITER, for example, pellets of mass between 2 and 5.5 · 1021 atoms with frequency between 1.5 and 3.5 Hz respectively should be suﬃcient to maintain the density required for a Q=10 baseline ELMy H-mode scenario at 15 MA. Active research on pellet fuelling focuses on its compatibility with integrated plasma scenario constraints, including control of MagnetoHydroDynamic (MHD) modes such as Edge Localised Modes (ELMs), plasma exhaust, core turbulent transport, and desired isotope composition. Previous integrated tokamak plasma simulation (integrated modelling) including pellets focused on various aspects of the pellet cycle: improved conﬁnement regimes [2, 3, 4], edge and fuelling requirements [5], the impact of fuelling on divertor heat-loads [6, 7] and the extrapolation of pellet penetration and transport [8]. Pellet fuelling and simultaneous ELMs mitigation have been demonstrated experimentally [9, 10, 11], ensuring the viability of this fuelling method. Regarding turbulent transport, the pellets have a signiﬁcant impact. During the ablation phase of the pellet cycle, the density and temperature proﬁles are transiently modiﬁed, changing the micro-instability properties of the discharge. While the heightened negative radial density gradient that develops in the region outside the pellet ablation region is expected to destabilize Trapped Electron Modes (TEM) and lead to a strong outward particle ﬂux [12], the positive density gradient that develops at radii within the ablation location may stabilize Ion Temperature Gradient (ITG) driven turbulence. This was observed for example in the Mega Amp Spherical Tokamak (MAST) [13]. The stabilization was instead counteracted by a larger R/LT again in MAST, with diﬀerent plasma conditions, [14] and in a similar Joint European Torus (JET) experiment [15]. R here is the major radius, while LT is the logarithmic temperature gradient LT = [∂(lnT )/∂r]−1. In reactors, pellet injection with varying isotope ratios will be used to maintain the desired concentrations of deuterium and tritium in the core; equal ratios ensures maximal fusion power, and burn control is achieved by modifying the relative isotope concentrations. Understanding the timescales for the transport of diﬀerent isotopes following modiﬁcation of the pellet isotope composition is fundamental for understanding and predicting burn control. Since the electron and ion particle ﬂuxes must always be equal (ambipolarity), diﬀerences in their transport can only be observed experimentally in presence of multiple ion types, e.g. hydrogenic isotopes. Previous experiments observed a fast mixing of T-trace in the Tokamak Fusion Test Reactor (TFTR) [16], while modelling found helium diﬀusion coeﬃcients on the order of the eﬀective heat conductivity in ITER simulations [17]. Theoretical analysis recently explained the fast isotope mixing by Di/De > 1 and |Vi| > |Ve| in ITG dominated plasmas [18], where Ds and Vs are the species dependent diﬀusion and pinch coeﬃcients respectively. In a multiion plasma the diﬀerent ions can interchange at diﬀerent timescales to the electron particle transport. The opposite relation holds for TEM dominated regimes, as shown experimentally in the Large Helical Device (LHD) [19]. Previous multiple-isotope experiments at JET allowed a detailed investigation of ion particle transport [20], suggesting fast isotope mixing. Those experimental observations were successfully reproduced in stationary-state, multiple-isotope integrated modelling [21], applying the quasilinear gyrokinetic transport model QuaLiKiz [22, 23], strengthening QuaLiKiz validation in multiple-isotope regimes. The fast mixing will be most prevalent during transient states, such as during pellet injections, due to the signiﬁcant modiﬁcations of the local density gradients created by the short ablation time of the pellets. Modifying the pellet isotope ratio compared to the background isotope ratio rapidly changes the core isotope mix without aﬀecting the time averaged electron proﬁle. An experiment was performed at JET precisely with the aim of using pure deuterium pellets to control the core isotope ratio, starting from a pure hydrogen plasma [24], and is investigated next. Relevant parameters of the discharge under investigation are shown in table 1. In this experiment the size of the pellets, scaled to the plasma volume, lead to shallow deposition and transient inverted density proﬁle, similarly to what is expected in ITER. ∼ 10% of the pellet was ablated in the pedestal region, between 0.95 < ρ < 1.0, where the ad-hoc pedestal model is used. Most of it was instead ablated inside the pedestal top, where the transport is predicted by QuaLiKiz, with ∼ 88% between 0.6 < ρ < 0.95”. ρ here indicates the normalised toroidal ﬂux coordinate ρtor = ( ψtor 2 . The experiment managed to reach the desired core isotope composition, measured by Balmer-alpha Charge Exchange (CX) spectroscopy and D-D neutron rate. A rapid increase in the neutron rate following the initial pellet injection was observed. The delay between the start of the pellet ablation and the local peak in the neutron rate was ∼ 100ms, which is comparable with the energy conﬁnement time, ∼ 120ms for this experiment. This timescale is much faster than the particle conﬁnement time, which is ∼ 600ms, and indicated fast isotope mixing. In this interpretive analysis, the isotope particle transport coeﬃcients were determined by interpretative modelling, using the semi-empirical Bohm/Gyrobohm turbulent transport model and matching the transient response of the thermal D-D neutron rates [9]. The key observation was that DD/χeff ∼ 1 was inferred at the beginning of the pellet train, where DD is the diﬀusion coeﬃcient for Deuterium and χeff is the eﬀective heat conductivity. Since De/χeff ≪ 1 is expected in the experiment, this ﬁnding implies a large DD/De, indeed consistent with the fast isotope mixing. This paper demonstrates, for the ﬁrst time, that multiple pellet cycles and the associated fast deuterium penetration can be captured by turbulent transport models within an integrated modelling framework, in an ITER-relevant pellet deposition regime.
Automatic Segmentation, Localization, and Identification of Vertebrae in 3D CT Images Using Cascaded Convolutional Neural Networks<|sep|>Automatic segmentation, localization, and identiﬁcation of individual vertebrae from 3D CT (Computed Tomography) images play an important role in a preprocessing step of automatic analysis of the spine. However, many previous works are not able to perform segmentation, localization, and identiﬁcation simultaneously and require a priori knowledge of which part of the anatomy is visible in the 3D CT images.
Space-compatible cavity-enhanced single-photon generation with hexagonal boron nitride<|sep|>Near-future optical quantum information processing[1] relies on sources of pure and indistinguishable singlephotons. Promising candidates include quantum dots[2], trapped ions[3], color centers in solids[4] and singlephoton sources (SPSs) based on heralded spontaneous parametric downconversion[5]. The recent discovery of ﬂuorescent defects in two-dimensional (2D) materials has added yet another class of quantum emitters to the solid state color centers. Stable quantum emitters have been reported in the transition metal dichalcogenides WSe2[6– 10], WS2[11], MoSe2[12] and MoS2[13]. The optical transition energies for these emitters, however, are located in close vicinity to the electronic band gap. Thus, cryogenic cooling below 15 K is required to resolve the zero phonon lines (ZPLs). For room temperature quantum emission, defects hosted by large band gap materials are ideal, as has been demonstrated in 2D hexagonal boron nitride (hBN)[14–16]. In this case, the energy levels introduced by the defects into the band structure are well isolated. The large band gap of 6 eV[17] also prevents non-radiative decay, which in turn allows for high quantum eﬃciencies. Unlike for solid state quantum emitters in 3D systems, the 2D crystal lattice of hBN allows for an intrinsically high extraction eﬃciency. More precisely, the single-photon emitters have an in-plane dipole resulting in out-of-plane emission, where the emitters are not surrounded by high refractive index materials. Hence, total internal or Frensel reﬂection does not aﬀect the collection of the single-photons. Furthermore, 2D crystals can be easily attached by Van der Waals forces to components such as ﬁbers or waveguides, making them suitable for integration with photonic networks[18, 19]. The exceptionally high thermal and chemical robustness of hBN beneﬁts the durability of the quantum emitters, achieving long-term stable operation[20] over a huge temperature range[21]. Moreover, the quantum emitters (and 2D materials in general) have a high tolerance to ionizing radiation, allowing for use in space applications[22]. In spite of large experimental research eﬀorts and theoretical calculations[23, 24], the exact nature of the defects yet has to be determined. Furthermore, the identiﬁcation is hampered by the large variations of the lifetime and ZPL wavelength from defect to defect. Lifetimes ranging from 0.3 up to 20 ns[18, 20] and ZPLs in the UV[25] and the full visible spectrum[15, 26] have been reported. In addition to naturally occurring defects[14], the emitters can also be created artiﬁcially using diverse methods, including chemical etching[27], plasma etching[20, 28], ion[29] and electron irradiation[29, 30] as well as near-deterministic stress-induced activation[31]. Although most researchers agree that quantum emitters in hBN provide a number of unique opportunities, the performance still lags behind state-of-the-art SPSs. Moreover, the reported quality of single-photons from hBN is not suﬃcient for practical quantum information processing like quantum key distribution (QKD)[32] or photonic quantum computing[33]. A straightforward path for improving the performance of a spontaneous emission process is to use the Purcell eﬀect by coupling the emitter to an optical resonator[34]. The optical resonator reduces the number of modes the emitter can couple to, thereby enhancing emission into the resonant modes. This even works in the ”bademitter” regime, when the emitter linewidth is larger than the cavity linewidth[35]. Work on cavity-integration of emitters in 2D materials has been reported, with quantum emitters hosted by WSe2 coupled to plasmonic nanocavities[36, 37] and microcavities[38]. Quantum emitters hosted by hBN have been coupled to plasmonic nanocavities[39]. Hexagonal boron nitride can also be used to fabricate photonic crystal cavities, however, this makes the required spectral matching between optical cavity mode and emitter diﬃcult[40]. Yet, the performance is still not suﬃcient for use in quantum information experiments. In this article, we report room temperature singlephoton emission from multilayer hBN ﬂakes coupled with a microcavity. The plano-concave cavity fully suppresses the phonon sideband (PSB) and other oﬀ-resonant noise, while at the same time greatly enhances directionality and the spontaneous emission rate. The hemisphere is fabricated using focused ion beam (FIB) milling, allowing for a small radius of the accurate and precise curvature. This leads to an ultra-small mode volume on the order of λ3. We fully characterize the SPS and assess its feasibility for QKD and quantum computing. Moreover, the single-photon source in its current conﬁguration is fully self-contained and compact enough for integration on a pico-class satellite, making it interesting for satellitebased quantum communication[41].
Randomness for Free<|sep|>Games on graphs. Games played on graphs provide the mathematical framework to analyze several important problems in computer science as well as mathematics. In ✩A preliminary version of this paper appeared in the Proceedings of the 35th International Symposium on Mathematical Foundations of Computer Science (MFCS), Lecture Notes in Computer Science 6281, Springer, 2010, pp. 246-257. ✩✩This research was partly supported by Austrian Science Fund (FWF) Grant No P 23499- N23, FWF NFN Grant No S11407-N23 and S11402-N23 (RiSE), ERC Start grant (279307: Graph Games), Microsoft faculty fellows award, the ERC Advanced Grant QUAREM (267989: Quantitative Reactive Modeling), European project Cassting (FP7-601148), European project COMBEST, and the European Network of Excellence ArtistDesign. ∗Corresponding author: Laurent Doyen, LSV, CNRS UMR 8643 & ENS Cachan, 61 avenue du Pr´esident Wilson, 94235 Cachan Cedex, France. Email addresses: Krishnendu.Chatterjee@ist.ac.at (Krishnendu Chatterjee), doyen@lsv.ens-cachan.fr (Laurent Doyen), hugo.gimbert@labri.fr (Hugo Gimbert), tah@ist.ac.at (Thomas A. Henzinger) particular, when the vertices and edges of a graph represent the states and transitions of a reactive system, then the synthesis problem (Church’s problem) asks for the construction of a winning strategy in a game played on a graph [5, 24, 23, 21]. Game-theoretic formulations have also proved useful for the veriﬁcation [1], reﬁnement [18], and compatibility checking [14] of reactive systems. Games played on graphs are dynamic games that proceed for an inﬁnite number of rounds. In each round, the players choose moves; the moves, together with the current state, determine the successor state. An outcome of the game, called a play, consists of the inﬁnite sequence of states that are visited. Strategies and objectives. A strategy for a player is a recipe that describes how the player chooses a move to extend a play. Strategies can be classiﬁed as follows: (a) pure strategies, which always deterministically choose a move to extend the play, and (b) randomized strategies, which may choose at a state a probability distribution over the available moves. Objectives are generally Borel-measurable sets [19]: the objective for a player is a Borel set B in the Cantor topology on Sω (where S is the set of states), and the player satisﬁes the objective if the outcome of the game is a member of B. In veriﬁcation, objectives are usually ω-regular languages. The ω-regular languages generalize the classical regular languages to inﬁnite strings; they occur in the low levels of the Borel hierarchy (they lie in Σ3 ∩ Π3) and they form a robust and expressive language for determining payoffs for commonly used speciﬁcations. Classiﬁcation of games. Games played on graphs can be classiﬁed according to the knowledge of the players about the state of the game, and the way of choosing moves. Accordingly, there are (a) partial-observation games, where each player only has a partial or incomplete view about the state and the moves of the other player; (b) onesided complete-observation games, where one player has partial knowledge and the other player has complete knowledge about the state and moves of the other player; and (c) complete-observation games, where each player has complete knowledge of the game. According to the way of choosing moves, the games on graphs can be classiﬁed into turn-based and concurrent games. In turn-based games, in any given round only one player can choose among multiple moves; effectively, the set of states can be partitioned into the states where it is player 1’s turn to play, and the states where it is player 2’s turn. In concurrent games, both players may have multiple moves available at each state, and the players choose their moves simultaneously and independently. Sources of randomness. There are two sources of randomness in these games. First is the randomness in the transition function: given a current state and moves of the players, the transition function deﬁnes a probability distribution over the successor states. The second source of randomness is the randomness in strategies (when the players play randomized strategies). In this work we study when randomness can be obtained for free; i.e., we study in which classes of games the probabilistic transitions can be simulated by deterministic transitions and the classes of games where pure strategies are as powerful as randomized strategies. Motivation. The motivation to study this problem is as follows: (a) if for a class of games it can be shown that randomness is for free in the transition function, then all future works related to analysis of computational complexity, strategy complexity, and algorithmic solutions can focus on the simpler class with deterministic transitions (the randomness in transition function may be essential for modeling appropriate stochastic reactive systems, but the analysis can focus on the deterministic subclass); (b) if for a class of games it can be shown that randomness is for free in strategies, then all future works related to correctness results can focus on the simpler class of pure strategies, and the results would follow for the more general class of randomized strategies; and (c) the characterization of randomness for free will allow hardness results obtained for the more general class of games (such as games with randomness in the transition function) to be carried over to simpler class of games (such as games with deterministic transitions). Contribution. The contributions of this paper are as follows: 1. Randomness for free in the transition function. We show that randomness in the transition function can be obtained for free for complete-observation concurrent games (and any class that subsumes complete-observation concurrent games) and for one-sided complete-observation turn-based games (and any class that subsumes this class). The reduction is polynomial for complete-observation concurrent games, and exponential for one-sided complete-observation turn-based games. It is known that for complete-observation turn-based games, a probabilistic transition function cannot be simulated by a deterministic transition function (see discussion in Section 3.4 for details), and thus we present a complete characterization when randomness can be obtained for free in the transition function. 2. Randomness for free in the strategies. We show that randomness in strategies is free for complete-observation turn-based games, and for 1-player partialobservation games (POMDPs). For all other classes of games randomized strategies are more powerful than pure strategies. It follows from a result of Martin [20] that for 1-player complete-observation games with probabilistic transitions (MDPs) pure strategies are as powerful as randomized strategies. We present a generalization of this result to the case of POMDPs. Our proof is totally different from Martin’s proof and based on a new derandomization technique of randomized strategies. 3. Concurrency for free in games. We show that concurrency is obtained for free with partial-observation, both for one-sided complete-observation games as well as for general partial-observation games (see Section 3.5). It follows that for partial-observation games, future research can focus on the simpler model of turn-based games, and concurrency does not add anything in the presence of partial observation. 4. New undecidability results. As a consequence of our characterization of randomness for free, we obtain new undecidability results. In particular, using our results and results of Baier et al. [2] we show for one-sided complete-observation deterministic games, the problems of almost-sure winning for coB¨uchi objectives and positive winning for B¨uchi objectives are undecidable. Thus we obtain the ﬁrst undecidability result for qualitative analysis (almost-sure and positive winning) of one-sided complete-observation deterministic games with ω-regular objectives. Applications of our results. While we already show that our results allow us to obtain new undecidability results, they have also been used to simplify proofs and analysis of POMDPs and partial-observation games [6, 7, 8, 9, 16] (e.g. [7, Lemma 21] and [9, Claim 2. Lemma 5.1]) as well as extended to other settings such as probabilistic automata [17].
Deep high-resolution X-ray spectra from cool-core clusters<|sep|>The hot (few 107K) intracluster medium (ICM) in galaxy clusters is primarily seen in emission in the X-ray waveband. The ICM contains most of the baryonic cluster mass. In a large fraction of clusters of galaxies, observations show that the X-ray surface brightness steeply rises towards their centres (e.g. Stewart et al. 1984). In addition, spectrally-derived temperature proﬁles of clusters typically show drops in temperatures by a factor of 2 or 3 from the outskirts (e.g. Allen et al. 2001). As the X-ray surface brightness is proportional to the integral of the density squared along a line of sight, peaked surface brightness proﬁles imply that the gas in the cores is cooling much more rapidly than in the outskirts. In the centre, the mean radiative cooling time, often calculated as the ratio of the luminosity of a region to its enthalpy, often drops below 1 Gyr. Assuming steady-state and the absence of heating, there will be material with short cooling times in the cluster centre cooling out of the X-ray emitting band. As the volume of this material becomes much smaller, there must be a ﬂow of material in to replace it to preserve the steady state. The steep surface brightness proﬁles imply rates of 10s to 1000s of solar masses per year cooling out of the X-ray band (see Fabian 1994 for a review). This cooling material would be expected to eventually give rise to star formation. This picture changed when high spectral resolution X-ray studies of nearby clusters of galaxies using the Reﬂection Grating Spectrometer (RGS) instruments on XMM-Newton revealed a lack of cool X-ray emitting gas in these objects (Tamura et al. 2001a,b; Peterson et al. 2001; Kaastra et al. 2001; Sakelliou et al. 2002; Peterson et al. 2003). The main spectral indicator missing in these spectra are the emission lines of Fe XVII, which are strong from material between 0.15 and 0.8 keV (1.7 to 9.4 MK). The general accepted picture is that there is a lack of material below a factor 2 or 3 of the outer temperature, which matches the results from Chandra spatially-resolved temperature proﬁles. This is not completely correct, however. Deep observations of nearby clusters show a much larger range of temperature. Fe XVII emission lines have been seen in Centaurus (Sanders et al. 2008), showing a temperature range of more than 10, Abell 2204 (Sanders et al. 2009b), showing a range of 15, 2A 0335+096 (Sanders et al. 2009a), showing a range of 8. Fe XVII emission was also detected in Abell 262, M87 and NGC 533 in a large study by Peterson et al. (2003). There is much less material than would be expected to be seen in the case of steady-state radiative cooling without any heating, however.
A thermodynamically consistent quasi-particle model without density-dependent infinity of the vacuum zero point energy<|sep|>The investigation of the equation of state (EOS) for cold and dense strongly interacting matter is still a contemporary focus [1–11]. The EOS represents an important interrelation of state variables describing matter in thermodynamical equilibrium and we can get all the thermodynamic properties of the system from it. It is also well known that in astrophysics the study of the neutron star depends crucially on the EOS [12–15], because it provides the key information needed to evaluate stellar equilibrium conﬁgurations (e.g, recent astronomical observations have measured the highest neutron star mass ever determined in a precision weighing [16]). Nowadays Quantum Chromodynamics (QCD) is the generally accepted fundamental theory of strongly interacting matter. According to the phase diagram of QCD, at zero temperature and suﬃciently high density, there is a phase transition from hadronic matter to a state which can be called plasma built of quark-gluon constituents (QGP). However, at present it is very hard to get a reliable EOS of cold and dense quark matter from the ﬁrst principle of QCD. Lattice calculations are of limited use at ﬁnite chemical potential due to the appearance of the notorious sign problem. In addition, some analytical perturbative approaches have been considered, but in the physically relevant region, due to the large coupling, these methods seem basically to fail [17]. In this case, one has to develop various phenomenological models which incorporate some basic features of QCD. Up to now, there have been many diﬀerent QCD models [18– 20]. Among those models, the quasi-particle quark-gluon plasma model with few ﬁtting parameters was widely used to describe the nonperturbative behavior of EOS observed in Monte Carlo (MC) simulations in the case of ﬁnite temperature [21–29] (here it should be noted that due to lack of the data of MC simulations at ﬁnite chemical potential, there are very limited number of papers to apply quasi-particle model to study the EOS of QCD at ﬁnite density). In the quasi-particle model, at ﬁnite temperature, instead of real quarks and gluons with QCD interactions, one can consider the system to be made up of non-interacting quasi-particles with temperaturedependent eﬀective mass, quasi-quarks and quasi-gluons. This model was ﬁrst proposed by Goloviznin and Satz [30], and later by Peshier et al. [31]. After some time, Gorenstein and Yang found that this model is thermodynamically inconsistent, and remedied this ﬂaw by reformulating statistical mechanics [32]. After that, Bannur also proposed a new quasi-particle model using standard statistical mechanics and avoided the thermodynamical inconsistency from the energy density rather than the pressure [26]. Further, in Ref. [33] Gardim and Steffens showed that the two models proposed by Peshier and Bannur are two extreme limits of a general formulation. Nevertheless, as the authors pointed out in Ref. [34], in all early works on quasi-particle model the problem of the temperature-dependent inﬁnity of the vacuum zero point energy and its inﬂuences have not been seriously considered. In Ref. [34], the authors used the series expansion method inspired by Walecka [35] to deal with the temperature-dependent inﬁnity of the vacuum zero point energy and constructed a new thermodynamically consistent framework of quasi-particle model for QGP without the need of any reformulation of statistical mechanics or thermodynamical consistency relation. So, when Peshier et al. and Bannur generalized their respective quasi-particle models to the case of ﬁnite chemical potential, there will automatically emerge a term of chemical potential-dependent inﬁnity of the vacuum zero point energy in the partition function, in other words, the partition function of quasi-particle model at ﬁnite chemical potential is also ill-deﬁned. In order to overcome this ﬂaw, in the present paper we will generalize the improved quasi-particle model proposed in Ref. [34] from ﬁnite temperature and zero chemical potential to the case of zero temperature and ﬁnite chemical potential to make the partition function of the quasi-particle model at ﬁnite chemical potential well-deﬁned.. This article is organized as follows. In section II, we shortly review the problem existing in previous quasiparticle models for the purpose of self-consistency. In sections III, we generalize our improved quasi-particle model from ﬁnite temperature and zero chemical potential to the case of zero temperature and ﬁnite chemical potential. We ﬁrst calculate the partition function of (2+1) ﬂavor QGP at ﬁnite temperature T and chemical potential µ, then go to the limit T = 0 in order to ﬁnd the EOS at zero temperature. Furthermore, we calculate the quark-number density, the energy density, the quarknumber susceptibility and the speed of sound of (2+1) ﬂavor QGP at zero temperature and ﬁnite chemical potential and compare our results with the corresponding results in the existing literature. Finally we conclude our work with a summary.
Effective carrying capacity and analytical solution of a particular case of the Richards-like two-species population dynamics model<|sep|>Population dynamics models are useful when one tries to understand, describe or predict the behavior of a wide range of time dependent processes in several disciplines. To easily formulate and write the solutions of the population dynamics model, we present a one-parameter generalization of the logarithmic and exponential functions. This generalization has been ﬁrst introduced in the context of non so that for ˜q = 0, one retrieves the usual exponential function. The use of these functions is convenient since it has allowed us to ﬁnd and simplify (using their properties) the solution of the models with time-dependent intrinsic and extrinsic growth rates. The simplest way to deal with population growth is to consider one-species models. In these models, individuals do not explicitly interact with the external ones and, at time t, the number of individuals is N(t) ≥ 0, with initial condition N0 ≡ N(0) > 0. The parameters are the intrinsic growth rate κ > 0 and the environment carrying capacity K = N(∞) > 0, which takes into account all possible interactions among individuals and resources [8]. If N(t)/K ≪ 1, a general model is the von Foerster et al. model [9], where the per capita growth rate is d ln N/dt = κN ˜q. Considering τ ≡ κt ≥ 0 as a dimensionless time unity, the model solution is: N(τ) = N0e−˜q[τ/(˜qT )], with N0 = (˜qT )−1/˜q and produces a population size divergence at a ﬁnite time T , obtained from the ˜q-exponential ˜qτ/(˜qT ) > 1 [10]. As ˜q → 0, the exponential population growth (Malthus model) N(t) = N0eκt, with divergence at an inﬁnite time, is obtained. The population size divergence can be dismantled considering a ﬁnite carrying capacity. The richness ecological community cannot be properly described only by the one-species models. The interactions between species becomes better formalized with the (Malthus-like) Lotka-Volterra equations [15], which only explains prey-predator behavior in its original formulation and presents stability problems. Besides the predation interaction, there are many other diﬀerent kinds of interaction taking place between two biological species. For instance, if species negatively aﬀects each other, when they occupy the same ecological niche and use the same resources, there is competition. If species favor each other, there is mutualism or symbiosis. These ecological richness is partially appreciated in the competitive (Verhulst-like) Lotka-Volterra model studied in Ref. [19]. This model presents complete analytical solutions. It has a non-trivial phase diagram and solves the stability problem of the Malthus-like two species model. Here, we consider a generalized two-species population dynamic model and analytically solve it for particular ecological interactions. These two-species models can be simpliﬁed to a one-species model, with a time dependent extrinsic growth factor. With a one-species model with an eﬀective carrying capacity, one is able to retrieve the steady state solu tions of the previous one-species model. The equivalence obtained between the eﬀective carrying capacity and the extrinsic growth factor is complete only for the Gompertz model. Our presentation is organized as follows: In the Sec. 2, the Richards model is presented in terms of the generalized functions, which has the Verhulst and Gompertz models as particular cases. We show that the steady state solution of the Richards model with extrinsic growth rate is the same for the Richards model without extrinsic growth rate, but with a modiﬁed carrying capacity. Also, for a particular case of the Richards model (˜q → 0), that is the Gompertz model, not only the steady state is the same but also the whole system evolution (transient). In the Sec. 3, we present a generalized model of two interacting species. We show that the interaction between species can also be interpreted as an extrinsic growth factor. This allows one to represent a two-species system by a one-species model with a modiﬁed carrying capacity. In Sec. 4, we present our ﬁnal remarks.
The Atlas3D Project -- XI. Dense molecular gas properties of CO-luminous early-type galaxies<|sep|>Molecular gas is an essential ingredient for star formation found in many, but not all, galaxies. Early-type galaxies (ellipticals and lenticulars) were classically thought to completely lack molecular gas and to be passively-evolving ‘red and dead’ galaxies. However, we have long known that not all early-types are empty of cold gas; molecular gas was ﬁrst detected in early-type galaxies by Wiklind & Rydbeck (1986) and Phillips et al. (1987). Shortly after these ﬁrst detections, surveys detected 10-20 galaxies, but were biased towards early-types with particular properties, such as those bright in the far infra-red (Wiklind & Henkel 1989; Sage & Wrobel 1989) or with optically obscuring dust (Wang et al. 1992). These selection effects made it easy to dismiss any early-type galaxies with molecular gas as peculiar systems. The next generation of surveys increased numbers (30-50 galaxies) and were less biased, if still not complete (Welch & Sage 2003; Sage et al. 2007; Combes et al. 2007). In Young et al. (2011, hereafter Paper IV), we have recently completed an extensive (259 galaxies) molecular gas detection campaign for the volume-limited ATLAS3D sample of early-type galaxies (Cappellari et al. 2011a, hereafter Paper I). We ﬁnd a 22% detection rate, down to a typical detection threshold of 6×107 M⊙ of H2, robustly establishing that many early-type galaxies host a substantial amount of molecular gas. The majority of early-type galaxies with molecular gas are obviously star-forming, based on ultraviolet (UV), optical or infrared data. Indeed, the detection rate for star formation seen in UV nearly matches the molecular detection rate (≈ 30%, Kaviraj et al. 2007). According to the current observations, the star formation efﬁciencies of early-type galaxies broadly follow the Kennicutt-Schmidt law (Shapiro et al. 2010; Crocker et al. 2011). A subset shows no obvious sign of ongoing star formation (Crocker et al. 2008, 2011), although these determinations are difﬁcult due to their very low speciﬁc star formation rates. Recently, Saintonge et al. (2011) have de ⋆ Based on observations carried out with the IRAM 30-m telescope. IRAM is supported by INSU/CNRS (France), MPG (Germany) and IGN (Spain). † E-mail: crocker@astro.umass.edu ‡ Dunlap Fellow termined that star formation efﬁciencies are reduced for more massive, more concentrated and higher stellar surface density galaxies, all properties that positively correlate with early-type morphology. In a spiral galaxy, the presence of a stellar disc renders the gas disc more locally unstable to axisymmetric perturbations, likely boosting its star formation efﬁciency (e.g. Jog & Solomon 1984). In Krajnovi´c et al. (2011, hereafter Paper II) and Emsellem et al. (2011, hereafter Paper III), we found that the vast majority of earlytype galaxies (the fast rotators) are consistent with being a family of disc-like systems resembling spiral galaxies with the gas and dust removed (Cappellari et al. 2011b, hereafter Paper VII). However, the fast rotators are generally characterized by larger spheroids than spiral galaxies. This increase in the depth of the potential well is expected to make their gas discs more stable against fragmentation (Kawata et al. 2007). Indeed, simulations with a centrally-concentrated stellar mass distribution (as found in spheroids) and no stellar disc show that the cool gas is more stable than in spiral galaxies (Martig et al. 2009). This stability (termed ‘morphological quenching’) should lower the efﬁciency of star formation and produce a cool interstellar medium (ISM) with properties (velocity dispersion, density distribution, etc.) different from those of galaxies with stellar discs. In this paper, we present the ﬁrst major attempt at constraining the empirical properties of the molecular gas in early-type galaxies, especially looking for any divergence from the properties found for spiral galaxies. The surveys mentioned above have used the bright 12CO(1-0) emission line to measure the total molecular content of early-type galaxies, but little work has been done to constrain the molecular gas properties using other species and transitions. Several other molecular species are bright enough to measure, including 13CO, HCN and HCO+. These species have been widely observed in starburst and Seyfert galaxies and also in some local spiral galaxies. The observed 12CO/13CO ratio is widely used to indicate the average optical depth of the molecular gas1, although it may also be inﬂuenced by chemical processes. The 12CO isotope is far more
Mechanical energy fluxes associated with saturated coronal heating in M dwarfs: comparison with predictions of a turbulent dynamo<|sep|>Many late-type stars emit radiant energy of a non-thermal nature from chromospheres and  coronae. These non-thermal processes can be quantified by ratios such as LHK/Lbol (from the  chromospheric H and K lines) and LX/Lbol (using the coronal X-rays). These ratios can be  considered as measures of the efficiencies with which the total output power of the star is  converted into chromospheric heating and coronal heating, respectively. Different stars emit at  different intensities, with ranges spanning 4 or more orders of magnitude in LX/Lbol. The data  indicate that a well-defined upper limit exists, above which no cool dwarf (so far) has been  found to emit coronal X-rays. The upper boundary of the range of emission in chromospheric  and/or coronal emissions is referred to as the “saturated level”. Vilhu & Walter (1987: VW)  provide data for coronal emission from several dozen stars ranging from spectral type A to M5  and later. The M dwarfs plotted by VW are taken from a compilation by Bookbinder (1985) of  the most active M dwarfs within 25 parsecs. The upper limit on LX/Lbol for the most active M dwarfs reported by VW is about 2 x 10-3,  indicating that of the entire energy flux emerging from one such star, some 0.2% is converted  into coronal heating. To convert to actual fluxes of X-rays, we note that the photospheric fluxes  from M dwarfs with effective temperatures in the range 4000-3300 K (corresponding to spectral  sub-types M1-M5 according to five different calibrations of M dwarfs illustrated by Muzic et al.  2014) are in the range (1.5-0.7) x 1010 erg cm-2 s-1. A fraction 0.2% corresponds to upper limits  on the coronal heating fluxes in the range (1.4-3) x 107 erg cm-2 s-1. We note that although M  dwarfs have the largest values of the ratio LX/Lbol in the VW dataset, the largest absolute values  of LX itself (reaching almost as large as 108 erg cm-2 s-1) are found among early G stars. The results of VW indicate that there is a clear distinction between stars in which the corona is at  the “saturated level” and stars where the coronal heating is “unsaturated”. On the one hand, in  the “unsaturated” stars, the data show that the LX/Lbol value rises steeply as the stellar rotation  period P becomes shorter (see Fig. 10 in VW): in a log-log plot of LX/Lbol versus Rossby number  Ro (= τc /P, where τc is the convective turnover time), the slope β has a (positive) value of 3 or  more. Thus, the rotation-activity correlation (RAC), i.e. LX/Lbol ~ Pa, is expected to have a steep  negative slope a for the unsaturated stars. However, even if β is known for a sample of stars with  a given spectral type, the actual numerical value of a for that sample depends on having access to  a method which allows us to remove the variations of τc with spectral type. On the other hand, for  the “saturated” stars, VW find that the value of LX/Lbol is essentially independent of Ro. That is,  the RAC for saturated stars has a slope a which is essentially zero. Other examples of “saturation” in X-ray fluxes generated by the stars with fastest rotations have  been provided by Micela et al. (1985), James et al. (2000), Pizzolato et al. (2003), and Wright et  al. (2011). The sample of X-ray emitting stars presented by Wright et al. (2011: their Fig. 2)  contains 824 stars, an order of magnitude larger than the VW sample. But the Wright et al.  sample confirms the VW result that the RAC has a clearly defined slope of zero among the saturated stars. The latter have an upper limit on LX/Lbol of (2-3) x 10-3, essentially identical to the  upper limit reported by VW. Among the unsaturated stars in the Wright et al. sample, a fit to the Rossby number indicates a  well-defined (positive) slope of 2.7, comparable to the slope obtained by VW. Wright et al. also  present a log-log plot of LX/Lbol versus P: in this case, there is more scatter in the data: an  “eyeball” fit data suggests that a functional form LX/Lbol ~ Pa leads to a values which are at least  as steep as -1. The aim of the present paper is to consider a quantitative explanation for the existence of a  mechanical flux of up to (1.4-3) x 107 erg cm-2 s-1 originating in M dwarfs. Moreover, the flux  must be in a form which can cause deposition of heating energy at distances which are relatively  far from the source. In particular, the flux must be in a form which allows the energy to  propagate all the way up to altitudes which are relevant to the corona. The main focus of the present paper is on M dwarfs with masses near to (or later than) the  transition between partially convective stars and fully convective stars. The reason for this focus  is that we have recently (Mullan et al. 2015: MHM) presented a calculation of dynamo action in  stars where a specific physical structure exists inside the star: an interface between convective  envelope and radiative core. In such stars, MHM quantify the possibility that dynamo action can  be driven by rotational motion at the interface. In the present paper, we switch to consideration  of a dynamo which relies for its operation only on the presence of convective motions. Such a  dynamo can operate in any cool star where a convective envelope exists, whether or not the star  also contains an interface. In its purest form, the dynamo we consider here operates in stars  which are completely convective: such stars have no access to an interface dynamo, and  therefore rely solely on the dynamo we consider here. In principle, it is possible that both types  of dynamos may operate simultaneously in partially convective stars. Therefore, the results we  present here are not meant to apply exclusively to fully convective stars. But here, in order to  keep the discussion as simple as possible, we focus on a dynamo which has no need of an  interface at all. In this sense, the present paper is meant to serve as a complement to MHM.
Using Partial Monotonicity in Submodular Maximization<|sep|>Over the last two decades, submodular function maximization has been the workhorse of many discrete optimization problems in machine learning applications such as data summarization [17, 19, 31, 32, 42, 49], social graph analysis [45], adversarial attacks [36], dictionary learning [15], sequence selection [41, 50], interpreting neural networks [18] and many more. Traditionally, the study of submodular functions was based on binary properties of functions. A function can be either submodular or non-submodular, monotone or non-monotone, etc. Such properties are simple, but they have an inherit weakness—if an algorithm assumes functions that have a particular property, then it provides no guarantee for functions that violate this property, even if the violation is very slight. Given the above situation, recent works began to consider continuous versions of function properties. Probably the most signiﬁcant among these continuous versions so far are the submodularity ratio and the curvature. The submodularity ratio (originally deﬁned by Das and Kempe [16]) is a parameter γ ∈ [0, 1] replacing the binary submodularity property that a set function can either have or not have. A value of 1 corresponds to a fully submodular function, and lower values of γ represent some violation of submodularity (the worse the violation, the lower γ). Similarly, the curvature (deﬁned by Conforti and Cornu´ejol [13]) is a parameter c ∈ [0, 1] replacing the binary linearity property that a set function can either have or not have. A value of 1 corresponds to a fully linear function, and lower values of c represent some violation of linearity. A central conceptual contribution of Das and Kempe [16] was that they were able to demonstrate that continuous function properties further extend the usefulness of submodular maximization to new machine learning applications (such as subset selection for regression and dictionary selection). This has motivated a long list of works on such properties (see [3, 25, 26, 29, 34] for a few examples), including works that combine both the submodularity ratio and the curvature (see, e.g., [3]). However, to the best of our knowledge, no continuous version of the binary monotonicity property has been suggested so far.1 We note that the monotonicity property of set functions plays a central role in submodular maximization, and basically every problem in this ﬁeld has been studied for both monotone and non-monotone objective functions. Naturally, monotone objective functions enjoy improved approximation guarantees compared to general functions, and it is natural to ask how much of this improvement applies also to functions that are almost monotone (in some sense). Since such functions often arise in machine learning applications when a diversity promoting component is added to a basic monotone objective, obtaining better guarantees for them should strongly enhance the usefulness of submodular maximization as a tool for many machine learning applications. Formally, a non-negative set function f : 2N → R≥0 over a ground set N is (increasingly) monotone if f(S) ⊆ f(T) for every two sets S ⊆ T ⊆ N. Similarly, we deﬁne the monotonicity ratio of such a function f as the maximum value m ∈ [0, 1] such that m · f(S) ≤ f(T) for every two sets S ⊆ T ⊆ N. Equivalently, one can deﬁne the monotonicity ratio m by where the ratio f(T)/f(S) is assumed to be 1 whenever f(S) = 0. Intuitively, the monotonicity ratio measures how much of the value of a set S can be lost when additional elements are added to S. One can view m as a measure of the distance of f from being monotone. In particular, m is equal to 1 if and only if f is monotone. Our main contribution in this paper is demonstrating the usefulness of the monotonicity ratio in machine learning applications, which we do in two steps. • First, we show (in Sections 3, 4 and 5) that for many standard submodular maximization algorithms one can prove new approximation guarantees that depend on the monotonicity ratio. These approximation guarantees interpolate between the known approximation ratios of these algorithms for monotone and non-monotone submodular functions. • Then, using the above new approximation guarantees, we derive new approximation ratios for the standard applications of movie recommendation, quadratic programming and image summarization. Our guarantees improve over the state-of-the-art for most values of the problems’ parameters. See Section 6 for more detail. 1Following the appearance of the pre-print version of this paper, we learned that Iyer deﬁned in his Ph.D. thesis [30] such a property, and moreover, this property is identical in name and deﬁnition to the one we deﬁne. However, Iyer only used this property to prove the result that appears below as Theorem 4.1; and thus, our work is the ﬁrst to systematically study this property. Remark. Computing the monotonicity ratio m of a given function seems to be a diﬃcult task. Therefore, the algorithms we analyze avoid assuming access to m, and the value of m is only used in the analyses of these algorithms. Nevertheless, in the context of particular applications, we are able to bound m, and plugging this bound into our general results yields our improved guarantees for these applications.
Liquid velocity fluctuations and energy spectra in three-dimensional buoyancy driven bubbly flows<|sep|>Bubble laden ﬂow appears in a variety of natural (Clift et al. 1978; Gonnermann & Manga 2007) and industrial (Deckwer 1992) processes. Presence of bubbles dramatically alters the transport properties of a ﬂow (Mudde 2005; Ceccio 2010; Biferale et al. 2012; Pandit et al. 2017; Risso 2018; Elghobashi 2019; Mathai et al. 2019). A single bubble of diameter d, because of buoyancy, rises under gravity. Its trajectory and the wake ﬂow depend on the density and viscosity contrast with the ambient ﬂuid, and the surface tension (Clift et al. 1978; Bhaga & Weber 1981; Tripathi et al. 2015). A suspension of such bubbles at moderate volume fractions generates complex spatiotemporal ﬂow patterns that are often referred to as pseudo-turbulence or bubble-induced agitation (Mudde 2005; Risso 2018). Experiments have made signiﬁcant progress in characterizing velocity ﬂuctuations of the ﬂuid phase in pseudo-turbulence. A key observation is the robust power-law scaling in the energy spectrum with an exponent of −3 either in frequency f or the wave-number k space (Mercado et al. 2010; Riboux et al. 2010; Mendez-Diaz et al. 2013). The scaling range, however, remains controversial. Riboux et al. (2010) investigated turbulence in the wake of a bubble swarm and found the k−3 scaling for length scales larger than the bubble diameter d (i.e., k < 2π/d), whereas Mercado et al. (2010); Prakash et al. (2016) observed this scaling for scales smaller than d in a steady state bubble suspension. Experiments on bouyancy driven bubbly ﬂows in presence of grid-turbulence (Lance & Bataille 1991; Prakash et al. 2016; Almras et al. 2017) observe Kolmogorov scaling for scales larger than the bubble diameter and smaller than the forcing scale and a much steeper k−3 scaling for scales smaller than the bubble diameter and larger than the dissipation scale. Lance & Bataille (1991) argued that, assuming production because of wakes to be local in spectral space, balance of production with viscous dissipation leads to the observed k−3 scaling.
Location Aided Energy Balancing Strategy in Green Cellular Networks<|sep|>During the past several years, there has been tremendous growth in cellular networks. With the introduction of Android and iPhone devices, use of ebook readers such as iPad and Kindle and the success of social networking giants such as Facebook, WeChat and QQ, the number of customers and the demand for cellular trafﬁc have escalated signiﬁcantly. The huge amount of customers and very high volume data transmission result in serious problems of energy consumption[1]. The rising energy costs of operating cellular networks have led to an emerging trend of addressing energy efﬁciency and energy balance utilization amongst the network operators and regulatory bodies such as 3GPP and ITU[2]. This trend has stimulated the interest of researchers in an innovative new research area called green cellular networks. In this regard, the European Commission has recently started new projects to address the energy issue of mobile communication systems, such as “Energy Aware Radio and Network Technologies (EARTH)”, “Towards Real Energy-efﬁcient Network Design” and “Cognitive Radio and Cooperative strategies for Power Saving in Multi-standard Wireless Devices” [3]. Energy in cellular network is a vast research discipline that needs to cover all the layers of the protocol stack and various system architectures and it is important to identify the fundamental trade-offs linked with energy efﬁciency and the overall performance [4]. Scholars have addressed four key issues in terms of energy efﬁciency with network performance[2]. They are deployment efﬁciency (balancing deployment cost, throughput), spectrum efﬁciency (balancing achievable rate), bandwidth (balancing the bandwidth utilized) and delay (balancing average endto-end service delay). To address the challenge of increasing power efﬁciency in future cellular networks and thereby to maintain proﬁtability, it is crucial to consider various paradigm-shifting technologies, such as energy efﬁcient wireless architectures and protocols, efﬁcient base station(BS) redesign, smart grids, opportunistic network access or cognitive radio, cooperative relaying and heterogeneous network deployment based on smaller cells. In addition to academia, governments and industries have recently shown keen concerns on the critical issues related to energy efﬁciency and its balance utilization and in the ICT(Information and Communication Technology) area. However, as studied in the recent literature [5][6], most of the techniques applied to current mobile networks have been designed by taking into account non-energy-related factors, such as throughput, Quality of Service (QoS), availability, scalability, and so on. Moreover, in real-world systems, mobile users (MUs) are not evenly distributed across cells, resulting in that MUs in a hot cell will be affected by the load imbalance and they might unable to get services. Meanwhile, as novel network architectures that include picocells, hierarchical cells and femtocells emerge, the density of base stations and mobile users is becoming larger, and the cells go smaller. The appearance of this high density cellular networks introduces more variety of load across different cells and makes the load imbalance problem more serious. The new generation cellular networks allow mobile users to connect relay station and then connect to base station. The busy trafﬁc near a relay station often makes the energy of the relay station go down very quickly. However, the relay station with low trafﬁc is idle with high energy. In order to balance the energy utilization among different cells, it is needed to transfer the over-loaded trafﬁc from hot cells to neighboring cooler ones. The challenge is how to balance the energy utilization of the relay stations in order to get the best trade-off among all the relay stations in a cellular network. In this paper, we develop a statistical parameter based energy balance utilization algorithm, in which each relay station maintains the acceleration and the variance of energy consumption that represent the station’s historical energy consumption acceleration and the variance. Together with its current energy quantity, the relay station is able to predict its future energy quantity, which is considered as its potential energy capacity. Before payload data transmission between a mobile user and a relay station, the mobile user disseminates a message to a portion of its neighbor relay stations depending on their geographic locations . Then it selects and hands over to the relay station with highest potential energy. Because each relay station only needs to maintain two parameters, the overhead of this scheme is very low. Our simulation results illustrate that our approach signiﬁcantly increases the aggregate throughput in the network and the average life time of the relay stations compared with existing approaches. The rest of the paper is organized as follows. Section II discusses the related research on this topic. Section III proposes a novel method that select the best relay station. We evaluate the proposed schemes by simulations and describe the performance results in Section IV. Section V concludes the paper.
Dual Smoothing and Level Set Techniques for Variational Matrix Decomposition<|sep|>Linear superposition is a useful model for many applications, including nonlinear mixing problems. Surprisingly, we can perfectly distinguish multiple elements in a given signal using convex optimization as long as they are concise and look sufﬁciently different from one another. Robust principal component analysis (RPCA) is a key example, where we decompose a signal into the sum of low rank and sparse components. RPCA is a special case of stable principal component pursuit (SPCP), where we also allow an explicit noise component within the RPCA decomposition. Applications include alignment of occluded images (Peng et al., 2012), scene triangulation (Zhang et al., 2011), model selection (Chandrasekaran et al., 2012), face recognition, and document indexing (Cand`es et al., 2011a). For SPCP, our model is A = L + S + E (1) where A is the observed matrix, L is a low-rank matrix, S is a sparse matrix, and E is an unstructured nuisance matrix (e.g., a stochastic error term). The classic RPCA formulation (Cand`es et al., 2011a) assumed E = 0, but in general we do not distinguish between RPCA and SPCP. The RPCA problem uses regularization on the summands L and S in order to improve the recovery of the solution. In (Cand`es et al., 2011b), the 1-norm regularizer is applied to S to promote sparsity, and the nuclear norm is applied to L to penalize rank: The 1-norm ∥ · ∥1 and nuclear norm |||·|||∗ are given by ∥S∥1 = � i,j |si,j| and |||L|||∗ = � i σi(L), where σ(L) is the vector of singular values of L. The parameter λ > 0 controls the relative importance of the low-rank term L vs. the sparse term S. This problem has been analyzed by (Chandrasekaran et al., 2009; Cand`es et al., 2011a), and it has perfect recovery guarantees under stylized incoherence assumptions. There is even theoretical guidance for selecting a minimax optimal regularization parameter λ (Cand`es et al., 2011a). There are several modeling choices underlying formulation (2). First is the choice of the ℓ1-norm to promote sparsity and the trace norm (aka nuclear norm) to promote low-rank solutions. We will keep with these choices throughout the entire chapter, noting where it is possible to use more general penalties. Second, (2) assumes the data are ﬁt exactly. Unfortunately, many practical problems only approximately satisfy the idealized assumptions. This motivates the SPCP variant: where the ε parameter accounts for the unknown perturbations A − (L + S) in the data not explained by L and S. It is useful to deﬁne φ(L, S) = |||L|||∗ + λ∥S∥1 as a regularizer on the decision variable (L, S). The formulation (2) then tries to ﬁnd the tuple (L, S) that ﬁts the data perfectly, and is minimal with respect to φ. Third, the functional form of φ is important; in (2) as well as (SPCPsum) the component penalties are added with a tradeoff parameter λ, but other choices can be made as well. In particular, Aravkin et al. (2014a) propose a new variant called “max-SPCP”: where λmax > 0 acts similar to λsum, and this new formulation offers both modeling and computational advantages over (SPCPsum) (see Section 5.2). We show that cross-validation with (SPCPmax) to estimate (λmax, ε) is signiﬁcantly easier than estimating (λsum, ε) in (SPCPsum). Given an oracle that provides an ideal separation A ≃ Loracle + Soracle, we can use ε = ∥Loracle + Soracle − A∥F in both cases. However, while we can estimate λmax = |||Loracle|||∗/∥Soracle∥1, it is not clear how to choose λsum from data, without using cross-validation or assuming a probabilistic model. Finally, both (SPCPsum) and (SPCPmax) assume a least-squares penalty functional to measure the error up to level ϵ. We can consider a more general choice of penalty ρ: Robust losses as well as ρ arising from probabilistic models have been explored in (Aravkin et al., 2014c, 2016). Once ρ and φ have been selected, we can choose the type of regularization formulation one wants to solve. Formulation (3) minimizes the regularizer subject to a constraint on the misﬁt error. Two other common formulations are All three formulations can be effectively used, and are equivalent in the sense that solutions match for certain values of parameters λ, τ, and ε. Formulation (3) is preferable from a modeling perspective when the misﬁt level ε is known ahead of time, or can be estimated. However, formulations (4) and (5) often have fast ﬁrst-order algorithms available for their solution. It turns out that we can exploit algorithms for (4) to solve (3) using the graph of the value function for problem (4); this relationship can be used to show that the problems have the same complexity (Aravkin et al., 2016). Level set optimization was ﬁrst applied for sparsity optimization by van den Berg and Friedlander (2008), and later extended to gauge optimization (van den Berg and Friedlander, 2011) and to general convex programming (Aravkin et al., 2013). The second approach we consider is the TFOCS algorithm (Becker et al., 2011) and software1, which is based on the proximal point algorithm, and can also handle generic convex minimization problems. We present a new analysis of TFOCS, along with stronger convergence guarantees, and also apply TFOCS method to RPCA. TFOCS solves all standard variants of RPCA and SPCP, and can easily add non-negativity or other types of additional constraints. We brieﬂy detail how the algorithm can be specialized for the RPCA problem in particular. The chapter proceeds as follows. In Section 2, we provide the necessary convex analysis background to understand our algorithms and results. In Section 3, we look at level set techniques in the context of the RPCA problem; in particular we describe previous work and algorithms for SPCP and RPCA in Section 3.2, discuss computationally efﬁcient projections as optimization workhorses in Section 3.4, and develop new accelerated projected quasi-Newton methods for the ﬂipped and Lagrangian formulations in Section 3.5. We then present a view of dual smoothing, describe the TFOCS algorithm, and show to to apply it to RPCA in Section 4. We describe the general class of problems solvable by TFOCS in Section 4.1, detail the dual smoothing approach in Section 4.2, and present new convergence results in Section 4.5. Finally, we demonstrate the efﬁcacy of the new solvers and the overall formulation on synthetic problems and real data problems in Section 5.
Non-Markovian finite-temperature two-time correlation functions of system operators of a pure-dephasing model<|sep|>A quantum system is inevitably subject to the inﬂuence of its surroundings or environments [1–6]. An environment usually consists of a practically inﬁnite number of degrees of freedom and acts statistically as a whole identity referred as a reservoir or bath of the open quantum system. Most often, one is concerned with only the system dynamics and the key quantity is the reduced system density matrix ρ(t) deﬁned as the partial trace of the total system-plus-reservoir density operator ρT (t) over the reservoir degrees of freedom; i.e., ρ(t) = TrR[ρT (t)]. If the time evolution of the reduced density matrix that can be Markovian or non-Markovian is known, one is able to calculate the (one-time) expectation values or quantum average of the physical quantities of the system operators. But knowing the time evolution of the reduced density matrix is not suﬃcient to calculate the two-time (multiple-time) correlation functions (CF’s) of the system operators in the non-Markovian case [7–9]. In the Markovian case, an extremely useful procedure to calculate the two-time (multiple-time) CF’s is the so-called quantum regression theorem (QRT) [1–4] that gives a direct relation between the time evolution equation of the single-time expectation values and that of their corresponding two-time (multiple-time) CF’s. So knowing the time evolution of the system reduced density matrix allows one to calculate all of the two-time (multiple-time) Markovian CF’s. For the non-Markovian case, it is known that the QRT is not valid in general [10– 13]. Recently, using the stochastic Schr¨odinger equation approach and the Heisenberg equation of system operator method, an evolution equation, valid to second order in system-environment coupling strength, for the twotime (multiple-time) CF’s of the system operators has been derived for an environment at the zero temperature and for a system in an initial pure state [7–9]. This evolution equation has been applied to calculate the emission spectra of a two-level atom placed in a structured non-Markovian environment (electromagnetic ﬁelds in a photonic band-gap material) [14]. In Ref. [8], an evolution equation for the reduced propagator of the system state vector, conditioned on an initial state of the environment diﬀering from the vacuum, was derived using the stochastic Schrodinger equation approach. It is thus possible to use the reduced propagator to evaluate the expectation values and CF’s of the system observables for general environmental initial conditions, not necessarily an initial vacuum state for the environment [8]. By using another commonly used open quantum system technique, the quantum master equation approach [1–6], we are able to extend the two-time CF evolution equation to a non-Markovian ﬁnite-temperature environment for any initial system-environment separable state. The detailed derivation will be presented elsewhere [15] but the essential results will be summarized in Sec. II. The derived evolution equation that generalizes the QRT to the non-Markovian ﬁnite-temperature case is believed to have applications in many diﬀerent branches of physics. The purpose of this article is twofold: (a) We show that in general the time evolution of the reduced density matrix of the system (or the reduced quantum master equation) alone is not suﬃcient to calculate the twotime CF’s of the system operators of non-Markovian open systems, even in the weak system-environment coupling case. We present an evaluation of an exactly solvable non-Markovian model, i.e., a pure-dephasing spin-boson model [8, 16–21], to justify the statement. The exact non-Markovian ﬁnite-temperature two-time CF’s of the system operators of this model, to our knowledge, have not been presented in the literature. (b) This exactly solvable model allows us to test the validity of the derived non-Markovian ﬁnite-temperature evolution equation of two-time CF’s presented in Sec. II. It will be shown that the two-time CF’s obtained using the evolution equation in the weak system-environment coupling limit [15] in Sec. II for the exactly solvable non-Markovian model happen to be the same as those obtained from the exact evaluation. However, these results signiﬁcantly diﬀer from the non-Markovian CF’s obtained by wrongly applying directly the QRT. This demonstrates clearly that the derived evolution equations generalize correctly the QRT to non-Markovian ﬁnite-temperature cases. The article is organized as follows. We ﬁrst summarize the important results of the newly obtained evolution equations [15] that generalizes the QRT to the nonMarkovian ﬁnite-temperature case in Sec. II. After brief description of the pure-dephasing spin-boson model in the beginning of Sec. III, we calculate the exact time evolution of the reduced density matrix of the system and one-time expectation values in Sec. III A. The exact twotime CF’s are evaluated in subsection III B. In Sec. IV, we use the derived evolution equations in Ref. [15] to calculate the one-time and two-time CF’s. It is shown that the results obtained in Sec. IV are the same as those by the exact evaluation in Sec. III. This demonstrates the validity and practical usage of the derived evolution equations in Ref. [15]. Numerical results and discussions are presented in Sec. V. A short conclusion is given in Sec. VI.
UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning<|sep|>Optical ﬂow estimation has been a fundamental computer vision task for decades, which has been widely used in various applications such as video editing [14], behavior recognition [31] and object tracking [3]. The early solutions focus on minimizing a pre-deﬁned energy function with optimization tools [4, 33, 30]. Nowadays deep learning based approaches become popular, which can be classiﬁed into two categories, the supervised [11, 26] and unsupervised ones [29, 37]. The former one uses synthetic or humanlabelled dense optical ﬂow as ground-truth to guide the motion regression. The supervised methods have achieved leading performance on the benchmark evaluations. However, the acquisition of ground-truth labels are expensive. In addition, the generalization is another challenge when trained on synthetic datasets. As a result, the latter category, i.e. the unsupervised approaches attracts more attentions recently, which does not require the ground-truth labels. In unsupervised methods, the photometric loss between two images is commonly used to train the optical ﬂow estima Figure 1. An example from Sintel Final benchmark. Compared with previous unsupervised methods including SelFlow [22], EpiFlow [44], ARFlow [20], SimFlow [12] and UFlow [15], our approach produces sharper and more accurate results in object edges. tion network. To facilitate the training, the pyramid network structure [34, 10] is often adopted, such that both global and local motions can be captured in a coarse-to-ﬁne manner. However, there are two main issues with respect to the pyramid learning, which are often ignored previously. We refer the two issues as bottom-up and top-down problems. The bottom-up problem refers to the upsampling module in the pyramid. Existing methods often adopt simple bilinear or bicubic upsampling [20, 15], which interpolates cross edges, resulting in blur artifacts in the predicted optical ﬂow. Such errors will be propagated and aggregated when the scale becomes ﬁner. Fig. 1 shows an example. The top-down problem refers to the pyramid supervision. The previous leading unsupervised methods typically add guidance losses only on the ﬁnal output of the network, while the intermediate pyramid levels have no guidance. In this condition, the estimation errors in coarser levels will accumulate and damage the estimation at ﬁner levels due to the lack of training guidance. To this end, we propose an enhanced pyramid learning framework of unsupervised optical ﬂow estimation. First, we introduce a self-guided upsampling module that supports blur-free optical ﬂow upsampling by using a selflearned interpolation ﬂow instead of the straightforward in terpolations. Second, we design a new loss named pyramid distillation loss that supports explicitly learning of the intermediate pyramid levels by taking the ﬁnest output ﬂow as pseudo labels. To sum up, our main contributions include: • We propose a self-guided upsampling module to tackle the interpolation problem in the pyramid network, which can generate the sharp motion edges.
Micropattern gas detector technologies and applications, the work of the RD51 collaboration<|sep|>The working principle of all gas detectors is similar: radiation causes ionization in the gas, electrons and ions drift apart in an electric ﬁeld, and the electrons create further electron-ion pairs in an avalanche process in a region with a strong electrostatic ﬁeld. Gaseous detectors diﬀer in how this strong ﬁeld region is created; many examples will be given to illustrate this. For several decades the most popular way was using thin wires, either one or many, where close to the wire the ﬁeld strength is inversely proportional to the distance to the wire. This is illustrated in Fig. 1, the ﬁrst two pictures. The avalanche takes place few tens of microns from the wire, and the electrons are collected immediately. The ions drift back all the way to the cathode; the signals from proportional wires are therefore almost entirely based on the movement of ions. In recent years, many planar structures have emerged that generate an enhanced ﬁeld region in various ways. Several examples are shown in Fig. 1 and still many more have been developed. Common feature among all these structures is a narrow ampliﬁcation gap of typically 50–100 microns, compared to many millimeters for wire-based structures. These devices are now known under the common name of micropattern gaseous detectors (mpgds). The ﬁrst such structure to gain popularity was the microstrip gas chamber [1] (msgc), of which the ﬁeld pattern is shown in Figure 1. Electric ﬁeld patterns of various gas detector technologies. Each has a region of very strong ﬁeld where multiplication takes place. the third picture in Fig. 1. The principle of an msgc resembles a wire chamber, with ﬁne printed strips instead of thin wires, see Fig. 2. Due to the microelectronics techniques employed in manufacturing the spacing between anode strips was as narrow as 200 microns, compared to at least several millimeters for wire chambers. Most ions created in the avalanche process drift to the wider cathode strips, which are spaced only 60 microns away from the anodes. This short drift path for ions overcomes the space charge eﬀect present in wire chambers, where the slowly drifting ions may remain in the gas volume for milliseconds, and modify the electric ﬁeld (thereby reducing the gain). Figure 3 shows how this space charge eﬀect limits the rate capability of wire chambers, and how the ﬁne granularity of msgcs pushes this limit by two orders of magnitude. The high rate capability of the msgc made it an attractive Figure 2. Left: wires of a multiwire proportional chamber (mwpc) soldered to a frame. Right: microscope image of a microstrip gas chamber (msgc) Figure 3. Gain as a function of particle rate in otherwise constant conditions, for wire chambers in blue and msgcs in red. technology for many applications. However, the development of the msgc also indicated some new limitations, most of which are common to all micropattern devices. One common issue is the charging of insulating surfaces which modiﬁes the ﬁeld shape locally, limiting the time stability. For msgcs this could be solved by surface treatment of the glass substrate to decrease the surface resistivity. Possibly most important is the issue of discharges, which eventually led high-energy physics experiments to abandon msgc technology. Msgcs suﬀered severely from such discharges, induced by heavily ionizing particles or high particle rates, which could fatally damage the fragile anode strips, see ﬁgure 4. In 1997 the gas electron multiplier (gem) was introduced [2] as a preampliﬁcation stage for the msgc. This allowed the msgc to work at a lower voltage, thereby lowering the probability of discharges as well as the energy involved in discharges when they occurred. The gem principle was so successful that it soon became the basis for a detector in its own right. The gas electron multiplier is a copper clad polyimide foil with a regular pattern of densely spaced holes, see Fig. 5. Upon applying a voltage between top and bottom electrodes, a dipole ﬁeld is formed which focuses inside the holes where it is strong enough for gas ampliﬁcation. As a gem is only an ampliﬁcation structure it is independent of the readout structure, which can Figure 4. Damage done to msgcs by discharges. In the rightmost frame anode strips are cut, leaving part of those anodes inactive. With its very thin metal layers msgcs are particularly vulnerable for discharge damage. be optimized for the application (see a few examples in Fig. 6). Due to the separation from the readout structure, possible discharges do not directly impact the front-end electronics, thus making the detector more discharge tolerant. Also, it can be cascaded to achieve higher gain at lower gem voltage, which decreases the discharge probability, see Fig. 7. The triple gem has now become a standard which is used in many high rate applications [4]–[6].
LP Formulations of sufficient statistic based strategies in Finite Horizon Two-Player Zero-Sum Stochastic Bayesian games<|sep|>Because of the multi-agent nature, game theory has great potential in solving or explaining economic, social, and engineering problems. Game theory has been used in addressing AdWord problems [3], enhancing the security of Los Angeles airport [14], advising in presidential election and nuclear disarmament [1, 7], explaining and anticipating disease spreading [6], and many other problems. One common property of these problems is that the individuals or agents in the problems have their own private information not shared with the others. For example, didders in AdWord problems may not reveal its budget to the other bidders. If one or more agents in a game don’t have complete information about the game, we call the game a game with incomplete information, which was ﬁrst introduced in [13]. In this case, a player in the game makes its strategy according to its observations like the other players’ actions and/or its own payoﬀ. Two-player zero Nabiha Nasir Orpa and Lichun Li Department of Industrial and Manufacturing Engineering, FAMU-FSU College of Engineering, 2525 Pottsdamer St, Tallahassee, FL 32310, USA E-mail: no18k@my.fsu.edu, lichunli@eng.famu.fsu.edu
Are stable instances easy?<|sep|>Computational complexity theory as we know it today is concerned mostly with worst-case analysis of computational problems. For example, we say that a problem is NP-hard if the existence of an algorithm that correctly decides every instance of the problem implies that SAT can be decided in a polynomially equivalent time complexity. However, the study of decision and optimization problems is motivated not merely by theoretical considerations. Much of our interest in such problems arises because they formalize certain real-world tasks. From this perspective, we are not interested in all problem instances, but only in those which can actually occur in reality. This is often the case with clustering problems, which are ubiquitous in most ﬁelds of engineering, experimental and applied science. Any concrete formulation of the clustering problem is likely to be NP-hard. However this does not preclude the possibility that the problem can be solved efﬁciently in practice. In fact, in numerous application areas, large-scale clustering problems are solved on a regular basis. As mentioned above, we are only interested in instances where the data is actually made up of fairly well-deﬁned clusters - the instances where solving the problem is interesting from the practical perspective. Put differently, the usual way for proving that clustering is NP-hard is by a reduction to, say, SAT. This reduction entails the construction of instances for the clustering problem, such that the existence of an algorithm that can solve all of them efﬁciently implies the existence of an algorithm that efﬁciently solves SAT. However, it may well be the case that all these instances are clearly artiﬁcial, and solving them is of no practical interest. As a concrete example, consider the problem of clustering protein sequences into families. Out of the enormous space of all possible sequences, only a tiny fraction is encountered in nature, and it is only about these (or slight modiﬁcations thereof) that we actually care. Our case in point is the Max-Cut problem, which can be thought of as a clustering into two clusters. It is well known that this problem is NP-complete, and so it is believed that there is no algorithm that solves it correctly on all graphs, in polynomial time. In this work we strive to identify properties of instances of the Max-Cut problem (i.e., of weighted graphs), which capture the notion that the input has a well-deﬁned structure w.r.t Max-Cut (i.e., the maximal cut “stands out” among all possible cuts). Our goal is to show that Max-Cut can be solved efﬁciently on inputs that have such properties. Consideration of a similar spirit have led to the development of Smoothed Analysis initiated in [15], (see [16] for some of the exciting developments in that area. The similarity has two main facets: (i) Both lines of research attempt to investigate the computational complexity of problems from a non-worst-case perspective, (ii) Both are investigations of the geometry of the instance space of the problem under consideration. The goal being to discover interesting parts of this space in which the instances have complexity lower than the worst case. Viewed from this geometric perspective, the set-up that we study here is very different than what is done in the theory of smoothed analysis. There one shows that the hard instances form a discrete and isolated subset of the input space. Consequently, for every instance of the problem, a small random perturbation is very likely to have low computational complexity. In the problems that we study here the situation is radically different. The “interesting” instances (stable instances as we shall call them) are very rare. Indeed, it is not hard to show that under reasonable models of random instances the probability that a random instance be stable is zero, or at least tends to zero as the problem size grows. What we wish to accomplish is to efﬁciently solve all instances within this subspace. We claim that this tiny set is interesting because it includes all realistic clustering problems. The notion of stability is central to our work. This is a concrete way to formalize the notion that the only instances of interest are those for which small perturbation in the data (which may reﬂect e.g. some measurement errors) do not change the optimal partition of the graph. Deﬁnition 1.1. Let W be an n × n symmetric, non-negative matrix. A γ-perturbation of W, for γ ≥ 1, is an n × n matrix W ′ such that ∀i, j = 1, . . . , n, Wi,j ≤ W ′ i,j ≤ γ · Wi,j. Let (S, [n]\S) be a maximal cut of W, i.e. a partition that maximizes � i∈S,j /∈S Wi,j. The instance W (of the Max-Cut problem) is said to be γ-stable, if for every γ-perturbation W ′ of W, (S, [n]\S) is the unique maximal cut of W ′. However this deﬁnition is, perhaps, not sufﬁcient. Consider two bipartite graphs which are joined togther by a single edge. The resulting graph is γ-stable for all γ, but the alignment of the two bipratite graphs with respect to one another completely depends on the adjoining edge. Hence, to better capture our intution of what it means for a solution to be stable, it is reasonable to demand that in addition to stability the graph contains no small cuts. We show that the combination of both these properties indeed allows solving Max-Cut efﬁciently (Example 4.3). In section 3 we present an algorithm that solves correctly and in polynomial time γ-stable instances of MaxCut: (i) On simple graphs of minimal degree δ, when γ > 2n ∆n. In section 4 we explore several spectral conditions which make Max-Cut amenable on stable instances. This involves analyzing the spectral partitioning heuristic for Max-Cut. In particular, we show that Max-Cut can be solved efﬁciently on (locally) stable expander graphs, and on graphs where the solution is sufﬁciently distinct from all other cuts. We conclude by deducing an improved approximation bound for the Goemans-Williamson algorithm on stable instances, and by showing that MaxCut is easy in a certain random model for such instances. Finally, we should mention that this is just a ﬁrst step. In particular, it is of great interest to study more permissive notions of stability where a small perturbation can slightly modify the optimal solution. There are also other natural ways to capture the concept of stability. Similar considerations can be applied to many other optimization problems. Some of these possibilities are brieﬂy discussed below, but these questions are mostly left for future investigations.
Inner structure of ZnO microspheres fabricated via laser ablation in superfluid helium<|sep|>Dielectric microspheres have the ability to conﬁne light to a small volume with a high quality factor through internal total reﬂection forming the electromagnetic eigen modes known as whispering gallery modes (WGMs) [1]. The strong coupling between light and matter oﬀers a variety of applications such as microlasers [2], enhanced nonlinear optical devices [3], and cavity quantum electrodynamics’ platform [1]. The light-matter coupling can be naturally resulted by fabricating microspheres from materials having large oscillator strength. Excitons in direct band gap semiconductors have large oscillator strengths [4] and high luminescence quantum yields, which make them suitable to be coupled with the microcavity optical modes. The fabrication of semiconductor microspheres remains challenging [5, 6] owing to their crystalline structure. This can be compared with the ease of fabrication of the amorphous microspheres of silica or polymer [7,8]. As the symmetry of the crystal structure determines the thermodynamically favored shape of the materials, the crystal with slow growth rate results in a non-spherical shape. Laser ablation is one of the widely used methods to fabricate microand nanoparticles from various materials including semiconductors [9], which is the reverse of the slow crystal growth. In particular, nanosecond-pulsed laser ablation in superﬂuid helium can produce semiconductor microspheres with high symmetry and smooth surface [10,11]. The fabricated microspheres show a crystalline structure even at the surface and at remarkably low threshold WGM lasing with continuous-wave laser excitation [12]. The laser ablation in superﬂuid helium can also produce metallic microspheres [13] and semiconductor nanoparticles [14]. An in-depth investigation of the microscopic fabrication mechanism is necessary to open the possibilities of the method targeting the microsphere fabrication with diﬀerent materials under diﬀerent conditions [15]. The observation of the inner structure of the fabricated microspheres would provide essential insight into the fabrication mechanism. Although semiconductor spheres with a size ≲ 300 nm fabricated via the same method had very few dislocations or defects and were proved to be single crystals by transmission electron microscopy (TEM) [12], the investigation of the detailed inner structure of the microspheres with a size of ≳ 1 µm is diﬃcult owing to limited electron beam penetration depth. Here, we have demonstrated that the semiconductor ZnO microspheres fabricated via laser ablation in superﬂuid helium contain bubble-like voids with a size from a few tens of nanometers to sub-micrometers. Through experiments, we also proved that the microspheres with voids can maintain WGM resonances. Based on the calculation of light intensity distribution within the microsphere, it can be assumed that the a void at the center of the microsphere can have little eﬀect on WGMs. Furthermore, all the observed microspheres contained voids, although their sizes and the locations were diﬀerent. The presence of a large number of voids indicates that helium gas or the gas phase of any ablated material may play some role during the formation of the microspheres after laser ablation. Moreover, our ﬁndings suggest that the size and location of the voids are some hidden parameters that explain the diﬀerences among the fabricated microspheres of the quality factor of the WGMs [11].
Exact Scalar Minimum Storage Coordinated Regenerating Codes<|sep|>Codes allow to implement redundancy in distributed storage systems so that device failures do no hurt the whole system. Yet, to keep preventing failures, once failures have occurred, codes must be repaired: the redundancy level must be kept above some minimum level. The na¨ıve approach to repairing codes consists in decoding the whole code (thus downloading all blocks) so as to encode it again to recreate the few lost blocks. This induces huge repair costs in term of network bandwidth. It has recently been shown that this repair cost can be signiﬁcantly reduced by repairing without decoding using regenerating codes. Lower bounds on costs (i.e., tradeoffs between storage and bandwidth) have been established for both the single failure case [1], [2], and the multiple failures case [3]–[5]. Adaptive regenerating codes, departing from the other studies by allowing the number of devices involved to differ between repairs, have been deﬁned in [3]. The two extreme points of the optimal tradeoffs are Minimum Bandwidth (MBR/MBCR), which minimizes repair cost ﬁrst, and Minimum Storage (MSR/MSCR), which minimizes storage ﬁrst. Codes matching these theoretical tradeoffs can be built using non-deterministic schemes such as random linear network codes. However, non-deterministic schemes for regenerating codes are not desiderable since they (i) require a great ﬁeld size, (ii) require homomorphic hash functions to provide basic security (integrity checking), (iii) cannot be turned into systematic codes, which offer access to data without decoding, and (iv) provide only probabilistic guarantees. Deterministic schemes overcome these issues by offering exact repair (i.e., during a repair, the regenerated block is equal to the lost block and not only equivalent). For the single failure case (t = 1), code constructions with exact repair have been given for both the MSR point (n, k, d ≥ 2k − 2 [6] and n, k, d when the size of the ﬁle is inﬁnite [7], [8]) and the MBR point (n, k, d [6]) where n is the number of encoded blocks, k is the number of original blocks, and d is the number of devices contacted during repairs. Recent works on this problem are surveyed in [9]. However, the existence of codes supporting the exact repair of multiple failures (t > 1) (i.e., exact coordinated/adaptive regenerating codes) is an open question. In this paper, we focus on this problem, thus extending our previous work on coordinated regenerating codes in [3] with exact repair. We consider the case of n, k, d > k, t > 1 for scalar constructions (i.e., β = 1) and make the following contributions: • In the line of exact scalar minimum storage regenerating codes [6], [10], [11], we propose exact scalar minimum storage coordinated regenerating codes (MSCR) for the case n, k = 2, d ≥ k, t = n − d. This interference alignment based construction is inspired by [10], [11]. (Section III) • Inteference alignment has been applied to scalar MSR codes by aligning the various interferences independently. We show that when k ≥ 3, aligning interferences independently, as in [10], [11], is not sufﬁcient to repair exactly scalar MSCR codes. (Section IV). Note that these results, which correspond to the MSCR point, also apply to exact scalar adapative regenerating codes [3]. As explained earlier, most previous works have been limited to single failures (t = 1). For the multiple failures, there only exist results for the case n, k, d = k, t = n − k, a degenerated case where the repair of regenerating codes and the na¨ıve approach to repairing erasure correcting codes are the same. In this case, the exact repair of MSCR boils down to performing, in parallel, the repair of t independent erasure correcting codes [5]. A similar construction exists for MBCR codes [12]. The position of our codes among existing codes constructions is detailled in Section V.
Relativistic causality and clockless circuits<|sep|>Regular and important advances in semiconductor technology allow more and more complex processors to be designed. Meanwhile, progress in computing capacity is ∗ c⃝ ACM, 2011. This is the author’s version of the work. It is posted here by permission of ACM for your personnal use. Not for redistribution. The deﬁnitive version was published in ACM Journal of Emerging Technologies in Computing Systems, Vol. 7, No. 4, Article 20, December 2011, http://doi.acm.org/10.1145/2043643.2043650. accompanied by a need for good representations of physical devices, which are required to remain pertinent at smaller and smaller scales. This trend naturally increases the number of physical properties to be accounted for when implementing logical devices. Appropriate models become mandatory prerequisites for developing new techniques. This particularly is the case when adopting the approach of distributed systems or clockless circuits. Obviously, time plays an important part in the operation of computing systems. The signiﬁcant eﬀects induced by time delays in modern electronic circuits point at limits that one rapidly reaches in a classical framework when looking for physically precise models. The only way to go beyond these limits is to adopt a new framework, able to match physical reality at a more fundamental level. Practical diﬃculties met in disseminating and managing clock signals over complex designs have made the approach of clockless circuits particularly interesting and promising. But removing the clock does not mean that designs are freed from all the constraints associated with time. Time is by itself a very complex notion, which is hardly captured in totality, even in most advanced physical theories. Designing circuits around a single clock appears as a convenient way to implement known and classical properties related to time. In the absence of a clock, other solutions have to be found to ensure the proper time evolution of a circuit. To this end, one must reconsider the classical models of time that have been useful when designing existing electronic circuits. Important progress in the control of time constraints in clockless circuits has been made with the notion of delay insensitivity [12, 22, 24, 2, 26]. By removing any dependence on time lapses due to signal propagation, one becomes able, with a simple set of rules, to ensure the correct functioning of clockless logical devices. With this eﬃcient trick comes a new concept of the way distributed systems can synchronize their actions, by means of communication, in order to realize speciﬁed computations. Remarkably, this approach follows, in the context of logical devices, a line which is very close to that followed by Einstein, in the general context of physics, when introducing the founding concepts of relativity theory [4, 5]. The necessity of constructing a new framework emerges from the remark that no physical system exists that can deliver time over all space simultaneously. Physical time, as it can be observed, is necessarily a spatially localized quantity. To promote time to a physical quantity that can be shared by remote observers, propagation signals must be used. Resulting propagation delays must be taken into account when synchronizing diﬀerent “local times”, that profoundly change the relations between space and time assumed in classical theories. A change of the notion of time has major consequences for the expression of causality [17, 18]. As causality clearly depends on a notion of order in time, a drastic change in the possibility of time ordering, as raised by the relativistic framework, implies a revision of the dependence of all operations, including logical ones, on causal relations. The eﬀect of the change in the properties of time induced by relativity on distributed systems has already been discussed in the literature ([7, 8, 10, 9]). Many formalisms also exist that describe computing concurrency and have provided efﬁcient models for designing and analyzing asynchronous circuits. On one hand, Petri networks [14, 13] and their interpretation called signal transition graphs (STG) [16, 1] have provided powerful representations of partial orders and causal relations in logic circuits. On the other hand, trace semantics, communicating sequential processes (CSP) [6], and related approaches [22, 24, 2, 11, 15], are preferred in modelling complex systems. A formalism sharing both qualities would be particularly helpful for eﬃciently simulating and implementing causal relations in asynchronous circuits of high complexity. We shall show here that a closer account of physical causality, based on the relativistic notion of time ordering, provides a way to modify the formalism of traces so that causal relations can be represented in a natural way. This representation should make them easier to implement in arbitrarily complex designs. In order to fulﬁll that aim, a key step is to give communications between components a representation that naturally satisﬁes the constraints imposed by relativistic time orders. Remarkably, Udding’s rules [22], which are used to deﬁne communications within DI systems, appear to suit this purpose. Rewriting these rules in terms of R-traces, we shall exhibit their intrinsic connection with relativistic causality. In the ﬁrst two sections we recall the profound changes brought by the relativistic framework to the notions of time, time-ordering and causal relations. In the following sections, we introduce generalizations of traces, called R-traces, and use them to rewrite Udding’s rules. We also brieﬂy describe how R-traces map onto the usual traces in a classical environment. In conclusion, we point at some properties and other potential applications of R-traces.
Data linkage algebra, data linkage dynamics, and priority rewriting<|sep|>The data structures involved in programming are quite often dynamic data structures, i.e. data structures that may vary in size and shape. Dynamic data structures are data structures whose constituent parts are linked in one way or another to facilitate the insertion and deletion of constituent parts. Scientiﬁc work on dynamic data structure is generally concerned with speciﬁc dynamic data structures that show a simple shape, such as linked lists and various kinds of tree structures, or with dynamic data structures in general. In the former case, the dynamic data structures concerned are regularly considered in abstraction from their representation by means of pointers. In the latter case, however, dynamic data structures are primarily considered at the level of their representation by means of pointers. Although it is often useful to abstract from the representation by means of pointers, it seems that no serious attempts have been made to provide a setting in which this is possible. The aim of the current paper is to provide such a setting. We introduce an algebra, called data linkage algebra, of which the elements are intended for modelling the states of computations in which dynamic data structures are involved. We also present a simple model of computation, called data linkage dynamics, in which states of computations are modelled as elements of data linkage algebra and state changes take place by means of certain actions. We describe the state changes and replies that result from performing those actions by means of a term rewriting system with rule priorities [2]. Term rewriting systems take an important place in theoretical computer science. Moreover, because term rewriting is a practical mechanism for doing calculations, term rewriting systems have many applications in software engineering. Term rewriting systems with rule priorities, also called priority rewrite systems, were ﬁrst proposed in [2]. Further studies of priority rewrite systems can, for example, be found in [23, 37, 40, 42]. Applications of priority rewrite systems are found in various areas, see e.g. [18, 31, 45]. The rule priorities add expressive power: the reduction relation of a priority rewrite system is not decidable in general. It happens that it is quite convenient to describe the state changes and replies that result from performing the actions of data linkage dynamics by means of a priority rewrite system. Moreover, the priority rewrite system in question turns out computationally unproblematic: its reduction relation is decidable. We take the view that the behaviours produced by sequential programs under execution are threads as considered in basic thread algebra [7] (see also [12, Chapter 2]).1 These threads represent in a direct way the behaviours produced by instruction sequences under execution. A thread proceeds by performing actions in a sequential fashion. Upon each action performed by a thread, a reply from the execution environment, which takes the action as an instruction to be processed, determines how the thread proceeds. In [13], basic thread algebra has been extended with services, which represent an abstract view on the behaviours exhibited by the components of an execution environment that are capable of processing particular instructions independently, and use and apply operators, which have to do with the eﬀects of the interaction between threads and services that takes place during instruction processing (see also [12, Chapter 3]). The state changes and replies that result from performing the actions of data linkage dynamics can be achieved by means of services. In the current paper, we explain how basic thread algebra extended with services and use operators can be combined with data linkage dynamics such that the whole can be used for studying issues concerning the use of dynamic data structures in programming. Data linkage dynamics is an upgrade of molecular dynamics, which was ﬁrst described in [3]. The name molecular dynamics refers to the molecule metaphor used to explain the model. By that, there is no clue in the name itself to what it stands for. To remedy this defect, the upgrade has been renamed to data linkage dynamics. The upgrading is mainly concerned with the features to deal with values and the features to reclaim garbage. In data linkage dynamics, calculations in a non-trivial ﬁnite meadow [5, 16, 17], such as a ﬁnite ﬁeld with the multiplicative inverse operation made total by imposing that the multiplicative inverse of zero is zero, can be done. The features to reclaim garbage include: full 1 In [7], basic thread algebra is introduced under the name basic polarized process algebra. Prompted by the development of thread algebra [9], which is a design on top of it, basic polarized process algebra has been renamed to basic thread algebra. garbage collection, restricted garbage collection (as if reference counts are used), safe disposal of potential garbage, and unsafe disposal of potential garbage. In [11], a description of the state changes and replies that result from performing the actions of molecular dynamics was given in the world of sets. In the current paper, we relate this description to the description based on data linkage algebra by widening the former to a description for data linkage dynamics and showing that the widened description agrees with the description based on data linkage algebra. Data linkage dynamics in itself is meant to convey a theoretical understanding of the pragmatic concept of a dynamic data structure as exploited in the practice of programming. Such a theoretical understanding is a valuable complement of the understanding of how speciﬁc programs use dynamic data structures, which is acquired by means of dynamic analysis tools that analyze how programs build and modify them (see e.g. [39]). We expect that a theoretical understanding will become increasingly important to the development of successful software systems. Below, we give a ﬁrst impression of what is to be expected from data linkage dynamics as a setting in which issues concerning dynamic data structures are studied. In work on dynamic data structures, on the one hand issues concerning speciﬁc dynamic data structures, in particular performance issues and computational complexity issues, are studied (see e.g. [22, 34, 38, 44]). In the studies in question, the dynamic data structures concerned are mostly considered in abstraction from their representation by means of pointers. We believe that issues like these ones can also be studied in the setting of data linkage dynamics, and we expect that the use of a single setting facilitates uniformity in the way in which the same issue is approached for diﬀerent dynamic data structures. In work on dynamic data structures, on the other hand issues concerning dynamic data structures in general, particularly issues related to optimization and parallelization of sequential programs that make use of dynamic data structures, are studied (see e.g. [19, 20, 27, 41]). In the studies in question, dynamic data structures are mostly considered at the level of their representation by means of pointers. We consider it likely that, for issues like these ones, more general results can be obtained by studying the issues concerned in the setting of data linkage dynamics, where they can be considered in abstraction from their representation by means of pointers. This paper is organized as follows. First, we introduce data linkage algebra (Section 2). Next, after a short review of priority rewrite systems (Section 3), we present data linkage dynamics (Sections 4, 5, 6, and 7). After that, we review basic thread algebra and its extension with services and use operators (Sections 8 and 9) and explain how this extension of basic thread algebra can be combined with data linkage dynamics (Section 10). Following this, we give the alternative description of data linkage dynamics in the world of sets (Sections 11, 12, and 13). Finally, we make some concluding remarks (Section 14). Some familiarity with term rewriting systems is assumed. The desirable background can, for example, be found in [21, 32, 33]. For convenience, the basic
Influence of structural disorder and Coulomb interactions in the superconductor-insulator transition applied to boron doped diamond<|sep|>The interplay of superconductivity and disorder close to the Anderson-Mott transition has long been of signiﬁcant fundamental interest in understanding quantum phase transitions and emergent phenomena1–3. The pivotal role of disorder in these phase changes has come to the forefront as the inﬂuence of diﬀerent forms of disorder are widely studied4–9. Studies of 2D localized superconductors using the Hubbard model with Anderson on-site disorder have shown that the spectral gap remains ﬁnite even at very high levels of disorder10–12. The transition from macroscopic superconducting coherence to localized regions with a ﬁnite pairing amplitude interspersed by insulating regions has been studied using the BdG theory as well as Monte Carlo analysis12. However, the Coulomb interaction, which is prevalent in many systems close to the Anderson-Mott transition is often neglected6. While the role of structural disorder has been examined in the context of localization of electrons,13 many aspects of the role of structural disorder in the superconductorinsulator transition are yet to be studied. As an unconventional, disordered covalent superconductor boron doped diamond is of fundamental interest in studying the superconductor-insulator transition. Boron doped diamond is a type II superconductor, with a Tc as high as 10 K14,15. Immediately following the unexpected discovery of superconductivity in BDD came along a proposal that BDD may be a new form of high Tc super conductor, based expectedly on the resonating valence band (RVB) pairing mechanism16. However, experimental evidence so far strongly suggests a phonon mediated pairing15,17,18 without any evidence for the involvement of spins. Diamond is intrinsically a high band-gap insulator with a band-gap of 5.5 eV . Boron can be substitutionally incorporated in diamond, creating a deep, narrow acceptor band 0.37 eV above the valence band19,20. While the acceptor states are three fold degenerate, the small spinorbit coupling (6 meV ) as well as the random distribution of boron impurities is thought to lift this degeneracy16. The transition with boron doping from insulator to superconductor has been studied in detail21, with the critical concentration found to be around 3×1020 cm−3. To explain the transition with boron doping, density functional theory studies have indicated a rigid band shift of the Fermi level with doping, however experimental studies suggest impurity states without a rigid band22. It has been illustrated that the Tc in boron doped diamond is limited by the low density of states at the Fermi energy20. Photoemission studies of polycrystalline boron doped diamond have revealed a superconducting gap in the range expected for a weakly coupled Bardeen-CooperSchrieﬀer (BCS) superconductor while the characteristic broadening of the spectra is attributed to disorder23. Scanning transmission spectroscopy studies of BNCD have indicated regions which exhibit superconductivity consistent with the weak BCS limit interspersed with regions which are non-BCS like24. In addition, BNCD grains have also shown broadening of the gap, believed to be related to disorder, as well as to the distribution of grains with diﬀerent onset of Tc25. The sharp diﬀerential conductance peaks (referred to as coherence peaks) associated with s-wave superconductivity have also been observed15. Temperature dependent studies of the gap showed that the diﬀerential conductance conformed in some regions with BCS theory for weak coupling while the temperature dependence did not follow the conventional BCS dependence in other regions24. Theoretical studies employing the coherent potential approximation, with disorder in the form of a random potential at the boron sites, predict an exponentially increasing Tc with boron concentration22,26 for disordered boron-doped crystalline diamond while experimental studies show an initially sharp increase in the Tc with boron incorporation followed by a less prominent increase or even saturation17,21,27,28 indicating deviations from what would be expected from the standard BCS dependence29. The Berlitz theory however has been shown to result in saturation of the Tc which is in good agreement with experimental ﬁndings30. These studies have indicated that the Coulomb interaction between holes in the acceptor band should be signiﬁcant (although it has not been included in the calculations)16,22,26. Density functional theory studies assuming phonon mediated superconductivity with random substitutional boron have suggested that the Tc for BDD could be as high as 55 K31, providing motivation for understanding the role disorder plays in limiting the Tc. In addition, ab − initio calculations have shown a diﬀerence in the bond-length associated with C − C and C − B bonds31. These eﬀects, associated with structural disorder, have yet to be studied theoretically and applied to BDD or BNCD. In this study, we address the role of structural and on-site disorder within the narrow boron acceptor level. We consider a triangular lattice with a single boron acceptor band employing the inhomogeneous BdG theory. Disorder is treated in the form of random potential ﬂuctuations at the boron sites as well as structural disorder in the form of a non-uniform hopping integral in the tight-binding term between adjacent sites. The level of structural disorder is correlated to the atomic boron concentration as this disorder is expected to increase due to local changes in bond length between C − B bonds and surroungding C − C bonds. We include the Coulomb interaction between holes in the acceptor band as it is a narrow band so the eﬀect of the Coulomb interaction needs to be recknoned with16. We study the local pairing amplitude as well as the variation of the mean pairing amplitude, spectral gap and superﬂuid density as the disorder level increases. Through interpretation of the density of states, mean pairing amplitude, spectral gap, local superﬂuid density and phase stiﬀness we isolate the differences between structural and on-site disorder. We illustrate some overlap with experimental work, highlight
Detection of Keplerian dynamics in a disk around the post-AGB star AC Her<|sep|>Disks orbiting AGB and post-AGB stars are thought to play a major role in the late AGB evolution and planetary nebulae formation. In the vast majority of cases, the circumstellar envelopes around AGB stars are roughly spherical and in isotropic expansion at moderate velocities (10 – 20 kms−1), see, for example, Castro-Carrizo et al. (2010). Most protoplanetary and planetary nebulae (PPNe, PNe) show a clear axial symmetry and fast bipolar outﬂows (∼ 100 km s−1), which carry a good fraction of the total mass (∼ 0.1 M⊙) and very large amounts of linear momentum, see, for instance, Bujarrabal et al. (2001) and Alcolea et al. (2001). This spectacular transition takes place in a very short time, a thousand or a few thousand years. The most widely accepted scenario to theoretically explain this evolutionary phase implies that material is reaccreted by the star or a companion from a rotating disk, followed by the launching of very fast jets, in a process similar to that at work in forming stars, see Soker (2002), Blackman & Lucchini (2014), and references therein. Such orbiting disks have been frequently searched for, but their detection has been extremely diﬃcult. Except for one source (the Red Rectangle, see below), in all AGB shells or post-AGB nebulae in which spectroscopical observations allow measuring the velocity ﬁeld, only expansion has been detected. Many of them show oblate, even very ﬂat structures, but without any sign of rotation in the observations. This is the case of axisymmetric shells around semiregular variables (e.g., Libert et al. 2010; Castro-Carrizo et al. 2010) and of ﬂat equatorial structures that are often found in bipolar PPNe (e.g. Alcolea et al. 2007; Castro-Carrizo et al. 2002, 2012). Various indications of the existence of Keplerian disks have been proposed. The very low expansion velocity (<∼ 1 km s−1) sometimes found in the equatorial structures mentioned above suggests that the gas is ejected from a stable, and therefore probably Keplerian, component at long distances to the star, in which outward forces are weak. On the other hand, a class of optically bright post-AGB stars are found to show a remarkable NIR excess, which has been attributed to the emission of hot dust at about 1000 K. The high temperature would show that dust grains are kept close to the star, again in a stable disk structure, see Van Winckel (2003) and van Aarle et al. (2011). These stars are found to be systematically multiple (Van Winckel et al. 2009) and the grains show a high degree of evolution (i.e., a high critallinity; Gielen et al. 2011), which supports this interpretation. Systematic observations of the 12CO and 13CO J=2−1 and J=1−0 lines in objects with such a NIR excess (Bujarrabal et al. 2013a) have shown narrow line proﬁles formed by a single or double peak and weaker wings, which are very similar to those found in the Red Rectangle. These proﬁles are also very similar to those in young stars known to be surrounded by remnants of IS material in rotation and to predictions of models of CO emission from orbiting disks, see Guilloteau & Dutrey (1998), Guilloteau et al. (2013), Bujarrabal & Alcolea (2013), etc. These results led Bujarrabal et al. (2013a) to conclude that the CO lines in these sources probably come from Keplerian disks. However, the unambiguous detection of the Keplerian dynamics requires very accurate spectroscopic observations with high spatial and spectral resolution, able to describe the velocity ﬁeld at subarcsec scales and with spectral resolutions better than 0.5 km s−1. To date, this type of observations has resulted in the detection of rotation in just one evolved object, the Red Rectangle, a well known PPN showing the above-mentioned NIR excess (Bujarrabal et al. 2005, 2013b).
Simulating Using Deep Learning The World Trade Forecasting of Export-Import Exchange Rate Convergence Factor During COVID-19<|sep|>The world has become a global village and trade plays the main role between the countries as an interaction channel. Trade is a process of exchanging products and services  between states or countries and moreover, it can see in between different companies, or  any other substances. Trade plays an awfully important part in setting up a higher global  economy. We can talk about China’s trade as an example of world trade’s importance.  By addressing “World Factory” China is known in numerous nation's views to the  world currently, and the trademark “Made in China” shows up in nearly every corner  of the world [2]. Theories about worldwide trade expanding total efficiency at the nation's level are about as ancient as economics [3]. By the word ‘world trade’ or ‘foreign  trade’, means that it is a process in which required goods or services are received and  domestic products are sold in the global market [4]. Import and export are the two most  important words in world trade, where import means purchasing foreign products or  services and bringing them to one’s home country and export means selling of products  from home country to a foreign country. The relationship between foreign trade and  economic growth is still a topic of debate from ancient times till now [5]. As a major  factor of openness, worldwide trade has made a progressively significant contribution  to economic growth [6]. This relationship (between foreign trade and economic growth)  is based on two approaches- one is growth theory based on import and another one is  growth theory based on export [7]. To capture the perfect view of the relationship between foreign trade and economic growth, time series analysis is always the best way  [8]. The time series analysis method is utilized to analyze and understand past data and  predict the future, generally, time-series data sets are annual data of different periods.  When a set of observations is organized chronologically, then it is called a time series,  time series analysis forms one of the most significant tools of the economist [9]. Longterm time series have various tricks or strategies like recursive, direct, and DirRec  which are examined and compared [10]. In our paper, we collect the world trade data  for the period January 01, 2015-May 30, 2021. The main purpose of the investigation  is to compare the import and export data for all countries for the period January 1st,  2015-May 30, 2021, and based on the result the system predicts the future possibility  of world trade. That's why we apply the time series model LSTM to analyze the data  and prepare future predictions for the next 180 days of 2021.
3-regular matchstick graphs with given girth<|sep|>In August of 1986, a special conference on recreational mathematics was held at the University of Calgary to celebrate the founding of the Strens Collection. Leading practitioners of recreational mathematics from around the world gathered in Calgary to share with each other the joy and spirit of play that is to be found in recreational mathematics, see [8]. Heiko Harborth, one of the presenters, took the chance to insist that “Matchsticks are the cheapest and simplest objects for puzzles which can be both challenging and mathematical”. More than 20 years later our knowledge on matchstick graphs, i. e. noncrossing arrangements of matchsticks, is still very limited. It seems to be hard to obtain rigid mathematical results about them. One of those puzzles asks for the complete r-regular matchstick graph, see deﬁnitions 1.1 and 1.2, with the minimum number of vertices, see e. g. [10]. Checking all possibilities, as one can do in most puzzles, is not that easy for matchstick graphs. There are some computational and mathematical obstructions. These geometric graphs may be ﬂexible, i. e. one can not determine an up to isomorphisms ﬁnite list of sets of coordinates for the vertices. Indeed the smallest complete 3-regular matchstick graph is ﬂexible. Also in the case where the graphs are rigid (i. e. not ﬂexible) it can be a hard task to determine the coordinates of the vertices. As an example we refer the interested reader to [7] where the coordinates of the so-called Harborth graph. i. e. the smallest known 4-regular matchstick graph, were determined. Some minimal polynomials of vertex coordinates have a degree of 22. And indeed testing whether a given planar graph can be realized in the Euclidean plane is an NP-hard problem, see [5, 6]. What is known about this speciﬁc puzzle? Due to the Eulerian polyhedron formula there can not exist (ﬁnite) complete r-regular matchstick graphs for r ≥ 6. For r = 1 the only example is a single edge and for r = 2 the only examples are circles Cn for n ≥ 3. In the next case the smallest possible number n of vertices of a complete 3-regular matchstick graph is 8. We leave it as an easy but entertaining exercise to the reader to proof that complete 3-regular matchstick graphs exist if and only if n ≥ 8 is an even number and that there is at least one example. For r = 4 we have already mentioned that the smallest known example is the so called Harborth graph consisting of n = 52 vertices. Very recently one of the authors proves the non-existence of a (ﬁnite) complete 5-regular matchstick graph, see [12], and indeed a lot of non-trivial mathematics is involved. So only the case r = 4 remains open, but so far it seems to be out of reach. A generalization of this problem were more than one possible edge lengths is allowed, is considered in [1]. In [11] the author considers complete r-regular graphs where the edges have unit length but are allowed to cross. In this article we consider another matchstick puzzle – complete 3regular matchstick graphs with given girth and minimum number of vertices. At the end of 2005 Erich Friedman posed this problem on his “Math magic”-homepages1. He was especially interested in an example for girth g = 4. Very soon Gavin Theobald found such an example consisting of 40 vertices, which was beaten by an example of one of the authors consisting of only 32 vertices in 2006. Locating these examples is a creative and recreational task. For some matchstick problems constructing the minimal example can be quite challenging. But the really hard task is to rigidly prove that no smaller example can exist. Here we want to demonstrate that it is possible, with admittedly quite some effort, to rigidly solve a matchstick puzzle where the minimal answer has 20 vertices. Nevertheless we aim to solve a very speciﬁc puzzle we try to present the underlying ideas and techniques from a more general point of view. In the remaining part of this article we prove that a complete 3-regular matchstick graph with girth 4 exists if and only if n is an even number greater or equal to 20. We give an example of a complete 3-regular matchstick graph with girth 5 consisting of 180 vertices and provide a ﬁrst lower bound on the minimum number of necessary vertices. As a simple consequence from the Eulerian polyhedron formula there are no complete 3-regular matchstick graphs with girth at least 6 (besides from the inﬁnite honeycomb lattice), see Lemma 2.2 and Equation (1). Now let us go into the details. Deﬁnition 1.1 An (incomplete) r-regular matchstick graph M consists of a graph G = (V, E) and an embedding f : V → R2 in the plane which fulﬁll the following conditions: (2) The nodes on the outer face of M all have degree at most r and all other nodes have degree exactly r. (2) If {i, j} ∈ E then we have ∥f(i), f(j)∥2 = 1, where ∥x, y∥2 denotes the Euclidean distance between the vectors x and y. (4) If {i1, j1}, {i2, j2} ∈ E for pairwise different i1, i2, j1, j2 ∈ V then the line segments f(i1)f(j1) and f(i2)f(j2) do not have a common point. So in other words an r-regular matchstick graph M is an embedded planar graph where the inner vertices have degree r and the edges are straight line segments of length 1. Deﬁnition 1.2 We call an r-regular matchstick graph complete if all nodes on the outer face of M have degree exactly r.
Computation of extremes values of time averaged observables in climate models with large deviation techniques<|sep|>The study of high impact rare events, like extreme droughts, heat waves, rainfalls and storms, is a major topic of interest in climate science. Extreme events can have a severe impact on ecosystems and socio-economic systems [1, 17, 18], and it is crucial to better understand their dynamics and statistical properties. A general problem is that it is diﬃcult to sample a suﬃcient amount of rare extreme events to have a robust statistics. Observational records are typically too short to study events with return times longer than a few decades. State of the art general circulation models are computationally extremely expensive, and can be run at most for a few thousands of years, making it unrealistic to study even approximately extreme events with return times longer than a century. In climate science several techniques are adopted to compensate for the lack of data. The general idea is to extract informations about rare, unobserved events from the limited but available statistics of less rare, observed events. Purely statistical approaches are usually framed in the context of extreme value theory [5, 14, 24]. Stochastic weather generators provide to some extent a hybrid statistical-dynamical approach [33, 2]. A new approach entirely based on the dynamics of numerical models was proposed in [26], where we have introduced the use of rare event algorithms to improve the sampling eﬃciency of climate models. These techniques allow to increase the number of extreme events observed for a given computational cost, by generating trajectories that are real solutions of the equations of the model, without additional statistical assumptions. Diﬀerent types of extreme events have diﬀerent spatio-temporal characteristics. Events like wind storms or ﬂash ﬂoods are transient, typically localized phenomena due to large ﬂuctuations of an observable on short time scales (from a few hours to a few days) compared to the typical time scale of the synoptic variability. Events like heat waves or ﬂoods due to persisting rains are larger spatial scale phenomena characterized by large anomalies of the time average of an observable over longer time scales (from several weeks to months). A statistical framework to analyze time persistent events is provided by Donsker-Varadhan large deviation theory. Large deviation theory deals in general with the exponential decay of probabilities of ﬂuctuations in random systems, providing an extension of the law of large numbers and central limit theorem [30]. Typically one obtains a large deviation scaling as asymptotic behavior of probability distributions depending on a small parameter. Donsker-Varadhan large deviation theory is a particular case of large deviation theory, which deals with the scaling of the statistics of time averages over a time T . It predicts the asymptotic behavior of the probability distribution function of the time average of an observable for any large enough T (where the small parameter is given by 1/T ), described by a large deviation rate function. Establishing a large deviation result for a climatic observable would mean that the probability of extremely rare ﬂuctuations of the time average of that observable can be inferred from the probability of much less rare events. For example, the probability of having a (very rare) heat wave characterized by a value a of the temperature anomaly over several months could be obtained from the probability of having a (much less rare) heat wave characterized by the same value a of the temperature anomaly over just a few weeks. With the exception of a few works on multifractal modeling of rainfall [31], the use of large deviation theory to study climate extremes has not been considered until very recently. In [26] we used a rare event algorithm developed for the computation of Donsker-Varadhan type large deviation functions, and applied it to study rare heat waves, although for durations shorter than what necessary to be in the large deviation limit. [13] recently performed a comparison of extreme value theory and large deviation theory based approaches to study time and space averages of climatic observables in an idealized general circulation model. Considering the increasing interest in time persistent climate extremes, it is of interest to explore more in depth the applicability of large deviation theory to study rare ﬂuctuations of time averages of climatic observables, and to discuss the methodological challenges one faces to perform this type of analysis. The ﬁrst step of an empirical analysis of the large deviations of the time average of an observable is to determine the minimum length of the averaging period T for which the convergence to the large deviation limit is satisﬁed up to an acceptable degree of accuracy. The second step is to determine if the data available are enough to study the non Gaussian tails of the rate function for the chosen range of values of T . [27] have recently provided a systematic analysis of the convergence of statistical estimators of large deviation functions of sums of independent and identically distributed random variables, including a detailed study of the maximum range of ﬂuctuations for which the rate functions can be computed, given a ﬁnite sample of data. These results in principle could be used, with some additional considerations, to analyse the large ﬂuctuations of the time averages of an observable from time series of a dynamical process [27], and could therefore be of interest to perform precise large deviation analysis of climatic observables. Given the typical limitations in the amount of available data in these applications, it is likely that it will be rather diﬃcult to go substantially (if at all) beyond the Gaussian regime given by the central limit theorem. In this case, one could use rare event algorithms dedicated to compute Donsker-Varadhan type large deviation functions in numerical models that have been developed in recent years [16, 21], and very recently applied to study heat waves in a climate model [26]. The tools to perform a large deviation analysis of time averages of climatic observables are thus available; however, a systematic description and evaluation of such tools speciﬁcally framed for climate studies is still lacking. In this paper we describe how to use large deviation principles and rare event algorithms in a climatic study. The paper is structured as follows. First we introduce the basic formalism of Donsker-Varadhan large deviation theory. Then we provide a detailed description of how to compute large deviation functions from time series of an observable of a dynamical system, following [27]. We give a practical example of such analysis, computing large deviation functions of the time average of the European surface temperature from long simulations with the climate model Plasim [12]. We then show how the tails of the large deviation functions can be computed very eﬃciently using the rare event algorithm we have used in [26], giving here more details about the method and focusing on the role of large deviation theory. Finally we discuss the potential for further studies.
Statistical Estimation of Mechanical Parameters of Clarinet Reeds Using Experimental and Numerical Approaches<|sep|>Clarinettists experience every day the crucial importance of clarinet reeds for the quality of sound. Their characterization is a real challenge for musicians who wish to obtain reeds that are suited to their personal needs. The present paper address this complex ﬁeld of research. Its scope is restricted to the development of an objective method for a mechanical characterization of single reeds of clarinet type. >From the shape and the resonance frequencies of each individual reed (measured with heterodyne holography), we intend to deduce the mechanical properties of the material composing it. A subsequent study should then examine how these mechanical properties are correlated with the musical properties of the reeds. Generally, the physicist chooses a model in order to validate it by observations. In the present study, the complexity of the problematic forced us to adopt the reverse attitude: We observe the mechanical behavior of clarinet reeds with a statistically representative sample and exploit afterward the statistical results for establishing a satisfactory mechanical model designed with a minimal number of parameters. Natural materials, as wood or cane, are often orthotropic and exhibit a diﬀerent stiﬀness along the grain (longitudinally) as in the others directions. The problem is then obviously multidimensional. Nevertheless, reed makers classify their reeds by a single parameter: the nominal reed "strength" (also called "hardness"), in general from 1 to 5, which basically reﬂects the stiﬀness of the material (cane, Arundo donax L.), since all reeds of the same model have theoretically the same shape. The method of measurement is generally not publicized by manufacturers, but this "strength" is probably related to the static Young modulus in the longitudinal direction EL. "Static" (i.e. low frequency) measurements of the elastic parameters of cane are available in the literature, for instance Spatz et al. [1]. A viscoelastic behavior has been reported in experimental situations (see e.g. Marandas et al. [2], Ollivier [3] or Dalmont et al. [4]) and this fact seems generally well accepted in wood sciences and biomechanics (for instance Speck et al. [5, 6]). Marandas et al. proposed a viscoplastic model of the wet reed. Viscoelastic behavior for cane was already demonstrated by Chevaux [7], Obataya et al. [8, 9, 10, 11] and Lord [12]. These authors study only the viscoelasticity of the longitudinal Young modulus EL, leaving aside the case of the shear modulus in the longitudinal/tangential plane GLT . Furthermore, they give no really representative statistics about the variability of the measured parameters. The observation of mechanical resonance frequencies can be achieved by diﬀerent methods. The methods used by Chevaux, Obataya and Lord are destructive for the reed, which cannot be used for further musical tests. On the contrary, holography is a convenient non-destructive method, the reed being excited by a loudspeaker. For instance Pinard et al. [13] measured with this method the frequency of the 4 lowest resonances and focused their attention on the musical properties of the reeds. The digital Fresnel holography method was used by Picart et al. [14, 15] and Mounier et al. [16] to measure high amplitude motion of a reed blown by an artiﬁcial mouth. Guimezanes [17] used a scanning vibrometer. Recent technological developments provide very eﬃcient and convenient measurements with holography, without having to manually identify the modes of resonance and to be satisﬁed with a single picture of their vibration: in a few minutes hundreds of holograms are acquired showing the response of a reed for many frequencies. The temperature and the moisture content can be considered as constant during a measurement series1. The Sideband Digital Holography technique provides additional facilities (see 2.1.1). Diﬀerent authors (among them Casadonte [18, 19], Facchinetti et al. [20, 21] and Guimezanes [17]) modeled the clarinet reed by Finite Elements Method (FEM) and computed the ﬁrst few eigenmodes. They chose appropriated values of the elastic parameters in the literature, ignoring however viscoelastic behavior. The goodness of ﬁt between observations and model was of 1The signiﬁcantly lower correlations between resonance frequencies (compared to our data) shows that it was probably not the case in Pinard’s study. This fact may also reﬂect an unprecise determination of the resonance frequencies. secondary importance, except for Guimezanes. This latter author built a 2-D elastic model of the reed with longitudinally varying parameters. He ﬁtted his model quite adequately with his observations (only 5 resonances were measured), but the ﬁtted parameters seem not really plausible physically. His model didn’t respect the assumption of a radial monotonically decrease of stiﬀness from the outer side to the inner side of the cane. Under such conditions, the frequency of the ﬁrst resonance would increase in comparison to homogeneous material, and not decreased, as observed experimentally. In Section 2 the measurement method is presented. The experimental setup is described in Section 2.1 and the method for observing resonance frequencies is detailed in 2.2. The results for 55 reeds are given in Section 3 (statistics, Principal Component Analysis (PCA)[22]). In Section 4, the development and the selection of a satisfactory mechanical model with minimal structure is described. First, a numerical analysis of the resonance frequencies of a reed assumed to be perfectly elastic is done by Finite Element Method (FEM), and a metamodel computing the resonance frequencies from elastic parameters is given in Section 4.3. This allows solving the inverse problem in a fast way. However, because the elastic model is not very satisfactory, viscoelasticity has to be introduced and some parameters are added to the model in Section 4.4. The viscoelastic model has however too many degrees of freedom, according to PCA. Consequently, the viscoelastic parameters of the model are assumed to be correlated and PCA indicates that these parameters can be probably reconstructed from 4 orthogonal components, as a linear combination, by multiple regression (Section 5). The relationships between the components and the viscoelastic parameters is given, and ﬁnally the resulting values for these parameters are discussed in Section 5.3 and compared with the results of the literature.
Robust Sequential Steady-State Analysis of Cascading Outages<|sep|>Modern society depends on the secure and reliable operation  of the electric grid. Cascading outages represent a class of  events that can significantly impact the electric grid and  create wide-spread socio-economic damages. The North  American Electric Reliability Corporation (NERC) defines  cascading outages “as the uncontrolled loss of any system  facilities or load, whether because of thermal overload,  voltage collapse, or loss of synchronism, except those  occurring as a result of fault isolation” [1]. likelihood of cascading outages occurring in the grid.  Amongst those, NERC standards TPL-001-4 [2] and CIP014-2 [3] require evaluation of the impact of extreme  contingencies that may cause cascading outages. On the  operations side, Emergency Operations EOP-003-1 [4]  requires that: “After taking all other remedial steps, a  Transmission Operator or Balancing Authority operating  with insufficient generation or transmission capacity shall  shed customer load rather than risk an uncontrolled failure of  components or cascading outages of the Interconnection” by  implementing Special Protection Systems (SPS) and other  routines to automatically shed load under adverse events. simulation framework must (i) solve extreme contingency  cases from initial conditions that are far from the solution; (ii)  identify and locate collapsed (infeasible) grid locations; and  (iii) include frequency state into its framework to model the  impact of generator droop control and automatic protection schemes such as frequency dependent load shedding. A  framework capable of satisfying these requirements will  allow planning engineers to distinguish a cascaded grid  scenario from a divergent scenario. Furthermore, such a  framework would be able to converge infeasible test cases  (i.e., cases operating beyond the tip of the nose curve) and  allow planning engineers to locate weak sections of the grid.  Additionally, it is also important for the framework to  robustly simulate any remedial actions in order to accurately  analyze the grid during a cascade outage. These include  Under-Frequency Load Shedding (UFLS) and Under Voltage  Load Shedding (UVLS) schemes. have tried to incorporate these features in both sequential  power flow analysis [5], [6] as well as transient analysis [7][8]. In general, transient analysis is slow and is therefore only  performed for critical contingencies in the system. A  sequential steady-state power flow analysis offers runtime  advantages to study a broad range of outages. However,  existing steady-state tools in the industry and academia do  not  satisfy  the  previously  stated  requirements  of infeasible cases. This is highlighted in a recent report [9] by  the Task Force on Understanding, Prediction, Mitigation,  and Restoration of Cascading Failures that stated that “the  tools for directly assessing and mitigating large cascading  failures are not yet well developed.” addressing key elements required to develop a robust tool for  cascading analysis. [10]-[11] have improved the robustness  of convergence of complex and large “hard-to-solve” cases  by incorporating limiting and homotopy methods. Modeling  the frequency state is also broached by many existing works  [11]-[13]; however, these approaches use outer loops to  resolve discontinuous models which can cause simulation  convergence issues. Furthermore, detection and localization  of infeasible grid states is also ongoing work [14]-[15].  Continuation power flow [16] was previously proposed to  solve infeasible test cases (operating beyond the tip of the  nose curve), but requires solving the base case first, which  itself is hard to achieve for complex, large-scale test cases  when the initial condition is far from the solution. Other  optimization-based methodologies [15] have also attempted  to solve infeasible test cases, but they generally suffer from  a lack of convergence robustness and have only been tested  on small, well-conditioned networks. formulation in [11] proposes to model the transmission and  distribution grid networks as equivalent circuits. This  approach has been demonstrated to enable robust  convergence of complex transmission or distribution  networks via use of circuit simulation methods [11].  Recently, we have expanded this simulation framework to  further solve infeasible test networks and locate weak system  areas that would correspond to a collapsed grid state [14]. In  this paper, we expand the circuit-based formulation to model  frequency deviations while implicitly capturing UFLS and  UVLS without the need for outer loops. This enables us to  develop a framework to accurately simulate cascading  outages while localizing and identifying collapsed sections of  the grid.
Probabilistic Latent Tensor Factorization Model for Link Pattern Prediction in Multi-relational Networks<|sep|>Modeling relational data has been an active area of research in recent years, and is becoming an increasingly important problem in many applications such as social network analysis and recommender systems. Link prediction [1] as one basic challenge is concerned with predicting unobserved links between object pairs based on the observed structure in the network. A typical example is a social network where people are linked via explicit relations, such as friendship or membership; or implicit ones like the sharing of similar interests. Up to now, most of the related models developed for link prediction either consider only single-type relations among objects or treat the diﬀerent relations in the network homogeneously [2] [3], thus ignoring the multi-dimensional nature of interactions and the potential complexity of the interaction schemes in the networks. In this paper, we focus on the task of predicting multiple relation types among object pairs in multi-relational networks. For that, we deﬁne the overall relations between each pair of objects as a link pattern, which consists in interaction Fig. 1. Example of link pattern prediction task. There are four relation types (Alumni relations, Colleagues, Gym membership and Common interest in the social network. The sets of ”?” represent unknown link patterns. pattern and connection structure among objects. This task is illustrated in Figure 1. The left part of Figure 1 shows a social network composed of set of individuals with multiple relations among them, where the link patterns involving multiple relations between certain object pairs are unobserved (indicated by ”?” in the ﬁgure). The task here is to infer the missing link patterns from the observed part of the network, which we refer to as Link Pattern Prediction (LPP) problem. With the extracted link patterns, the ﬁne and subtle network structure in the multi-relational networks can be captured eﬀectively, and can be used to improve the range and performance of various aspects of social network applications, including community detection and person recommendation with diﬀerent social roles. Therefore, in the context of Link Pattern Prediction problem, we propose a probabilistic tensor factorization framework to model the multi-relational data by considering the tensor factorization as a latent factor model. Our model addresses two major challenges that are ignored by previous work on link prediction. The ﬁrst challenge is the multi-relational nature of networked data. In addition to using latent factors to characterize object features, we also introduce another latent factor for diﬀerent relations to capture the correlations among multiple relation types and reveal the impact of distinct relation types on performance quality. The second challenge is data sparsity problem. For example, the social networks are usually very sparse, and the presence of relations among users only hold a very small number of all possible pairs of nodes. To solve the overﬁtting problem caused by the sparse data we extend our probabilistic model by employing Bayesian learning method to infer the latent factors. The Bayesian treatment can signiﬁcantly avoid overﬁtting by placing prior information on the model parameters and handling the missing data easily. Moreover, we deal with the parameter learning by an eﬃcient Markov Chain Monte Carlo (MCMC) method in the real world datasets. We conduct experiments on several real world multi-relational datasets, the empirical results demonstrate the improvement of prediction accuracy and the eﬀectiveness of our models. The rest of the paper is structured as follows. We ﬁrst brieﬂy review related work in Section 2. Then we introduce our link pattern prediction task and formulate the probabilistic latent tensor factorization model for solving the LPP problem in Section 3. We also provide a fully Hierarchical Bayesian treatment to optimize the probabilistic model and derive an eﬃcient Markov Chain Monte Carlo optimization method in Section 4. We describe experiments on three realworld datasets to study the eﬃciency of our model and compare it to several models in Section 5. In Section 6 we present conclusions and future work.
2D velocity fields of simulated interacting disc galaxies<|sep|>Recently it has become technically feasible to observe the full 2D velocity ﬁeld (VF) of local galaxies in optical wavebands using integral ﬁeld units (IFUs) such as SAURON (e.g. Ganda et al. 2006) or Fabry-Perot interferometry (e.g. Chemin et al. 2006, Garrido et al. 2002). For intermediate and high redshift galaxies, however, there are by now hardly any observational studies of 2D velocity ﬁelds available. Flores et al. (2006) observed the 2D velocity ﬁeld of 35 galaxies at intermediate redshift (0.4 < z < 0.75) using FLAMES/GIRAFFE at VLT. One aspect was to investigate the redshift evolution of the Tully-Fisher relation. A diﬀerent approach, used by our group, was presented by Ziegler et al. (2006) and Kutdemir et al. (2007) who utilize multiple-object spectroscopy from the VLT with different slit positions on each galaxy in order to construct the full velocity ﬁeld for each galaxy. Most of the other studies of distant, faint, and small galaxies are still based on slit spectroscopy (e.g. Vogt 2001, B¨ohm et al. 2004). To account for distortions and irregularities in the veloc ity ﬁelds is in both cases critical, especially when aiming at a distant Tully-Fisher study. In two recent papers we showed that observational constraints and galaxy–galaxy interactions can severely inﬂuence the determination of the rotation curve of observed disc galaxies (Kapferer et al. 2006, Kronberger et al. 2006). In this paper we investigate to what extent the full 2D velocity ﬁeld of a galaxy can be used to gain information on its internal kinematics and can hence improve the quality of e.g. Tully-Fisher studies. This question is also important to possibly disentangle diﬀerent interaction processes by mapping 2D velocity ﬁelds and to study their impact on galaxy evolution. We focus on the question how the visibility of distortions depends on the redshift of the observed galaxy, i.e. the actual spatial resolution of the galaxy. For that investigation we place a galaxy at diﬀerent redshifts and bin the velocity according to the spatial resolution at this redshift. A similar study for observed galaxies was presented by Epinat et al. (2006). Additionally to the examination of 2D velocity ﬁelds of intermediate redshift galaxies from optical spectroscopy we investigate the performance of near-infrared integral ﬁeld spectrographs that are used together with adaptive optics. As a prototype we take the characteristics of
Robust Distributed Accelerated Stochastic Gradient Methods for Multi-Agent Networks<|sep|>Advances in sensing and processing technologies, communication capabilities and smart devices have enabled deployment of systems where a massive amount of data is collected by many distributed autonomous units to make decisions. There are numerous such examples including a set of sensors collecting and processing information about a time-varying spatial ﬁeld (e.g., to monitor temperature levels or chemical concentrations) (Blatt et al., 2007), a collection of mobile robots performing dynamic tasks spread over a region (Nedi´c et al., 2018), federated learning on edge devices (Koneˇcn`y et al., 2016; McMahan et al., 2017), on-device peer-to-peer learning (Koloskova et al., 2019) and distributed model training across a network or computers (Arjevani et al., 2020; G¨urb¨uzbalaban et al., 2020; Scaman et al., 2018). In such systems, most of the information is often collected in a decentralized, distributed manner, and processing of information has to go hand-in-hand with its communication and sharing across these units over an undirected network G = (V, E) deﬁned by the set of (computational units) agents V = {1, 2, . . . , N} connected by the edges E ⊆ V ×V. In such a setting, we consider the group of agents (i.e., the nodes) collaboratively solving the following optimization problem: where each fi : Rd → R is known by agent i only and therefore referred to as its local objective function. We assume each fi is µ-strongly convex with L-Lipschitz gradients (hence f is also µ-strongly convex with L-Lipschitz gradient and we refer to κ = L/µ as its condition number). We also use x∗ to denote the unique optimal solution of (1). In addition, we denote the local model of node i at iteration k by x(k) i ∈ Rd. We consider the setting where each agent i has access to noisy estimates ˜∇fi(x) of the actual gradients satisfying the following assumption: We assume at iteration k, node i has access to ˜∇fi � x(k) i , w(k) i � which is an estimate of E � ˜∇fi � x(k) i , w(k) i � ���x(k) i � = ∇fi � x(k) i � , E ���� ˜∇fi � x(k) i , w(k) i � − ∇fi � x(k) i ���� 2 ���x(k) i To simplify the notation, we suppress the w(k) i dependence, and denote ˜∇fi � x(k) i , w(k) i � by This arises naturally in distributed learning problems where fi(x) represents the expected loss Eηi [fi(x, ηi)] where ηi are independent data points collected at node i (see e.g. Pu and Nedi´c (2018); Lan et al. (2020); Pu et al. (2019)). For this setting, ˜∇fi(x) is an unbiased estimator of ∇fi(x) which we assume satisﬁes the bounded variance assumption of Assumption 1. In Appendix E, we will discuss the unbounded variance assumption (Assumption 5) that extends Assumption 1, and show that all the main results in the paper can be extended. Note that in our setting, a master node that can coordinate the computations is not available unlike the master/slave architecture studied in the literature (see e.g. Mishchenko et al. (2018); Agarwal and Duchi (2011); Hakimi et al. (2019); Lee et al. (2018); Meng et al. (2016); Jaggi et al. (2014); Xin and Khan (2020)). Furthermore, our setting covers an arbitrary network topology that is more general than particular network topologies such as the complete graph or ring graph. Deterministic variants of problem (1) have been studied extensively in the literature. Much of the work builds on the Distributed Gradient (DG) method proposed in Nedic and Ozdaglar (2009) where each agent keeps local estimates of the optimal solution of (1) and updates by a combination of weighted average of neighbors’ estimates and a gradient step (normalized by the stepsize αk) of the local objective function. Nedic and Ozdaglar (2009) analyzed the case with convex and possibly nonsmooth local objective functions, constant stepsize αk = α > 0, and agents linked over an undirected connected graph and showed that the ergodic average of local estimates of the agents converge at rate O(1/k) to an O(α) neighborhood of the optimal solution of problem (1) (where k denotes the number of iterations). Yuan et al. (2016) considered this algorithm for the case that local functions are smooth, i.e., ∇fi(x) are Lipschitz continuous, and when fi(x) are either convex, restricted strongly convex or strongly convex. For the convex case, they show the network-wide mean estimate converges at rate O(1/k) to an O(α) neighborhood of the optimal solution, and for the strongly convex case, all local estimates converge at a linear rate O(exp(−k/Θ(κ))) to an O(α) neighborhood of x∗.1 There have been many recent works on developing new distributed deterministic algorithms with faster convergence rate and exact convergence to the optimal solution x∗. We start by summarizing the literature in this area that are most relevant to this work. First, Shi et al. (2015) provides a novel algorithm which can be viewed as a primal-dual algorithm for the constrained reformulation of problem (1) (see Mokhtari and Ribeiro (2016) for this interpretation) that achieves exact convergence with linear rate to the optimal solution; however the linear convergence rate with the recommended stepsize is ρ = 1 − O( 1 κ2 ) where κ is the condition number (see Table 1). This convergence guarantee will be slow for ill-conditioned problems when κ is large. Second, Qu and Li (2018) proposes to update the DG method such that agents also maintain, exchange, and combine estimates of gradients of the global objective function of (1). This update is based on a technique called “gradient tracking” (see e.g. Di Lorenzo and Scutari (2015, 2016)) which enables better control on the global gradient direction and yields a linear rate of convergence to the optimal solution (see Jakoveti´c (2019) for a uniﬁed analysis of these two methods). In a follow up paper, Qu and Li (2020) also considered an acceleration of their algorithm and 1. For two real-valued functions f and g, we say f = Θ(g) if there exist positive constants Cℓ and Cu such that Cℓg(x) ≤ f(x) ≤ Cug(x) for every x in the domain of f and g with ∥x∥ being suﬃciently large. achieved a linear convergence rate O(exp(−k/Θ(κ5/7))) to the optimal solution. To our best knowledge, whether an accelerated primal variant of the DG algorithm can achieve the non-distributed O(exp(−k/Θ(√κ))) linear rate to a neighborhood of the optimum solution with √κ dependence has been an open problem. Alternative distributed ﬁrst-order methods besides DG have also been studied. In particular, if additional assumptions are made such as the explicit characterization of Fenchel dual of the local objective functions, referred to as the dualable setting as in Scaman et al. (2018); Uribe et al. (2021)), then it is known that the multi-step dual accelerated (MSDA) method of Scaman et al. (2018) achieves the O(exp(−k/Θ(√κ))) linear rate to the optimum with √κ dependence. For deterministic distributed optimization problems under smooth and strongly convex objectives, Dvinskikh and Gasnikov (2019) proposed the PSTM algorithm and provided accelerated convergence guarantees. Recently, Scaman et al. (2019) provided lower bounds which matches the upper bounds of Dvinskikh and Gasnikov (2019) up to logarithmic factors (see also Scaman et al. (2019) for a discussion of deterministic optimal algorithms under diﬀerent assumptions (Lipschitz continuity, strong convexity, smoothness, and a combination of strong convexity and smoothness)). This paper focuses on the Distributed Stochastic Gradient (D-SG) method (which is a stochastic version of the DG method) and its momentum enhanced variant, Distributed Accelerated Stochastic Gradient (D-ASG) method. These methods are relevant for solving distributed learning problems and are natural decentralized versions of the stochastic gradient and its variant based on Nesterov’s momentum averaging (Nesterov, 2004; Can et al., 2019). In this paper, we focus on strongly convex and smooth objectives. Several works studied D-SG under these assumptions although D-ASG remains relatively understudied except the deterministic case (see e.g. Jakoveti´c et al. (2014); Xi et al. (2017); Li et al. (2020); Qu and Li (2016)). The performance of distributed algorithms such as D-SG and their deterministic versions depend on the connectivity of the underlying network structure as expected. In particular, when D-SG and D-ASG are run on undirected graphs, the propagation of information among neighbors is governed by a symmetric mixing matrix W which depend on the network structure and its eigenvalues aﬀect the convergence rates. In particular; the largest eigenvalue of the matrix W is one, and the second largest (in modulus) of the eigenvalues of W, which we refer to as γ in this paper (formally deﬁned in (8)), arises in the study of distributed algorithms such as D-SG. We summarize the existing convergence rate results for D-SG in Table 1.2 Among these, Rabbat (2015) studied composite stochastic optimization problems and showed a O(σ2/k) convergence rate for D-SG and its mirror descent variant. Koloskova et al. (2019) studied decentralized stochastic gradient algorithms when the nodes compress (e.g. quantize or sparsify) their updates. Pu et al. (2019) provided an asymptotic network independent sublinear rate. In our approach, we use a dynamical system representation of these iterative algorithms (presented in Lessard et al. (2016) and further used in Hu and Lessard (2017); Aybat et al. (2020, 2019)) to provide rate estimates for convergence of the local agent iterates to a neighborhood of the optimal solution of problem (1). Our bounds are presented in terms of three components: (i) a bias term that shows the decay rate of the initialization error (i.e., distance of the initial estimates to the optimal solution) independent of gradient noise, (ii) a variance term 2. See also Shamir and Srebro (2014) for a diﬀerent noise model than ours in the mini-batch setting, where each objective fi can be expressed as a ﬁnite sum. that depends on the error level σ2 of local objective functions’ gradients, measuring the “robustness” of the algorithm to noise (in a sense that we will deﬁne precisely later), (iii) a network eﬀect that highlights the dependence on the structure of the network. In this paper, in addition to the convergence analysis for D-SG and D-ASG, our purpose is to study the trade-oﬀs and interplays between these three terms that aﬀect the performance. Shi et al. (2015) E ���x(k) i − x∗ ��� 2 ≤ O � ρk� where ρ = 1 − O(1/κ2) Table 1: Summary for D-SG and D-ASG. ¯x(k) denotes the average of nodes’ estimates at time k, i.e., ¯x(k) := 1 N �N i=1 x(k) i , and, x(k) avg is a weighted average deﬁned in Koloskova et al. (2019). Also, γ ∈ (0, 1) is the second largest modulus of the eigenvalues of the mixing matrix W (formally deﬁned in (8)). In the table, µ denotes the strong convexity constant, L is the gradient Lipschitz constant and κ = L/µ is the condition number, whereas ˜κ := κ+1 (formally introduced in (38)), where λW N is the smallest positive eigenvalue of W, D2 is deﬁned in (27) such that D2 = O � L2E∥x(0) − x∗∥2 + ∥∇F(x∗)∥2� as α → 0, and C0 is an explicitly computable constant such that C0 = O(1) as α → 0. †: The authors analyze a D-SG method with a slightly diﬀerent update then ours. Contributions. We have three sets of contributions. First, we study the convergence rate of DSG with constant stepsize which is used in many practical applications (Alghunaim and Sayed, 2020, 2018; Dieuleveut et al., 2020). Our bounds provide tighter guarantees on the bias term as well as novel guarantees on the variance term for this algorithm. For quadratic functions, we provide sharper estimates for the bias, variance, and network eﬀect terms that are tight, as there exist simple quadratic functions that achieve these bounds. Second, we consider D-ASG with constant stepsize. We show that the bias term decays linearly with rate O(−k/√κ) to a neighborhood of the optimal solution, and thus, it achieves an accelerated rate. We also provide an explicit characterization for this neighborhood, in terms of noise and network structure parameters, with the variance term dominating for small enough stepsize. When the objectives fi are all quadratic, we obtain non-asymptotic guarantees that are explicit in terms of their linear convergence rate and dependence to noise, generalizing available known guarantees for ASG to the distributed setting (Can et al., 2019). For both algorithms, following earlier work on non-distributed versions of these algorithms (Aybat et al., 2020), we use our explicit characterization of bias, variance, and network eﬀect terms to provide a computational framework that can choose algorithm parameters to trade-oﬀ these diﬀerence eﬀects in a systematic manner. In the centralized setting, it has been observed and argued that accelerated algorithms are often more sensitive to noise than non-accelerated algorithms (see e.g. Flammarion and Bach (2015); d’Aspremont (2008); Aybat et al. (2019); Hardt (2014)), however to our knowledge this behavior has not been systematically studied in the context of decentralized algorithms. We study the asymptotic variance of the D-SG and D-ASG iterates as a measure of robustness to random gradient noise and provide explicit expressions for this quantity for quadratic objectives as well as upper bounds for strongly convex objectives. This allows us to compare D-SG and D-ASG in terms of their robustness to random noise properties. Our results (see the discussion after Theorem 7) show that indeed D-ASG can be less robust compared to D-SG depending on the choice of the momentum and stepsize parameters, shedding further light into the tuning of hyperparameters (stepsize and momentum) in the distributed setting. Finally, we study a multistage version of D-ASG, building on the non-distributed method in Aybat et al. (2019), whereby a distributed accelerated stochastic gradient method with constant stepsize and momentum parameter is used at every stage, with parameters carefully varied over stages to ensure exact convergence to the optimal solution x∗. Similar to Aybat et al. (2019), a momentum restart is used to enable stitching the improvement obtained over consecutive stages. We show that our proposed method achieves an accelerated O(−k/√κ) linear decay in the bias term as well as a O(σ2/k) term in the variance term and O((1 − γ)−2/k4) in terms of network eﬀect, where 1 − γ is the spectral gap of the network, see (8) for a formal deﬁnition. We also show that the node averages also achieves O( 1 Nk) for the variance term with a tight dependency to the number of nodes N. This dependency to k and √κ is optimal in the context of centralized black-box stochastic optimization. This suggests that our analysis is tight in terms of its k and √κ dependency, although the problems we consider is not black-box optimization but ﬁnite-sum problems. Such a dependency on k and √κ was obtained previously for the PBSTM algorithm of Dvinskikh and Gasnikov (2019) which is optimal up to logarithmic terms. To the best of our knowledge, our analysis provides the best bounds for the D-ASG algorithm. Our results show that D-ASG without noise converges to a ﬁxed point with the accelerated rate, i.e. the rate has a √κ dependency to the condition number. A summary of all our convergence results is provided in Table 1. We also provide numerical experiments that show the eﬃciency of the D-ASG method in a number of decentralized optimization settings. Other Related work. There has been a growing recent interest in the dynamical system representation of distributed optimization algorithms to facilitate their analysis and design. In particular, Sundararajan et al. (2020) provides a framework to design a broad class of distributed algorithms for deterministic decentralized optimization for time-varying graphs. This framework provides worst-case certiﬁcates of linear convergence via semideﬁnite programming. Other related papers (Sundararajan et al., 2017, 2019) allow analysis and design of deterministic distributed optimization algorithms. However, these results and approaches are targeted for deterministic distributed algorithms and they do not directly apply to the stochastic algorithms we consider in this paper. Robustness of stochastic optimization algorithms to stepsize have also been considered in the literature. In particular, the accelerated gradient methods of Lan (2012, Theorem 2, Corollary 1) do enjoy various robustness properties to noise; in particular, for appropriate stepsize choices, if L is a Lipschitz constant of the gradient, σ2 the noise, and D the diameter of the underlying domain, one may achieve rates roughly where γ > 0 is a particular stepsize multiplier choice. Thus, misspecifying γ does not force a massive degradation in convergence rates, which reﬂects the robustness considerations of Nemirovski et al. (2009). The work of Duchi et al. (2012b, Theorem 2.1.) also shows a similar robustness result to stepsize speciﬁcation. Notation. Let Sµ,L(Rd) denote the set of functions from Rd to R that are µ-strongly convex and L-smooth, that is, for every x, y ∈ Rd,
Bayesian estimation of one-parameter qubit gates<|sep|>Let us consider a system prepared in a known quantum state which enters an apparatus performing an operation on the state. The evolution imposed by the apparatus depends on the value of some parameters and the experimenter is interested in the estimation of those parameters. A natural strategy to obtain the parameter is to detect the state at the output and infer the value of the parameters from the global sample coming from a number of repeated measurements. The optimization of this strategy, i.e. the choice of the best probe, measurement and data processing, is the subject of quantum parameter estimation, which is a relevant subject in the quantum characterization of states and operations [1, 2]. The operation on the state may be unitary or not [3, 4] and may depend on one or more unknown parameters, which, in turn, may correspond to quantities that are not directly observable. The parameters of interest may be the amplitude of the carrier signal, the position and orientation of an object, or the strength of an external ﬁelds. Communications, image analysis and precision metrology provide relevant examples. In the simplest scenario, a parameter estimation problem consists in the determination of the value of the interaction parameter θ for unitaries of the form Uθ = {−iθG} where G is a Hamiltonian operator that generates the transformation. Generally speaking, quantum estimation is concerned with the problem of ﬁnding optimal ways to estimate quantum states and processes. In turn, it has recently attracted much interest in quantum information [5, 6, 7, 8, 9] as a tool for characterization of signal and gates at the quantum level [10, 11, 12, 13, 14, 15, 16, 17, 18]. The canonical way to address estimation of states and operation is by quantum tomography, (see [19] for a review) i.e by measuring a complete set (a quorom [14]) of observables, which allows or the complete characterization. For single- and two-qubit systems this involves the measurement of Pauli matrices and has been realized for polarization qubits [20, 21]. Process Tomography, i.e the reconstruction of quantum operations [22, 23, 24], is itself critical for verifying the actions of quantum logic gates [25] and characterizing decoherence processes [26]. On the other hand, there are many situations where the full tomography of signals and devices is not needed, either because the focus is on speciﬁc features of the transformation, or the dynamics is partially known. In this cases the relevant point is to ﬁnd an optimal and stable way to achieve quantum characterization by parameter estimation. For this reason, in this paper we address estimation of one-parameter unitary gates for qubit systems, i.e. transformation of the form Uθ = {−iθG} where G is a combination of Pauli operators and θ is the parameter of interest. We consider the gate probed either by one-qubit and two-qubit states and compare the performances of standard measurements with the ultimate quantum limit to precision (accuracy) of estimation. As we will see, ultimate bounds are determined by the initial quantum state of the probe, the type of interaction and the readout measurements that is used to extract information from the probe. In particular, we are going to assess the performances of Bayes estimators, which themselves play a central role in many signal processing problems [27]. The precision (variance) of any unbiased estimator of a parameter θ is limited by the Cram´er-Rao bound (CR), given by the inverse of the Fisher information [7, 28, 29, 30, 31, 32, 33]. Bayes estimators are known to be asymptotically unbiased and, in turn, to saturate CR asymptotically. For measurements that are related to the unknown θ through a linear Gaussian model, the maximum likelihood estimate of θ also achieves the CR. Furthermore, when θ is estimated from independent identically distributed (iid) measurements, under suitable regularity assumptions on the probability density, the maximum likelihood estimator is asymptotically unbiased and achieves the CR [29, 34]. On the other hand, being interested in realistic measurement schemes, here we consider estimation procedure based on a limited number of measurements. As a consequence, we have to take into account the biased nature of Bayes estimators. The variance of any estimator with a given bias is bounded by the biased CR [35, 36], which is an extension of the CR taking into account the a priori distribution of the parameter of interest. In turn, it is a fundamental rule of estimation theory that the use of prior knowledge leads to a more accurate estimator. In this paper we address the estimation of the interaction parameter of unitary qubit transformations. We derive ultimate quantum limits to precision and assess performances of Bayesian estimators [37, 38]. In particular, we focus our attention on measurement schemes as those in Fig. 1 and Fig. 5, where a single-qubit gate is probed by single- and two-qubit probes, respectively. We evaluate the a posteriori distribution for the gate parameter, derive the ultimate bound on precision, and compare the asymptotic performances of Bayes estimator to that of Monte Carlo simulated experiments, thus showing that asymptotic optimality is achieved after a limited number of runs. The paper is structured as follows. In Section 2 we introduce notation and derive the a posteriori distribution, also discussing the Bayesian version of the CRB. In Section 3 we discuss limits to precision in estimating unitary gates for qubit systems. A comparison between single- and two-qubit entangled probes shows that entanglement improves the overall stability of the estimation procedure. We also compare the asymptotic a posteriori distribution for the gate parameter to the results of the Monte Carlo simulated experiments. Section 4 closes the paper with some concluding remarks.
Optical properties of coupled metal-semiconductor and metal-molecule nanocrystal complexes: the role of multipole effects<|sep|>Nanomaterials incorporating semiconductor quantum dots (SQDs), metal nanoparticles (MNPs), metal surfaces, and dye molecules have been studied intensively [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]. Such hybrid structures take advantage of physical properties of different material systems and demonstrate useful sensor and light-harvesting properties. In a complex made of emitter (SQD or dye) and MNPs, excitons and plasmons interact via the Coulomb forces. Emission of SQD (dye) in the presence of MNPs can be suppressed or enhanced depending on the size and organization of nano-assembly [3, 5, 11]. This occurs due to the following physical factors: (1) modiﬁed density of states of photons, (2) ampliﬁed absorption in the presence of plasmon resonance in the MNPs, and (3) shortening of exciton lifetime due to an increased radiation rate and energy transfer to the MNPs. Theoretically, an emitting dipole in the vicinity of metal objects has been studied in many publications [3, 5, 11, 12, 13, 14]. The exciton-plasmon interaction between an emitting dipole and metal objects leads to energy transfer, a shift of energy of quantum emitter, and vacuum Rabi oscillations [11, 12, 13, 14]. The energy transfer mechanism is similar to F¨orster transfer between two dye molecules [15]. In most theoretical publications, the energy transfer in nanoscale systems was treated as uni-directional ﬂow of energy from a donor to accepter using the rate equations. This rateequation approach describes non-coherent interactions at elevated temperatures. Very recently, several theoretical studies were performed for the quantum regime of exciton-plasmon interaction between a SQD/dye and MNP [16, 17, 18, 19]. These studies revealed a novel feature of strongly-interacting hybrid nanocrystals - interference eﬀects. The absorption spectra of coupled SQD and MNP acquire characteristic asymmetry due to the in terference of external and induced electric ﬁelds. At low temperatures and small exciton broadening, the absorption line shapes are described by the Fano formula [20]. However, previous theoretical results [16, 17, 18, 19] were based on the dipole approximation. The dipole approximation is valid when an exciton-MNP distance is large, compared to the sizes of components, and the excitonplasmon interaction is relatively weak. But, the most interesting regime of strong exciton-plasmon interaction appears when a exciton-MNP distance is small and the dipole approximation is not valid anymore. Therefore, to describe the regime of strong exciton-plasmon coupling, one should treat the Coulomb interaction exactly, including electric multipole eﬀects. In this paper we obtain an exact solution for the problem of interacting dipole and MNP. Here we present a theory of strong exciton-plasmon interaction and show that the multipole eﬀects are of crucial importance for the understanding of the excitonplasmon interaction in the most interesting regime of small exciton-MNP separations. A strong interaction between excitons and plasmons at small exciton-MNP distances reveals itself in absorption spectra as asymmetric lineshapes and anti-resonances (deep minima). Moreover, we consider two types of nanoscale hybrid complexes (SQD-MNP and dye-MNP) and show that, for small exciton-MNP distances, the interference eﬀects can appear in room-temperature experiments. For the multipole regime of Coulomb interaction, the visibility of exciton resonance grows dramatically due to both the interference eﬀect and the plasmon-induced electromagnetic enhancement. For large inter-particle distances, the absorptions by SQD and MNP add up constructively. For the hybrid structures of small dimensions, the spectra exhibit very strong eﬀects of constructive and destructive interference. We think that these interference eﬀects can be observed experimentally using presently available material systems. In this paper, we discuss three material systems: colloidal nanocrystals, self-assembled dots, and dye molecules. Most of the current experiments on metalsemiconductor and metal-dye assemblies employ colloidal nanoparticles [5, 6, 7, 8, 9] and nano-wires [6]. To observe the eﬀects described in this paper, single colloidal nanocrystal complexes can be deposited on a surface or buried in a polymer ﬁlm. Assemble measurements of colloidal complexes in a solution can also be suitable. Presently, there are attempts to fabricated metal nanoparticles inside epitaxial structures [21]. Potentially, colloidal and epitaxial nanocrystals can be combined in one structure [22]. For example, an epitaxial dot can be buried close to the surface [23] and a MNP can be attached to the surface. The paper is organized as follows. In section II, we give a description of the electric ﬁeld inside the system. The optical properties of a nanocrystal molecule are presented in section III. Section IV discusses the absorption line shape. The numerical results for SQD-MNP molecules and dye molecule-MNP nanocrystals are given in section V. Finally, a brief conclusion is presented.
Characterization of the Atmospheric Muon Flux in IceCube<|sep|>IceCube is a particle detector with an instrumented volume of about one cubic kilometer, located at the geographic South Pole [1]. The experimental setup consists of 86 cables (“strings”), each supporting 60 digital optical modules (“DOMs”). Every DOM contains a photomultiplier tube and the electronics required to handle data acquisition, digitization and transmission. The main active part of the detector is deployed at a depth of 1450 to 2450 meters below the surface of the ice, which in turn lies at an altitude of approximately 2830 meters above sea level. The volume detector is supplemented by the surface array IceTop, formed by 81 pairs of tanks ﬁlled with - due to ambient conditions solidiﬁed - water. The main scientiﬁc target of IceCube is the search for astrophysical neutrinos. At the time of design, the most likely path to discovery was expected to be the detection of upward-going tracks caused by Earthpenetrating muon neutrinos interacting shortly before the detector volume. All DOMs were consequently oriented in the downward direction, such that Cherenkov light emission from charged particles along muon tracks can be registered after minimal scattering in the surrounding ice. The ﬁrst indication for a neutrino signal exceeding the expected background from cosmic ray-induced atmospheric ﬂuxes came in the form of two particle showers with a total visible energy of approximately 1 PeV [2]. Detailed analysis of their directionality strongly indicated an origin from above the horizon. The result strengthened the case for the astrophysical nature of the events, since no accompanying muons were seen, as would be expected for neutrinos produced in air showers. This serendipitous detection motivated a dedicated search for high-energy neutrinos interacting within the detector volume, which led ﬁrst to a strong indication [3] and later, after evaluating data taken during three full years of detector operation, to the ﬁrst discovery of an astrophysical neutrino ﬂux [4]. In each case, the decisive contribution to the event sample were particle showers pointing downward. Despite the large amount of overhead material, the deep IceCube detector is triggered at a rate of approximately 3000 s−1 by muons produced in cosmic rayinduced air showers. Formerly regarded simply as an irksome form of background, these have since proved to be an indispensable tool to tag and exclude atmospheric neutrino events in the astrophysical discovery region [5]. Apart from their application in neutrino searches, muons can be used for detector veriﬁcation and a wide range of physics analyses. Examples are the measurement of cosmic ray composition and ﬂux in coincidence with IceTop [6], the ﬁrst detection of an anisotropy in the cosmic ray arrival direction in the southern hemisphere [7, 8, 9], investigation of QCD processes producing high-pt muons [10] and the evaluation of track reconstruction accuracy by taking advantage of the shadowing of cosmic rays by the moon [11]. Remaining to be demonstrated is the possibility to develop a comprehensive and consistent picture of atmospheric muon physics in IceCube. The goal of this paper is to outline how this could be accomplished, illustrate the scientiﬁc potential and discuss consequences of the actual measurement for the understanding of detector systematics.
Training Region-based Object Detectors with Online Hard Example Mining<|sep|>Image classiﬁcation and object detection are two fundamental computer vision tasks. Object detectors are often trained through a reduction that converts object detection into an image classiﬁcation problem. This reduction introduces a new challenge that is not found in natural image classiﬁcation tasks: the training set is distinguished by a large imbalance between the number of annotated objects and the number of background examples (image regions not belonging to any object class of interest). In the case of sliding-window object detectors, such as the deformable parts model (DPM) [12], this imbalance may be as extreme as 100,000 background examples to every one object. The recent trend towards object-proposal-based detectors [15, 32] mitigates this issue to an extent, but the imbalance ratio may still be high (e.g., 70:1). This challenge opens space for learning techniques that cope with imbal Unsurprisingly, this is not a new challenge and a standard solution, originally called bootstrapping (and now often called hard negative mining), has existed for at least 20 years. Bootstrapping was introduced in the work of Sung and Poggio [30] in the mid-1990’s (if not earlier) for training face detection models. Their key idea was to gradually grow, or bootstrap, the set of background examples by selecting those examples for which the detector triggers a false alarm. This strategy leads to an iterative training algorithm that alternates between updating the detection model given the current set of examples, and then using the updated model to ﬁnd new false positives to add to the bootstrapped training set. The process typically commences with a training set consisting of all object examples and a small, random set of background examples. Bootstrapping has seen widespread use in the intervening decades of object detection research. Dalal and Triggs [7] used it when training SVMs for pedestrian detection. Felzenszwalb et al. [12] later proved that a form of bootstrapping for SVMs converges to the global optimal solution deﬁned on the entire dataset. Their algorithm is often referred to as hard negative mining and is frequently used when training SVMs for object detection [15, 16, 32]. Bootstrapping was also successfully applied to a variety of other learning models, including shallow neural networks [25] and boosted decision trees [9]. Even modern detection methods based on deep convolutional neural networks (ConvNets) [19, 20], such as R-CNN [15] and SPPnet [16], still employ SVMs trained with hard negative mining. It may seem odd then that the current state-of-the-art object detectors, embodied by Fast R-CNN [14] and its descendants [24], do not use bootstrapping. The underlying reason is a technical difﬁculty brought on by the shift towards purely online learning algorithms, particularly in the context of deep ConvNets trained with stochastic gradient descent (SGD) on millions of examples. Bootstrapping, and its variants in the literature, rely on the aforementioned alternation template: (a) for some period of time a ﬁxed model is used to ﬁnd new examples to add to the active training set; (b) then, for some period of time the model is trained on the ﬁxed active training set. Training deep ConvNet detectors with SGD typically requires hundreds of thousands of SGD steps and freezing the model for even a few iterations at a time would dramatically slow progress. What is needed, instead, is a purely online form of hard example selection. In this paper, we propose a novel bootstrapping technique called online hard example mining1 (OHEM) for training state-of-the-art detection models based on deep ConvNets. The algorithm is a simple modiﬁcation to SGD in which training examples are sampled according to a non-uniform, non-stationary distribution that depends on the current loss of each example under consideration. The method takes advantage of detection-speciﬁc problem structure in which each SGD mini-batch consists of only one or two images, but thousands of candidate examples. The candidate examples are subsampled according to a distribution that favors diverse, high loss instances. Gradient computation (backpropagation) is still efﬁcient because it only uses a small subset of all candidates. We apply OHEM to the standard Fast R-CNN detection method and show three beneﬁts compared to the baseline training algorithm: • Its effectiveness increases as the training set becomes larger and more difﬁcult, as demonstrated by results on the MS COCO dataset. Moreover, the gains from OHEM are complementary to recent improvements in object detection, such as multiscale testing [16] and iterative bounding-box regression [13]. Combined with these tricks, OHEM gives state-ofthe-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012, respectively.
Chern numbers of topological phonon band crossing determined with inelastic neutron scattering<|sep|>Ever since the discovery of topological quantum numbers in quantum Hall states [1, 2], the concept of band topology has shed light on the exploration and classiﬁcation of crystalline materials [3–7]. Topological insulators, semimetals, and superconductors are extensively studied, both as novel phases of matter and for their potential applications. Unlike conventional phases of matter described by symmetry in the Landau paradigm, topological phases are classiﬁed by topological invariants, which do not change over adiabatic deformations of the band structure. An important topological invariant is called the Chern number, which is associated with a mapping from a twodimensional (2D) closed surface in reciprocal space to the Hilbert space of Bloch states. The Chern number characterizes the topological structure of such mapping and has observable consequences. In the gapped energy spectrum of 2D quantum Hall systems, nonzero Chern numbers correspond to the number of edge states which lead to the quantization of the Hall conductance [3, 8]. In three-dimensional (3D) Weyl semimetals, Weyl nodes act as monopoles of Berry ﬂux and have nonzero Chern numbers (deﬁned by the mapping from their enclosing surface in momentum space to the Hilbert space), which dictate the number of Fermi-arc surface states [9–11] and the quantized magnitude of circular photogalvanic eﬀect [12, 13]. As band topology is independent of the statistics of the constituent quasiparticles, similar phenomena are also expected in bosonic systems. For example, topological photonic and acoustic bands and their corresponding surface states have been found in artiﬁcial structures [14–18]. In natural crystals, a variety of Dirac and Weyl nodes have been pre dicted and/or observed in phonon [19–25] and magnon bands [26–30]. Previous experiments on these systems mainly focused on the bulk dispersion relation near the band-crossing points, rather than on topological invariants such as the Chern numbers, partly because it is diﬃcult to measure phonon and magnon surface states and transport behaviors. It is experimentally possible to determine Chern numbers in phonon and magnon bands, if one can measure the topological structure of wave functions, i.e., eigenvectors of motion, in momentum space. Scattering methods such as inelastic neutron scattering (INS) and X-ray scattering are suitable for this purpose, because their dynamical structure factor S(Q, ω) is related to the excitations’ eigenvectors [31, 32]. As the eigenvectors vary strongly in the vicinity of Weyl and Dirac points, the observed intensities are expected to undergo strong and distinct modulations, which can reﬂect the topological structure. Such modulations have been recently reported in several topological magnon systems around Dirac points [33, 34] and nodal lines [35, 36]. Here, we report an INS study of MSi (M = Co, Mn) single crystals, which host multiple types of topological phonon band crossing nodes [21, 22, 37]. By comparing the observed INS intensities with our ﬁtted model based on density functional perturbation theory (DFPT) calculations, we verify the theoretically predicted coexistence of two-fold quadruple Weyl points, three-fold spin-1 Weyl points, and four-fold charge-2 Dirac points in MSi. We further explore the spectroscopic signatures of topological structures near the topological band crossing points and show, theoretically and in some cases with comparison to the experimental data, that the number of intensity extrema on a momentum sphere enclosing the band-crossing node equals the Chern number of the FIG. 1. (a) Cubic primitive cell of MSi (M = Mn, Co). (b) and (c) Representative INS intensities of MnSi and CoSi, respectively, along a high-symmetry momentum trajectory, measured at T = 40 K. Data measured with Ei = 57 meV and 90 meV are combined after proper intensity normalization. (d) The Brillouin zone (BZ), with the momentum trajectory marked in red. (e) and (f) S(Q, ω) calculated from the ﬁtted-force-constant model along the same trajectories as in (b) and (c). Solid lines in (b), (c), (e), and (f) indicate the calculated phonon dispersions. node. Our result demonstrates the capability of INS for direct Chern-number determination. This paper is organized as follows: In Section II, we describe the INS experiment and the ﬁtted-force-constant model based on DFPT. In Sec. III, we show phonon dispersions, both in a global view and close to topological band crossing nodes. In Sec. IV, we discuss INS spectroscopic features near the band-crossing nodes and investigate their relation with the Chern numbers. In Sec. V, we make a brief discussion and a summary.
Statistical Estimation of Confounded Linear MDPs: An Instrumental Variable Approach<|sep|>Oﬄine reinforcement learning (oﬄine RL, Sutton and Barto [2018], Levine et al. [2020]) is a machine learning paradigm which aims to learn a policy for sequential decision-making from a pre-collected oﬄine dataset. With its huge empirical success [Mnih et al., 2015, Lillicrap et al., 2015, Fujimoto et al., 2019, Kidambi et al., 2020], oﬄine RL has also been studied extensively from a theoretical perspective in recent years [Chen and Jiang, 2019, Jin et al., 2020b, Duan et al., 2020, Agarwal et al., 2020, Duan et al., 2021, Zhan et al., 2022]. A critical problem in oﬄine RL is oﬀ-policy evaluation (OPE), which aims to estimate the long-term expected cumulative reward received by a target policy using the oﬄine dataset collected by a diﬀerent behaviour policy [Duan et al., 2020, Bennett et al., 2021, Min et al., 2021]. Typically, existing works on OPE develop algorithms and theories based on the model of Markov decision process (MDP), where the target policy is evaluated using data collected by a behavior policy. The limitation is that this model cannot characterize the situation when unobserved confounders exist in oﬄine data generation, as is often the case in many real-world applications. For example, in data collection in the domains of healthcare, a physician may take treatments based on a patient’s mental state or socioeconomic status, which is hard or prohibited to be recorded in the data due to privacy concerns. Meanwhile, such information can aﬀect the clinical outcomes, which makes the oﬄine data confounded [Zhang and Bareinboim, 2016, Tennenholtz et al., 2020]. To better adapt to these applications, Zhang and Bareinboim [2016], Wang et al. [2021], Liao et al. [2021], Bennett et al. [2021] propose and study confounded MDPs. In a confounded MDP, there exist unobserved confounders which can inﬂuence both the action and the reward, causing confounding issues [Pearl, 2009, Zhang and Bareinboim, 2016] in the collected data. As a result, conventional MDP-based ∗School of Mathematical Sciences, Peking University; email: lumiao@stu.pku.edu.cn. †Academy for Advanced Interdisciplinary Studies, Peking University; email: yangwenhaosms@pku.edu.cn. ‡Academy for Advanced Interdisciplinary Studies, Peking University; email: zhangliangyu@pku.edu.cn. §School of Mathematical Sciences, Peking University; email: zhzhang@math.pku.edu.cn. OPE estimators, which do not handle the confounding issues in the data, may fail to identify the true value of the target policy in this case, causing an estimation bias. In this work, we present the ﬁrst statistical results of OPE in a confounded MDP based on the tool of instrumental variable (IV) [Angrist and Imbens, 1995a, Brookhart and Schneeweiss, 2007, Baiocchi et al., 2014, Michael et al., 2020]. IV is a widely-used tool in statistics, econometrics, and causal inference, which can help us to identify the desired causal eﬀects in the face of unobserved confounders. For example, in healthcare domains, existing works have explored various kinds of IVs across preference-based IV [Brookhart and Schneeweiss, 2007, Komorowski et al., 2018] and diﬀerential-travel-time-based IV [Lorch et al., 2012, Michael et al., 2020, Chen and Zhang, 2021]. Generally, we can identify the true value of a target policy in a confounded MDP using only observable variables when a set of IVs is available. Recently, Li et al. [2021a], Liao et al. [2021] have paid attention to applying IVs in addressing confounding issues in RL problems However, few works study the statistical properties of doing OPE in confounded MDPs. In particular, it remains open that (a) how can we design a both statistically and computationally eﬃcient OPE estimator based on observable IVs for confounded MDPs? (b) how many samples are suﬃcient to guarantee accurate estimation with such an estimator? (c) can we perform statistical inference using this estimator? All these critical questions necessitate further theoretical understandings of OPE in confounded MDPs via IVs from a statistical perspective.
Inferring Networks of Substitutable and Complementary Products<|sep|>Recommender systems are ubiquitous in applications ranging from e-commerce to social media, video, and online news platforms. Such systems help users to navigate a huge selection of items with unprecedented opportunities to meet a variety of special needs and user tastes. Making sense of a large number of products and driving users to new and previously unknown items is key to enhancing user experience and satisfaction [2, 14, 15]. While most recommender systems focus on analyzing patterns of interest in products to provide personalized recommendations [14, 30, 34, 36], another important problem is to understand relationships between products, in order to surface recommendations that are relevant to a given context [17, 35]. For example, when a user in an online store is examining tshirts she should receive recommendations for similar t-shirts, or otherwise jeans, sweatshirts, and socks, rather than (say) a movie even though she may very well be interested in it. From these relationships we can construct a product graph, where nodes represent products, and edges represent various types of product relationships. Such product graphs facilitate many important applications: Navigation between related products, discovery of new and previously unknown products, identiﬁcation of interesting product combinations, and generation of better and more context-relevant recommendations. Despite the importance of understanding relationships between products there are several interesting questions that make the problem of building product graphs challenging: What are the common types of relationships we might want to discover? What data will allow us to reliably discover relationships between products? How do we model the semantics of why certain products are related?—For example, the semantics of why a given t-shirt might be related to a particular pair of jeans are intricate and can only be captured by a highly ﬂexible model. And ﬁnally, how do we scale-up our methods to handle graphs of millions of products and hundreds of millions of relations? Inferring networks of product relationships. Here we are interested in inferring networks of relationships between millions of products. Even though our method can be used to learn any type of relationship, we focus on identifying two types of links between products: substitutes and complements [21]. Substitutable products are those that are interchangeable—such as one t-shirt for another, while complementary products are those that might be purchased together, such as a t-shirt and jeans. We design a system titled Sceptre (Substitute and Complementary Edges between Products from Topics in Reviews), that is capable of modeling and predicting relationships between products from the text of their reviews and descriptions. At its core, Sceptre combines topic modeling and supervised link prediction, by identifying topics in text that are useful as features for predicting links between products. Our model also handles additional features such as brand, price, and rating information, product category information, and allows us to predict multiple types of relations (e.g. substitutes and complements) simultaneously. Moreover, Sceptre harnesses the fact that products are arranged in a category hierarchy and allows us to extend Figure 1: Sceptre learns the concept of substitute and complement goods from product information (descriptions, reviews, etc.). Given a query item, Sceptre allows us to generate substitute and complementary recommendations as shown above. this hierarchy to discover ‘micro-categories’—ﬁne-grained categories of closely related products. An example of the output of Sceptre is shown in Figure 1. Here, given a query item (a hiking boot), our system identiﬁes a ranked list of potential substitutes (other hiking boots), and complements (heavy-duty socks, shoe polish, etc.). We train Sceptre on a large corpus of 9 million products from Amazon, with 237 million connections derived from browsing and co-purchasing data. We evaluate Sceptre in terms of its accuracy at link prediction and ranking, where we ﬁnd it to be signiﬁcantly more accurate than alternatives. We also use Sceptre to build a product graph, where for every product we recommend a list of the most related complementary and substitutable products. Finally, we show that Sceptre can be applied in ‘coldstart’ settings, by using other sources of text when reviews are unavailable. Overall, we ﬁnd that the most useful source of information to identify substitutes and complements is the text associated with each product (i.e., reviews, descriptions, and speciﬁcations), from which we are able to uncover the key features and relationships between products, and also to explain these relationships through textual signals. We envision several applications of the product graphs produced by our system. Our system can help users to navigate, explore and discover new and previously unknown products. Or, it can be used to identify interesting product combinations, e.g. we can recommend outﬁts by matching a shirt with complementary trousers and a jacket. And, our system can be used as a
Anelastic Versus Fully Compressible Turbulent Rayleigh-B\'enard Convection<|sep|>Thermal convection is of primary importance in astrophysical objects. It carries the heat ﬂow over large regions in stellar and planetary interiors, and is one of the major sources of mechanical mixing in these objects. Furthermore, some of the most striking, large-scale features of stars and planets are powered by convective motions, such as intrinsic dynamo-generated magnetic ﬁelds, plate tectonics on Earth and possibly also the zonal winds on Jupiter and other giant planets (e.g. Brun et al. 2004; Brandenburg and Subramanian 2005; Trompert and Hansen 1998; Tackley 2000; Bercovici 2003; Heimpel et al. 2005; Verhoeven and Stellmach 2014). The convective regions in stellar and planetary objects typically feature a non-negligible density stratiﬁcation, and the ﬂows are often subsonic. In this paper, we compare two approaches that are commonly used for modeling convection in these systems numerically—the fully compressible approach and the so-called anelastic approximation. Our goal is to quantify the accuracy and eﬃciency of both methods in a given situation, guiding modelers in making the right choice for their particular problem at hand. The fully compressible equations are the most fundamental equations governing thermal convection. They can directly be derived from ﬁrst principles of physics, such as mass, energy, and momentum conservation, equipped with constitutive relations that characterize the ﬂuid. The resulting equations are thus very general and encompass the full range of physical behavior, from the temporal evolution of the convective motions to the propagation of sound waves. On the one hand, this allows to study regions such as the outermost layers of the Sun, where the Mach number, i.e. the ratio of convective velocity to the sound speed, becomes O(1). On the other hand, problems arise in low Mach number regions where the ﬂow velocities are much slower than the sound speed, which is typically the case in the bulk of deep stellar and planetary interiors. Even though the convective motions in such regions occur on time scales which are many orders of magnitude larger than the acoustic time scale, standard numerical schemes have to explicitly resolve the sound waves for stability reasons. This forces modelers to assume artiﬁcially large Mach numbers, which reduces the diﬀerences between the convective and acoustic time scales to numerically tractable values (e.g. Tobias et al. 1998; Brummell et al. 2002; K¨apyl¨a et al. 2010). Errors introduced by this procedure occur as an unavoidable side-eﬀect in the fully compressible framework. Still, most of the numerical resources typically go into capturing acoustic wave propagation phenomena, which are generally believed to be irrelevant for the investigated convection dynamics (but see Bogdan et al. 1993; Meakin and Arnett 2006). To circumvent the problems arising from the numerical stiﬀness of the fully compressible equations, diﬀerent ”sound-proof” models, such as the low Mach number approach (e.g. Majda and Sethian 1985; Bell et al. 2004; Almgren et al. 2006), the pseudo-incompressible approximation (Durran 1989) or the anelastic approximation (Batchelor 1953; Ogura and Phillips 1962; Gough 1969; Gilman and Glatzmaier 1981; Lantz and Fan 1999) have been developed. Instead of prescribing artiﬁcially large Mach numbers, all these approaches take the opposite route by considering the small Mach number limit of the fully compressible equations. The same time scale disparities which make solving the fully compressible equations numerically challenging are thus exploited to substantially simplify the equations. As a result, the pressure ﬁeld adapts instantaneously, which eﬀectively ﬁlters out the sound waves. This comes, however, at the price of loosing the ability to study regions where the Mach number is not small. Among the sound-proof approaches described above, the anelastic approximation is the one most commonly deployed for modeling stellar and planetary interiors (e.g. Glatzmaier and Roberts 1996; Miesch et al. 2008; Brun et al. 2011; Jones et al. 2011). The anelastic equations are theoretically predicted to hold for low Mach number systems in which only slight thermodynamic perturbations from a hydrostatic background state occur (e.g. Gough 1969). In convective systems, the background state is typically assumed to be adiabatic. The above conditions are believed to be satisﬁed in the deep interiors of giant planets and in the bulk of the solar convection zone, but break down in their outermost parts which feature relatively small sound speeds (Ulrich 1970; Bahcall and Ulrich 1988; Christensen-Dalsgaard et al. 1996; Guillot et al. 2004). The dynamics of these outer layers thus cannot be accounted for within the anelastic framework, and modelers are forced to exclude them from the simulation domain. The dynamical consequences of neglecting these regions remain unclear. In summary, both approaches have advantages and drawbacks. While the fully compressible approach is the method of choice for modeling O(1) Mach number ﬂows in near-surface regions of stellar objects, the anelastic approximation seems to be beneﬁcial in the deep interiors where the Mach numbers are usually very small and where the thermodynamic state is close to the adiabat. Unfortunately, in many astrophysical applications, it remains unclear which approach performs best, with anelastic and fully compressible models being used side by side. The main goal of this study is thus twofold: First, we aim to quantify and compare the errors inherent in modeling turbulent convection in both approaches. Secondly, we seek to compare their computational eﬃciency, thereby guiding modelers in minimizing the tradeoﬀ between accuracy and eﬃciency for any given situation. Perhaps somewhat surprisingly, comparing results from the anelastic models currently used in astro- and geophysics to standard fully compressible simulations is non-trivial. This is because the anelastic models usually parameterize the turbulent, subgrid-scale entropy ﬂux, while similar turbulence models are typically not used in fully compressible models. The popularity of turbulence modeling in the anelastic framework stems from the fact that it allows further simpliﬁcations of the governing equations, which eases the numerical implementation considerably. Typically, molecular heat conduction is neglected and replaced by an artiﬁcial eddy diﬀusion model that represents turbulent mixing of entropy (Gilman and Glatzmaier 1981; Glatzmaier 1984; Braginsky and Roberts 1995; Lantz and Fan 1999). This turbulent entropy diﬀusion model, however, is not mandatory for the actual anelastic approximation, and anelastic equations have been formulated that do not rely on parameterizations of the subgridscale transport (Gough 1969). These equations have not found widespread use so far. In order to provide direct comparability between the anelastic and the fully compressible approach, in this study we will restrict ourselves to molecular thermal heat conduction in both cases. While direct comparisons of anelastic and fully compressible gravity wave dynamics in stably stratiﬁed set-ups have been performed in several studies (e.g. Davies et al. 2003; Klein et al. 2010; Brown et al. 2012), the unstable thermal convection case considered in this paper has received less attention so far. The work of Berkoﬀ et al. (2010) focussed on linear magnetoconvection and found good agreement between both approaches for the weakly superadiabatic case. Subsequently, Lecoanet et al. (2014) studied diﬀerences between temperature and entropy diﬀusion, while Calkins et al. (2014, 2015) focussed on the inﬂuence of rotation on the onset of anelastic and fully compressible convection. Their linear study identiﬁed shortcomings of the anelastic equations for rapidly rotating, low Prandtl number ﬂuids, where fast density oscillations were found to become nonnegligible. Calkins et al. (2014) conclude that fully non-linear studies tracing the validity range of the anelastic approximation are crucial in both rotating and non-rotating systems, especially in the turbulent regime characterized by a broadband frequency spectrum. A ﬁrst step in this direction has been taken by Meakin and Arnett (2007), who compared non-linear anelastic and fully compressible simulations of stellar oxygen burning. The diﬀering physical processes included in each model, however, precluded a one-to-one Fig. 1.— Compressible convection is modeled in Rayleigh-B´enard geometry, i.e. in a Cartesian box that is cooled from above and heated from below. Gravity g is pointing downward, antiparallel to the z-axis. comparability of the anelastic and fully compressible inﬂuences. In this paper, we present the ﬁrst systematic one-to-one comparisons between fully compressible and anelastic numerical simulations of convection in the fully nonlinear, turbulent regime. As a starting point, we neglect important ingredients of stellar convection, such as spherical geometry, rotation, compositional inhomogeneities, nuclear reactions, magnetic ﬁelds, penetration and overshooting in stably stratiﬁed layers, and the corresponding wave-emission. This allows us to quantify the respective errors, as well as the computational eﬃciency encountered in both approaches in the simplest setup possible. The inﬂuences of the above physical processes will be investigated in future studies. The paper is organized as follows: In section 2, we start with deﬁning our idealized model, which is followed by discussing the fully compressible equations along with the anelastic approximation in section 3. A brief overview of the applied numerical methods is given in section 4, while a direct comparison of anelastic and fully compressible results and the computational eﬃciencies of both approaches are discussed in section 5. Finally, general conclusions are drawn in section 6.
Multi-Step Bayesian Optimization for One-Dimensional Feasibility Determination<|sep|>We consider the problem of adaptively allocating sampling eﬀort to eﬃciently estimate sub- and super-level sets of a one-dimensional Markov process, or more general additive functionals of this process. We use a decomposition property to show how the optimal procedure may be computed eﬃciently, circumventing the curse of dimensionality. We then use our ability to compute the optimal policy to study the suboptimality gap of commonly used one-step lookahead procedures in this problem. The problem we consider falls within the class of problems considered in the large and rapidly growing literature on Bayesian optimization [22, 25, 21, 12, 33], which seeks to develop adaptive sampling algorithms that estimate functionals, especially the location of a global maximum, of some underlying and unknown function in a query eﬃcient way. Such problems arise when optimizing an objective that is computed via a long-running computer code [12, 33] or some other expensive process [4, 15] that severely limits the number of times it may be sampled. This literature places a Bayesian prior distribution on the underlying function, and views it as a realization of a stochastic process, most frequently a Gaussian process. In such problems, a Bayes-optimal algorithm is one that minimizes the expected loss under the prior suﬀered from mis-estimation of the underlying functional of interest, where the cost of sampling is either factored directly into the objective (as considered by [10, 3]), or a sampling budget is enforced as a constraint (as considered by [17, 13]). When optimization is the goal, this loss function is the opportunity cost — the diﬀerence in value between the point that is believed to be the best, and the value of the true global optimum — but when other functionals are of interest another loss function may be appropriate. In principle, a Bayes-optimal algorithm may be computed using stochastic dynamic programming by understanding that this problem is a partially observable Markov decision process (POMDP) [17]. However, the curse of dimensionality [29] prevents actually computing the solution through brute-force approaches. Thus, almost all of the literature has focused on approximate schemes, which in many cases are inspired by this view of the problem as a partially observable Markov decision process, but that do not actually solve the POMDP. Two commonly used methods of this type are the expected improvement method [25, 21] and the knowledge-gradient method [14, 32], which use one-step lookahead approaches, based on diﬀerent assumptions about what points are eligible for selection once sampling stops [15]. Two-step lookahead approaches have also been implemented computationally in [2, 17]. In contrast, we focus on calculating the Bayes-optimal algorithm. Our primary contribution is to show that it can be computed eﬃciently in Bayesian optimization problems that satisfy four assumptions: • the Bayesian prior on this function has the Markov property (e.g., a Wiener process prior, as used by [22, 27, 1, 38, 30, 24, 23, 9], or an Ornstein-Uhlenbeck prior [26, 28]). • the loss function is additive across location, as arises when the goal is to determine feasibility of points, as in [16], or to determine the set of points that are better than some known standard, as in [37]. • the limit on sampling is imposed as an additive cost in the objective, as in [10, 3], or as a constraint on the expected number of samples taken, as in [17, 13]. As a second contribution, we also provide an upper bound on the value of the Bayes optimal policy when the limit on sampling is imposed as an almost sure constraint on the number of samples taken. While one dimensional feasibility determination problems do arise in practice [20], and we expect that the optimal policy can provide a great deal of value in those settings, a large fraction of practical Bayesian optimization problems violate one or more of the assumptions above, because many problems are in more than one dimension, and because non-Markov Gaussian processes are often used as priors [18, 31, 4]. Optimization is also a more common goal in the literature than super-level set determination (though in practice it is often just as useful to provide a set of points that perform well, i.e., that reside in some super-level set, from which a ﬁnal decision can be selected based on other criteria). Thus, we view our primary contribution as providing a specialized but nevertheless rich class of Bayesian optimization problems on which the performance of widely applicable heuristic procedures, such as the onestep lookahead procedures described above, may be studied relative to Bayes-optimal procedures. This guides algorithm development — if a heuristic procedure performs close to optimal on a set of problems, then this suggests that further improvement is not necessary even for other similar problems for which the optimality gap cannot be evaluated. In contrast, if all known heuristic procedures perform substantially worse than optimal on a set of problems, then this suggests that further algorithm development is worthwhile. There is some complementary theoretical analysis in the literature of Bayes-optimal procedures for this and related problems. Much of it focuses on asymptotic analyses, and includes proofs of consistency for the Eﬃcient Global Optimization (EGO) [35] and P algorithms [6], as well as convergence rates for these and closely related algorithms [7, 5, 9]. In terms of ﬁnite-time analyses, [34, 19] provide regret bounds for the closely related problem of Bayesian optimization in the bandit setting, but while these bounds characterize performance, slack in the bounds’ constants creates a potentially large multiplicative gap in which performance may lie. In the problems of multiple comparisons with a known standard and stochastic root-ﬁnding, procedures for computing explicit Bayes optimal procedures have been developed [37, 36], but these problems are only distantly related to Bayesian optimization. Thus, exact performance of optimal ﬁnite-time procedures has remained unknown in Bayesian optimization. Below, in Section 2, we provide a formal description of the problem. Our main results are in Section 3, where we signiﬁcantly reduce the state-space for a dynamic program giving rise to a Bayes-optimal policy. In Section 4 we consider the relationship between the cost-per-sample setting and the constrained-budget setting, showing how the optimal value for the former can be used to compute the optimal value of the latter. In Section 5 we present numerical results, illustrating the behavior of the optimal policy and using it to analyze the optimality gap for a one-step lookahead procedure. Finally, in Section 6, we conclude.
Multimodal Logical Inference System for Visual-Textual Entailment<|sep|>Multimodal inference across image data and text has the potential to improve understanding information of different modalities and acquiring new knowledge. Recent studies of multimodal inference provide challenging tasks such as visual question answering (Antol et al., 2015; Hudson and Manning, 2019; Acharya et al., 2019) and visual reasoning (Suhr et al., 2017; Vu et al., 2018; Xie et al., 2018). Grounded representations from image-text pairs are useful to solve such inference tasks. With the development of large-scale corpora such as Visual Genome (Krishna et al., 2017) and methods of automatic graph generation from an image (Xu et al., 2017; Qi et al., 2019), we can obtain structured representations for images and sentences such as scene graph (Johnson et al., 2015), a visuallygrounded graph over object instances in an image. While graph representations provide more interpretable representations for text and image than embedding them into high-dimensional vector spaces (Frome et al., 2013; Norouzi et al., 2014), there remain two challenges: (i) to capture complex logical meanings such as negation and quan  No cat is next to a pumpkin.(1)  There are at least two cats. (2) All pumpkins are orange. (3) Figure 1: An example of visual-textual entailment. An image paired with logically complex statements, namely, negation (1), numeral (2), and quantiﬁcation (3), leads to a true () or false () judgement. tiﬁcation, and (ii) to perform logical inferences on them. For example, consider the task of checking if each statement in Figure 1 is true or false under the situation described in the image. The statements (1) and (2) are false, while (3) is true. To perform this task, it is necessary to handle semantically complex phenomena such as negation, numeral, and quantiﬁcation. To enable such advanced visual-textual inferences, it is desirable to build a framework for representing richer semantic contents of texts and images and handling inference between them. We use logic-based representations as uniﬁed meaning representations for texts and images and present an unsupervised inference system that can prove entailment relations between them. Our visualtextual inference system combines semantic parsing via Combinatory Categorial Grammar (CCG; Steedman (2000)) and ﬁrst-order theorem proving (Blackburn and Bos, 2005). To describe information in images as logical formulas, we propose a method of transforming graph representations into logical formulas, using the idea of predicate circumscription (McCarthy, 1986), which complements information implicit in images using the closed world assumption. Experiments show that our system can perform visual-textual inference with semantically complex sentences. Figure 2: Overview of the proposed system. In this work, we assume the input image is processed into an FOL structure or scene graph a priori. The system consists of three parts: (a) Graph Translator converts an image annotated with a scene graph/FOL structure to formula M; (b) Semantic parser maps a sentence to formula T via CCG parsing; (c) Inference Engine checks whether M entails T by FOL theorem proving.
Toward Few-step Adversarial Training from a Frequency Perspective<|sep|>An adversarial example is the addition of a perturbation to image such that an image classiﬁer, which correctly classiﬁes the original image, will misclassify this perturbed image. These adversarial examples often look similar to the original images and may be a concern for safety-critical systems that use these image classiﬁers. Many machine learning models are susceptible to these adversarial examples [1, 11]. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org. SPAI ’20, October 6, 2020, Taipei, Taiwan © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7611-2/20/10...$15.00 https://doi.org/10.1145/3385003.3410922 Recent research examined adversarial perturbations in the frequency domain to understand the eﬀectiveness of defenses that utilize this frequency information to mitigate adversarial examples [12]. Others found that ﬁltering the high-frequency components of a perturbationcan mitigate adversarial examples [14]. However, most procedures to generate adversarial examples manipulate individual pixels via gradient updates and not frequency domain information. We outline these approaches in Section 2. An alternative approach to generating adversarial examples is to manipulate the individual frequency components of the perturbation. We explored this approach and extended the 푙∞ bounded Projected Gradient Descent (PGD) to the frequency domain by explicitly optimizing the frequency components of the perturbation. We call the new method, Spectral Projected Gradient Descent (SPGD), and formally describe it in Section 3. We also explore how this method can confer robustness to models via adversarial training. We will show that using SPGD during adversarial training provides competitive robustness while potentially requiring fewer attack steps. Because adversarial training is computationally expensive owing to the attack procedure used, it is beneﬁcial to reduce the number of attack steps used. Given a small budget of attack steps, we show in Section 4 that the SPGDtrained models provide better robustness than the PGD-trained model. However, we will also prove that SPGD is equivalent to a variant of PGD and show it is not necessary to explicitly optimize in the frequency domain. We theoretically and empirically show in Section 4 and appendix A this equivalence for a variant of PGD where the sign operation is not applied to the gradient.
Isospin violating dark matter in St\"uckelberg portal scenarios<|sep|>Uncovering the properties of Dark Matter (DM) and, in particular, its possible non gravitational interactions with visible matter is one of the greatest challenges of modern physics, and is accordingly the object of important experimental and theoretical eﬀorts. A common theoretical framework for DM studies is the hidden sector scenario. In its minimal form, visible matter resides in a sector of the theory that hosts the Standard Model (SM) gauge and matter content (or simple extensions thereof), while DM resides in a hidden sector, with its own gauge and matter content, but is otherwise neutral under the SM group. Within such a framework, several mechanisms have been proposed to mediate non-gravitational interactions between the diﬀerent sectors, usually referred to as portals [1–16]. Among them, perhaps the most popular is the Higgs portal [1] in which the SM Higgs boson has renormalizable couplings to scalar ﬁelds of the hidden sector. This kind of construction leads to important phenomenological consequences such as the contribution of hidden ﬁnal states to the branching fraction of the Higgs [17, 18]. Another popular kind of portal is the Z′ portal. In this scenario, a hidden sector communicates with the SM via a gauge boson, provided that the SM is enlarged with an extra abelian gauge group [19]. The phenomenology of these constructions is very rich, and ranges from colliders to direct and indirect searches of DM [20–26]. Particularly important for direct detection experiments are the isospin violation properties of the interactions of DM particles with nuclei induced by diﬀerent portals. For example, the Higgs portal (at least in its simplest form) automatically predicts isospin preserving interactions. A similar rigidity in the pattern of isospin interactions is present in many other portals. Previous works have shown that a deviation from these patterns would require the presence of several mediators whose contributions to the cross section interfere, and hence, generate an amount of isospin violation potentially tunable [22, 27–29]. It is the purpose of this work to show that, in contrast, U(1) extensions of the SM with St¨uckelberg Z′ bosons acting as portals naturally accommodate rich patterns of isospin violating interactions. The latter, in turn, provide a clear and testable phenomenological consequence of such models. Extra abelian gauge factors are among the most common extensions of the SM [19], and also among the best motivated from string theory, where massive extra U(1) gauge bosons appear ubiquitously (for reviews see e.g. [30–35]). In fact, when one tries to implement a visible sector with the SM gauge group in, say, intersecting brane models, one generically obtains not SU(3)c × SU(2)L × U(1)Y , but rather U(3)c × U(2)L × U(1)p which contains several extra abelian factors (including the centers of U(3)c and U(2)L). The models we will consider along this work are based on this type of string constructions. The symmetry structure of this scenario can be represented schematically in the following form, where the U(1)n v are n abelian gauge factors to which the visible matter ﬁelds Ψv couple. All of the corresponding gauge bosons acquire a mass through the St¨uckelberg mechanism, except for a particular linear combination of them that corresponds to hypercharge and remains massless (in the phase of unbroken electroweak symmetry). U(1)m h are m abelian gauge factors (some of which could be massless) to which only hidden matter Ψh couples, and Gh represents the semi-simple part of the hidden gauge group. As mentioned, these type of scenarios can fairly easily be implemented in models of intersecting D6-brane of type IIA string theory. Intuitively, each sector consists of several intersecting stacks of branes wrapping 3-cycles of a six-dimensional compactiﬁcation space. Each stack hosts a U(N) gauge factor and chiral matter arises at the brane intersections. Diﬀerent sectors arise from brane stacks that do not intersect each other and can hence be separated in the internal space (see Fig. 1). The extra abelian gauge bosons of Eq. (1.1) can provide a portal into the hidden sector in two diﬀerent ways. The most thoroughly studied is a small kinetic mixing of a light hidden gauge boson with the visible massless photon [36–44]. This generates an eﬀective coupling of DM with visible ﬁelds which is proportional to their electric charge, and hence, the DM particles only couple to protons and do not couple to neutrons. From the point of view of direct DM searches, this is very important, since the elastic scattering of DM oﬀ nucleons only receives contributions from protons. This is, the ratio Figure 1: Left: Schematic representation of a hidden sector scenario (1.1) with intersecting D-branes. Green and red branes do not intersect each other and hence host diﬀerent sectors. Right: Diagram contributing to the elastic scattering of hidden sector DM, ψ, oﬀ quarks. The mediator of this interaction is a mixing of diﬀerent string axions, φ, and the vectors Av and Ah. The second mechanism, which will be the main subject of this paper, was pointed out in Refs. [8,9] (see also [45–48]). It results from the mixing of massive U(1)s of the visible sector with U(1)s of the hidden one. Despite living in diﬀerent sectors, the U(1) gauge bosons An v and Am h often have St¨uckelberg couplings to the same axions, e.g. RR closed string axions in type II string models. As a consequence the resulting mass matrix can be highly non-diagonal. The ‘physical’ Z′ eigenstates obtained upon diagonalization of the kinetic and mass matrices are largely mixed combinations of An v and Am h and hence couple simultaneously to both, visible and hidden, matter sectors. This mass mixing is a tree-level eﬀect that provides an eﬀective portal into hidden sectors, provided the associated Z′ bosons are light enough. Despite the potentially complex gauge and matter structure of the hidden sector, it seems reasonable to assume that it hosts a Dirac fermion ψ that plays the role of DM in the Universe. The stability of these particles is easily guaranteed by the perturbatively conserved U(1)m h symmetries or by non-perturbatively exact discrete subgroups thereof, or simply because they are the lightest particles of the whole sector. In any case, their interaction with the SM fermions will be driven by the exchange of a Z′ boson. For DM direct detection experiments the leading interaction of the elastic scattering of ψ with quarks is depicted in the right panel of Fig. 1. Following this reasoning, it is clear that the charges of the SM fermions under the U(1)n v groups that mix with the hidden sector will determine the prospects for detecting ψ in these experiments. In this work we study the phenomenology of a class of scenarios of this kind that can be embeded into well known string theory constructions. In particular, we focus on the isospin violation character of the DM interactions with protons and neutrons induced by the Z′ bosons. As we will see, isospin violation could distinguish these St¨uckelberg portal models from other popular setups, such as the Higgs portal or the Z-mediation scenarios. This work is organized as follows. In section 2 we review the general theoretical framework that underlies our models. We describe the mixing mechanism that generates an oﬀ-diagonal mass matrix for the U(1) gauge bosons and study general properties the eigenstates of such matrix, which are the physical Z′ ﬁelds that communicate the hidden and the visible sectors. We also discuss how the general form of the eﬀective Lagrangian arises from certain string compactiﬁcations with intersecting D6-branes. In section 3 we take a well known class of such string models and determine the SM couplings to the lightest Z′ mediator in terms of a few mixing parameters. In section 4 we study the isospin violation properties of the DM-nucleon interactions in these constructions in terms of these parameters, and compare them to those arising in other popular scenarios. In section 5 we incorporate to our analysis bounds from direct detection (LUX) and collider searches (LHC) for six benchmark points in the parameter space of the model. Finally, we give some concluding remarks in section 6.
Adversarial Attacks on Deep Learning Based mmWave Beam Prediction in 5G and Beyond<|sep|>Due to the algorithmic and computational advances in deep learning (DL), various applications in different domains, such as computer vision [1] and speech recognition [2], have been empowered by deep neural networks (DNNs) in solving complex problems by effectively learning from rich data representations. Similarly, DL has been applied for various wireless communication tasks, such as waveform design, signal classiﬁcation, spectrum sensing, and interference management [3] by capturing the intrinsic characteristics of the spectrum data. One particular application of DL is in the domain of initial access (IA), where user equipments (UEs) need to establish their initial connection to an access point or base station when they attempt to join the communication network for the ﬁrst time [4]. As 5G and beyond communication systems rely on millimeter wave (mmWave) and higher frequency bands to sustain high data rates over large available bandwidths, transmissions become more directional using narrow beams. This paradigm makes the beam alignment in the IA process more difﬁcult as many narrow beams need to be swept to ﬁnd the most suitable beam for each UE [5], [6]. In the IA, the transmitter such as the 5G base station (gNodeB) sequentially transmits pilot signals over different narrow beams. The UE calculates the received signal strength (RSS) for each beam, determines the beam that provides the highest RSS, and informs the gNodeB of this beam selection. Since the time for This effort is supported by the U.S. Army Research Ofﬁce under contract W911NF-20-C-0055. The content of the information does not necessarily reﬂect the position or the policy of the U.S. Government, and no ofﬁcial endorsement should be inferred. the IA is limited, it is essential to reduce the number of beams swept as checking each beam consumes time and delays the UE to gain access to time-sensitive services such as ultrareliable low-latency communications (URLLC) in 5G. To overcome this issue, a DL-based approach has been proposed in [7], [8] to reduce the number of beams that need to be swept before making a decision, compared to the conventional beam sweeping approach that needs to sweep all possible beams. In this DL-based approach, the UE predicts the best beam from a large set of narrow beams by using only the RSSs for a subset of these possible beams. It is well known that DNNs are highly susceptible to carefully crafted adversarial perturbations that induce incorrect output or misclassiﬁcation, as ﬁrst shown in computer vision applications [9]. Wireless medium is shared and open to jamming attacks that can also be launched via adversarial machine learning to target the underlying DNNs. Therefore, adversarial machine learning has recently gained attention as the emerging attack surface for wireless security [10]. The attacks built upon adversarial machine learning include exploratory (inference) attacks [11], [12], adversarial (evasion) attacks [13]–[26], poisoning (causative) attacks [27]–[30], membership inference attacks [31], Trojan attacks [32], and spooﬁng attacks [33], [34]. These attacks are stealthier (harder to detect) than conventional jamming schemes [35], [36]. In this paper, we consider an adversarial attack that aims to manipulate the input to a DNN in test time. Most of the applications of adversarial attacks to the wireless domain have focused on wireless signal classiﬁers such as modulation classiﬁers [13]–[25] and spectrum sensing classiﬁers [26]– [28]. There have been efforts to extend adversarial attacks to communication problems such as autoencoder-based endto-end communications [37], power control for MIMO communications [38], and dynamic channel access [39], [40]. In addition, adversarial machine learning has been used to develop attacks on 5G-speciﬁc tasks such as 5G spectrum sharing with incumbent users [41], 5G signal authentication [41], and 5G radio access network (RAN) slicing [42], [43]. Our goal in this paper is to investigate the vulnerability of a DNN that is used for mmWave beam prediction as part of the IA process in 5G and beyond communications. We ﬁrst generate a non-targeted attack using fast gradient method (FGM) [44] to cause any misclassiﬁcation at the DNN classiﬁer at the receiver (independent of the wrong labels for beams). Then, we introduce the k-worst beam attack that not only causes misclassiﬁcation but also enforces the DNN classiﬁer to select one of the k worst beams to further reduce the IA performance. We compare the non-targeted FGM attack and k-worst beam attack with benchmark jamming attacks with Gaussian and uniform noise added across RSSs from input beams. Our results show that the beam prediction of the IA process is highly vulnerable to adversarial attacks that can signiﬁcantly reduce the beam prediction accuracy. The effect of this attack translates to notable reduction in communication performance in terms of data rate since the UEs connect to the gNodeB using beams that are not well aligned and the corresponding RSSs drop signiﬁcantly. The rest of the paper is organized as follows. Section II provides the system model. Section III describes the adversarial attacks considered in this paper, namely, the nontargeted attack and the k-worst attack. Section IV presents the performance results. Section V concludes the paper.
Bias-corrected methods for estimating the receiver operating characteristic surface of continuous diagnostic tests<|sep|>Before applying a diagnostic test in clinical settings, a rigorous statistical assessment of its performance in discriminating the disease status from the nondisease status is necessary. For a continuous-scale test T, the diagnosis is dependent upon whether the test result is above or below a speciﬁed cut point c. Assuming, without loss of generality, that higher test values indicate a higher likelihood of disease, a result is called positive if its value exceeds the cut point, and negative otherwise. A positive test indicates presence of the disease. At a ﬁxed cut point c, the accuracy of the test can be evaluated by its true positive rate (TPR) and its true negative rate (TNR), which are deﬁned as the probabilities that the test correctly identiﬁes the diseaded and non-diseaded subjects, respectively. The Receiver Operating Characteristic (ROC) curve is the plot of TPR versus 1-TNR by varying the cut point c. Usually, the ROC curve is monotone and lies in the upper triangle of the unit square, which consist of three vertices (0, 0), (0, 1) and (1, 1). The shape of ROC curve allows to evaluate the ability of the test. For example, a ROC curve equal to a straight line joining points (0, 0) and (1, 1) represents a diagnostic tests which is the random guess. A commonly used summary measure that aggregates performance information across the range of possible cut points is the area under ROC curve (AUC). Reasonable values of AUC range from 0.5, suggesting that the test is no better than chance alone, to 1.0, which indicates a perfect test. Clearly, the ROC curve and the AUC of a test under assessment are unknown and the statistical evaluation of the test requires suitable inferential procedures. See, for example, Zhou et al. [18] and Pepe [14] as general references. In principle, knowledge of the true disease status of the subjects under study is obtained by the most accurate available test, called gold standard (GS) test. In practice, there may be many drawbacks to implement the GS test, which can be too expensive, or too invasive, or both for regular use. Thus, often only a subset of patients undergoes disease veriﬁcation and the decision to send a patient to veriﬁcation typically depends on the test result and other patient characteristic. Statistical evaluations based only on data from subjects with veriﬁed disease status are typically biased. This bias is known as veriﬁcation bias. In the last ﬁfteen years, various methods have been developed to deal with the veriﬁcation bias problem, most of which assume that the true disease status, if missing, is missing at random (MAR, Little and Rubin [11]). Among the others, we cite the papers by [1], [3], [6], [7] and [15]. In particular, Alonzo and Pepe [3] proposed four types of partially parametric estimators of TPR and TNR, i.e., full imputation (FI), mean score imputation (MSI), inverse probability weighting (IPW) and semiparametric eﬃcient (SPE) estimators. In some medical studies, however, the disease status often involves more than two categories; for example, Alzheimer’s dementia can be classiﬁed into three categories (see [4] for more details). In such situations, quantities used to evaluate the accuracy of a diagnostic test are the true class fractions (TCF’s). These are well deﬁned as a generalization of TPR and TNR. In a three–class diagnostic problem, given a pair of cut points (c1, c2), with c1 < c2, subjects are classiﬁed into class 1 if T < c1; class 2 if c1 ≤ T < c2; and class 3 otherwise. The true class fractions of the test T at (c1, c2) are deﬁned as The plot of (TCF1, TCF2, TCF3) by varying the pair (c1, c2) produces the ROC surface of T in the unit cube. Scurﬁeld [16] and Nakas and Yiannoutsos [12] mentioned that a ROC surface is well deﬁned as a generalization of the ROC curve. Indeed, the projection of ROC surface to the plane deﬁned by TCF2 versus TCF1 yields the ROC curve between classes 1 and 2. Similarly, on projecting the ROC surface to the plane deﬁned by the axes TCF2 and TCF3, the ROC curve between classes 2 and 3 is produced (see also [13]). The ROC surface will be the triangular plane with vertices (0, 0, 1), (0, 1, 0), and (1, 0, 0) if all of three TCF’s are equal for every pair (c1, c2). In this case, we say that the diagnostic test is the random guess, again. In practice, one can imagine that the graph of ROC surface lies in the unit cube and above the plane of the triangle with three vertices (0, 0, 1), (0, 1, 0), and (1, 0, 0). A summary of the overall diagnostic accuracy of the test under consideration is the volume under the ROC surface (VUS) which can be seen as a generalization of the AUC. Reasonable values of VUS vary from 1/6 to 1, ranging from bad to perfect diagnostic tests. Nakas and Yiannoutsos [12] and Nakas [13] gave some interesting results about ROC surface analysis in absence of veriﬁcation bias. In particular, the authors formularized the ROC surface by a functional form and proposed a nonparametric approach for VUS estimation. Again without veriﬁcation bias, parametric estimation of VUS is supplied in the work of Xiong et al. [17], where the assumption of normality distribution was used, whereas Li and Zhou [10] tackled the nonparametric and semi-parametric estimation of the ROC surface. Li et al. [9] proposed a regression approach to ROC surface, and in Kang and Tian [8] a kernel smoothing based approach for estimation of VUS is employed. The issue of correcting for the veriﬁcation bias in ROC surface analysis is very scarcely considered in the statistical literature. Until now, only Chi and Zhou [4] discussed about the issue. The authors proposed maximum likelihood estimates for ROC surface and VUS. However, these results only concern ordinal diagnostic tests. This motivated us to develop bias-corrected methods for continuous diagnostic tests with three–class disease status. In this paper, we propose several veriﬁcation bias-corrected estimators of TCF1, TCF2 and TCF3 for continuous diagnostic tests. The proposed estimators are the extension of FI, MSI, IPW and SPE estimators for the ROC curves in [3]. The new estimators allow to obtain bias-corrected ROC surfaces. Corresponding estimators of the VUS are also presented. Consistency and asymptotic normality of the proposed estimators are established under the MAR assumption. The rest of paper is organized as follows. In Section 2, we review the estimators of ROC curves discussed in [3]. The proposed extension, giving biascorrected estimators of the ROC surface and of VUS, is presented in Section 3, along with the relevant asymptotic results. In Section 4, some simulation results are produced and two applications of the methods are contained in Section 5. Finally, conclusions are drawn in Section 6.
Cavity quantum electrodynamics with mesoscopic topological superconductors<|sep|>Condensed matter systems are an endless resource of emergent physical phenomena and associated quasiparticles. Majorana fermions, which are particles that are their own antiparticles and which have been ﬁrst proposed as particles in the context of high energy physics, emerge beautifully as zero energy excitations in condensed matter setups [1, 2]. Speciﬁcally, they are predicted to occur as zero energy excitations in solid-state systems, such as genuine p-wave superconductors [3–5], or engineered from topological insulators [6], semiconductor wires in a magnetic ﬁeld [7–9], or in chains of magnetic atoms [10–16], all in the proximity of s-wave superconductors. These exotic objects are robust against local perturbations and, moreover, they obey non-Abelian statistics [4, 17, 18] under braiding operations, thus recommending them as qubits for the implementation of topological quantum computation. Electronic transport is the foremost experimental tool for investigating the Majorana fermions physics but alternative, non-invasive, methods that preserve the quantum states would be highly desired to address these objects. Cavity quantum electrodynamics (cavity QED) has been established as an extremely versatile tool to address equilibrium and out-of-equilibrium electronic and spin systems non-invasively [19–27]. Majorana fermions, too, have been recently under theoretical scrutiny in the context of cavity QED physics [28–37]. However, most of these studies dealt with eﬀective low energy models that involved Majorana fermions only, leaving the bulk physics, which is at the heart of the Majorana physics, largely unexplored. The basic idea behind cavity QED with electronic system is that it allows one to extract various properties of the latter, such as its spectrum and its electronic distribution function, from photonic transport measurements, as opposed to electronic transport. Such photonic transport is quantiﬁed by the complex transmission coeﬃcient τ = A exp(iφ) that relates the output and input photonic FIG. 1. A sketch of the system: a one dimensional system (red rectangle) is placed at the maximum of the electrical ﬁeld (green straight arrows) inside a superconducting microwave cavity (blue). The electromagnetic ﬁeld inside the cavity is probed by sending input ﬁelds of amplitude and phase Ain and φin, respectively, and measuring the ﬁeld at the end with Aout and φout. The diﬀerence between the two gives a direct access to the electronic correlation function in the wire (see text). The presence of Majorana end modes in the ﬁnite wire (black curves) is also signaled in the cavity response. where ωc and κ are the frequency and the escape rate of the cavity, respectively, while Π(ω) is an electronic correlation function that depends on the actual coupling between the two systems, and which contains information about the spectrum of the electronic system. The phase and amplitude response of the cavity close to resonance ω ≈ ωc are related to the susceptibility Π(ω) as follows: δφ = Π′(ω)/κ and δA/Ain = Π′′(ω)/κ, where δφ = φout −φin, δA = Ain −Aout, and Π′(ω) = Re[Π(ω)] (Π′′(ω) = Im[Π(ω)]) is the real (imaginary) part of the susceptibility. In this paper, we evaluate the function Π(ω) ﬁrst for the simple case of a one-dimensional (1D) p-wave superconductor described by the Kitaev chain and then for more realistic model of a 1D topological semiconducting wire in proximity of a superconductor. We assume in both cases that these 1D systems are coupled to a microwave cavity, as showed schematically in Fig. 1. We address various physical situations for this coupling and show that such a method allows us to ascertain the topological phase transition point, the occurrence of Majorana fermions, and the parity of the ground state, all in a global and non-invasive fashion. The paper is structured as follows. In Sec. II, we describe our model Hamiltonians for the two systems under consideration and discuss the coupling between the microwave photons and the electrons in the 1D topological systems. In Sec. III, we show how the optical transmission through the cavity is able to probe the topological phase transition. In IV, we demonstrate that the cavity allows to detect the occurrence of Majorana fermions and the parity of the Majorana fermionic state in a non-invasive fashion. Finally, in Sec. V we provide a brief summary of our results. Technical details of the calculations are given in the appendices.
The Completed SDSS-IV extended Baryon Oscillation Spectroscopic Survey: BAO and RSD measurements from the anisotropic power spectrum of the Quasar sample between redshift 0.8 and 2.2<|sep|>Understanding the expansion history of the Universe is one of the crucial questions in cosmology. The latest results from the measurements of the angular temperature and polarisation ﬂuctuations in the cosmic microwave background (Planck Collaboration et al. 2018) and the analysis of type Ia supernovae light curves (Scolnic et al. 2018) highly favors a Universe that can be described in the framework of General Relativity (GR) by a standard cosmological model, ΛCDM. In this model, the Universe is made of collisionless cold dark matter (CDM), baryons, photons, and neutrinos and of an unknown component, usually called ‘dark energy’ which behaves as a ﬂuid of negative pressure. In the ΛCDM context, a cosmological constant Λ is inserted in the equation of general relativity to take account of the late-time acceleration of the expansion of the Universe. In the last 15 years, this picture of the Universe has been shown to work remarkably well using the phenomenon of baryon acoustic oscillations (BAO) in the primordial plasma. BAO leave their imprint on the distribution of matter in the Universe as a characteristic separation scale between matter over-densities. This distance is found in the separation of gravitationally collapsed structures such as galaxies (Eisenstein et al. 2005a; Cole et al. 2005; Alam et al. 2017) and quasars (Ata et al. 2017) and can be used as a ‘standard ruler’ by large-scale surveys to measure the evolution of the expansion of the Universe at diﬀerent epochs. As the eﬀort to measure the BAO scale to increasingly better precision continues, large-scale surveys have started to provide valuable information on the linear growth rate of structure. This is of signiﬁcant importance as it is a promising way to test GR (Linder & Cahn 2007). The growth of structure is measured from coherent peculiar velocities that lead to Redshift Space Distortions (RSDs) along the line of sight (Kaiser 1987). These distortions can be related to 𝑓 (𝑧)𝜎8(𝑧), where 𝜎8(𝑧) is the normalization of the linear power spectrum on scales of 8 h−1Mpc at redshift 𝑧 and 𝑓 is the linear growth rate of structure. Anisotropies in the clustering signal may also appear because the cosmology assumed to convert redshift to distance is diﬀerent from the true cosmology. This is known as the Alcock-Paczynski eﬀect (Alcock & Paczynski 1979) and is key to constraining the cosmological expansion history. In this paper, we present and analyse the power spectrum of the complete quasar sample of the extended Baryon Oscillation Spectroscopic Survey (eBOSS; Dawson et al. 2016) which is part of the SDSS-IV program (Blanton et al. 2017). The observations were made at the 2.5 m Sloan Foundation Telescope (Gunn et al. 2006) at the Apache Point Observatory (New Mexico, USA) using the two-arm optical spectrograph of BOSS (Smee et al. 2013). This study is part of a coordinated release of the ﬁnal eBOSS measurements of BAO and RSD in the clustering of luminous red galaxies (LRG (0.6 < 𝑧 < 1.0); Bautista et al. 2020; Gil-Marín et al. 2020), emission line galaxies (ELG (0.6 < 𝑧 < 1.1); Rai choor et al. 2020; Tamone et al. 2020; de Mattia et al. 2020), and quasars ((0.8 < 𝑧 < 2.2); Hou et al. 2020)1. At the highest redshifts (𝑧 > 2.1), the coordinated release of ﬁnal eBOSS measurements includes measurements of BAO in the Lyman-𝛼 forest (du Mas des Bourboux et al. 2020). The cosmological interpretation of these results in combination with the ﬁnal BOSS results and other probes is found in eBOSS Collaboration et al. (2020). Due to their high intrinsic luminosity, quasars can be used as tracers of the large scale structure at high redshifts (Croom et al. 2009; Myers et al. 2007; Ross et al. 2009; Shen et al. 2009; White et al. 2012; Karagiannis et al. 2014; Eftekharzadeh et al. 2015; Laurent et al. 2016). The Data Release 14 of the ﬁrst two years of eBOSS data (Ata et al. 2018; Gil-Marín et al. 2018; Hou et al. 2018; Zarrouk et al. 2018) demonstrated how well quasars are suited for cosmological clustering analyses and currently provide the most precise clustering information on large scales in the redshift range 0.8 < 𝑧 < 2.2. With the Data Release 16, the number of quasars is more than doubled. We present the measurement of the redshift space power spectrum with the ﬁrst three even Legendre multipoles. We perform both a standard ‘BAO-only’ analysis where we focus on the BAO features of the power spectrum and a ‘Full-Shape’ RSD analysis using the TNS model (Taruya et al. 2010). The BAO-only analysis allows us to constrain the Hubble distance, 𝐷H(𝑧)/𝑟drag, and the transverse comoving distance, 𝐷M(𝑧)/𝑟drag. In addition, we also constrain these two quantities together with the linear growth rate of structure, 𝑓 (𝑧)𝜎8(𝑧), using the ‘Full-Shape’ RSD analysis. The analysis presented in this paper uses the complete ﬁve years of the eBOSS sample and is accompanied by several companion papers. The clustering catalogues used in this analysis are described in Ross et al. (2020) and speciﬁc information relevant to the complete DR16Q quasar catalogue is given in Lyke et al. (2020). The quasar mock challenge upon which the model of the power spectrum is tested is described in Smith et al. (2020). Approximate mocks used for determining the covariance matrix and testing observational systematic eﬀects are described in Zhao et al. (2020). The analysis of the quasar sample in conﬁguration space is presented in Hou et al. (2020) and a consensus analysis of the work presented here is common to both articles. Cosmological implications of the measured quasar clustering properties are discussed in eBOSS Collaboration et al. (2020). This paper is structured as follows. In Section 2, we give an overview of the quasar sample, the estimator of the power spectrum and the set of mocks that we used for the estimation of the covariance and the assessment of the systematic errors. In Section 3, we discuss the measurement of the BAO scales. In Section 4, we present the Full Shape RSD analysis and describe the systematic errors that aﬀect the 1 A summary of all SDSS BAO and RSD measurements with accompanying legacy ﬁgures can be found here: https://www.sdss.org/science/ﬁnalbao-and-rsd-measurements/. The full cosmological interpretation of these measurements can be found here: https://www.sdss.org/science/cosmologyresults-from-eboss/. measurement. Our ﬁnal result, and the consensus analysis performed in our companion paper on the 2-point correlation function analysis, are presented in Section 5.
Data-Driven Control and Data-Poisoning attacks in Buildings: the KTH Live-In Lab case study<|sep|>Recent trends have shown a surge of interest in methods that intelligently learn from the data. This trend is also motivated by recent successes in using deep-learning based methods for supervised learning tasks or control problems. In control systems data-driven control approaches, a branch of adaptive control, have gathered much attention over the last few decades [1]–[6], due to some interesting features, such as being able to directly compute a control law from experimental data gathered on the plant. This type of technique avoids identifying a model for the plant, which is particularly troublesome in those cases where it is difﬁcult to derive, from ﬁrst-principles, a mathematical description of the system, thus enabling direct data-to-controller design. In this work, we will analyze the feasibility of using the Virtual Reference Feedback Tuning (VRFT) method [1], [6], [7] for temperature control in buildings. VRFT, compared to other data-driven control methods such as those based on Willems’ lemma [4], [8], allows to specify which requirements the closed-loop system should satisfy and aims at deriving a control law that satisﬁes the prescribed requirements. This particular feature of VRFT, coupled with the fact that the method is straightforward to use, makes it appealing in many control scenarios, from wastewater ⋆ Corresponding author 1Alessio Russo and Alexandre Proutiere are in the Division of Decision and Control Systems of the EECS School at KTH Royal Institute of Technology, Stockholm, Sweden. {alessior,alepro}@kth.se 2 Marco Molinari is in the Division of Applied Thermodynamics and Refrigeration at KTH Royal Institute of Technology, Stockholm, Sweden. marcomo@kth.se treatment [9] to unmanned aerial vehicle control [10] and control of solid oxide fuel cells [11]. Despite these advantages, the performance of VRFT is tightly coupled with the data being used and can be seen as an identiﬁcation problem. As such, it inherits the weaknesses of using data-based methods. For example, recently, it has been shown in the supervised learning community that a malicious agent can severely affect the performance of classiﬁers at test time by means of slight changes in the data used at training time [12]–[14]. A recent analysis demonstrated that data-driven control techniques are also affected by this particular attack for simple PID-like controllers [15], whilst the case where VRFT is used with non-linear controllers is left unexplored. Similar attacks, conducted at test time, have also been shown to work in the case of systems controlled through Reinforcement Learning controllers [16]. Contributions: the objectives of this work are twofold: (1) We ﬁrst analyze the feasibility of using VRFT for temperature control in buildings. This is validated by using a digital replica of the KTH Live-In Lab testbed [17], a model of the real building set up using IDA Indoor Climate and Energy (IDA ICE) [18], a software used to simulate buildings performance. (2) We then analyze the susceptibility of VRFT to data poisoning attacks, using the IDA ICE environment. We believe this is an important example of how data-driven control laws can be attacked. In buildings, the probability of sensors being hijacked is far from remote, and a malicious agent can use the data in several ways. This data could be used to determine the number of people present in the building or be poisoned to decrease the building’s energy efﬁciency. Gartner [19] predicts that through 2022 30% of all AI cyberattacks will leverage training-data poisoning, AI model theft, or adversarial samples to attack AI-powered systems. In [20], Microsoft engineers analyzed 28 companies and found out that only 3 of them have the right tools in place to secure their ML systems. This further stresses the importance of studying such problems. Organization of the paper: §II introduces the notation, the VRFT method, and the KTH Live-in Lab Testbed, which is a smart residential building located at the KTH campus. In §III, the VRFT method is used to derive a controller that can control the temperature in the KTH Live-in Lab testbed’s model. Finally, in §IV, the data poisoning attack from [15] is presented and applied to the VRFT method introduced in the previous section.
Interface solitons in quadratically nonlinear photonic lattices<|sep|>Electromagnetic surface waves are waves localized at an interface separating either two homogeneous (one of them has to be surface-active, i.e., exhibiting a negative permittivity [1]) or homogeneous and periodic dielectric media [2]. In addition, nonlinear dielectric media can support nonlinear guided waves localized at or near the surfaces, and diﬀerent types of nonlinear guided waves in planar waveguides have been studied extensively about 20 years ago [3, 4]. Recently, the interest in the study of electromagnetic surface waves has been renewed after the theoretical prediction [5] and subsequent experimental demonstration [6] of nonlinearity-induced self-trapping of light near the edge of a one-dimensional waveguide array with self-focusing nonlinearity, that can lead to the formation of a discrete surface soliton. A related eﬀect of light localization and the formation of surface gap solitons have been predicted theoretically and observed experimentally for defocusing periodic nonlinear media [7, 8]. In addition, the concept of nonlinear surface and gap solitons has been extended to the case of an interface separating two diﬀerent nonlinear periodic media [9, 10, 11, 12]. Surface solitons are usually considered for onefrequency modes propagating in cubic or saturable nonlinear media. However, multicolor discrete solitons in quadratically nonlinear lattices have been studied theoretically in both one- and two-dimensional lattices [13, 14, 15, 16] irrespective to the surface localization eﬀects. Only Siviloglou et al. [17] studied discrete quadratic surface solitons experimentally in periodically poled lithium niobate waveguide arrays, and they employed a discrete model with decoupled waveguides at the second harmonics to model some of the eﬀects observed experimentally. More elaborated theory of one-dimensional surface solitons in truncated quadratically nonlinear photonic lattices, the so-called two-color surface lattice solitons, has been developed recently by Xu and Kivshar [18] who analyzed the impact of the phase mismatch on the existence and stability of nonlinear parametrically coupled surface modes, and also found novel classes of onedimensional two-color twisted surface solitons which are stable in a large domain of their existence. The purpose of this paper is twofold. First, we extend the analysis of two-color surface solitons to the case of two semi-inﬁnite photonic lattices with quadratic nonlinearities. We study, for the ﬁrst time to our knowledge, twocolor interface solitons in photonic lattices with quadratic nonlinear response. We analyze the eﬀect of mismatch on the existence, stability, and generation of such novel surface states. Second, for the analysis outlined above we employ two diﬀerent approaches widely used in the literature: The coupled-mode theory described by the discrete parametrically coupled equations for the fundamental and second-harmonic ﬁelds, and also the continuous model with a periodic potential, and demonstrate that both models give the same qualitative results. The paper is organized as follows. In Sec. II we discuss the two-color interface localized modes in the framework of a discrete model where the interface is modeled by a jump of the propagation constant, assuming that the matching conditions are nearly satisﬁed for both semiinﬁnite lattices. Section III is devoted to the analysis of a more general case described by a continuous model with a periodically varying potential. Thus we consider the light beam propagating along the interface between two dissimilar optical lattices imprinted in quadratic nonlinear media. Besides the study of the properties of interface solitons, we demonstrate the manipulation of surface solitons by tuning the lattice and waveguide parameters. Finally, Sec. IV concludes the paper.
Excitonic fine structure splitting in type-II quantum dots<|sep|>Excitonic ﬁne structure splitting (FSS) refers to a tiny energy splitting of two bright exciton states conﬁned in quantum dot (QD) heterostructures with a typical magnitude ranging from units to hundreds µeV. It is manifested in a doublet structure of the exciton recombination band. It was observed for the ﬁrst time in GaAs/AlGaAs quantum wells with ﬂuctuating thickness1 and then in various QD systems.2–5 Soon after its discovery it was attributed to the electron-hole exchange interaction6 and its ﬁnite value was related to the reduced symmetry, which needs to be lower than D2d.2 The interest in FSS is triggered by both fundamental and application point of view. FSS helps to distinguish the spectral features originating in the recombination of exciton (doublet), biexciton (doublet with opposite polarization-energy dependence), and trion (singlet). Benson’s proposal of the source of entangled photon pairs relying on zero FSS7 has called for the preparation of QD systems with low FSS. Using (111) substrates for the growth of InAs QDs reduced both structural asymmetry and piezoelectric contribution.8 Another attempt involved strain-free GaAs/AlGaAs QDs with zero piezoelectric ﬁeld, which, however, still exhibited a ﬁnite FSS due to structural elongation.5,9 Post-growth annealing10 of InAs QDs allowed to decrease FSS from 96 µeV to mere 6 µeV. Another class of approaches is based on in-operation tuning, where the originally large value of FSS is reduced by applying the external ﬁeld: elec tric,11,12 magnetic,2,13 or strain. The external strain ﬁeld allowed to reach FSS below experimental resolution in GaAs/AlGaAs QDs;14,15 the simultaneous application of electric ﬁeld allowed for a more powerful symmetry restoration and rather universal recovery of low FSS.16 Various eﬀects contributing to the FSS can be divided into two classes based on the involved length scale: atomic and macroscopic. Atomic-scale eﬀects are connected with the irregularities of the crystal lattice such as the interfaces, particular elements distribution in alloys,17 charged defects,18 etc. The magnitude of these eﬀects is still subject of investigation; the atomistic simulations based on the tight-binding method19 predict considerably larger values than those relying on the empirical pseudopotential method.20 In general, atomic-scale eﬀects are weak compared to those on macroscale. For example, the magnitudes of about 1 µeV are reported for a speciﬁc alloy distribution in the AlGaAs barrier17 of GaAs QDs. The eﬀect is more pronounced when the dot material is an alloy, which should be therefore avoided when aiming at low FSS. A lower bound of several µeV was predicted for strain-tuned FSS in ternary In0.6Ga0.4As QDs.21 By macroscopic scale we mean for the purpose of the foregoing discussion that the characteristic length of the eﬀect is comparable with the dimensions of a QD and the underlying crystal lattice is perceived as a homogeneous environment. Thus, the crystal symmetry is no longer relevant and the ﬁnite values of FSS are now related to the symmetry lower than C4, i.e., to the lateral elongation of the wave functions (e.g. envelope functions of the k · p theory). Principal contributions to the FSS on macroscopic scale arise from the asymmetric (elongated) shape of a QD and piezoelectric ﬁeld. Further, it is possible to use the external strain ﬁeld to induce the anisotropic eﬀective mass tensor and modify the elongation of the hole wave functions and the related value of FSS.14 Further information is contained in the polarization properties of the exciton doublet. Simple considerations assuming a purely heavy-hole exciton in an elliptic-diskshaped QD6 predicted a linear polarization of both transitions with the low-energy component polarized parallel with the long QD axis and the high-energy component having the orthogonal polarization. Typically, both structural-elongation and piezoelectric axes are parallel with the crystal axes [110] and [1¯10], and so are the polarizations of both components. However, in some structures with shallow irregular conﬁnement potential, such as quantum well thickness ﬂuctuations, stochastic polarization directions were observed.5 Further, when the light-hole contribution to the exciton ground state becomes important, the polarization orthogonality of both components is lost.14,22 We focus here on QDs with type-II conﬁnement, in which one type of charge carriers is conﬁned in QD volume and the other in the barrier close to the QD vicinity. The particular system of interest are InAs QDs with a thin GaAs1−ySby overlayer embedded in GaAs. One reason for selecting this material system is the possibility to induce a smooth crossover between type-I and type-II conﬁnement regime simply by changing y; the crossover values between 0.14 and 0.18 have been reported.23–25 The other is that it belongs to the minority of systems with holes bound outside. Owing to their large eﬀective mass the holes are more susceptible to the local potential proﬁle or external perturbations, oﬀering a larger potential for tuning their wave functions and the related FSS. The photoluminescence of GaAs1−ySby capped QDs is rather intense despite the type-II conﬁnement with the radiative lifetimes as low as 10 ns.26 The strain-reducing eﬀect of GaAs1−ySby layer together with the surfacting eﬀect of antimony allow to increase the emission wavelengths of standard InAs QDs and reach the telecommunication wavelength of 1.3 and 1.55 µm.27,28 Various shapes of GaAs1−ySby QDs have been reported, including a lens29 or a pyramid with a graded In concentration.30 Notably, the hole wave function is expected to be composed of two segments localized in the minima of the piezoelectric potential.24 In this work we present a theoretical study of excitonic ﬁne structure splitting of InAs QDs with GaAs1−ySby overlayer. We propose a method to tune the FSS by setting the thickness of the GaAs1−ySby layer. The values comparable with the natural linewidth can be achieved even in low-symmetry QDs. The paper is organized as follows: In Section II a theory of FSS is described. To gain a qualitative understanding of the relations between the wave functions and FSS we discuss in Section III a simpliﬁed single band model with Gaussian wave functions. The full calculations are presented in Section IV. We summarize and conclude in Section V.
Designing ML-Resilient Locking at Register-Transfer Level<|sep|>Integrated circuits (ICs) are a critical layer for security in modern electronic systems. However, there are security concerns due to third parties in the supply chain. As external design houses and foundries have full access to the IC intellectual property (IP) during production, attackers could reverse engineer the IP for malicious purposes, such as IP theft and hardware Trojan insertion [19]. Design-for-trust methodologies aim to counteract such threats. Logic locking has been recognized as a premier technique to safeguard ICs throughout the supply chain [10]. Logic locking builds on the concept of design obfuscation [2, 9, 15–17], where designers insert key-driven logic to functionally and structurally alter ICs, thus concealing functional intent. Only the correct activation key unlocks the intended functionality of the IC. Recently, machine learning (ML) techniques have challenged the security of gate-level locking [8]. ML-driven attacks exploit the predictable relation between the key value and the functional or structural aspects of locking. This has led to potent attacks that can either predict the correct key value or remove the locking circuitry from the netlist [6, 11, 12, 18]. While ML-driven attacks often lack output certainty, their applicability adds another requirement for logic locking success—prevention of key-related residue within the locking mechanics. As long as the structural change is related to key values, it is possible to use ML to guess the keys. Traditional gate-level locking schemes are limited to local changes and do not use the semantic information of the circuit, as logic synthesis and optimization disperse the semantics to a low granularity. Therefore, gate-level locking schemes operate “blindly” on the design without considering its functional traits. In response, RTL locking has emerged as a way to overcome this issue [1, 5]. At the RTL, locking can use the full spectrum of semantic information, including operations, constants, and control flow constructs. Hence, RTL locking is a promising basis to build ML-resilient locking. However, compared to gate-level locking, ML attacks on RTL locking remain unexplored as shown in Fig. 1. Contributions: Thus, this study explores ML resilience of RTL locking focusing on operation obfuscation, where we: • Expose security faults in ASSURE RTL locking [5]. • Define ML-resilience security metrics for RTL locking. • Introduce two ML-resilient locking algorithms: (1) ERA: Exact ML-Resilient Algorithm and (2) HRA: Heuristic MLResilient Algorithm. To the best of our knowledge, the presented concepts and locking procedure are the first to address the challenges of ML resilience on RTL. The implementation of this study will be made available to the community once published.
A preprocessing perspective for quantum machine learning classification advantage using NISQ algorithms<|sep|>Machine Learning (ML) is a predominant tool nowadays to solve several challenges in different industries, such as credit scoring Provenzano et al. [2020], fraud analysis Tiwari et al. [2021], product recommendation Rohde et al. [2018], and demand forecasting Masini et al. [2020], among other extensively explored use cases. Under this premise, the research of the quantum computing properties applied to ML has expanded rapidly in recent years since a proven advantage could be a highly useful cross-industry. The recent progress of these explorations in Quantum Machine Learning (QML) Mishra et al. [2021] put a spotlight on quantum technology, introducing a challenge to determine if QML will provide an advantage over classical machine learning or not. The actual devices are noisy, meaning that the depth or consecutive gate operations are limited [Ristè et al., 2013, Burnett et al., 2019, Wang et al., 2021]. Qubits will lose their entanglement and so, the information. These devices make up the NISQ era Preskill [2018] and limit the use of quantum algorithms or hybrid algorithms to be useful Callison and Chancellor [2022]. A few cases are already in the market showing promising results and some companies’ commitment to the quantum machine learning journey. One example is CaixaBank (Spanish Bank) which is working and testing QML models using the Pennylane quantum framework to deﬁne a scoring model for risk assessment CaixaBank [2022]. One of the major concerns to even grasp a reliable result remains on the input/output concept and on the number of available good qubits to use. The bound of 100+ was reached by IBM Chow et al. [2021], but it is still insufﬁcient to use complex algorithms that require thousands or millions of qubits depending on the type of problem to be addressed Dalzell et al. [2020]. To be practical in a business context, QML techniques need to avoid the small number of qubits constraint and create a methodology to use big datasets on the current NISQ devices. Previous works showed great potential in splitting big circuits and learning the weights of the different gates separately, or reusable qubits for image classiﬁcation Haug et al. [2021]. In this paper, we approach the input problem by comparing different preprocessing methods and classiﬁer methods on small and larger datasets with a binary target. The objective is to determine a speciﬁc architecture for preprocessing, dimensionality reduction of the dataset structure, the encoding manner and the corresponding classiﬁer. We demonstrate that using Linear Discriminant Analysis (LDA) within the preprocessing phase is better than Principal Component Analysis (PCA) when the dataset possesses an important number of features. We generalize this approach by studying the effect of LDA on the encoding qubits. Different tabular datasets (Section 2) are used to understand the link between the number of features and the encoding process (Section 5). We develop a pipeline (Section 3) to compare the classical and quantum classiﬁer. This study leads us with a review of the current literature to determine and discuss rules to obtain a quantum advantage with the current NISQ devices.
Electrohydrodynamic channeling effects in narrow fractures and pores<|sep|>Electric double layers (EDL) play an important role in many chemical and physical processes, and is a controlling factor in many industrially applied microﬂuidic devices [1] and electrochemical cells [2]. Examples include nanoﬂuidic devices for electrophoretic separation or the large-scale harvesting of energy by mixing ﬂuids of diﬀerent salinity (“Blue energy”) [3]. In biological systems, EDLs are important e.g. for ion transport across membranes or for polymer aggregation [4–6]. In ﬂuidsaturated low-permeability rock, the presence of an EDL can signiﬁcantly alter the mineral transport and thereby inhibit or amplify transformation reactions, as demonstrated by ﬁeld observations and nanopore molecular dynamics simulations [7]. Furthermore, EDLs alter the effective wetting properties of mineral surfaces (see e.g. [8] for a study of reservoir sandstone), which could play an important role in enhanced oil recovery based on injection of low salinity ﬂuids. The transport of ﬂuid and minerals in ﬂuid saturated porous rock often occurs in networks of narrow fractures or pores, many of which have (sub) micrometer-sized apertures. When the pore walls are charged, and the resulting EDL extends signiﬁcantly into the pore ﬂuid, it may change the bulk ﬂow properties of single fractures and pores, and consequently of the whole network. Electrokinetic ﬂow, however, is a highly non-linear process, which is hard to quantitatively describe in even the most simple systems. In general, mean-ﬁeld approximations are often used to model systems beyond the nanometer range [9, 10]. From a number of simplifying assumptions, e.g. neglecting ion-ion correlations and non-Coulomb forces (so-called Gouy–Chapman theory), one obtains ﬁeld equations, which can be used for basic theoretical considerations. Even then, only simple geometries permit analytical solutions, such as cylindrical capillaries [11]. In equilibrium and when the electric ﬁeld is weak, the linearized Poisson–Boltzmann equation can be applied: where ϕ is the electric potential and κ−1 is the Debye length characterizing the extent of the EDL. However, when ion transport is coupled to ﬂuid advection, the equilibrium assumption generally breaks down and other means must be pursued. Further, numerical simulations can be challenging, and have in general been limited to simple geometries such as ﬁnite-length symmetric channels e.g. in studies of transient streaming potentials in single-phase ﬂow [12, 13] or in studies of electroconvection near permselective membranes [14, 15]. Here, we consider electrokinetic ﬂow in a model porous material or fracture by solving numerically the Stokes– Poisson–Nernst–Planck (SPNP) equations. In particular, we will quantify how the permeability changes as the extent of the EDL compared to channel size is varied, and we also describe how the EDL can switch the channeling of the ﬂow in our system from regions of small aperture to regions of larger aperture. The paper is organized as follows. In Sec. II, we present the model set-up, the governing equations and their dimensionless form, in Sec. III we present the simulation method and our numerical scheme, and in Sec. IV we present the results of the simulations, including validation, and eﬀects of varying amplitude and relative Debye length. In Sec. V we discuss technical aspects of our work and ﬁnally the conclusions and future directions follow in Sec. VI.
Visualization of short-term heart period variability with network tools as a method for quantifying autonomic drive<|sep|>Network methods have been successfully used to capture and represent properties of multilevel complex man-made systems [13] and living organisms [3]. The use of network representations in the characterization of time series complexity is a relatively new but quickly developing branch of time series analysis ([10], [12]). The most direct method is to map a time series into a graph in which vertices represent signal values, while edges link values that are consecutive in a signal. The correspondence between time series formed by consecutive cardiac interbeat intervals, so called RR-intervals, and such networks was studied by Campanharo et al. [7]. The topology in these networks appeared as a clique, i.e., each state is reachable from each other in a single step. Understanding RRinterval dynamics arising from a network with such a structure is not straightforward. It appears, in general, that information provided by a network graph strongly depends on the nature of sequences and our knowledge about the underlying dynamics ([12], [13]). Therefore the use of network methods, e.g., visualization or/and structure decomposition, is eﬀective only if they are used in conjunction with other sources of learning. We show how tools developed within the scope of complex networks can be fruitfully applied to the qualiﬁcation and quantiﬁcation short-term heart period dynamics. Fluctuations in RR-intervals are known to have a scale-invariant structure which demonstrates fractal ([17], [31], [22]) and multifractal ([15]) properties. These ﬂuctuations appear as a result of many component interactions acting over a wide range of time and space scale. Competing stimuli from the autonomic nervous system are assumed to be the reason for the fractal organization observed in RR-intervals [28]. By observing subsequent changes in RR-intervals, we obtain beat-to-beat information about the resulting force of these interactions. We have found that important dynamical aspects about the autonomic competitive regulation can be described by changes in RR-intervals, namely by RRincrements ([20], [21]). Signal increments can be decomposed into their magnitude (absolute value) and their direction (sign). Magnitude and sign analysis has been used to investigate the scaling properties of RR-intervals [2]. It has been found that magnitude series are long-range correlated, while sign series are anticorrelated. Furthermore, it has also been shown that during sleep, the strength of these correlations varies depending on the stage of the sleep: rapid-eye-movement (REM) or other (non-REM) sleep stages [16]. It appears that both the strongest anticorrelations in the sign signals, and largest exponents for long-range correlations for the magnitude signals are in REM sleep. Furthermore, it is assumed that during REM sleep, the nonlinear properties of the heartbeat dynamics are more pronounced. During sleep, the heart rate is mostly regulated by the autonomic nervous system and is less inﬂuenced by physical or mental activity. Moreover, during the night, vagal (parasympathetic) predominance is present, which makes this period a useful state to observe autonomic activity. The classical tools applied to measuring heart rate variability during physiological sleep have shown that the REM stage is characterized by a likely sympathetic predominance associated with a vagal withdrawal, while the opposite trend is observed during non-REM sleep (see [29] for a review). Previous studies have also shown that alternations in nocturnal heart rate variability have clinical importance, e.g., may explain why sudden cardiac death in many cases occurs during the night [30]. Heart transplantation surgery destroys the nerve connections between the organism and the graft — the donor heart is completely denervated, the vagal ganglia at the sinus node are cut oﬀ from medulla signals. The regulation is driven by the intrinsic heart mechanisms. The sympathetic control works indirectly at the level of circulating adrenergic hormones in the blood. The lack of vagal activity has the eﬀect, for example, that heart transplant recipients have a resting heart rate higher than the average in healthy people, and their heart rate variability is signiﬁcantly reduced [5]. The exception is a small respiratory sinus arrhythmia ([25], [11]), which is assumed to be an eﬀect of the intracardiac reﬂex ([1], [32]) or mechanical stretch of the sinus node. Cardiac reinnervation has been demonstrated in long-term heart transplant recipients ([6], [24], [9]), but it seems that it is limited to the sympathetic nerves. Therefore, the comparison of the nocturnal heart rate variability in healthy young individuals and heart transplant patients gives a unique opportunity to show the impact of autonomic (especially vagal) activity on heart rate regulation. Following [2], to discover in which way properties of networks constructed from RRincrements demonstrate nonlinear or/and linear dependences among consecutive RRintervals, we investigate properties of artiﬁcially modiﬁed RR-interval data [26]. In the following, we argue that network methods are successful in detecting nonlinear properties in the dynamics of autonomic nocturnal regulation in short-term variability. Two modes of visualization of networks constructed from RR-increments are proposed. The ﬁrst is based on the handling of a state space. The state space of RR-increments can be modiﬁed by a bin size used to code a signal, and by the role of a given vertex as the representation of events occurring in a signal. The second mode relies on the matrix representation of the network on the two-dimensional plane. This approach is similar to the accepted method, known as the Poincar´e Plot representation of time series for evaluation of heart rate variability. The methods introduced will be applied to nocturnal Holter signals recorded from healthy young people and from cardiac transplant recipients. Thus we obtain a way to ﬁlter out the intrinsic heart variability from the autonomic drive and then to quantify complexity in the short-term RR-interval variability related to nocturnal rest. Changes in RR-increments in a heart deprived of autonomic supervision provide us with insight into beat-to-beat dependences in forces governing the intrinsic heart dynamics.
Distribution of the position of a driven tracer in a hardcore lattice gas<|sep|>Studying the dynamics of an active particle or a particle submitted to an external force travelling in a crowded environment is a frequent problem in physics and in biology. Diﬀerent examples are found in biophysics, when one considers molecular motors, motile living cells or bacteria [1, 2], or in the study of biased intruders in granular systems [3] or colloidal suspensions [4]. The determination of the dynamics of this tracer particle (TP) is consequently an important question, with diﬀerent applications. In particular, new experimental tools allow the study of a medium using a microscopic probe particle submitted to an external force. This ﬁeld of research, commonly named active microrheology, has been applied to diﬀerent systems in the past decades, among which biological cells [5, 6], complex ﬂuids [7, 8] and colloidal suspensions [4, 9]. From a theoretical point of view, the diﬃculty lies in the modeling of the environment of the tracer particle (TP), which is constituted of a large number of interacting bath particles. In most analytical approaches, the evolution of the position of the TP is studied with some eﬀective description of the bath of particles [10], which do not take into account the correlations between the position of the TP and the density proﬁles of the surrounding bath. In the situation where the TP and the bath particles have comparable sizes, the response of the probe to the external forcing, and in particular its ﬂuctuations, cannot be accounted for correctly with an eﬀective treatment. Here, we study the diﬀusion of a driven tracer in a lattice gas of hardcore particles. This minimal model explicitly takes into account the bath dynamics: the driven TP performs a biased nearest-neighbor random walk, in a bath of particles performing symmetric nearest-neighbor random walks, with the restriction that each site is occupied by at most one particle. We assume that the lattice is in contact with a reservoir of particles, so that the bath particles present on the lattice may desorb back to the reservoir, and particles from the reservoir may adsorb onto vacant lattice sites. This so-called Langmuir kinetics is relevant to describe situations where a gas or a vapor is brought in contact with a solid surface, on which the gas particles may form an adsorbed layer. The transport properties of the adsorbed particles have been shown to control many diﬀerent processes, such as spreading of molecular ﬁlms on solid surfaces [11] or dewetting [12, 13]. The particular case where the Langmuir kinetics is coupled to a Totally Asymmetric Exclusion Process was investigated theoretically [14, 15], and has been show to be relevant to describe the directional motion of molecular motors on a cytoskeletal ﬁlament, with random attachment and detachment of the motors [16, 17]. Studying the transport properties of a biased TP in a hardcore lattice gas is actually a complex N-body problem. In the situation where the density of bath particles ρ is very high and where the number of particles on the lattice is conserved, the problem can be treated exactly to obtain results at leading order in (1 − ρ) [18, 19, 20, 21]. For an arbitrary density of particles, exact results were established concerning the mean position of the TP in the one-dimensional situation [22], and concerning the validity of the Einstein relation [23]. The situation where the lattice is populated by an arbitrary density of particles and where it is in contact with a reservoir of particles was addressed by resorting to a mean-ﬁeld-type approximation consisting of the decoupling of relevant correlation functions, allowing the computation of the mean position of the TP and of the bath density proﬁles in the long-time limit, in the case of a one-dimensional lattice [24] and for lattices of higher dimension [25, 26, 27, 28]. Numerical simulations sampling exactly the master equation of the problem revealed the accuracy of the decoupling approximation in a wide range of parameters. The situation where the bath particles are ﬁxed and can appear and disappear with prescribed rates, known as dynamical percolation [29], was also considered [30]. More recently, by extending the decoupling approximation initially proposed to study the mean position of the TP, its ﬂuctuations have been studied [31]. An evolution equation for the ﬂuctuations of the TP on a lattice of arbitrary dimension was obtained. This equation was solved explicitly in the case of a one-dimensional lattice, and in the stationary limit. The analysis of the solutions revealed a striking feature of the diﬀusion coeﬃcient: in a wide range of parameters, it is shown to be a nonmonotonic function of the density of particles on the lattice. Counterintuitively, the diﬀusion coeﬃcient of the TP can then be enhanced by the presence of bath particles in its environment. Recently, this nonmonotonicity was also observed in the situation where the TP is dragged in a bath of soft particles [32, 33], and then appears to be a generic feature of biased intruders in crowded environments. In this paper, we ﬁrst give a detailed computation of the results presented in [31]: we establish the evolution equation of the ﬂuctuations of the TP position in arbitrary dimension under the decoupling approximation, and solve it in the case of a onedimensional lattice. We show that the non-monotonicity of the diﬀusion coeﬃcient with respect to the density of particles on the lattice is correlated to a non-monotonicity of some cross-correlation functions with respect to the distance to the TP. The main result of this paper is the following: we go one step further and generalize the mean-ﬁeld-type approximation in order to calculate the cumulant generating function of the position of the TP, and therefore its complete probability distribution. Denoting by Xt the position of the TP, by Xt = Xt·e1 its projection along the direction of the bias, and deﬁning the cumulant generating function Ψ(u; t) ≡ ln⟨eiuXt⟩, we obtain where pν is the jump probability of the TP in the direction ν of the lattice, τ is its characteristic jump time, σ is the lattice spacing and ηr is the occupation number at site r. The evolution equations for the correlation functions �wr(u; t) = � eiuXtηXt+r � / � eiuXt� are obtained using an extension of the mean-ﬁeld-type approximation proposed to study the mean and ﬂuctuations of the position of the TP. This result is given by Eqs. (74) and (75), where the quantities kr(t) = ⟨ηXt+r⟩ are the density proﬁles in the reference frame of the TP and are the solutions of Eqs. (15) and (16). The equations satisﬁed by the correlation functions �wr(u; t) are solved in the particular case of a one-dimensional lattice, and we compute the probability distribution of the position of the TP. We show that all the cumulants of the TP position scale as t. We also consider the random variable obtained by rescaling the position of the TP by √ t. For this random variable, all the cumulants of order greater than 2 vanish in the long-time limit, which shows that the position of the TP is asymptotically Gaussian. The article is organized as follows. In section 2, we present the model and give the master equation governing the joint probability of the position of the TP and of the bath particles conﬁguration. In section 3, we give the evolution equations of the ﬁrst two cumulants. These equations involve the density proﬁles around the TP and some tracer-bath cross-correlation functions, whose evolution equations are explicitly given in a closed form by resorting to a mean-ﬁeld-type approximation. In section 4, we generalize this approximation in order to obtain a closed set of equations for the cumulant generating function of the TP position. These equations are valid in arbitrary dimension, and they give in principle the whole probability distribution of the TP position. As a particular case, we also obtain the equation satisﬁed by the third-cumulant of the distribution. In section 5, we solve the equations obtained under the decoupling approximation in the particular case of a one-dimensional lattice, and then obtain explicit expressions for the ﬁrst three cumulants, as well as an implicit determination of the cumulant generating function. In section 6, these analytical solutions are compared with results from Monte-Carlo numerical simulations that exactly sample the master equation. We summarize our results and give an outlook in section 7.
Equitable Scheduling for the Total Completion Time Objective<|sep|>Fairness-related issues are an increasingly important topic in all areas of resource allocation [3, 10, 19, 28], and in particular, in the area of scheduling [18]1. There are several standard scheduling objective functions that naturally address fairness in a very broad sense, such as maximum lateness or maximum completion time. Baruah et al. [1] introduced the concept of “proportionate fairness” for periodic scheduling problems, and there have also been studies related to “envy-free scheduling” [4] and fairness in shared resources [8]. Nowadays, equity and fairness in resource allocation is a widely discussed topic, leading to considerations such as the “price of fairness” [2] or to discussions about the abundance of fairness metrics [13]. However, previous research into fairness in scheduling did not seem to address scenarios where a sequence of repeated scheduling requests occur throughout a given time period. Heeger et al. [14] studied a new model that attempts to directly tackle this issue. In this new equitable scheduling model, we have n clients who submit jobs to a single machine over a period of m days, one job per day. The goal is to determine whether there exists a k-equitable set of schedules for all jobs on all days, i.e. a schedule for each day so that no client has more than k tardy jobs over the entire m-day period. Thus, fairness in this model is interpreted in such a way so that no client gets signiﬁcantly worse service compared to any other client. Here, k represents the equability parameter, where the smaller k is, the more equitable (or fair) the scheduling scheme becomes. Heeger et al. [14] presented several exact and approximate algorithms for this problem, as well as a few complementary hardness results. Building upon the work of Heeger et al. [14], we consider the same basic model under a diﬀerent objective function, namely the total completion time. This objective function naturally arises whenever the completion times of the jobs play a crucial factor (which it almost always does in real-world applications). To motivate the problem we study, consider the following simple scenario: Suppose you are the head of a research group with n PhD students (the clients). Each week (analogue to the days) the students hand in drafts of their current research project to get feedback. As the head of the group, you read the drafts one after another, and the time to provide feedback depends on the length of the draft. Ideally, over the course of a scientiﬁc year, the total (or, equivalently, average) waiting time for feedback of all PhD students should be as small as possible, so as to provide a fair research environment. This is precisely the setting we model with our equitable scheduling problem. We formally deﬁne the equitable scheduling problem that we study in this paper as follows: We are given a set of n clients, each associated with a single job on each of a set of m diﬀerent days. On each day, a single machine is available to process the n jobs non-preemptively. The job of client j on day i has processing time pi,j ∈ N, where we also allow pi,j = 0 when client j has no job on day i. A schedule σi for day i is a permutation of the n jobs available at that day representing the order in which they are processed. Given a schedule σi for day i, we use Ci,j(σi) = � j′:σi(j′)≤σi(j) pi,j′ to denote the completion time of client j’s job on day i. Given a set of schedules σ = {σ1, . . . , σm}, we use Cj(σ) = � i Ci,j(σi) to denote the total completion time of client j. We use Ci,j and Cj whenever the set of schedules is clear from the context. A solution to our scheduling problem is a set of m diﬀerent schedules {σ1, . . . , σm}, one for each day. We say that the solution is k-equitable, where k ∈ N is the equitability parameter given alongside the input, if � i Ci,j ≤ k for each client j ∈ {1, . . . , n}. The goal is thus to determine whether there exists a k-equitable solution for the set of input jobs. Adapting the classical three-ﬁeld notation of Graham [12] to the equitable scheduling setting, we denote our problem by 1 || maxj � i Ci,j. Input: A set of n clients, each having a job with processing time pi,j ∈ N for each day i ∈ {1, . . . , m}, and an integer k. Task: Find a set of m schedules {σ1, . . . , σm} so that for each j ∈ {1, . . . , n} we have � i Ci,j ≤ k. Our contribution. We introduce and investigate the computational complexity of the equitable scheduling problem 1 || maxj � i Ci,j, which is based on the framework introduced by Heeger et al. [14]. We analyze our problem in terms of parameterized complexity [5, 7, 9] with a focus on exact algorithms. To start things oﬀ, we consider in Section 2 the case where the equitability parameter k is constant. We show that 1 || maxj � i Ci,j is NP-hard in this case. This in fact also implies that the problem is strongly NP-hard, since we can assume that all processing times are bounded by a constant when k = O(1). This leads us to investigate the two natural special cases of 1 || maxj � i Ci,j where either the number of clients or the number of days are small. We begin with the case of a small number of days in Section 3. For this setting, we have the following results: • 1 || maxj � i Ci,j is weakly NP-hard even if the number m of days is four. This naturally leads to the question whether we can solve the problem in polynomial time when the number m of days is smaller than four; this is partially answered by our next result. • 1 || maxj � i Ci,j is polynomial-time solvable when m = 2. More speciﬁcally, we show that, surprisingly, a very similar approach to solving the classical F2 || Cmax problem [24]2 can be used for our problem. This leaves the case of m = 3 open. • 1 || maxj � i Ci,j is ﬁxed-parameter tractable (FPT) when parameterized by m + k. We show this using a so-called n-fold IP-formulation [6, 15] of our problem. Note that our previously mentioned hardness results rule out FPT algorithms when taking either m or k as single parameters. • 1 || maxj � i Ci,j is W[1]-hard when parameterized by m, even if all processing times are encoded in unary. This implies that a pseudo-polynomial time algorithm for the special case of m = O(1) must have (assuming FPT ̸= W[1]) a running time where the degree of the polynomial depends on m. However, we leave open the question whether such an algorithm exists. • 1 || maxj � i Ci,j is FPT when parameterized by n plus the number of diﬀerent processing times p#. This result is obtained by using an ILP-formulation of our problem with a bounded number of variables [21]. Note that our previously mentioned hardness results rule out FPT algorithms for either parameter n or parameter p# (since we can assume w.l.o.g. that p# ≤ k). • 1 || maxj � i Ci,j admits a pseudo-polynomial algorithm when n = O(1). We show this via dynamic programming. Finally, the paper is concluded in Section 5, where we provide an overview of immediate open questions and future research directions. Related work. Our equitable scheduling problem can be seen as inspired by the classical scheduling problem of minimizing the total completion time, i.e. the 1 || � Cj problem. It is wellknown that even the weighted version of the problem 1 || � wjCj can be solved in O(n log n) time by processing the jobs in a non-decreasing order of their processing-time-over-weight ratio (that is, according to the weighted shortest processing time (WSPT) rule) [26]. Adding precedence constraints to 1 || � wjCj turns the problem NP-hard [20], although there are restricted cases of precedence constraints which allow for polynomial-time solvability [22, 23, 25]. It is important to note, however, that our problem 1 || maxj � i Ci,j does not generalize 1 || � Cj: On a single day, our problem simpliﬁes to 1 || maxj Cj. Note that the maximum completion time among all clients is just the sum of all processing times, hence 1 || maxj � i Ci,j is trivial. Similarly, 1 || maxj � i Ci,j becomes trivial when n = 1.
Quantum harmonic oscillator with superoscillating initial datum<|sep|>Superoscillating phenomena have been known in physics for a long time [1, 10, 11, 12, 13, 14] and recently they have also been studied from the mathematical point of view [2, 3, 4, 5, 6, 7, 8, 9]. An interesting question concerning superoscillating functions in quantum mechanics is to ask whether the superoscillatory behavior persists when these functions evolve in time according to the Schr¨odinger equation. The prototype of a sequence of superoscillating functions that is of interest for this problem is {Fn(x)}∞ n=1, where �n k � is the binomial coeﬃcient, p ∈ R, p ̸= 0 and a > 1. Taking the limit n → ∞, we ﬁnd that Fn(x) → eiapx/ℏ pointwise on R but uniformly only on compact sets in R. The lack of uniform convergence is responsible for many subtle properties of superoscillations. It has been shown that the Cauchy problem (the initial value problem for the timedependent Schr¨odinger equation) for a quantum-mechanical free particle in one spacial ∗Schmid College of Science and Technology, Chapman University, Orange, CA 92866, USA †Politecnico di Milano, Dipartimento di Matematica, Via E. Bonardi 9, 20133 Milano, Italy where the convergence is uniform on compact sets. This means that the superoscillatory phenomenon for the functions {ψn(t, x)}∞ n=1 persists for all times t ∈ [−T, T] for any T > 0. A crucial fact in the proof of this result (see [5]) is that ψn(t, x) can be written as dz � acts continuously on a class of holomorphic functions with certain growth conditions that contains the analytic extension {Fn(z)} of the sequence {Fn(x)}. Thus, by restricting both to R, we have The recently developed [7, 8] mathematical strategy to generate superoscillatory functions consists in explicitly solving the Cauchy problem for suitable convolution equations with superoscillating initial datum. This works well in the case when explicit solutions for Green’s functions and propagators are known. Here we solve the Cauchy problem for the quantum driven harmonic oscillator and ﬁnd that the superoscillations are ampliﬁed by the potential and that the analytic solution develops a singularity in ﬁnite time. Moreover, even for a ∈ (0, 1) the harmonic oscillator displays a superoscillatory phenomenon since the solution contains the term that increases arbitrarily with t (up to the time of singularity). This phenomenon does not occur for the free particle. The physical interest in the quantum driven harmonic oscillator lies in its connection to the semiclassical (WKB) approximation in the path integral formulation of quantum mechanics [17, 18, 21]. It turns out that the leading behavior of the propagator for any quantum-mechanical system when ℏ → 0 is obtained by a series expansion of the corresponding action functional, which, without loss of generality, is given by the action for the driven harmonic oscillator. Thus our results about the superoscillatory behavior of such an oscillator directly apply to a much broader problem of establishing presence of superoscillations in any quantum system (at least in the semiclassical limit).
Supersymmetric Chern-Simons Theory in Presence of a Boundary<|sep|>The ABJM theory is thought to describe the world-volume of multiple M2branes in M-theory at low energies [1, 2]. It is a three dimensional ChernSimons-matter theory with gauge group U(N)k × U(N)−k at levels k and −k on the world-volume of N M2-branes placed at the ﬁxed point of R8/Zk. Although this construction explicitly realises only N = 6 supersymmetry, the supersymmetry is expected to be enhanced to full N = 8 supersymmetry for k = 1, 2 [3]. The ABJM theory coincides with the Bagger-Lambert-Gustavsson (BLG) action [4, 5, 6, 7], based on the Basu-Harvey equation [8], for the only known example of the Lie 3-algebra. The BLG model has been analysed in the N = 1 superﬁeld formalism [9]. First, an octonionic self-dual tensor is used to construction a real super-potential with manifest SO(7) invariance. Then for specially chosen couplings, the component action coincides with the BLG action, and hence the full SO(8) symmetry is restored. After reduction using the novel Higgs mechanism [10], Higherderivative corrections to super-Yang-Mills on D2-branes were analysed in the N = 1 superspace formalism [11]. Chern-Simons theory with N = 1 supersymmetry has also been studied in relation to axion gauge symmetry which occurs in supergravity theories arising from ﬂux compactiﬁcations of superstrings and Scherk-Schwarz generalised dimensional reduction in M-theory [12]. The ABJM and BLG actions are formulated for M2-branes without a boundary. However, it is of interest to allow the inclusion of a boundary. Such boundaries correspond to M2-branes ending on other objects in M-theory. In [13] appropriate boundary conditions were derived for the ABJM and BLG actions, describing M2-branes ending on M5-branes, M9-branes or gravitational waves. Boundary conditions in the presence of background ﬂux were derived in [14]. The M5-brane is of particular interest, and certainly one motivation for studying open M2-branes is to learn about the physics of the M5-brane. For example, by considering a system of M2-branes ending on an M5-brane with a constant C-ﬁeld turned on, the BLG model was used to motivate a novel quantum geometry on the M5-brane world-volume [15]. Another interesting relation between multiple M2-branes and the M5-brane is the identiﬁcation of the BLG action (with Nambu-Poisson 3-bracket) as the M5-brane action with a large worldvolume C-ﬁeld, as reviewed in [16]. While these results involve a model for multiple M2-branes, we note that earlier work using the action for single open M2-branes suggested a form of non-commutative string theory on the M5-brane worldvolume [17, 18, 19]. It would be interesting to understand how these results arising from diﬀerent approaches are related. One of our motivations is to make further progress towards a superspace description of the ABJM action with a boundary. Rather than specifying boundary conditions as in [13, 14], the idea here is to add additional boundary terms and degrees of freedom to make the action consistent. The prescription is motivated by the symmetries of the bulk action. In particular, we follow the general prescription given in [20] to add boundary terms so that half the bulk supersymmetry is preserved. This procedure has been applied to supersymmetric Abelian Chern-Simons theories in [20] and particularly to various models including Chern-Simons matter theories and the ABJM model in [21]. However, in addition to supersymmetry, it is necessary to consider preservation of gauge symmetry. This issue was considered, with the aim of describing the physics of multiple self-dual strings in [22]. In doing so bosonic Chern-Simons theory on a manifold with a boundary was analysed. It was found that even though the Chern-Simons theory was not gauge invariant by itself in the presence of a boundary, the sum of it with a Wess-Zumino-Witten model living on the boundary was gauge invariant. Thus, new degrees of freedom were identiﬁed on the boundary and these degrees of freedom generated a U(2N)×U(2N) Kac-Moody current algebra. While it is possible to introduce the fermionic sector and derive a supersymmetric action in component form, it seems somewhat natural to derive a manifestly supersymmetric gauge invariant action, in some sense combining the results of [21] and [22]. This will be the result of section 4, although for simplicity we limit ourselves to N = 1 superspace and don’t address the issue of a background C-ﬁeld as there has been limited progress in extending the ABJM action to include coupling to a general C-ﬁeld [23, 24, 25]. Because the issue of preservation of gauge symmetry is speciﬁc to the Chern-Simons term, this is considered separately in section 3. While there is a well-known connection between (2 + 1)-dimensional (topological) Chern-Simons theories and (1+1)-dimensional CFTs [26], the situation is less clear for Chern-Simons matter theories. As shown in [27, 28] for pure Chern-Simons theory with suitable boundary conditions, a component of the gauge ﬁeld, say A0, appears linearly in the action and so can be integrated out, imposing the constraint F12 = 0. This constraint can be solved explicitly (e.g. for a manifold of the form of a disk for each constant time slice) and the result is a (1 + 1)-dimensional WZW model where the bulk gauge potential has been replaced by the boundary gauge degrees of freedom. Now Chern-Simons matter theories are not topological so we should not expect such a connection to (1 + 1)-dimensional CFTs. Of course, in cases such as ABJM theory where the Chern-Simons matter theory is conformal, the boundary theory may still be conformal. However, an important diﬀerence to the pure Chern-Simons case is that due to the gauged scalar kinetic terms, A0 will no longer appear as a Lagrange multiplier – even the classical equation of motion will couple F12 to the scalars rather than simply requiring F12 = 0. We therefore cannot expect the Chern-Simons action to be replaced by a WZW model in general. However, it is possible to use the principle of gauge invariance in the presence of a boundary to couple the Chern-Simons theory to a boundary theory. The general result is a gauge invariant action coupling the Chern-Simons gauge potential to a boundary WZW model, and which reproduces the pure WZW action when starting from a pure Chern-Simon action [22]. Supersymmetric Chern-Simons theories have also been studied as interesting examples of the AdS4/CFT3 correspondence [29, 30, 31, 32, 33]. Three dimensional N = 1 super-conformal ﬁeld theories have the property of being supersymmetric without having any holomorphic property. This is a peculiarity of the AdS4/CFT3 correspondence with respect to the usual AdS5/CFT4. Thus, the results of this paper may be useful in analysing certain aspects of the AdS4/CFT3 correspondence. We need to ﬁx a gauge before we can quantize any theory which has a gauge symmetry associated with it. This is done by the addition of a gauge ﬁxing term and a ghost term to the original action. The action thus obtained is invariant under two new symmetries called the BRST symmetry [34, 35] and the antiBRST symmetry [36]. These symmetries are important to show the unitarity of the S-matrix and thus the consistency of the theory at quantum level [37]. The BRST symmetry of the bosonic Chern-Simons theory has been thoroughly investigated [38, 39] and the BRST symmetry of the N = 1 Chern-Simons theory has been analyzed in the superspace formalism [40, 41]. The BRST and the anti-BRST symmetries of the ABJM theory have also been studied [42]. In this paper we will analyse the BRST and the anti-BRST symmetries of the ABJM theory in presence of a boundary.
Water evaporation from solute-containing aerosol droplets: effects of internal concentration and diffusivity profiles and onset of crust formation<|sep|>Evaporation from solute-containing droplets is an important process in many industrial and  scientific  applications ranging from pharmacology, agriculture, food and cosmetics production  to medical, biochemical, material, and soil sciences [7-12]. In addition, it plays an important  role in the transmission of infectious diseases and respiratory viruses via the airborne route [6,  13-16], which is the main focus of the present study. Water evaporation from a saliva droplet  containing non-volatile solutes might eventually produce a droplet nucleus, which is a small  light particle at a minimal moisture level that stays floating in air for a long time [17]. The  creation of such droplet nuclei can significantly influence the infection risk from viruscontaining respiratory droplets, especially in the indoor environment [14, 17], by increasing  the sedimentation time of droplets. It is, therefore, of utmost importance to investigate how  respiratory droplets dry out and how their evaporation kinetics is affected by different factors  such as the droplet composition and its initial size, the ambient relative humidity, temperature,  and the surrounding air flow. Although there are many studies [11, 18-21] concerning the drying process of droplets placed  on solid surfaces, also known as sessile droplets, fewer investigations have been conducted on  free droplets surrounded by air, which is partly due to a lack of experimental data. Indeed,  designing and conducting experiments on free droplets faces many challenges, mostly since  such droplets are ephemeral and difficult to follow [8]. Therefore, analytical and numerical  modelling methods must be used to better understand the details of the evaporation mechanism  that is relevant for sedimenting droplets. Although modelling of free droplets does not face  complexities due to substrate-droplet interactions that control the droplet shape at the  solid/liquid/air interface [22, 23] and Marangoni flow [24, 25],  which occur in the case of  sessile droplets, other factors such as evaporation-induced concentration gradients inside the  droplet [26] and the possibility of crust formation on the droplet surface [27], which are  consequences of the increasing solute surface concentration during evaporation [28], cause  difficulties in modelling free droplets in the presence of solutes. In addition, physical and  chemical properties of the drying droplets, such as the internal viscosity [29, 30], the diffusivity  of solvent and solutes in the liquid phase [30], and the activity coefficient of solvent [31], are  dependent on the local concentration of solutes (and, consequently, on both position and time),  which makes the problem even more complex. An important challenge in modelling  evaporating droplets is, therefore, to account for the concentration gradients created and  developed within the droplets during the solvent evaporation process. There are a few studies that address concentration gradients inside a drying droplet containing  solutes [26, 32-34] and propose analytical and numerical methods for modelling the  evaporation process before [33-36] and after [27, 35, 37] crust formation. Also, internal  concentration gradients have been experimentally captured for drying respiratory fluids  suspended on superhydrophobic substrates using optical and fluorescence microscopy [38].  Such concentration gradients are found to affect the evaporation process not only by decreasing  the evaporation rate [34], but also by influencing the morphological evolution of the drying  droplet [27, 39], which is a determining factor in the shape of the droplet nuclei produced at  the end of the evaporation process. An important physical parameter is the ratio of the evaporation rate to the diffusive transport rate of solute particles inside the droplet [40]. When  the evaporation process is very slow or, alternatively, the internal diffusion is very fast, the  solute particles have enough time to redistribute during the drying process. In such case, the  solute particles remain evenly distributed within the droplet to form a full solid particle at the  end of the drying process (Fig.1, scenario a). In the case of fast evaporation, however, the  solvent evaporation from the droplet surface increases the solute concentration at the surface  and creates a concentration gradient in the droplet (see Fig. 1, scenarios b-f). If the evaporation  rate is not high enough to increase the solute concentration at the droplet surface up to its  solubility limit, the concentration gradient gradually disappears as the evaporation proceeds  (due to the decreased evaporation rate) and a full solid particle remains at the end of the drying  process (Fig.1, scenario b). When the surface becomes highly populated with solutes, however,  two-phase coexistence occurs and the solute particles at the surface form a solid crust (see Fig.  1, scenarios c-e), which is expected to mainly affect the drying mechanism and to considerably  slow down the evaporation process [39]. Depending on the type of solutes, the forming crust might be either a dry crust (expected for  salty droplets) [8, 41] or a gel-like wet skin consisting of polymers or proteins and other  suspended particles [42, 43]. In the presence of a dry crust, the liquid from the internal liquid  core reaches the crust surface through capillary action within the crust pores [39, 44]. As the  evaporation proceeds, the crust will become thicker and the crust pores will lengthen, which  increases the resistance to heat and mass transfer and decreases the evaporation rate. In the case  of a wet crust, water evaporation continues via diffusion through the gel crust [42]. Since the  diffusion coefficient and the concentration gradient within the gel phase are both much lower  than in solution, the evaporation rate is expected to fall after the formation of a wet crust as  well. Regardless of the crust type, crust formation is expected to determine the final  morphology of the droplet by producing a hollow structure, with a size larger than expected in  the absence of crust. Depending on the type of the solute particles within the droplet, the hollow  structure produced at the end of the evaporation process may or may not contain small  agglomerates of solutes [27] (Fig.1, scenarios c and d). Also, a crust collapse might occur when  the crust cannot withstand the pressure difference caused by the continued solvent  evaporation[39] (Fig.1, scenario e). Another scenario accounts for non-uniform drying  conditions due to amplification of concentration inhomogeneities over the droplet surface,  which is probable when the solvent evaporation is very fast. In such case, an irregularly shaped  dry particle, also known as wrinkled particle, will form at the end of the drying process [39]  (Fig.1, scenario f). In this paper, a droplet evaporation model is presented that accounts for effects of non-volatile  solutes on the drying process. We address evaporation-induced water concentration gradients  inside the droplet, the dependence of the internal diffusivity on the solute concentration, the  water vapor-pressure reduction in the presence of solutes, and the effect of solutes on the  evaporation cooling. We also include the possibility of crust formation in our calculations to  evaluate its effect on the droplet size. All our calculations are done in the one-phase regime,  meaning from the beginning of the evaporation process up to the point where crust formation  sets in at the surface. We find that the presence of non-volatile solutes slows down the droplet  evaporation process due to the water vapor-pressure reduction and the presence of solute induced water concentration gradients inside the droplet. The effect of solutes on the internal  water diffusivity is found to play only a minor role in determining the droplet evaporation time,  and can therefore be neglected. Crust formation is suggested to affect the morphology of the  final droplet nuclei by producing a hollow particle with a size larger than expected in the  absence of crust. Figure 1 Schematic illustration of possible scenarios for the morphological evolution of a solutecontaining droplet during the drying process. When the solvent evaporation is very slow (or the internal  particle diffusion inside the droplet is sufficiently rapid), the solute particles have enough time to  redistribute in the liquid phase and thus the internal solute concentration remains uniform throughout  the evaporation process (except the possibility of two-phase coexistence when the solute-solubility limit  is reached). In such case, a solid particle will be produced at the end of the drying process (scenario a).  In the case of fast solvent evaporation, however, the shrinkage rate of the droplet radius overcomes the  diffusion rate of the solutes in the solution, which leads to an increased solute concentration at the  droplet surface. In this case, depending on the type of the solutes and their solubility limit in the solvent,  the dry particle produced at the end of the evaportion process might be a solid particle (scenario b) or a  hollow particle that may (scenario c) or may not (scenario d) contain small agglomerates of solutes [27].  Alternatively, a collapsed dry particle (scenario e) will be produced when the crust formed at the droplet  surface cannot withstand the pressure difference caused by the continued internal solvent evaporation  [39]. Very fast solvent evaporation can lead to non-uniform drying conditions due to concentration  inhomogeneities of the droplet [39]. In such case, an irregularly shaped dry particle, also known as  wrinkled particle, will be formed at the end of the drying process (scenario f).    2. Results and discussion The main objective of the present study is to model water evaporation from a solute-containing  droplet that is sedimenting in air. For later comparison with our main results, we first assume  that the particle diffusion inside the droplet is sufficiently rapid, so that the water concentration  at the droplet surface does not differ significantly from the mean water concentration in the droplet. Also, the effect of solute concentration on evaporation cooling is neglected at first. The  radius-dependent evaporation time ????(????), which is the time it takes for the droplet radius to  decrease from its initial value ????0 to ????, can be approximated from the simultaneous solution of  the water concentration and energy diffusion equations as (see reference [6] and Appendix A) where ????0 and ???????????? are the initial and the evaporation-equilibrium radii of the droplet,  respectively, ???????? denotes the relative humidity, and γ is the water activity coefficient  accounting for non-ideal effects due to water-solute interactions (more details on γ are provided  in Appendix B.1). The prefactor ???? in Eq. 1 is given by where ???????? is the water diffusion constant in air, ???????? is the saturated water vapor concentration,  and ???????? is the water molecular volume in the liquid phase. As discussed in Appendix A, the coefficients ???????? and ???????? describe the reduction of the water vapor concentration at the droplet  surface due to the temperature depression (see Eq. A6) and the dependence of the temperature  depression at the droplet surface on the relative humidity and the momentary solute volume  fraction (see Eqs. A9 and A10), respectively. For a water droplet at 25°C, the coefficients ???????? factor is equal to ∼ 0.36, which indicates that evaporation cooling considerably increases the  droplet evaporation time. It should be noted that the evaporation-cooling factor θ in Eq. 2  neglects the effect of non-volatile solutes (which will be taken into account later), meaning that  this factor is derived for a pure water droplet, to make the governing equations analytically  solvable (see Appendix A). An approximate expression for the droplet evaporation time follows by setting ???? = ???????????? in Eq.  1, with ???????????? being the evaporation-equilibrium droplet radius given by (see Eq. A14) Here, Φ0 denotes the initial volume fraction of solutes. Neglecting the logarithmic term in  equation 1, which reflects the osmotic slowing down of the evaporation due to the water vapor As mentioned above, equation 4 is derived neglecting the water vapor-pressure reduction  during the evaporation process as well as the concentration dependence of evaporation cooling.  The importance of these factors is evaluated next. To account for the solute-induced water vapor-pressure reduction, the logarithmic term in  equation 1 is now included. Since near the evaporation equilibrium, the droplet radius varies  very slowly with time, as follows from the logarithmic term in Eq. 1 (see Fig. 2a), we can no  longer use the mentioned definition for evaporation time (i.e., the time at which ???? = ????????????) that  leads to Eq. 4. Instead, we define the evaporation time as the time at which the equilibrium  droplet radius ???????????? is 99% of the droplet radius ???? From Eq. 1 and considering the definition provided by Eq. 6, the evaporation time in the  presence of solutes follows as The logarithmic term ???????????? ???????????? in Eq. 7 reflects the increase in the evaporation time due to the soluteinduced water vapor-pressure reduction Figure 2b shows the evaporation time obtained from equation 7 along with its logarithmic and  non-logarithmic parts as a function of the initial volume fraction of solutes Φ0. This figure indicates that at very low values of Φ0, ???????????? ???????????? is negligible compared to ???????????? ????????????−????????????, meaning that  equation 4 is quite accurate. An increase in Φ0, however, intensifies the effect of the solute induced water vapor-pressure reduction and thus increases ???????????? ????????????, while nevertheless ???????????? ????????????−????????????  decreases with Φ0 due to the increased equilibrium radius of the droplet (see equations 3 and  4). In the case of droplets with high initial volume fractions of solutes Φ0, therefore, the  logarithmic terms in equations 1 and 7 are important and can no longer be neglected. By equating equations 4 and 8, one finds that the effect of the water vapor-pressure reduction  becomes dominant when Φ0 becomes higher than ~0.13(1 − ????????/????). Figure 2b also shows that an increase in Φ0 causes a non-monotonic variation in the  evaporation time, which cannot be observed when the water vapor-pressure reduction is  neglected. By differentiating equation 7 with respect to Φ0, the initial volume fraction of  solutes at which the droplet experiences the maximum evaporation time can be estimated as Figure 2 Effect of the solute-induced water vapor-pressure reduction on the drying process in the limit  of an infinitely large internal water diffusion constant (the solute-concentration dependence of  evaporation cooling is neglected here). (a) Variation of the droplet radius ???? with time ???? for a droplet  with initial radius of ????0 = 50 ???????? for initial solute volume fraction Φ0 = 0.01 and ???????? = 0.5. The  green line shows the results obtained from numerical solution of the differential equation A15, which  is derived from the mass conservation of the droplet (see Appendix A). Red and orange lines indicate  the results estimated from Eq. 1, which is the approximate analytical solution of Eq. A15, with and  without considering the logarithmic term that reflects the water vapor-pressure reduction. (b)  Evaporation time ???????????? ???????????? as a function of the initial volume fraction of solutes Φ0. Solid lines show the  evaporation times calculated from equations A15 and 7 at ???????? = 0.5 and broken lines indicate the nonlogarithmic and the logarithmic contributions to the evaporation time, which are obtained from  equations 4 and 8, respectively.    Effect of non-volatile solutes on evaporation cooling The evaporation-cooling factor 1/(1 + ????????????????????) in Eq. 2 is obtained for a pure water droplet.  The presence of non-volatile solutes affects this term by decreasing the saturated water vapor Equation 10 reveals that not only Δ???? is linearly related to the relative humidity, as observed in  the absence of solutes [6], but it also depends on the momentary volume fraction of solutes  Φ0(????0 3/????3), which itself is time dependent. This equation clearly shows that at evaporation  equilibrium, i.e., when the momentary volume fraction of solutes reaches 1 − ????????/???? and thus  the evaporation flux vanishes (see Eq. A11), Δ???? goes to zero, meaning that the droplet reaches  thermal equilibrium with its environment as well (see Fig. 3a). Considering the concentration-dependent temperature depression at the droplet surface, which  is given by Eq. 10, one can rewrite the prefactor ???? in Eq. 2 in the presence of solutes as (see  Eq. A21) and repeat the calculations to rederive the evaporation time. As detailed in Appendix A, the  evaporation time is determined by the condition of total mass conservation of the droplet, which  gives rise to the differential equation A13 that is not analytically solvable. Instead, this equation  is numerically solved to yield ????(????). Figure 3b shows the time-dependent droplet radii  calculated from equations A13 and A15, which are derived with and without considering the  concentration dependence of the evaporation-cooling effect, respectively. This figure indicates  that the solute-induced reduction of ∆???? slightly speeds up the evaporation process. As  expected, this speed up becomes more significant as the evaporation proceeds because of the  increased solute concentration within the droplet. Also, the solute effect on evaporation cooling  is found more pronounced for droplets with higher initial solute volume fractions (see Fig. 3b  and its inset). Using the definition provided by Eq. 6, the evaporation time is calculated from numerical  solution of equations A13 and A15 (i.e., with and without considering the effect of solutes on  ∆????). The results are shown by solid and broken lines in Fig. 3c. This figure also shows the  results obtained from equation 7 (dotted lines), which is an approximate analytical solution of  equation A15 (see Appendix A). This figure clearly shows that the solute-concentration  dependence of ∆???? causes a decrease in the evaporation time, as discussed above. The  approximation used in derivation of equation 7 (see Eq. A19) is, however, found to partly  compensate the difference caused by neglecting such concentration dependence, especially at  high relative humidities. According to this figure, when the initial volume fraction of solutes  is equal to or less than Φ = 0.01, which is reported as a normal volume fraction of solutes in  saliva droplets [45], one can safely use equation 7 to calculate ????????????. At higher values of Φ0,  however, this approximation starts to become inaccurate. Figure 3c also shows that the initial  volume fraction of solutes at which the droplet exhibits a maximum in the evaporation time  decreases with ????????, as can be seen from equation 9. Figure 3 Effect of non-volatile solutes on evaporation cooling and, consequently, on the evaporation  time (the results are shown in the limit of infinitely high internal water diffusivity). (a) Evaporationinduced temperature reduction at the droplet surface, Δ????, as a function of the droplet radius ????. The  results are obtained from Eq. 10 for fixed relative humidity of ???????? = 0.5 and different initial solute  volume fractions Φ0. The dotted line indicates the results neglecting the solute-concentration  dependence of evaporation cooling, i.e., for Φ0 = 0. (b) Variation of the droplet radius ???? with time ????.  The results are shown for ???????? = 0.5 and for two different initial solute volume fractions Φ0 = 0.1  (shown in the main figure) and Φ0 = 0.01 (shown in the inset). Solid lines indicate the results from  numerical solution of Eq. A13, which account for the solute-concentration dependence of evaporation  cooling, and broken lines show the results from numerical solution of Eq. A15 that is derived neglecting  such solute-concentration dependence. (c) Evaporation time ???????????? ???????????? as a function of initial solute volume  fraction Φ0. Solid and broken lines indicate the results obtained from numerical solution of equations  A13 and A15, respectively, and dotted lines show the Eq. 7 (which results from the approximate  analytical solution of Eq. A15). In the previous section, the particle diffusivity inside the droplet was assumed to be sufficiently  rapid, so that the solute particles remain evenly distributed throughout the liquid phase. In the  fast evaporation scenario (see Fig. 1), however, the droplet experiences an increased solute  concentration at the droplet surface. To account for the resulting water concentration gradient  in the liquid phase, one needs to solve the diffusion equation not only outside, but also inside  the droplet Here, ????????(????, ????) is the liquid water concentration profile inside the droplet and ???????? ???? [????????(????, ????)] is the  water diffusion coefficient in liquid water, which in general depends on the water concentration  and, consequently, on time and position. Equation 12 has a singularity at the center of the  droplet, which is preempted because in the model used here, the droplet volume is divided into  two regions: an internal core, where the concentration profile remains uniform, and an outer  shell, where the liquid phase experiences a concentration gradient (see Fig. 4). To calculate the  water concentration profile inside the droplet, therefore, it is sufficient to solve the diffusion  equation (Eq. 12) in the outer shell. Here, we consider a two-stage evaporation model. In the  first stage (see Fig. 4a), the outer shell grows towards the droplet core. In this stage, the radius  of the internal core ???????? shrinks with time while the water concentration in the internal core ????????  remains equal to its initial value. When ???????? reaches a cut off value ???????? ° (which is of molecular  size), the drying process turns into the second stage (Fig. 4b). In this stage, the internal core  radius ???????? is fixed to ???????? ° while ???????? is considered as a time-dependent parameter (which is expected  to reach the equilibrium concentration at the end of the second drying stage). Figure 4 The model used to account for the evaporation-induced water concentration gradient at the  droplet surface. Here, the water concentration profile within the internal core of radius ????????(????) (the bright  area) is assumed to remain uniform throughout the drying process ???????? = ???????? while the outer shell of radius  ????(????) (the dark area) exhibits a water concentration gradient ???????? = ????????(????). This model leads to two drying  stages: (a) in the first stage, the internal concentration ???????? is kept constant while the radius of the internal  core ???????? varies with time, (b) when the radius of the internal core reaches a very small value of ???????? ° (which  is of molecular size) the drying process turns into the second stage, where ???????? is time-dependent and ????????  is fixed to ???????? °.    We first assume that the internal water diffusion coefficient ???????? ????  is independent of solute  concentration (concentration dependence of ???????? ????  is discussed next). Using the two-stage model  described above, the liquid water density profiles in the first drying stage ???????? 1????????(????) and in the  second drying stage ???????? 2????????(????) are given as (the calculations are detailed in Appendix B) ???? and ???????? in the above equations are time dependent and ???? in equation 14 is a function of ????,  which is given by The numerical prefactor ???? in equations 13-15 is given by ???? = ???????? ???? /(????????????????????????) and describes the  ratio of internal and external water diffusivities. Considering the values listed in table 2, this  prefactor is approximately equal to ???? ≈ 4 at 25°C. Using the assumption of infinitely rapid  diffusion in the droplet, ???? goes to infinity. In the present study, the calculations are done for  different values of ???? to investigate the effect of the internal water diffusion constant on the  water evaporation process. Neglecting the probability of crust formation due to the increased solute concentration at the  droplet surface (which is discussed later), one can calculate the evaporation time as where ????1???????? and ????2???????? are the times of the first and the second drying stages, respectively. In the  present study, ????1???????? and ????2???????? are obtained from equations B32 and B39, using the numerical  approaches elaborated in Appendix B. In our calculations, ???????? ° (i.e., the radius of the internal  core in the second drying stage) is set to 1 ????????, which is small enough so that the calculated  evaporation time is independent of ???????? ° (as demonstrated in Fig. 5a). Figure 5b shows the  resulting evaporation time according to Eq. 16 as a function of the internal water diffusion  constant for a droplet with an initial radius of ????0 = 50 ????????. This figure demonstrates that an  increase in ???????? ????  decreases the evaporation time, as expected. At high internal diffusion  coefficients, however, ???????????? becomes independent of ???????? ???? . Figure 5b also confirms that for Φ0 = 0.01, equation 7 accurately predicts the evaporation time in the limit of ???????? ???? → ∞, as discussed  in the previous section. The dotted vertical line in Fig. 5b indicates the value reported [2] for the water diffusion  coefficient in pure water at 25°C ???????? ????,25°C = 2.3 × 10−9 ????2/???? (see table 2). Neglecting the  concentration dependence of the diffusion coefficient, this value can be used to estimate the  evaporation time for a solute-containing water droplet at 25°C. Figure 5c shows the resulting  evaporation times together with those calculated for ???????? ???? → ∞, i.e., the results obtained  assuming that the internal concentration gradients are infinitely small. According to this figure,  although the effect of internal concentration gradients is negligible at low initial volume  fractions of solutes, neglecting this effect can make a relatively large error at higher values of  Φ0, especially when Φ0 becomes close to Φ0 ????????????,???????????? (see Eq. 9). The relative error is found to be  more significant in dry environments because the drier the air, the faster the water evaporates  from the droplet surface and the larger the concentration gradient is. As mentioned before, the differential equations B32 and B39 that are used here to calculate the  evaporation time are not analytically solvable and, hence, the evaporation time is calculated  using numerical methods. To provide an equation for estimating ???????????? as a function of the  relevant parameters considering all the factors that have been discussed so far (i.e., soluteinduced water vapor-pressure reduction, concentration dependence of the evaporation-cooling  effect, and the evaporation-induced concentration gradients inside the droplet), we construct a  heuristic fit function inspired by equation 7
Knowledge Association with Hyperbolic Knowledge Graph Embeddings<|sep|>Knowledge graphs (KGs) have emerged as the driving force of many NLP applications, e.g., KBQA (Hixon et al., 2015), dialogue generation (Moon et al., 2019) and narrative prediction (Chen et al., 2019). Different KGs are usually extracted from separate data sources or contributed by people with different expertise. Therefore, it is natural for these KGs to constitute complementary knowledge of the world that can be expressed in different languages, structures and levels of speciﬁcity (Lehmann et al., 2015; Speer et al., 2017). Associating multiple KGs via entity alignment (Chen et al., 2017) or type inference (Hao et al., 2019) particularly provides downstream applications with more comprehensive knowledge representations. Entity alignment and type inference seek to ﬁnd two kinds of knowledge associations, i.e., sameAs and instanceOf, respectively. An example showing such associations is given in Figure 1. Speciﬁcally, entity alignment is to ﬁnd equivalent entities from different entity-level KGs, such as United States in DBpedia and United States of America in Wikidata. Type inference, on the other hand, associates a speciﬁc entity with a concept describing its type information, such as United States and Country. The main difference lies in whether such knowledge associations express the same level of speciﬁcity or not. Challenged by the diverse schemata, relational structures and granularities of knowledge representations in different KGs (Nikolov et al., 2009), traditional symbolic methods usually fall short of supporting heterogeneous knowledge association (Suchanek et al., 2011; Lacoste-Julien et al., 2013; Paulheim and Bizer, 2013). Recently, increasing efforts have been put into exploring embeddingbased methods (Chen et al., 2017; Trivedi et al., 2018; Jin et al., 2019). Such methods capture the associations of entities or concepts in a vector space, which can help overcome the symbolic and schematic heterogeneity (Sun et al., 2017). Embedding-based knowledge association methods still face challenges in the following aspects. (i) Hierarchical structures. A KG usually consists of many local hierarchical structures (Hu et al., 2015). Besides, a KG also usually comes with an ontology to manage the relations (e.g., subClassOf) of concepts (Hao et al., 2019), which typically forms hierarchical structures as illustrated in Figure 1. It is particularly difﬁcult to preserve such hierarchical structures in a linear embedding space (Nickel et al., 2014). (ii) High parameter complexity. To enhance the expressiveness of KG embeddings, many methods require high embedding dimensions, which inevitably cause excessive memory consumption and intractable parameter complexity. For example, for the entity alignment method GCN-Align (Wang et al., 2018), the embedding dimension is selected to be as large as 1, 000. Reducing the dimensions can effectively decrease memory cost and training time. (iii) Different scales. The KGs that we manipulate may differ in scales. For example, while the English DBpedia contains 4, 233, 000 entities, its ontology only contains less than a thousand concepts. Capturing the associations between entities and concepts has to deal with drastically different scales of structures and search spaces, while most existing methods do not consider such difference. To tackle these challenges, we propose a novel hyperbolic knowledge association method, namely HyperKA, inspired by the recent success of hyperbolic representation learning (Nickel and Kiela, 2017; Dhingra et al., 2018; Tifrea et al., 2019). Unlike the Euclidean circle circumference that grows linearly w.r.t. the radius, the hyperbolic space grows exponentially with the radius. This property makes the hyperbolic geometry particularly suitable for embedding the hierarchical structures that drastically span their sizes along with their levels. It is also capable of achieving superior expressiveness at a low dimension. To leverage such merit, HyperKA employs a hyperbolic relational graph neural network (GNN) for KG embedding and captures multi-granular knowledge associations with a hyperbolic transformation between embedding spaces. For each KG, HyperKA ﬁrst incorporates hyperbolic translational embeddings at the input layer of the GNN. Then, several hyperbolic graph convolution layers are stacked over the inputs to aggregate neighborhood information and obtain the ﬁnal embeddings of entities or concepts. On top of the KG embeddings, a hyperbolic transformation is jointly trained to capture the associations. We conduct extensive experiments on entity alignment and type inference. HyperKA outperforms SOTA methods on both tasks at a moderate dimension (e.g., 50 or 75). Even with a small dimension (e.g., 10), our method still shows competitive performance.
A Penalty Approach for Normalizing Feature Distributions to Build Confounder-Free Models<|sep|>Modern machine learning approaches rely on automatically learning features from data [28] using approaches such as convolutional neural networks (CNNs) [2,8] and attention-based transformer models [6,9]. Although these methods solve challenging problems, they are known to capture spurious associations and biases introduced by confounding or protected variables [27]. These limitations conﬁne the neuroscientiﬁc impact of these algorithms, in which controlling for (and explaining the eﬀects of) confounding variables is crucial. To remedy this, several approaches have been proposed, such as based on adversarial training [27,15], counterfactual generative models [19,13], disentanglement [22,16], and
Continuing EVN monitoring of HST-1 in the jet of M87<|sep|>Understanding of the formation of Active-Galactic-Nuclei (AGN) jets as well as their connection to γ-ray productions up to tera-electronvolt (TeV) energy is one of the primary goals in current high-energy astrophysics. The nearby radio galaxy M87 is one of the best studied AGN jets through radio to TeV γ-ray, and especially VLBI observations are able to access the jet structure on its formation and collimation scale thanks to the proximity (1 mas = 0.08 pc = 140 Rs for D = 16.7 Mpc and MBH = 6×109M⊙), allowing us to obtain detailed observational constraints on relativistic-jet phenomena at an unprecedented linear resolution [1, 2, 3, 4, 5, 6]. At a distance of ∼120 pc or ∼4.8×105 Rs from the nucleus, the M87 jet exhibits a remarkable feature known as HST-1 [7]. Probing this feature provides a clue to the above key question on AGN jets. In 2005, a large TeV ﬂare from M87 was accompanied by the radio-to-X-ray outbursts from HST-1, subsequently with the emergence of superluminal features [8]. On the other hand, HST-1 seems to be located in a ‘transition zone’ of the M87’s jet-collimation proﬁle, where the jet shape changes from a parabolic (at r ≲ 105Rs) to a conical one (at r ≳ 106Rs), together with a possible smaller cross section on HST-1 itself [9]. These results leads to an interesting hypothesis that HST-1 marks a recollimation shocked region at the end of the acceleration and collimation zone, resulting in a signiﬁcant energy release in the form of TeV ﬂares [8, 9, 10]. However, the exact kinematics of HST-1 and its mas-scale structure remain uncertain. To better understand these properties and their possible connection to the γ-ray production, it is necessary to keep a long-term, continuous VLBI monitoring. In this context, from mid 2009 we started a detailed monitoring program of HST-1 with EVN at 5 GHz [11]. As reported in Giroletti et al. [12], this project already provided interesting ﬁdings on the HST-1 kinematics by combining the earlier (dating back to 2006) VLBA archival data at 1.7 GHz; (i) we revealed a detailed mas-scale structure and evolution, where the HST-1 complex was resolved into several compact subcomponents; (ii) we determined the individual proper motions for these components, where most of them are moving superluminally over 5 years; (iii) moreover in 2010, we discovered the ejection of a new feature from the upstream edge of HST-1, which occurred coincidentally with the large TeV ﬂare from M87. These results imply that HST-1 can be actually associated with the high-energy γ-ray production.
Flavor Cosmology: Dynamical Yukawas in the Froggatt-Nielsen Mechanism<|sep|>The origin of the highly hierarchical structure of the Standard Model Yukawa couplings remains an open question. This is the so-called ﬂavor problem. While many models and mechanisms have been advocated to explain this, the cosmological dynamics behind it has so far not been addressed. This has several interesting ramiﬁcations. One ﬁrst interesting question in this respect is to investigate the possible interplay with electroweak symmetry breaking. Can the physics responsible for settling down the present values of the CabibboKobayashi-Maskawa (CKM) matrix be related to the dynamics of electroweak symmetry breaking? There are many diﬀerent aspects of this question. One is to check whether a model in which the new physics responsible for ﬂavor arises at the TeV scale can still be compatible with all experimental constraints. This is a direction which has already been vastly explored. The next step we want to take is to investigate whether the dynamics in the early universe at electroweak scale temperatures can naturally lead to Yukawa couplings of order one before the EW phase transition. This question is of high interest for two reasons. First it can lead to a ﬁrst-order electroweak phase transition [1]. Second, it results in suﬃcient CP violation from the CKM matrix for electroweak (EW) baryogenesis [2]. As a result, the baryon asymmetry of the universe (BAU) can be generated during the electroweak phase transition (EWPT), in a process known as electroweak baryogenesis (EWBG). In the Standard Model, EWBG fails. On the one hand, CP violation is suppressed by the quark masses and is way too small [3–5]. On the other hand, the Higgs is too massive, which results in a crossover transition [6] with no departure from thermal equilibrium. Our motivation in this series of papers is to show that the interplay between Higgs and ﬂavor dynamics can naturally provide all conditions for successful EWBG. We already showed in Ref. [1] in a model independent formulation that the variation of Standard Model Yukawa couplings induced during the EW phase transition leads to drastic modiﬁcations of the phase transition. Yukawa couplings which decrease from ∼ O(1) to their present values during the EWPT can create a thermal barrier between the symmetric and broken phase minima and hence make the EWPT strongly ﬁrst-order. The origin of the BAU could therefore be closely linked to the ﬂavor puzzle and the mechanism explaining the large hierarchy in the SM fermion masses. In fact, we also show in Ref. [2] how the variation of SM Yukawa couplings during the EWPT naturally gives the correct amount of CP violation for EW baryogenesis. A very interesting aspect of this framework is that it can circumvent usual CP violation limits. One generic constraint on EW baryogenesis is that any new source of CP violation needed to explain the BAU will typically be constrained by Electric Dipole Moment (EDM) bounds or leads to observable EDMs in the near future. In our scenario of time-dependent CP-violating source, we circumvent this usual constraint.1 We are therefore interested in exploring the model building conditions which favour such a scenario of variable CKM matrix during the EW phase transition. This should be done in the various classes of models which address the ﬂavor problem. Two main classes of models of ﬂavor hierarchies are Froggatt–Nielsen models and Randall-Sundrum (RS) models. This paper is about the Froggatt–Nielsen case. The RS case is presented in [8]. In the Froggatt–Nielsen mechanism, the eﬀective Yukawa couplings of the SM fermions depend on the vacuum expectation value (VEV) of an exotic scalar ﬁeld [9]. It was pointed out by Berkooz, Nir and Volansky that the CP violation associated with the CKM matrix could be large enough to explain the BAU if the VEV of the exotic Higgs — and hence the eﬀective Yukawa couplings — were large at the start of the EWPT [10]. However this scenario was not explored further. The aim of this paper is to analyse this scenario, incorporating the dynamics of the Froggatt–Nielsen scalar ﬁeld(s). We include the physics which actually leads to the variation of the Yukawas, which we ignored in [1] and go through all experimental and cosmological constraints. The order of the paper is as follows. In section 2 we introduce the Froggatt–Nielsen mechanism. In section 3 we present two classes of models that realize the dynamics of Yukawa couplings at the EW scale. The details of each class of models are presented in Section 4 and 5. We discuss the CP violation and baryonic yield in Section 6 and conclude in Section 7. Some derivations are moved to the Appendix. In particular, experimental constraints are presented in Appendices B and C. We also argue in Appendix D that our qualitative conclusions are expected to hold in general and do not depend on the speciﬁc form of the scalar potential which we chose for illustration and simplicity.
Utility of the Weak Temperature Gradient Approximation for Earth-Like Tidally Locked Exoplanets<|sep|>In recent years the number of known exoplanets has grown from only a handful of gas giants to over a thousand planets, including several roughly Earth-sized rocky planets. There are several ongoing and planned surveys searching nearby stars for planets including small, Earth-like habitable zone planets around M dwarf stars, such as MEarth (Berta et al. 2013), APACHE (Giacobbe et al. 2012), CARMENES (Quirrenbach et al. 2012), and NGTS (Wheatley et al. 2013). This is important because M dwarfs represent 75% of the stars in the galaxy and their planets are likely to be the most easily detectable Earth analogs. Planets near the habitable zone, with temperatures moderate enough to maintain liquid water, are of particular interest because water is essential for life on Earth. It is likely that planets in the habitable zone of M dwarf stars will be in spin-orbit resonance, usually in the “tidally locked” conﬁguration with one side always facing the star (Kasting et al. 1993). Climate modeling studies have shown that a tidally locked planet with an atmosphere only 10% of the mass of Earth’s atmosphere is capable of transporting enough heat to the cold nightside to prevent atmospheric collapse by condensation there (Joshi et al. 1997; Joshi 2003; Tarter et al. 2007; Scalo et al. 2007). Understanding the characteristics of tidally locked, rocky planet atmospheres is therefore of great interest. Observational techniques have been proposed to probe the climate and structure of such planets both by photometry (e.g., Seager & Deming 2009; Cowan et al. 2012b; Cowan et al. 2012a; Fujii & Kawahara 2012) and low resolution spectroscopy (e.g., Beichman 1998; Fridlund 2000; Selsis et al. 2011). Tidally locked planets in the habitable zone of M dwarfs should tend to have a relatively long rotation period (typically tens of days) and therefore a weak Coriolis force. An analogous situation arises in the tropics of rapidly rotating planets, such as Earth, because the horizontal component of the Coriolis force vector is small there. When the Coriolis force is weak (Rossby Number, Ro, ≳ 1), advection balances the pressure gradient. For typical wind speeds this allows only a very weak pressure gradient force and therefore temperature gradient (Charney 1963) and we may reasonably approximate horizontal atmospheric temperature gradients as zero (uniform temperature) above the boundary layer, the lower region of the atmosphere where frictional forces are important (Pierrehumbert 1995). This situation is referred to as the “weak temperature gradient” (WTG) approximation (Sobel et al. 2001), and it is brought about by gravity waves. The WTG approximation is therefore only valid if the timescale for gravity wave propagation around a planet is short compared to the radiative timescale, which excludes its application to extremely hot and/or thin atmospheres (Perez-Becker & Showman 2013; Showman et al. 2013). 3D general circulation model (GCM) simulations suggest that the WTG approximation is a useful guiding principle for tidally locked habitable zone planets (Merlis & Schneider 2010), which leads to greatly simpliﬁed models of the atmospheric dynamics of such planets (Pierrehumbert 2011) that can be coupled to models of other critical processes for planetary climate (Kite et al. 2011). In addition to elucidating relevant physics, a WTG model can be run millions of times extremely quickly and could therefore be used in conjunction with observations to constrain atmospheric parameters. There are two main advantages to ﬁtting a WTG model to data as opposed to a simple phenomenological model (Cowan & Agol 2008; Cowan et al. 2013) or energy balance model (Cowan & Agol 2011; Lewis et al. 2013). First, a WTG model is based on a dynamically consistent framework so that behavior constrained by the phase curve is guaranteed to satisfy the relevant dynamical equations. In contrast, an energy balance model might, for example, diﬀuse heat in an unphysical way. Second, as we will explain in section 2, a WTG model includes a solid surface and solves for the surface temperature, which would be useful for interpreting phase curves of Earth-like planets. The goal of this paper is to determine whether a WTG model could be feasibly distinguished from a GCM using thermal phase curve photometry, the type of observations relevant to atmospheric dynamics likely to be available over the next decade. To do this we run a GCM at a variety of rotation rates, keeping other parameters constant, then tune a WTG model to the GCM output and compare the thermal phase curves that each would produce when measured by a remote observer. In practice a WTG model would be tuned to observations and a GCM requires many unknown input parameters that would also need to be tuned to observations, so tuning the WTG model to the GCM is a reasonable methodology. We ﬁnd that in the regime of dry Earth-like ∼1 bar atmospheres, the phase curve a WTG model produces would be nearly indistinguishable from that a GCM produces for a tidally locked habitable zone planet orbiting a nearby M dwarf star. This suggests that for many situations the WTG approximation is a suﬃcient description of atmospheric dynamics for such planets. GCMs would remain useful for calculating the vertical structure of the atmosphere, which is critical for radiative transfer, and for cloud formation and other moisture driven processes, which can strongly aﬀect the thermal phase curve if present (Yang et al. 2013).
Recurrent Dirichlet Belief Networks for Interpretable Dynamic Relational Data Modelling<|sep|>Dynamic data is a common feature in many real-world applications, including relational data analysis [Mucha et al., 2010; Phan and Airoldi, 2015; Yang and Koeppl, 2018] for learning time-varying node interactions, and text modelling [Guo et al., 2018; Schein et al., 2019] for exploring topic evolution. Modelling dynamic data has become a vibrant research topic, with popular techniques ranging from non-Bayesian methods, such as Collaborative Filtering with Temporal Dynamics (SVD++) [Koren, 2009], to Bayesian deep probabilistic frameworks such as Deep Poisson-Gamma Dynamical Systems (DPGDS) [Guo et al., 2018]. The main advantage of Bayesian deep probabilistic frameworks is the ﬂexible model design and the strong modelling performance. However, most of these frameworks are static so that they cannot account for the evolution of relationships over time. It would be highly beneﬁcial if the frameworks can be extended to the dynamic setting to enjoy the modelling advantages. The Dirichlet Belief Network (DirBN) [Zhao et al., 2018] has been proposed recently as a promising deep probabilistic framework for learning interpretable deep latent structures. To date, the DirBN has mainly been used in two applications: (1) topic structure learning [Zhao et al., 2018], where latent representations are used to model the word distribution for topics; and (2) relational models [Fan et al., 2019a], where latent representations model the nodes’ membership distribution over communities. By constructing a deep architecture for latent distributions, the DirBN can model high-order dependence between topic-word distributions (in topic models) and nodes’ membership distributions (in relational models). In this work, we propose a Recurrent Dirichlet Belief Network (Recurrent-DBN) to explore the complex latent structures in dynamic relational data. In addition to constructing an interpretable deep architecture for the data within individual time steps, we also study the temporal dependence in the dynamic relational data through (layer-to-layer) connections crossing consecutive time steps. Consequently, our Recurrent-DBN can describe long-term temporal dependence (i.e., the dependence between the current variables and those in the previous several time steps), improving over the one-order Markov structures that usually describe the dependence between the current variables and those in the previous one time step only. For model inference, we further develop an efﬁcient Gibbs sampling algorithm. Besides upward propagating latent counts as done by DirBN, we also introduce a backward step to propagate the counts from the current time step to the previous time steps. Our experiments on real-world dynamic relational data show signiﬁcant advantages of the Recurrent-DBN over the state-of-the-art models in tasks of interpretable latent structure discovery and link prediction. Similar to DirBN that can be considered as a self-contained module [Zhao et al., 2018], our Recurrent-DBN could be ﬂexibly adapted to account for dynamic data other than evolving relational data, Results Signiﬁcantly improved model performance in realworld dynamic relational models compared to the stateof-the-art, including better link prediction performance and enhanced interpretable latent structure visualisation. 2 Background information of DirBN We ﬁrst give a brief review of the DirBN model. In general, the DirBN constructs a multi-stochastic layered architecture to represent interpretable latent distributions for objects. We describe it within the relational data setting for illustrative purposes. Given a binary observed linkage matrix RRR ∈ {0, 1}N×N for N nodes, where Rij denotes whether node i has a relation to node j, the DirBN constructs an L-layer and K-length community membership distribution πππi ={πππ(l) i }L l=1 for each node i. The generative process for the membership distributions {πππ(l) i }L l=1, as well as the observed matrix RRR, can be brieﬂy described as:
A Comprehensive Study of Bright Fermi-GBM Short Gamma-Ray Bursts: II. Very Short Burst and Its Implications<|sep|>Gamma-ray Burst (GRB) is the most intense transient astrophysical phenomena in the universe. Long GRB (LGRB, duration longer than 2 s) is believed to origin from the corecollapse of a massive star [1–6]. However, short GRB (SGRB, duration less than 2 s) shares a distinct origin, such as a merger process of two compact objects, e.g., two neutron stars (NS-NS), which is ﬁrst proved by a gravitational-wave GRB, GRB 170817A [7]. For both types of GRBs, the dominate component of the gamma-ray emission at the photospheric radius, where energy dissipation takes place in the optically thick regime, is the so-called quisa-thermal component, which gradually fades across the burst duration. This thermal component represented by a standard blackbody function is discovered in several bright GRBs, such as GRB 090902B, GRB 120323A and GRB 170206A [8–12]. Some modiﬁed blackbody modesl to reproduce the thermal photospheric emission are proposed and employed to ﬁt the spectra of several GRBs [13–16]. Subphotospheric emission is also a popular mechanism for the thermal component in the Fermi era [17–19]. Photospheric emission from a structured jet or a hybrid relativistic outﬂow is invoked in some GRBs [20–22]. Photospheric emission models via the Compton scattering are also proposed to broaden the thermal peak [23–26]. In the ﬁreball model, GRB photosphere often occurs in the early phase, after which there is the emission region dominated by the internal shock, thus the thermal component and the non-thermal component would dominate the different emission phase [27]. The spectral evolution that includes the thermal and non-thermal component is also conﬁrmed in several GRBs [28–31]. In this work, we select a GRB sample with a very short duration detected by Fermi Gamma-Ray Burst Monitor (Fermi/GBM) to judge these features, such with a burst duration shorter than 0.05 s, which often cannot be performed the time-resolved spectral analysis. In these GRBs, the thermal component would dominate prompt gamma rays if the ﬂux of the thermal emission exceeds that of the non-thermal emission. In Section 2, we present the sample selection and describe the method of spectral ﬁtting. The results for four spectral models are presented in Section 3. A short discussion is given in Section 4. We present the summary and conclusions in Section 5.
Detection of very high energy gamma-ray emission from the gravitationally-lensed blazar QSO B0218+357 with the MAGIC telescopes<|sep|>Even though there are already over 60 blazars detected in the very high energy (VHE, ≳ 100 GeV) range, most of them are relatively close-by sources with redshift z ≲ 0.5. Until mid 2014, the farthest sources observed in this energy range were 3C 279 (z = 0.536, Albert et al. 2008), KUV 00311-1938 (z > 0.506, Becherini et al. 2012) and PKS1424+240 (z = 0.601, Acciari et al. 2010). In the last two years the MAGIC (Major Atmospheric Gamma Imaging Cherenkov) telescopes discovered VHE gamma-ray emission from QSO B0218+357 at z = 0.944 (Mirzoyan 2014) and afterwards PKS1441+25 at z = 0.940 (Ahnen et al. 2015) almost doubling the boundaries of the known gamma-ray universe. Observations of distant sources in VHE gamma-rays are diﬃcult due to strong absorption in the extragalactic background light (EBL, see e.g. Gould & Schréder 1966). At a redshift of ∼ 1 it results in a cut-oﬀ at energies1 ∼ 100 GeV. Such energies are at the lower edge of the sensitivity range of the current generation of Imaging Atmospheric Cherenkov Telescopes (IACTs), making such observations challenging. To maximize the chance of detection, the observations are often triggered by a high state observed in lower energy ranges. In particular, Fermi-LAT (Large Area Telescope) scanning the whole sky every 3 hours provides alerts on sources with high ﬂuxes and information about the spectral shape of the emission in the GeV range. QSO B0218+357, also known as S3 0218+35, is classiﬁed as a ﬂat spectrum radio quasar (FSRQ, Ackermann et al. 2011). The classiﬁcation is based on the optical spectrum (Cohen et al. 2003). It is located at a redshift of zs = 0.944 ± 0.002 (Cohen et al. 2003). One of the ﬁve features from which Cohen et al. (2003) derived the redshift was conﬁrmed by (Landoni et al. 2015). The object is gravitationally lensed by the face-on spiral galaxy B0218+357G located at a redshift of zl = 0.68466 ± 0.00004 (Carilli et al. 1993). Strong gravitational lensing forms multiple images of the source (see e.g. Kochanek et al. 2004). The ﬂux magniﬁcation of an image is the ratio of the number of photons gravitationally deﬂected into a small solid angle centered on the observer to the number of photons emitted by the source in such a solid angle. The 22.4 GHz VLA radio image shows two distinct components with an angular separation of only 335 mas and an Einstein ring of a similar size (O’Dea et al. 1992). Observations of variability of the two radio components led to a measurement of a delay of 1012 days between the leading and trailing images (Corbett et al. 1996; Biggs et al. 1999; Cohen et al. 2000; Eulaers & Magain 2011). In the radio image, the leading component (also dubbed ‘image A’ in literature) is located to the west from the trailing component (image B). The delayed component had a 3.57-3.73 times weaker ﬂux (Biggs et al. 1999). However, the observed ratio of magniﬁcation varies with the radio frequency (Mittal et al. 2006), presumably due to free-free absorption in the lensing galaxy (Mittal et al. 2007). In the optical range the leading image is strongly absorbed (Falco et al. 1999). In 2012 QSO B0218+357 went through a series of outbursts registered by the Fermi-LAT (Cheung et al. 2014). Even though Fermi-LAT does not have the necessary angular resolution to disentangle the two emission components, the statistical analysis of the light curve auto-correlation function led to a measurement of a time delay of 11.46±0.16 days. Interestingly the average magniﬁcation factor, contrary to radio measurements, was estimated to be ∼ 1. Changes in the observed GeV magniﬁcation ratio were interpreted as microlensing eﬀects on individual stars in the lensing galaxy (Vovk & Neronov 2016). Microlensing on larger scale structures has been considered as well (Sitarek & Bednarek
Collective excitations of a quantized vortex in $^3P_2$ superfluids in neutron stars<|sep|>The properties of high density nuclear matter are still not sufﬁciently understood. However, in recent years, neutron star (NS) observations began to place stronger restrictions [1–3] on the equation of state (EoS) of dense matter and to probe structure and composition of NS cores. Most valuable information into the state of their interiors came from the detections of pulsar glitches via pulsar timing measurements, optical and X-ray observations of cooling and accreting NSs and from neutrino emission measurements from proto-neutron stars. These observations provide evidence for superﬂuidity in the interiors of NSs [4]. The ﬁrst direct evidence [5, 6] that the NS core should exist in a superﬂuid state has been reported in the study of the NS in the supernova remnant known as Cassiopeia A. It has been measured that Cassiopeia A’s surface temperature has rapidly decreased from 2.12 × 106 K to 2.04 × 106 K. This pronounced drop in surface temperature can be naturally explained [5–7] if one assumes that neutrons have recently become superﬂuid in the NS core. As the neutrons combine to form Cooper pairs, a splash of neutrinos is emitted accelerating the NS cooling process. Another evidence for a superﬂuid NS core comes from the observation of pulsar glitches, which are sudden jumps in the NS rotation frequency. Glitches like the ones observed in the Crab and Velar pulsar can be explained by two main physical mechanism – either they are due to starquakes from the NS core [8, 9] or crust [10–12] or they are caused by the sudden unpinning and displacement of a large number of vortices [11–13] in the NS superﬂuid. In the starquake glitch model [12, 14, 15], the glitches are caused by a sudden reduction in the moment of inertia of some solid component of the NS. For example, the differential rotation between the crust and the superﬂuid NS core can produce stresses [12] in the NS crust leading to crustquakes. The resulting crustquakes distort the star’s shape and hence generate sudden jumps in the NS rotational frequency, seen as glitches. In contrast, in the pinned superﬂuid model angular momentum is suddenly transferred from the superﬂuid NS core to the non-superﬂuid crust via vortex unpinning, spinning it up. Neutron superﬂuid vortices can become pinned to nuclear clusters in the inner crust [11] or to magnetic ﬂux tubes in the core [16]. This prevents angular momentum from being transferred to the NS crust. Hence, a differential rotation builds up between the NS core and crust. When this differential becomes large enough angular momentum is suddenly transferred from the core to the crust through the catastrophic unpinning of vortices, resulting in a sudden spin-up in the NS. Furthermore, the observed long time relaxation after glitches can be explained only by assuming the coexistence of normal and superﬂuid components [8, 10]. Therefore, understanding the properties of neutron superﬂuidity and the formation and dynamics of superﬂuid vortices can give us further insights into the evolution of NSs and their composition. At the high density central part of NSs the neutron superﬂuidity is attributed to the 3P2 pairing interaction [17–22] rather than to 1S0 pairing familiar from the conventional BCS theory of superconductors. Analysis of nucleon-nucleon scattering data [18] shows that the transition from an isotropic 1S0 superﬂuid to an anisotropic 3P2 superﬂuid occurs at densities above 1.6 × 1014 g/cm3. The Ginzburg-Landau (GL) energy functional generalized for 3P2 superﬂuids was developed in Refs. [21, 22] in the weak coupling limit. Note that the general GL free energy formally agrees with the angular momen tum l = 2 GL functional solved by Mermin [23]; depending on the GL parameters, the ground state is in either nematic, ferromagnetic or cyclic phase. The ground states of 3P2 superﬂuids in the weak coupling limit were found in Ref. [24] to be in the nematic phase in the absence of a magnetic ﬁeld. They are continuously degenerated when we ignore the sixth order term and can be decomposed by unbroken O(2), D2 and D4 symmetry groups into the uniaxial, D2- and D4-biaxial nematic phases, respectively. (This is also known from spin-2 spinor Bose-Einstein condensates, see Refs. [25, 26].) This degeneracy is lifted and the uniaxial nematic phase becomes the unique ground state once the sixth order term is taken into account. The ground states in the presence of a magnetic ﬁeld were recently determined [27]; the ground state is in the uniaxial nematic phase for smaller magnetic ﬁelds relevant for ordinary NSs and in the D2- or D4-biaxial nematic phase for large magnetic ﬁeld relevant for magnetars. Beyond the GL theory, the ground states in the (T,H) phase diagram have been obtained recently in the Bogoliubov-de Gennes (BdG) formalism [28]. The D2- and D4-biaxial nematic phases appear in lower T and H region and in higher T and H region, respectively. The phase boundary is of second order at higher temperature while it is of ﬁrst order at lower temperature, and a tricritical point connects these boundaries. One of the most important results of the BdG formalism is that 3P2 superﬂuids were found to be topological superﬂuids predicting gapless Majorana fermions on the surface [28]. The strong-coupling corrections to the 3P2 NS matter GL free energy including spin-orbit and central forces were calculated in Ref. [29]. As phenomenological aspects relevant for NSs physics are concerned, 3P2 superﬂuids provide new mechanisms for neutrino emission of dense neutron matter [30–39], explain the entrainment [40] of superconducting protons by rotating superﬂuid neutrons in NSs and offer possible explanations of the anomalously rapid cooling of NSs [5–7]. Since superﬂuids are rotating inside NSs, a large number (∼ 1019) of superﬂuid vortices exist in their interior. The rich structure and magnetic properties of vortices emerging in the GL equations for 3P2 superﬂuids were explored in Refs. [22, 27, 41–43]. 3P2 vortices in NS matter turn out to be structurally different from their counterparts in 1S0 superﬂuids. For example, different to 1S0 vortices, 3P2 vortices exhibit spontaneous magnetization in the vortex core region [27, 42]. For magnetic ﬁeld strengths of orders of magnitude as they appear in magnetars, the ground state is in the D4-biaxial nematic phase, in which the ﬁrst homotopy group π1 is nonAbelian, thereby admitting non-Abelian (non-commutative) vortices which carry half-quantized circulation [43]. In this article, we study low-energy collective modes [or (pseudo) moduli] and solitonic excitations of an integer vortex in a 3P2 superﬂuid. By solving numerically the GL equation, we reconstruct the axially symmetric integer vortex solution in the absence of magnetic ﬁelds and sixth order terms. Because of the off-diagonal elements of the tensor order parameter, there exists spontaneous magnetization at the vortex core as described above. Here, we calculate the net magnetic moment per femtometer along vortex line to be one order less than the neutron magnetic moment. We then study collective modes in the presence of a single vortex. As usual, there are gapless (massless) Kelvin modes (translational moduli) due to the spontaneously broken translational symmetry in the presence of the vortex. In addition, the phase δ of the off-diagonal elements of the tensor order parameter gives rise to a gapfull (massive) mode. We construct the low-energy (long distance) effective free energy of this gapfull mode and ﬁnd it is a double sine-Gordon model, consisting of the potential terms cos δ and cos 2δ. For GL parameter values describing typical NSs, it allows only 2π kink solutions [44]. We ﬁnd that the core magnetization of the vortex ﬂips its direction at the kink. The article is structured as follows. After introducing the GL free energy relevant for 3P2 superﬂuids in the weak coupling limit, 3P2 vortex solutions are constructed in Section II and their spontaneous magnetization in the vortex core region is evaluated. Then, in Section III, we discuss the effect of the parameter δ on the free energy density by writing down the associated effective free energy functional which takes the simple form of a double sine-Gordon model. The associated kink soliton solution is derived in Section IV. Finally, our conclusions and possible future lines of investigation are presented in Section V. To facilitate the reader to reproduce our numerical results, we add two appendices to this article. In Appendix A, we brieﬂy review the GL theory for 3P2 superﬂuid states and state all the parameter values used in our numerical simulations. We list explicitly all the vortex equations together with the imposed boundary conditions in Appendix C. Note that a detailed investigation of vortex structure and dynamics in the presence of nonzero external magnetic ﬁelds and higher order terms, in particular the inclusion of the sixth order term, will be published in a forthcoming paper.
A fast approximate skeleton with guarantees for any cloud of points in a Euclidean space<|sep|>Potential molecules for new colloidal products are tested by simulations that produce unorganized ﬁnite clouds of points (one point per molecule in Fig. 1). Molecules tend to form clusters (called micelles) whose shapes (degrees of branching, edgelengths) affect physical properties of colloidal products, e.g. their viscosity. These 3D micelles can have complicated branched shapes as in Fig. 7 and are visually analyzed by human experts who struggle to make reliable measurements quickly. To substantially speed-up the discovery of new molecules, we propose a new Approximate Skeleton ASk(C) to solve the following problem. The tree reconstruction problem. Given a point cloud C ⊂ Rm and an error ε, design a fast algorithm to build a straight-line tree T ⊂ Rm (see Deﬁnition 1) that has a minimum number of vertices and whose ε-offset (neighborhood) covers C.
Radiative hydrodynamics simulations of red supergiant stars: I. interpretation of interferometric observations<|sep|>Massive stars with masses between roughly 10 and 25 M⊙ spend some time as red supergiant (RSG) stars being the largest stars in the universe. They have eﬀective temperatures, Teﬀ, ranging from 3 450 to 4 100 K, luminosities of 20 000 to 300 000 L⊙ and radii up to 1 500 R⊙ (Levesque et al. 2005). Their luminosities place them among the brightest stars, visible to very large distances. There is however a number of open issues. They shed large amounts of mass back to the interstellar medium, but their mass-loss mechanism is unidentiﬁed, although Alfv´en and acoustic waves have been proposed (Hartmann & Avrett 1984; Pijpers & Hearn 1989; Cuntz 1997), as well as acoustic waves and radiation pressure on molecules (Josselin & Plez 2007). Their chemical composition is largely unknown, despite the work of e.g. Carr et al. (2000), and Cunha et al. (2007), due to diﬃculties in analysing their spectra with broad, asymmetric lines with variations suspected to stem from a convection pattern consisting of large granules and (super-)sonic velocities (Josselin & Plez 2007; Gray 2008). Progress has been made recently, with their Teﬀ-scale being revised both at solar and Magellanic Clouds metallicities using 1D hydrostatic models (Levesque et al. 2005, 2006; Massey et al. 2007; Levesque et al. 2007). Although these MARCS models (Gustafsson et al. 2008) give a good ﬁt of the optical spectra allowing the derivation of Teﬀ and reddening, problems remain. There is a blue-UV excess in many of the observed spectra, that may point to scattering by circumstellar dust, or to an insuﬃciency in the models. There is also a mismatch in the IR colours, that could be due to atmospheric temperature inhomogeneities characteristic of convection (Levesque et al. 2006). In recent years, hydrodynamical modeling of convection in RSGs has lagged behind that of solar type stars due to the necessity to include the whole star in the simulation box. Freytag et al. (2002) have succeeded in doing such numerical simulations of a typical RSG. We have thus engaged in an important eﬀort to improve our understanding and description of RSGs using detailed numerical simulations and a large set of observational material. This paper is the ﬁrst in this series and it is aimed to explore the granulation pattern of RSGs and its impact on interferometric observations.
Numerical treatment of interfaces in Quantum Mechanics<|sep|>For some years now there has been numerical techniques to deal with interfaces (boundaries between numerical grids) when solving hyperbolic or parabolic equations. Some of them use interpolation between regions of overlap, while others use penalties which modify the system at boundary grid points including information from the same space points at other grids [1]. This last method is preferable in many situations for it has very nice properties, the more interesting one is the fact that it is constructed so that the resulting semi-discrete system preserves the corresponding continuum energy estimate of the corresponding constant coeﬃcient linear systems. Thus we can ensure that, at least for linear, constant coeﬃcients system, the scheme is stable. Another important property this technique has arises when dealing with parallel codes using, say MPI. In that case, data near the boundaries is needed to be passed along diﬀerent MPI process (usually a grid or part of it), each one of them usually running at diﬀerent hosts, and so it is important to pass as little information among them as possible. For these cases this method is optimal, since only the boundary points need to be shared among grids the amount of data among processes is minimal and does not depend on the precision of the numerical scheme. This property is also important since otherwise interpolation is needed when the geometry of the grids do not coincide beyond the shared boundary points. In this paper we show that the penalty method not only works for hyperbolic and parabolic systems but also can be extended to the Schr¨odinger equation, having similar properties as the ones mentioned above. In the next section we derive the boundary terms needed to be added to the equations for the method to work, is this case there are two diﬀerent types of terms. Only one of them depends only on the values of the ﬁelds at the same grid, its presence is needed to cancel the usual boundary term from the elliptic part of the operator, the other term is really an interaction between the ﬁelds at both grids, it is a penalty only in the sense that depends on the diﬀerence of ﬁelds on both sides of the grids, but contrary to the hyperbolic and parabolic cases, where the penalties introduced a large negative eigenvalue to the system which would bring down to zero the diﬀerence on both sides of the grid, in this case the contribution is a large purely imaginary eigenvalue which keeps the l2 norm constant, but nevertheless does the allow the waves to pass through the interface from one grid to the other. In the last section we show some numerical results using this method. We compare the results of evolving a one dimensional system in a circle, ﬁrst using periodic boundary conditions in a single grid (that is a homogeneous scheme using centered diﬀerence operators) and then using the interface scheme between the ﬁrst and ﬁnal point of the grid. We show convergence of the scheme compatible with the discretization method used and discuss the quality of the approximation.
Spin alignment and violation of the OZI rule in exclusive $\omega$ and $\phi$ production in pp collisions<|sep|>The Okubo-Zweig-Iizuka (OZI) rule [1] was formulated in the early days of the quark model, stating that all hadronic processes with disconnected quark lines are suppressed. It qualitatively explains phenomena like suppression of φ meson decays into non-strange particles and suppression of exclusive φ production in non-strange hadron collisions. Using the known deviation from the ideal mixing angle of the vector mesons ω and φ, δV = 3.7o, the production cross section of φ with respect to that of ω should be suppressed according to σ(AB → Xφ)/σ(AB → Xω) = tan2 δV = 0.0042, where A, B and X are non-strange hadrons [2]. At low energies, where baryonic and mesonic degrees of freedom are most relevant, the ratio can be expressed in terms of meson-meson or meson-nucleon couplings: g2 φρπ/g2 ωρπ = g2 φNN/g2 ωNN = tan2 δV = 0.0042, where N denotes the nucleon. This is valid provided the coupling ratios gφρπ/gωρπ and gφNN/gωNN are equal as advocated in Ref. [3]. The OZI rule was tested in several experiments and is remarkably well fulﬁlled in many reactions (for a review, see e.g. Refs. [4] and [5]). Apparent violations of the OZI rule – observed in p ¯p annihilations at rest and in nucleon-nucleon collisions – can be interpreted either as a true violation due to gluonic intermediate states (see e.g. Ref. [6]) or as an evasion from the OZI rule because of a hidden strangeness component in the nucleon [7]. Such a strangeness component, possibly polarised, was suggested as an explanation of the apparent OZI violations observed in pN → N pV, V = ω,φ by the SPHINX collaboration [8]. Large OZI violations at low energies have also led to speculations about crypto-exotic baryon resonances decaying to Nφ [9]. Although being phenomenological in its origin, the OZI rule has been connected to QCD [2]. In a ﬁeld theoretical approach to the OZI rule, a perturbative treatment based on quark-gluon degrees of freedom requires the scale of a speciﬁc process to be much larger than the QCD cut-off parameter ΛQCD ≈ 200 MeV/c. In charmonium production, where the scale is governed by the charm quark current mass mc ≈ 1275 MeV/c2, the quark–antiquark pair is generated by gluon splitting, g → c¯c. This is in contrast to the case of strangeness production, where the scale corresponds to the strange quark current mass ms ≈ 95 MeV/c2, which is close to ΛQCD. The validity of the quark-gluon picture can thus be questioned, and the relevant degrees of freedom need to be determined. Gluon splitting can only be used in an effective sense. This has also been discussed in connection to hyperon production in ¯pp → ¯ΛΛ production near threshold, where neither meson exchange models nor quark-gluon models give a complete explanation of the experimental data [10]. However, probed at virtualities Q2 or p2 ⊥ ≫ 1 (GeV/c)2, which are large compared to (2ms)2c2 ≈ Λ2 QCD ≈ 0.04 (GeV/c)2, the process can be described in the quark-gluon picture and we expect strangeness suppression to disappear, restoring ﬂavour SU(3) symmetry. In this work, we present an attempt to understand the effective scale governing the (hidden) strangeness production in the exclusive process pp → pφp by studying the degree of OZI violation. The difﬁculty lies in the separation of different reaction mechanisms as a function of transferred energy and angular momentum. The latter is reﬂected in the anisotropy of the decay angular distributions which can be expressed via the spin density matrix [11]. In the analysis of data from an unpolarised beam impinging on an unpolarised target, symmetries leave one independent element of the spin density matrix, ρ00, which is a measure for spin alignment (tensor polarisation). It can be extracted from distributions of the angle between the decay plane (3-body decay) or decay axis (2-body decay) of the vector meson and a well-chosen reference axis [12]. The MOMO collaboration measured ρ00 of the φ meson in pd → 3Heφ near the kinematic threshold and the result was consistent with a complete alignment of the φ meson with respect to the incoming beam [13]. This is in sharp contrast to the case of the ω meson, which is produced unaligned at the same excess energy and in the same initial state, as found by the WASA collaboration [14]. The alignment of the ω meson in pp collisions was measured close to threshold by the COSY-TOF collaboration [16] and in pN collisions at a beam momentum of 70 GeV/c by SPHINX [15], whereas the φ alignment was measured at high energies by ACCMOR [17] and by STAR at RHIC [18]. Prior to our measurement, the only simultaneous measurement of φ and ω alignment using the same experimental set-up was performed by the SAPHIR collaboration [19, 20] in photoproduction. At COMPASS, the exclusive reaction pbeam ptarget → pfast V precoil is measured at a beam momentum of 190 GeV/c. For simplicity, this will from now on be denoted pp → pV p. Apart from this notation and unless otherwise stated explicitly, the symbol p without subscript and the Feynman variable xF = pL/pLmax, pL denoting the longitudinal momentum, will refer to the fast proton. The reduced 4momentum transfer squared t′ from the beam to the recoil proton is deﬁned as t′ = |t| − |t|min, where t = (ppbeam −(ppfast +pV ))2 and |t|min the minimum value of |t|. For exclusive vector meson production, there are contributions from mainly two classes of processes: resonant and non-resonant production. First, resonant production means diffractive dissociation of the fast proton, where a Pomeron is emitted in the t-channel from the target and excites the beam particle (see Fig. 1, left panel). The target particle receives a small recoil but stays intact. The vector meson is then produced via a baryon resonance. On the other side, there is the non-resonant process including the case when a vector meson is radiated from the proton in the initial or ﬁnal state. This is possible due to a ﬁnite coupling of the vector meson to the meson cloud of the nucleon [21]. These non-resonant processes are summarised in the middle panel of Fig. 1, where the blob in the upper vertex represents point-like and non-point-like interactions. Non-resonant vector meson production also includes central production where a Reggeon or Pomeron from the target and a Reggeon or Pomeron from the beam particle fuse in a central vertex (see Fig. 1, right panel). The production of ω and φ in Pomeron-Pomeron collisions does not conserve G-parity and is thus forbidden. Central Production is characterised by large rapidity gaps between all three ﬁnal state particles. This is equivalent to large gaps between the xF distributions of the outgoing particles. For the pp → pV p process this results in large xF of the fast proton. Another special case of non-resonant production is the shake-out (see e.g. Ref. [7]) of a qq pair from the sea of one nucleon which becomes on-shell when interacting with a Pomeron from the other nucleon. In the case of shake-out, a rapidity gap is expected between the recoil particle and the other two particles, but not necessarily between the fast proton and the vector meson. Central production and shake-out can in this sense be considered as similar processes in two different regions of phase space. The dynamics of the vector meson is determined by the incoming particles of the production vertex. In the case of Pomeron–Reggeon fusion and shake-out, the dynamics of the vector meson depends on the exchange object(s) while in resonant diffractive production, it depends on the intermediate resonance.
Automatic morphological classification of galaxies: convolutional autoencoder and bagging-based multiclustering model<|sep|>Galaxy morphology is an important characteristic relevant to other key physical properties, such as the stellar mass, the color, the star-formation rate (SFR), the gas content, the galaxy age, and the environment (e.g., Dressler 1980; Kauffmann et al. 2003, 2004; Omand et al. 2014; Schawinski et al. 2014; Kawinwanichakij et al. 2017; Gu et al. 2018; Lianou et al. 2019). The morphological diversity of galaxies implies the difference in the evolutionary histories of galaxies. As the development of telescopes and instruments advances, the automatic morphological analysis and classiﬁcation of galaxies are imperiously demanded to help understand galaxy formation rate and evolution. To exploit the potentials of galaxy morphologies from current and future surveys, we present an unsupervised machine-learning (UML) method for the morphological classiﬁcation through the astronomical imaging data. The apparent visual morphology might be the earliest measurement of galaxies (Hubble 1926). Human classiﬁers can judge the morphological type of galaxies by a certain classiﬁcation scheme (e.g, Hubble 1926; de Vaucouleurs 1959; van den Bergh 1960). Large projects can employ many human classiﬁers to give the label or probability of galaxy morphology (e.g., Lintott et al. 2011). Indeed, conventional visual classiﬁcation may have some incompatible results due to subjective deviation. However, what’s more important is that conventional visual classiﬁcation is quite low inefﬁciency since galaxies should be inspected one by one. Except the visual classiﬁcations by eyes, many techniques are developed to quantify the galaxy structure and the morphology. To obtain the morphological classiﬁcations of galaxies, the key is to extract the morphological features from the raw images. Parametric measurements describe the light proﬁles of galaxies using the mathematical models with a set of parameters (e.g. S´ersic 1963; van der Wel et al. 2012; Lang et al. 2014). Nonparametric measurements design several model-independent parameters to describe some characteristics of galaxy morphologies (e.g. Abraham et al. 1994; Conselice 2003; Lotz et al. 2004; Freeman et al. 2013; Also see the review by Conselice 2014). Relying on the parameter spaces constituted of the morphological features, several approaches are proposed to distinguish the morphological types, such as the concentration, asymmetry, smooth ness method (Conselice 2014), the principal component analysis (Scarlata et al. 2007), and the support-vector machine (Huertas-Company et al. 2008). The structural parameters reduce the complexity of morphological descriptions. However, it rejects the abundant information hidden in all pixels and may lead to failures of morphological distinction in some cases. The convolutional neural network is one of the most popular methods of supervised machine learning applied in astronomy to classify galaxy morphologies automatically (e.g., Huertas-Company et al. 2015; Dieleman et al. 2015; Walmsley et al. 2019). It can extract enormous amount of information contained in the pixels themselves hierarchically and have a good performance in mimicking the human perceptions with a high efﬁciency. However, it is a supervisedlearning method which means that it is highly dependent on the prelabeled training data set. By training the model on labeled data, the supervised-learning method is good at mimicking human perceptions. The methodology of UML might be another way to the morphological classiﬁcation, which does not need a prelabeled training set labeled by human classiﬁers. Hence it has no subjective deviations of humans and may exploit a new angle of galaxy morphologies from the machine’s view. The methodology of UML is ideally suited to the morphological analysis of Big Data surveys, which has been applied to the automatic classiﬁcation of optical/NIR and radio images (e.g., Ralph et al. 2019; Galvin et al. 2020). Hocking et al. (2018) and Cheng et al. (2020) apply the growing neural gas algorithm (Fritzke 1995) to extract features from images and the hierarchical clustering technique is responsible for gathering the galaxies with similar features. The convolutional autoencoder (CAE) (Masci et al. 2011) is also another effective technique to extract features from images. The combination of autoencoder and clustering algorithm is able to obtain the reasonable results on strong lensing identiﬁcation (Cheng et al. 2020) and morphological classiﬁcation (Cheng et al. 2021). The upcoming China Space Station Telescope (CSST) will image the sky in 7 bands (NUV, u, g, r, i, z, and y) and vastly enrich the photometric data with a wide survey covered 17,500 deg2 with 5σ depth of r = 26.0 mag and a deep survey covered 400 deg2 with 5σ depth of r = 27.2. To pre pare for and to exploit the imaging data of the CSST, we plan to develop an UML method for galaxy classiﬁcations. In this paper, we pioneer in the use of unsupervised classiﬁcation in the ﬁve CANDELS ﬁelds. The CAE is trained for the extraction of unsupervised features from the raw imaging data. After that, to avoid the bias from one single clustering algorithm, a bagging-based multiclustering method is proposed which considers the results of three different clustering algorithms. Although at the cost of eliminating the disputed sources, galaxies are clustered into 100 groups under a comprehensive deﬁnition of similarity. As a result, the purity in each classiﬁcation is signiﬁcantly improved. To test the feasibility of our method, we merge 100 groups into ﬁve subclasses by visual veriﬁcation, including spheroid (SPH), early-type disk (ETD), late-type disk (LTD), irregular (IRR), and unclassiﬁable (UNC). After discarding the disputed sources and the UNC category, we investigate the connection with colors and morphological parameters using the massive galaxies (M∗ > 1010M⊙). Moreover, by using the t-SNE visualize technique (van der Maaten & Hinton 2008), the comparisons between our result and that of CANDELS visual classiﬁcations (Kartaltepe et al. 2015) and the supervised deep-learning method (Huertas-Company et al. 2015) are given. It suggests that the proposed method, combination of CAE and multiclustering strategy, is an effective method to cluster galaxies with similar features and can yield high-quality morphological classiﬁcations, which are useful in other downstream tasks. This paper is organized as follows. The data set and our sample construction are described in Section 2. The method is described in Section 3, which includes the CAE and the bagging-based multiclustering method. We compare our clustering result with other physical properties of massive galaxies and the results of other works in Section 4. Main conclusions and outlooks are summarized in Section 5. When converting the effective radii of galaxy from observational scales (arcsec) to physical scales (kpc), we assume the cosmological parameters as following: H0 = 70 km s−1 Mpc−1, Ωm = 0.30, ΩΛ = 0.70.
A Super-Jupiter Microlens Planet Characterized by High-Cadence KMTNet Microlensing Survey Observations of OGLE-2015-BLG-0954<|sep|>The Korea Microlensing Telescope Network (KMTNet; Kim et al. 2016) is a system of three 1.6m telescopes, each equipped with a 4 deg2 340-Mpixel camera, located in Chile, South Africa, and Australia. The system was designed to conduct a microlensing survey that would detect Earth-mass planets without additional followup data. While microlensing events typically last for tens of days, many of their most interesting features occur on much shorter timescales, especially features that permit the detection and characterization of planets, which where mp is the mass of the planet. Most microlensing planets have been discovered by combining survey and followup observations. In their original suggestion of this approach, Gould & Loeb (1992) argued that Galactic bulge ﬁelds should be monitored at relatively low cadence (e.g., once per night). This would enable them to cover the widest possible area, which is necessary to detect a large sample of microlensing events in spite of their low frequency Γ ∼ 10−5yr−1 star−1, but would not generally permit the detection of planets. Hence, the survey teams would have
HeatER: An Efficient and Unified Network for Human Reconstruction via Heatmap-based TransformER<|sep|>Understanding human structure from monocular images is one of the fundamental topics in computer vision. The corresponding tasks of Human Pose Estimation (HPE) and Human Mesh Reconstruction (HMR) have received a growing interest from researchers, accelerating progress towards various applications such as VR/AR, virtual try-on, and AI coaching. However, human pose estimation and mesh reconstruction from a single image still remain challenging tasks due to depth ambiguity, occlusion, and complex human body articulation. With the blooming of deep learning techniques, Convolutional Neural Network (CNN) [1–3] architectures have been extensively utilized in vision tasks and have achieved impressive performance. Most existing HPE and HMR models [3, 4] utilize CNN-based architectures (such as ResNet [2] and HRNet [3]) to maintain a 2D heatmap representation which encodes the position of each keypoint into a feature map with a Gaussian distribution. Since the heatmap representation can make the training process smoother, it has become the de facto representation in HPE [5, 3, 6]. Recently, the transformer architecture has been fruitfully adapted from the ﬁeld of natural language processing (NLP) into computer vision, where it has enabled state-of-the-art performance in HPE and HMR tasks [7–11]. The transformer architecture demonstrates a strong ability to model global dependencies in comparison to CNNs via its self-attention mechanism. The long-range correlations between tokens can be captured, which is critical for modeling the dependencies of different human body parts in HPE and HMR tasks. Since each heatmap represents a keypoint location related to the human pose and mesh, we aim to utilize the transformer architecture to reﬁne the coarse heatmaps (extracted by a CNN backbone) by capturing global correlations between them. However, inheriting from NLP where transformers embed each word to a feature vector, Vision Transformer architectures such as ViT [12] can only deal with the ﬂattened features when modeling attention. This is less than ideal for preserving the structural context of the many 2D heatmaps during the reﬁnement stage. Furthermore, another issue is that the large embedding dimension brought about by the ﬂattening process makes the transformer computational expensive. This is not friendly to the real-world applications of HPE and HMR, which often demand real-time processing capabilities on deployed devices (e.g. AR/VR headsets). Therefore, in this paper, we propose a Heatmap-based transformER (HeatER) architecture to properly reﬁne the coarse heatmaps through natural global correlations in a resource-friendly manner. Compared to the vanilla transformer architecture, HeatER has two advantages: • First, HeatER preserves the heatmap representation in the transformer encoder when modeling self-attention, which is naturally adherent with the HPE and HMR tasks. Rather than conducting the self-attention based on ﬂattened features, HeatER ensures that the self-attention is conducted based on the original heatmaps (location of keypoints), which is more structurally meaningful. To accomplish this, HeatER is designed with a novel dimensional decomposition strategy to handle the extracted stack of 2D heatmaps. • Second, this decompositional design simultaneously provides a signiﬁcant reduction in computational cost compared with the vanilla transformer 1. This makes HeatER more suitable for the needs of real-world applications. Equipped with HeatER, we present a uniﬁed and efﬁcient framework for human representation tasks including 2D HPE, 3D HPE, and HMR. For the more challenging 3D HPE and HMR portion, a heatmap reconstruction module is integrated into the framework. Here, a subset of heatmaps are randomly masked and then reconstructed by HeatER, enabling more robust 3D pose and mesh predictions for in-the-wild inference. We conduct extensive experiments on human representation tasks, including 2D human pose estimation on COCO, 3D human pose estimation and human mesh reconstruction on Human3.6M and 3DPW datasets. HeatER consistently outperforms SOTA methods on these tasks with signiﬁcant computation and memory cost reduction (e.g. HeatER outperforms MeshGraphormer [11] with only requiring 5% of Params and 16% of MACs).
Activity-Based Search for Black-Box Contraint-Programming Solvers<|sep|>Historically, the constraint-programming (CP) community has focused on developing open, extensible optimization tools, where the modeling and the search procedure can be specialized to the problem at hand. This focus stems partially from the roots of CP in programming languages and partly from the rich modeling language typically found in CP systems. While this ﬂexibility is appealing for experts in the ﬁeld, it places signiﬁcant burden on practitioners, reducing its acceptance across the wide spectrum of potential users. In recent years however, the constraint-programming community devoted increasing attention to the development of black-box constraint solvers. This new focus was motivated by the success of Mixed-Integer Programming (MIP) and SAT solvers on a variety of problem classes. MIP and SAT solvers are typically black-box systems with automatic model reformulations and general-purpose search procedures. As such, they allow practitioners to focus on modeling aspects and may reduce the time to solution signiﬁcantly. This research is concerned with one important aspect of black-box solvers: the implementation of a robust search procedure. In recent years, various proposals have addressed this issue. Impact-based search (Ibs) [10] is motivated by concepts found in MIP solvers such as strong branching and pseudo costs. Subsequent work about solution counting can be seen as an alternative to impacts [8] that exploits the structure of CP constraints. The weighted-degree heuristic (wdeg) [1] is a direct adaptation of the SAT heuristic Vsids[5] to CSPs that relies on information collected from failures to deﬁne the variable ordering. This paper proposes Activity-Based Search (Abs), a search heuristic that recognizes the central role of constraint propagation in constraint-programming systems. Its key idea is to associate with each variable a counter which measures the activity of a variable during propagation. This measure is updated systematically during search and initialized by a probing process. Abs has a number of advantages compared to earlier proposals. First, it does not deal explicitly with variable domains which complicates the implementation and runtime requirements of Ibs. Second, it does not instrument constraints which is a signiﬁcant burden in solution-counting heuristics. Third, it naturally deals with global constraints, which is not the case of wdeg since all variables in a failed constraint receive the same weight contribution although only a subset of them is relevant to the conﬂict. Abs was compared experimentally to Ibs and wdeg on a variety of benchmarks. The results show that Abs is the most robust heuristic and can produce signiﬁcant improvements in performance over Ibs and wdeg, especially when the problem complexity increases. The rest of the paper is organized as follows. Sections 2 and 3 review the Ibs and wdeg heuristics. Section 4 presents Abs. Section 5 presents the experimental results and Section 6 concludes the paper.
Source properties of the lowest signal-to-noise-ratio binary black hole detections<|sep|>The LIGO [4] and Virgo [5] collaborations (LVC) have to date reported the detection of 10 binary black hole (BBH) systems and one binary neutron star (BNS) system in the data collected during their ﬁrst two observing runs [3]. An independent analysis of the public data released by the LVC [6–9] has revealed 9 additional potential gravitational-wave (GW) signals [1, 2, 10] (see also Ref. [11] for a re-analysis of LIGO–Virgo’s ﬁrst observing run and Ref. [12] for the second observing run). In this study we focus on the 7 signals presented in Zackay et al. [1] and Venumadhav et al. [2]. If of astrophysical origin, then these systems are also BBHs, thus nearly doubling the total number of BBH systems detected in the ﬁrst two observing runs. The parameters of observed BBHs [2, 13] encode information about the underlying BBH population and about the evolutionary history of the black holes and their progenitors. The masses and spins of the black holes in particular can be used to infer the formation mechanism of the observed binaries. Usually, two families of formation scenarios are considered: classical binary evolution in the galactic ﬁeld [14–28], or dynamical formation either in the galactic ﬁeld [29], or in dense environments such as clusters [30, 31] or AGN disks [32–35].1 This latter sce nario could also result in repeated mergers, which would produce heavier black holes [36–38]. The spins of the black holes, and speciﬁcally the relative orientation of black hole spins in a binary, can be used to discriminate between these formation channels: formation in the ﬁeld is expected to result in spins which are nearly aligned with the orbital angular momentum (if tides are eﬃcient in spinning up the progenitors), while dynamical formation should not set any such preferential direction [39]. Unfortunately, it is often hard to measure the individual spins of black holes in binaries with any signiﬁcant precision [40–42]. While it is still possible to measure the relative occurrence of BBHs in the diﬀerent formation channels using the component spins and their orientation, hundreds of detections would be required before a ﬁrm measurement can be achieved [43–45]. While individual black hole spins (S) are diﬃcult to measure, a combination of the two spins called the eﬀective spin χeﬀ [46–49] is usually much better measured [1– 3, 13, 42, 50]. The eﬀective spin is the mass-weighted projection of the dimensionless spins of the components, χi = cSi/G, along the orbital angular momentum L: Formation channels that preferentially align the spins with the orbital angular momentum should thus have positive values of χeﬀ. This is not necessarily true for dynamically formed BBHs: since for those all black hole spins orientations are equally likely, the expected distribution for χeﬀ is centered around zero. The eﬀective spin can thus be used to infer the astrophysical origin of individual sources, and to reconstruct the overall population of black holes in binaries and of their progenitors [13, 51, 52]. Remarkably, all of the BBHs reported by the LVC to date are consistent with having small or zero χeﬀ [3] at 90% conﬁdence. The two sources for which the largest χeﬀ was measured are GW151226 (0.2+0.2 −0.1, median and 90% credible interval) and GW170729 (0.4+0.2 −0.3). Among the BBHs reported by Ref. [1, 2, 10], four signals have appreciable χeﬀ: GW151216, GW170403, GW170121 and GW170817B. Especially remarkable are the spins reported for GW151216 and GW170403, where GW151216 was reported as having a large and positive eﬀective spin of χeﬀ = 0.8+0.1 −0.2 [1], while χeﬀ = −0.7+0.5 −0.3 [2] was inferred for GW170403, making it the largest negative eﬀective spin BBH so far. The algorithms in Refs. [2, 53] were optimized to detect the faintest individually observable events in the population of merging BBHs, and hence several of the detected signals had relatively modest values of the signal-to-noise ratio (SNR). As the information content of observed signals scales with their SNR2, the data will have the least constraining power on the intrinsic parameters for faint events such as GW151216 and GW170403. In light of this fact, it is worth carefully considering the impact on the inferred parameters of the various analysis choices adopted in parameter estimation: the Bayesian priors, GW waveform model, and the treatment of the instrumental noise, in particular its power spectral density (PSD) [54]. For example, Ref [55] has shown how the χeﬀ measurement of the LVC detection GW151012 [3] is sensitive to the prior choice; while Ref. [56] has shown how the treatment of the noise PSD can impact the source characterization analysis. In this paper we analyze all of the BBHs reported by Refs. [1, 2]. We perform parameter estimation with the procedures discussed in [1, 2, 57], as well as the ones used by the LVC in the analysis of GWTC-1 [3]. We use three distinctive sets of analysis choices including method of PSD estimation, sampling algorithms, signal models, and prior assumptions for the source parameters. Our results underline that the speciﬁc conﬁguration of the analysis can have a signiﬁcant impact on the astrophysical inference of some of the BBHs detected to date, especially if they have low SNRs. In particular, we show that the high χeﬀ of GW151216 and GW170403 can be signiﬁcantly reduced depending on the spin priors used in the analysis. The tails of the distribution need to be interpreted with care and in the context of the analysis choices made, such as the method used to estimate the PSD and the length of data analyzed. For studies that build on the estimated parameter distributions for such sources, it is important to be aware of these analysis choices before interpreting the results.
Critical Fluctuations in Polymer Solutions: Crossover from Criticality to Tricriticality<|sep|>Almost thirty years ago, Michael E. Fisher published a review on phase transitions in ionic ﬂuids,1 in which he discussed a remarkable analogy between the demixing of polymer solutions at an inﬁnite degree of polymerization and a tricritical phase transition, as observed in a variety of systems, such as He3-He4 mixtures2,3 and metamagnets,4,5 with coupled order parameters.6–8 We illustrate this analogy in Fig. 1 by comparing the phase behavior in solutions of polystyrene in cyclohexane with a ferromagnet containing a non-magnetic impurity. In this paper, we describe the physics that governs the crossover between Ising-like asymptotic critical behavior and tricritical theta-point behavior in high-molecular-weight polymer solutions. Thermodynamic behavior of substances asymptotically close to their critical points (second-order phase transitions), follows the principle of critical-point universality: the microscopic details become unimportant if the correlation length of the order-parameter ﬂuctuations exceeds the range of intermolecular interactions.14–20 All ﬂuids and ﬂuid mixtures, simple and complex, belong to the threedimensional Ising-model universality class.16,19 Asymptotically, their thermodynamic properties are described by power laws with universal Ising-model critical
Mixture Proportion Estimation via Kernel Embedding of Distributions<|sep|>Mixture proportion estimation (MPE) is the problem of estimating the weight of a component distribution in a mixture, given samples from the mixture and component. Solving this problem happens to be a key step in solving several “weakly supervised” learning problems. For example, MPE is a crucial ingredient in solving the weakly supervised learning problem of learning from positive and unlabelled samples (LPUE), in which one has access to unlabelled data and positively labelled data but wishes to construct a classiﬁer distinguishing between positive and neg ative data (Liu et al., 2002; Denis et al., 2005; Ward et al., 2009). MPE also arises naturally in the task of learning a classiﬁer with noisy labels in the training set, i.e., positive instances have a certain chance of being mislabelled as negative and vice-versa, independent of the observed feature vector (Lawrence & Scholkopf, 2001; Bouveyron & Girard, 2009; Stempfel & Ralaivola, 2009; Long & Servido, 2010; Natarajan et al., 2013). Natarajan et al. (2013) show that this problem can be solved by minimizing an appropriate cost sensitive loss. But the cost parameter depends on the label noise parameters, the computation of which can be broken into two MPE problems (Scott et al., 2013a). MPE also has applications in several other problems like anomaly rejection (Sanderson & Scott, 2014) and crowdsourcing (Raykar et al., 2010). When no assumptions are made on the mixture and the components, the problem is ill deﬁned as the mixture proportion is not identiﬁable (Scott, 2015). While several methods have been proposed to solve the MPE problem (Blanchard et al., 2010; Sanderson & Scott, 2014; Scott, 2015; Elkan & Noto, 2008; du Plessis & Sugiyama, 2014; Jain et al., 2016), to the best of our knowledge no provable and efﬁcient method is known for solving this problem in the general non-parametric setting with minimal assumptions. Some papers propose estimators that converge to the true proportion under certain conditions (Blanchard et al., 2010; Scott et al., 2013a; Scott, 2015), but they cannot be efﬁciently computed. Hence they use a method which is motivated based on the provable method but has no direct guarantees of convergence to the true proportion. Some papers propose an estimator that can be implemented efﬁciently (Elkan & Noto, 2008; du Plessis & Sugiyama, 2014), but the resulting estimator is correct only under very restrictive conditions (see Section 7) on the distribution. Further, all these methods except the one by du Plessis & Sugiyama (2014) require an accurate binary conditional probability estimator as a sub-routine and use methods like logistic regression to achieve this. In our opinion, requiring an accurate conditional probability estimate (which is a real valued function over the instance space) for estimating the mixture proportion (a single number) is too roundabout. Our main contribution in this paper is an efﬁcient algorithm for mixture proportion estimation along with convergence rates of the estimate to the true proportion (under certain conditions on the distribution). The algorithm is based on embedding the distributions (Gretton et al., 2012) into a reproducing kernel Hilbert space (RKHS), and only requires a simple quadratic programming solver as a sub-routine. Our method does not require the computation of a conditional probability estimate and is hence potentially better than other methods in terms of accuracy and efﬁciency. We test our method on some standard datasets, compare our results against several other algorithms designed for mixture proportion estimation and ﬁnd that our method performs better than or comparable to previously known algorithms on most datasets. The rest of the paper is organised as follows. The problem set up and notations are given in Section 2. In Section 3 we introduce the main object of our study, called the C-distance, which essentially maps a candidate mixture proportion value to a measure of how ‘bad’ the candidate is. We give a new condition on the mixture and component distributions that we call ‘separability’ in Section 4, under which the C-distance function explicitly reveals the true mixture proportion, and propose two estimators based on this. In Section 5 we give the rates of convergence of the proposed estimators to the true mixture proportion. We give an explicit implementation of one of the estimators based on a simple binary search procedure in Section 6. We give brief summaries of other known algorithms for mixture proportion estimation in Section 7 and list their characteristics and shortcomings. We give details of our experiments in Section 8 and conclude in Section 9.
Stability of generalized Einstein-Maxwell-scalar black holes<|sep|>Stability is one of the most important measures in physics, telling us how likely a solution could exist for a long period. It discriminates cosmological solutions from transient situations. Electrovacuum black holes (BHs) in general relativity (GR) are stable. Indeed after the seminal work by Regge and Wheeler [1], where they proved partially the stability of the Schwarzschild black hole, Zerilli [2] provided the stability equations for the even-parity perturbations. Later, following the same approach, Moncrief [3, 4] derived the stability of the Reissner-Nordstrom black hole. But because of the impossibility to separate variables in the rotating case, Teukolsky [5, 6] developed a new approach which helped to study the linear stability of the Kerr black hole. Even if the Newman-Penrose formalism used by Teukolsky turned out to be extremely diﬃcult to implement for other rotating black holes, such as the Kerr-Newman, the metric approach to non-rotating solution has been widely used in various beyond GR models, see e.g. [7–9]. We see that stability is an essential analysis of any solution. On the other hand, thermodynamic stability is also widely studied. It is important to mention that (mechanical) stability should be a condition before any thermodynamic analysis. Indeed, if the black hole is mechanically unstable, any Hawking radiation would destabilize it and therefore render any thermodynamic analysis impossible. Finally we should also mention the interesting aspects of instabilities in nature which provide for example phase transitions. In this direction, any unstable black hole in an asymptotically AdS spacetime could be associated via the AdS/CFT to a phase transition. From a more astrophysical aspect, BHs turn out to be extremely useful to study models beyond GR, in a strong gravitational regime. As we know, BHs are extremely simple objects. They are described in the vacuum by mainly two parameters, the mass and the angular momentum (the electric charge being usually negligible). These parameters, which describe entirely the BH, can be measured, in particular via the frequency emitted during the last moments of BH mergers, the so-called quasinormal modes (QNMs). Any hairy BH could be discriminated against through these modes. QNMs are easily computed if their equation is known. The analysis performed in this paper will provide these equations. As we mentioned, in this paper, we address the problem of black hole stability for a generic class of theories beyond GR. Modifying Einstein’s theory of gravity is an easy game but building a new physical theory turns out to be complicated. Many modiﬁed gravity theories turn out to be highly constrained by the data, from the Brans-Dicke model to massive gravity à la dRGT. We should follow a simpler road. Could we deﬁne theories with suﬃcient generality? For that, some of the simple elements that we will consider are
LOFT as a discovery machine for jetted Tidal Disruption Events<|sep|>Mounting observational evidence is supporting a scenario where most galactic nuclei host supermassive black holes (SMBHs). Gas inﬂow from larger scales causes a small (∼ 1%) fraction of SMBHs to accrete continuously for millions of years and shine as quasars. Most of them are instead “quiescent”, accreting –if at all– at a highly sub-Eddington rate. Observationally, it is therefore hard to assess the presence and measure the mass of most of SMBHs, beyond the local universe. Occasionally, however, a sudden increase of the accretion rate may occur, if a large mass of gas, e.g. a star, falls into the tidal sphere of inﬂuence of the black hole and ﬁnds itself torn apart and accreted. We call these events “tidal disruption events” (TDEs). TDEs can result in a sudden rise of electromagnetic emission. They can reach the luminosity of a quasar but they are rare (∼ 10−5yr−1 per galaxy) and last several months, or years in soft X-ray Komossa (2002). The detection and study of these ﬂares can deliver other important astrophysical information, beyond probing the presence of a SMBH. After stellar disruption, part of the stellar material is accreted onto the black hole, causing a luminous ﬂare of radiation. If the star is completely disrupted, its debris is accreted at a decreasing rate of ˙M ∝ t−5/3 (Rees, 1988; Phinney, 1989). Therefore, TDEs allow us to study the formation of a transient accretion disc and its continuous transition through diﬀerent accretion states. The super-Eddington phase — which occurs only for SMBH masses M <∼ 107 M⊙ — is theoretically uncertain, but it may be associated with a powerful radiatively driven wind (Rossi & Begelman, 2009), that thermally emits ∼ 1041 − 1043 erg s−1, mainly at optical frequencies (Strubbe & Quataert, 2009; Lodato & Rossi, 2011). The disc luminosity (∼ 1044 − 1046 erg s−1), instead, peaks in the far-UV/soft X-rays (Lodato & Rossi, 2011). Of great theoretical importance would also be the possibility to observe the formation and evolution of an associated jet, powered by this sudden accretion episode. Furthermore, TDEs are signposts of supermassive binary black holes, as their lightcurves look characteristically diﬀerent in the presence of a second BH, which acts as a perturber on the stellar stream (Liu et al., 2009), as recently observed in a ﬁrst candidate event (Liu et al., 2014). Finally, we note that, in X-rays, TDEs represent a new probe of strong gravity, for instance tracing precession eﬀects in the Kerr metric. The ﬁrst TDEs were discovered in ROSAT surveys of the X-ray sky (e.g., Komossa & Bade, 1999; Grupe et al., 1999) — see Komossa (2002) for a review. Later, GALEX allowed for the selection of TDEs at UV frequencies (Gezari et al., 2009; Gezari et al., 2012); many of the most recent TDE candidates are now found in optical transient surveys (Van Velzen et al., 2011a; Cenko et al., 2012; Chornock et al., 2014). An alternative method to select TDE candidates is to look for optical spectra with extreme high-ionization and Balmer lines (Komossa et al., 2008; Wang et al., 2012).This “thermal” Spectral Energy Distribution (SED) of TDEs is believed to be associated with emission from the disc or the radiatively driven wind. Recently the Swift Burst Alert Telescope (BAT) instrument (Bloom et al., 2011; Burrows et al., 2011; Cenko et al., 2012), triggered two TDE candidates in the hard X-ray band. A multi-frequency follow-up from radio to γ-rays revealed a new class of non-thermal TDEs. It is widely believed that emission from a relativistic jet (Γ ≈ 2, Zauderer et al., 2013; Berger et al., 2012) is responsible for the hard X-ray spectrum (with power-law photon index α ∼ 1.6 − 1.8) and the increasing radio activity (Levan et al., 2011), detected a few days after the trigger. The best studied of the two events is Swift J1644+57 (Sw J1644 in short). The two main features that support the claim that Sw J1644 is a TDE are i) the X-ray lightcurve behaviour, that follows Lx ∝ ˙M ∝ (t −τ)−5/3 after a few days (τ ≈ 3 day) from the trigger, and ii) the radio localization of the event within 150 pc from the centre of a known quiescent galactic nucleus (Zauderer et al., 2011). In this White Paper, we use the Sw J1644 X-ray lightcurve (see next section) to estimate detection rate prospects for the proposed M4 mission LOFT (Feroci et al., 2012). We chose to focus on predictions for non-thermal TDEs, best suited to be a LOFT target (LOFT could also see the late corona hard X-ray emission in disk-dominated TDEs but only in the local Universe; Strubbe & Quataert, 2009). On board there are two main instruments: the Large Area Detector (LAD, 2-50 keV) optimally suited for X-ray timing studied thanks to the 8.5 m2 area peaking at ∼ 8 keV and the Wide Field Monitor (WFM) with a ∼ 4 sr FoV aimed at triggering follow-up observations with the LAD for a variety of sources (both galactic and extra-galactic). We consider both and give possible discovery and identiﬁcation strategies. Our conclusions can be summarized as follows: • the Wide Field Monitor can allow to serendipitously detect non-thermal TDEs, at a higher rates (between one and a few tens per year) than any currently planned survey missions. • the Large Area Detector can eﬀectively follow up and identify radio triggered candidates. In synergy with SKA in survey mode, LAD can in principle successfully repoint each radio candidate, and measure its lightcurve decay index and spectral properties. LAD, together with Athena, will be the best X-ray follow up instrument in the SKA era. The expected rate is between a few to a few hundreds per yr. • LOFT is a unique mission for jetted TDEs, because it can combine the beneﬁts of a WFM and a follow-up instrument with a wide energy range (up to 50 keV). The attractive prospect is that of building for the ﬁrst time a sample of well studied TDEs associated with non-thermal emission from jets. This will allow us to study disc-jet formation and their connection, in a complementary way with respect to other more persistent sources such as active galactic nuclei and X-ray binaries. In addition, jetted TDEs can uniquely probe the presence of supermassive black hole in quiescent galaxies well beyond the local universe.
Extended Birkhoff's Theorem in the f(T) Gravity<|sep|>Since the discovery of the accelerating expansion of the universe, people have made great eﬀorts to investigate the hidden mechanism, which also provides us with great opportunities to deeply probe the fundamental theories of gravity. As one of modiﬁed gravitational theories, the f(T) gravity is ﬁrstly invoked to drive inﬂation by Ferraro and Fiorini [1]. Later, Bengochea and Ferraro [2], as well as Linder [3], propose to use the f(T) theory to drive the current accelerated expansion of our universe without invoking the mysterious dark energy. The framework is a generalization of the so-called Teleparallel Equivalent of General Relativity (TEGR) which is ﬁrstly propounded by Einstein in 1928 [4] and maturates in the 1960s (For some reviews, see [5, 6]). Contrary to the theory of general relativity which is based on Riemann geometry involving only curvature, the TEGR is based on the so named Weitzenb¨ock geometry with the non-vanishing torsion. Owing to the deﬁnition of Weitzenb¨ock connection rather than the Levi-Civita connection, the Riemann curvature is automatically vanishing in the TEGR framework, which brings the theory a new name, Teleparallel Gravity. For a speciﬁc choice of parameters, the TEGR behaves completely equivalent to the Einstein’s theory of general relativity. Furthermore, by using the torsion scalar T as the Lagrangian density, the TEGR can give a ﬁeld equation with second order only, instead of the fourth or der as in the Einstein’s ﬁeld equation, and avoids the instability problems caused by higher order derivatives as demonstrated in the metric framework f(R) gravity models. Similar to the generalization of Einstein’s theory of general relativity to the f(R) theory (For some references, see [7–23]), the modiﬁed version of teleparallel gravity assumes a general function f(T) as the model Lagrangian density. Also, the f(T) theory can be directly reduced to the TEGR if we choose the simplest case, that is, f(T) = T. The Lorentz invariance and conformal invariance of the f(T) theory is also investigated [24, 25], with many interesting results presented. A class of f(T) models with diagonal tetrad are proposed in succession to explain the late-time acceleration of the cosmic expansion without the mysteriously so-called dark energy, and are ﬁtted the cosmological data-sets very well (e.g. [2, 3, 26–32]). Most of the previous works consider the f(T) gravity with diagonal tetrad ﬁeld only. Noting that the tetrad ﬁeld has sixteen components rather than ten as in the metric frame, there are more freedoms and more physical meaning from the extra uncertain six components. In our previous work [33], we have proved the validity of Birkhoﬀ’s theorem in f(T) gravity with a speciﬁc diagonal tetrad. In this letter, we study this issue more generally with also the oﬀ diagonal tetrad ﬁeld, and discuss the physical meaning in a more extended context. The Birkhoﬀ’s theorem is also called Jebsen-Birkhoﬀ theorem, for it was actually discovered by Jebsen two years before George D. Birkhoﬀ in 1923 [34, 35]. The theorem states that the spherically symmetric gravitational ﬁeld in vacuum must be static, with a metric uniquely given by
Dual-Scale Single Image Dehazing Via Neural Augmentation<|sep|>Visual signals are distorted in adverse weather conditions which are usually classiﬁed as dynamic (such as rain and snow) or steady (such as haze, mist, and fog) [1], [2]. This paper focuses on the haze issue. Due to the effect of light scattering through small particles accumulated in the air, hazy images suffer from contrast loss of captured objects [1], color distortion [3], [4], and reduction of dynamic range [5], [6]. Due to rain-streak accumulation in line of sight, haze is also an issue for heavily rainy images [2]. Existing computer vision relevant such as object detection algorithms might not perform well on hazy images, especially for those real-world images with heavy haze or rain. It is thus important to study single image dehazing. Single image dehazing is widely studied because of its broad applications. Two popular types of single image dehazing algorithms are model-based ones [4], [7], [8], [9], [10] and data-driven ones [11], [12], [13], [14], [15], [16]. The modelbased ones are on top of the Koschmieder’s law [17]. They can improve the visibility of real-world hazy images well regardless of haze degree but they usually cannot achieve high PSNR and SSIM values on synthetic sets of hazy images. On the other hand, the data-driven ones perform well on the * correspondence author Zhengguo Li and Haiyan Shu are with SRO department, the Institute for Infocomm Research, Singapore, 138632, (emails: {ezgli, hshu}@i2r.astar.edu.sg). Chaobing Zheng and Shiqian Wu are with the school of Information Science and Engineering, Wuhan University of Science and Technology, Wuhan 430081, China (e-mail: {zhengchaobing, shiqian.wu}@wust.edu.cn) synthetic sets while their performance could be poor for realworld hazy images, especially for those images with heavy haze [18], [19]. It is desired to have a single image dehazing algorithm which is applicable for both synthetic and real-world hazy images without sacriﬁcing visual quality. In this paper, a novel neural augmented single image dehazing algorithm is proposed through integrating the model-based and data-driven approaches. Same as the model-based methods in [4], [7], [8], [9], [10] and the data-driven ones such as [12], the atmospheric light and transmission map are required by the proposed algorithm to restore the haze-free image. Rather than using the model-based methods in [4], [8], [9], [10] or the datadriven methods in [12], [20], both the atmospheric light and the transmission map are ﬁrst estimated by using model-based methods, and then reﬁned by data-driven approaches. The atmospheric light is estimated by using the hierarchical searching method in [21] which is derived from the Koschmieder’s law [17]. The transmission map is initialized by using the dark direct attenuation prior (DDAP) in [10], and is subsequently processed by the haze line averaging (HLA) algorithm in [10] to alleviate morphological artifacts caused by the DDAP. The atmospheric light and transmission map are then reﬁned via data-driven approaches. Pre-trained deep neural networks (DNNs) can be served as fast solvers for complex optimization problems [22], [23]. Such an advantage of data-driven approaches is well utilized by our reﬁnement. Since it is very difﬁcult or even impossible to have a pair of a realworld hazy image and its corresponding haze-free image, the popular generative adversarial network (GAN) [24] is utilized to reﬁne the atmospheric light and transmission map. Both the generator and the discriminator are dual-scales and are based on the Laplacian pyramid hazy image model in [10]. They are different from the single-scale generator and discriminator in [25]. The atmospheric light and transmission map estimated by the model-based methods can be regarded as noisy atmospheric light and transmission map. Therefore, the main function of the proposed dual-scale reﬁnement is to reduce/remove the noise from the atmospheric light and transmission map estimated by the model-based methods. Based on this observation, the generator of the proposed GAN is constructed from the recursive residual group (RRG) in [26]. Deep neural networks (DNNs) are usually biased towards learning low-frequency functions [27]. Shortcut connections are thus adopted in the proposed GAN to preserve the highfrequency information in the dehazed image. The discriminator of the proposed GAN is based on the PatchGAN in [28]. To reduce the training cost, the proposed dual-scale GAN is trained by only using 500 hazy images which are generated from 500 realistic images in the realistic single image dehazing (RESIDE) datasets in [29]. Depth is estimated by using the algorithm in [30]. The hazy images are synthesized by using the Koschmieder’s law [17]. This is different from the training of the model-based deep dahazing algorithm in [25]. Besides the 500 synthesized hazy images, 500 hazy images from the multiple real-world foggy image defogging (MRFID) dataset in [19] are also utilized to train the algorithm in [25]. Existing data-driven dehazing algorithms such as [14], [15], [33], [34] are trained by using more than 10K hazy images. Four loss functions are applied to train the proposed dual-scale GAN. Two single-scale loss functions are deﬁned by using an extreme channel [25] and the gradients of the restored and ground-truth images, respectively. Besides them, the dual-scale adversarial loss function [24] and dual-scale l1 loss function are also utilized to train the proposed GAN. It is worth noting that the adversarial loss function usually produces sharper images but with lower PSNR and SSIM values than the l1 and l2 loss functions. Since the haze-free image is restored by the Koschmieder’s law, the high visual quality property from model-based approaches is well preserved. The model-based estimation and the data-driven reﬁnement form a neural augmentation [23], [31], [32]. Theoretical analysis in the appendix indicates that the proposed neural augmentation framework converges faster than the corresponding data-driven approach. Experimental results show that the proposed algorithm is applicable to synthetic and real-world hazy images, and outperforms existing data-driven dehazing algorithms for the real-world hazy images from the dehazing quality index (DHQI) [35] and fog aware density evaluator (FADE) [36] points of view. Overall, four contributions of this paper are: 1) a novel neural augmentation which combines model-based and data-driven approaches for single image dehazing. The framework is applicable to synthetic and real-world hazy images. The number of training data can also be reduced signiﬁcantly; 2) a new initiative on analyzing the neural augmentation theoretically by leveraging control theory [37], [38]. The analysis indicates that the neural augmentation framework converges faster than the corresponding data-driven approach if the model-based method is accurate. 3) generator and discriminator on top of a dual-scale dehazing algorithm which can preserve the high-frequency information of the restored haze-free image better than that on top of a singlescale dehazing algorithm; and 4) new loss functions for the deep-learning based single image dehazing. The remainder of this paper is organized as below. Relevant works on single image dehazing are summarized in Section II. Details of the proposed algorithm are presented in Section III. Experimental results are provided to verify the proposed algorithm in Section IV. Finally, conclusion remarks are provided in Section V.
Hydrogen bond analysis of confined water in mesoporous silica using the reactive force field<|sep|>Understanding the structural and dynamical properties of water in conﬁnement is essential to many scientiﬁc ﬁelds and technological applications, such as permeation in the ion channels of a biological membrane, capacitance of an electrical double layer in fuel cells, and controlled drug release [1–4]. Mesoporous materials have attracted attention in recent years because their high speciﬁc surface area, large speciﬁc pore volume, and narrow pore size distribution allow a wide range of possible industrial applications. In particular, mesoporous silicate MCM-41, which has an amorphous structure, is one of the most widely used mesoporous material [5] and has potential application in drug delivery [6–8], in which water plays an important role [4]. In conﬁned systems, surface properties have a signiﬁcant impact on the physical properties of the conﬁned substance. However, water behavior near an amorphous surface is much more diﬃcult to investigate than that near a crystalline surface because the topology of an amorphous surface is ill-deﬁned at the molecular level. The behavior of conﬁned water has been widely studied by both theoretical [9–25] and experimental [26–31] techniques in the past few decades. These works have indicated that the properties of conﬁned water may strongly diﬀer from that of bulk water depending on surface properties and size of conﬁnement space. It is thus important to properly evaluate the inﬂuence of conﬁnement on water. In recent years, the dynamical structure factor of water in mesoporous silica has been measured by quasi-elastic neutron scattering (QENS), and the self-diﬀusion coefﬁcients (D) of water in fast and slow modes were evaluated by dividing the space into two regions. Yamada et al. reported that D signiﬁcantly diﬀers between the vicinity of the interface and pore center [32]. However, the method of dividing the space is empirical, and its relationship with the microscopic atomic structure and dynamics is not suﬃciently conﬁrmed. This question is expected to be resolved through the analysis of trajectories from molecular dynamics (MD) simulations. In MD simulations, the choice of force ﬁeld is very important in reasonably estimating the interfacial eﬀect. Recently, Bourg and Steefel reproduced the experimental diﬀusion constant obtained by QENS through MD simulations using the Clay force ﬁeld (CLAYFF) and extended simple point charge (SPC/E) water model [17]. They noted that the use of an ab initio method [33–35] or the reactive force ﬁeld (ReaxFF) in MD simulations allows a good description of proton transfer reactions. van Duin and co-workers developed the ReaxFF package [36], which has been applied in various research areas [37]. In recent years, MD simulations of the silica/water interface have been performed using ReaxFF [38–42]. This method has a lower computational cost than quantum-mechanics-level calculations such as ab initio MD simulations. A drawback of MD simulations using classical force ﬁelds is that an appropriate chemical structure must be created before starting the simulation, and this does not change during the simulation because the chemical bond is ﬁxed. This problem is particularly conspicuous when dealing with interfacial systems. Using ReaxFF, the physical properties of conﬁned water can be obtained with high accuracy while avoiding the problem of interaction between water and silica. In this study, we performed MD simulations of water in mesoporous silica using ReaxFF and analyzed water dynamics to clarify the inﬂuence of the surface on the physical properties of water at the atomic and molecular levels. Speciﬁcally, the hydrogen bond (H-bond) dynamics in the inhomogeneous system composed of amorphous silica (a-SiO2) and water was predicted. To obtain suﬃcient statistics, we used a large system since the pore was analyzed by dividing it into thin layers. The remainder of this paper is organized as follows. The next section provides an overview of ReaxFF potentials and details of the preparation of the silica-water system. Section 3 presents the structural properties and diﬀusion of water in nanoporous silica and discusses the results of the analyses of H-bond conﬁgurations and dynamics. The ﬁnal section summarizes the main ﬁndings and consequent conclusions.
Statistical properties of magnetic structures and energy dissipation during turbulent reconnection in the Earth's magnetotail<|sep|>Magnetic reconnection is a process by which the topology of the magnetic ﬁeld within a plasma is altered, allowing for the rapid conversion of magnetic energy into kinetic energy (Parker, 1957). It is responsible for the penetration of solar wind plasma into the magnetosphere (Russell & Elphic, 1978), and plays an important role in powering solar ﬂares and coronal mass ejections (Sweet, 1969; Lin & Forbes, 2000). When reconnecting current sheets are suﬃciently stretched to have large aspect ratios, plasmoids are expected to form via the tearing mode instability (N. Loureiro et al., 2007), leading to the multi-scale evolution of fast reconnection (e.g. Shibata & Tanuma, 2001; Bhattacharjee et al., 2009) across space and astrophysics including Earth’s magnetotail (Ji & Daughton, 2011). In the latter case, plasmoids have been observed via the ISEE-3 and GEOTAIL satellites over an extended period of time (Baker et al., 1984; Hones Jr et al., 1984; Richardson et al., 1987; Moldwin & Hughes, 1992; Nagai et al., 1994; Ieda et al., 1998; Slavin et al., 2003) and more recently by the CLUSTER mission on the ion scales (e.g. Chen et al., 2008; Chen et al., 2012). Plasmoids are also routinely seen in kinetic simulations (e.g. Daughton et al., 2006; Drake et al., 2006). Therefore, a thorough analysis of the structures present in a reconnecting current sheet can shed light on the dynamics of fast reconnection, which in turn aﬀect the global dynamics of the magnetosphere. An important feature of magnetic reconnection is the dissipation of magnetic energy to plasma particle energy through J · E where J and E are current density and electric ﬁeld, respectively. There is an ongoing debate about whether the component of J ·E along or across the local magnetic ﬁeld, expressed as J∥E∥ and J⊥ ·E⊥, respec tively, is the primary source of particle energization (e.g. Drake & Swisdak, 2014; Yamada et al., 2018; Fox et al., 2018; Pucci et al., 2018). Furthermore, whether the dissipation within the localized reconnection structures is signiﬁcant (e.g. Egedal et al., 2012) or can be ignored (e.g. Drake et al., 2019) in a large system is still unclear. From the same MMS data used in this Letter, Ergun et al. (2018) found that the main positive contributor to the overall J·E was J⊥·E⊥ at frequencies at or below the ion cyclotron frequency, but did not examine the spatial correlation between energy dissipation and magnetic structures. Therefore, a detailed statistical study of magnetic dissipation, including the decomposition into parallel and perpendicular components within and outside of the magnetic structures can provide insight on these ongoing debates. Many analytic and numerical studies have characterized possible size distributions of secondary islands in various regimes (Uzdensky et al., 2010; Fermo et al., 2010, 2011; N. F. Loureiro et al., 2012; Y.-M. Huang & Bhattacharjee, 2012; Takamoto, 2013; Guo et al., 2013; Lingam & Comisso, 2018; Petropoulou et al., 2018). Many of these studies have used Magnetohydrodynamic (MHD) models that are not generally applicable to kinetic scale plasmoids. However, the model developed by Fermo et al. (2010) is statistical in nature, and therefore can potentially be applied in a multiscale fashion. It postulates that plasmoids start small, then grow in size both by expansion and by plasmoid merging, leading to a smooth energy spectrum via an inverse-cascade (Nakamura et al., 2016). A characteristic of the model of Fermo et al. (2010) is that for suﬃciently large size (represented as a characteristic length scale), the number of plasmoids present in a reconnecting current sheet decreases exponentially with increasing plasmoid size. Studies have determined plasmoid size scalings in experimental plasmas (Dorfman et al., 2014; Olson et al., 2016), in solar plasmas via ex-situ methods (Guo et al., 2013), and in space plasmas via in-situ methods (Fermo et al., 2011; Vogt et al., 2014; Akhavan-Tafti et al., 2018). In-situ studies provide more detailed information on each plasmoid, but no insitu study thus far has utilized structures present in only a single turbulently reconnecting region. Plasma conditions varied considerably between each observation and introduced unquantiﬁed uncertainties to the observed scaling. An analysis of the distribution of structures within a single turbulently reconnecting current sheet is desirable and necessary for accurately quantifying the plasmoid size scaling. The most common type of plasmoid observed in the magnetosphere is the ﬂux rope, which is a helical magnetic ﬁeld structure with a strong core ﬁeld and an enhancement of the total magnetic ﬁeld. Flux ropes have been extensively studied in space, and models of cylindrical force-free (Elphic & Russell, 1983) and non-force-free (Lundquist, 1950; Lepping et al., 1990) ﬂux ropes are widely used. Flux ropes have been observed with complex internal structures (Stawarz et al., 2018), including enclosed waves (Wang et al., 2016) and ongoing magnetic reconnection (Øieroset et al., 2016). Various other plasmoids have been observed in the magnetotail current sheet that do not have the typical cylindrical structure, including ﬂattened ﬂux ropes (Sun et al., 2019) and plasmoids which have loop-like ﬁeld lines rather than helical (Zhang et al., 2013). These non-ideal plasmoids are indicative of the dynamic nature of magnetic reconnection. In a turbulent region, plasmoids may experience external forces which could slow or prevent their evolution into ideal cylindrical states. Therefore, for a turbulently reconnecting current sheet, in order to get a comprehensive survey of the plasmoids present, it is necessary to search for plasmoids that do not necessarily ﬁt the ideal cylindrical ﬂux rope model. Another question for a statistical survey of plasmoids is whether to identify the plasmoids ‘by eye’, or to attempt an automated detection method. Automated methods are more rigorously deﬁned and repeatable, and thus are less susceptible to human sources of bias. For example, methods have been developed to automatically detect ﬂux ropes in satellite data (Smith et al., 2017; S. Huang et al., 2018). These methods are repeatable, rigorous, and calculate valuable parameters such as the spacecraft’s distance of closest approach to the center of the ﬂux rope and the ﬂux rope’s radius. However, both meth ods are based on cylindrical ﬂux rope models, force-free (Lundquist, 1950) and non-forcefree (Elphic & Russell, 1983) respectively. These methods will not be suitable in a dynamic turbulent reconnection region which is likely to have large numbers of plasmoids which do not ﬁt cylindrical ﬂux rope models and unlikely to have obvious quiescent magnetic ﬁeld backgrounds to compare the magnetic ﬁeld ﬂuctuations against. Therefore, an automated method has been developed to detect non-ideal plasmoids, as well as current sheets resulting from two diﬀerent physical processes. This method has been used to probe the structure, dynamics, and dissipation of a turbulently reconnecting current sheet observed in the magnetotail.
Heavy quark potential and jet quenching parameter in a D-instanton background<|sep|>One main purpose of the heavy-ion collision experiments is to explore the properties of the new state of matter created through collisions. The experiments at RHIC and LHC have produced a new state of matter so-called ”strong quark-gluon plasma(sQGP)” [1–3]. Thus, non-peturbative techniques are required such as the AdS/CFT correspondence [4–6]. AdS/CFT, the duality between the type IIB superstring theory formulated on AdS5 × S5 and N = 4 SYM in four dimensions, has yielded many important insights into the dynamics of strongly-coupled gauge theories. In this approach, many quantities such as the heavy quark potential and the jet quenching parameter can be studied. The heavy quark potential is an important quantity which can be related to the melting of heavy quarkoniums, one of the main experimental signatures for sQGP formation. The ﬁrst calculation of the heavy quark potential for N = 4 SYM at zero temperature was carried out by Maldacena [7]. It was observed that for the AdS5 space the energy shows a purely Coulombian behavior, agrees with a conformal gauge theory. This work has attracted lots of interest. After [7], the heavy quark potential in the context of AdS/CFT has been investigated in many papers. For example, the potential for N = 4 SYM at ﬁnite temperature has been discussed in [8, 9]. The potential for diﬀerent spaces is investigated in [10]. The sub-leading order corrections to this quantity are considered in [11] and [12]. For study of the potential in some AdS/QCD models, see [13–15]. Other important results can be found, for example, in [16–19]. Another important quantity sensitive to the in-medium energy loss is jet quenching parameter ˆq(or transport coeﬃcient). This quantity describes the average transverse momentum square transferred from the traversing parton, per unit mean free path [20, 21]. The jet quenching parameter for N = 4 SYM theory was ﬁrst proposed by H.Liu et al in their seminal work [22]. Interestingly, the magnitude of ˆqSY M turns out to be closer to the value extracted from RHIC data [23, 24] than pQCD result for the typical value of the ’t Hooft coupling, λ ≃ 6π, of QCD. After [22], there are many attempts to address the jet quenching parameter from AdS/CFT. For instance, the sub-leading order corrections to ˆq due to worldsheet ﬂuctuations has been discussed in [25]. Charge eﬀect and ﬁnite ’t Hooft coupling correction on the ˆq is investigated in [26]. The ˆq in medium with chemical potential is studied in [27–29]. The jet quenching parameter in STU background is analyzed in [30]. Investigations are also extended to some AdS/QCD models [31, 32]. Other related results can be found, for example, in [33–39]. Actually, there is another check of gauge/gravity duality, the correspondence between non-perturbative objects such as instantons. It was argued [40, 41] that the Yang-Mills instantons are identiﬁed with the D-instantons of type IIB string theory. The near horizon limit of D-instantons homogeneously distributed over D3-brane at zero temperature has been discussed in [42]. The holographic dual of uniformly distributed D-instantons over D3-brane at ﬁnite temperature has been investigated in [43]. It is shown that the features of D3-D(-1) conﬁguration is similar to QCD at ﬁnite temperature. Therefore, one can expect the results obtained from this theory should shed qualitative insights into analogous questions in QCD. In this paper, we are going to study the heavy quark potential and the jet quenching parameter in a D-instanton background. We will investigate the eﬀect of the instanton density on these two quantities. Moreover, we would like to compare the results with those of conformal cases and experimental data. This is the purpose of the present work. This paper is organized as follows. In the next section, the background geometry of D3-D(-1) brane conﬁguration at ﬁnite temperature is brieﬂy reviewed. In section III, we investigate the heavy quark potential in this background. Then we study the jet quenching parameter in this background in section IV. The last part concludes the paper along with some discussions of the results.
The weighted Tower of Hanoi<|sep|>The Tower of Hanoi puzzle was introduced to the world by Edward Lucas in 1884 [10], since then those interested in the puzzle have created new variants in order to raise its diﬃculty level and to study it more, sometimes by increasing the number of pegs [4,6,12] or considering an arbitrary initial and ﬁnal states [8] (which can be considered as a generalization) and at other times, by forbidding certain movements of discs between certain pegs [3,11,12] (which is a kind of restriction of the problem), while others have chosen to allow the discs to be placed on top of smaller discs [13] (which is considered a relaxation of the problem). For more on the Tower of Hanoi problem, we point the interested reader to [8]. The original Lucas’s Tower of Hanoi problem is stated as follows: Consider three pegs, source peg, intermediate peg and destination peg; and n ∈ N0 discs of distinct diameters. Initially, all discs are stacked on the ﬁrst peg (the source) ordered by their diameters, with the smallest one on top and the largest one on the bottom. The goal is to transfer the n discs to the third peg (the destination) using the minimum number of moves, and respecting the following rules: Solving the problem requires exactly 2n − 1 moves, which can be shown to be optimal [8]. However, there has been continued interest in the problem from several viewpoints and not just the minimum number of moves. The Tower of Hanoi has its applications in many ﬁelds such as didactic of mathematics, psychology, industry and logistics. The Tower of Hanoi and its variants are used to introduce the concept of mathematical induction to students in computer science, and discrete mathematics. The Tower of Hanoi is a model commonly used in cognitive psychology and neuropsychology to study and examine problem-solving skills which can be tested by calculating moves and strategies while predicting possible outcomes, we reefer to [2, 5, 9] for more information about the Tower of Hanoi applications in psychology. The Tower of Hanoi puzzle can be used to model a class of logistic problems which called Pile problems [7], in [1] authors discussed the application of Towers of Hanoi in logistics management and in Pile problems in particular. However, the Tower of Hanoi can be seen as a scheduling problem, where the hand playing with discs is a machine or a crane in a big harbor and discs are the containers in the harbor. Only three zones in the harbor are used to stack containers which represents the three pegs, the crane move the containers from a zone to another while respecting the rules (i−iii) of the Tower of Hanoi, where the discs in this case are the containers of diﬀerent sizes. Given an initial state of the containers in harbor, the crane is asked to move containers to reach another state while minimizing the number of moves of containers between the three zones, this problem is an optimization problem. In order to expand research on the Tower of Hanoi problem we present in this paper a new optimization problem which is a new generalization of the Tower of Hanoi problem that has never been considered before in the literature, in which the movements of the discs between the pegs are weighted using a deﬁnite weighting function of the set of pegs in the set of positive reals R+, which makes the problem less trivial than the classical version of the problem. We call this new problem by the weighted Tower of Hanoi, this optimization problem models the harbor problem mentioned before more eﬀectively, because it is more realistic to assume that a movement of a container from a zone to another have a cost, so the goal here is to minimize the total cost and not the number of moves.
A matter of time: Using dynamics and theory to uncover mechanisms of transcriptional bursting<|sep|>67. Devilbiss, F. & Ramkrishna, D. Addressing the Need for a Model Selection Framework in Systems Biology Using Information Theory. Proceedings of the IEEE 105, 330–339 (2017). 68. Bowman, G. D. & Poirier, M. G. Post-translational modiﬁcations of histones that inﬂuence nucleosome dynamics. Chemical Reviews 115, 2274–2295 (2015).
Focal-plane wavefront sensing with photonic lanterns I: theoretical framework<|sep|>High-contrast imaging is becoming one of the primary tools for the direct detection and characterization of exoplanets. This class of techniques combines ground-based extreme adaptive optics (AO), which corrects for wavefront aberrations induced by passage of light through the atmosphere and the instrument, and coronagraphy, which suppresses on-axis starlight to reveal the circumstellar environment, as well as contrast-boosting post-processing techniques such as angular diﬀerential imaging [2] and spectral diﬀerential imaging [3]. Together, these techniques enable contrasts down to ∼ 10−6 and angular separations down to 200 mas. So far, some 30 exoplanets have been detected through high-contrast imaging techniques [4]; however, almost all are widely separated gas giants with masses several times that of Jupiter. One of the main roadblocks in increasing current sensitivity are non-common-path aberrations (NCPAs): quasi-static aberrations evolving on the timescale of minutes to hours that occur due to instrument instabilities induced by humidity, temperature, and gravity vector changes [5,6]. Because these aberrations appear downstream from the wavefront sensor, they cannot be removed via typical pupil-plane wavefront control systems. As a result, wavefront control must be improved before Fig. 1. The photonic lantern, a tapered waveguide that can adiabatically transfer light distributed between multiple ﬁber modes into multiple single-mode cores, or vice-versa. The particular lantern shown above is an idealization of a 3-port lantern, with all jacket, cladding, and core cross-sections assumed to be circular throughout the transition zone. Darker regions indicate higher refractive index. Adapted from [8]. instruments can attain the necessary contrasts and angular separations typical for systems similar to the Sun and Earth: ∼ 10−10 and ∼ 100 mas, at a distance of 10 pc, in visible light [7]. One way forward is to sense wavefront aberrations in the ﬁnal focal plane with the science camera, so that sensor and science light travel down the same optical path. This approach, known as focal-plane wavefront sensing (FPWFS), removes NCPAs. In parallel, a number of new ideas and techniques are being proposed to further advance direct exoplanet characterization. One development is in short-exposure exoplanet imaging, which leverages statistical diﬀerences in planet and star speckle behavior at millisecond timescales to distinguish between planet light from starlight [9,10]. This technique is distinct from ADI and SDI. Coherent detection, which exploits the incoherence of planet light, presents an alternative pathway for separating planet light and starlight. A related technique is nulling interferometry, an alternative to conventional coronagraphy that can achieve smaller inner working angles, and which works by destructively interfering starlight collected from diﬀerent subapertures or telescopes. Other advances in direct characterization will need to be made not in the isolation of planet light, but the spectral analysis of that light. The high-resolution spectral analysis of faint objects like exoplanets will require methods for both the eﬃcient coupling of light into the science instrument, and stabilization of that same light, which will vary with time due to passage through the atmosphere and instrument. These two requirements are typically in tension [8], and thus hard to achieve simultaneously. The photonic lantern (PL; [11]) provides a capable platform for the above applications; other notable applications include OH line suppression through ﬁber Bragg gratings [12, 13], and spectroastrometry [14]. As seen in Figure 1, the PL is a tapered waveguide that gradually transitions from a few-mode optical ﬁber (FMF) geometry to multiple widely-spaced single-mode cores, similar to a multi-core ﬁber (MCF), which can then be fanned out to an array of single-mode ﬁbers (SMFs). When the FMF end is placed in the focal plane, the PL can eﬃciently couple multi-modal telescope light into multiple SMFs. While PLs come in a wide array of port counts and geometries, they can be largely classiﬁed into three groups. In what we call the “standard” PL, embedded cores are uniform in structure and refractive index. At the other extreme, “mode-selective” PLs use diﬀering single-mode core radii or index contrasts, so that each ﬁber mode at the FMF-like lantern entrance routes to a distinct output port [15]. Lastly, we term lanterns that operate between these two extremes “hybrid lanterns.” These lanterns have one core mismatched from the rest, thereby funnelling light from the fundamental ﬁber mode into a single output port while mixing the remaining light in the rest of the ports. This concept is similar to the “mode-group selective” lantern, introduced in [16]. Critically, in the process of coupling light into an array of SMFs, PLs map phase aberrations into intensity variations in a one-to-one manner, at least for small aberrations. This behavior enables the PL to additionally act as a 100% duty cycle focal-plane WFS [17–19]. Because PLs have a limited number of outputs (set by the manufacturing process, though PLs with up to 511 modes have been reported [11]), these devices as of now can only give low-order wavefront information. Therefore, while PLs are well-suited to sense low-order aberrations like NCPAs [20] and island modes [21], they are not a standalone WFS solution in XAO systems, which correct upwards of 1000 modes. In such applications, PLs will likely need to work in tandem with pupil-plane sensors like the Shack-Hartmann or pyramid WFS. We show an example of this phase-to-intensity mapping in Figure 2, which plots the nondegenerate intensity responses of a 6-port PL in the presence of positive and negative astigmatism. The focus of this work is to assess the performance of the photonic lantern wavefront sensor (PLWFS), in contexts like instrument coupling or coherent detection where PLs are already being considered for use. In these scenarios, the utility of the PL is doubled, enabling both the aforementioned non-WFS applications as well as focal-plane wavefront sensing. We focus on two contexts the ﬁrst being ﬁber-fed, high-resolution spectrometry, mentioned above; and vortex-ﬁber nulling (VFN), a high contrast imaging technique which exploits symmetries in optical ﬁber modes to separate star and planet light [22]. In turn, we restrict our analysis to the infrared, since this wavelength regime will be the staging ground for the next push in direct exoplanet spectrometry, with upcoming instruments such as HISPEC and MODHIS [23]. Research in PL wavefront sensing is ongoing. For instance, [18] recently combined a 19port PL with a neural net to enable nonlinear wavefront reconstruction of the ﬁrst 9 non-piston Zernike modes. In comparison, we take a broader, but less in-depth approach: our goal is to provide a general baseline overview of the capabilities of the PLWFS, as well as the methods through which the sensing properties of these devices might be controlled. We place added emphasis on the linear analysis of the PLWFS, in order assess the limits of the PLWFS under more standard and simplistic linear AO control schemes. In Section §2, we establish the math that will enable wavefront reconstruction with the PLWFS. To begin, we present power series expansions for the PLWFS intensity response to ﬁrst and second order in phase (§2.1-§2.3). We also consider methods through which these models can be inverted, thereby enabling wavefront sensing. Next, we expand our models to arbitrary modal basis (§2.4): this both increases computational eﬃciency of the reconstruction models and allows them to be expressed in terms of common phase aberration bases such as the Zernike polynomials. In Section §3, we apply our models to quantify the behavior of the PLWFS. This analysis includes deriving conditions for WFS linearity (§3.1-§3.3), and estimating maximum amount of WFE that can be handled by these sensors (§3.4). Finally, we combine our models with numerical simulations, to provide a ﬁrst look at the wavefront-sensing abilities of a standard, hybrid, and mode-selective 6-port PL. Our aim in this work is to develop an initial understanding of the capabilities of the PLWFS, and in doing so we assume “perfect” lanterns and neglect noise (though we provide some reference to noise propagation in the linear regime in §2.2). We present an overview of our numerical method in §4, and the corresponding results in §5. In a companion paper [1], we extend these simulations to Fig. 2. Simulated response of a 6 port lantern in the presence a: -1 rad rms astigmatism; b: 0 rad rms astigmatism; and c: +1 rad rms astigmatism. The photonic lantern converts phase variations into unique intensity variations among the output cores. Circles show the jacket-cladding interface and the cladding-core interfaces. Optical propagation is simulated using the Python packages HCIPy and Lightbeam. cover a range of PLWFS conﬁgurations beyond the 6-port geometries considered in this paper, in order to establish a rough baseline of the sensing abilities of PLWFSs. There, we also investigate potential strategies through which PLWFS performance can be further controlled and optimized.
Soft vortex matter in a type-I/type-II superconducting bilayer<|sep|>Soft matter physics deals with systems as diﬀerent as colloids, polymers, gels, glasses, liquid crystals and others, where one common feature is their self-organization into very rich mesoscopic phases.1 To model this behavior, one often uses a pairwise inter-particle interaction possessing several length scales and/or mixture of attraction and repulsion.2–7 Such interaction potential, as a function of the particle density, indeed leads to the formation of clusters, particle chains, labyrinthal gel-like structures and (almost) regular lattices. This in turn questions the known analogy between charged colloids and vortices in superconductors, since the latter typically repel and form a triangular (Abrikosov) lattice. On the other hand, type-I superconductors are known to exhibit lamellar and labyrinthal ﬂux patterns, which lose distinction of individual vortices but resemble the soft-matter structures in their macroscopic shape.8 With this in mind, we here investigate magnetic ﬂux patterns in a coupled bilayer of two superconducting ﬁlms - one type-I and one type-II, under perpendicular magnetic ﬁeld (see Fig. 1), in attempt to reveal unique vortex phases. In addition to the crystalline vortex lattice, one now envisages vortex ﬂocculation, gelation and glassy phases, some similar to vortex matter encountered in high-temperature,9 multiband,10 and other unconventional superconducting11 and superﬂuid systems.12 The ﬁlm geometry is chosen for an easy realization in experiment, but also in order to have asymptotic long-range 1/r repulsion between vortices13 - similar to the electrostatic Coulomb interaction in charged colloids. We will show that the complexity of the obtained patterns stems from the changes in the short- and mid-range interaction between vortices, whose relative strength depends on the parameters of the layers, especially their coherence length ξ and penetration depth λ, but also their thicknesses, electronic coupling between them, and chosen temperature with respect to their individual critical temperatures. FIG. 1: The oblique view of the considered bilayer sample. The two superconducting layers are separated by an ultrathin oxide/insulating layer. The magnetic ﬁeld is applied in the direction perpendicular to the layers (along z-axis). The paper is organized as follows. In Section II, we present the theoretical formalism. Section III summons and classiﬁes the observed magnetic ﬂux patterns, which are further characterized using radial distribution function in Section IV. Further we discuss the inﬂuence of temperature in Section V, where we also show the behavior of the heat capacity and its changes aﬃliated with diﬀerent ﬂux phases. Our results are summarized in Section VI.
Quantum-Reduced Loop Gravity: Cosmology<|sep|>The realization of a quantum theory for the gravitational ﬁeld must provide an explanation to the current puzzles of General Relativity (GR), i.e. the presence of mathematical singularities. These singularities have been shown to be unavoidable in some symmetry reduced models describing relevant physical situations, such as the collapse of standard matter and the beginning (eventually also the end) of the Universe evolution [1]. Hence, it is demanded to a quantum formulation of gravity to answer to the questions posed by the unpredictability of GR in these cases. Loop Quantum Gravity (LQG) [2, 3] constitutes the most advanced model which pursues the quantization of geometric degrees of freedom. It is based on a canonical quantization a la Dirac of the holonomy-ﬂux algebra associated with Ashtekar-Barbero variables [4] in the Hilbert space of distributional connections. One ﬁrst deﬁnes a kinematical Hilbert space in which the Gauss constraint is then solved. The resulting basis elements are the so-called spinnetworks: these are labeled by graphs Γ and belong to L2(SU(2)E/SU(2)V ), E and V being the total number of edges and vertexes of Γ, respectively. The invariance under diﬀeomorphisms is then implemented by summing over the orbit of the associated operator, which gives the so-called s-knots [5]: these are distributional states representing the equivalence class of spinnetworks under diﬀeomorphisms. In the space of s-knots the superHamiltonian operator can be regularized [6, 7] and thanks to diﬀ-invariance the regulator can be safely removed leading to an anomaly free quantization of the Dirac algebra. However, particularly in view of the presence of the volume operator [8, 9], the explicit analytical expression for the matrix elements of the superHamiltonian and the properties of the physical Hilbert space are still elusive. For these reasons other approaches such as the master constraint program [10] or the more recent deparametrized system in terms of matter ﬁelds [11] have been introduced in the canonical framework. Cosmology is a natural arena to test the theory and its dynamics due to the high degree of symmetry of the conﬁguration space. The cosmological implementation of LQG has been realized in the framework of Loop Quantum Cosmology (LQC) [12, 13] (see [14–16] for alternative proposals). This is based on the implementation of a minisuperspace quantization scheme, in which the phase space is reduced on a classical level according with the symmetries of the model. Because the Universe is described by a homogeneous (and eventually isotropic) space-time manifold, the resulting conﬁguration space is parametrized by three spatial-independent variables. These variables describe the connections and the momenta of the reduced model after a gauge-ﬁxing of both the SU(2) gauge symmetry and diffeomorphisms invariance has been performed. As a consequence, the regularization of the superHamiltonian operator can be accomplished by ﬁxing an external parameter ¯µ related with the existence of an underlying quantum geometry [17] (see [18] for a critical discussion on the regularization in LQC). The resulting theory is a well established research ﬁeld with several remarkable features and physical consequences, the main ones being a bounce replacing the initial singularity [17, 19–22], the generation of initial conditions for inﬂation to start [23, 24] and the prediction of peculiar eﬀects on the cosmic microwave background radiation spectrum [25–30] (see also [31–33]). However LQC has not yet been shown to be the cosmological sector of LQG and in order to solve the tension between the regularization procedures of the two theories, new approaches have been recently envisaged in order to provide an alternative deﬁnition of the superHamiltonian operator in the full theory (see [34] that bring it closer to the ¯µ scheme of LQC). In this paper, we give a detailed presentation of the procedure introduced in [35], in which we adopt the opposite view-point assuming LQG as the correct theory obtained by quantizing GR and then we look for its cosmological sector imposing a symmetry-reduction at the quantum level. This way we construct a theory in which we ﬁrst quantize and then reduce instead of ﬁrst classically reducing and then quantizing as it’s usually done in LQC. This approach is not expected to work only in cosmology, but it can be extended also to other symmetric sectors of the theory. This way, we deﬁne a new framework for the analysis of the implications of LQG in relevant (symmetry-reduced) physical cases (Quantum-reduced Loop Gravity). Our cosmological quantum model will then be a proper truncation of the full kinematical Hilbert space of LQG. The virtue of our approach mainly consists in the possibility to realize a fundamental description of a cosmological space-time, which ﬁlls the gap with the full theory and on which Thiemann’s regularization procedure for the superHamiltonian [6] can be applied. The paper is organized as follows: In sections II we quickly review the main tools of the LQG quantization of GR, while in section III the homogeneous Bianchi models are presented and the LQC framework is shortly discussed. Then in section IV we perform a classical analysis and we outline how, by considering a proper inhomogeneous extension, it is possible to retain a certain dependence from spatial coordinates into the reduced variables describing a Bianchi I model. Within this scheme, we get the following set of additional symmetries: i) three independent U(1) gauge transformation, denoted by U(1)i (i = 1, 2, 3),deﬁned in the 1-dimensional space generated by ﬁducial vectors ωi = ∂i, and ii) reduced diﬀeomorphisms, which act as 1-dimensional diﬀeomorphisms along a given ﬁducial direct i and rigid translations along the other directions j ̸= i. We also outline how a similar formulation will be relevant within the BKL conjecture [36] scheme. In section V we discuss the implications of this formulation in a reduced quantization scheme. The elements of the associated Hilbert space are deﬁned over reduced graphs, whose edges are parallel to ﬁducial vectors and to each edge ei//∂i is associated a U(1)i group element. Within this scheme, a proper quantum implementation can be given to the algebra of reduced holonomy-ﬂux variables. The additional symmetries can then be implemented as in full LQG and they imply the conservation of U(1)i quantum numbers along the integral curves of ﬁducial vectors ∂i and that states have to be deﬁned over reduced s-knots. However, we will note that no meaningful expression for the superHamiltonian operator can be given. The failure of reduced quantization to account for the proper dynamics is the motivation for considering a diﬀerent approach, in which a truncation of full LQG is performed. This is done in section VI where the truncation is realized such that 1. the elements of the full Hilbert space are deﬁned over reduced graph: this is implemented via a projection and this implies the restriction of arbitrary diﬀeomorphisms to reduced ones. 2. The SU(2) gauge group is broken to the U(1)i subgroups along each edge ei: this is realized by imposing weakly a gauge-ﬁxing condition on each group element over an edge ei. A proper quantum-reduced kinematical Hilbert space is found by mimicking the analogous procedure adopted in SpinFoam models to solve the simplicity constraints [37]. In particular, we develop projected U(1)i-networks [38] by which we can embed functionals over the U(1)i group into functionals over the SU(2) group. Hence, we impose strongly a Master constraint condition obtained by squaring and summing all the gauge-ﬁxing conditions. This requirement ﬁxes the relation between SU(2) and U(1)i quantum numbers and the resulting projected U(1)i networks solve the gaugeﬁxing conditions weakly. At the end, the reduced U(1)i elements are obtained from full SU(2) ones by projecting over the states with maximum magnetic number along the internal direction i. The projection to U(1)i elements can then be applied directly to SU(2)-invariant states. As a result some non-trivial intertwiners are induced between U(1)i group elements for diﬀerent values of the index i. These intertwiners coincide with the projection of the coherent LivineSpeziale intertwiners [39] on the usual intertwiners base. Hence, the U(1)i states are not kinematically independent, but they realize a true three-dimensional vertex structure. This result allows us to implement the superHamiltonian operator according with Thiemann regularization scheme [6]. In fact, by deﬁning states over reduced s-knots it is possible to remove the regulator and get a well-deﬁned expression. Moreover, thanks to the simpliﬁcations due to the reduced Hilbert space structure (the volume operator is diagonal!), we evaluate in section VII the explicit expression of the superHamiltonian matrix elements in the case of a 3-valence vertex. Concluding remarks follow in section VIII.
A Review of Verbal and Non-Verbal Human-Robot Interactive Communication<|sep|>While the ﬁrst modern-day industrial robot, Unimate, began work on the General Motors assembly line in 1961, and was conceived in 1954 by George Devol [1], [2], the concept of a robot has a very long history, starting in mythology and folklore, and the ﬁrst mechanical predecessors (automata) having been constructed in Ancient Times. For example, in Greek mythology, the God Hephaestus is reputed to have made mechanical servants from gold ([3] in p.114, and [4] verse 18.419). Furthermore, a rich tradition of designing and building mechanical, pneumatic or hydraulic automata also exists: from the automata of Ancient Egyptian temples, to the mechanical pigeon of the Pythagorean Archytas of Tarantum circa 400BC [5], to the accounts of earlier automata found in the Lie Zi text in China in 300BC [6], to the devices of Heron of Alexandria [7] in the 1st century. The Islamic world also plays an important role in the development of automata; AlJazari, an Arab inventor, designed and constructed numerous automatic machines, and is even reputed to have devised the ﬁrst programmable humanoid robot in 1206AD [8]. The word “robot”, a Slavic word meaning servitude, was ﬁrst used in this context by the Czech author Karel Capek in 1921 [9]. However, regarding robots with natural-language conversational abilities, it wasnt until the 1990’s that the ﬁrst pioneering systems started to appear. Despite the long history of mythology and automata, and the fact that even the mythological handmaidens of Hephaestus were reputed to have been given a voice [3], and despite the fact that the ﬁrst general-purpose electronic speech synthesizer was developed by Noriko Omeda in Japan in 1968 [10], it wasnt until the early 1990’s that conversational robots such as MAIA [11], RHINO [12], and AESOP [13] appeared. These robots cover a range of intended application domains; for example, MAIA was intended to carry objects and deliver them, while RHINO is a museum guide robot, and AESOP a surgical robot. In more detail, the early systems include Polly, a robotic guide that could give tours in ofﬁces [14], [15]. Polly had very simple interaction capacities; it could perceive human feet waving a “tour wanted” signal, and then it would just use pre-determined phrases during the tour itself. A slightly more advanced system was TJ [16]. TJ could verbally respond to simple commands, such as “go left”, albeit through a keyboard. RHINO, on the other hand [12], could respond to tour-start commands, but then, again, just offered a pre-programmed tour with ﬁxed programmer-deﬁned verbal descriptions. Regarding mobile assistant robots with conversational capabilities in the 1990s, a classic system is MAIA [11], [17], obeying simple commands, and carrying objects around places, as well as the mobile ofﬁce assistant which could not only deliver parcels but guide visitors described in [18], and the similar in functionality Japanese-language robot Jijo-2 [19], [20], [21]. Finally, an important book from the period is [22], which is characteristic of the traditional natural-language semantics-inspired theoretical approaches to the problem of human-robot communication, and also of the great gap between the theoretical proposals and the actual implemented systems of this early decade. What is common to all the above early systems is that they share a number of limitations. First, all of them only accept a ﬁxed and small number of simple canned commands, and they respond with a set of canned answers. Second, the only speech acts (in the sense of Searle [23]) that they can handle are requests. Third, the dialogue they support is clearly not ﬂexibly mixed initiative; in most cases it is just humaninitiative. Four, they dont really support situated language, i.e. language about their physical situations and events that are happening around them; except for a ﬁxed number of canned location names in a few cases. Five, they are not able to handle affective speech; i.e. emotion-carrying prosody is neither recognized nor generated. Six, their non-verbal communication [24] capabilities are almost non-existent; for example, gestures, gait, facial expressions, and head nods are neither recognized nor produced. And seventh, their dialogue systems are usually effectively stimulus-response or stimulusstate-response systems; i.e. no real speech planning or purposeful dialogue generation is taking place, and certainly not in conjunction with the motor planning subsystems of the robot. Last but quite importantly, no real learning, off-line or on-the-ﬂy is taking place in these systems; verbal behaviors have to be prescribed. All of these shortcomings of the early systems of the 1990s, effectively have become desiderata for the next two decades of research: the 2000s and 2010s, which we are in at the moment. Thus, in this paper, we will start by providing a discussion giving motivation to the need for existence of interactive robots with natural human-robot communication capabilities, and then we will enlist a number of desiderata for such systems, which have also effectively become areas of active research in the last decade. Then, we will examine these desiderata one by one, and discuss the research that has taken place towards their fulﬁllment. Special consideration will be given to the socalled “symbol grounding problem” [25], which is central to most endeavors towards natural language communication with physically embodied agents, such as robots. Finally, after a discussion of the most important open problems for the future, we will provide a concise conclusion.
An unbiased estimate for the mean of a {0,1} random variable with relative error distribution independent of the mean<|sep|>Say X1, X2, X3, . . . are independent, identically distributed (iid) Bernoulli random variables with mean p. Write Xi ∼ Bern(p) to denote P(Xi = 1) = p and P(Xi = 0) = 1 − p. The purpose of this work is to present a new algorithm for estimating p with ˆp so that the relative error ˆp/p − 1 has a known distribution that does not depend on the value of p. In other words, with this algorithm it is possible to compute P(a ≤ ˆp/p − 1 ≤ b) exactly for any a ≤ 0 ≤ b, without needing any kind of approximation or limiting behavior. This problem of estimating p, which is also known as estimating the parameter of a binomial given a large sample, arises in a wide diversity of contexts. Examples include estimating the percentage of farms growing a particular crop [9], estimating the prevalence of a disease in a population [13, 12], and any situation where it is desirable to know the percentage of a population with a speciﬁc property. Another application is in exact p-values. Given a statistical model and a statistic, let “heads” be when the statistic applied to a draw from the model is more unusual than the same statistic applied to the data, and all other events are “tails.” Then the p-value for the data is just the probability of heads on the coin. This allows estimation of the exact p-value for any statistical model that can be simulated from by ﬂipping coins. Models where this has been applied include testing if a population is in Hardy-Weinberg equilibrium [4, 5], the Rasch model [1], two-sample survival data [14], and nonparametric testing of sequential association [10]. In theoretical computer science, many problems of approximation can be reduced to the problem of estimating the binomial parameter. In particular, approximating the permanent of a matrix with positive entries [6], the number of solutions to a disjunctive normal form expression [7], the volume of a convex body [8], estimating exact p-values for a model (see for instance [5]) and counting the lattice points inside a polytope can all be put into this framework. In general, anywhere an acceptance rejection method is used to build an approximation algorithm, this problem arises. The cost here is usually dominated by the number of Bern(p) ﬂips of the coin that are needed, and so the focus here is on minimizing the expected number of such ﬂips needed. Deﬁnition 1. Suppose A is a function of X1, X2, . . . iid ∼ Bern(p) and auxiliary randomness (represented by U ∼ Unif([0, 1])) that outputs ˆp. Let T be a stopping time with respect to the natural ﬁltration so that the value of ˆp only depends on U and X1, . . . , XT . Then call T the running time of the algorithm. p(1 − p)/n. Therefore, to get an estimate which is close to p in the sense of having small relative error, k should be of the form C/p (for some constant C) so that the standard deviation is p � (1 − p)/C and so roughly proportional to p. From the Central Limit Theorem, roughly 2ǫ−2 ln(2/δ)/p samples are necessary to get ˆpn/p ∈ [1−ǫ, 1+ǫ] for ǫ ∈ (0, 1). (See Section 4 for a more detailed form of this argument.) But p is unknown at the beginning of the algorithm! Dagum, Karp, Luby and Ross [3] dealt with this circularity problem with their stopping rule algorithm. In this context of Bern(p) random variables, their algorithm can be written as follows. Fix (ǫ, δ) with ǫ ∈ (0, 1) and δ > 0. Let T be the smallest integer such that X1 + · · · + XT ≥ 1 + (1 + ǫ)4(e − 2) ln(2/δ)ǫ−2. Then ˆpDKLR = (X1 + · · · + XT )/T. Call this method DKLR. They showed the following result for their estimate (Stopping Rule Theorem of [3]). and E[T] ≤ [1 + (1 + ǫ)4(e − 2) ln(2/δ)ǫ−2]/p. They also showed that any such (ǫ, δ) approximation algorithm that applies to all p ∈ [0, 1/2] (Lemma 7.5 of [3]) must satisfy There are several drawbacks to DKLR. First, the factor of 4(e−2) (which is about 2.873) in the running time of DKLR is an artifact of the analysis rather than coming from the problem itself. As mentioned before, a heuristic Central Limit Theorem argument (see Section 4) indicates that the correct factor in the running time should be 2. Second, the DKLR estimate is biased. Our algorithm has a form similar to DKLR, but with a continuous modiﬁcation that yields several desirable beneﬁts. The DKLR estimate (X1 + · · · + XT )/T is a ﬁxed integer divided by a negative binomial random variable. In the algorithm proposed here, the estimate is a ﬁxed integer divided by a Gamma random variable. Since Gamma random variables are scalable, the relative error of the estimate does not depend on the value of p. This allows a much tighter analysis of the error, since the value of p is no longer an issue. In particular, the algorithm attains (to ﬁrst order) the 2ǫ−2p−1 ln(2δ−1) running time that is likely the best possible. The new algorithm is called the Gamma Bernoulli approximation scheme (GBAS). Theorem 1. The GBAS method of Section 2, for any integer k ≥ 2, outputs an estimate ˆp using T samples where E[T] = k/p, E[ˆp] = p, and p/ˆp has a Gamma distribution with shape parameter k and rate parameter k − 1. The density of the relative error ˆp/p − 1 is To understand the eﬀectiveness of the new estimate, suppose that in fact the value of p was known exactly. Then for a given n, the probability that the relative error was at least ǫ could be calculated exactly, and the smallest value of n that makes this probability below δ would be used. The table below presents to four signiﬁcant digits the number of samples used by the new algorithm, by DKLR and by using the optimal value for n assuming that p was known ahead of time. The ﬁnal column gives the expected number used by the new method over the number needed by the exact binomial approach. (0.1, 0.05) 0.05 7700 23340 7299 1.067 (0.1, 0.05) 0.01 3.850 · 104 11.67 · 104 3.755 · 104 1.025 (0.1, 10−6) 0.05 5.122 · 104 9.174 · 104 4.551 · 104 1.125 (0.1, 10−6) 0.01 2.561 · 105 4.587 · 105 2.375 · 105 1.078 (0.01, 0.05) 0.05 7.683 · 105 21.41 · 105 7.280 · 105 1.055 (0.01, 0.05) 0.01 3.842 · 106 10.70 · 106 3.795 · 104 1.012 (0.01, 10−6) 0.05 4.790 · 106 8.240 · 106 4.545 · 106 1.054 (0.01, 10−6) 0.01 2.395 · 107 4.210 · 107 2.369 · 107 1.011 It is important to note that the exact binomial column is not an actual algorithm. This is because to use this would require the knowledge of the exact value of p, which is the unknown that we are trying to ﬁnd. In some sense, this represents the optimal number of draws possible necessary to achieve (ǫ, δ) performance. The fact that the running time of the new estimate comes so close to the optimal number of draws without needing to know p is one of the great strengths of the new approach. In [3] a lower bound for the number of samples that any method would require was given in the more general case of [0, 1] random variables. For {0, 1} random variables, this can be improved. The following theorem is proved in Section 4. Theorem 2. For ǫ > 0 and δ ∈ (0, 1) any algorithm that returns ˆp for p ∈ [0, 1/2] satisfying P(−ǫ ≤ (ˆp/p) − 1 ≤ ǫ) > 1 − δ must have As ǫ and δ go to 0, the ratio between the upper and lower bounds converges to 10 for these results. From Central Limit Theorem considerations, it is likely that the upper bound constant of 2 is the correct one (see Section 4).
A Mixture of Expert Based Deep Neural Network for Improved ASR<|sep|>In recent years, replacing Gaussian mixture model (GMM) hidden Markov model (HMM) [1], deep neural networks (DNN) [2, 3], and more recently recurrent neural network based acoustic models have become the state-of-the-art. At the same time, the striving demand for human-machine interaction has called for development of new architectures for acoustic model that yields even lower word error rates (WERs) than the conventional models. The ASR task is complicated by the fact that speech signal is a highly stochastic process in nature – the stochasticity arises due to different speech attributes, namely, the inherent phonetic variability as well as extrinsic factors such as speaker and background noises. In natural speech, different acoustic-classes (such as phonemes) are characterized by different frequency responses that ASR systems exploit for classiﬁcation. For instance the frequency response of a voiced sounds may be very different from fricatives. However due to coarticulation and other natural variations in human speech production, the distributions of these classes may be strongly overlapped in the acoustic-space, that leads to inter-class confusion and mis-classiﬁcation. The ASR accuracy is likely to improve by modifying existing architecture of acoustic models to make them more suitable to segregate the classes, which is the objective of this paper. Mixture of Experts (MoE) are popular in the areas of machine learning for region-dependent processing of the features using an ensemble of experts, which could be either classiﬁers or regressors [4, 5, 6, 7]. Different experts are specialized to operate on speciﬁc regions in the input space. Outputs of the experts are linearly combined using data dependent weights generated by an additional auxiliary classiﬁer. The role of this classiﬁer is to “select” (soft or hard) the best expert that is akin to the location of the feature vector in the input space. In this paper, we explore MoE to extend two popular deep learning architectures for acoustic modeling, namely DNN-HMM and long short term memory HMM (LSTM-HMM) [9]. For brevity we will refer to this architecture as MixNet. It is argued that the proposed architecture helps to reduce the overlap between different broad acoustic-classes, as a consequence ability of acoustic model to discriminate the classes is improved, which leads to signiﬁcant improvement in ASR accuracy. All results reported in this paper are based on DNN-HMM and LSTM-HMM models. The main contribution of this paper is to show that MixNet outperforms these models by a signiﬁcant margin, i.e., 13.6% and 10.0% relative reduction in WER, respectively. Detailed analysis is conducted to examine the distribution of the classes in the MoE space by plotting the scatter diagram. It is found that compared to the baseline features, MoEs provide a signiﬁcant improvement in the inter-class separation, which is reﬂected in the improved ASR accuracy. The organization of the rest of the paper is as follows. In Section 2, we present previous work related to proposed method. Section 3 presents the proposed MixNet architecture. Experimental setup and results are discussed in Section 4 and 5. Section 6 presents the conclusions.
Entanglement between two subsystems, the Wigner semicircle and extreme value statistics<|sep|>L1 and L2 qubits in a random pure state |ψ⟩ of L qubits (L1 +L2 ≤ L) (see Fig. (1)). While of ρT2 12 by µi. Negativity of the state ρ12 is deﬁned as the sum of the moduli of the negative tr (ρ12) = tr � ρT2 12 � = 1, tr(ρ12 2) = tr � (ρT2 12)2� , tr(ρ12 m) ̸= tr � (ρT2 12)m� m ≥ 3. for the partial transpose leads to the bound on L1 + L2 for ρ12 to be NPT is in fact L/2.
On Exploiting Transaction Concurrency To Speed Up Blockchains<|sep|>Consensus protocols are currently the fundamental obstacles that prevent blockchain systems from scaling. There is a large gap between the cost of consensus and the cost of other blockchain layers, in particular the execution and data model layer [8]. Most recent works that seek to improve blockchain performance focus on scaling the consensus layer, either by designing new protocols [9], [20], by leveraging sharding [5], [12], or by weakening security guarantees [1]. Despite these efforts, state-of-the-art blockchains with novel consensus protocols can only achieve a few thousands of transactions per second in throughput, which is far below what a typical distributed database can do [8], [16]. We argue that it is time to look at other layers of the blockchain for opportunities to increase performance. We posit that the execution layer, where blockchain transactions are executed, offers ample opportunities to improve both the performance and security of blockchains. There are three reasons for that. First, many modern blockchains employ sharding, which splits the network into small committees that run consensus protocols independently from the other committees. Within a small committee, the gap between the cost of consensus and transaction execution shrinks signiﬁcantly [5]. In other words, reducing the cost of the transaction layer can lead to signiﬁcant performance gains at each committee, which in turn improves blockchain performance as a whole. Second, some private blockchains such as Hyperledger Fabric [1] abandon classic consensus protocols for other designs that require a trusted third party service, such as the ordering service discussed in [1]. By sacriﬁcing security, these blockchains shift their bottlenecks away from consensus to other parts of the systems, one of which being the execution layer [15], [19]. Third, the cost of executing transactions negatively affects blockchains’ incentive mechanisms, as captured by the Veriﬁer’s Dilemma [13]. As a consequence, making transaction execution faster strengthens the incentive mechanisms, which in turns strengthens the overall security. We ask the following question: how much can we speed up blockchains by speeding up the execution layer? Although a large number of techniques from databases can be employed to speed up transaction execution, we focus on a single technique: exploiting concurrency to execute multiple transactions in parallel. The fact that existing blockchains execute transactions in batches (i.e., one batch per block), but within each batch execute transactions only sequentially, means there could be a large amount of untapped concurrency. In this work, we take ﬁrst steps at answering the above question. We have two goals. The ﬁrst goal is to understand the amount of concurrency available in existing blockchains. To this end, we conduct an extensive empirical analysis of seven public blockchains, namely Bitcoin, Bitcoin Cash, Litecoin, Dogecoin, Ethereum, Ethereum Classic, and Zilliqa. We choose public blockchains over their private (or permissioned) alternatives because of their wide adoption and data availability. The selected blockchains cover a large design space, including state-of-the-art sharding-based systems. We measure concurrency using the conﬂict rate per block: a lower rate means higher concurrency. We compare the seven blockchains against two variants of this metric: the single-transaction conﬂict rate, and the group conﬂict rate. Our analysis differs signiﬁcantly from recent work that evaluates concurrency in Ethereum [17] in that our approach is much more lightweight and can extract more concurrency, and that our analysis covers a more comprehensive dataset that includes more than one blockchain. Our second goal is to understand how much execution speed-up can be achieved by exploiting the available concurrency. To this end, we propose an analytical model for the computation of the potential speed-up from the conﬂict rates per block. An accurate model is not trivial, because it must take into account variables other than conﬂict rate, for instance the number of cores per machine, scheduling policies, and sychronization overhead. In summary, we make three important contributions. 1) We present an extensive data-driven analysis of the amount of concurrency in seven public blockchains. Our methodology is more lightweight and able to capture more concurrency from more comprehensive datasets and systems than existing works. 2) We discuss important ﬁndings from the analysis, including: • There is more concurrency in UTXO-based blockchains than in account-based ones. For example, the rate of single-transaction conﬂicts in Bitcoin is around 13% whereas in Ethereum it is close to 80%. Although the difference may seem unsurprising because of the nature of the two data models, a more interesting observation is that the amount of concurrency in UTXO-based blockchains is lower than expected. One extreme example is the Bitcoin block 358624,1 in which 3217 out of the total 3264 transactions are dependent on each other (i.e., there is no concurrency between them and they must be executed sequentially). • In every blockchain, the group conﬂict rate is lower than the single-transaction conﬂict rate. Although this is true by deﬁnition, the difference is considerable. For example, in Ethereum the former is around 20% whereas the latter is closer to 60% on average. The implication is that there is much more concurrency to be exploited when transactions are considered in groups as opposed to individually. • Blockchains with more transactions per block often have a lower group conﬂict rate. For example, on average Ethereum has an order of magnitude more transactions per block than Ethereum Classic, but its group conﬂict rate is much lower than that of the latter, namely 20% compared to 70%. The implication is that blockchains with a higher load potentially have more concurrency, or that blockchains with more users have both a higher network load and more concurrency. 3) We present a model that enables the extrapolation of transaction execution speed-ups. Applying the model to the seven blockchains under consideration, we show potential performance gains of up to 6×. The next section presents the relevant background on blockchains and discusses our motivation for speeding up the execution layer. Section III details the methodology of our empirical analysis, including the metrics and data collection process. Section IV discusses our ﬁndings. Section V describes the speed-up model. Section VI discusses the related work, before Section VII concludes the paper.
Generating Gender Augmented Data for NLP<|sep|>Recent studies have exposed challenging systematic issues related to bias that extend to a range of AI applications, including Natural Language Processing (NLP) technology (Costa-jussà, 2019; Blodgett et al., 2020). Observed bias problems range from copying biases already existing in data to claims that the training process can lead to an exacerbation or ampliﬁcation of observed biases (Zhou and Schiebinger, 2018; Vanmassenhove et al., 2021). The algorithms learn to maximize the overall probability of an occurrence, leading to preferences for more frequently appearing training patterns. With this work, we propose a method for generating (more) balanced data in terms of one of the main types of bias frequently observed in language: gender bias. Gender bias can occur in language due to the fact that some languages have a way of explicitly marking (natural or grammatical) gender while others do not (Stahlberg et al., 2007). Gender bias in translation is usually manifested when animate entities (e.g. professions) are translated from gender neutral language (e.g. English) into a gendered language (e.g. Spanish) because the instances seen in training data are biased. Also, conversational utterances are prone to bias, both in machine translation as well as in other NLP applications, because systems often do not have the ability to provide multiple gender variants. Therefore, users are simply presented with the most probable option which is prone to bias. In our work, we aim to enable the generation of multiple gender variants by expanding each sentence with the missing gender variants, thus fostering inclusion in online conversations/NLP applications. Generating gender variants can and should also be used to create gender balanced conversational data that can be used to train less biased NLP models such as machine translation models, language models, chat bots, etc. Unlike previous studies, we did not want to limit ourselves to one speciﬁc gender phenomenon, such as gender markings on professions (Zmigrod et al., 2019)) (for which the gender can easily be swapped by using hand-crafted lists) or ﬁrst person personal pronouns (Habash et al., 2019)). The objective of this research aims to include as many cases as possible of gender alternatives related not only to gender of persons but also to grammatical gender of the objects referred to. In Example 1, (a) illustrates an example of two alternatives for a sentence where there is agreement with the grammatical gender of an object referred to in the previous sentence, while in (b) there is agreement with the gender of the speaker/writer (i.e. a person). At this stage, our approach does not discriminate between human referents and objects. It is furthermore limited to the generation of binary gender alternatives. We are aware of the importance and challenge of dealing with non-binary gender(Ackerman, 2019) which we aim to tackle in future work. The research was carried out in collaboration with an anonymous industry partner with a speciﬁc application in mind that deals with conversational sentences. Our approach aims to alleviate gender bias in the said application. We focus on one gender-rich language (Spanish), however, scalability and generalizability were kept in mind while designing the approach. Our approach can be summarized as follows: 1. Identifying (appropriate) sentences/segments that should have the opposite gender variant for some words. POS sequences were used to extract such segments from the OpenSubtitles corpus3.
Post-selection inference for e-value based confidence intervals<|sep|>One of the most classical problems in statistics is the problem of parameter estimation e.g. estimating the mean of a distribution. The time-tested solution is to produce a confidence interval (CI) in the parameter space that covers the true parameter with high probability. However, many scientists are not simply interested in estimating a single parameter — they may have many potentially interesting parameters to estimate concurrently. For example, a scientist might be experimenting with 𝐾 vaccines for a strain of virus. The scientist may only be interested in reporting CIs for the vaccines that the data suggest to be effective. However, she does not know which vaccines may be effective until after looking at the data. A reasonable thing the scientist might do is to report the 95% CIs for the vaccines where the CI is positive. Consequently, she is using the same data to both select the parameters she wishes to estimate, and construct estimates of these parameters. As a result, the reported CIs do not provide valid statistical coverage. To see this, consider a scenario where none of the vaccines reduce mortality. Any reported CI will have a coverage probability of 0%, since the CI for a vaccine is reported only if it is positive. This example illustrates the issue of selection bias for post-selection inference that was first noted by Benjamini and Yekutieli [3], hereafter referred to as BY. How can we correct our CIs so that they remain statistically valid? We provide a method, the e-BY procedure, that provides an answer to this question. In short, one should scale the error level of the reported CIs inversely with the number of parameters that are selected for reporting. Figure 1: The post-selection inference with confidence intervals (CI) problem. We must choose corrected confidence levels (1 − 𝛼𝑖) for the marginal CI constructed for each 𝜃* 𝑖 that is selected. The goal is to correct for the bias introduced by the selection rule, and still provide a coverage guarantee for the resulting CIs. The coverage metric we are interested in is the expected proportion of selected parameters that are not covered by their respective CIs , i.e., the false coverage rate (FCR). Our corrected confidence level must guarantee the FCR is below some fixed level of error 𝛿 ∈ (0, 1). Figure 1 illustrates the setup of this post-selection inference problem. Recall that we have 𝐾 true parameters we may wish to estimate: 𝜃* 1, . . . , 𝜃* 𝐾. We sample some data, X. Then, we use this data to select a subset of parameters, 𝒮 ⊆ [𝐾] = {1, . . . , 𝐾}, and construct 𝐶𝑖(𝛼𝑖), i.e. CI at level (1 − 𝛼𝑖), for each 𝑖 ∈ 𝒮. BY proposed a method for choosing each 𝛼𝑖 to retain statistical validity of the CIs in an aggregate form. Their procedure targets the false coverage rate (FCR), which is the expectation of the false coverage proportion (FCP). Both of these quantities are defined as follows: In essence, we aim to bound the expected proportion of selected CIs that do not cover their respective parameters , i.e., ensure FCR ≤ 𝛿 for a fixed 𝛿 ∈ (0, 1). The work of BY inspired much followup research on methodology that provide FCR guarantees in post-selection inference [40, 39, 41]. To summarize why FCR is considered over other error metrics, we note that (1) a CI which provides a coverage probability guarantee when conditioned on the selection rule requires knowing the selection rule beforehand and (2) simultaneous coverage of all CIs requires a conservative Bonferroni correction — we discuss this in more depth in Appendix F. With that said, we will focus our attention to FCR control in this work. Figure 2: A flowchart for deciding when to use e-BY vs. BY. The only case where the BY procedure should be employed is when strong assumptions are made upon both the selection rule and the type of data dependence, while the e-BY procedure is an uniform improvement over the BY procedure in all other cases. In cases where only e-CIs can be constructed for some parameters, and standard CIs for others, we can calibrate the standard CIs to e-CIs. In fact, calibration reduces the BY procedure to a special case of the e-BY procedure (Section 2.3). Our contributions Our key contributions are two-fold. First, we study a novel class of CIs: e-CIs. E-CIs are characterized by e-values [31, 25, 12, 21], a concept recently developed as an alternative to p-values. We show that a reasonably large array of existing CIs are already e-CIs. This new categorization sets the stage for our second main contribution: the e-BY procedure for FCR control (Definition 4). The e-BY procedure is complementary to the BY procedure and requires much fewer assumptions about the sampling process and selection rule than the BY procedure — the BY procedure will be the primary point of comparison in this work. The e-BY procedure is quite straightforward — it simply sets the confidence level of the CI for each selected parameter to be (1 − 𝛿|𝒮|/𝐾). This is sufficient to guarantee FCR ≤ 𝛿 regardless of the selection rule or dependence structure in the data used to construct the CIs. On the other hand, the BY procedure can only achieve these confidence levels in the best case i.e. under assumptions about both the dependence structure and the selection rule. One limitation of the e-BY procedure, however, is that it can only be applied to e-CIs. Figure 2 maps out a decision flow for when a practitioner should use the e-BY procedure. The e-BY procedure dominates the BY procedure when e-CIs are available or when one cannot make assumptions about either the selection rule or dependence structure. Consequently, the e-BY procedure and the BY procedure differ from each other primarily in the three following ways. 1. Selection rule must be known for the BY procedure to achieve tight confidence levels. Under independence (and a more general condition of PRDS — see Benjamini and Yekutieli 2), the Table 1: Tradeoffs between the e-BY procedure in this paper and the BY procedure [3]. The confidence levels of the e-BY procedure are not penalized for certain selection rules, nor under different types of dependencies between CIs. However, the e-BY procedure only being applicable to e-CIs, a special class of CIs derived from e-values [31, 25, 12, 21]. BY procedure guarantees FCR control when the 𝑖th CI has a confidence level of 1 − 𝛿𝑅min 𝑖 /𝐾. Here, 𝑅min 𝑖 is an integer between 1 and |𝒮| that is determined by the selection rule S and the data X — see (4) for definition. BY showed that for some common selection rules (e.g. selecting parameters with estimates greater than a threshold, Benjamini-Hochberg, etc.), 𝑅min 𝑖 = |𝒮|. However, this is not true in general. For example, when we do not know the selection rule in advance, the above guarantee does not provide FCR control unless the confidence level is 1 − 𝛿/𝐾. This is no better than a Bonferroni correction! Thus, the BY procedure also has a fallback guarantee that can provide FCR control under no assumptions on the selection rule or dependence, but requires the error levels to be shrunk by an additional factor of approximately log 𝐾. The e-BY procedure, in contrast, does not pay an approximately log 𝐾 penalty and always guarantees FCR control. It outputs CIs at level 1 − 𝛿|𝒮|/𝐾 i.e. the smallest c onfidence level that can be produced by the BY procedure even with assumptions. 2. CIs produced by the BY procedure are larger when the CIs are arbitrarily dependent. Even if the selection rule S is known, the BY procedure produces more conservative intervals than the e-BY procedure when the the data each CI is constructed from can be arbitrarily dependent. As noted before, the BY procedure must correct the confidence levels to be 1 − 𝛿|𝒮|/(𝐾ℓ𝐾) while the e-BY procedure maintains the same confidence levels of 1 − 𝛿|𝒮|/𝐾. Arbitrarily dependent data can arise when we make multiple measurements of a single unit of experimentation e.g. measuring different gene expression levels in a single cell. Dependence is also prevalent in settings where the data is collected sequentially — we explore this usage further in Section 7, where we analyze the e-BY procedure with the BY procedure on real data collected by an information technology company for the purposes of product testing. 3. The e-BY procedure requires e-CIs. The main limitation of the e-BY procedure, is that e-BY only provides valid FCR control for e-CIs. While “e-CI” is a new term, many existing CIs are e-CIs. These include nonasymptotic CIs based on Chernoff-type inequalities such as Hoeffding (Appendix E), Bernstein, and empirical Bernstein CIs [14, 5, 19], universal inference CIs [36] (Section 2.1), and CIs formed by stopping confidence sequences [22, 15, 16, 37, 34] (Section 2.2). Thus, requiring e-CIs is not a restrictive condition in many applications. Further, we will demonstrate in Section 2.3 that we can always construct an e-CI from a CI, at a cost to tightness. Remark 1. As an aside, we want to emphasize that the e-BY procedure is not simply a direct application of the e-BH procedure [35] to the post-selection inference problem. The e-BY procedure allows for arbitrary selection of parameters for CI reporting and subsumes the e-BH procedure, which restricts the hypotheses it rejects based on the e-values. We expand on the relationship between these procedures in Appendix B. Notation Denote the vector of the 𝐾 true parameters to be 𝜃* := (𝜃* 1, . . . , 𝜃* 𝐾). Formally, let the 𝑖th parameter, 𝜃* 𝑖 , lie in a corresponding parameter space Θ𝑖. Such a parameter is sometimes called a statistical property, [10] or a functional of a statistical distribution. Let 𝒫 be the universe of all possible distributions, and P ∈ 𝒫 is one such distribution in this universe. Consequently, 𝑇𝑖 : 𝒫 ↦→ Θ𝑖 is a function that maps a distribution P to the its 𝑖th parameter — we will use 𝜃𝑖 to denote the 𝑖th parameter when the distribution P is obvious. Thus, the formal guarantee of a (1 − 𝛼) marginal CI, 𝐶𝑖(𝛼), for the 𝑖th parameter is that P (𝑇𝑖(P) ∈ 𝐶𝑖(𝛼)) ≥ 1 − 𝛼 for all P ∈ 𝒫. We will write EP to denote the expectation of some random variable under some distribution P. Further, let E𝜃 and P𝜃 refer to the expectation and probability under any distribution with parameter 𝜃, respectively. E without any subscript refers to taking expectation under the true distribution, P. We sample X ∼ P where X := (𝑋1, . . . , 𝑋𝐾) is a vector of data. 𝐶𝑖 uses only 𝑋𝑖 to construct the CI for the 𝑖th parameter. Let 𝒳𝑖 be the domain of the 𝑖th data component, 𝑋𝑖. Denote the selection rule as S, i.e., 𝒮 = S(X). We will use the notation ℱ𝑡 to denote the sigma-algebra at index 𝑡 ∈ I of a filtration (a sequence of nested sigma-algebras) where I will be some index set for the filtration. We will denote the overall filtration as the sequence (ℱ𝑡)𝑡∈I and drop the index subscript when the index set is apparent. Outline Since we the e-BY procedure requires e-CIs, we first describe what an e-CI is and how one can be constructed in Section 2. Then, in Section 3 we introduce the e-BY procedure, prove its FCR guarantee, and contrast it with the BY procedure. To complement our upper bound, we also show some lower bound results for the e-BY procedure — in Section 4, we prove that the e-BY procedure has sharp FCR control and in Section 5 we also show that it is admissible among a general class of e-CI reporting procedures. Lastly, we compare the empirical behavior of the e-BY and BY procedures on simulations in Section 6 and real user metric data from A/B testing experiments at Twitter in Section 7.
ODMTCNet: An Interpretable Multi-view Deep Neural Network Architecture for Image Feature Representation<|sep|>With the advancement of large scale data-based techniques (e.g., big data, deep learning, etc.), there has come urgent demand to better understand and reveal the intrinsic properties of data structure in order to effectively explore useful information from massive sets of data. As a solution to this challenge, feature representation has been attracting widespread attention and investigation [1-3]. In general, the main streams of feature representation are divided into three different categories: handcrafted feature representation, prior knowledge-based feature representation and learned deep-level feature representation. In what follows, the three different categories of feature representation are introduced brieﬂy. Due to its ability to extract geometry and texture characteristics of given samples reliably (such as local binary pattern, scaleinvariant feature transforms and gradient orientation histogram, etc.), handcrafted feature representation methodology has drawn tremendous interest and has been utilized in a variety of real applications. However, suffering from the intra-class modiﬁability problem [4], handcrafted methodology is difﬁcult to perform well on unseen samples. To tackle this problem, prior knowledgebased feature representation solutions have been proposed and investigated extensively, such as manifold representation [5], subspace representation [6] and dictionary representation [7], etc. Generally, according to certain prior knowledge, the studied distinct information from given samples can be applied to new or unseen samples successfully, leading to better performance. It should be noted that, although more efforts have been committed to the development of prior knowledge-based feature representation algorithms, plenty of room is still available for further improvement, especially when working with large scale data sets [84-86]. In recent years, by generating learned deep-level feature representation, deep neural network (DNN)-based algorithms have opened up a new frontier to promote state-of-the-art in visual computing, image classiﬁcation and many other computer vision tasks [8-10]. Compared with the handcrafted and prior knowledge-based feature representation strategies, deep-level feature representation attempts to mimic the workings of the human brain in processing data/information. In general, deep-level feature representation is generated by utilizing a multi-layer cascade architecture and the learning process needs enormous data samples L. Gao, Z. Guo and L. Guan are with the Department of Electrical and Computer Engineering, Ryerson University, Toronto, ON M5B 2K3, Canada (email:iegaolei@gmail.com; zjguo@ryerson.ca; lguan@ee.ryerson.ca). to guarantee that the parameters in the deep layers are tuned properly. Particularly, there are two outstanding issues, among others, with the current deep learning architectures: (a) Model interpretability, a persistent challenge since the inception of DNN research; (b) Over-ﬁtting in the learned deep-level feature representation when there is no sufﬁcient data available due to DNNs‘ data-driven nature. One recently proposed solution to model interpretability is to modify a DNN model so that each convolutional layer generates knowledge representations, such as the loss method presented in [91] and an interpretable feedforward (FF) solution in [92]. In addition, the integration of prior knowledge with DNN-based architectures is becoming an increasingly popular research area for tackling the model interpretability problem (e.g., PCANet [11], DCTNet [12], etc.). Amongst these solutions, the model parameters are usually calculated by a certain prior knowledge-based function while the ﬁnal outputs are generated by the deep cascade architecture. On the other hand, to address the over-ﬁtting problems in DNN-based models, the multi-view feature representation solutions have attracted enormous attention. [13] explores the integration of multi-view features for better data augmentation to tackle small sample size (SSS) problems, and yields improved performance. The study in [13] suggests that one desirable solution to balancing the small scale and deep-level feature representation is to smartly utilize multi-view features and the deep cascade structure. A canonical correlation analysis network (CCANet) is presented in [4]. CCANet can obtain more comprehensive information from the inputs, resulting in enhanced performance. However, due to its unsupervised nature, CCANet lacks the power to extract discriminant information of high quality, leading to suboptimal performance. To address this problem, a discriminative canonical correlation network (DCCNet) [14] is introduced. However, in the DCCNet, only between-class and within-class scatter matrices are employed. Since the purpose of the scatter matrices is simply to reveal and extract discriminant representations of the given samples from a single data set, such a matrix is incapable of gaining the discriminant information across different sets of features adequately. Therefore, DCCNet essentially cannot be applied to multi-view feature representation directly. In this paper, an interpretable multi-view deep learning architecture, namely optimal discriminant multi-view tensor convolutional network (ODMTCNet), is proposed to address the model interpretability and over-ﬁtting problems jointly. ODMTCNet is essentially based on the integration of statistical machine learning (SML) principles with DNN architecture. Speciﬁcally, in ODMTCNet, a discriminant multi-view tensor convolutional strategy is proposed and applied to the multi-view data sources. In addition, correlation matrices instead of scatter matrices are utilized and optimized to explore the discriminant information across different data sets. Note that, unlike the scatter matrices, the power of which is to reveal the discriminant relation among various samples in a single data set, the correlation matrices are capable of exploring the discriminant information across multiple data sets. Thus, the correlation matrices can more adequately explore the discriminant and intrinsic relation amongst different data sets in multi-view feature representation. In other words, the proposed ODMTCNet potentially generates a high quality feature representation for image based applications. The difference between scatter matrices and correlation matrices is depicted graphically in Figure. 1. Since there is no loss function with given/ﬁxed values associated with ODMTCNet, the popular backpropagation (BP) algorithm is not used in this work and the parameters of the convolutional layers in the network are determined by analytically solving a SML-based optimization problem in each convolutional layer independently instead. Moreover, according to SML principles, the relation between the optimal performance and parameters (e.g., the number of convolutional ﬁlters in each layer) in ODMTCNet is predicted analytically, providing a simple and appealing alternative to the popular try-and-error practice in DNN architecture design. Hence, each convolutional layer in the ODMTCNet model generates justiﬁed knowledge representations, making the proposed model interpretable. In addition, by utilizing the designed discriminant multi-view tensor convolution, ODMTCNet yields more distinct multiview feature maps for data augmentation, providing a solution for improving performance on small scale data sets. Due to the characteristics of the deep cascade architecture, ODMTCNet can also exploit abstract semantics from the large scale data sources, and therefore works well on large-scale sets as well. Finally, an information theoretic based descriptor, information quality (IQ) is introduced to generate the high quality feature representation from the original multi-view data sources. To validate the effectiveness and generality of the proposed network, we conduct experiments on four image data sets of different scales: the Olivetti Research Lab (ORL) database, Facial Recognition Technology (FERET) database, ETH–80 database and Caltech 256 database. Experimental results show the superiority of ODMTCNet over state-of-the-art for multi-view feature representation. This paper is organized as follows: A review of related work is given in Section 2. The proposed ODMTCNet is presented in Section 3. Experiments, comparison with state-of-the-art and discussions are conducted in Section 4. Section 5 draws the conclusions.
Resonant Atom Traps for Electromagnetic Waves<|sep|>Periodic lattices of atoms, placed in a monochromatic electromagnetic wave with its  frequency close to that of some transition frequency between atomic levels, produce either  oscillatory solutions for the wave function in the space domain, corresponding to  propagating waves, or exponentially growing/decaying solutions in the limit of zero electric  field.  When the fields are strong, nonlinearity changes the nature of the interaction, giving  rise to finite in space (or trapped) solutions for the coupled atom-wave equations. The  equation for trapped waves in the Rotating Wave Approximation (RWA) (see, e.g., [1]) for a  1D lattice of simple hydrogen-like atoms is given in the next section. Section III introduces  approximate solutions for these equations. Appendix A provides trapped wave numerical  solutions for 1D random atom lattices. It is shown that the localization phenomenon,  discovered by Anderson in connection with random lattices in metallic alloys (see [2]) may  play a significant role in creation of trapped modes as well as in the nonlinearity of the atomwave interaction. Section IV presents a generalization of the results for 2D and 3D cases for  an averaged equation in the case of high atom density. Section V gives practical estimations  for the achievable electric field of the trapped wave in the case of real media. The conclusion  summarizes the results.
Relaxation of electrons in quantum-confined states in Pb/Si(111) thin films from master equation with first-principles-derived rates<|sep|>The thermalization of hot carriers in metals after optical excitation is accomplished both by the Coulomb scattering among the carriers (electron-electron (e-e) interactions) and by the scattering of electrons and holes by lattice vibrations (electron-phonon (e-ph) interaction). In a well-established picture1,2, the relaxation can be understood as a two-step process: at early times (t < 0.3 ps), e-e scattering dominates and brings the electrons to a thermal (or possibly non-thermal) distribution. At later times (> 0.3 ps) the e-ph interaction establishes equilibrium between the electronic distribution and the lattice temperature. In this second stage, the high density of excited carriers close to the Fermi energy (within an energy interval corresponding to few phonon quanta) is thought to be responsible for most of the energy ﬂow between the electronic and the phononic system. If so, the e-ph coupling inferred from thermalization experiments should relate directly to the microscopic e-ph coupling constant that governs electric resistivity or the superconducting transition temperature3. In this prevailing view, the role of e-ph interactions already in the early stages of relaxation is usually ignored. However, this simple picture is questioned by studies, both experimental and theoretical4,5, suggesting overlapping timescales of e-e and e-ph-driven thermalization. Moreover, there is little knowledge how the electrons far above the Fermi level (several tenth of eV) interact with the phonons. For instance, long-lived population of such states, e.g. at the Pb-covered Si(111) surface, has been observed in photoemission experiments6. The situation at high energies is in contrast to the e-ph interaction in close vicinity to the Fermi surface, which is crucial for a variety of physical phenomena such as electrical resistivity or supercon ductivity induced by electron-phonon coupling in thin ﬁlms7, and is quite well explored utilizing the concept of the Eliashberg function (for an overview, see Ref. 8). In conclusion, there is a need for more studies of the eph interaction at energies further away from the Fermi energy. In this paper, we attempt to obtain a better understanding of the relative importance of e-e and e-ph interaction in highly excited states of a metal and their respective contributions to the early stage of relaxation. To introduce our approach, we have chosen thin multilayer Pb ﬁlms on Si(111). The fact that this materials system shows a highly structured electronic density of states due to conﬁnement eﬀects9,10 has been a great advantage for analyzing the energy-dependent lifetime of the excited electrons using time-resolved pumpprobe spectroscopy.11 The experimental results were rationalized in Ref. 11 in terms of e-e interaction only, and it was concluded that the electronic lifetime closely follows the behavior expected from Landau’s theory of Fermi liquids.12,13 Yet, a contribution of e-ph scattering to the lifetime cannot be excluded completely based on the achieved level of agreement between experiment and theory. Therefore, we aim at a detailled analysis of the role of e-ph scattering for the features observed in photoemission. Since ample experimental and computational data are available for the Pb/Si(111) ﬁlms, we consider this system a good test case for quantitative studies of electronic relaxation dynamics. In a previous paper14 by us, we have worked out a realistic atomistic description for multilayer Pb ﬁlms on Si(111) and have carried out ﬁrst-principles calculations of the electronic and phononic band structure and of e-ph coupling in electronic states far away from the Fermi level. While the e-ph interaction in bulk solids has become accessible to ﬁrst-principles calculations by using density functional perturbation theory together with Wannier interpolation methods to enhance the number of reciprocal-space sampling points15–18, thin ﬁlms on a substrate are still diﬃcult to treat on a microscopic level because the adequate supercell typically contains tens to hundreds of atoms and computational costs are high. For the Pb ﬁlms on Si(111), for instance, the complex phase diagram19,20 results in various reconstructions requiring large supercells for their description21. In the present work, we constructed a √ 3 unit cell of Si(111) matched with a (2 × 2) unit cell of Pb(111) to describe the atomic structure consisting of 40 Pb and 30 Si atoms.14 While the two-dimensional Brillouin zone of reconstructed surface plus interface is smaller than the Brillouin zone of a bulk material, the supercell contains a large number of bands, both in the electronic and phononic spectra. Therefore, a thoughtful selection of bands will be required to arrive at a tractable model for e-ph coupling. The approach via density functional perturbation theory and the calculation of the Eliashberg function would be too cumbersome for large supercells. In this paper, building upon the knowledge of our previous work14, we elaborate on the consequences of these microscopic data for the e-ph scattering rate using a kinetic master equation. The detailed modeling of e-ph scattering is combined with a description of the e-e interaction at the level of Fermi liquid theory. This combination allows us to simulate the temporal evolution of electronic populations on the relevant scales and to make contact with experimental observations.
Chemical abundance analysis of symbiotic giants. I. RW Hya and SY Mus<|sep|>Binary systems are an invaluable source of knowledge of the physical parameters of the stars. During some stage of stellar evolution most binary stellar systems undergo interactions between the components. The interactions turn on and oﬀ at various evolutionary stages, depending on the separation of components. Among the most interesting examples of such systems are the symbiotic systems. These are still often called symbiotic stars because their binary nature which we now consider obvious was controversial only thirty years ago. Symbiotic systems are long-period interacting binaries in which an evolved giant transfers material to a hot and luminous companion surrounded by an ionized nebula. The hot component of the vast majority of symbiotic systems is a white dwarf (WD) although in two cases, V2116 Oph (Davidsen et al. 1977) and V934 Her (Garcia et al. 1983), a neutron star has been found. There are two distinct classes of symbiotic binaries. The S-type (stellar) have normal red giants and orbital periods in the range ∼1–15 years. The D-type (dusty) have Mira primaries usually surrounded by a warm dust shell and orbital periods of typically decades to cen turies. Mass exchange is the property that deﬁnes the symbiotic class. Even in the D-type systems the components are still close enough to allow the WDs to accrete material from the giant’s stellar wind. Symbiotic systems are the interacting binaries with the longest orbital periods. Their study is essential to understand the evolution and interaction of detached and semi-detached binary stars. The rich and luminous circumstellar environment surrounding the interacting symbiotic stellar members results from the presence of both an evolved giant with a heavy mass loss and a hot companion copious in ionizing photons often producing its own wind. The cool giant and the hot dwarf produce strongly diﬀerent environments such as ionized and neutral regions, dust forming region(s), accretion/excretion disks, interacting winds, bipolar outﬂows, and jets. Such a complex multi-component structure makes symbiotic stars a very attractive laboratory to study many aspects of stellar evolution in binary systems. Firming links between symbiotic systems and related objects are essential to the understanding, for instance, of the role of interacting binaries in the formation of stellar jets, planetary nebulae, novae, supersoft X-ray sources (SSXS), and supernovae type Ia (SN Ia). Many of these issues concerning the late stages of stellar evolution are presently poorly understood but have important implications on our understanding of the stellar
Investigation of the Spatially Dependent Charge Collection Probability in CuInS$_2$/ZnO Colloidal Nanocrystal Solar Cells<|sep|>Colloidal nanocrystals (NCs) have received immense interest as functional materials for photovoltaic applications.1–4 Especially, the facile solution processability, the tunability of the band gap due to the quantum-size eﬀect, as well as the ability of multiple exciton generation5,6 make them attractive as absorber material in solar cells. In particular, quantum dots made of lead chalcogenides such as PbS and PbSe have attracted attention in recent years,7,8 mainly because of their adjustable spectral response, ranging from the visible to the mid-infrared.9 By careful optimization of the optical and electrical properties, remarkable power conversion eﬃciencies of up to 9.2% have been achieved for PbS and PbSe colloidal quantum dot solar cells.10,11 However, there are concerns about the potential toxicity and environmental impact of lead compounds, which makes the search for alternative materials an active area of research. One promising class of materials is copper-based nanocompounds such as Cu2S,12 Cu2ZnSnS4 13–17 and Cu(In,Ga)(Se,S)2.13,18–22 These materials have demonstrated reasonable photovoltaic performance when applied to device architectures and fabrication methods known from thin-ﬁlm solar cells, including the utilization of a cadmium sulﬁde (CdS) buﬀer layer12,14–22 and extensive post-deposition techniques like high-temperature sintering and selenization.14,15,18 In contrast, there is still a lack of reports on the application of copper-based NCs to device concepts that are related to the lead chalcogenide quantum dot solar cells, which would enable low-temperature processing and avoid the usage of a CdS buﬀer layer. Recently, we studied the potential of ternary copper indium disulﬁde (CuInS2) NCs23 as absorber material in such device architectures and successfully implemented them into Schottky solar cells based on a CuInS2/metal junction,24 as well as heterojunction devices in combination with n-type zinc oxide (ZnO) NCs.25 However, the device performance remained limited up to now, which we mainly attribute to insuﬃcient charge collection and transport properties. The collection of charge carriers has been designated as a critical issue in NC solar cells and can become a limiting factor for the external photocurrent. Due to the moderate diﬀusion length in nanocrystalline solids,26 the collection of minority carriers via diﬀusion is rather ineﬃcient.27 Because of that, drift collection must account for a large proportion of the generated photocurrent.28,29 One promising concept addressing this issue are the so-called depleted heterojunction devices,30 in which the commonly p-type light-harvesting NC ﬁlm is combined with a wide band gap n-type semiconductor such as titania (TiO2) or ZnO. The working principle of these solar cells is based on the formation of a p– n junction, with the depletion region extending over a wide range of the light-harvesting layer, presuming the doping concentrations of both parts of the heterojunction are adequately chosen.31 Due to the large extent of the space charge region, where the transport is driftdominated, the heterojunction devices are clearly advantageous over Schottky junction devices, which posses only a narrow depletion region in the vicinity of the metal contact.7,30,32 However, the optimum condition of a fully depleted absorber is often not fulﬁlled, especially when the layer thickness is increased to ensure eﬀective light absorption. Consequently, the charge collection eﬃciency can become dependent on the spatial position within the light-harvesting layer.31,33 Substantial “dead zones” for charge collection have been reported on quantum dot solar cells,31,34 as well as other low-mobility materials like polymer/fullerene bulk heterojunction ﬁlms.33 One possibility to overcome these limitations has been proposed by Kramer et al.,35 who realized quantum dot absorber layers with a band gap gradient to introduce an additional driving force for charge carriers generated beyond the depletion region. The most commonly used approach to investigate the charge collection properties are electrical characterization techniques in combination with electro-optical modeling.31,36–38 These methods typically require knowledge of various material parameters such as the carrier mobilities, carrier lifetimes, and doping concentrations. However, some of these properties are diﬃcult to determine or simply unknown, in particular when it comes to new materials. The aim of the present work is to utilize an alternative approach to investigate the charge collection deﬁciencies in CuInS2/ZnO heterojunction solar cells. Our approach is based on purely optical simulations, so that only optical properties of the involved materials need to be known. This clearly reduces the amount of necessary input parameters compared to a complete electro-optical description and is believed to make the analysis reliable and easily transferable to other material systems. In detail, we utilized an inverted device structure31,39,40 with a p-type substrate and the ZnO NC layer placed between the CuInS2 and the reﬂective metal electrode. In that conﬁguration, the functionality of the ZnO is twofold, both as part of the heterojunction and optical spacer layer. It is well-known that the spatial distribution of the optical electric ﬁeld intensity, which is directly related to the charge carrier generation proﬁle, can be highly structured due to thin-ﬁlm interference eﬀects.41–44 Because of the optical microcavity formed between the electrodes, systematic variations of the optical spacer thickness can be utilized to manipulate the spatial generation proﬁle within the active layer. The insertion of an optical spacer is a common strategy to improve the photocurrent in organic solar cells.45–48 Recently, Kim et al.49 reported that, because of the constructive interference from the optical spacer layer, the inverted architecture can be advantageous for colloidal quantum dot solar cells. Here, we report on systematic variations of the ZnO thickness to realize diﬀerent generation proﬁles within the CuInS2 layer. These variations had strong impact on the photocurrent and the external quantum efﬁciency (EQE), as it is expected for an optical spacer layer. However, we demonstrate that also the internal quantum eﬃciency (IQE), which was accurately determined under consideration of optical cavity and parasitic absorption eﬀects,43,50 was highly aﬀected by the optical spacer thickness and exhibited a strong dependence on the excitation wavelength. We present an optical model to describe these experimental ﬁndings with the help of the spectrally and spatially resolved absorption proﬁles, provided by simulations based on the transfermatrix method (TMM)41–43 and making assumptions on the spatial dependence of the collection probability for excess carriers. Using that approach, we present evidence that charges are only collected from a narrow zone in the vicinity of the CuInS2/ZnO interface. We interpret this as a hint that the depletion region is not suﬃciently extenting into the CuInS2 layer, which provides a reasonable explanation for the inferior photovoltaic performance.
A new Hamiltonian for the Topological BF phase with spinor networks<|sep|>Let us recall the basics of the Kitaev model [24] for topological order. Every link e of a lattice is attached a spin which can be up or down. The Hamiltonian has two parts: star operators Hs (or electric part) acting on nodes s, and plaquette operators (magnetic part) Hp acting on plaquettes p, Hs ﬂips the spins adjacent to a node, while Hp measures the product of spins around a plaquette. One can check that [Hs, Hp] = 0. The ground states are those states which are preserved by the all Hs and Hp. On a Riemann surface of genus h, the ground state degeneracy is 2(2h) and protected by a gap. It is useful to understand the model as a lattice gauge theory for the group Z2. There is an element of Z2 on each link. Invariance under Hs means local Z2 invariance, which the Z2 Gauß law. Invariance under Hp means that the Z2 magnetic ﬂux is trivial through each plaquette. This way of thinking generalizes to compact Lie group, such as SU(2). Let us put a SU(2) element ge on each link, called its holonomy. Gauge invariance requires invariance under SU(2) translations on each node. Triviality of the magnetic ﬂuxes means that the product of holonomies around each plaquette is the unit of the group, where δ is the Dirac function over the group (with respect to the Haar measure). It is well-deﬁned for instance on Riemann surfaces of genus higher than two and is then independent of the lattice used to deﬁne it [18]: this is the meaning of saying it is topological. Conditions for convergence, divergence and invariance are given in [62]. This lattice model is actually an exact discrete realization of a ﬁeld theory, called BF topological ﬁeld theory, and known as the zero-coupling limit of Yang-Mills theory [18, 19]. Though it is a ﬁeld theory, it only has a ﬁnite number of degrees of freedom at the classical level which are determined by spacetime topology. At the quantum level, it is mostly known from the fact that its partition function is the integral of the analytic torsion over the moduli space of ﬂat connections, thus generalizing the two-dimensional case [18]. The continuum action is in dimension d, where B is a su(2)-valued (d−2)-form, and A a su(2)-valued connection 1-form, whose curvature tensor is F(A). Quite clearly, B is a Lagrange multiplier imposing the vanishing of F(A). In addition to the standard SU(2) gauge invariance, there is another gauge invariance (which actually contains diﬀeomorphism invariance), in which B transforms like B �→ B + dAη. This symmetry is responsible for the disappearance of all local degrees of freedom. The path integral treatment was done in [13], and most of the Hamiltonian analysis and canonical quantization in [12]. We refer to [8–11] for the relevance of this theory in the eﬀective description of topological insulators and topological superconductors. Thanks to its simplicity, it provides spin network states with a dynamics we are able to control [32, 52] in the case d = 3 2. The situation is more subtle in higher dimensions and under current investigations. And in spite of its apparent simplicity, it produces an interesting connection with a non-trivial equation from Lie group representation, the Biedenharn-Elliott identity. The relation to the SU(2) lattice model is the following. The holonomies ge along the links are the Wilson lines of the gauge ﬁeld A. The SU(2) action at each node of the lattice comes from the eﬀect of gauge transformations on those Wilson lines: they transform it only on its end points. Hence, the Gauß law indeed asks for translation invariance at each node. The ﬁeld strength F(A) is regularized as usual in lattice gauge theory, by Wilson loops. In particular, the ﬂatness equation F(A) = 0 becomes the statement that Wilson loops around the plaquettes are trivial.
Measurement of the time-dependent CP asymmetry in B0 -> J/{\psi} KS0 decays<|sep|>The source of CP violation in the electroweak sector of the Standard Model (SM) is the single irreducible complex phase of the Cabibbo-Kobayashi-Maskawa (CKM) quark mixing matrix [1,2]. The decay B0 → J/ψK0 S is one of the theoretically cleanest modes for the study of CP violation in the B0 meson system. Here, the B0 and B0 mesons decay to a common CP-odd eigenstate allowing for interference through B0–B0 mixing. In the B0 system the decay width diﬀerence ∆Γd between the heavy and light mass eigenstates is negligible. Therefore, the time-dependent decay rate asymmetry can be written as [3,4] Here B0(t) and B0(t) are the states into which particles produced at t = 0 as B0 and B0 respectively have evolved, when decaying at time t. The parameter ∆md is the mass diﬀerence between the two B0 mass eigenstates. The sine term results from the interference between direct decay and decay after B0–B0 mixing. The cosine term arises either from the interference between decay amplitudes with diﬀerent weak and strong phases (direct CP violation) or from CP violation in B0–B0 mixing. In the SM, CP violation in mixing and direct CP violation are both negligible in B0 → J/ψK0 S decays, hence CJ/ψ K0 S ≈ 0, while SJ/ψ K0 S ≈ sin 2β, where the CKM angle β can be expressed in terms of the CKM matrix elements as arg |−VcdV ∗ cb/VtdV ∗ tb|. It can also be measured in other B0 decays to ﬁnal states including charmonium such as J/ψK0 L, J/ψK∗0, ψ(2S)K(∗)0, which have been used in measurements by the BaBar and Belle collaborations [5,6]. Currently, the world averages are SJ/ψ K0 S = 0.679 ± 0.020 and CJ/ψ K0 S = 0.005 ± 0.017 [7]. The time-dependent measurement of the CP parameters SJ/ψ K0 S and CJ/ψ K0 S requires ﬂavour tagging, i.e. the knowledge whether the decaying particle was produced as a B0 or a B0 meson. If a fraction ω of candidates is tagged incorrectly, the accessible timedependent asymmetry AJ/ψ K0 S(t) is diluted by a factor (1 − 2ω). Hence, a measurement of the CP parameters requires precise knowledge of the wrong tag fraction. Additionally, the asymmetry between the production rates of B0 and B0 has to be determined as it aﬀects the observed asymmetries. In this Letter, the most precise measurement of SJ/ψ K0 S and CJ/ψ K0 S to date at a hadron collider is presented using approximately 8200 ﬂavour-tagged B0 → J/ψK0 S decays.
GMRT Radio Halo Survey in galaxy clusters at z = 0.2 -- 0.4. II.The eBCS clusters and analysis of the complete sample<|sep|>Radio and X–ray observations of galaxy clusters prove that the intracluster medium (ICM) in clusters of galaxies is a mixture of hot plasma, magnetic ﬁeld and relativistic particles. While X–ray observations reveal the presence of diffuse hot gas, the spectacular synchrotron radio emission extended on the Mpc scale and observed in a growing number of massive clusters is the signature of the the presence of relativistic electrons (with Lorentz factor γ >> 1000) and magnetic ﬁelds spread over the whole cluster volume (e.g. Feretti 2005). Recent studies revealed that the magnetic ﬁelds in galaxy clusters are weak, with strength in the range ∼ 0.1 − 1 µG (for recent reviews see Govoni & Feretti 2004 and Ferrari et al. 2008). The recently discovered hard X–ray tails in excess of the thermal Bremsstrahlung spectrum in a few galaxy clusters (Fusco–Femiano et al. 2004; Rephaeli et al. 2008; Fusco–Femiano & Orlandini 2008) are also considered a piece of evidence in favour of a non–thermal component in the ICM (see also Rossetti & Molendi 2007 for further discussion on this issue). The extended cluster radio emission may take the form of radio halos, relics and core–halos (or mini–halos). While the latter reach extensions of the order of <∼500 kpc, are associated with the dominant galaxy in cooling core clusters and are thought to be related to the radio emission of the central AGN, halos and relics are much larger in size (reaching and exceeding the Mpc scale) and are not associated with AGN activity in individual galaxies. Radio halos are usually located at the centre of galaxy clusters, and show a fairly regular morphology; relics are found at the cluster periphery, are highly polarized and exhibit a variety of radio morphologies, the most common being sheet, arc, toroids. There is some consensus on the fact that the origin of radio relics resides in cluster mergers and/or matter accretion: the strong peripheral shocks developing during these energetic events may be eﬃcient particle accelerators (Sarazin 1999; Ryu 2003; Pfrommer 2006). The origin of the synchrotron radio emission in halos is still an open problem, since the life–time of the electrons emitting synchrotron radiation is much shorter than the diﬀusion time necessary to cover their large extent, typically of the order of the Mpc. Historically, two main possibilities have been suggested to account for the existence of radio halos: (1) “re–acceleration models”, or “primary electron” models, where electrons are re–accelerated in–situ, due to the turbulence injected in the cluster volume by massive merger events (e.g. Brunetti et al. 2001; Petrosian 2001; and the review papers by Brunetti 2003; Sarazin 2004; Petrosian & Bykov 2008); (2) “secondary electron models”, which predict that relativistic electrons are secondary products of hadronic collisions between the cluster ICM and cosmic rays (e.g. Dennison 1980; Blasi & Colafrancesco 1999; Dolag & Enßlin 2000).
Maintainable Log Datasets for Evaluation of Intrusion Detection Systems<|sep|>Cyber attacks pose a threat to network and system security at any scale. To achieve their goals, which usually range from intrusion, espionage, sabotage, and system takeover, adversaries typically utilize a wide range of tools and attack techniques to discover previously unknown vulnerabilities and ﬁnd new attack vectors. While system operators seek to keep their network components patched, the ever-changing threat landscape implies that ultimate security is impossible to guarantee as networks continue to grow dynamically over time. To counteract these problems, manual security-related tasks of system operators have long been supported by automatic tools that continuously monitor networks and systems for both known and unknown threats. Thereby, these so-called intrusion detection systems (IDS) usually ingest network traﬃc or system log data and analyze their contents for malicious activities. Many IDSs also carry out ﬁle integrity checks or scan registry keys and system memories; however, in the context of this paper we solely focus on intrusion detection techniques that leverage log data, i.e., sequentially generated and chronologically ordered events that usually comprise a timestamp and a message containing parameters. IDSs that analyze such log data are most often diﬀerentiated into signature-based detection systems, that search for predeﬁned indicators such as hash sums that are known to correspond to malware, and anomaly-based detection systems, that employ self-learning techniques to capture the baseline system behavior and detect any deviations of this learned model as potential threat [1, 2]. Independent of their type, evaluating IDSs for their ability to detect attacks is crucial to compare diﬀerent approaches and objectively select appropriate detection techniques for speciﬁc system environments. Thereby, publicly available benchmark log datasets are an indispensable prerequisite to enable evaluations. Unfortunately, such log datasets are scarce and usually do not fulﬁll the requirements set by security researchers. In particular, one of the most crucial aspects of evaluations is to compute detection accuracies, which requires a ground truth that speciﬁes all malicious log events. However, datasets collected from real infrastructures generally lack a reliable ground truth as it is not possible to ensure that only normal and benign activities are carried out on the network, except for purposefully injected attacks [3]. Moreover, adjusting conﬁgurations of components in productive environments or launching attack cases is often only possible in a limited scope since the security and availability of these systems are of utmost importance to the organizations hosting the infrastructures [4]. In addition, datasets collected in real environments most often cannot be published due to privacy concerns as log data frequently contains user data or parts of sensitive ﬁle contents. To alleviate problems with real infrastructures altogether, security analysts recreate networks and systems in testbeds and use simulations to generate a base load of normal system operation. However, even datasets created in such controlled environments have been criticized for several reasons, for example, missing documentation that explains installed services [5, 6], limited generalizability [6], outdated or too simple attack cases [7, 8], heavy preprocessing such as removal of event parameters [9], involvement of closed-source software [8, 4], lack of periodic behavior [5], missing reproducibility [4], insuﬃcient duration [10], focus on single hosts rather than the whole network [5], or lack of variations of attack parameters [3]. In addition, testbeds generally require a high eﬀort to setup, conﬁgure, update, and adjust components. In our earlier work [11], we therefore propose to introduce concepts from model-driven engineering in testbed deployment processes. Figure 1 visualizes our procedure for generating labeled log datasets from model-driven testbeds. Contrary to common testbed generation approaches that result in single static test environments, our approach implies to generate models for infrastructure setup, normal behavior simulation as well as attack execution that act as templates by leaving out several parameters as variables, and to deﬁne transformation rules that dynamically ﬁll out these parameters when launching a testbed. The main advantage of this methodology is that it is simple to generate an arbitrary number of datasets that stem from diﬀerent testbeds with variations, i.e., normal and malicious traces are slightly diﬀerent across datasets and thus enable more robust evaluations. We recognize some shortcomings of our implementation, including a fairly simple network structure and an unreliable labeling strategy. To overcome these problems, we largely extend the scope of our simulation and integrate an automatic labeling mechanism [12]. Alongside with this paper, we publish a collection of log datasets generated with the presented approach as well as all code that is necessary to run our testbed and simulations within it so that other researchers are able to replay or augment the simulation runs. Our datasets are therefore maintainable and allow for continuous improvements such as enlargements of the labeling range as well as additions of datasets from new testbeds. We summarize our contributions as follows: The remainder of this paper is structured as follows. Section 2 reviews existing log datasets. In Sect. 3 we outline our methodology for generating log datasets and explain our modeled scenario. We analyze the generated datasets in Sect. 4 and discuss the results in Sect. 5. Finally, Sect. 6 concludes the paper.
On the Use of Policy Iteration as an Easy Way of Pricing American Options<|sep|>An American option is a ﬁnancial instrument that gives its buyer the right, but not the obligation, to buy (or sell) an asset at an agreed price at any time up to a certain time T. When working in a partial diﬀerential equation (PDE) framework, the option value V = V (t, S), where t ∈ [0, T] and S ∈ R+ denote time and value of the underlying stock, respectively, is usually (e.g. cf. [7, 30]) the solution of a linear complementarity problem (LCP) with terminal condition V (T, S) = P(S), where L is a linear (parabolic) diﬀerential operator and P = P(S) denotes the payoﬀ of the option. Furthermore, if a fully implicit or weighted time-stepping scheme is applied to the operator L in the above LCP, one usually has to solve a discrete LCP in the form of Problem 1.1 at every time step (cf. [7, 29, 32]). Here, N ∈ N denotes the length of the space grid. A simple example is given at the start of Section 3. Problem 1.1. Let A ∈ RN×N be an M-matrix, and let b, c ∈ RN be vectors. Find x ∈ RN such that Date: September 20, 2011. The authors acknowledge support from Balliol College, University of Oxford, the UK Engineering and Physical Sciences Research Council (EPSRC), and the Oxford-Man Institute of Quantitative Finance, University of Oxford.
Space Efficient Breadth-First and Level Traversals of Consistent Global States of Parallel Programs<|sep|>Parallel programs are not only diﬃcult to design and implement, but once implemented are also diﬃcult to debug and verify. The technique of predicate detection [16,11] is helpful in veriﬁcation of these implementations as it allows inference based analysis to check many possible system states based on one execution trace. The technique involves execution of the program, and modeling of its trace as a partial order. Then all possible states of the model that are consistent with the partial order are visited and evaluated for violation of any constraints/invariants. A large body of work uses this approach to verify distributed applications, as well as to detect data-races and other concurrency related bugs in shared memory parallel programs [10,13,18,22]. Finding consistent global states of an execution also has critical applications in snapshotting of modern distributed ﬁle systems [1,26]. A fundamental requirement for this approach is the traversal of all possible consistent global states, or consistent cuts, of a parallel execution. Let us call the execution of a parallel program a computation. The set of all consistent cuts of a computation can be represented as a directed acyclic graph in which each vertex represents a consistent cut, and the edges mark the transition from one global state to another by executing one operation. Moreover, this graph has a special structure: it is a distributive lattice [23]. Multiple algorithms have been proposed to traverse the lattice of consistent cuts of a parallel execution. Cooper and Marzullo’s algorithm[11] starts from the source — a consistent cut in which no operation has been executed by any process — and performs a breadthﬁrst-search (BFS) visiting the lattice level by level. Alagar and Venkatesan’s algorithm[2] performs a depth-ﬁrst-search (DFS) traversal of the lattice, and Ganter’s algorithm [14] enumerates global states in lexical order. The BFS traversal of the lattice is particularly useful in solving two key problems. First, suppose a programmer is debugging a parallel program to ﬁnd a concurrency related bug. The global state in which this bug occurs is a counterexample to the programmer’s understanding of a correct execution, and we want to halt the execution of the program on reaching the ﬁrst state where the bug occurs. Naturally, ﬁnding a small counter example is quite useful in such cases. The second problem is to check all consistent cuts of given rank(s). For example, a programmer may observe that her program crashes only after k events have been executed, or while debugging an implementation of Paxos [21] algorithm, she might only be interested in analyzing the system when all processes have sent their promises to the leader. Among the existing traversal algorithms, the BFS algorithm provides a straightforward solution to these two problems. It is guaranteed to traverse the lattice of consistent cuts in a level by level manner where each level corresponds to the total number of events executed in the computation. This traversal, however, requires space proportional to the size of the biggest level of the lattice which, in general, is exponential in the size of the computation. In this paper, we present a new algorithm to perform BFS traversal of the lattice in space that is polynomial in the size of the computation. In short, the contribution of this paper are: – For a computation on n processes such that each process has m events on average, our algorithm requires O(m2n2) space in the worst case, whereas the traditional BFS algorithm requires O( mn−1 n ) space (exponential in n). – Our evaluation on seven benchmark computations shows the traditional BFS runs out of the maximum allowed 2 GB memory for three of them, whereas our implementation can traverse the lattices by using less than 60 MB memory for each benchmark. The exponential reduction in space is sometimes at the cost of a loss in time required to perform the BFS traversal. Our analysis in experimental results
Weak Models of Distributed Computing, with Connections to Modal Logic<|sep|>We introduce seven complexity classes, VVc, VV, MV, SV, VB, MB, and SB, each deﬁned as the class of graph problems that can be solved with a deterministic distributed algorithm in a certain variant of the widely-studied port-numbering model. We present a complete characterisation of the containment relations between these classes, as well as their constant-time counterparts, and identify connections between these classes and questions related to modal logic.
Dynamics of a planar thin shell at a Taub-FRW junction<|sep|>Similar concepts that appear in diﬀerent ﬁelds in physics are not uncommon. We know from classical electromagnetism (EM) that non-overlapping regions of space must be described by piecewise continuous scalar and vector potentials, but the derivatives of these potentials, i.e. electric and magnetic ﬁelds, need not be continuous at the junctions. Discontinuities in the electric and magnetic ﬁelds at the junctions give rise to surface charge and current distributions. The same scenario can be seen in classical general relativity (GR) when one tries to patch together nonoverlapping regions of spacetime described by diﬀerent geometries. In this case, the components of the metric tensor are analogous to the 4-potential in classical EM
Fuzzy Logic based Autonomous Parking Systems -- Part I: An Integrated Multi-functional System<|sep|>The past decade has seen a sharp increase in the number  of vehicles. One of the side effects is the growing difﬁculty  of parking. Searching for parking space is time-consuming.  Besides, scratches and bumps often occur in parking process.  A reliable autonomous-parking module is desired to resolve  the trouble. Key issues in successful autonomous parking  include posture stabilization, steering angle control and path  planning. Various control theories have been proposed to  address these issues. Continuous time-varying feedback and  invariant manifold techniques are proved effective in  achieving posture stabilization in nonholonomic systems([1]-  [2]). The following works ([3]-[4]) present other methods to  stabilize steering angle for unmanned vehicles. Path planning  strategies have also been modelled in [5]-[6]. Recently, fuzzy  logic control in auto-parking has received increasing atten-  tion. Controllers based on fuzzy logic, mimicking human  reasoning, are inherently superiors than other controllers in  handling uncertainties in the parking process([7]-[11]). Most of the past researches on autonomous parking have   a narrow focus. The auto-parking vehicles simply move into  the empty space once it is placed properly in front of a  parking slot. It is still dependent on human observation to  ﬁnd a suitable slot for parking, which cannot be completely  trustworthy. In this paper, an intelligent autonomous parking  system which achieves both slot detection and parking is  proposed. The system is based on fuzzy logic control. Three  fuzzy logic controllers are designed and implemented to  tackle the issues of posture stabilization, steering angle  control and parking decision-making. Parameter tuning of  control rules through experiments yields stable performance. Large car parks are common in rural areas. Parking in  such places is easier and safer due to less environmental  versatility. However, it is time-consuming to ﬁnd an empty  slot. The intelligent autonomous parking system is designed  to facilitate parking in this setting. Car owners can simply  leave their car at the entrance and fetch it at the exit.  The  intermediate process, like searching for an empty slot,  parking, and moving out are handled by the car, with aid of  the control center. Such an intelligent system has great potential in real-life  application. Large companies can implement this system to  reduce wasted time on parking during peak hours. Shopping  centers can built such a car park to avoid congestions.  Neighbourhood can also integrate this system as part of their  public infrastructure.
Indirect Rate-Distortion Function of a Binary i.i.d Source<|sep|>The optimal trade-off between bit-rate and average distortion in the representation of an information source is given by the Rate-Distortion Function (RDF): the RDF provides the minimum rate necessary to describe a source when its reconstruction is allowed to be to within a given average distortion from the original sequence. A natural extension of this source coding problem is the scenario in which the encoder cannot observe the source directly but obtains only noisy observations. This could be due to a number of phenomena such as environmental noise, ﬁnite precision quantization and sub-sampling [1]. In this setup, the encoder is required to describe the source from another process statistically correlated with the source itself: this problem is known as indirect or remote source coding [2, Sec. 3.5]. An interesting motivation for the indirect source coding problem arises in centralized sensing networks in which each sensor in the network is required to transmit its observation to a remote processing unit. Restrictions on the computational complexity and power consumption of the sensors make local processing infeasible and thus the uncompressed data has to be communicated over the network. The communication toward the central unit introduces noise in the sensors’ observations and the compression rate of the data acquired at the central node is determined by the indirect RDF. The general structure of an indirect source coding problem is depicted in Figure 1: the source process, Xn, is passed through the noisy channel P n Y |X to obtain the signal Y n. The encoder compresses the sequence Y n at rate R and the compressed observation is provided noiselessly to the decoder. The receiver produces the sequence � Xn which is a reconstruction of the original signal Xn to within a prescribed average distortion. While in the direct source coding problem the RDF describes the optimal trade-off between the code rate R and distortion D, another quantity of merit in the indirect problem is the channel PY |X. By characterizing the trade-off in the indirect problem, namely by an indirect RDF, it is possible to study the effect of the channel quality on the optimal rate-distortion trade-off. For instance, it is of interest to characterize the amount of additional code-rate needed to maintain a ﬁxed distortion level as the observations become noisier. It has long been noticed [2], [3], [4] that an indirect source coding problem can be reduced to a standard source coding problem by the following argument: it is possible to consider the observable process Y n as the source in the standard source coding problem by amending the ﬁdelity criterion to capture the distance between the reconstructed symbol � Xn and all possible realizations of the original source realization Xn weighed according to the probability of their appearance given Y n. A particularly intuitive form of this observation appears in the case of a quadratic distortion, where the amended ﬁdelity criterion can be decomposed as the sum of two terms: (i) the mean squared error (MSE) estimation of the source from its observation plus (ii) the error in describing the MSE estimate under a rate-limited description [4]. This separation allows one to obtain the closed form expression of the indirect RDF in the Gaussian source, quadratic distortion and additive Gaussian noise case [5], [1]. While, in general, similar separation results for other models do not exist, it may still be possible to solve the direct problem using the amended distortion measure. This approach is explored in this paper for the important case of a binary i.i.d source, bit ﬂipping noise and the Hamming distortion. Related Work: The source coding problem was ﬁrst introduced by Shannon in [6] while he provided the ﬁrst of the source coding theorem in [7]. Indirect rate-distortion problem was ﬁrst introduced by Dobrushin and Tsybakov in [5]. The authors of [5] derived a closed form solution for the indirect RDF in the Gaussian stationary case and, implicitly, showed an equivalence of the indirect problem to a direct source coding problem with an amended ﬁdelity criterion. Wolf and Ziv [4] showed that, in the case of a quadratic distortion, the new ﬁdelity criterion identiﬁed in [3] decomposes into the sum of two terms, only one of which depends on the source coding rate R. Wolf and Ziv also computed the indirect RDF (iDRF) in various cases include the case of a Bernoulli source observed through a binary symmetric channel under quadratic distortion. Since the quadratic distortion of a binary sequence is not larger than its Hamming distance, their result provides a lower bound on the iRDF of the same source under the Hamming distance considered in this work. Berger [2] noted the equivalence of the indirect problem to a modiﬁed direct problem with a new ﬁdelity criterion, and gave an interpretation of the new ﬁdelity criterion as the conditional expectation of the the original distortion measure given the source and noise realizations. In the special case of a Bernoulli observed through a binary channel, the computation of the iRDF is greatly simpliﬁed when the source is symmetric P(Xn = 1) = 1/2 [2, Exc. 3.8]. In our setting where the observation are given by a binary symmetric channel, this iDRF is given by
Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs<|sep|>In this paper, we explore the problem of iterative approximate Byzantine consensus in arbitrary directed graphs. In particular, we prove a necessary and suﬃcient condition for the existence of iterative Byzantine consensus algorithms. Additionally, we use our suﬃcient condition to examine whether such algorithms exist for some speciﬁc graphs. Approximate Byzantine consensus [5] is a natural extension of original Byzantine Generals (or Byzantine consensus) problem [9]. The goal in approximate consensus is to allow the fault-free nodes to agree on values that are approximately equal to each other. There exist iterative algorithms for the approximate consensus problem that work correctly in fully connected graphs [5, 12] when the number of nodes n exceeds 3f, where f is the upper bound on the number of failures. In [6], Fekete studies the convergence rate of approximate consensus algorithms. Ben-Or et al. develop an algorithm based on Gradcast to solve approximate consensus eﬃciently in a fully connected network [3]. There have been attempts at achieving approximate consensus iteratively in partially connected graphs. In [8], Kieckhafer and Azadmanesh examined the necessary conditions in order to achieve “local” convergence and performed a case study on global convergence in some special graphs. Later, they extended their work to asynchronous systems [2]. In [1], Azadmanesh et al. showed how to build a special network, called Partially Fully Connected Network, in which global convergence is achieved. Srinivasan and Azadmanesh studied the application of iterative approximate consensus in data aggregation, and developed an analytical approach using Markov chains [13, 14]. In [16], Sundaram and Hadjicostis explored Byzantine-fault tolerant distributed function calculation in an arbitrary network assuming a broadcast model. Under the broadcast model, every transmission of a node is received by all its neighbors. Hence, faulty nodes can send false data, but they have to send exactly the same piece of data to all their neighbors. They proved that distributed function calculation is possible if network connectivity is at least 2f + 1. Their algorithm maintains more “history” (a sequence of previous states) than the iterative algorithms considered in this paper. In [18], Zhang and Sundaram studied the suﬃcient conditions for iterative consensus algorithm under “f-local” fault model. They also provided a construction of graphs satisfying the suﬃcient conditions. LeBlanc and Koutsoukos [10] address a continuous time version of the Byzantine consensus problem in complete graphs. Recently, for the broadcast model, LeBlanc et al. have independently developed necessary and suﬃcient conditions for f-fault tolerant approximate consensus in arbitrary graphs [17]; in [11] they have developed some suﬃcient conditions for correctness of a class of iterative consensus algorithms. To the best of our knowledge, characterization of tight necessary and suﬃcient conditions for iterative approximate consensus in arbitrary directed graphs in the presence of Byzantine faults under point-to-point model is still an open problem. Iterative approximate consensus algorithms without any fault tolerance capability (i.e., f = 0) in arbitrary graphs have been explored extensively. The proof of convergence presented in this paper is inspired by the
On the Theory of Transfer Learning: The Importance of Task Diversity<|sep|>Transfer learning is quickly becoming an essential tool to address learning problems in settings with small data. One of the most promising methods for multitask and transfer learning is founded on the belief that multiple, differing tasks are distinguished by a small number of task-speciﬁc parameters, but often share a common low-dimensional representation. Undoubtedly, one of the most striking successes of this idea has been to only re-train the ﬁnal layers of a neural network on new task data, after initializing its earlier layers with hierarchical representations/features from ImageNet (i.e., ImageNet pretraining) [Donahue et al., 2014, Gulshan et al., 2016]. However, the practical purview of transfer learning has extended far beyond the scope of computer vision and classical ML application domains such as deep reinforcement learning [Baevski et al., 2019], to problems such as protein engineering and design [Elnaggar et al., 2020]. In this paper, we formally study the composite learning model in which there are t + 1 tasks whose responses are generated noisily from the function f ⋆ j ◦ h⋆, where f ⋆ j are task-speciﬁc parameters in a function class F and h⋆ an underlying shared representation in a function class H. A large empirical literature has documented the performance gains that can be obtained by transferring a jointly learned representation h to new tasks in this model [Yosinski et al., 2014, Raghu et al., 2019, Lee et al., 2019]. There is also a theoretical literature that dates back at least as far as [Baxter, 2000]. However, this progress belies a lack of understanding of the basic statistical principles underlying transfer learning1: How many samples do we need to learn a feature representation shared across tasks and use it to improve prediction on a new task? In this paper we study a simple two-stage empirical risk minimization procedure to learn a new, j = 0th task which shares a common representation with t different training tasks. This procedure ﬁrst learns a representation ˆh ≈ h⋆ given n samples from each of t different training tasks, and then uses ˆh alongside m fresh samples from this new task to learn ˆf0 ◦ ˆh ≈ f ⋆ 0 ◦ h⋆. Informally, our main result provides an answer to our sampling-complexity question by showing that the excess risk of prediction of this two-stage procedure scales (on the new task) as2, where C(H) captures the complexity of the shared representation, C(F) captures the complexity of the task-speciﬁc maps, and ν encodes a problem-agnostic notion of task diversity. The latter is a key contribution of the current paper. It represents the extent to which the t training tasks f ⋆ j cover the space of the features h⋆. In the limit that n, t → ∞ (i.e., training task data is abundant), to achieve a ﬁxed level of constant prediction error on the new task only requires the number of fresh samples to be m ≈ C(F). Learning the task in isolation suffers the burden of learning both F and H—requiring m ≈ C(F ◦ H)—which can be signiﬁcantly greater than the transfer learning sample complexity. Maurer et al. [2016] present a general, uniform-convergence based framework for obtaining generalization bounds for transfer learning that scale as O(1/ √ m) (for clarity we have suppressed complexity factors in the numerator). Perhaps surprisingly, the leading term capturing the complexity of learning h⋆ decays only in t but not in n. This suggests that increasing the number of samples per training task cannot improve generalization on new tasks. Given that most transfer learning applications in the literature collect information from only a few training tasks (i.e., n ≫ t), this result does not provide a fully satisfactory explanation for the practical efﬁcacy of transfer learning methods. Our principal contributions in this paper are as follows: • We introduce a problem-agnostic deﬁnition of task diversity which can be integrated into a uniform convergence framework to provide generalization bounds for transfer learning problems with general losses, tasks, and features. Our framework puts this notion of diversity together with a common-design assumption across tasks to provide guarantees of a fast convergence rate, decaying with all of the samples for the transfer learning problem. • We provide general-purpose bounds which decouple the complexity of learning the task-speciﬁc structure from the complexity of learning the shared feature representation. Our results repose on a novel user-friendly chain rule for Gaussian processes which may be of independent interest (see Theorem 7). Crucially, this chain rule implies a form of modularity that allows us to exploit a plethora of existing results from the statistics and machine learning literatures to individually bound the sample complexity of learning task and feature functions. • We highlight the utility of our framework for obtaining end-to-end transfer learning guarantees for several different multi-task learning models including (1) logistic regression, (2) deep neural network regression, and (3) robust regression for single-index models.
Metallicity of high stellar mass galaxies with signs of merger events<|sep|>In the most commonly accepted cosmological paradigm, galaxy interactions and mergers play a crucial role in determining galaxy properties and are considered as one of the main mechanisms by which galaxies experience signiﬁcant changes in morphology, stellar population content, and star formation activity (e.g., Barton et al. 2000; Lambas et al. 2003; Alonso et al. 2006). Thus, it is generally accepted that the diﬀerent merger histories of galaxies deﬁne their evolution, origin, and present day properties. It is also important to consider the chemical properties of galaxies. These can provide fossil records of their history of formation (Freeman & Bland-Hawthorn 2002) since the metallicity content of a galaxy is expected to depend strongly on its evolutionary state, namely, how and when was the gas transformed into stars. The relation between interactions/mergers and chemical properties have been studied by different authors (Donzelli & Pastoriza 2000; M´arquez et al. 2002; Fabbiano et al. 2004; Kewley et al. 2006a; Michel-Dansac et al. 2008). The underlying idea in these studies has been that a close companion can induce gas inﬂows which lower the metallicity in the central regions of galaxies (Kewley et al. 2006a; Michel-Dansac et al. 2008). A stellar mass-gas metallicity relation has been well established (hereafter MZR, Tremonti et al. 2004). Although it is a clearly observed trend of increasing gas-phase metallicity with stellar mass, there is considerable scatter in the relation, which could be attributed to the particular star formation history of galaxies. Accepted theories for the origin of the MZR have as a central proposition that eﬃcient galactic outﬂows remove metals from galaxies with shallow potential wells (Larson 1974; Dalcanton 2007; Finlator & Dav´e 2008). In this context, mergers and interactions could play an important role in determining the shape and scatter of the MZR. Kewley et al. (2006a) detected a shift in the luminosity metallicity (LZ) relation towards lower metallicities by ≈ 0.2 dex for galaxy pairs of a given luminosity, compared to a control sample. The spectra analysed in this work corresponded only to the central 10% of the galaxy and the authors interpreted this result as a signature of metal-poor gas being funneled into the center of the galaxies. Following this approach, Ellison et al. (2008) studied the LZ relation of 1716 galaxies with companions with radial velocity ∆V < 500 km.s−1 and projected separation rp < 80 kpc h−1, and they found an oﬀset to lower metallicities (by ≈ 0.1 dex) for a given luminosity in pair galaxies. Michel-Dansac et al. (2008) studied the stellar mass - gas metallicity relation of galaxies in close pairs (morphologically classi
Supervised Domain Adaptation using Graph Embedding<|sep|>It has been repeatedly shown that deep convolutional neural networks (CNNs) perform remarkably well for various supervised tasks in computer vision [1, 2]. However, the usefulness of these deep learning methods for real-world applications are limited when the training data is scarce. This is because the convolutional ﬁlters require huge amounts of training examples to learn and extract useful features from images [3]. Moreover, the fully connected layers of deep CNNs easily overﬁt small training data because their large capacity allows them to memorise the training examples [4]. Inspired by how humans tackle new learning tasks by relying on prior knowledge from related domains, transfer learning aims to alleviate the issue of data scarcity. The main idea is to leverage information contained in a related larger dataset in order to improve the performance on a smaller dataset. We denote these by source domain DS and target domain DT respectively. More formally, a domain D = {X, p(X)} can be deﬁned as a feature space X and a marginal probability distribution p(X) of samples in that space X = {x1, . . . , xN} ∈ X. We want to learn an objective predictive function f(·) that predicts a label yi of a label space Y from the corresponding sample xi. This constitutes a task, T = {Y, f(·)} [5]. Transfer learning is the attempt of learning a target task TT from both source and target data XS and XT when there is a discrepancy in either the domains (DS ̸= DT ) or tasks (TS ̸= TT ). A source domain and a target domain are said to be different if the feature spaces are not the same (XS ̸= XT ) or their marginal probability distributions are unequal, i.e. p(XS) ̸= p(XT ). In such cases, an effective way to learn the target task is to explicitly map the data to a common, domain invariant, representation. This learning tactic is called domain adaptation (DA). When the domain difference is one of distribution difference (p(XS) ̸= p(XT )), sometimes referred to as domain shift or covariate shift, it is called homogenous domain adaptation [6]. This is the most prevalent type domain adaptation and also the focus of our effort. In this line of research, it is usually assumed that the source and target label spaces are the same. Depending on the availability of labels in the target domain, DA methods are categorised further into three buckets [6]. Supervised DA methods assume that the available target data is labelled albeit small. In the semi-supervised setting, in addition to a small amount of labelled target data, a larger amount of unlabelled target data is available. If only unlabelled target data is available, is it denoted unsupervised. The reader should note that many different and occasionally conﬂicting deﬁnitions of semi-supervised and unsupervised DA were presented in Fig. 1: Siamese network architecture for domain adaptation. Samples from the source and target domains are introduced to an embedding function ϕ consisting of a convolutional neural network. This produces a lower dimensional embedding for which an inter-domain loss can be computed. A prediction function h uses these embeddings to predict corresponding labels, for which a categorical cross-entropy loss is applied to evaluate each prediction at training time. Network updates are performed jointly from cross-entropy and domain adaptation losses, and network parameters are shared across both streams. [5, 6, 7, 8, 9, 10, 11]. When training data (e.g. images) is scarce, it might be straightforward to acquire a large number of images by scraping the web. However, the process of labelling these images is time-consuming and costly because of the manual human labour associated with the data annotation process. It is, therefore, of no surprise that the majority of domain adaptation methods in the literature focus on the unsupervised settings. However, in the case where the number of samples per class is very small, supervised domain adaptation approaches outperform the unsupervised methods [12]. A popular approach employed in the few-shot supervised DA methods is to learn a feature transformation that maps same-class samples close together in a common latent space while pushing samples with different labels farther apart [12], [13]. This can be done by utilising a Siamese network architecture [14] (see Fig. 1) and feeding pairs of samples from both domains as input. A loss function is deﬁned at the output of the feature extractor of a deep CNN to encourage it to generate domain-invariant features. In our work, we view domain adaptation from the vantage point of dimensionality reduction, and introduce a generic framework for the supervised DA problem using graph embedding. Inspired by Yan et al. [15], we create two graphs. An intrinsic graph allows us to encode within-class compactness criteria and a penalty graph models between-class separability criteria. By connecting samples across domains, we can enforce desired properties such as domain invariance. One of the key advantages of this approach is that domain knowledge and design decisions can be easily encoded as rules of graph construction. We show how to construct an effective graph for domain adaptation inspired by Linear Discriminant Analysis. Traditionally, graph embedding methods are optimised by solving the generalised eigenvalue problem. Recently, an unsupervised DA method solving the eigenproblem was proposed [16]. By contrast, our method uses the Siamese network architecture and is optimised in an end-to-end fashion using Backpropagation. Similar to other representation learningbased DA methods, we want to learn a non-linear feature transformation which generates domain-invariant and semantically meaningful features. Our contributions are: 1) We propose a graph-based domain adaptation method which performs on par with the current state-of-the-art. Graphs are conceptually easy to understand and it is straightforward to tailor them to speciﬁc problems. 2) We show how graph embedding-based domain adaptation methods can be optimised in an end-to-end fashion in a deep neural network instead of solving the generalised eigenvalue problem. Our method can scale when the number of samples in the source domains is very large. The rest of the paper is structured as follows. In Section II, we discuss other existing methods in the supervised DA setting. Section III details how we derive a general framework by posing the domain adaptation problem as dimensionality reduction. Moreover, we present a conﬁguration of the proposed Domain Adaptation using Graph Embedding (DAGE) framework inspired by Linear Discriminant Analysis. We evaluate the algorithm on few-shot domain adaptation tasks using two canonical image datasets Ofﬁce31 and MNIST to USPS in Section IV. Finally, in Section V we summarise and provide suggestions for future research.
Elliptical Insights: Understanding Statistical Methods through Elliptical Geometry<|sep|>Whatever relates to extent and quantity may be represented by geometrical ﬁgures. Statistical projections which speak to the senses without fatiguing the mind, possess the advantage of ﬁxing the attention on a great number of important facts. Michael Friendly is Professor, Psychology Department, York University, 4700 Keele St, Toronto, Ontario, M3J 1P3, Canada e-mail: friendly@yorku.ca. Georges Monette is Associate Professor, Mathematics and Statistics Department, York University, 4700 Keele St, Toronto, Ontario, M3J 1P3, Canada e-mail: georges@yorku.ca. John Fox is Senator William McMaster Professor of Social Statistics, Department of Sociology, McMaster University, 1280 Main Street West, Hamilton, Ontario, L8S 4M4, Canada e-mail: jfox@mcmaster.ca. This is an electronic reprint of the original article published by the Institute of Mathematical Statistics in Statistical Science, 2013, Vol. 28, No. 1, 1–39. This reprint diﬀers from the original in pagination and typographic detail. In the beginning, there was an ellipse. As modern statistical methods progressed from bivariate to multivariate, the ellipse escaped the plane to a 3D ellipsoid, and then grew onward to higher dimensions. This paper extols and illustrates the virtues of the ellipse and her higher-dimensional cousins for both didactic and data analytic purposes. When Francis Galton (1886) ﬁrst studied the relationship between heritable traits of parents and their oﬀspring, he had a remarkable visual insight— contours of equal bivariate frequencies in the joint distribution seemed to form concentric shapes whose outlines were, to Galton, tolerably close to concentric ellipses diﬀering only in scale. Galton’s goal was to to predict (or explain) how a characteristic, Y , (e.g., height) of children was related to that of their parents, X. To this end, he calculated summaries, Ave(Y |X), and, for symmetry, Ave(X|Y ), and plotted these as lines of means on his diagram. Lo and behold, he had a second visual insight: the lines of means of (Y |X) and (X|Y ) corresponded approximately to the locus of horizontal and vertical tangents to the concentric ellipses. To complete the picture, he added lines showing the major and minor axes of the family of ellipses, with the result shown in Figure 1. 1
Stabilizing Open Quantum Systems by Markovian Reservoir Engineering<|sep|>The dynamics of open quantum systems and especially the possibility of controlling it have attracted signiﬁcant interest recently. One of the fundamental tasks of interest is the stabilization of quantum states in the presence of dissipation. In recent years a large number of articles have been published on control of closed quantum systems or, more precisely, on systems that only interact coherently with a controller, with applications from quantum chemistry to quantum computing [1]. The essential idea in most of these articles is open-loop Hamiltonian engineering by applying control theory and optimization techniques. Although open-loop control design is a very important tool for controlling quantum dynamics, it has limitations. For instance, while open-loop Hamiltonian engineering can be used to mitigate the eﬀects of decoherence, e.g., using dynamic decoupling schemes [2], or to implement quantum operations on logical qubits, protected against errors due to environmental interactions by a redundant encoding [3], Hamiltonian engineering has intrinsic limitations. One task that is diﬃcult to achieve using Hamiltonian engineering alone is stabilization of quantum states. Alternatively, we can try to engineer open quantum dynamics described by a Lindblad master equation [4, 5] by changing not only the Hamiltonian terms but also the dissipative terms. Various ideas along these lines have been proposed in several articles [6–11]. There are two major sources of dissipative terms in the Lindblad equation: the interaction of the system with its environment, and measurements we choose to perform on the system. Accordingly, we can engineer the open dynamics by either modifying the system’s reservoir or by applying a carefully-designed quantum measurement. In this sense, the quantum Zeno eﬀect is a simple model for reservoir engineering [12]. In addition, the open dynamics can be modiﬁed by feeding the measurement outcome (e.g. the photocurrent from homodyne detection) back to the controller. This idea was ﬁrst proposed in [11], where a feedback-modiﬁed master equation was derived and it was shown in [6] that such direct feedback could be used to stabilize arbitrary single qubit states with respect to a rotating frame. More recently, there have been several attempts to extend this work to stabilize maximally entangled states using direct feedback [6–10]. The idea of reservoir engineering can also be used to stabilize the system in the decoherence-free subspace (DFS) [13]. In [14], it is illustrated that N atoms in a cavity can be entangled and driven into a DFS. In [15], several interesting physical examples are presented showing how to design the open dynamics such that the system can be stabilized in the desired dark state. Such stabilization problems are a motivation for thorough investigation of the properties of a Lindblad master equation. Important questions include, for instance, which states can be stabilized given a certain general evolution of the system and certain resources. There are a number of classical articles discussing the stationary states and their (asymptotic) stability, as well as suﬃcient conditions for the existence of a unique stationary state [16–21]. More recently, a detailed analysis of the structure of the Hilbert space with respect to the Lindblad dynamics was carried out in [22, 23], implying that all stationary states are contained in a subspace of the Hilbert space that is attractive. Necessary and suﬃcient conditions for the attractivity of a subspace or a subsys tem have been further considered in [24? ]. Nonetheless there are still important issues that deserve further study. One is the issue of asymptotic stability of stationary states. It is often assumed that uniqueness implies attractivity of a steady state. Although this turns out to be true for the Lindblad equation, it does not follow trivially from the linearity of the master equation, and a rigorous derivation of this result is therefore desirable, as is a summary of various suﬃcient conditions for ensuring uniqueness of a stationary state. Similarly, linear dynamical systems can have invariant sets or center manifolds surrounding the set of steady states. The existence of such invariant sets usually precludes converges of the system to a steady state, but criteria for the existence of non-trivial invariant sets are also of interest as they are natural decoherence-free subspaces. Finally, many investigations of the steady states have been based on considering the dynamics on the Hilbert space of the system, e.g., giving criteria for the attractivity of a subspace of the Hilbert space. However, since the steady states are points in the convex set of positive operators on this Hilbert space, such criteria are not always useful. For instance, only systems with steady states at the boundary of the state space (e.g., pure states) have (nontrivial) attractive subspaces of the Hilbert space. While these states may be of special interest, since the states at the boundary form a set of measure zero, most systems will have steady states in the interior. We may not be able to engineer a steady state at the boundary, but perhaps we could stabilize a state arbitrarily close to it, which may be entirely suﬃcient for practical purposes. Thus, complete characterization of the steady states requires considering the set of positive operators on the Hilbert space rather than the Hilbert space itself. The purpose of this article is twofold: (i) to further investigate the properties of the stationary states of the Lindblad dynamics and the invariant set of the dynamics generated by imaginary eigenvalues, including the relationship between uniqueness and asymptotic stability and (ii) to present several suﬃcient conditions for the existence of a unique steady state, apply them to diﬀerent physical models, and show how these criteria could in principle be used to stabilize an arbitrary quantum state using Hamiltonian and reservoir engineering. In Sec. II, we introduce the Bloch representation of Lindblad dynamics, which will be used throughout the article. In this representation, the spectrum of the dynamics can be easily derived and stability analysis can be conveniently presented. In Sec. III, we characterize the set of all stationary states as a convex set generated by a ﬁnite number of extremal points, analyze the properties of the extremal points and give several suﬃcient conditions for the uniqueness of the stationary state. We also state a theorem that uniqueness implies attractivity, which is proved in the appendix. In Sec. IV these conditions are applied to diﬀerent systems including two and four-level atoms, the quantum harmonic oscillator, and composite and decomposable systems, and several useful results are derived, including: (i) if the Lindblad terms include the annihilation operator, then the system has a unique stationary state regardless of the other Lindblad terms or the Hamiltonian; (ii) for a composite system, if the Lindblad equation contains dissipation terms corresponding to annihilation operators for each subsystem, then the stationary state is also unique; (iii) how any pure or mixed state can be stabilized in principle via Hamiltonian and reservoir engineering. Finally, in Sec. V, we discuss the invariant set generated by the eigenstates of the dynamics with purely imaginary eigenvalues, and its relation to decoherence-free subspaces (DFS), including examples how to ﬁnd or design a DFS.
Differential Cross Section of DP-Elastic Scattering at Intermediate Energies<|sep|>The study of the deuteron-proton elastic scattering has a longtime story. The ﬁrst nucleon-deuteron experiments were performed already in ﬁfties of the previous century [1]-[7]. Diﬀerential cross sections [1]-[4] and polarization [5]-[7] were measured at few hundred MeV. Nowadays this reaction is still the subject of investigations [8]-[10]. This process is the simplest example of the hadron nucleus collision that is why the interest to this reaction is justiﬁed. A number of experiments on deuteron- nucleon elastic scattering is aimed at getting some information about the deuteron wave function and nucleon-nucleon amplitudes from Nd scattering observables. Moreover, the study of the reaction mechanisms, investigations of the few-body scattering dynamics are also very important to understand the nature of nuclear interactions. A good theoretical description of the deuteron-nucleon process was obtained for low energies, where the multiple scattering formalism based on the solution of the Faddeev equations, has been applied to solve this problem [11]. However, at the energies above 150 MeV there is some discrepancy between the experimental data and theoretical predictions in the minimum of the diﬀerential cross section. At present many eﬀorts are undertaken to extend the Faddeev calculation technique into the relativistic regime [12]-[15]. But up to now there are no reasonable descriptions of the experimental data obtained in the Faddeev equation framework at intermediate energies. Also p-d scattering at low energies was considered in the approach based on the solution of the three-particle Schr¨odinger equation using the Kohn variational principle (KVP)[16],[17]. Special attention in these works was given to the study of the Coulomb eﬀects. It has been shown that at the energies below 30 MeV the inﬂuence of the Coulomb interaction is appreciable, while it considerably reduces at Tlab = 65 MeV [17]. The high-energy deuteron-proton scattering in the forward hemisphere is successfully described by the Glauber theory which takes both single and double interactions into account [18],[19]. In [20] it is shown that ﬁlling of the minimum, due to the interference between the single- and double-scattering amplitudes, is explained by the presence of the D-wave in the deuteron wave function. However, as it is shown in [21], [22], the oﬀ-energyshell eﬀects begin to play an important role at scattering angles larger than 300 in c.m. In [23] the deuteron-proton backward elastic scattering was considered in the BetheSalpeter approach. Here various relativistic eﬀects were studied. It has been shown, that the relativistic corrections coming from the negative energy P-waves are negligible while Lorentz boost eﬀects become evident already at Plab ∼ 0.2÷0.3 GeV/c (here,Plab is the scattered proton momentum in the laboratory frame). The present paper considers the intermediate energy range from 200 MeV up to 600 MeV of the initial nucleon. On the one hand, the energies are not large enough to apply the Glauber theory. On the other hand, in this region it is already necessary to take relativistic eﬀects into account and as a consequence the Faddeev calculation technique does not work properly at these energies. Nevertheless, at these energies it is still possible to use a non-relativistic deuteron wave function due to transfer into the deuteron Breit frame. Here, it is also very important to describe correctly the nucleon- nucleon vertices, namely the spin structure as well as angular and energy dependences.
Sequential Decentralized Parameter Estimation under Randomly Observed Fisher Information<|sep|>1A quick reminder for the deﬁnitions of the notations o(·), O(·), Θ(·), and ω(·): f(x) = o (g(x)) if f(x) grows with a lower rate than g(x); f(x) = O (g(x)) if f(x) grows with a rate that is no larger than the rate of g(x); f(x) = Θ (g(x)) if f(x) grows with exactly the same rate as g(x); and f(x) = ω (g(x)) if f(x) grows with a larger rate than g(x).
Efficient Data Exchange in Unmanned Aerial Vehicle Networks Utilizing Unsupervised Learning-Based Clustering<|sep|>Unmanned aerial vehicle (UAV) networks have wideranging applications in wireless communications of both military and commercial ﬁeld because of their signiﬁcant advantages. High mobility, high ﬂexibility, and low complexity make UAV networks capable of supporting H. Song is with the Next Generation and Standards (NGS) group, Intel Corporation, Santa Clara, CA 95054, USA (hao.song@intel.com). L. Liu is with Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VT 24060, USA. A. Balasubramanian is with Qualcomm Inc, San Diego, USA. The corresponding author is L. Liu (ljliu@ieee.org). wireless access services when wireless communications infrastructures, like base stations (BSs), are not available or cannot work [1]–[3]. For example, UAV networks are able to provide emergency communications in disaster areas where local infrastructures have already got crippled [4], [5]. Moreover, compared to deploying costly infrastructures, UAV networks are a cost-effective alternative to provide wireless coverage in some special scenarios. For example, UAV networks could be used in sensor networks, ﬂying to sensor nodes and collecting data from them [6]. UAVs are also useful to provide efﬁcient operations in some mechanical motions and routine missions, such as trafﬁc monitoring [7] and agricultural surveillance [8]. Besides, UAVs could enhance communication quality and system capacity by being deployed in communication hotspots and areas with weak signal strength [9]. In this paper, we focus on studying the application of utilizing UAV networks as small aerial BSs, where UAV networks serve as relays, forwarding data from macro BSs to users [1], [2], [9]. In such an application, UAVs need to periodically receive packets from macro BSs. Additionally, it is likely for UAV networks to be deployed far away from macro BSs to cover areas where macro BSs cannot provide effective coverage. Wireless transmissions between UAVs and macro BSs in a longdistance propagation cause weak received signal strength and high lost data rate. Traditionally, lost packets are retrieved through retransmissions of macro BSs [10], [11]. After a UAV receives a packet from a macro BS, it will feed acknowledgement (ACK) back to the macro BS. If the macro BS found that there still are UAVs not receiving transmitted packets according to ACK feedback record, the macro BS will keep retransmitting the packets until all UAVs successfully receive them [12]. Apparently, in UAV networks, lost packets retrieved through retransmissions of macro BSs may cause a severe delay because of long-distance transmissions and a large amount of UAVs. To be speciﬁc, when UAV networks work in a bad wireless environment with high lost data rate, for example located far away from macro BSs, multiple retransmissions may have to be conducted before all UAV successfully receive a packet with a large delay. Moveover, the delay issue may be deteriorated with a large amount of UAVs, as more retransmissions may be encountered with more UAVs. Besides a severe delay, enormous retransmissions may result in serious overhead and power consumption for both UAVs and macro BSs. Essentially, enabling feedback in broadcast communications with large number of users becomes inefﬁcient. This is because UAVs are required to feed back an ACK for each received packet, while the BS has to record and process enormous ACKs from all UAVs. In a UAV network working as a small aerial BS, UAVs normally stay together, working as a swarm to handle complicated missions [13]–[15]. Thus, wireless transmissions between UAVs in a short distance would be more reliable. It would also be more friendly to delay, overhead, and power if the lost packet retrieval can be conducted by data exchange between UAVs rather than by retransmissions of macro BSs. In this paper, an data exchange scheme leveraging unsupervised learning is proposed in UAV networks, enabling intelligent and efﬁcient data exchange between UAVs. In the proposed data exchange scheme, a UAV network is partitioned into multiple clusters using the agglomerative hierarchical clustering, a type of unsupervised learning [16], [17]. Unlike supervised learning and reinforcement learning that require training data and reward information exploration, respectively, to learn a particular model [18], [19], unsupervised learning does not need such data or information, which is easy to be executed. Clustering methods have been proven to be able to improve the performance of spectrum sensing and wireless sensor networks [20], [21]. According to our knowledge, this is the ﬁrst work using clustering in data exchange. Instead of retrieving lost packets by retransmissions of macro BSs, a UAV will request its lost packets to other UAVs in the same cluster, so that lost packets retrieval could be conducted in a parallel fashion to enhance efﬁciency. By utilizing the agglomerative hierarchical clustering, appropriate UAVs will be clustered together, facilitating that UAVs in the same cluster are able to supply and supplement each other’s lost packets. To further improve the efﬁciency of data exchange, a data exchange mechanism is developed to determine the order of UAVs in requesting their lost packets and replying received requests. The priorities of UAVs base on the number of its lost packets or the number of requested packets that it can provide in order to make each requestreply process provide maximum beneﬁts to other UAVs’ lost packets retrieval in the same cluster. In addition, a new structure of contention window is designed based on the carrier sense multiple access with collision avoidance (CSMA/CA) to support the data exchange mechanism. The remainder of the paper is organized as follows. In Section II, system model considered in the paper is presented along with problem analysis of data exchange in UAV networks. The proposed data exchange scheme is introduced in Section III and Section IV, which elaborate the developed clustering method using unsupervised learning and data exchange mechanism, respectively. Section V showS simulation studies to demonstrate the effectiveness of the proposed scheme. Finally, Section VI concludes the whole paper.
Lagrange formalism of memory circuit elements: classical and quantum formulations<|sep|>Circuit elements with memory, namely, memristive1,2, memcapacitive and meminductive3 systems are attracting considerable attention in view of their application in diverse areas of science and technology, ranging from solid-state memories4–6 to neuromorphic circuits7–12 and understanding of biological processes13,14. The general axiomatic deﬁnition of memory elements considers any two fundamental circuit variables, u(t) and y(t) (i.e., current I, charge q, voltage V , or ﬂux φ ≡ � t −∞ V (t′)dt′) whose relation, the response g, depends also on a set, x = {xi}, of n state variables describing the internal state of the system. These variables could be, e.g., the spin polarization of the sample15,16 or the position of oxygen vacancies in a thin ﬁlm17. The resulting n-th order u-controlled memory circuit element is described by3 where f is a continuous n-dimensional vector function. It is assumed on physical grounds that, given an initial state u(t = t0) at time t0, Eq. (2) admits a unique solution. If u is the current and y(t) is the voltage then Eqs. (1), (2) deﬁne memory resistive (memristive) systems. In this case g is the memristance (for memory resistance). In memory capacitive (memcapacitive) systems, the charge is related to the voltage so that g is the memcapacitance (memory capacitance); while in memory inductive (meminductive) systems the ﬂux is related to the current with g the meminductance (memory inductance). These systems are characterized by a typical “pinched hysteretic loop” in their constitutive variables when subject to a periodic input (with exceptions as discussed in Ref. 18). Indeed, we have recently argued that essentially all two-terminal electronic devices based on memory materials and systems, when subject to time-dependent perturbations, behave simply as - or as a combination of memristors, memcapacitors and meminductors3,19. This unifying description is a source of inspiration for novel However, despite the wealth of applications and new ideas these concepts have generated, it is nonetheless important to stress that so far these memory elements have been discussed only within their classical circuit theory deﬁnition, with quantum mechanics entering at best in the microscopic parameters that determine the state variables responsible for memory3,17,22. However, it seems that these features are common at the nanoscale where the dynamical properties of electrons and ions are likely to depend on the history of the system, at least within certain time scales23,24. Mindful of the trend towards extreme miniaturization of devices of all sorts, it is thus natural to ask whether true quantum eﬀects can be associated with the memory of these systems and which phenomena could emerge from the quantization of memory elements. Of course, examples of memory eﬀects in quantum phenomena can be found in the specialized literature (see, e.g., Ref. 25). Here instead, we want to provide a general framework of study of the quantum excitations (memory quanta) associated to general degrees of freedom that lead to memory in these systems. We then ﬁrst introduce the general Lagrange-Euler formalism for these systems. This is the non-trivial extension of the corresponding formalism for the “standard” circuit elements. Since it is well known that the Lagrangian formulation of circuit elements oﬀers great advantages in the analysis of complex circuits26, we expect that this generalization would be of great value in itself. Moreover, our work extends previous studies related to the formulation of Lagrange and Routh equations for non-linear circuits involving ideal memristors27 and to the port-Hamiltonian modeling for the case of memristive components28. In the present context our work also sheds light on the general relation between the internal degrees of freedom that lead to memory and the constitutive variables - the charge, current, voltage and ﬂux that deﬁne the diﬀerent elements. Along the way we also deﬁne mutual meminductors, namely mutual inductors with memory, which add additional ﬂexibility and hence new functionalities to the ﬁeld of memory elements. We ﬁnally proceed to quantize the corresponding equations in the standard way. This leads us to consider the memory excitations of these systems. In this paper we consider only the quantization of non-dissipative elements, and we will devote a subsequent paper to the discussion of quantum eﬀects in dissipative memory elements. We will provide examples of applications of the Lagrangian formalism to selected cases and discuss experimental conditions under which these memory quanta could be detected. This paper is organized as follows. In Sec. II we introduce a general scheme of the approach. Sec. III is dedicated to the Lagrangian formulation of memristive systems, while Secs. IV and V deal with memcapacitive and meminductive systems, respectively. We then show how to write the Lagrangian (Sec. VI) and Hamiltonian (Sec. VII) of a circuit of memory elements and also give the work-energy theorem and generalized Joule’s ﬁrst law for such a circuit. We introduce the concept of memory quanta in Sec. VIII focusing on speciﬁc examples. Finally, in Sec. IX we report our conclusions.
Asymptotic High Energy Total Cross Sections and Theories with Extra Dimensions<|sep|>There is a wealth of possible constraints on new physics which can be obtained without the need to look for speciﬁc exclusive processes which may be diﬃcult to detect and distinguish from backgrounds simply by looking at how cross sections grow with energy. We review simple arguments for how cross sections are expected to grow with energy, and connect them with black hole thermodynamics and string-theoretical models. Allowing for a number of space dimensions greater than 3 we ﬁnd, in agreement with earlier discussions of the FroissartMartin bound in higher dimensions, much faster growth of cross sections with energy than is allowed by unitarity in 3+1 dimensions. Since the experimental results saturate the unitarity bounds, we ﬁnd there is no room for extra dimensions[1] at scales below 100 TeV and point out that the best way to search for such dimensions may well be via measurements of the energy dependence of cross sections rather than via searches for exclusive processes.
On fits to correlated and auto-correlated data<|sep|>In particle physics, physical information is often extracted from ﬁts to correlated data. When their covariance matrix is known well, the minimization of the so-called correlated χ2 is the ideal method to obtain the best estimate of the ﬁt parameters from the statistical point of view. In many cases model functions are guided by theoretical principles, but in practice one has to examine their validity case by case, and often speciﬁc model functions or data points are discarded based on a measure of their goodness-of-ﬁt. For Nx data points normally distributed, and for correlated ﬁts to model functions depending on NA parameters, the expectation value of χ2 (at the minimum) equals the number of degrees of freedom, Nx −NA. When there is a large number of data points, a reduced χ2 close to 1 (deﬁned by the ratio of the observed χ2 over (Nx−NA)), provides a ﬁrst good indication that a given model describes well the data. In addition, the goodness-of-ﬁt is judged by computing the probability Q, often called p-value, of observing a χ2 larger or equal than the observed χ2, which we call χ2 obs. The p-value is easily obtained in closed analytic form due to the exact cancellation between the covariance matrix, C, of the underlying data and C−1 used in the deﬁnition of the correlated χ2. It depends only on χ2 obs and on Nx − NA. However, data sets with limited statistics often lead to close-to-singular C−1. Moreover, for data generated from Monte-Carlo (MC) methods, speciﬁcally from Markov chains, additional complications arise from the presence of auto-correlations, namely correlations along the MonteCarlo history (or “time”). Hence for all cases where reliable estimations of the inverse of the covariance matrix are not possible, and correlated ﬁts can not be performed, the standard χ2-test is not applicable. In this work we point out a robust solution to the problem. Our object of study is primarily lattice QCD, where Monte-Carlo methods based on Markov chains are an essential ingredient to obtain non-perturbative predictions. Field conﬁgurations are generated by successive repetitions of a predeﬁned update scheme, and as a consequence, expectation values of correlation functions, obtained from averages over measurements on these ﬁeld conﬁgurations, are both correlated and auto-correlated. The Γ-method is the preferable choice to estimate the statistical errors of primary and derived observables [1] obtained from a Markov process, but it is diﬃcult to turn it into a practical method for general matrix elements of the covariance matrix.1 Because of these issues and the general diﬃculty in estimating covariances for large number of data points [2], ad hoc modiﬁcations of C−1 are often used, like so-called SVD cuts (see e.g. [3,4]) or uncorrelated ﬁts. We will show how the expected value of χ2 and the goodness-of-ﬁt can be estimated robustly for all these cases. In fact, we emphasize that this can also be done when the weights of the data points are independent of the covariance matrix, which may be a good strategy when ﬁts are used to perform extrapolations away from the region where data exists. Our text is organized as follows: in Section 2 we present the derivation of the formulae for the expectation value of χ2 and the correponding p-value; in Section 3 we describe how to handle auto-correlations, while in Section 4 we present a numerical test in a toy model, before concluding.
Investigating signatures of cosmological time dilation in duration measures of prompt gamma-ray burst light curves<|sep|>Gamma-ray burst (GRB) prompt emission has been detected at high-energies for over 40 years (von Kienlin et al. 2014; Sakamoto et al. 2011; Kaneko et al. 2006; Frontera et al. 2009; Klebesadel et al. 1973). It is only in the last decade, however, that a signiﬁcant fraction of detected GRBs have suﬃcient ground-based follow-up to obtain a redshift measurement. This is largely thanks to the Swift satellite (Gehrels et al. 2004), which combines the capabilities of its wide ﬁeld Burst Alert Telescope (BAT; Barthelmy et al. 2005) with arcsecond positional accuracies of the X-Ray Telescope (XRT; Burrows et al. 2005). With these X-ray positions, ground-based facilities have been able to build a comprehensive sample of GRBs with associated redshift using both photometric and spectroscopy methods in the optical and near IR wavelength regimes (e.g. Hjorth et al. 2012). Knowing the redshift associated with a GRB places strong constraints on many properties of the transient event. Indeed, it was the ﬁrst GRB redshift that ﬁnally settled the debate regarding whether the transients were Galactic or cosmological in origin (Metzger et al. 1997). Even with ground-based telescopes dedicated to GRB follow-up, and target of opportunity (ToO) programmes in place on large aperture facilities, approximately two thirds of Swift GRBs do not have an associated redshift. Additionally, other highenergy instruments such as the Gamma-ray Burst Monitor (GBM; Meegan et al. 2009) on the Fermi satellite cannot provide burst locations with suﬃcient accuracy to allow narrow ﬁeld ground-based facilities to obtain a redshift. With such a large fraction of GRBs lacking redshift, searches within the high-energy prompt light curves for tracers of redshifts have been previously attempted. As GRBs occur at cosmological distances and share a common central engine, it might expected that a signature of cosmological time dilation would be measurable in these light curves. Previous studies have considered variability of the high-energy light curve (Reichart et al. 2001) and the time lag between the same morphological light curve structure being observed in diﬀerent energy bands (Norris et al. 2000) as an indicator of intrinsic burst luminosity. More recently, Zhang et al. (2013) have considered traditional measures of duration, T90 and T50 (Kouveliotou et al. 1993), of a sample of Swift/BAT GRBs in a ﬁxed rest frame energy band. T90 and T50 are the inter
Alternating minimal energy methods for linear systems in higher dimensions. Part II: Faster algorithm and application to nonsymmetric systems<|sep|>In this paper we develop the results of [9]. We consider tensor–structured linear systems, which arise naturally from high–dimensional problems, e.g. PDEs. The number of unknowns grows exponentially w.r.t. the number of dimensions d, which makes standard ∗Partially supported by RFBR grants 12-01-00546-a, 11-01-12137-oﬁ-m-2011, 11-01-00549-a, 12-01-33013, 12-01-31056, Russian Fed. Gov. contracts No. Π1112, 14.740.11.0345, 16.740.12.0727 and EPSRC grant EP/H003789/1 at the University of Southampton. This work was initiated when D.S. was with the Institute of Numerical Mathematics RAS, Moscow. †Max-Planck-Institut für Mathematik in den Naturwissenschaften, Inselstr. 22-26, D-04103 Leipzig, Germany, and Institute of Numerical Mathematics, Russian Academy of Sciences, Gubkina str. 8, 119333 Moscow, Russia (sergey.v.dolgov@gmail.com) ‡University of Southampton, Department of Chemistry, Highﬁeld Campus, Southampton SO17 1BJ, United Kingdom (dmitry.savostyanov@gmail.com) algorithms ineﬃcient even for moderate d. This problem is known as the curse of dimensionality, and is attacked by diﬀerent low–parametric approximations, e.g. sparse grids [32, 3] and tensor product methods [20, 18, 12]. A particularly simple, elegant and eﬃcient representation of high–dimensional data is a linear tensor network, also called the matrix product states (MPS) and tensor train (TT) format. The MPS approach was originally proposed in the quantum physics community to represent the quantum states of many–body systems [10, 19]. This representation was rediscovered as the TT format by Oseledets and Tyrtyshnikov [24], who were looking for a proper method to generalize a low–rank decomposition of matrices to high–dimensional arrays (tensors). The MPS approach came with the alternating least squares (ALS) and density matrix renormalization group (DMRG) [36, 26] algorithms for the ground state problem. The ALS considers the minimization of the Rayleigh quotient over the vectors with a ﬁxed tensor structure, while DMRG does the same allowing the rank of the solution to change. Experiments from quantum physics point out that the convergence of the DMRG is usually notably fast, while the one of the ALS can be rather poor. The general numerical linear algebra context in which the TT format is introduced allows to think more widely about the power of tensor representations. For instance, we can apply DMRG–like techniques to high–dimensional problems other than just the ground state problem, e.g. interpolation of high-dimensional data [25, 31], solution of linear systems [14, 8], fast linear algebra in tensor formats [23]. We can also consider better alternatives to the DMRG, which follow the same alternating linear scheme (ALS) framework, but are numerically more eﬃcient. A tempting goal is to obtain an algorithm which has the DMRG-like convergence and the ALS-like numerical complexity. In [9] we present such an algorithm for a solution of symmetric positive deﬁnite (SPD) linear systems in higher dimensions. The central idea in [9] is to support the alternating steps, i.e. optimization in a ﬁxed tensor manifold, by steps which expand the basis in accordance with some classical iterative algorithms. A steepest descent (SD) algorithm is a natural choice for SPD problems. The enrichment step uses the essential information about the global residual of the large high–dimensional system on the local optimization step, that helps to escape the spurious local minima introduced by the nonlinear tensor formulation and ensure the global convergence. The convergence rate of the whole method can then be established adapting a classical theory. In contrast, optimization in the ﬁxed tensor manifolds can be analyzed via the Gauss–Seidel theory and only local convergence estimates are available [28], which hold only in a (very) small vicinity of the exact soution. The global enrichment step used in algorithms “t + ALS(z)” and “ALS(t + z)” in [9] modiﬁes all components of the tensor train format simultaneously. There is nothing particularly wrong with this, but it is interesting to mix the same steps diﬀerently to obtain the algorithm which works with only one or two neighboring components at once, similarly to the DMRG technique. In this paper we develop such a method, namely the alternating minimal energy (AMEn) algorithm. We prove the global convergence of AMEn and estimate the convergence rate w.r.t. the one of the steepest descent algorithm. We also propose several methods to compute the required local component of the global residual, using either the SVD–based approximation, or incomplete Cholesky decomposition, or low–rank ALS approximation. The rest of the paper is organized as follows. In Section 2 we introduce necessary deﬁnitions and notations. In Section 3 we propose the AMEn algorithm, then we compare it with similar algorithms from [9] and prove the convergence theorem. In Section 4 we discuss eﬃcient methods to compute the required component of the residual. In Section 5 we test the algorithm on a number of high–dimensional problems, including the nonsymmetrical Fokker–Planck and chemical master equations, for which the eﬃciency of the method is not fully supported by the theory. In all examples we observe a convincing fast convergence and high eﬃciency of the proposed method, as well as the advantages of the AMEn algorithm over the previously proposed ones.
A Portable Diagnostic Device for Cardiac Magnetic Field Mapping<|sep|>Chest pain is responsible for one of the highest rates of emergency hospital visits in industrialized countries [1] and accounts for a large proportion of hospital admissions. Statistics show that around 75% of patients who present at the Emergency Department with chest pain do not have a cardiac related condition [2–5], yet they still need to go through a full diagnostic pathway which can take more than 10 hours [2]. This leads to several thousand people occupying bed spaces placing an additional burden on health care systems. A diagnostic that is capable of rapidly stratifying the cases and removing those patients who don’t need an overnight stay is therefore valuable in both triage and cost saving [2]. Magnetocardiography (MCG) involves capturing Magnetic Field Maps (MFM’s) of current distributions resulting from cardiac action potentials [6–14]. It has been shown that MCG gives signiﬁcant improvements in diagnostic capability over an ECG [15–26]. Signiﬁcantly, in this respect, it has been demonstrated that MCG is capable of reliable detection of Non-ST-Elevated Myocardial Infarction (NSTEMI) [15, 22], which are by deﬁnition diﬃcult to detect using ECG. For this reason all ECG negative chest pain patients are treated as having an NSTEMI until other diagnostic results can be obtained [3]. Hence, the short time to produce a MCG (typically <10 minute measurement) dramatically reduces the time for diagnosis and removes otherwise healthy patients earlier in the process and is therefore a tool with obvious clinical beneﬁts. The principle focus of the current research was the creation of a portable MCG device that would be capable of providing a rapid assessment of acute coronary syndrome (ACS) in an Emergency Department. To meet this goal, the device requirements are sensitivity to a magnetic window of between 0.1pT and 300pT, in the frequency range of around 1–40 Hz [27] and a spatial resolution suﬃcient to detect anomalies with a spacing of 10–15cm (for a sensor operated 10cm from the chest wall) [28]. Cardiac MFM devices typically use an array of sensitive magnetometers detectors to collect the magnetic ﬁeld of the heart by simultaneously sampling at many positions across the chest. Sensors include liquid helium cooled SQUID detectors, which have been used in commercially available devices for over 40 years [29], and atomic physics detectors and giant magnetoresistance detectors have also been developed [27,30,31]. These devices are not always suitable for an Emergency Department as the associated apparatus is bulky, they often require liquid helium, specialist training to use, they are ﬁxed in place and typically require an electromagnetically shielded room. In contrast, induction coil magnetometers have been used several times for cardiac magnetic ﬁeld detection by several authors. They meet the demands of signal sensitivity [32–37], they are inexpensive, do not require cooling and can be run from batteries. However earlier eﬀorts required noisy high gain ampliﬁers, large and heavy coils which are unsuitable for magnetic ﬁeld mapping and ﬁxed electronically implemented gradiometer arrangements. Here we present a more compact coil design which when combined with modern analog-to-digital converters (ADC’s) and digital signal processing (DSP) produces a device capable of detecting the cardiac magnetic ﬁeld with an array of 19 sensors. We ﬁrst present the design of the sensors, array and DSP routine. Then we show that this design has the capability of resolving the ﬁeld of the heart within both shielded and unshielded environments.
Linear-Response Dynamics from the Time-Dependent Gutzwiller Approximation<|sep|>Recent advances in ultra-fast spectroscopy allow us to monitor the dynamics of electrons on a femtosecond scale. This is especially interesting for strongly correlated materials, such as high-temperature superconductors [1, 2, 3], since in their case the spectroscopic probe is able to investigate the intra-electronic redistribution of excitation energies before the relaxation via the lattice starts. From the theoretical point of view, this is obviously a challenging problem since it requires a method capable of treating the relaxation dynamics of a strongly correlated system out of equilibrium. In this regard, a state-of-the art approach is the dynamical mean-ﬁeld theory (DMFT) which has recently been applied [4] to the single-band Hubbard model in order to study the doubleoccupancy relaxation after laser excitation. However, for the application to real systems this method is rather demanding from a numerical point of view since it requires the self-consistent solution of complex single-impurity models driven out of equilibrium. In this regard, the time-dependent Gutzwiller approximation (TDGA) is a promising alternative since it joins the numerical simplicity of standard random phase approximation (RPA) with the ability to capture important many-particle eﬀects, as the Mott-Hubbard transition. In a series of papers [5, 6, 7, 8], we have developed the TDGA which is based on a variational Ansatz for the Hubbard model [9, 10] evaluated in the limit of inﬁnite spatial dimensions [11]. This approach has recently been generalised for the study of multi-band Hubbard models [12, 13] and is based on the expansion of the Gutzwiller energy functionals which depend on the density matrix and variational parameters related to the atomic eigenstates. In previous work [5, 6] the latter have been eliminated by assuming that they instantaneously adjust to the density ﬂuctuations (‘antiadiabaticity approximation’). As a result, one obtains an energy functional which only depends on the density matrix and therefore the RPA for density-dependent forces [14, 15] can be applied in order to evaluate response functions. This (approximate) version of the TDGA has been applied successfully to the evaluation of dynamical correlation functions in cuprate superconductors [16] including the optical conductivity [17] and the magnetic susceptibility [18, 19]. It has also been related to Auger spectroscopy by calculating pair excitations in one- [8] and three-band [20] Hubbard models. More recently the TDGA was extended by Schir´o and Fabrizio [21, 22, 23] towards the inclusion of time-dependent variational parameters. Concerning the evaluation of response functions this approach can supersede the ‘antiadiabaticity assumption’, mentioned above, since the double-occupancy dynamics follows from a time-dependent variational principle. However, in Refs. [21, 22] the authors focused on the study of quantum quenches for systems with a homogeneous ground state. In this case, the time dependence is captured by the double-occupancy dynamics whereas the single-particle density matrix is time-independent. Recent developments consider simultaneously the dynamics of the double occupancy and of the density matrix [24, 25]. In this paper we will re-derive the TDGA for a time-dependent Gutzwiller variational wave-function applied to multi-band Hubbard models. Our resulting equations of motion will explicitly capture the coupling between the time-dependent variational parameters and the density matrix. We will analyse these equations in the small-amplitude (i.e., linear response) limit and apply the theory to the evaluation of dynamical charge correlations in the single-band case. It turns out that the previous formulation of the TDGA [5, 6] is recovered in the low-frequency limit. However, the incorporation of ﬂuctuations in the time-dependent density of double occupied states (“doublons”) leads to additional spectral weight above the band-like excitations in very good agreement with exact diagonalisation and DMFT. The paper is organised as follows: In Sec. 2.1 we introduce the time-dependent variational principle which is underlying the present work. Since the corresponding expectation values are evaluated with multi-band Gutzwiller wave-functions the latter are presented in Sec. 2.2. The evaluation of time-dependent matrix elements is performed in Sec. 2.3 which allows for the derivation of the Lagrangian and corresponding equations of motion in Sec. 2.4. Our investigations are speciﬁed for the single-band Hubbard model in Sec. 3, where we also discuss a two-site example which can be treated analytically. Finally, the small amplitude limit of the TDGA is derived in Sec. 4 and discussed in the context of response functions in Sec. 5. Numerical results for the dynamical charge susceptibility are presented in Sec. 6 and compared with dynamical mean-ﬁeld theory (DMFT) and exact diagonalisation. We ﬁnally conclude our investigations in Sec. 7.
Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction<|sep|>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM ’21, October 20–24, 2021, Virtual Event, China © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8651-7/21/10...$15.00 https://doi.org/10.1145/3474085.3475710 the cost of video storage and transmission. As the compression rate increases, these algorithms reduce the bit-rate greatly but also introduce undesirable artifacts, which severely degrade the Quality of Experience (QoE). Besides, the artifacts in low-quality compressed videos also harm the performance of down-stream video-oriented tasks (e.g. action recognition and localization [29, 43], video summarization [1, 32]). Accordingly, video compression artifact reduction, which aims to reduce the introduced artifacts and recover missed details for heavily compressed videos, becomes a hot topic in the multimedia field [7, 11, 21, 33, 34, 36]. Recently, many deep neural network based compression artifact removal works have emerged with significant performance improvement. These works roughly fall into three types: singleimage based [5, 9, 12, 16, 24, 41], various video compression priors based [6, 14, 39], and additional temporal information based [7, 8, 11, 19–21, 33–35, 37], respectively. In detail, [9, 16, 41] are designed for JPEG quality enhancement. These methods can be adapted to videos by restoring each frame individually. [6, 14, 39] consider the fact that I/P/B frames are compressed with different strategies and should be processed by different modules, they take a single frame as input, and ignore the temporal information of videos. In order to remedy this defect, [11, 21] exploit two nearest high-quality peak-quality frames (PQFs) as reference frames, [19, 20] utilize the deep Kalman filter network and capture spatiotemporal information from preceding frames, and [7, 33] employ non-local ConvLSTM and deformable convolution respectively to capture dependency among multiple neighboring frames. In summary, recent works employ either the preceding frames, nearby PQFs, or multiple adjacent frames as reference frames to exploit the spatiotemporal information among video frames. Although these methods have made great progress in this task, their performance is still limited by the narrow temporal scope, which makes them fail to fully exploit the spatiotemporal information in the preceding frames. To address the aforementioned issues, on the one hand, we propose a Recursive Fusion (RF) module for video compression artifact reduction. Specifically, in order to leverage the relevant information in a large temporal scope, we develop a recursive fusion scheme and combine the preceding compensated feature with the current feature recursively with limited extra computational cost. Fig. 1 shows an example that RF exploits the details from far away frames. As we can see, when the RF module is not used, we cannot restore the details in the 56th frame, while our method with the RF module can recover the details of the area between the two scoreboards successfully by exploiting the corresponding areas in the 49-51th frames. This is attributed to the enlarged temporal receptive field of the RF module. On the other hand, it is unreasonable to treat different areas of a frame equally when reconstructing its high-quality
Neutrino masses and cosmology with Lyman-alpha forest power spectrum<|sep|>The ﬂux power spectrum of the Lyman-α (Lyα) forest in quasar absorption spectra is a powerful tool to study clustering in the Universe, at redshifts ∼ 2 − 4. Compared to a model derived from a set of dedicated hydrodynamical simulations, the Lyα-ﬂux power spectrum can provide valuable information on the formation of structures and their evolution. Furthermore, by probing scales down to a few Mpc, the 1D ﬂux power spectrum is also sensitive to neutrino masses through the suppression of power on small scales that neutrinos induce because they become non-relativistic at small redshift and they therefore free-stream during most of the history of structure formation. We here use the 1D Lyα ﬂux power spectrum measured with the DR9 release of BOSS quasar data, and a grid of 36 hydrodynamical simulations having a resolution equivalent to 3×30723 particles in a (100 h−1 Mpc)3 box, to constrain both cosmology and the sum of the neutrino masses � mν. Cosmic Microwave Background (CMB) can also constrain � mν. In the standard thermal history of the Universe, massless neutrinos have an average momentum corresponding to 3.15Tν ∼ 0.58 eV at the epoch of last scattering. For � mν > 3 × 0.58 = 1.7 eV, the neutrinos are still relativistic at recombination, and have no signiﬁcant impact on the primary CMB anisotropies. However, for any mass value, neutrinos leave a signature on the CMB angular power spectrum through the integrated Sachs-Wolf eﬀect and through lensing [3, 4]. The latest limit on � mν from CMB data alone is at the level of 0.7 eV [5]. Lyα data alone have sensitivity to � mν at the level of about 1 eV due to the fact that the scales probed by Lyα forests are in the region where the ratio of the power spectra for massive to massless neutrinos is quite ﬂat (cf. Figure 1). However, a tight constraint on � mν can be obtained by combining CMB data, which probe the initial power spectrum unaﬀected by � mν, and Lyα data, which probe the suppressed power spectrum. Thus, Lyα measures the power spectrum level, deﬁned by σ8 and Ωm, CMB provides the correlations between these parameters and � mν, and the joint use of these two probes signiﬁcantly improves the constraint on � mν compared to what either probe alone can achieve. Figure 1: Linear theory prediction for the matter power spectra with massive neutrinos, normalized to the corresponding massless neutrino case. The grey zone delimits the range of k covered by the 1D Lyα ﬂux power spectrum from the BOSS survey. The layout of the paper is as follows. The ﬁrst part of section 2 presents the upgrades in the Lyα, CMB and Baryon Acoustic Oscillation (BAO) data sets used for this work. The second part summarizes a number of improvements in the methodology: changes in the accounting of the uncertainties of the hydrodynamical simulations, and updates of the likelihood parameters to allow for additional freedom in the IGM model or in the instrumental systematic eﬀects. The main objective of section 3.1 is to present what Lyα data alone have to say about cosmology. The base model we consider is a ﬂat ΛCDM cosmology with massive neutrinos, thereafter referred to as the base ΛCDMν cosmology. We start by giving the constraints measured on the ﬁve relevant parameters (σ8, ns, Ωm, H0, � mν), and we brieﬂy discuss the values of the ‘nuisance’ parameters. In section 3.2, we include additional data, namely several conﬁgurations of CMB data and, occasionally, BAO measurements. We present the results obtained on the parameters of our base ΛCDMν cosmology with various combinations of these data sets. Finally, we discuss extensions to the base ΛCDMν cosmology. We present how Lyα data can contribute to constraining additional parameters through their correlations with parameters that Lyα data are sensitive to; we thus give constraints on the reionization optical depth in section 3.3, and on primordial ﬂuctuations (e.g., the running of the scalar spectral index dns/d ln k and the ratio of tensor to scalar modes r) in section 3.4. We discuss the small impact of the running of ns on the constraints on � mν. We do not discuss here the combined Lyα + CMB constraint on the number of neutrino species Neﬀ, since this is the object of a dedicated study [6]. This paper refers extensively to the earlier paper [2] that reported the ﬁrst constraints on cosmological parameters and total neutrino mass using Lyα data from the SDSS-III/BOSS survey. To simplify the presentation and make it easier for the reader to identify any reference to this earlier paper, we will henceforth refer to it as Paper I. We also refer the reader to [7] for a detailed description of the grid of hydrodynamical simulations used in this work, and to [8] for the implementation of neutrinos and their impact on the 1D ﬂux power spectrum. Deﬁnitions of the most relevant symbols used in this paper can be found in Tables 1 and 2. δ = ρ/ ⟨ρ⟩ . . . . . . . Normalized baryonic density ρ of IGM T . . . . . . . . . . . . . . . Temperature of IGM modeled by T = T0 · δγ−1 T0 . . . . . . . . . . . . . . Normalization temperature of IGM at z = 3 γ . . . . . . . . . . . . . . . Logarithmic slope of δ dependence of IGM temperature at z = 3 ηT0 . . . . . . . . . . . . . Logarithmic slope of redshift dependence of T0 (diﬀerent for z < or > 3) ηγ . . . . . . . . . . . . . . Logarithmic slope of redshift dependence of γ Aτ . . . . . . . . . . . . . . Eﬀective optical depth of Lyα absorption at z = 3 ητ . . . . . . . . . . . . . . Logarithmic slope of redshift dependence of Aτ fSi III . . . . . . . . . . . . Fraction of Si III absorption relative to Lyα absorption fSi II . . . . . . . . . . . . Fraction of Si II absorption relative to Lyα absorption
Electronic Properties of Carbon Nanostructures<|sep|>The carbon nanostructures are the materials whose molecular structure is derived from graphene - the hexagonal carbon plain lattice (Figure 1). Because of their electronic structure, they are the promising materials for the construction of nanoscale devices (quantum wires, nonlinear electronic elements, transistors, molecular memory devices or electron ﬁeld emitters) and the inventions in the material science. The planar geometry of the molecular surface is disrupted by the disclinations in the molecule structure which are most often presented by the pentagons and the heptagons in the hexagonal lattice. This change of the geometry is manifested by the positive or the negative curvature, respectively which can be enlarged by the supply of higher number of the defects. In this way, by the supply of 1 to 5 pentagonal defects, we get conical structures with diﬀerent values of the vortex angle (Figure 2). One more defect can be added and a nanotube is created. This nanostructure can be considered closed as well as opened, i.e. without the cap which contains the pentagonal defects. The second case is more common (Figure 3, left part). The number of the defects can be increased up to 12 and in this way, a completely closed, spherical nanostructure arises (fullerene - Figure 3, middle part). Analogical manipulations with the graphene lattice can be made by the supply of the heptagonal defects (Figure 4). For the case of 12 heptagonal defects, if they are placed appropriately, the wormhole structure is created (Figure 3, right part). A lot of other variants of the graphitic nanostructures can be created using diﬀerent combinations of the pentagonal and the heptagonal defects. Some of them are presented in Figure 5. We investigate the electronic properties of several kinds of the carbon nanostructures. After the explanation of the computational methods, we demonstrate how to utilize these methods for the purpose of the investigation of graphene and some simple forms of the nanostructures - diﬀerent kinds of nanoribbons and their modiﬁcations. Then, we will concentrate on the calculation of the properties of more complicated forms - the graphitic nanocone [1–3] and the graphitic wormhole [4–6]. In the ﬁrst case, we consider the inﬂuence of the additional eﬀects like the spin–orbit coupling and the boundary eﬀects coming from the ﬁnite size and from the extreme curvature of the surface geometry in the tip. In the second case, we investigate the eﬀects which arise in the place of the wormhole bridge. Here, 2 additional eﬀects appear: ﬁrst, the spin–orbit coupling arising in the connecting nanotube and second, the increase of the electron mass due to relativistic eﬀects coming from the extreme curvature of the surface geometry. As the result, the chiral massive electrons should be observed.
Orientational instability and spontaneous rotation of active nematic droplets<|sep|>Numerous life forms around us feature left-right asymmetry or chirality, that is not externally imposed but arises spontaneously through symmetry-breaking events, as in the course of embryogenesis [1, 2]. There exist experimental evidence that in morphogenesis chirality emerges at the level of individual cells and impacts all subsequent stages of the process [3–5]. Chirality of individual cells also aﬀects their mechanical characteristics: individual epithelial cells suspended in the bulk of protein solution may start rotating spontaneously and the direction of this rotational motion is linked to the chirality of the cell [6]. Beyond the biological realm, spontaneous emergence of asymmetry has been identiﬁed at the individual or collective level in the dynamics of chemicallyactive colloids and droplets [7–10]. Here, we demonstrate that chirality may similarly arise spontaneously in isotropic active colloidal systems using a chemicallyactive nematic droplet as a model object. The behavior of liquid crystal (LC) droplets suspended in a bulk ﬂuid is determined by the interplay between hydrodynamic stresses, interfacial tension, surface anchoring and liquid crystal elasticity [11, 12]. For instance, the director ﬁeld conﬁguration within an LC drop may be altered by a change in the bulk ﬂuid’s chemical composition [13]. It is also possible to transform the structure of the droplet interface by tuning its LC conﬁguration [14]. In the examples above, the behavior of LC drops is controlled externally. In contrast, active droplets may act autonomously, powered by the energy released through their chemical activity [15, 16]. In particular, nematic droplets undergoing gradual solubilization in aqueous surfactant solutions may self-propel spontaneously, while their smectic counterparts exhibit formation of ﬁlamentlike structures [17]. Chirality of cholesteric active drops allows them to self-propel along a helical trajectory [18]. We focus here on the behavior of active nematic drops with homeotropic anchoring that are completely isotropic in the absence of any hydrodynamic ﬂow or droplet motion. Such active nematic microdroplets self-propelling in the bulk of surfactant solution were recently observed to alter their propulsion regime depending on the ordering in LC phase: drops in nematic state exhibit helical self-propulsion trajectories that were not found in isotropic droplets [19, 20]. Emergence of the curly trajectories is particularly intriguing, since nematic droplets possess no intrinsic chirality. In that regard, the nature of spontaneous curling of self-propulsion trajectories in nematic drops is fundamentally diﬀerent from the similar phenomenon observed in cholesteric drops [18]. It was further reported that in experiments the hedgehog defect located in the center of a motionless nematic drop with homeotropic anchoring is displaced and becomes a boojum when the droplet self-propels, yet another manifestation of the intimate coupling of the hydrodynamic stresses and elasticity of the LC phase [19]. This observation led to the assumption that the helical motion results from anisotropic stresses caused by the defect displacement which, in turn, is sustained by the ﬂow in the LC. However, a detailed modeling of the coupling of hydrodynamics, physico-chemical activity and transport, and internal droplet structure and elasticity is still lacking, which has so far prevented a full understanding of the emergence of spontaneous chirality in the droplet trajectories. In addition to straight and helical self-propulsion trajectories, active nematic drops also exhibit chaotic behavior [20]. Chaotic self-propulsion trajectories were also observed in isotropic droplets [8, 20, 21], suggesting the existence of a universal mechanism of transition to chaos in active drops. It was recently argued that emergence of random behavior of isotropic active drops is linked to the nonlinear eﬀect of surfactant advection and to the precise physico-chemical mechanism responsible for the ﬂow actuation [22]. Speciﬁcally, the actuation mechanisms of the ﬂow from local physico-chemical gradients along a ﬂuid-ﬂuid interface can be decomposed into two categories: (i) phoretic phenomena typically modeled by a discontinuity of the ﬂow velocity across the droplet’s surface and (ii) the Marangoni eﬀect associated with discontinuous stresses at the interface [23]. The latter is typically considered as the sole mechanism of active droplet mobility [15, 16]. Yet, recent numerical simulations indicate that, in the case when the mobility mechanisms from the two categories act simultaneously, the Marangoni eﬀect may hinder the transition to chaos [22]. On the other hand, ubiquity of chaotic regimes in experiments with active droplets hints that some contribution of diﬀusiophoresis might be important for successful modeling of surfactant-laden interfaces of active LC droplets [8, 20, 21]. In this paper, we propose a minimal model of an active nematic drop suspended in a surfactant solution. Our model captures all three types of self-propulsion trajectories observed experimentally in active nematic drops of increasing radius, i.e. straight, helical, and random trajectories. We employ our model to explain the emergence of chiral trajectories in nematic drops and elucidate the transition to chaos for drops in both isotropic and nematic states. The paper is organized as follows. The problem, model equations and relevant dimensionless parameters are introduced in Sec. II, while Sec. III presents the results of 3D numerical simulations of an active drop in isotropic and nematic states. Finally, we discuss our ﬁndings and perspectives opened by this work in Sec. IV.
Stochastic Processes, Slaves and Supersymmetry<|sep|>Stochastic processes have applications in many areas of physics. They are a natural way of describing the eﬀect of thermal interactions on dynamical systems subject to force ﬁelds of various kinds. These force ﬁelds could be, for example, the inter-atomic potentials represented by an energy landscape appropriate to a molecular model [1]. In another type of application, stochastic processes provide a method for achieving a desired probability distribution for the system. This method is the basis for the theory of stochastic quantisation in quantum ﬁeld theory. It has been exploited in the numerical simulation of quantum ﬁeld theories [2, 3, 4]. One way of investigating the structure of a dynamical system is to subject it to external disturbances. This can be accommodated in the stochastic dynamics by inducing small changes in the motion of the particle either by changing its initial position or by altering the force ﬁeld guiding the particle throughout its motion. If these external inﬂuences are inﬁnitesimal the result, for a given sample from the noise ensemble, is an inﬁnitesimal shift in the path of the particle. It can be regarded as an inﬁnitesimal line element carried by the particle along the path of the original motion. Because the equation describing the evolution of this line element is derived from the original stochastic equation and turns out not to contain the noise term we refer to it as a slave equation and to the inﬁnitesimal line element as a slave variable. The response of the mean position of the system to external inﬂuences, that is its susceptibility, can be calculated once the statistical properties of the slave variables are known. Some aspects of slave variable statistics in a one dimensional case have been investigated in [5]. In higher dimensions there exists also the possibility of examining the evolution and statistical properties of inﬁnitesimal area and volume elements that can be constructed from the the line elements. The evolution of area and volume elements plays a signiﬁcant role in turbulent ﬂuid ﬂow and in magnetohydrodynamics [6, 7, 8, 9, 10, 11, 12]. In the models studied in this paper these higher dimensional area and volume elements can be thought of as the images of area and volume elements in the parameter space of the external ﬁelds. Their statistical properties thus give rise to higher dimensional susceptibilities. These generalised susceptibilities can be readily computed in numerical simulations and their evaluation can help to elucidate the topography of the landscape function. In addition we will ﬁnd that the hierarchy of generalised susceptibilities ﬁts naturally into the supersymmetric structure underlying the stochastic dynamical system. In turn the supersymmetric analysis makes possible the calculation of these quantities. A comprehensive review of material that is relevant particularly to one-dimensional systems and the supersymmetry approach we adopt here is contained in reference [13]. Witten [14] has explained the relationship between supersymmetric quantum mechanics and the topology of manifolds. Making use of the idea that diﬀusion processes can be regarded as quantum mechanics in imaginary time, Tˇanase-Nicola and Kurchan [15, 16] have developed these ideas emphasising the diﬀusion point of view and using it to construct practical simulation techniques for picking out the saddle point structure of the landscape function. In this paper we extend the work of Tˇanase-Nicola and Kurchan [15, 16] and show it can be developed to compute signiﬁcant physical observables. We do this in part by developing an electromagnetic analogy based in the Principle of Minimum Dissipation. This allows us to understand the structure of the physical states of the system in a way that is particularly revealing in the low temperature limit. It can also be the basis of an optimised approximation scheme at higher temperatures though we do not pursue this idea here. The structure of the paper is as follows. In section 2 we describe the basic stochastic model that we study. In section 3 we derive the slave equations, ﬁrst for an inﬁnitesimal change in the initial position of the particle and second for a weak external ﬁeld. We show how they can be encoded in the supersymmetric formalism by introducing an appropriate supersymmetric Hamiltonian. Section 4 contains a brief exposition of the numerical method we use for simulating the stochastic diﬀerential and slave equations. In section 5 we give, for completeness and to establish notation, a brief account of the state space on which the Hamiltonian acts and an outline of the structure of its eigenstates. In section 6, we develop an electromagnetic analogy that helps us to understand the structure of the eigenstates of the supersymmetric Hamiltonian and their time evolution under various circumstances. The analogy is developed in section 7 where we show that the evolution of the eigenstates obeys a Principle of Minimum Dissipation. In section 8, the low temperature limit in which it is possible to compute low lying eigenvalues of the supersymmetric Hamiltonian and associated decay exponents is discussed. The application of the supersymmetric theory to the case of an external electric ﬁeld together with a derivation of the associated Einstein Relation is set out in section 9. Explicit examples of the calculations, in one and two dimensions, are exhibited in sections 10 and 11, where we compare the results of various numerical techniques. Finally we we discuss the results and their potential signiﬁcance in section 12.
The Herschel Virgo Cluster Survey: II. Truncated dust disks in HI-deficient spirals<|sep|>It is now well established that the evolution of spiral galaxies signiﬁcantly depends on the environment they inhabit. The reduction in the star formation rate (e.g., Lewis et al. 2002) and atomic hydrogen (Hi) content (e.g., Giovanelli & Haynes 1985) of galaxies when moving from low- to high-density environments indicates that clusters are extremely hostile places for star-forming galaxies. However, a detailed knowledge of the effects of the environment on all the components of the interstellar medium (ISM) is still lacking. Particularly important is our understanding of how the environment is able to aﬀect the dust content of cluster spirals. Dust plays an important role in the process of star formation, since it acts as a catalyzer for the formation of molecular hydrogen (H2, from which stars are formed) and prevents its dissociation by the interstellar radiation ﬁeld. Thus, the stripping of dust might signiﬁcantly aﬀect the properties of the ISM in infalling cluster spirals. Since dust is generally associated with the cold gas component of the ISM, it is expected that when the Hi is stripped part of the dust will be removed as well, but no deﬁnitive evidence of a reduced dust content in cluster galaxies has been found so far. For a ﬁxed morphological type, Hi-deﬁcient galaxies1 appear to have higher IRAS f(100 µm)/f(60 µm) ﬂux density ratios (i.e., colder dust temperatures, Bicay & Giovanelli 1987) and lower far-infrared (FIR) ﬂux densities per unit optical area (Doyon & Joseph 1989) than gas-rich galaxies. However, by using ISO observations of the Virgo cluster (Tuﬀs et al. 2002), ⋆ Herschel is an ESA space observatory with science instruments provided by European-led Principal Investigator consortia and with important participation from NASA. 1 The Hi-deﬁciency (defHI) is deﬁned as the diﬀerence, in logarithmic units, between the observed Hi mass and the value expected from an isolated galaxy with the same morphological type and optical diameter (Haynes & Giovanelli 1984). Popescu et al. (2002) ﬁnd no strong variation with clustercentric distance in the dust properties of each morphological type. Only the most extreme Hi-deﬁcient galaxies appear to be lacking a cold dust component. More recently, Boselli & Gavazzi (2006) have revealed an interesting trend of decreasing dust masses per unit of H-band luminosity with decreasing distance from the center of Virgo. Thus, it is still an open issue whether or not dust is removed from infalling cluster spirals. The launch of Herschel (Pilbratt et al. 2010) has opened a new era in the study of environmental eﬀects on dust. Thanks to its high spatial resolution and sensitivity to all dust components, Herschel will be able to determine if cluster galaxies have lost a signiﬁcant amount of their dust content. Ideally, this analysis should be done on a large, statistically complete sample, following the same criteria used to deﬁne the Hi-deﬁciency parameter (Haynes & Giovanelli 1984): i.e., by comparing the dust content of galaxies of the same morphological type but in diﬀerent environments. By observing a signiﬁcant fraction (∼64 deg2) of the Virgo cluster at 100, 160, 250, 350 and 500 µm, the Herschel Virgo Cluster Survey (HeViCS, Davies et al. 2010, hereafter Paper I; see also http://www.hevics.org) will soon provide the optimal sample for such an investigation. In the meantime, with the ﬁrst HeViCS data it is possible to use a more indirect approach and compare the extent of the dust disk in gas-rich and gas-poor cluster galaxies. Since previous studies have shown that the Hi stripping is associated with a ‘truncation’2 of the gas (Cayatte et al. 1994) and starforming disk (Koopmann & Kenney 2004; Catinella et al. 2005; Boselli & Gavazzi 2006), if the dust follows the atomic hydro 2 The term ‘truncation’ is used here to indicate either an abrupt steepening of the surface-brightness proﬁle or, more simply, a signiﬁcant reduction in the disk scale-length compared to the optical one.
On the origin of M81 group extended dust emission<|sep|>It is clear that galaxy-galaxy interactions play a major role in the evolution of primordial gas clouds into the spectacular galaxies we see today. Particularly important and striking interactions occur when the speed of interaction is well matched to the internal velocities of the stars and gas. Thus small galaxy groups can potentially provide just the environment for dramatic gravitational disturbance. Beyond the Local Group the closest example of this is in the environment around M81. The M81 group lies at a distance of about 3.6 Mpc (Karachentsev et al. 2004) with radial velocities ranging from -34 km s−1 for M81 to 203 km s−1 for M82 (Chynoweth et al. 2008), thus tidal disruption is highly likely. Living up to expectations M81 is known to be surrounded by debris which has previously been best delineated by its emission at 21cm (Yun et al. 1993), although diﬀuse emission at wavelengths from the optical to the far-infrared can also be readily detected. Previously others have explicitly described the diﬀuse optical emission from between M81 and M82 as arising from stars either deposited there during a M81/M82 collision or subsequently formed in the gas that was stripped during this event (Sun et al. 2005). Additional support for this comes from a number of recent studies that have identiﬁed individual stars way beyond the discs of the individual galaxies involved (Davidge, 2008; Williams et al. 2009; De Mello et al. 2008; Mouhcine and Ibata, 2010). Contrary to this a study by Sollima et al. (2010) concludes that these stars can only represent a small fraction of the optical emission with the majority of the light being due to back scattering from foreground dust in our Galaxy, dust associated with 21cm cirrus emission. By looking at various far-infrared luminosity ratios Sollima et al. also conclude that the majority of the far-infrared emission is not associated with M81 but is again from Galactic cirrus. Deciding on this issue is not a new problem. The area of sky around M81 has long been known for diﬀuse emission at optical wavelengths. Sandage (1976) describes it as ’high-latitude reﬂection nebulosity illuminated by the Galactic plane’. Arp (1965) describes faint diﬀuse emission that he assigns to the M81 group and one of the most prominent features has become known as ’Arp’s loop’(see ﬁg 1). So the origin of this emission has been somewhat controversial for many years. Our attention was drawn to this problem because recent wide ﬁeld Spitzer and Herschel data of other regions of the sky show extended ﬁlamentary structures that are best explained by thermal emission from dust above the plane of our Galaxy, i.e. the Galactic cirrus whose large scale structure was ﬁrst deﬁned at lower resolution by IRAS. The emission looping around and apparently connected to M81 appears very similar in shape to this cirrus emission, yet one might also expect structures just like this within a galaxy group. To shed new light on this problem we compare Herschel high resolution far-infrared observations with high spatial and velocity resolution 21cm data.
Dark Photons from the Center of the Earth: Smoking-Gun Signals of Dark Matter<|sep|>Dark matter may live in a dark sector with its own forces. This possibility has some nice features. For example, the dark matter’s stability may be ensured not by some discrete parity imposed by hand, but simply by its being the lightest fermion in the dark sector. If the dark sector contains an Abelian gauge symmetry, dark electromagnetism, the dark photon and the Standard Model (SM) photon will generically mix kinetically. This mixing is of special interest because it is one of the few ways for a dark sector to interact with the known particles through a renormalizable interaction and it is non-decoupling: a particle charged under both dark and standard electromagnetism induces this interaction at loop-level, and the eﬀect is not suppressed for very heavy particles. In this way, this is a prototype for simpliﬁed dark matter models with light mediators. The idea of a separate sector with its own photon [1, 2] and the further possibility of kinetic mixing between these two photons [3, 4] were ﬁrst explored long ago, and the myriad implications for dark matter detection have recently attracted widespread interest [5, 6]. In this framework, dark matter will collect in the center of the Earth and annihilate to dark photons XX → A′A′. These dark photons may then travel to near the surface of the Earth and decay to SM particles, which may be detected in a variety of experiments, from under-ice/underwater/underground experiments, such as the current experiments IceCube, SuperK, and ANTARES, and future ones, such as KM3NeT, IceCube II, DUNE, and HyperK, to space-based cosmic ray detectors, such as the current experiments Fermi-LAT and AMS-02, and future ones, such as CALET, ISS-CREAM, and others. The resulting signals of electrons, muons, photons, and hadrons that point back to the center of the Earth are potentially striking signals of dark matter. The possibility of dark matter signals from the centers of large astrophysical bodies was ﬁrst proposed and investigated many years ago [7–16], and there have been important advances for the particular case of the Earth in recent years [17–25]. Typically these signals rely on annihilation to neutrinos, resulting in single-particle signals with a continuum of energies. In contrast, dark photons decay into two charged particles, which may be seen at the same time in a single experiment, and the total energy of these charged particles is equal to the dark matter particle’s mass, producing potentially spectacular results. A schematic picture of this chain of events is given in Fig. 1. A number of processes must be evaluated to determine the resulting signal. For the speciﬁc case of dark photons, it is tempting to simplify the analysis by making a number of assumptions. For example, one may assume that the dark matter capture and annihilation processes have reached equilibrium in the Earth and that the capture cross section has some ﬁxed value, such as the maximal value consistent with current direct detection bounds. Alternatively, the calculations simplify immensely for dark matter masses large compared to all relevant nuclear masses, mX ≫ mN, or dark photon masses mA′ large compared to the characteristic momentum transfer so that the interaction is point-like. We show that none of these assumptions are valid in the regions of parameter space of greatest interest; the large mX approximation may lead to errors of an order of magnitude for mX ≈ 100 GeV, and the large mA′ approximation may also lead to mis-estimates of factors of a few for very light mA′ ∼ MeV. To accurately determine the sensitivity of experiments to probe the relevant parameter space, we carry out a general analysis, without making these simplifying assumptions. An early exploration of dark matter accumulation on the Earth mediated by massless dark photons is Ref. [4]. For previous work exploring the case of massive dark photons, see Ref. [26] for the case of dark FIG. 1: Dark matter is captured by elastic XN → XN scattering oﬀ nuclei, collects in the center of the Earth, and annihilates to dark photons, XX → A′A′. These dark photons then travel to near the surface of the Earth and decay to SM particles, which may be detected by a variety of experiments, including neutrino telescopes and space-based cosmic ray detectors. As an example, we show IceCube and various signatures there resulting from A′ decays to electrons, muons, and hadrons. We discuss the possibility that double tracks (showers) may be resolved spatially (temporally) in the detector. matter capturing in the Earth and annihilating into neutrinos, Refs. [27–29] for early work on celestial body capture of dark matter annihilating into dark photons, and in particular Ref. [30] for a description of the general framework of annihilation to light mediators that highlights the speciﬁc case of solar capture and gamma ray signatures, which were later searched for by the Fermi-LAT collaboration [31]. Finally, recent work has highlighted the eﬀect of self-capture [32, 33] and boosted dark matter [34]. These results are timely for several reasons. Dark photons have attracted signiﬁcant interest and are probed in many ways, including direct detection experiments, accelerator and beam dump experiments, and astrophysical observables [5, 6]. The signals we discuss are detectable for dark photon masses mA′ ∼ MeV − GeV and mixing parameters ε ∼ 10−10 − 10−8, an interesting and large region of parameter space that includes territory that has not yet been probed. These values of mA′ can also produce dark matter self-interactions that have been suggested to solve small-scale structure anomalies [35–39]. The range of ε values are naturally induced, for example, by degenerate bi-fundamentals in grand uniﬁed theories [40]. It was recently pointed out that combining kinetic mixings of this size with the self-interacting models for small-scale structure can also explain the excess of gamma rays from the galactic center recently observed by Fermi-LAT [41]. At the same time, this work motivates a new class of searches for current indirect detection experiments to discover dark matter. At present there are a number of landmark experiments, including those mentioned above, that are transforming the ﬁeld of indirect detection with high precision measurements and increasingly large statistics. In many cases, however, their sensitivities for dark matter searches are clouded by uncertainties in astrophysical backgrounds. The signals we highlight here come from a speciﬁc direction (the center of the Earth), cannot be mimicked by astrophysics and, in many cases, are essentially background-free. As a result, the processes discussed here provide an opportunity for both current and future experiments to detect a smoking-gun signal of dark matter.
Deep Cross-Modality and Resolution Graph Integration for Universal Brain Connectivity Mapping and Augmentation<|sep|>Modern network science opens new frontiers of representing the complex functionality and structure of biological systems by analyzing the intercommunication within their fundamentals [1]. The wealth of technological advances in the ﬁeld of neuroscience paves the way for gathering massive and high-quality biological datasets such as Human Connectome Project [2], Southwest University Longitudinal Imaging Multimodal (SLIM) Brain Data Repository [3] and UK Biobank [4] using diﬀerent magnetic resonance imaging (MRI) modalities including functional, structural T1-weighted and diﬀusion MRI. Representing such connectomic datasets using graphs (i.e., networks) aims to reveal the complex interconnections between brain regions. More speciﬁcally, each brain graph allows to investigate particular connectivity patterns and functionalities of the brain elements, where each anatomical brain region of interest (i.e., ROI) is represented with a node and the biological connectivity between two ROIs is represented by weighted edges [5,6,7]. Using graphs that are derived from such rich multimodal datasets serves as an exemplary tool for examining the human brain structure and state [8,9] by mapping the brain wiring at the individual level. In addition to ﬁngerprinting the brain of an individual, graph representations allow for mapping brain connectivity at the population level, thereby distinguishing between contrasting states (e.g., healthy versus unhealthy) of diﬀerent populations. Emerging studies focused on learning how to integrate a set of unimodal single-resolution brain graphs into a single connectome (i.e., connectional brain template) that encodes the shared traits across the individuals of the population [10,11,12]. Despite their overwhelming success, existing methods [13] are not particularly designed to handle multimodal multiresolution connectomic datasets, which, if solved, can pave the way for holistically detecting anomalies and abnormalities across varying brain networks. Speciﬁcally, generating a universal connectional brain template (i.e., CBT) from a multimodal multiresolution connectomic population remains an uncharted territory [13]. By mitigating such a challenging issue, a single compact representation, from which one can span new multimodal multiresolution brain graph populations for data augmentation [14,15], can be learned to reveal typical and atypical alterations in the brain connectome across modalities and various individuals. One can also leverage the universal CBT for graph augmention to alleviate clinical data scarcity [16,17,18,14] in classiﬁcation and regression tasks [19,20,21,22,23]. Related work. Existing works tailored for graph integration or fusion in general are limited to training on unimodal, single-resolution brain networks [24,10,11,12]. For example, based on message passing between the neighbors of a particular node, similarity network fusion (SNF) [24] learns how to integrate a set of biological graphs by diﬀusing the local connectivity of each individual graph across the global connectivity of all samples in the population in an iterative manner. Still, such method cannot handle multiresolution graphs as well as heterogeneous samples drawn from multimodal distributions. Later on, [10] proposed a novel method for estimating a CBT (also termed with brain network atlas) over a population of brain networks which are derived from the same modality by exploiting diﬀusive-shrinking and fusing graph techniques. However, the mathematical formalization of the proposed graph diﬀusion and fusion method is not capable of handling multigraph population, where each sample is represented by a set of graphs. To remedy the lack of methods for multigraph data integration, where a multigraph allows for multiple edges connecting two nodes, [11] introduced a novel approach for multi-view graph construction. However, such method utilizes disparate learning modules that learn independently without any feedback mechanism between them; as such the errors accumulate throughout the dichotomized learning pipeline. To address this issue, [12] introduced Deep Graph Normalizer (DGN) framework, the ﬁrst graph neural network that integrates a population of ﬁxed-size multigraphs in an end-to-end learnable way. Although compelling, DGN is limited to aggregating the information only across multi-view brain graphs with a ﬁxed resolution. Besides, it relies on a random sampling technique to generate CBTs, which is agnostic to data heterogeneity. Moreover, DGN uses edge-conditioned convolution, which is not fundamentally tailored for easing the memory consumption, thereby undermining the population representative CBT estimation for large-scale graph populations. Other related works [25,26] focused only on integrating single-resolution brain network populations for disorder proﬁling and CBT learning. We note a few works that were also dedicated to brain graph super-resolution [27,28,29], which primarily aimed to generate brain graphs across diﬀerent resolutions rather then integrating them. To address all these limitations, we propose Multimodal Multiresolution Graph Integrator (M2GraphIntegrator) Network, the ﬁrst framework for integrating a population of multimodal multi-resolution brain networks into a centered and representative CBT. Tapping into the nascent ﬁeld of GNNs, we design a set of resolution-speciﬁc autoencoders to map a given population of brain networks of diﬀerent resolutions derived from multiple modalities to a shared embedding space. Next, given the learned embeddings, we generate the CBT through the integrator network, which is an architecture specialized in learnable embedding integration. To train our framework, we design a novel CBT centeredness loss that ensures the heterogeneity of training samples via clustering. As such, the selected training samples from diﬀerent clusters represent each and every distribution present in the input graph population. In that way, our estimated CBT can capture the connectivity patterns across all subjects in a diverse population. Furthermore, to preserve the brain graph topology in the integration process, we propose a novel topology loss which aims to minimize the topological gap between the ground-truth and the reconstructed brain graphs in terms of node strength, a measure quantifying the local hubness of each brain node (i.e., anatomical region of interest). Fig. 1: Overview of the proposed Multi-modal Multi-resolution Graph Integrator (M2GraphIntegrator) architecture for estimating a centered connectional brain template from a given population. (A) Multi-modal multi-resolution brain network representation. We represent each subject in the population by multiple connectivity matrices, each denoted by Xm,rk s ∈ Rrk×rk. (B) Subjectbased CBT generation. Our framework consists of 3 co-learning modules: the resolution-speciﬁc graph autoencoders, the self-mapper and the integrator. We generate the subject-based CBT by integrating the feature vector embeddings Zm,rk s of each encoder and the self-mapper. (C) Subject-speciﬁc loss calculation. For each subject s, we calculate both reconstruction and topological losses using the whole training set. As for the CBT centeredness loss, using clustering we select a heterogeneous subset of training samples against which we evaluate the centeredness of the learned subject-speciﬁc CBT. (D) Universal CBT generation. To capture the most centered connectional patterns across all subjects, we feed each brain multigraph through our trained model to generate subject-based CBTs. Next, we perform element-wise median operation to estimate the universal CBT. To simplify the illustration, we denote the encoder ErM K by ErK and DrM K by DrK.
Multipartite entanglement in qubit systems<|sep|>Entanglement is one of the most striking features of quantum phenomena [41]. It plays very important roles in quantum information processing such as quantum computation [37], quantum teleportation [6] (for discussions on experimental realizations see [11,12,25,38]), dense coding [7] and quantum cryptographic schemes [17,18,24]. Nevertheless, the quantiﬁcation of multipartite entanglement is no simple matter. Entanglement is intimately related to the very mathematical structure of quantum mechanics and complex Hilbert spaces. In particular it is a straightforward consequence of linearity (superposition principle) in tensor product Hilbert spaces (composite quantum systems). Consider a quantum system composed of two parts (e.g. two particles): part A, whose Hilbert space is HA, and part B, whose Hilbert space is HB. According to quantum mechanics, the composite system lives in the tensor product Hilbert space H = HA ⊗ HB. The most familiar example is that
SDSSJ143244.91+301435.3 at VLBI: a compact radio galaxy in a narrow-line Seyfert 1<|sep|>Narrow-line Seyfert 1 (NLS1) galaxies represent a class of Active Galactic Nuclei (AGN) characterized by narrow (<2000 km s−1) Balmer lines, weak [OIII]λ5007Å emission compared to the Balmer lines ([OIII]λ5007Å/Hβ ﬂux ratio below 3) and strong optical FeII emission (e.g. Osterbrock & Pogge 1985; Goodrich 1989; Pogge 2000; Veron-Cetty et al. 2001). The optical properties of NLS1 are usually explained as the consequence of a combination of a low-mass (≲108 M⊙) central supermassive black-hole (SMBH) with a high accretion rate (close to the Eddington limit). Most of the NLS1 are radio-quiet (RQ) objects while a minority (∼7 per cent, Komossa et al. 2006) are radio-loud (RL), using the common deﬁnition based on the 5 GHz to 4400Å ﬂux density ratio (R5, where RL NLS1 have R5 >10) or an (almost) equivalent deﬁnition based on the 1.4 GHz ﬂux density (R1.4, where RL NLS1 have R1.4 >19, Komossa et al. 2006). The ∼110 RL NLS1 discovered so far, have been discussed in a number of papers (e.g. Komossa et al. 2006; Yuan et al. 2008; Foschini et al. 2015; Berton et al. 2015; Gu et al. 2015). A peculiarity of the RL NLS1 discovered to date is that most of them present a compact radio emission (linear size below a few kpc, e.g. Doi et al. 2012 and references therein). Some of the RL NLS1 with the highest values of radio-loudness show strict similarities with blazars (BL Lac objects and ﬂat spectrum radio quasar, FSRQ): a ﬂat or inverted radio spectrum, high brightness temperatures (TB >1011 K, e.g. Yuan et al. 2008) and a detectable gammaray emission (in Fermi-LAT, Abdo et al. 2009a; Abdo et al. 2009b; Foschini 2011; Foschini et al. 2015; Liao et al. 2016; Yao et al. 2015a; Yao et al. 2015b). Since blazars are usually believed to be radio galaxies whose relativistic jets are pointing towards the observer (e.g. Urry & Padovani 1995), a natural conclusion is that
Pose Guided Human Image Synthesis with Partially Decoupled GAN<|sep|>Due to its portability and simplicity, two-dimensional (2D) human synthesis has become more and more popular in the multimedia and computer vision ﬁelds. Human image synthesis has huge potential in many commercial applications, such as virtual clothes try-on Ren et al. (2021), person re-identiﬁcation (Re-ID) Zheng et al. (2015) and so on. A challenging task of 2D human synthesis is pose guided human image synthesis (PGHIS) Zhang et al. (2022), Ren et al. (2022), which renders the photo-realistic image of arbitrary pose from a reference pose human image. Because of the considerable changes in geometry and texture, rendering adequate details during pose transfer is challenging. Early image-based Ma et al. (2017) pose transfer methods directly combined human images, pose key points Cao et al. (2017), and the inshop clothing representations together without taking into account the correlation between them, which brings missing and blurred texture. Most of existing methods process the pose and texture separately Tang et al. (2020), Zhang et al. (2021a), Cui et al. (2021), i.e., two diﬀerent encoders are used to map the reference pose and the whole human texture to the high-dimensional latent space, and then a decoder is used to fuse them to generate the human image of the target pose. This allows for generating human images with complete structure, but usually does not produce very realistic images with detailed information. Because the quality of synthesized images heavily depends on the encoder model, which is inherently challenging due to the diﬃculty of learning the topologically complex human texture Zhang et al. (2022). Some methods make the encoder learn more details of human texture mainly by increasing the size of the model Ma et al. (2017), Lv et al. (2021), but this improves the performance of detail rendering much less than the increasement of computational cost. Inspired by this, we argue that an encoder will learn more details when it encodes a part of a human body, so we propose to structurally decouple the human image. Speciﬁcally, we decouple the human image into 8 parts, then encode these parts using a weight-sharing encoder, and ﬁnally fuse them into a texture code to guide the synthesis of a realistic human image. This not only allows the encoder to focus on enough detailed texture information, but also ensures the lightweight nature of our model because of the weight-sharing operation. In addition, due to the inherent structure of vanilla convolutional neural networks (CNN) Zhao et al. (2022) processing one local neighborhood at a time, they can only focus on shortrange information, which results in their inability to achieve eﬃcient spatial transformation Vaswani et al. (2017). And CNN-based methods tend to have poor results when the pose transfer process involves large changes Ren et al. (2020). Attention mechanism Touvron et al. (2021), Zhang et al. (2019) computes the response of a target position as a weighted sum of all reference features, which let attention-based methods have the ability of longrange modeling. But in the pose transfer task, most of the existing methods either learn the correlation of the feature from diﬀerent poses based on CNN or copy vanilla self-attention directly into image features. Few methods design a transformer module speciﬁcally for the pose transfer task. To ﬁll this gap, we designed an eﬃcient attention-based module (called transformer module) for pose transfer task. Speciﬁcally, the transformer module consists of two attention modules, the ﬁrst one is the multi-headed self-attention module, which is responsible for capturing the correlation between the reference pose and the texture. The second is the multi-headed cross-attention module, which is used to migrate the reference texture features to the target pose. It is worthy that we introduce a Residual Fast Fourier Transform (Res FFT) Mao et al. (2021) block instead of a traditional residual block into the transformer module, which makes the model focus on more detailed information like time-domain features and frequency-domain features Wu et al. (2022b). Based on the above ideas, we propose a Partially Decoupled GAN (PD-GAN) for PGHIS. The structure can be seen in Figure 1. Speciﬁcally, The body part decoupling branch and the feature texture transformer branch make up the two main branches of our model. The body part decoupling branch encodes the body parts into high-dimensional latent codes, which are then fused into features FC that assist in generating realistic texture images. The transformer branch mainly renders the textures of the reference pose to the target pose, and we particularly design a new transformer module for the task. Our contribution can be summarized in the following three points: • We propose an eﬀective model named PD-GAN for pose transfer task, which consists of a human partial decoupled module and transformer module. The two modules play crucial roles for generating realistic human images. • To improve the ability of capturing the long-range dependency of pose transfer, we design a transformer module to obtain more global information. Besides, we use the Res FFT block that can capture both time-domain and frequency-domain information to replace the traditional residual block, which brings more realistic human images generated by our method. • We perform extensive experiments to conﬁrm the eﬀectiveness of our method in comparison with other baseline methods. Additionally, a comprehensive ablation research indicates the role played by each component in the increased eﬃcacy.
Extracting Conflict-free Information from Multi-labeled Trees<|sep|>Multi-labeled trees, also known as MUL-trees, are phylogenetic trees that can have more than one leaf with the same label [4,6,8,13,18] (Fig. 1). MUL-trees arise naturally and frequently in data sets containing multiple genes or gene sequences for the same species [17], but they can also arise in bio-geographical studies or co-speciation studies where leaves represent individual taxa yet are labeled with their areas [5] or hosts [10]. MUL-trees, unlike singly-labeled trees, can contain conﬂicting species-level phylogenetic information due, e.g., to whole genome duplications [11], incomplete lineage sorting [16], inferential error, or, frequently, an unknown combination of several factors. However, they can also contain substantial amounts of conﬂict-free information. Here we provide a way to extract this information; speciﬁcally, we have the following results. – We introduce a new quartet-based measure of the information content of a MUL-tree, deﬁned as the set of conﬂict-free quartets the tree displays (Section 2). – We introduce the concept of the maximally-reduced form (MRF) of a MUL-tree, the smallest MUL-tree with the same information content (Section 3), and show that any two MUL-trees with the same information content have the same MRF (Theorem 3). – We present a simple algorithm to construct the MRF of a MUL-tree (Section 4); its running time is quadratic in the number of leaves and does not depend on the multiplicity of the leaf labels or the degrees of the internal nodes. – We present computational experience with an implementation of our MRF algorithm (Section 5). In our test data, the MRF is often signiﬁcantly smaller than the original tree, while retaining most of the taxa. We now give the intuition behind our notion of information content, deferring the formal deﬁnitions of this and other concepts to the next section. Quartets (i.e., sets of four species) are a natural starting point, since they are the smallest subsets from which we can draw meaningful topological information. A singly-labeled tree implies exactly one topology on any quartet. More precisely, each edge e in a singly-labeled tree implies a bipartition (A, B) of the leaf set, where each part is the set of leaves on one the two sides of e. From (A, B), we derive a collection of bipartitions ab|cd of quartets, such that {a, b} ⊆ A and {c, d} ⊆ B. Clearly, if one edge in a singly-labeled tree implies some bipartition q = ab|cd of {a, b, c, d}, then there can be no other edge that implies a bipartition, such as ac|bd, that is in conﬂict with q. Indeed, the quartet topologies implied by a singly-labeled tree uniquely identify it [20]. Fig. 1: A MUL-tree. Numbers in parenthesis next to labels indicate the multiplicity of the respective labels and are not part of the labels themselves. The situation for MUL-trees is more complicated, as illustrated in Fig. 1. Here, the presence of two copies of labels b and c — b(1) and b(2), and c(1) and c(2) — leads to two conﬂicting topologies on the quartet {b, c, d, e}. Edge (u, v), implies the bipartition bc|de, corresponding to the labels {b(1), c(1), d, e}, while edge (v, w) implies bd|ce corresponding to the leaves {b(2), c(2), d, e}. On the other hand, the quartet topology af|bc, implied by edge (t, u), has no conﬂict with any other topology that the tree exhibits on {a, b, c, f}. We show that the set of all such conﬂict-free quartet topologies is compatible (Theorem 1). That is, for every MUL-tree T there exists at least one singly-labeled tree that displays all the conﬂict-free quartets of T — and possibly some other quartets as well. Motivated by this, we only view conﬂict-free quartet topologies as informative, and deﬁne the information content of a MUL-tree as the set of all conﬂict-free quartet topologies it implies. Conﬂicting quartets may well provide information, whether about paralogy, deep coalescence, or mistaken annotations. In some cases, species-level phylogenetic information can be recovered from conﬂicted quartets through application of, e.g., gene-tree species-tree reconciliation, an NP-hard problem. However, this is not feasible when the underlying cause of multiplicity is unknown and when conducting large-scale analyses. Our deﬁnition of information content speciﬁcally allows us to remain agnostic with respect to the cause and conservative with respect to species relationships, i.e., it does not introduce quartets not originally supported by the data. A MUL-tree may have leaves that can be pruned and edges that can be contracted without altering the tree’s information content, i.e., without adding or removing conﬂict-free quartets. For example, in Fig. 1, every quartet topology that edge (v, w) implies is either in conﬂict with some other topology (e.g., for set {b, c, d, e}) or is already implied by some other edge (e.g., af|ce is also implied by (t, u)). Thus, (v, w) can be contracted without altering the information content. In fact, the information content remains unchanged if we also contract (u, v) and remove the leaves labeled b(1) and c(1). We deﬁne the MRF of a MUL-tree T as the tree that results from applying informationpreserving edge contraction and leaf pruning operations repeatedly to T, until it is no longer possible to do so. For the tree in Fig. 1, the MRF is singly-labeled as shown in Fig. 2. Note that, in general, the MRF may not be singly-labeled (see the example in Section 4.4). If the MRF is itself a MUL-tree, it is not possible to reduce the original to a singly-labeled tree without either adding at least one quartet that did not exist conﬂict-free in T or by losing one or more conﬂict-free quartets. Since any two MUL-trees with the same information content have the same MRF, rather than comparing MUL-trees directly, we can instead compare their MRFs. This is appealing mathematically, because it focuses on conﬂict-free information content, and also computationally, since an MRF can be much smaller than the original MUL-tree. Indeed, on our test data, the MRF was frequently singly-labeled. This reduction in input size is especially signiﬁcant if the MUL-tree is an input to an algorithm whose running time is exponential in the label multiplicity, such as Ganapathy et al.’s algorithm to compute the contract-and-reﬁne distance between two area cladograms [5] or Huber et al.’s algorithm to determine if a collection of “multi-splits” can be displayed by a MUL-tree [7]. For our experiments, we also implemented a post-processing step, which converts the MRF to a singly-labeled tree, rendering it available for analyses that require singlylabeled trees, including supermatrix [3,22] and supertree methods [2,15,1,21]. On the trees in our data set, the combined taxon loss between the MRF computation and the postprocessing was much lower than it would have been had we simply removed all duplicate taxa from the original trees. Previous work on MUL-trees has concentrated on ﬁnding ways to reduce MUL-trees to singly-labeled trees (typically in order to provide inputs to supertree methods) [18], and to develop metrics and algorithms to compare MUL-trees [5,14,12,9]. In contrast to our approach — which is purely topology-based and is agnostic with respect to the cause of label multiplicity —, the assumption underlying much of the literature on MUL-trees is that taxon multiplicity results from gene duplication. Thus, methods to obtain singly-labeled trees from MUL-trees usually work by pruning subtrees at putative duplication nodes. Although the proposed algorithms are polynomial, they are unsatisfactory in various ways. For example, in [18] if the subtrees are neither identical nor compatible, then the subtree with smaller information content is pruned, which seems to discard too much information. Further, the algorithm is only efﬁcient for bi nary rooted trees. In [14] subtrees are pruned arbitrarily, while in [12] at each putative duplication node a separate analysis is done for each possible pruned subtree. Although the latter approach is better than pruning arbitrarily, in the worst case it can end up analyzing exponentially many subtrees.
The Smart Mask: Active Closed-Loop Protection against Airborne Pathogens<|sep|>The world has been witnessing the increasing spread of COVID-19 since the beginning of 2020. This novel virus has brought everyone’s lives to a standstill and the economy to its knees. Although most people are actively following social distancing norms, proper hygiene, and other preventive measures, it is likely that normal day-to-day life will continue to be affected until an effective vaccine is developed. To mitigate this situation and return to some semblance of normalcy, we believe there is a need for preventive methods that actively combat the virus instead of providing passive protection (e.g., physical barriers). Implementing such improved methods requires smart devices that can detect, quantify, and actively eradicate viruses and other pathogens. Typical Personal Protective Equipment (PPE), such as face masks (cloth, surgical, or N95), face shields, eye protection, disposable gloves, and coveralls all provide passive protection: these devices only prevent pathogens from entering the body by ﬁltering them out. By contrast, active protection devices can actively attack and destroy pathogens near vulnerable parts of the body (e.g., the nose and mouth). Here we consider closed-loop active or “smart” masks for use in places where potentially virus-laden respiratory droplets (typically 0.1-10 µm in diameter) are most likely to be transmitted; examples include bathrooms, doctor’s ofﬁces, daycare centers, and public transportation. Fig. 1 gives an overview of the proposed smart mask architecture. A detailed comparison of existing masks with the proposed smart mask is shown in Table I. The table shows that the proposed system detects airborne droplets (potentially containing viruses for COVID-19, inﬂuenza, measles, or other diseases) and limits their spread via an appropriate situation-aware mitigation strategy. The chosen strategy should be “smart”, i.e., adaptive based on sensor outputs that provide information on concentration, size distribution, and other properties of the droplets. In our initial implementation, mitigation is provided by a cold mist generator that loads the droplets (thus making them quickly fall to the ground), and adaptation is provided by algorithms running on an on-board controller that adjust the spray angle, intensity, and duration of the generator based on sensor data. Such active closed-loop protection can remove viruses (and other pathogens) from the air before they infect others, thereby reducing the need for periodic disinfection of the area, while providing increased protection to the wearer. The paper is organized as follows. The airborne transmission of viruses is brieﬂy reviewed in Section II. The system-level architecture and operating principles of smart masks are presented in Sections II-A and II-B, Filtration Low level Moderate level High level (95%) Removes all large (dia. > 5 µm), and tiny (dia. < 5 µm) airborne particles/droplets Breathability Breathable Breathable Difﬁcult Breathable User seal check requirement No No Yes Yes Limitation Wash after each use Discard after each use Ideally discard after each use Reusable and needs to be disinfected after each use respectively. The experimental setup, results, and key observations are explained in Sections III-A, III-B, and III-C, respectively. The commercial feasibility of smart masks is discussed in Section III-D, and future work and conclusions are presented in Section IV.
CHIP: CHannel Independence-based Pruning for Compact Neural Networks<|sep|>Convolutional neural networks (CNNs) have obtained widespread adoptions in numerous important AI applications [17, 48, 47, 12, 11, 44, 35]. However, CNNs are inherently computation intensive and storage intensive, thereby posing severe challenges for their efﬁcient deployment on resourceconstrained embedded platforms. To address these challenges, model compression is widely used to accelerate and compress CNN models on edge devices. To date, various types of compression strategies, such as network pruning [15, 16, 37, 60, 28, 14, 1, 49, 10, 63, 36, 18, 13, 3, 2, 25, 53, 50, 9, 39, 33], quantization [15, 55, 43, 8], low-rank approximation [56, 40, 58, 57], knowledge distillation [22, 41] and structured matrix-based construction [45, 29, 6], have been proposed and explored. Among them, network pruning is the most popular and extensively studied model compression technique in both academia and industry. Based on their differences in pruning granularity, pruning approaches can be roughly categorized to weight pruning [16, 15] and ﬁlter pruning [54, 27, 21, 38, 34]. Weight pruning focuses on the proper selection of the to-be-pruned weights within the ﬁlters. Although enabling a high compression ratio, this strategy meanwhile causes unstructured sparsity patterns, which are not well supported by the general-purpose hardware in practice. On the other hand, ﬁlter pruning emphasizes the removal of the entire selected ﬁlters. The resulting structured sparsity patterns can be then properly leveraged by the off-the-shelf CPUs/GPUs to achieve acceleration in the real-world scenario. Existing Filter Pruning Methods. Motivated by the potential practical speedup offered by ﬁlter pruning, to date numerous research efforts have been conducted to study how to determine the important ﬁlters – the key component of efﬁcient ﬁlter pruning. A well-known strategy is to utilize the norms of different ﬁlters to evaluate their importance. Such a "smaller-norm-less-important" hypothesis is adopted in several pioneering ﬁlter pruning works [27, 19]. Later, considering the limitations of norm-based criterion in real scenarios, [20] proposes to utilize geometric medianbased criterion. More recently, ﬁrst determining those important feature maps and then preserving the corresponding ﬁlters, instead of directly selecting the ﬁlters, become a popular strategy for ﬁlter pruning. As indicated in [31], the features, by their natures, reﬂect and capture rich and important information and characteristics of both input data and ﬁlters, and hence measuring the importance of features can provide a better guideline to determine the important ﬁlters. Built on this pruning philosophy, several feature-guided ﬁlter pruning approaches [31, 51] have been proposed and developed, and the evaluation results show their superior performance over the state-of-the-art ﬁlter-guided counterparts with respect to both task performance (e.g., accuracy) and compression performance (e.g., model size and ﬂoating-point operations (FLOPs) reductions). Determining Importance: Intra-channel & Inter-channel Perspectives. These recent advancements on ﬁlter pruning indeed show the huge beneﬁts of leveraging feature information to determine the importance of ﬁlters. To date, some feature-guided approaches measure the importance from the intra-channel perspective. In other words, no matter which importance metric is used, the importance of one feature map (and its corresponding ﬁlter), is measured only upon the information of this feature map in its own channel. On the other aspect, the inter-channel perspective, which essentially determines the ﬁlter importance via using cross-channel information [20, 42, 46, 52, 26], is still being further explored. To be speciﬁc, [20] and [42] adopt cross-channel geometric median and Hessian, respectively, to measure the channel importance. However, such measurement is based on ﬁlter instead of feature map information, and hence the rich and important feature characteristics are not properly identiﬁed and extracted. [52, 26] also explore inter-channel-based ﬁlter pruning via introducing budget constraints across channels. However, such exploration and utilization of the inter-channel information are implicit and indirect, thereby limiting the practical pruning performance. Beneﬁts of Inter-channel Perspective. In principle, the feature information across multiple channels, if being leveraged properly, can potentially provide richer knowledge for ﬁlter pruning than the intrachannel information. Speciﬁcally, this is because: 1) the importance of one ﬁlter, if being solely determined by its corresponding feature map, may be sensitive to input data; while the cross-channel feature information can bring more stable and reliable measurement; and 2) consider the essential mission of pruning is to remove the unnecessary redundancy, the inter-channel strategy can inherently better identify and capture the potential unnecessary correlations among different feature maps (and the corresponding ﬁlters), and thereby unlocking the new opportunity of achieving better task and compression performance. Technical Preview and Contributions. Motivated by these promising potential beneﬁts, in this paper we propose to explore and leverage the cross-channel feature information for efﬁcient ﬁlter pruning. To be speciﬁc, we propose Channel Independence, a cross-channel correlation-based metric to measure the importance of ﬁlters. Channel independence can be intuitively understood as the measurement of "replaceability": when the feature map of one ﬁlter is measured as exhibiting lower independence, it means this feature map tends to be more linearly dependent on other feature maps of other channels. In such a scenario, the contained information of this low-independence feature map is believed to have already been implicitly encoded in other feature maps – in other words, it does not contain useful information or knowledge. Therefore the corresponding ﬁlter, which outputs this low-independence feature map, is viewed as unimportant and can be safely removed without affecting the model capacity. Overall, the contributions of this paper are summarized as: • We propose channel independence, a metric that measures the correlation of multiple feature maps, to determine the importance of ﬁlters. Built from an inter-channel perspective, channel independence can identify and capture the ﬁlter importance in a more global and precise way, thereby providing a better guideline for ﬁlter pruning. • We systematically investigate and analyze the suitable quantiﬁcation metric, the complexity of the measuring scheme and the sensitiveness & reliability of channel independence, and then we develop a low-cost ﬁne-grained high-robustness channel independence calculation scheme for efﬁcient ﬁlter pruning. • We empirically apply the channel independence-based importance determination in different ﬁlter pruning tasks. The evaluation results show that our proposed approach brings very high pruning performance with preserving high accuracy. Notably, on CIFAR-10 dataset our solution can bring 0.90% and 0.94% accuracy increase over baseline ResNet-56 and ResNet-110 models, respectively, and meanwhile the model size and FLOPs are reduced by 42.8% and 47.4% (for ResNet-56) and 48.3% and 52.1% (for ResNet-110), respectively. On ImageNet dataset, our approach can achieve 40.8% and 44.8% storage and computation reductions, respectively, with 0.15% accuracy increase over the baseline ResNet-50 model.
The Cost of OSCORE and EDHOC for Constrained Devices<|sep|>CoAP is a widely used IoT application layer protocol. It was designed with two main goals: 1) to be efﬁcient for large deployments of constrained devices communicating over constrained networks and 2) to integrate easily with existing Representational State Transfer (REST) [23] protocols, such as HTTP [22, 45, 20]. One of the strategies that CoAP uses to achieve these goals is to leverage proxy software executed on middle boxes, such as the border routers connecting the constrained IoT networks and the non-constrained networks, e.g., the Internet [46, 31]. For example, a proxy may serve incoming requests from a cache containing previously received data that is still valid in order to save the constrained communicational and computational resources of the device. In other use cases, the proxy may act as a cross-protocol proxy and translate between CoAP and HTTP. Such cross-protocol proxies achieve integration of the constrained devices on the Internet that is independent of the application logic [20, 19, 29]. The CoAP speciﬁcation [45] states that DTLS or TLS [18, 48] has to be used to secure CoAP. Although (D)TLS provides strong security guaranties, (D)TLS channels must be terminated at the proxies. This is because the proxies need to access certain ﬁelds of the CoAP or the HTTP message, which are encrypted at the transport layer. Decrypting the secured messages at the proxies makes the necessary ﬁelds available, but unfortunately also exposes the sensitive message payload [44, 42]. In order to circumvent this security issue, an alternative to (D)TLS consisting of the protocols OSCORE [42] and EDHOC [43] was recently proposed by IETF. In contrast to (D)TLS, OSCORE and EDHOC operate on the application layer, thus making proxy operations possible without decrypting the message payload. In addition, in order for both protocols to be well suited for constrained devices and networks, they leverage the low overhead encoding formats Concise Binary Object Representation (CBOR) [17] and CBOR Object Signing and Encryption (COSE) [41]. Previous work in the area of OSCORE and EDHOC implementations only considers early protocol drafts and therefore does not include all protocol modes [36]. The previous work also does not provide detailed evaluation in terms of memory, execution speed and energy on constrained microcontrollers [25]. We, however, describe the design of our libraries and provide the ﬁrst detailed evaluation of the latest states of the speciﬁcations [42, 43]. Moreover, we consider all modes of operation. Furthermore, previous work does not consider the fact that IoT devices are often prone to software vulnerabilities, e.g., buffer overﬂows, which may compromise even the bestdesigned protocols. This is due to the fact the IoT devices often run ﬁrmware written in system languages such as C and C++. Moreover, such IoT ﬁrmware is often executed without an Operating System (OS) or on top of a Real-Time OS (RTOS) which provides very limited or even no security features. An attacker exploiting such a software vulnerability could, for example, leak the cryptographic keys used in the protocols and use them to impersonate the device. We address this issue by identifying the critical parts of the OSCORE and EDHOC protocols and placing them in a TEE. Such TEEs have become recently available on main stream microcontrollers, for instance microcontrollers based on the Cortex-M23/33 cores featuring ARM TrustZone-M TEE [8, 9, 10, 7]. Contributions We present the design of the ﬁrmware libraries µOSCORE and µEDHOC for regular microcontrollers. Our ﬁrmware designs consider all modes of operation of the ﬁnal version of the OSCORE speciﬁcation [42] and the latest EDHOC draft version [43]. The main feature of our designs is that they are completely independent of the CoAP library, embedded OS, the communication protocol stack and the crypto library. µOSCORE and µEDHOC are available as open source software [12]. We present the design of the ﬁrmware libraries µOSCORETEE and µEDHOC-TEE for microcontrollers featuring a TEE. These libraries separate the cryptographic keys and routines from the rest of the ﬁrmware, which could be vulnerable. We designed µOSCORE-TEE and µEDHOC-TEE especially for microcontrollers, but they can also be ported to more powerful computing environments, e.g., ARM Cortex A class of devices. We provide the ﬁrst detailed study about the applicability of the OSCORE and the EDHOC protocols on constrained IoT devices. More precisely, we provide a detailed evaluation of our libraries regarding RAM/FLASH requirements and execution speed on four broadly used low-end CPUs: Cortex M0, M4, M33 and Xtensa. We evaluate the overhead caused by using a TEE on the nRF9160 [9] radio SoC from Nordic Semiconductor featuring a Cortex-M33 CPU with TrustZone-M. In addition, we present an evaluation of the energy requirements of the protocols in an IPv6 over Bluetooth Low Energy (BLE) network [33]. II. BACKGROUND This section provides background for the rest of the paper. The CoAP introduction provides details which are required for understanding the OSCORE internals. The TrustZone-M introduction is required for understanding the separation of code and keys. A. CoAP CoAP is a RESTful application layer protocol especially designed for the IoT domain [45]. It considers two types of devices – clients and servers, which communicate using requests and responses. The servers host resources such as sensors and actuators. The clients may access those resources using PUT, GET, POST and DELETE methods. Each resource is reachable through a Uniform Resource Identiﬁer (URI). Each CoAP packet begins with a ﬁxed 4-byte header carrying the method type (PUT, GET, POST, DELETE) or a response code, among other information. The header is followed by an optional token used to correlate requests and responses. The token is followed by optional options that contain additional parameters for the requests/responses. These are followed by an optional payload, preﬁxed with the payload. A TEE is a state-of-the-art feature that has been available on application class processors for several years. Microcontroller class processors featuring a TEE called TrustZone-M were recently introduced by ARM with the ARMv8-M architecture. ARMv8-M chips such as nRF9160 and nRF5340 became recently available on the mass market. TrustZone-M separates a microcontroller in two domains called secure world and non-secure world. RAM/FLASH memory regions and memory-mapped peripherals of the microcontroller are associated with one of those domains. Secure world code, data and peripherals are not accessible from non-secure world code or peripherals. Secure world code and peripherals can access secure world and non-secure world code and peripherals. Functions in the secure world can call functions in the nonsecure world without any restrictions. Secure functions are called by non-secure functions using a special type of secure memory region called non-secure callable. Only functions in the secure world deﬁned with the non-secure callable attribute will contain a special secure gateway (SG) instruction, which allows them to be called from the non-secure world. Such functions are referred to as veneer functions [1]. This is the only way in which secure software can be accessed from the non-secure software. By default, after power-up or reset the execution starts in the secure world. At that point, all memory regions are secure. The mapping of memories and peripherals into the secure world and the non-secure world is done during the boot process before the non-secure application is called.
Human brain ferritin studied by muon Spin Rotation: a pilot study<|sep|>Ferritin is a protein that attracts much interest, not only because of its crucial role in iron storage and ferroxidase activity [1, 2], but also because of its magnetic properties. Ferritin is a nanoscopic hollow protein made of a shell (apoferritin) of molecular weight 450 kDa, containing a core of trivalent iron (Fe(III)) in the mineral form of ferrihydrite, a nano-crystal quite elusive to X-ray diﬀraction. Ferritin acquires Fe(II), catalyzes iron oxidation, and induces mineralization within its cavity [3]. The outer diameter of the shell is 12 nm, regardless of the iron loading, whereas the iron core diameter can vary between 2-3 nm and 7 nm [4], depending on the number of stored ions. It is generally agreed that the ferritin core is antiferromagnetic (AFM) below a temperature in the range of 340-500 K [4, 5, 6]. However, some AFM sublattices do not cancel out completely due to the small particle size. This results in an excess of spin orientation, giving rise to a magnetic moment of 225-400 µB [7, 4, 8]. Because of its hexagonal crystal structure (space group P63mc), the particle possesses a unique easy axis of magnetization [9]. The ”giant” magnetic moment of the particle can rotate about the crystal easy axis, if it overcomes an energy barrier Ea, which depends on the volume of the particle V and on the magnetocrystalline anisotropy constant, K [10]. If, upon decreasing the temperature, the dynamic time constant of the moment crossing the barrier (τc) is greater than the measuring time of the speciﬁc experimental technique (τm), the magnetic moment is said to be blocked [11]. This formalism was introduced by N´eel to describe the magnetism of nanoscopic single-domain particles with an internal magnetic order. The magnetic behaviour of an assembly of these ultra-ﬁne particles was termed superparamagnetism (SPM). This blocking occurs at about 12 K, for ferritin, when τm ∼ 100 s [12], in a DC magnetometry measurement. In addition to the AFM and SPM phases, it was also proposed that a Curie-Weiss-like behavior could be found at the core-shell interface, as a result of the reduced Weiss ﬁeld [7, 10]. More recently, other models have been proposed, yet the exact spin structure of the nanoparticle and its magnetic properties are still a matter of debate [7, 13, 4, 8]. Ferritin has also been extensively studied by neuroscientists, due to its central role in cellular iron homeostasis. Ferritin is at the center of many debates concerning iron toxicity in relation to neurodegeneration, and in particular to Alzheimer’s disease (AD). In the brain of AD patients, iron dis-regulation has been reported [14]. This may indicate a malfunction of the storage protein [15], leading to oxidative stress via Fenton and Haber-Weiss reactions [1, 16, 17, 18]. Microscopy studies revealed the existence of ferritins with a poly-phasic structure: ferritins found in the brain of AD patients seem to contain a higher amount of cubic crystalline phases consistent with magnetite and w¨ustite [19, 20], whereas the ’healthy type’-ferritins may be more abundant in the hexagonal ferrihydride phase. In this scenario, pathological ferritin would be better described by magnetoferritin, an artiﬁcial complex made of apoferritin containing a magnetite or maghemite crystal [21]. However, these observations were not conﬁrmed by nuclear magnetic resonance (NMR) [22]. Since magnetoferritin carries a larger magnetic moment than ferrihydrite, i.e. typically between 2000 µB and 9000 µB [23], one would expect a 200-fold (or larger) increase in the longitudinal and transverse relaxation rates of the water protons surrounding the protein. However, no signiﬁcant enhancement of the relaxation rate was observed. Additionally, Pan et al. showed that in human-liver ferritin, an increasing percentage of octahedrally coordinated Fe(III) migrated to tetrahedral sites and was partially reduced to Fe(II), upon increasing the electron dose in electron microscopy experiments [24]. Although it is rather unlikely that such alterations would happen only on the ’pathological’ ferritin [25], the poly-phasic composition of physiological versus pathological ferritins remains a debated issue. In order to unravel these controversies, here we propose a muon Spin Rotation (µSR) experiment, as an alternative investigation technique. µSR has been successfully employed in the past to study the magnetism of ﬁneparticles systems. In particular, horse-spleen ferritin [26, 27] and similar artiﬁcial compounds [28] have shown a two-component relaxation of the muon Asymmetry: while the fast-decaying Asymmetry (exponential- or Kubo-Toyabe-like) reﬂects the static order/collective excitation and the superparamagnetism of the iron core occurring respectively below and above the blocking temperature (TB), the slow exponential tail has been ascribed to the interaction of the muon spin with the protons of the organic shell. However, a study of ferritin puriﬁed from human tissue has not been undertaken yet. In this manuscript, we present a µSR study of a ferritin sample isolated from the brain of an AD patient, and an age- and gender-matched healthy control (HC). As a third sample and reference, a lyophilized commercial horse-spleen ferritin sample (HoSF) was used. Firstly, we evaluate the feasibility of µSR on a human sample. Then we propose a model to interpret the spin dynamics of the ferritin iron core, associated with the SPM eﬀect. Secondly, we draw some preliminary conclusions on the mineral composition of the protein core based on the magnetocrystalline anisotropy constant derived from our model, showing that all steps of the analysis are feasible. Finally, we discuss the limitations and the improvements that should be undertaken in future studies.
Dynamical evolution of massive perturbers in realistic multi-component galaxy models I: implementation and validation<|sep|>In the last decades, more and more interest has grown towards the modelling of orbital motions within the potential of a galaxy (e.g. Bovy 2015; Boubert et al. 2020; Granados et al. 2021). This has to be attributed to the advent of missions like Gaia (Gaia Collaboration et al. 2016, 2018), which is tracking the orbits of stars and stellar clusters sailing though the Galactic potential, and will release an unprecedented catalogue of Galactic stellar orbits. In addition, gravitational wave observatories such as LISA (Amaro-Seoane et al. 2017) and PTA (Verbiest et al. 2016) will probe massive black hole (MBH) mergers along the cosmic time; in order to interpret the rates of such mergers, one needs to model the inspiral phase of an intruder MBH within the potential of the host galaxy, down to the scale at which it will form a binary with another black hole that may already sit in the centre (Begelman et al. 1980). A detailed modelling of galactic orbits can only be achieved with a suﬃciently good description of the associated galactic potential wells; in addition, if one wants to accurately follow the orbit of a massive perturber (MP, i.e. an object whose mass is signiﬁcantly larger than the characteristic stellar mass), it is crucial to further account for the eﬀect of dynamical friction (DF, Chandrasekhar 1943; Binney & Tremaine 2008). At the present time, state-of-the art numerical simulations with sub-grid recipes for unresolved physical processes promise to be the best tool to properly model the orbital evolution of stars and MPs within galaxies (e.g. Tremmel et al. 2015; Pﬁster et al. 2017, 2019). However, they cannot be adopted when one needs to perform a throughout exploration of the parameter space as they are typically very computationally expensive. For this, diﬀerent tools, such as semi-analytical approaches, may be preferred owing to their much lower computational cost, that comes at the expense of some precision. In semi-analytical models, the motion of a test particle subject to the galactic potential can be followed more and more accurately as the description of the potential associated to each galactic component (the dark matter halo, disc, bulge etc.) gets closer to the one of real galaxies. While the spherically symmetric galactic components can be approximated reasonably well by
Constraints on the rate of supernovae lasting for more than a year from Subaru/Hyper Suprime-Cam<|sep|>The past decade saw the dawn of large-scale timedomain astronomy. Many transient surveys, such as Palomar Transient Factory (PTF, Law et al. 2009),
Microphone array post-filter for separation of simultaneous non-stationary sources<|sep|>Mobile robots with abilities to talk and listen should be able to discriminate and separate simultaneous sound sources while moving. For example, in the context of the cocktail party effect, the algorithms have to be robust and should allow the separation of simultaneous voices. In the present work we ﬁrst perform a crude linear separation of the sources and then use the proposed post-ﬁlter to further enhance the signals and suppress the contribution of the perturbating sources. Our post-ﬁlter is inspired by the original work of Cohen [1] who proposes a post-ﬁlter designed for a beamformer with one source of interest in the presence of stationary background and transient noises. In This research is funded by the Natural Sciences and Engineering Research Council, the Canada Research Chair Program and the Canadian Foundation for Innovation. c⃝2004 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. the present work we extend the principle to multiple localized sources of interest. We assume that both the signal of interest and the interferences may be present at the same time and for the same frequency bin. The novelty of our approach resides in the fact that, for each source of interest, we decompose the noise estimate into a stationary and a transient component assumed due to leakage between channels occuring during the initial separation stage. For each output channel of the linear source separator, we adaptively estimate the interference parameters (variance and SNR) and use them 1) to compute the probability of targeted speech presence 2) in the suppression rule when both speech and interference are present. We also propose the use of a Minimum Mean Square Estimation (MMSE) of the loudness – instead of the common log amplitude estimation – yielding a more efﬁcient cleaning of the signal when targeted speech is not present in the channel of interest. Section 2 gives an overview of the system and Section 3 describes the proposed post-ﬁlter. Results and discussion are then presented in Section 4 with the conclusion in Section 5.
Guided self-assembly of magnetic beads for biomedical applications<|sep|>The analysis of circulating tumor cells (CTCs) supports the monitoring of tumor growth and can be used to control the success of therapies. Microﬂuidic chips help to detect, to identify and to count these cells in peripheral blood. First time observed in 1869 [1] it becomes possible to gain a better understanding of how metastases form through the analysis of circulating tumor cells with the advance of technology platforms. Due to their rare appearance existing microﬂuidic ﬁlters [2] cannot ﬁnd every single CTC in the blood ﬂow. In these devises the distinct properties (size, aﬃnity, density) of the tumor cells are used to ﬁlter them. The technical challenge is to detect, count and isolate one CTC over one billion cells [3] (1–100 tumor cells per ml blood). A promising approach from Saliba et al. uses self organizing chains of ferromagnetic biofunctionalized beads [4]. An array of magnetic traps are prepared by microcontact printing in a microﬂuidic channel. Single particle chains line up which create a sieve like structure. This method has a limitation of ﬂow rate due to decreasing stability with higher velocity. Our proposed chip technology uses very thin Nickel seeding points with a diameter several times larger then the bead diameter. In this work we will analyze the microﬂuidic behavior of softmagnetic beads attracted by this seeding points in multiphysics simulations. Figure 1: Multiscale simulation environment: A) Magnetic ﬁeld source: N permanent magnets B) Zoom into the microﬂuidic chip: Hexagonal array of Ni seedings points (light green - area of interest) C) Trajectories of softmagnetic particles D) Areas of interest: Magnetic ﬁeld from permanent magnets and Ni discs for a closed simulation box ﬁlled with softmagnetic beads. Multiscale simulation environment A microﬂuidic chip (Fig. 1A) is placed on top of a single or multiple permanent magnets. On the bottom of the chip a hexagonal array of Nickel cylinders is placed. The cylinders act as a accumulation point (seeding point) for softmagnetic particles in the ﬂuid ﬂow. When applying the permanent magnets on the microﬂuidic chip the softmagnetic particles self-organize according to the ﬁeld created by the seeding points and the permanent magnets. The behavior of magnetic beads close to a single seeding point is discussed in Section 3.3. Cuboidal or cylindrical NdFeB permanent magnets are the magnetic source for the given scenario. Akoun [5] showed the analytic calculation of the magnetic ﬁeld created by a cuboidal permanent magnet. Derby [6] did the same for cylindrical permanent magnets. To get a higher magnetic ﬁeld ⃗H several magnets are combined having superposition of the ﬁeld values (Fig. 1A) In order to reduce simulation time and computational cost we are focusing on special areas in the microﬂuidic channel. Fig. 1B shows the microﬂuidic chip with the seeding point array at the bottom. The light green seeding points will be taken into account for further investigation. The simulation boundaries are set according to the results of Section 3.1. The magnetic ﬁeld ⃗H magnetizes the interacting seeding points. They create a highly non-homogeneous magnetic ﬁeld in the microﬂuidic chip. Fig. 1D shows the simulation area close to a single seeding point. Softmagnetic particles are randomly ﬁlled into the simulation box and interact with each other, the permanent magnets and the seeding points. Looking at a larger scale the trajectories of the magnetic beads to the seeding points are calculated with the software package Comsol [7] (Fig. 1C) . The particle distribution, i.e. the number of beads close to a single seeding point, depends on the ﬂuid velocity, the applied magnetic ﬁeld, the chip geometry as well as the seeding material.
Domain growth and aging scaling in coarsening disordered systems<|sep|>Domain growth taking place in coarsening systems is one of the best studied nonequilibrium phenomena, see [1,2, 3] for reviews of the ﬁeld. Our rather comprehensive understanding of domain growth in non-disordered systems has allowed us to gain new insights into generic properties of physical aging in situations where the single timedependent length scale increases as a power-law of time [4]. In this context the theoretical study of perfect, i.e. nondisordered, models has been most fruitful, see, for example, [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]. Progress in understanding coarsening in disordered systems has been much slower though. There are not many reliable theoretical tools at our disposal that allow us to cope with disorder when studying the dynamics out of equilibrium. In addition, the dynamics of the coarsening process is typically so slow that the characteristic dynamical length remains small within the time window accessible in numerical simulations. This eﬀect is well known for spin glasses [23,24], where the combination of disorder and frustration yields extremely slow dynamics [5,25,26, 27,28,29,30]. In the last years much eﬀort has been put into the study of disordered, but unfrustrated systems. Examples include coarsening of disordered magnets [31,32,33,34,35, 36,37,38,39,40], polymers in random media [41,42,43,44], or vortex lines in disordered type-II superconductors [45, 46,47,48,49,50,51]. Even though these systems are much less complex than those dominated by frustration eﬀects, their studies have yielded a fair share of controversies. In fact, already the most fundamental quantity, namely the growing length scale L(t), has resulted in long-lasting de bates. Whereas the classical theory of activated dynamics by Huse and Henley [52] predicts a logarithmic increase of this characteristic length, L(t) ∼ (ln t)1/ψ, with ψ > 0, early numerical simulations of disordered ferromagnets [31,32] yielded an algebraic increase, L(t) ∼ t1/z, with a dynamical exponent that was found to be nonuniversal and to depend on temperature as well as on the nature of the disorder. Recent simulations of the same models, however, revealed that this algebraic growth is only transient: the power-law increase is only an eﬀective one and masks the crossover to a slower asymptotic regime [38,3,40]. Convincing evidence of a crossover from a preasymptotic algebraic-like regime to a logarithmic regime follows from a recent series of studies that investigate the dynamics of elastic lines in random media [41,42,43,44]. A second issue is related to the concept of superuniversality [5] that states that scaling functions should be independent of disorder once time-dependent quantities are expressed through the characteristic length scale L(t). The available numerical evidence is rather contradictory, with some quantities supporting the claim of superuniversality, whereas for others clear deviations are observed [53,54,55, 56,57,35,36,38,3,40]. Even though there is growing consensus that superuniversality in the strictest sense is not fulﬁlled, there is strong evidence that in certain regimes scaling functions of one- and two-times quantities show a remarkable independence on the disorder. In our recent work [38] we studied the scaling behavior of the two-times space-time correlation function C(t, s; r) in the two-dimensional random-site Ising model. Here s and t > s are two diﬀerent times, both measured since the preparation of the system, that are called waiting and
Thermodynamic properties of the new multiferroic material (NH$_4$)$_2$[FeCl$_5$(H$_2$O)]<|sep|>Multiferroic materials with simultaneous ferroelectric and (anti-)ferromagnetic order in the same phase have attracted considerable interest during the last decade [1–4]. Especially the discovery of spin-driven ferroelectricity in magnetically frustrated systems [5], as, e.g., in the transition metal oxides REMnO3 (RE = Tb, Dy) [6], Ni3V2O8 [7], LiCu2O2 [8], MnWO4 [9–11], NaFeX2O6 (X=Si, Ge) [12,13], CuO [14] or CaMn7O12 [15] (but also in non-oxide systems such as, e.g., CuCl2 [16] or K3Fe5F15 [17]) revived the search for new multiferroic materials. Typically, these multiferroics show complex, noncollinear spin structures and a strong coupling between magnetic and ferroelectric order exists. Consequently, the spontaneous electric polarization can strongly be modiﬁed by applying external magnetic ﬁelds. Depending on the direction and strength of the magnetic ﬁeld, reversal, rotation or suppression of the electric polarization may occur. Such magnetic ﬁeld induced changes of the electric polarization or, vice versa, electric ﬁeld dependent magnetization changes, are not only interesting from the fundamental physical point of view, but are also interesting for potential new devices in the ﬁelds of data memory or sensor systems. Here, we report the discovery and the basic characterization of the new multiferroic material ammonium pentachloroaquaferrate(III), (NH4)2[FeCl5(H2O)]. It belongs to the family of erythrosiderite-type compounds A2[FeX5(H2O)], where A stands for an alkali metal or ammonium ion and X for a halide ion. As for most members of this family, the room-temperature crystal structure of (NH4)2[FeCl5(H2O)] is orthorhombic with space group Pnma [18] and lattice constants (at room temperature) a = 13.706(2) ˚A, b = 9.924(1) ˚A, c = 7.024(1) ˚A [19]. The structure consists of isolated (NH4)+ units and isolated complex groups [FeCl5(H2O)]2− of sixfold octahedrally coordinated iron(III), see ﬁgure 1. The unit cell contains eight symmetrically equivalent (NH4)+ groups and four [FeCl5(H2O)]2− octahedra. The Fe–O bonds of the octahedra are approximately lying parallel to the ac plane with alternating angles of about ±41◦ relative to the a axis. This results in a herringbone-like arrangement of the Fe–O bonds along a. Besides ionic bonds between the structural building blocks, there is a pronounced Hbonding (via O–H–Cl) between neighbouring [FeCl5(H2O)]2− octahedra along the b axis, which further stabilizes the crystal structure. These H-bonded octahedra form zig-zag chains along b and along these chains the Fe-O bonds of adjacent octahedra are oriented mutually antiparallel to each other [18–20]. The magnetic ordering phenomena occurring in the A2[FeX5(H2O)] series have been subject of various investigations and all members studied so far have been identiﬁed as antiferromagnets [21–26]. In this context, a N´eel temperature TN ≃ 7.25 K of the title compound has been derived from measurements of the magnetic susceptibility [21]. However, there are some clear diﬀerences between (NH4)2[FeCl5(H2O)] and the corresponding alkali compounds with A = K, Rb. Susceptibility data of the alkali compounds identify the a axis as the magnetic easy axis [21, 23, 24], in agreement with the antiferromagnetic collinear spin structure determined via neutron Figure 1. (a) Crystal structure of (NH4)2[FeCl5(H2O)]; NH4 groups are displayed by blue tetrahedra (with H atoms in red), Fe and Cl are marked by orange and green spheres, respectively, while the oxygen of H2O is given by blue spheres. The unit cell is marked by solid lines and the dashed lines indicate the hydrogen bonds between the H2O and Cl ligands of adjacent [FeCl5(H2O)] octahedra. Structural data are taken from [19]. A photograph of a grown (NH4)2[FeCl5(H2O)] crystal and the typical growth morphology with face indices are displayed in (b) and (c), repectively. scattering [25, 26]. In contrast, no easy axis could be derived from the susceptibility data of (NH4)2[FeCl5(H2O)], and, moreover, heat capacity measurements revealed a second phase transition at 6.87 K, while only single transitions are observed in the alkali compounds [21, 27]. These observations led to the suggestion of a canted antiferromagnetic spin structure with some kind of spin rearrangement at 6.87 K. Later on, there was some debate about the proposed spin canting and the occurrence of a structural phase transition around 240 K was suggested [28, 29]. However, no evidence for such a structural transition has been found in M¨ossbauer spectroscopy and powder X-ray diﬀraction experiments within the relevant temperature range [30]. The M¨ossbauer spectra also led to the suggestion of a more complex spin arrangement in (NH4)2[FeCl5(H2O)], because the interpretation of the spectra below TN was inconsistent with a simple canted spin structure [30]. To our knowledge, an unambiguous determination of the magnetic structure of (NH4)2[FeCl5(H2O)] is still missing. In the present work, by means of dielectric, pyroelectric and magnetoelectric measurements, (NH4)2[FeCl5(H2O)] is established as a new multiferroic material. Our data agree with the suggested N´eel ordering at ≃ 7.25 K followed by a spin reorientation at ≃ 6.9 K and we ﬁnd, that this lower transition is accompanied by a pronounced spontaneous electric polarization. The polarization and magnetization data are complemented by measurements of thermal expansion, magnetostriction and heat capacity in order to derive detailed magnetic ﬁeld versus temperature phase diagrams for diﬀerent directions of the magnetic ﬁeld.
Robustness Threats of Differential Privacy<|sep|>Although neural networks achieved state-of-the-art performance in many problems, they are vulnerable to diﬀerent malicious attacks. Adversaries might harm the systems by breaking the robustness of the neural network (security risks) or by inferring the secret information (privacy risks). These risks endanger their deployment in some speciﬁc environments, such as medical image analysis [17] and video surveillance [36], where models have to be both secure and private. Diﬀerential privacy (DP) [11] is a powerful concept of guaranteeing privacy of data. It provides a promise claiming that training data will not be aﬀected, adversely or otherwise, no matter what other information is available elsewhere [11]. This deﬁnition is not too strict, as surprisingly anonymous data can be robustly de-anonimized using other publicly available information, even for large sparse datasets [32,2]. Moreover, to preserve privacy of one training sample, we should assume that the adversary has access to some or even all other training samples. This scenario is not unrealistic, and can be observed when the data is crowdsourced. To mitigate this, Abadi et al. [1] proposed DP-SGD to apply DP to the learning procedure. This method protects individual privacy from strong adversaries with the full knowledge of the training procedure and model’s hyperparmeters, and learns useful information about the dataset as a whole.
An Optimal Fully Distributed Algorithm to Minimize the Resource Consumption of Cloud Applications<|sep|>the cloud providers to offer user friendly environments that  can be used by clients to execute applications. The services  are delivered to the cloud users through a pay-per-use-model,  which means that the owner of an application is required to  pay an amount of money that is (approximately) proportional  to the amount of resources the respective application  consumes during its execution on the cloud. Therefore,  applying intelligent techniques to minimize the resource  consumption is of paramount importance. identifying an assignment scheme between the interacting  components, such as processes and virtual machines of an  application, and the computing nodes of a cloud system, such  that the total amount of resources consumed by the  respective application is minimized. However, the problem is  NP-complete for general network graphs, even in the case of  unbounded capacity ([1], [8]). Therefore, many researchers  have focused on some variations of the network graph, such  as homogeneous arrays [5] or trees [6], which facilitates in  solving the problem in polynomial time. interesting when considering applications that are prone to  load changes. This is due to the fact that in such kinds of  problems, an assignment scheme may be optimal for some time interval, while sub-optimal for some time interval.  Therefore, it is meaningful to dynamically reassign the  processes  to  nodes  taking  into  account  the  new communication demands of the application components.  However, extra care must be taken to not make redundant  calculations for processes that are already optimally located  within the system. Therefore, approaches that build on the  min-cut max-flow technique, such as reported in [5] and [6]  become  inapplicable  when  considering  large-scale Assuming that an application is already hosted in a cloud,  reassign the application components to nodes of the system  so  as  to  minimize  the  total  communication  and distributed algorithm that leverages on the min-cut max-flow  technique. It should be stressed that the min-cut algorithm is  invoked only for very small portions of the application  graph. The main idea behind our algorithm is that each node  must identify, based only on local information, which  processes or group of processes (hosted by the respective  node) must migrate to other locations to minimize the  resources consumed by an application. Because the studied  problem is intractable for networks that are modeled as  general graphs, we restrict the optimality proof of our  algorithm to tree-structured networks. However, we extend  the functionality of our proposed algorithm to run also on  hierarchical networks. In this paper, we also provide a proof  about DRA’s convergence. The innovation of our work lies  in the fact that this is the first time a problem of such nature  has been solved in a distributed fashion, leading at the same  time to the optimal solution. related work; in Section III we describe the system model  and rigorously formulate the optimization problem; the  proposed algorithm is detailed in Section IV; in Section V  we provide a thorough discussion about the convergence and  optimality proof of our algorithm; while Section VI  concludes the work.
Robust Factorization of Real-world Tensor Streams with Patterns, Missing Values, and Outliers<|sep|>Tensors are high-dimensional arrays that are used to represent multi-way data. Data modeled as tensors are collected and utilized in various ﬁelds, including machine learning [1], urban computing [2], [3], chemometrics [4], image processing [5], [6], and recommender systems [7], [8]. As the dimension and size of data increase, extensive research has been conducted on tensor factorization, using which tensors can be analyzed efﬁciently and effectively. Given a tensor, tensor factorization extracts its underlying latent structure, which is meaningful and at the same time useful for various purposes. Among many tensor factorization methods, CANDECOMP/PARAFAC (CP) factorization [4], [8], [9] is most widely used due to its simplicity and effectivenss. Real-world tensors are often incomplete due to unintended problems such as network disconnection and system errors. The problem of imputing the missing entries based on observed data is called tensor completion. It is one of the most important and actively studied problems in tensor related research, and numerous solutions based on tensor factorization has been developed [6], [9], [10]. At the same time, real-world tensors are easily corrupted by outliers due to unpredicted events during data collection, such as sensor malfunctions and malicious tampering. Recovering incomplete and at the same time contaminated tensors is a challenging and unwieldy task since tensor factorization, which most tensor completion techniques are based on, is vulnerable to outliers. To ﬁnd latent structure behind such a noisy tensor accurately, many efforts have been made to design a ‘outlierrobust’ tensor factorization algorithm [5], [20], [21]. Moreover, it is common to incorporate temporal information into tensors by adding temporal modes that evolve over time. Tensor factorization also has been applied to such time-evolving tensors since latent temporal components provide considerable insight into temporal dynamics. Successful applications include anomaly detection [22], discussion tracking [23], recommender system [24] and link prediction [17]. In many applications, tensor data are usually collected incrementally over time in the form of a tensor stream. Since tensor streams can potentially be of inﬁnite length, it is infeasible, in terms of computational and storage costs, to use batch-based tensor factorization approaches to process them. Instead, tensor factorization should be performed in an online manner. That is, new tensor entries should be processed incrementally, as they arrive, without requiring too much space. Given that real-world tensors are evolving constantly over time with missing entries and unexpected outliers, how can we estimate the missing entries? Can we also predict future entries? Can both imputation and prediction be performed accurately in an online manner? In this work, we propose SOFIA, a Seasonality-aware Outlier-robust Factorization of Incomplete streAming tensors. SOFIA extends CP factorization to leverage two temporal characteristics inherent in real-world time series: graduality and seasonality.1 Note that even identifying these patterns itself
On the readability of overlap digraphs<|sep|>In this paper, we introduce and study a graph parameter called readability, motivated by applications of overlap graphs in bioinformatics. A string x overlaps a string y if there is a suﬃx of x that is equal to a preﬁx of y. They overlap properly if, in addition, the suﬃx and preﬁx are both proper. The overlap digraph of a set of strings S is a digraph where each string is a vertex and there is an arc from x to y (possibly with x = y) if and only if x properly overlaps y. Walks in the overlap digraph of S represent strings that can be spelled by stitching strings of S together, using the overlaps between them. Overlap digraphs have various applications, e.g., they are used by approximation algorithms for the Shortest Superstring Problem [Swe00]. Their most impactful application, however, has been in bioinformatics. Their variants, such as de Bruijn graphs [IW95] and string graphs [Mye05], have formed the basis of nearly all genome assemblers used today (see [MKS10,NP13] for a survey), successful despite results showing that assembly is a hard problem in theory [BBT13,NP09,MGMB07]. In this context, the strings of S represent known fragments of the genome (called reads), and the genome is represented by walks in the overlap digraph of S. However, do the overlap digraphs generated in this way capture all possible digraphs, or do they have any properties or structure that can be exploited? Braga and Meidanis [BM02] showed that overlap digraphs capture all possible digraphs, i.e., for every digraph D, there exists a set of strings S such that their overlap digraph is D. Their proof takes an arbitrary digraph and shows how to construct an injective overlap labeling, that is, a function assigning a unique string to each vertex, such that (x, y) is an arc if and only if the string assigned to x properly overlaps the string assigned to y. However, the length of strings produced by their method can be exponential in the number of vertices. In the bioinformatics context, this is unrealistic, as the read size is typically much smaller than the number of reads. To investigate the relationship between the string length and the number of vertices, we introduce a graph parameter called readability. The readability of a digraph D, denoted r(D), is the smallest nonnegative integer r such that there exists an injective overlap labeling of D with strings of length r. The result by [BM02] shows that readability is well deﬁned and is at ⋆ This is a full version of a conference paper of the same title at the 26th Annual Symposium on Combinatorial Pattern Matching (CPM 2015) most 2∆+1 − 1, where ∆ is the maximum of the in- and out-degrees of vertices in D. However, nothing else is known about the parameter, though there are papers that look at related notions [BFK+02,BFKK02,BHKdW99,GP14,LZ07,LZ10,PSW03,TU88]. In this paper, we study the asymptotic behaviour of readability as a function of the number of vertices in a graph. We deﬁne readability for undirected bipartite graphs and show that the two deﬁnitions of readability are asymptotically equivalent. We capture readability using purely graph theoretic parameters (i.e., without any reference to strings). For trees, we give a parameter that characterizes readability exactly. For the larger family of bipartite C4-free graphs, we give a parameter that approximates readability to within a factor of 2. Finally, for general bipartite graphs, we give a parameter that is bounded on the same sets of graphs as readability. We apply our purely graph theoretic interpretation to prove readability upper and lower bounds on several graph families. We show, using a counting argument, that almost all digraphs and bipartite graphs have readability of at least Ω(n/ log n). Next, we construct a graph family inspired by Hadamard codes and prove that it has readability Ω(n). Finally, we show that the readability of trees is bounded from above by their radius, and there exist trees of arbitrary readability that achieve this bound.
Enhancing magic sets with an application to ontological reasoning<|sep|>Datalog is a rule based language for knowledge representation and reasoning suitable for a natural declaration of inductive deﬁnitions and ontological reasoning (Eiter et al. 2012). Several extensions to the core language of Datalog exist, among them default negation (Gelder 1989; Gelder et al. 1991; Gelfond and Lifschitz 1991) and aggregates (Simons et al. 2002; Pelov et al. 2007; Liu et al. 2010; Bartholomew et al. 2011; Ferraris 2011; Gelfond and Zhang 2014). Restrictions on the use of these linguistic constructs lead to preserve the existence and uniqueness of the stable model associated with a knowledge base; speciﬁcally, such restrictions essentially enforce a stratiﬁcation on the deﬁnitions involving negation and aggregates (Faber et al. 2011). The semantics of the resulting language reached a broad consensus in the knowledge representation and reasoning community, as in fact the notions of perfect model, well-founded model, and stable model coincide for stratiﬁed programs (Przymusinski 1989; Gelder et al. 1991). The stable model of a Datalog program can be constructed bottom-up, starting from facts in the program, and deriving new atoms from rules whose bodies become true. Negation and aggregates are handled by partitioning the input program into different strata, so that the lowest stratum does not contain negation and aggregates, and each other stratum only negates and aggregates over predicates of lower strata. Such a bottom-up procedure is very efﬁcient for producing the
Three family unification in higher dimensional models<|sep|>The standard model (SM) is very well established to describe the physics below the electroweak scale. Theoretically, it is expected that there exists a model beyond SM. The conceptual motivation to consider the model beyond SM is to understand the variety of particles as well as the parameters in SM. In fact, the content of particles in SM is a collection of widely disparate ﬁelds: gauge bosons coming in three factors (color, weak and hypercharge), three replicated families of chiral fermions coming in many diﬀerent representations for quarks and leptons (q, uc, dc, ℓ and ec), and a scalar Higgs boson to break electroweak symmetry and give masses to the chiral fermions. This brings lots of parameters in SM: three gauge couplings, masses and mixings for the quarks and leptons, and a Higgs mass and a Higgs coupling. The Higgs scalar has a quadratic divergence in its mass squared, and thus the electroweak scale is not stable quantum mechanically if the cutoﬀ scale is very high such as the Planck scale. Therefore, there must exist a theory beyond SM around the TeV scale. Besides, the masses and mixings of the quarks and leptons originate from the Yukawa couplings with the Higgs boson, which are the most of the parameters in SM. In such a sense, the nature of the Higgs boson is a key ingredient to go beyond the standard model. It is expected that the Higgs boson will be found at the Large Hadron Collider (LHC) experiment. The idea of extra dimensions is an attractive candidate to build a model beyond SM. Kaluza and Klein (KK) showed that it is possible to interpret electromagnetism as the eﬀect of gravity in ﬁve dimensions under certain projections. After dimensional reduction, a vector/tensor ﬁeld in some higher dimensional compactiﬁed space decomposes into separate scalar, vector and tensor components in four dimensions (4D). The left- and right-handed Weyl fermions in 4D are uniﬁed in higher dimensional fermions. The idea of extra dimensions has not been treated in phenomenological issues, but it becomes fashionable since it may explain the large scale hierarchy [1]. The idea of compactiﬁcation is also applied to break symmetries by orbifold boundary conditions [2, 3, 4]. Though the gauge symmetry is explicitly broken by the orbifold conditions, the gauge couplings can be uniﬁed when the brane localized gauge interaction is suppressed by a large volume of the extra dimensions [3]. In such fashions, the idea of gauge Higgs uniﬁcation [5] is revived [6]. The scalar Higgs ﬁelds can be uniﬁed with the gauge ﬁelds in some higher dimensional vector ﬁelds. In a simple orbifold boundary condition, the gauge symmetry is broken since the broken generators of gauge bosons for 4D coordinates are projected out. At that time, the broken generators for extra dimensional components can have massless modes, and the Wilson line operator can be identiﬁed as the Higgs bosons to break the symmetry remained in 4D [7]. Interestingly, this idea is compatible to extend the SM gauge symmetry to a larger gauge group such as in grand uniﬁed theories (GUT). Besides, the mass of the Higgs scalar is forbidden by gauge invariance, and thus it can remain light at low energy when supersymmetry (SUSY) is combined in the model. The interesting consequence of the gauge-Higgs uniﬁcation is that the Yukawa interaction can originate from the gauge interaction when fermions are also higher dimensional bulk ﬁelds [8, 9]. Actually, the 4D zero modes of fermions can be chiral in orbifold projections, and the higher dimensional extension of the fermion kinetic term with covariant derivative, ¯ψγµ(∂µ − igAµ)ψ, includes Yukawa coupling when the gauge ﬁelds with higher dimensional components are identiﬁed as Higgs ﬁelds. In the left-right symmetric construction of the model [10], the matter representation to realize the gauge-Yukawa uniﬁcation can be much simpler than that of the SM construction, and the actual uniﬁcation of gauge and Yukawa coupling constants can be realized [9]. In the models of 5D N = 1 SUSY S1/Z2 orbifold with bulk gauge symmetries such as SO(11) and SU(8), which break down to Pati-Salam (PS) symmetry group GPS = SU(4)c × SU(2)L × SU(2)R [11] in 4D, matter ﬁelds are uniﬁed in hypermultiplets, and all three gauge couplings and third generation Yukawa couplings (top, bottom, tau and Dirac tau neutrino) can be uniﬁed. Then, the Yukawa couplings at the weak scale can be calculated assuming that the threshold corrections are small. Consequently, we can predict the top quark mass as well as tan β, which is a ratio of Higgs vacuum expectation values (VEVs) for upand down-type Higgs bosons. Actually, the prediction of the top quark mass can agree with the experiment if we take into account the threshold corrections [12]. If we say it inversely, we can survey if the uniﬁcation of gauge and Yukawa couplings is really realized in the future since the LHC and ILC experiments will provide us more accurate measurement of the Yukawa couplings above TeV scale. It is important that the uniﬁcation of the gauge and Yukawa coupling constants can be a signal of extra dimensions at ultra high energy scale. Therefore, we should investigate models in which gauge and Yukawa uniﬁcation can happen. In SUSY extensions, the matter fermions can be uniﬁed in higher dimensional gauge mul tiplets [13, 14], especially when the model consists of N = 4 vector multiplet in 4D language such as in 6D N = (1, 1) SUSY. Interestingly, three replications of family can be obtained in the T 2/Z3 orbifold [13]. The three families originate from the three chiral supermultiplets in the N = 4 gauge multiplet. Since the number of chiral multiplet is maximally three in 4D, it may explain why the family is three times replicated. The hypermultiplet which is adjoint representation under the bulk gauge symmetry in 5D N = 1 SUSY S1/Z2 orbifold model can be incorporated into the gauge multiplet in 6D N = (1, 1) SUSY orbifold models. In Ref.[15], it is found that all matter species for one family and Higgs doublets as well as gauge ﬁelds in SM can be uniﬁed in 6D N = (1, 1) SUSY SU(8) Yukawa couplings can be also uniﬁed in the model. The other gauge groups [16, 17] and other extensions including seven dimensional models [18, 19] are also considered. Since no other bulk matter ﬁelds can be introduced, the model can explain why only third family is heavy. If we choose T 2/Z3 orbifold in the SU(8) model, the discrete charge assignment is simple and almost unique when N = 1 SUSY remains at 4D. In the 6D SU(8) T 2/Z3 orbifold model, thus, three families and Higgs ﬁelds as well as gauge ﬁelds are naturally uniﬁed in one multiplet [20]. In this paper, we will study the cases where there are just three chiral families as zero modes of the bulk ﬁelds in the higher dimensional orbifold models. We investigate the PatiSalam branch of the E8 and its subgroup to break the symmetry by orbifold compactiﬁcation, and to extract the three chiral families as zero modes. In fact, the adjoint representation of E8 contains maximally four families of matters as decomposed representation under the Pati-Salam symmetry. There are three chiral superﬁelds in the bulk for the higher dimensional models, and thus, maximally twelve families are contained in the bulk. By orbifolding, many of the component representations are projected out, and we choose the orbifold charge assignment to extract just three chiral families as zero modes. We will show that there are ﬁve cases for the charge assignments to obtain the three chiral families, and give examples of the discrete charge assignments. The Yukawa interaction is generated from the bulk gauge interaction, and we will classify the structure of the mass eigenvalues in the three-family models. In some cases, global or gauged ﬂavor symmetry remains in the Yukawa interaction due to the bulk gauge interaction. The ﬂavor symmetries originates from the R symmetry or gauge symmetry in the bulk. In such cases, two of the eigenvalues are degenerate. In one case, the ﬂavor symmetry is broken by the orbifold projection, and the eigenvalues are not degenerate, but there is a restriction in the 4D gauge symmetry which contains Pati-Salam branch. We study the triniﬁcation branch in the typical case to obtain a hierarchical structure of the fermion masses. We also consider the case in which SM gauge symmetry GSM = SU(3)c × SU(2)L × U(1)Y remains in 4D. This paper is organized as follows: In section 2, we introduce the orbifold models which we consider in this paper. In section 3, we investigate the Pati-Salam branch from the E8 group and its subgroups. The Zn charges are assigned to the decomposed ﬁelds under the Pati-Salam symmetry in order break the bulk gauge symmetry down to the Pati-Salam symmetry. In section 4, we will show the ﬁve cases to obtain the three chiral families as zero modes from the bulk ﬁelds, and give examples of the orbifold charge assignments. In section 5, we make a comment to develop the example to build a model. In section 6, we consider the triniﬁcation
Optics in a nonlinear gravitational plane wave<|sep|>Some of the most important potential signatures of gravitational waves are associated with their eﬀects on the propagation of light. Collections of null rays can be deﬂected, sheared, delayed, or otherwise altered as they travel through a gravitational wave. Indeed, most contemporary attempts to observe gravitational waves rely on measurements of the relative time delays which accumulate as light travels between material bodies. This is particularly clear for interferometric detectors [1], where one or more beams of light are circulated between collections of mirrors and then recombined to reveal their relative phases. Eﬀorts to detect gravitational waves using pulsar timing arrays [2] exploit similar principles, but instead make use of time intervals observed on the earth between radio bursts emitted by distant pulsars. Besides temporal eﬀects such as these, gravitational waves can also aﬀect observations of an object’s sky location, brightness, shape, and so on [3, 4, 5, 6, 7, 8, 9]. Almost all prior discussions of these phenomena have been perturbative, involving calculations which are valid only through ﬁrst order in the gravitational wave amplitude (see, however, [3, 10, 11, 12, 13]). This has been justiﬁed, at least implicitly, by the minuscule size of even these lowest-order terms: In most cases of astrophysical interest, the gravitational wave strain amplitude ϵ is much smaller than unity. Enormous technological eﬀort is required to detect such waves at all, and waveform measurements which are accurate to more than a handful of decimal places cannot be expected for quite some time. In this context, it might appear reasonable to dismiss higher-order corrections as uninterestingly-small. One of the goals of this paper is to show that such reasoning can be misleading. Even if a dimensionless observable associated with a gravitational wave of amplitude ϵ ≪ 1 is bounded by Figure 1: Schematic of a physical system which could correspond to the model considered in this paper. A “source” S emits electromagnetic radiation which is viewed by an “observer” O. In between these objects, spacetime is assumed to be approximately ﬂat except for a nearly-planar gravitational wave. This wave may be generated by a distant binary, although all considerations here are restricted to the boxed region, and are therefore indiﬀerent to the precise nature of the wave generation mechanism. ϵ×(number of order 1) in linear perturbation theory, higher-order corrections are not necessarily bounded by ϵ2 × (another number of order 1). The coeﬃcient in front of the ϵ2 term can instead grow enormously with the distance between a light source and its observer, implying that nonlinearities may be signiﬁcant even when considering observations of very weak gravitational waves. Nonlinear eﬀects also tend to have very diﬀerent observational signatures from their lower-order counterparts, further increasing their potential detectability. Although it does not appear to have been previously pointed out in this context, the existence of large higher-order corrections is well-known in many types of perturbative calculations. A simple example is provided by the Mathieu-type equation If u denotes an appropriate phase coordinate, ξ2(u) may be shown to describe a particular metric component associated with a linearly-polarized, “monochromatic” gravitational plane wave with strain amplitude ϵ. Moreover, the coordinate system where this is true is constructed such that there is a sense in which electromagnetic observations of distant objects have properties which can be read oﬀ directly from ξ2(u). Solutions to (1) therefore serve as a convenient proxy for understanding nonlinear eﬀects associated with monochromatic gravitational waves in general relativity. Assuming ϵ ≪ 1 while adopting convenient initial conditions, The magnitude of the second-order term in this expansion clearly overtakes the ﬁrst when |u| ∼ ϵ−1/2 ≫ 1, signaling that the linear approximation fails for large |u|. This occurs no matter how small ϵ may be; weaker amplitudes merely delay such problems to larger scales. We show that similar eﬀects arise for a variety of gravitational wave observables in general relativity, thus implying that the results of linear perturbation theory cannot necessarily be applied on large scales. As a model, geometric optics is considered in the presence of a plane-symmetric gravitational wave. This may be viewed as an idealization of the system illustrated in Figure 1, where observations are performed suﬃciently far from a gravitational wave source that the curvature of the wavefronts may be neglected. Similar models are common (though restricted only to ﬁrst-order metric perturbations) in, e.g., descriptions for how gravitational waves can aﬀect pulsar timing measurements [2, 14]. Despite this, real astrophysical observations cannot rely solely upon plane wave calculations. Deviations from planarity, waves propagating in multiple directions, non-radiative metric perturbations, and other eﬀects could all be signiﬁcant in observationally-relevant systems. Although calculations which take into account many such eﬀects have been performed through ﬁrst post-Minkowskian order [7, 15], the optical characteristics of nonlinear gravitational waves have been essentially unexplored in this context. Moreover, it would likely be diﬃcult to understand the implications of any such calculations even if they did exist; the known ﬁrstorder expressions are already extremely complicated in their most general forms. Plane waves are, by contrast, suﬃciently simple that their physical eﬀects can be thoroughly explored even in the nonlinear regime. At the same time, these waves remain suﬃciently complicated to be interesting, and also to capture much of the relevant physics. Results obtained using plane wave descriptions may therefore be useful in the construction of speciﬁc hypotheses whose generality can later be tested using more complicated models. The technical details of the plane wave problem can also be used to suggest potential simpliﬁcations in more general calculations. A completely separate motivation for considering plane wave spacetimes follows from a mathematical device known as the Penrose limit [16, 17, 18]. This provides a sense by which the geometry near any null geodesic in any spacetime is equivalent to the geometry of an appropriate plane wave. It can be interpreted as a statement that the metric in a small region around any suﬃciently-relativistic observer in any spacetime is equivalent to that of an “eﬀective plane wave.” Although we make no attempt to prove it, the Penrose limit suggests that (at least some types of) observations performed by ultrarelativistic observers can in general be reduced to analogous observations in eﬀective plane wave spacetimes. Section 2 reviews gravitational plane waves in general relativity, ﬁrst from the viewpoint of perturbation theory, and then as exact solutions to Einstein’s equation. Relations between these two perspectives and their relative advantages are described in detail. Next, Section 3 considers the physical consequences of plane wave spacetimes by deriving exact time delays, frequency shifts, observed sky positions, area distances, and luminosity distances. With appropriate identiﬁcations, some of the resulting expressions are only slightly more complicated than their linearized counterparts. Formal perturbative expansions are nevertheless derived in Section 4 and then applied to speciﬁc examples in Section 5. The metric signature here is +2, c = G = 1, the Riemann tensor satisﬁes Rabcdωd = 2∇[a∇b]ωc for any ωc, Latin letters a, b, . . . denote abstract indices, Greek letters µ, ν, . . . denote four-dimensional coordinate indices, and i, j, . . . are used as twodimensional coordinate indices associated with directions transverse to the background gravitational wave. When convenient, transverse coordinate components are indicated using boldface symbols without indices [e.g., γij = (γ)ij, γijwivj = w⊺γv, and tr γ = δijγij]. The one exception where the boldface symbol doesn’t correspond to its component counterpart is the 2 × 2 identity matrix (I)ij = δij. Lastly, overdots
Analytical solution of the cylindrical torsion problem for the relaxed micromorphic continuum and other generalized continua (including full derivations)<|sep|>In this paper we continue our endeavour to ﬁnd analytical solutions to simple boundary value problems for families of generalized continua [3, 60, 61]. The focus is on non-homogeneous solutions that on one side activate the additional deformations modes oﬀered by generalized continua (curvature terms) and which may be used, on the other side, in calibrating the additional (many) material parameters. The renewed interest in models of generalized continua comes in part from the fact that for small specimens one may observe size-eﬀects, not accounted for by linear Cauchy elasticity. On the other hand, the description of man-made architecture materials/meta-materials need generalized continua to capture frequency band-gaps in the dynamic range, a prominent example being given by the relaxed micromorphic model [1, 2, 40, 59, 62]. Here, we consider the static St. Venant torsion problem. Since we aim at identifying material parameters, let us ﬁrst review what can be said for isotropic linear elasticity.
Zero-shot Visual Commonsense Immorality Prediction<|sep|>Despite the explosive developments of Artiﬁcial Intelligence, AI ethics research has been overlooked from many researchers. The previous research on ethical artiﬁcial intelligence has been analyzed solely from the philosophical view, but not from the computational perspective [3, 29, 48]. Philosophers kept speculating that if computer scientists had focused only on optimization or problem solving, artiﬁcial intelligence will encounter catastrophes of immorality [4, 9, 41]. Fortunately, the demand for ethical machine learning has been increasing and resulted in narrow ethical applications in Artiﬁcial Intelligence. © 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.
X-Vector based voice activity detection for multi-genre broadcast speech-to-text<|sep|>Large broadcasters such as the BBC produce thousands of  hours of AV content each day. Accurate automatic Speechto-Text (STT) systems can unlock more value from such  content, aiding discoverability, reusability and accessibility.  In previous decades, the costs, lack of specialisms and low  accuracy of generating STT transcripts on broadcast audio  have impeded large scale adoption. However, advancement  in accuracy and efficiency, as well as development of opensource toolkits such as Kaldi ASR [1], have enabled the use  of STT as a practical tool for broadcasters and journalists [2].  A series of academic challenges based on broadcast datasets  in multiple languages [3][4][5] demonstrates continuing  interests from both the academia and the industry in making  STT systems work better on real-world broadcast audio. One  of the many challenges in achieving a reliable broadcast STT  system is voice activity detection (VAD).    VAD refers to the task of distinguishing active speech presence of non-speech vocal sound, extended silences,  environmental noise and background music that often  coincide with speech. As it serves as an essential preprocessing step, developing a robust VAD system is of  particular importance to numerous downstream applications  such as STT, speaker diarisation and speaker recognition.  Earlier VAD approaches include energy-based thresholding  [6], spectral analysis [7][8] and pitch detection [9]. Following  the work of Sohn et.al. [10], where parameter estimation was  applied using the Gaussian distribution, several statistical  methods have been proposed [11][12][13]. More recently,  machine learning-based systems have achieved state-of-theart performance by considering VAD as a frame-level  speech/noise classification problem. Methods utilising  support vector machines (SVM) [14][15][16] and deep neural  networks (DNN) [17][18][19][20] are amongst the most  actively studied. extracted from a DNN [22] trained to classify speakers in the  training data. The x-vector system was shown to significantly  outperform i-vector baseline systems in speaker recognition  benchmarks [21]. Application of x-vectors ranges from  speaker diarisation [23][24], speaker recognition [25][26],  language detection [27] to acoustic scene classification [28],  emotion classification [29] and early detection of Parkinson’s  Disease [30][31]. audio classification tasks led us to reason that x-vectors can  be used as features to train a VAD system. To this end, we  propose that 1) X-vectors contain latent information to  differentiate speech from noise in audio signals and hence 2)  it is possible to use x-vectors as features to train a VAD  system which, when applied upstream, 3) improves the  performance of an STT system. In this paper, we present our  work to address these propositions, with a particular focus on  developing a robust and accurate VAD system that can detect  speech reliably in the presence of a variety of noise, and its  application to automatic STT transcription of broadcast audio.
Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems<|sep|>The exact description of the glassy phases of high-dimensional sphere systems and the discovery that universal predictions at jamming match ﬁnite dimensional observations [1] has renewed the interest in the statistical physics of random constraint satisfaction problems (CSP) with continuous variables. In CSPs, one seeks assignments of a set of N variables that satisfy a system of constraints. Sphere systems clearly belong to this class: one needs to ﬁnd the positions of N spheres inside a box with the conditions that the spheres do not overlap. Particularly interesting in a statistical physics perspective is the case where the constraints are taken at random from some ensemble [2–4]. In that case, quite generically in the limit of large systems one observes a phase transition as the density of constraints is increased passing from a satisﬁable (SAT) phase where admissible conﬁgurations exist to an unsatisﬁable (UNSAT) phase, where the minimum number of unsatisﬁed constraints is larger than zero. This SAT/UNSAT threshold is clearly analogue to the jamming transition in soft-spheres [5, 6], that separates the low density region where spheres do not overlap from the high density one where no-overlap conﬁgurations do not exist. The jamming transition in spheres has highly universal features, with exponents that appear to be independent of the dimension and of the protocol used to produce the jammed conﬁgurations [1, 5–12]. While some exponents, like e.g. the ones relating the number of contacts or the pressure to the packing fraction in the UNSAT phase, take simple semi-integer values [11], other exponents, e.g. the ones describing the distributions of forces and the interparticle distances have non trivial, presumably non-rational values [13]. In order to understand the origin of this universality, it is important to study the SAT/UNSAT transition in diﬀerent CSP models. A crucial ingredient for jamming is the continuous nature of variables. Jamming is the point where the volume of the set of solutions to the problem continuously shrinks to zero, and in its vicinity scaling laws can emerge. To this aim, in [14] it was suggested to study the random perceptron problem [15] as a prototype of a CSP with continuous variables, generalized to a region of non-convex optimization. It was found that the nontrivial criticality and universality of the SAT/UNSAT transition point was associated to non-convexity. In the convex regime jamming is reached from a liquid, ergodic phase and it is hypostatic and non-critical. In the non-convex regime jamming is reached from a glassy phase, it is critical and in the same universality class of spheres. This led to the conjecture that it exists a large set of continuous CSP that belong to the same universality class. The perceptron emerges therefore as the simplest continuous CSP where glassy phenomena and jamming can be studied. This paradigm has been fruitfully applied to study the vibrational spectrum of glasses at low temperatures. In [16] the spectrum of the Hessian matrix of the energy minima in the UNSAT phase of the perceptron has been computed showing that it captures essential features of the vibrations of low temperature glasses. Furthermore, in [17] it has been shown how to study systematically the free energy landscape of the model using a Thouless-Anderson-Palmer approach [18] to obtain, in particular, the vibrational spectrum in the SAT phase. In [19], the avalanches characterizing the glassy phase around jamming have also been studied. Thanks to these studies, the non-convex perceptron now emerges as the simplest model that captures, at the mean ﬁeld level, all the most important features of the glass and jamming transitions. The scope of this paper is to give a detailed account of the space of solutions of the random perceptron model. In particular we carefully study the scaling behavior close to jamming, coming both from the SAT and UNSAT phase. The paper is organized as follows. In Sec. II we give a general formulation of continuous CSPs, we discuss the properties of the SAT-UNSAT transition, and we brieﬂy discuss the case of sphere packings to motivate some denominations that are used throughout the paper. In Sec. III we deﬁne the random perceptron model, we introduce the replica method to solve it, and we give the main equations needed for its study. In Sec. IV we discuss the zero-temperature phase diagram of the model, and in Sec. V we completely characterize the SAT/UNSAT transition (jamming) line and its critical properties. Finally, we present concluding remarks and perspectives for future work.
Credulous Users and Fake News: a Real Case Study on the Propagation in Twitter<|sep|>Pervasiveness and ease of use of Online Social Media (OSM), like Twitter and Facebook, have lead to new ways for people to keep up with news and for their propagation. Recent studies [1– 3] conﬁrm the growing trend of using digital media as the favourite source of information, especially among youngsters [4]. By routinely checking pages of interest and channels, people get informed eﬀortlessly. With OSM, publishers reach larger audience than with traditional mass media, like newspapers, radio, etc., and news are disseminated easily and at a faster rate. This has however brought concerns about the veracity of the news circulating on OSM. Fake news and mis/disinformation [5] are well known problems, against which both government [6], researchers [7] and social media administrators1,2 are struggling. Several approaches have been developed to counteract the spread of this phenomenon. Some of them aim at detecting bots [8], i.e., automated accounts interacting with human users; others use natural language processing (NLP) techniques [9] by analyzing the actual contents of messages. In this work, we look at fake news from a diﬀerent perspective, aiming to ﬁgure out the extent of relationships between fake news and credulous users [10]. The latter are humanoperated accounts with a relatively high percentage of bot-followees among the total number of their followees and are thus more exposed to bots’ malicious activities. The motivation for studying the phenomenon from this perspective is twofold, indeed credulous users: (i) may unconsciously contribute to fake news dissemination [11,12]; (ii) may be aﬀected by malicious activities that are more eﬀective when performed on a targeted audience [13,14]. Starting from a publicly available dataset of fake news which concerned with politics and gossips [15] published on Twitter, we have studied the involvements of credulous users in terms of the number of their tweets with fake news they posted and the number of credulous that have shared fakes. By jointly exploiting bot detection and credulous detection approaches [11], we have seen that: (i) credulous users produce more fake news tweets than not-credulous ones; (ii) credulous users publish less real news than fake ones and (iii) the extent to which credulous publish fake news depends on the topic. This ﬁndings are calling for tools that, by performing data streaming on credulous’ users actions, enables us to perform targeted fact-checking. A possible exploitation of this could be a system that, by attentioning credulous users performs on line data streaming by ”listening” their activities (e.g., tweets, retweets, replies, mentions, etc.) in real time. As soon as a credulous user publishes on his social dashboard, the tool could analyze the content, with, e.g., text mining and/or NLP techniques, the reliability of the source. Obviously, to reduce the amount of content to inspect, a key component of the system is the part concerned with credulous detection that reduces the number of human-operated accounts to investigate. This can be made more eﬃcient by setting up a priority inspection measure, based on the content production rate of credulous users, to ﬁrstly focuses on the more active ones. The rest of the paper is structured as follows: Section 2 considers related works; Section 3 describes the used dataset and the approach we have followed; Section 4 illustrates the experimental results; Section 5 discusses the main ﬁndings, and Section 6 concludes the paper.
EvoGAN: An Evolutionary Computation Assisted GAN<|sep|>Figure 1: The architecture of EvoGAN. The input of EvoGAN is an image and a sevenlength array. The big boxes above shows the evolution process of EA. According to the ﬁtness of every individual, the selection, mutation and crossover are conducted and breeding the next generation with a mixed gene. The middle part of the image shows the evaluating process. Every individual and the original image are input of the GAN and an image with expression is generated. A facial expression recognizer evaluates the expression of the generated image, and then the distance between the target expression and it is calculated, which is the ﬁnal ﬁtness score of the individual. During the process, the image generated by GAN is saved. When the ﬁtness of the population reaches our target, all the images would be the output. Generating images has a wide application in various ﬁelds including cinematography, graphic design, video games, medical imaging, virtual communication and ML research[1]. Generative Adversarial Networks(GAN) plays a minimax game between a generator network and a discriminator network[2]. The two networks are trained simultaneously, the generator tries to fake data while the discriminator tries to distinguish the fake ones and real ones. This process enables GAN to learn the data distribution of the dataset when the discriminator cannot tell which is real. However, the input of GAN is some random noise, and GAN map these random data with the target so that sometimes we may not be able to control the target accurately. Conditional GAN is developed to generate output conditioned on class labels. Deep Convolutional Generative Adversarial Network(DCGAN)[3] combines both convolutional neural networks (CNNs) and GAN, allowing GANs to generate photorealistic images. With these ideas, recent works have made impressive breakthroughs in generating controllable face images. Evolutionary Algorithm(EA) was ﬁrst proposed by [4] with the idea of natural evolution and was developed by [5]. EA has a great proﬁciency in solving optimization problems and has made great progress in economics, mathematics, geology, industry, social sciences, bioinformatics, etc. The ﬁttest survives. EA uses a ﬁtness function to evaluate the goodness of the solutions, and then, through selection, eliminates the bad solutions and retain good ones. With crossover and mutation, EA does not easily trap into locally optimal solutions and is able to search for the better ones. More importantly, EA does not require any mathematical features of the target function, meaning it does not come with any problems like gradient extinction or gradient explosion. GAN is powerful in learning the data distribution. However, the existing facial expression datasets only have class labels instead of an accurate number, making it hard to generate accurate compound expressions. There are generally two kinds of ways to generate facial expression images. One uses class labels like happy or sad as the input to generate the facial expressions and cannot generate compound expressions. The other imitates the facial movements from another image, which is completely different from generating an expression as the feature of an individual also contributes to the expression. Further more, based on gradient descent, these methods only have one ﬁxed output with one input. In fact, human can use different facial movements to express the same emotion. Since GAN[2] ﬁrst generated some black and white low resolution facial images, the capabilities of GANs have progressed quickly[6]. Besides the impressive improvements of the image quality, StarGAN[7] made a great progress changing the image-to-image translation type from single domain to multi-domain. GANimation[8] introduces FACS to facial expression synthesis and uses an array instead of a label as the input to make the synthesis much more ﬂexible. [1]presents a novel framework for training GANs with explicit control over generated images, which is extendable beyond the domain of human faces unlike the traditional CGAN methods. For some applications like face swapping or bringing old photos alive, these works have made great contributions. However, from the perspective of affective computing, we are not satisﬁed with the current status that the generated expressions are basic emotions and they do not have any variations. Moreover, from the perspective of evolutionary algorithms, researchers have been discovering more ﬁelds that are more related to application instead of mathematical problems. Quite a few works explored ways to both optimizing GANs using EAs and optimizing EAs using GANs[9, 10, 11, 12]. EvoGAN is also an attempt to apply evolutionary algorithms in some applications. In this paper, we aim to propose a framework using EA to search for the target within the data distribution learned by GAN. With this framework, we develop a model that generates compound expressions. In this model, we do not try to learn the data distribution of every expression but learn how to synthesize facial images with any facial movements, and then use EA to search the target expression. EA uses Facial Action Coding System (FACS), which describes facial expressions with the contractions of speciﬁc facial muscles, as the encoding. As shown in Figure 1, A facial expression generator is used to transfer the individuals in EA to an image. Then we use a facial expression recognizer(FER) to evaluate the expression of that image. Finally the distance between the expression of the synthesized image and the target expression will be the evaluation of the ﬁtness of the individual. Through selection, crossover and mutation, EA can converge the result on the target expression. As a group random searching algorithm, EA maintains a group of good solutions, keeping the diversity of the output images. Quantitative results show that our method provides better results compared with generating images with a linear composition of two expressions. Quantitative results shows the convergence of EA, proving the feasibility of our method. Moreover, our framework does not depend on any of the parts of it, which means with the further development of GAN in the ﬁeld of facial expression editing, our framework still works and can generate better images and with the further development of FER, our framework can generate expressions more accurately. The contributions of this paper are summarized as follows: • We propose EvoGAN that generates face images with compound expressions with any combination of basic expressions based on the framework.
Detection of a pair of prominent X-ray cavities in Abell 3847<|sep|>Over the last three decades, observations of many clusters of galaxies showed that the X-ray emission is sharply peaked at the position of the central bright galaxy. A decrease in the temperature and an increase in the gas density towards the centre have been observed, suggesting the existence of cooling ﬂows in the core of clusters (Fabian 1994). In the densest central region with high cooling rate, the hot gas atmosphere continues to loose energy in the form of radiation (mostly in X-rays). The inferred short radiative cooling time towards (or at) the centre (shorter than the age of the cluster) indicates that the gas located at the centre cools faster than the gas in the outer part of the cluster and hence gas ﬂows towards the core to maintain hydrostatic equilibrium (Fabian 1994; Blanton et al. 2001a). The excess gas in the central region of the cluster subsonically ﬂows towards the core and leads to the formation of cool clouds or stars (Fabian 1994). As a result, the cen tral massive galaxies continue to grow with time. The rate of cooling ﬂow in nearby clusters is estimated to be in the range of 10 100 M⊙ yr−1(Fabian 1994). However, in a recent study of massive cluster SPT-CLJ2344-4243, the rate of cooling ﬂow was found to be extremely high ∼3820 M⊙ yr−1(McDonald et al. 2012). Present day high resolution (spatial and spectral) observing facilities like Chandra and XMM-Newton have provided us with direct evidences of heating of the intra-cluster medium (ICM) in the cores of galaxy clusters through active galactic nuclei (AGN) feedback. Positive gradients in temperature proﬁles and short cooling time of the ICM in the core region are the indicative features of the cool-core clusters and have been conﬁrmed through several studies in the last decade (Fukazawa et al. 1994; Kaastra et al. 2004; Hlavacek-Larrondo et al. 2012; McNamara et al. 2000; Sanders & Fabian 2002; Rafferty et al. 2006; Fabian et al. 2006; Bˆırzan et al. 2008; Dong et al. 2010; Dunn et al. 2010; O’Sullivan et al. 2011; David et al. 2009; Gitti et al. 2012). From recent studies, it is now established that the majority of cooling ﬂow clusters host a powerful radio source associated with the central dominant galaxy. High resolution imaging and spectroscopic studies of cool core clusters using Chandra and XMM-Newton
Exact calculations of first-passage properties on the pseudofractal scale-free web<|sep|>Random walks on complex media have been extensively studied in the past several years [1–6]. How long does it take a random walker to reach a given absorbing domain (or a trap)? This time is known in the random walk literature as ﬁrst passage time (FPT) or trap time [7, 8]. Its importance lies in the fact that ﬁrst passage underlies many stochastic processes in which the event, such as a dinner date, a chemical reaction, the ﬁring of a neuron, or the triggering of a stock option, relies on a variable reaching a speciﬁed value for the ﬁrst time. Therefore FPTs have generated a considerable amount of work over the last decade [9–13]. A ﬁrst step consists in the analysis of the mean of this random variable, the mean ﬁrst-passage time (MFPT). Noting that the MFPTs are deeply aﬀected by the structural properties of the complex systems [3, 4], lots of endeavors have been devoted to derive the exact result of the MFPT to some special nodes and the MFPT averaged over all the starting nodes (also called mean trap time) on diﬀerent networks, such as Sierpinski gaskets [14, 15], Apollonian network [16], Koch networks [17], deterministic recursive trees [18–27] and some deterministic scale-free networks [28, 29]. There are also some eﬀorts devoted to exactly calculate the MFPT between any pair of nodes and the MFPT averaged over all the starting nodes or target nodes on some special trees [30, 31]. However, the MFPT, being just the ﬁrst moment of the probability distribution of the FPT, is not a suﬃcient measure to fully characterize the ﬁrst passage dynamics of a system. One should further analyze the ﬁrst passage probability (FPP), i.e., the probability that the random walker ﬁrst reaches the absorbing domain at time n (0 < n < ∞). In general, the FPP is deeply aﬀected by the topology of the underlying structure, by the location of the starting site, and by the location of the absorbing domain. Recently, Ref. [32] has presented a method to calculate the asymptotic form of the FPP between any pair of nodes in conﬁned media. The method can be applied to various models of disordered media, such as fractal networks and percolation clusters. However, when focusing on ﬁnite graphs and ﬁnite times, the exact calculation of the FPP is still diﬃcult. Up to now, explicit results for the FPP have been obtained in 1 dimension, eﬀectively 1 dimension geometries and some tree or comb structures, such as Cayley trees, hierarchical trees and hierarchical combs. [7]. But for structures with more complicated typology, to the best of our knowledge, explicit results for the FPP are far more elusive [33]. The pseudofractal scale-free web (PSFW) considered here is a deterministically growing network which was introduced to model scale-free networks with small-world eﬀect [34]. Due to its self-similarity, the PSFW is feasible for exact analytical and precise numerical investigations and, in fact, in the past several years, much eﬀort has been devoted to the study of its properties, such as degree distribution, degree correlation, clustering coefﬁcient [34, 35], diameter [35], average path length [36], and the number of spanning trees [37]. As for random walks on the PSFW, the MFPTs from any starting node to the hub nodes (i.e., nodes which have maximum degree) and the mean trap time to a given hub node (i.e., the MFPT to the hub node averaged over all the starting nodes) were obtained in Ref. [39]. However, the FPP is still unresolved. In this paper, we study discrete time random walks on the PSFW, aiming at deriving the rigorous solutions of the ﬁrst passage properties, such as the ﬁrst passage probability (FPP), the survival probability and the mean ﬁrst passage time (MFPT). First, we classify the nodes of the PSFW into diﬀerent “levels” (brieﬂy, a lower level corresponds to a larger degree, vide infra) and propose a method to derive the generating function of the FPP from an arbitrary starting node to a given absorbing domain. Then, exploiting the generating function tool, we can obtain the FPP, the survival probability, the mean and the variance of the ﬁrst passage time. In particular, we focus on some illustrative examples, FIG. 1. The construction of the PSFW with generation t = 0, 1, 2. At any iteration, each link is replaced by a triangle, in such a way that, for each link, a new node and two new links are introduced. As a consequence, at any iteration, the degree of the existing nodes is doubled. corresponding to the case that the absorbing domain is located at nodes of low levels. Remarkably, in all these cases, we evidenced that the variance of the ﬁrst passage time scales quadratically with the mean itself, similarly to what happens in ﬁnite-degree self-similar graphs (see e.g., [40]). This article is organized as follows. Section II presents the network model of the PSFW and some preliminary material on generation function. Section III introduces a convenient labelling of nodes, which shall be useful in the following calculations. Section IV proposes the general method to calculate the ﬁrst passage properties from an arbitrary node to the absorbing domain. Section V presents the exact and explicit results for the ﬁrst passage properties in the case the absorbing domain is located at a main hub. Section VI presents the exact and explicit results of the ﬁrst passage properties in the case there are two absorbing nodes located at two given highlyconnected nodes. Finally, Section VII contains conclusions and discussions. Technical and lengthy calculations are collected in the Appendices.
Flux: FunctionaL Updates for XML (extended report)<|sep|>XQuery is a World Wide Web Consortium (W3C) standard, typed, purely functional language intended as a high-level interface for querying XML databases. It is meant to play a role for XML databases analogous to that played by SQL for relational databases. The operational semantics and type system of XQuery 1.0 has been formalized (Draper et al. 2007), and the W3C recently endorsed the formal semantics as a recommendation, the most mature phase for W3C standards. Almost all useful databases change over time. The SQL standard describes a data manipulation language (DML), or, more brieﬂy, update language, which facilitates the most common patterns of changes to relational databases: insertion, deletion, and in-place modiﬁcation of rows, as well as addition or deletion of columns or tables. Despite the effectful nature of these operations, their semantics is still relatively clear and high-level. SQL updates are relatively inexpressive, but they are considered sufﬁcient for most situations, as witnessed by the fact that in many SQL databases, data can only be updated using SQL updates in transactions. Moreover, the presence of SQL updates does no damage to the purely functional nature of SQL queries: updates are syntactically distinct from queries, and the language design and transactional mechanisms ensure that aliasing difﬁculties cannot arise, even when an update changes the structure of the database (for example, if a column is added or removed from a table). The XQuery standard lacks update language features analogous to SQL’s DML. While XML querying has been the subject of a massive research and development effort, high-level XML update languages have received comparatively little attention. Many programming languages for transforming immutable XML trees have been studied, including XML stylesheets (XSLT (Clark 1999)), and XML programming languages such as XDuce, CDuce, Xtatic, or OCamlDuce (Hosoya and Pierce 2003; Benzaken et al. 2003; Gapeyev et al. 2006; Frisch 2006). However, these languages are not well-suited to specifying updates. Updates typically change a small part of the document and leave most of the data ﬁxed. To simulate this behavior by transforming immutable XML values one must explicitly describe how the transformation preserves unchanged parts of the the input. Such transformations are typically executed by building a new version of the document and then replacing the old one. This is inefﬁcient when most of the data is unchanged. Worse, XML databases may employ auxiliary data structures (such as indices) or invariants (such as validity or key constraints) which need to be maintained when an update occurs, and updating a database by deleting its old version and loading a new version forces indices and constraints to be re-evaluated for the whole database, rather than incrementally. Instead, therefore, several languages speciﬁcally tailored for updating XML data in-place have been proposed. While the earliest proposal, called XUpdate (Laux and Martin 2000), was relatively simple and has been widely adopted, it lacks ﬁrst-class conditional and looping constructs. These features have been incorporated into more recent proposals (Tatarinov et al. 2001; Sur et al. 2004; Ghelli et al. 2006; Chamberlin et al. 2006; Ghelli et al. 2007b). The W3C is also developing a standard XQuery Update Facility (Chamberlin et al. 2008). Although they have some advantages over XUpdate, we argue that these approaches all have signiﬁcant drawbacks, because they unwisely combine imperative update operations with XQuery’s purely-functional query expressions. We shall focus our critique on XQuery!, since it is representative of several other proposals, including the W3C’s XQuery Update Facility. A deﬁning principle of XQuery! is that update operations should be “fully compositional”, which Ghelli et al. (2006) take to mean that an update operation should be allowed anywhere in an XQuery expression. Thus, the atomic update operations such as insertion, deletion, replacement, renaming, etc. may all appear within XQuery’s query expressions. Node-identiﬁers can be used as mutable references. To avoid nondeterminism, XQuery! ﬁxes a left-to-right evaluation order and employs a two-phase semantics that ﬁrst collects updates into a pending update list by evaluating an expression without altering the data, and then performs all of
Resonant cyclotron scattering in pulsar magnetospheres and its application to isolated neutron stars<|sep|>Three kinds of pulsar-like objects have additionally and greatly boosted up our konwledge about pulsar magnetospheres. They are anomalous X-ray pulsars and soft gamma-ray repeaters (magnetars candidates), radio quiet isolated neutron stars (INSs) (the magniﬁcent seven) , and rotating radio transients (RRATs). Figure 1 shows their positions on the P − ˙P diagram. Our conventional picture of pulsar magnetospheres is provided by e.g. Goldreich & Julian (1969), Ruderman & Sutherland (1975), and Cheng et al. (1986) (for a recent review, see Kaspi et al. 2006), which is mainly about the open ﬁeld line regions (OFLRs). Few people begin to realize that there could be interesting physics in the closed ﬁeld line regions (CFLRs) of pulsar magnetospheres. For magnetars, it is proposed that there is strong and twisted magnetic ﬁeld around the central star (Thompson et al. 2002; Lyutikov & Gavriil 2006). INSs are thought to be dead neutron stars, which provide a clear specimen for magnetospheric and cooling studies (Kaspi et al. 2006; Tong & Peng 2007; Tong et al. 2008). For RRATs, recent modeling also indicates interesting physics in CFLRs (Luo & Melrose 2007). The most direct evidence comes from the observations of the double pulsar system PSR J0737-3039A/B, and there could be also signatures of interesting physics in CFLRs of normal pulsars (Lyutikov 2008). The interesting physics in pulsar CFLRs are mainly related to the plasmas there. Roughly speaking, the electron density in magnetar CFLRs is about 4-5 orders higher than the Goldreich-Julian density (Rea et al. 2008). In the case of RRATs, Luo & Melrose (2007) have proposed an idea of “pulsar
Composite system in noncommutative space and the equivalence principle<|sep|>Recently, noncommutativity has received much attention owing to the development of String Theory [1, 2] and Quantum Gravity [3]. The idea that space might have a noncommutative structure has a long history. It was proposed by Heisenberg and was formalized by Snyder [4]. The noncommutative space can be realized as a space where the coordinate operators satisfy the following commutation relations where θij is a constant antisymmetric object. In the classical limit ¯h → 0 the quantummechanical commutator is replaced by the Poisson bracket It is important to note that a charged and massive particle in a strong magnetic ﬁeld B pointing in the Z direction moves in a noncommutative space. The commutation relation for the coordinates of a particle moving in the (X,Y) plane is given by here e is the charge of the particle and c is the speed of light [5]. Many physical problems have been studied in the framework of noncommutative quantum mechanics and noncommutative classical mechanics. Some of the ﬁrst articles on quantum mechanics with noncommutativity of canonical type are [6, 7, 8, 9]. Formal aspects of noncommutative quantum mechanics are addressed in [10, 11]. Neutrons in a gravitational ﬁeld with noncommutativity are considered in [12]. Interesting eﬀects arise when one considers noncommutativity in the context of quantum cosmology and black hole physics [13, 14, 15]. The Landau problem [16, 17, 18, 19, 20], harmonic oscillator [21, 22, 23, 24], two-dimensional system in central potential [6], classical particle in a gravitational potential [25, 26], classical systems with various potentials [27] are studied. Note, however, that it is important to consider many-particle problem in order to analyze the properties of a wide class of physical systems in noncommutative space. The classical problem of many particles in noncommutative space-time was examined in [28]. The authors considered two examples of many-particle systems, namely the set of N interacting harmonic oscillators and the system of N particles moving in the gravitational ﬁeld. The corresponding Newton equation for each particle in these systems was provided. In [29] the two-body system of particles interacting through the harmonic oscillator potential was considered on a noncommutative plane. The authors implemented the noncommutativity through deﬁning a new set of commutating coordinates and got the θ-dependent Hamiltonian in usual commutative plane. The coordinates of the center-of-mass position and relative motion, the total momentum and the relative momentum were introduced in the traditional way. Therefore, the authors rewrote the Hamiltonian as a sum of the freely moving part and a θdependent bounded term and derived the partition function of a two-body system of classical noncommutative harmonic oscillator. The problems of noncommutative multiparticle quantum mechanics are examined in [30]. The authors considered the case when the particles of opposite charges feel opposite noncommutativity. The coordinates of the center-of-mass and relative motion were introduced. It was shown that the magnitude of the center-of-mass coordinates noncommutativity is never large then the parameter of noncommutativity for elementary particle. In [31] a system of two quantum particles was considered in the context of noncommutative quantum mechanics, characterized by noncommutativity between the coordinates and momentum noncommutativity. The noncommutative correction to the energy spectrum of two-particle system was found. In [32] the system of two charged quantum particles was considered in a space with coordinates noncommutativity. The authors reduced the two-body problem to a one-body problem for the internal motion. The quantum model of many particles moving in twisted N-enlarged NewtonHooke space-time was proposed in [33]. The Schroedinger equation for arbitrary stationary potential was provided. As an example the author examined the system of N particles moving ”in” and interacting ”by” the Coulomb potential. In the case of Doubly Special Relativity the problem of composite system (so-called soccerball problem) was considered in [34, 35, 36]. This problem was also studied within the framework of relative locality in [37, 38, 39]. Composite system in deformed space with minimal length [ ˆX, ˆP] = i¯h(1 + β ˆP 2) was considered in [40]. The authors solved the two-body problem, studied the composite system made of N elementary particles, deﬁned an eﬀective deformation parameter and re-examined the estimation of the minimal length upper bound. In [41] the properties of the kinetic energy of a composite body were analyzed. The author considered the problem of violation of the equivalence principle and proposed the way to recover this principle in deformed space with minimal length. The violation of the equivalence principle is an important problem in noncommutative space. In [42] the authors examined the free-fall of a quantum particle in a uniform gravitational ﬁeld. It was argued that the equivalence principle extends to the realm of noncommutative quantum mechanics. One of the consequences of the twisted Poincare symmetry was investigated in [43]. In this context the author concluded that one can expect that the equivalence principle is not violated in the noncommutative space-time. However, in [44, 45, 46], the authors argued that noncommutativity leads to an apparent violation of the equivalence principle. In this Letter the two-particle and N-particle systems are examined in noncommutative space. We consider the general case when the diﬀerent particles satisfy noncommutative algebra with diﬀerent parameters of noncommutativity. Every macroscopic body consist of elementary particles which feel the eﬀect of noncommutativity with diﬀerent parameters. So, there is a problem of describing the motion of the center-of-mass of macroscopic body in noncommutative space. In order to solve this problem the total momentum is introduced as an integral of motion in noncommutative space and the center-of-mass position is found as its conjugate variable. We conclude that the coordinates of the center-of-mass satisfy noncommutative algebra with eﬀective parameter of noncommutativity. Taking into account this conclusion the condition to recover the equivalence principle in noncommutative space is proposed. Moreover, the same condition is derived from the independence of kinetic energy on the composition. This Letter is organized as follows. In Section 2 the two-body problem is solved. More general case of composite system made of N elementary particles in noncommutative space is studied in Section 3. The motion of a body in gravitational ﬁeld and the equivalence principle are considered in Section 4. The properties of the kinetic energy in noncommutative space are studied in Section 5. In Section 6 the upper bound of the parameter of noncommutativity is re-examined.
Ghost-vibrational resonance<|sep|>Response of nonlinear systems to a harmonic force with a single frequency has been investigated in detail. A non-monotonic variation of the amplitude of the response occurs [1, 2], in a typical nonlinear system when the frequency of the driving force is varied. In particular, the oscillation amplitude of the system output increases with the increase in the frequency of the external force, it reaches a maximum at a particular frequency and then it decreases with further increase in the frequency. This resonance phenomenon is widespread and has been utilized in several devices. In bistable and multistable systems when the amplitude of the external periodic force is below a threshold (that is, there is no switching motion between the coexisting stable states), then a transition between the coexisting states can be induced by a weak noise. At an appropriate optimum noise intensity, almost a periodic switching between coexisting states occurs resulting in a maximum system response. This noise-induced resonance phenomenon is termed as stochastic resonance [3, 4]. Resonance can be realized when the noise term is replaced by a high-frequency periodic force and is called vibrational resonance [5, 6]. Furthermore, it is possible to generate a chaotic signal that mimics the probability distribution of the Gaussian white noise. Such a signal can also give rise to a resonant eﬀect analogous to the noise-induced resonance and is called chaotic resonance [7]. In all the above resonance phenomena, in absence of a resonance inducing source, the system is driven by a weak harmonic force with a single frequency. There are signals with multiple frequencies. Examples include human speech, musical tones and square-waves. Design of an approximate multi-frequency signal is very important in minimizing the nonlinear distortion in the multi-frequency system identiﬁcation methods [8, 9]. Chialvo et al. [10, 11] investigated the response of a threshold device to an input signal containing several frequencies in the presence of noise. When the frequencies of the driving force are of a higher-order of a certain fundamental frequency, then the system is found to show a maximum response at the missing fundamental frequency at an optimum noise intensity. This fundamental frequency, which is absent in the input signal, detected by the device is called ghost-frequency and the underlying resonance phenomenon is termed as ghost-stochastic resonance [10, 11]. When the input signal is set into an anharmonic by introducing a same frequency shift to all the harmonic terms, the system is found to show a resonance at a certain shifted frequency. This ghost resonance phenomenon can be used to explain the missing fundamental illusion in which a third lower pitched tone is often heard when two tones occur together [11]. The occurrence of a ghost resonance induced by noise has been analysed mostly in excitable systems. For example, it was found in the sudden dropouts exhibited by a semiconductor laser [12], two laser systems coupled bidirectionally [13], vertical-cavity surface emitting lasers [14], monostable Schmitt trigger electronic circuit [15], an excitable Chua’s circuit [16], a chaotic Chua’s circuit [17] and a system of n-coupled neurons [18]. Subharmonic resonance behaviour in a nonlinear system with a multi-frequency force containing the fundamental frequency in the absence of a high-frequency input signal is studied in [19]. Because nonlinear systems with double-well and multi-well potentials are wide-spread it is foremost important to investigate the response of these systems to the multi-frequency force and analyse the occurrence of ghost resonance in them and also with sources other than external noise. Motivated by the above considerations, in the present work, we explore the possibility of a ghost resonance induced by a high-frequency deterministic force rather than a noise. We consider the Duﬃng oscillator driven by multi-frequency force F(t) and a high-frequency force g cos Ωt. The multi-frequency force F(t) is given by with k ≥ 2 and Ω ≫ ωn(= (k + n − 1)ω0). We begin our analysis with n = 2, k = 2 and ∆ω0 = 0. We show the occurrence of a resonance at the fundamental frequency ω0 missing in the input signal F(t). The value of g at which the resonance at the frequency ω0 occurs, increases monotonically while the value of the response amplitude Q(ω0) at resonance decreases with ω0. Interestingly, the case of n = 2 by applying a theoretical method, we are able to obtain an approximate analytical expression for the response amplitudes Q(ωi), i = 0, 1, 2. Theoretical results are in good agreement with the numerical predictions. We study the inﬂuence of the number of periodic forces n, the parameters k and g and the frequency shift ∆ω0 on Q(ω0). For values of k > 2 or ∆ω0 ̸= 0, the response amplitude Q(ω0) becomes 0 when the oscillation center of the orbit is at the origin and this happens for g values above a certain critical value. Next, we consider a network of unidirectionally coupled N-Duﬃng oscillators with the multi-frequency force and the high-frequency force applied to the ﬁrst oscillator only. The ﬁrst system is uncoupled. The coupling term is chosen to be linear. We denote Qi(ω0) as the response amplitude of the ith oscillator at the frequency ω0. For a coupling strength above a critical value, an undamped signal propagation, that is, QN(ω0) > Q1(ω0) occurs at the missing fundamental frequency, even in the absence of the high-frequency periodic force. Interestingly, in the undamped signal propagation case, the response amplitude increases with the unit number i and then becoming a constant. The saturation value of Q is found to be independent of the parameters k, n and ∆ω0 in F(t). Finally, we consider a network of unidirectionally coupled oscillators, where all the oscillators are driven by the external forces.
GRB 110530A: Peculiar Broad Bump and Delayed Plateau in Early Optical Afterglows<|sep|>It is generally believed that cosmic gamma-ray bursts (GRBs) are from ultra relativistic jets powered by newly-born black holes or pulsars during collapses of massive stars or mergers of compact stars (e.g., Colgate 1974, Paczynski 1986; Eichler et al. 1989; Narayan et al. 1992; Woosley 1993; MacFadyen & Woosley 1999; Zhang et al. 2003; see reviews by M´esz´aros 2002, 2006; Zhang & M´esz´aros 2004; Piran 2004; Woosley & Bloom 2006; Kumar & Zhang 2015). Their prompt gamma-ray emission may be from internal shocks in an erratic, unsteady, relativistic ﬁreball (e.g., Rees & M´esz´aros 1992; M´esz´aros & Rees 1993; Rees & M´esz´aros 1994), a dissipative photosphere (e.g., Beloborodov 2010; Vurm et al. 2011; Giannios 2008; Ioka 2010), or a Poynting-ﬂux-dominated outﬂow ( Zhang & Yan 2011 and reference therein). The broad band observations with the Fermi mission sharpen debating on the radiation mechanisms and the composition of the GRB jets (e.g., Abdo et al. 2009; Zhang et al. 2009, 2011; Zhang et al. 2013; Lyu et al. 2014). Long-lived afterglows in the X-ray, optical and radio bands following the prompt gammarays were discovered in the BeppoSAX mission era (van Paradijs et al. 2000 and references therein). They are well explained with the synchrotron emission from external shocks when GRB ﬁreballs propagate into the circumburst medium (e.g., M´esz´aros & Rees 1997; Sari et al. 1998). Afterglow observations was revolutionized by the Swift mission thanks to the promptly slewing and precisely localizing capacities of its X-ray telescope (XRT) (Gehrels et al. 2004; Burrows et al. 2005b). The number of GRBs that have optical and X-ray afterglow detections rapidly increases and the sample of well-sampled lightcurves are also growing quickly (Gehrels et al. 2009; Kann et al. 2010). Excluding the tail emission of the prompt gamma-rays and erratic ﬂares from the canonical XRT lightcurves (Nousek et al. 2006; Zhang et al. 2006), the X-ray afterglow lightcurves are generally consistent with the predictions of the external shock model by adding an extra energy injection (Zhang et al. 2006; Liang et al. 2007). Statistical analysis of the optical afterglow lightcurves observed from Feb, 1997 to Nov., 2011 shows that about 1/3 of the optical afterglow lightcurves well agree with the prediction of the external shock model in the thin shell case, and another 1/3 may require an extra energy injection to the external shocked medium (Li et al. 2012; Liang et al. 2013). An extensive analysis of the X-ray and optical afterglow data by Wang et al. (2015) shows that the standard external shock models are good for explaining the data by elaborately considering various eﬀects, such as long-lasting reverse shock, structured jets, circumburst medium density proﬁle. Well-sampled multi-wavelength lightcurves in broad temporal coverage from very early to late epochs are valuable for modeling the lightcurves and revealing the properties of the GRB jets and even the GRB central engines as well as the progenitors (e.g., Xin et al. 2016). This paper reports our very early optical observations for GRB 110530A and detailed modeling for the optical and X-ray afterglow lightcurves. Observations and data reductions are reported in §2. We present joint temporal and spectral analysis for the optical and X-ray afterglow data in §3, and present our modeling results in §4. Discussion on the possible implications for its jet composition and progenitor star are available in §5. Conclusions are presented in §6. Throughout, the notation Qn = Q/10n in cgs units are adopted.
Deep Learning for Procedural Content Generation<|sep|>Deep learning has powered a remarkable range of inventions in content production in recent years, including new methods for generating audio, images, 3D objects, network layouts, and other content types across a range of domains. It stands to reason that many of these inventions would be applicable to games. In particular, modern video games require large quantities of high-deﬁnition media, which could potentially be generated through deep learning approaches. For example, promising recent methods for generating photo-realistic faces could be used for character creation in games. At the same time, video games have a long tradition of procedural content generation (PCG) [132], where some forms of game content have been generated algorithmically for a long time; the history of digital PCG in games stretches back four decades. In the last decade and a half, we have additionally seen a research community spring up around challenges posed by game content generation [16, 93, 112, 129, 133, 134, 148]. This research community has applied methods from core computer science, such as grammar expansion [22]; AI, such as constraint solving [115] and evolutionary computation [7, 133]; and graphics, such as fractal noise [24].
Demonstrating Delay-based Reservoir Computing Using a Compact Photonic Integrated Chip<|sep|>The concept of reservoir computing (RC), a paradigm within neuromorphic computing, oﬀers a framework to exploit the transient dynamics within a recurrent neural network for performing useful computation. It has been demonstrated to have state-of-the-art performance for a range of tasks that are notoriously hard to solve by algorithmic approaches, e.g., speech and pattern recognition and nonlinear control. RC simpliﬁes the training procedure for recurrent neural networks, by keeping the neural network ﬁxed and relying on a trained output layer that consists of a linear combination of network states to generate the desired output signals. Hence, during training only the connections from the network to the output layer are trained. The ﬁxed network is called the reservoir and can actually be any dynamical system with a high dimensional state space. Due to this simpliﬁcation, RC rekindled neuromorphic computing activities in photonics. Today, multiple photonic RC systems can provide a practical yet powerful hardware substrate for neuromorphic computing [1]. Some examples include a network of semiconductor optical ampliﬁers [2, 3], an integrated passive silicon circuit forming a very complex and random interferometer, with nonlinearity introduced in the readout stage [4] and a semiconductor laser network based on diﬀractive coupling [5]. The concept of delay-based RC, using only a single nonlinear node with delayed feedback, was introduced some years ago by Appeltant et al. [6] as a means of minimizing the expected hardware complexity in photonic systems. The ﬁrst working prototype was developed in electronics in 2011 by Appeltant et al. [6] and several performant optical systems followed quickly after that [7,8], one of which is based on a semiconductor laser with external optical feedback [9]. Delay-based RC oﬀers a simple technological route to implement photonic neuromorphic computation. Its operation boils down to a time-multiplexing with the delay arising from propagation in the external feedback loop, limiting the resulting processing speed. As most optical setups end up to be bulky employing long ﬁber loops or free-space optics, the processing speeds are limited in the range of kSa/s to tens of MSa/s [8,9]. To increase the processing speed of delay-based reservoir computing using a semiconductor laser with delayed optical feedback, Fig. 1. Schematic depiction of our InP-based photonic integrated circuit (PIC). The PIC consists of a laser structure followed by a delay line of 5.4 cm. DBR: Distributed Bragg Reﬂector, SOA: Semiconductor Optical Ampliﬁer, PM: Phase Modulator. one can integrate the laser and the delay both on the same photonic chip. In this way, by using a waveguide structure with a compact footprint, an external cavity structure can be implemented which is small enough to reach high processing speeds, yet still long enough to have suﬃcient dimensionality for good computing performance. In the long term, this integrated approach will lead to a robust and low-cost design. Recently, Takano et al. [10] have presented a photonic integrated circuit (PIC) consisting of a distributed-feedback semiconductor laser, a semiconductor optical ampliﬁer (SOA), a phase modulator, a short passive waveguide, and an external mirror for optical feedback. The external cavity length in this system reached 10.6mm, corresponding to a round-trip delay time of 254 ps. However, only six virtual nodes could be stored within the delay line with node-spacings of 40 ps, not enough for good computational performance. This necessitated the authors to use masks with duration of multiple delay times, which slows down the computation speed. Our goal is to show that a delay-based reservoir computer can be built using an indium-phosphide PIC, that combines active and passive elements and is built on the JePPIX platform [11]. The PIC integrates a semiconductor laser with an external cavity of 5.4 cm, which corresponds to a round trip time of 1170 ps. This allows for 23 nodes and a processing speed of 0.87 GSa/s. The longer waveguide based external cavity will also have more loss associated to it. Therefore, we will address in this work the question if ampliﬁcation in the external cavity is needed or not. Contrary to other works [9,10,12], the semiconductor laser itself will be driven far above solitary lasing threshold to beneﬁt from a better signal to noise ratio in the read-out, as well as faster internal dynamics. Finally, we will introduce post-processing schemes that do not penalize computational speed. In Section II, we describe the experimental setup as well as the pre- and post-processing of data. In section III we present and discuss the results for the diﬀerent post-processing schemes. We also discuss the linear and nonlinear memory capacity of the system in section III.
Imprints of feedback in young gasless clusters?<|sep|>The conversion of molecular gas to stars results, at least initially, in stars geometrically grouped into clusters and associations (e.g. Lada & Lada 2003; Lada 2010; Gieles & Portegies Zwart 2011). The transition from embedded clusters whose dynamical behaviour is dominated by gas, to systems devoid of gas, is still not well understood. Stellar feedback, particularly in the form of photoionizing radiation, winds and supernovae from OB-type stars, but also photodissociating radiation and jets from lowand intermediate-mass stars, are strong candidates for the agency of gas expulsion and much theoretical work has been devoted to their study (e.g. Whitworth 1979; Matzner 2002; Li & Nakamura 2006; Krumholz, Klein & McKee 2012). Although the dynamics of gas-free clusters are, by deﬁnition, governed by gravity (including tidal interactions with other bodies) and stellar evolution, the prior hydrodynamic phase and the transition from that phase to a purely gravitational system are of crucial importance, since together they determine the initial stellar masses, positions and velocities for the subsequent dynamics-dominated evolution of the cluster. In a recent series of papers, Dale, Ercolano & Bonnell (2012) and Dale, Ercolano & Bonnell (2013) modelled the formation of massive clusters with and without the inﬂuence of feedback. These authors studied the inﬂuence of mass–loss driven by photoionizing radiation from massive stars (m > 20 M⊙) on the dynamical evolution of their model clouds by comparing each simulation with a corresponding control run in which feedback from the massive stars was switched oﬀ. The simulations were evolved for ∼3 Myr, during which time the clusters generally remained substructured, although differences between the runs with and without feedback were generally limited to the respective initial mass functions, which were somewhat more top-heavy for the simulations where star formation was not regulated by feedback. Recently, N-body simulations of cluster evolution have been used to demonstrate that the initial conditions of star formation can in some cases be discerned from a process of ‘reverse engineering’ – comparing snapshots from the simulations to observed clusters and re-calibrating the ini
$(O,G)$-granular variable precision fuzzy rough sets based on overlap and grouping functions<|sep|>1.1. Brief review of fuzzy rough sets Rough set, as a way to portray uncertainty problems, was originally proposed by Polish mathematician Pawlak in 1982 [32, 33], and it has been extensively developed in the ﬁelds of knowledge discovery [39] and data mining. Rough set theory uses indistinguishable relations to divide the knowledge of research domain, thus forming a system of knowledge representation that approximates an arbitrary subset of the universe by deﬁning upper and lower approximation operators [9]. As a generalization of the classical theory, Zadeh introduced the fuzzy set theory [47] in 1965, where objects can be owned by diﬀerent sets with diﬀerent membership functions. Since rough sets are deﬁned based on equivalence relations, they are mainly used to process qualitative (discrete) data [25], and there are greater restrictions on the processing of real-valued data sets in the database. In particular, fuzzy sets can solve this problem by dealing with fuzzy concepts. Therefore, complementing the features of rough sets and fuzzy sets with each other constitutes a new research hotspot. In 1990, Dubois and Prade [18] described fuzzy rough sets, which is the combination of two uncertainty models, and then extended the fundamental properties to fuzzy rough sets. As another innovation of rough set, Ziarko presented the variable precision rough set [48], which mainly solved the classiﬁcation problem of uncertain and inaccurate information with an eﬀective error-tolerance competence. More details about variable precision rough sets can refer to [30, 31, 49]. In addition, since the upper and lower approximation operators of fuzzy rough sets are deﬁned according to membership functions, while rough sets are described based on the union of some sets, there exists signiﬁcant diﬀerence in the granular structure of the two. To overcome this limitation, Chen et al. [10] explored the concept and related properties of granular fuzzy sets based on fuzzy similarity relations. Furthermore, from the perspective of granular computing, the granular fuzzy set is used to characterize the granular structure of upper and lower approximations. However, the above model cannot tolerate even small errors and is not suited to handle uncertain information well. Some extended fuzzy rough sets are applied to solve these problem, but some studies still have problems in dealing with mislabeled samples (see, e.g., [23, 24, 52]), and others have only considered the relative error cases. [20, 30]). To ﬁll these loopholes, the model of variable precision (θ, σ)-fuzzy rough sets over fuzzy granules were presented by Yao et al. [46]. However, the above model is based on fuzzy ∗-similarity relation, satisfying reﬂexivity, symmetry and ∗-transitivity, which is too strict to facilitate generalized conclusions. Thus, Wang and Hu [42] studied the GVPFRSs and then the equivalent expressions of the approximation operators are given with fuzzy implications and co-implications over arbitrary fuzzy relations. Subsequently, they gave the properties of GVPFRSs on diﬀerent fuzzy relations. In addition, compared with unit interval, the complete lattice has a wider structure, so Qiao and Hu expanded the content of [42] and [46], and further discussed the concept of granular variable precision L-fuzzy rough sets based on residuated lattices. In fact, both [35] and [42] are based on t-norm (t-conorm), which satisfying associative, commutative, increasing in each argument and has a identity element 1 (resp. 0). However, there are various applications [7, 8, 21] in which the associativity property of the t-norm (resp. t-conorm) is not necessary, such as classiﬁcation problems, face recognition and image processing. 1.2. Brief analysis of overlap and grouping functions Bustince et al. described the axiomatic deﬁnitions of overlap and grouping functions [6, 8], which stem from some practical problems in image processing and classiﬁcation. In fact, in some situations, the associativity of t-norm and t-conorm usually does not work. Therefore, as two types of noncombining fuzzy logic connectives, overlap and grouping functions have made rapid development in theoretical research and practical applications. In theory, there exists many studies involving overlap and grouping functions, such as crucial properties [3, 13, 43], corresponding implications [14, 15, 41], additive generator pairs [16], interval overlap functions and grouping functions [4, 36], distributive equations [27, 50, 51] and concept extensions [17, 53]. From an application point of view, overlap and grouping functions can ﬁnd interesting applications in classiﬁcations [28, 34], image processing [5, 7, 26], fuzzy community detection problems [5] and decision making [8, 19]. In [1], the authors have pointed out that O : [0, 1]2 −→ [0, 1] is an associative overlap function (resp. grouping function) if and only if O is a continuous and positive t-norm (resp. t-conorm). On the other side, we note that overlap and grouping functions can be considered as another extension of classical logical connective ∧ and ∨ on the unit interval, which diﬀer from t-norms and t-conorms. Hence, we can use them to replace the classical logical operators and then deﬁne the granular variable precision approximation operators. Meanwhile, from the application aspect, the study of fuzzy rough sets based on overlap and grouping functions has a pivotal role in practical problems. Therefore, based on aforementioned consideration, and as a supplement of the GVPFRSs [42], this paper continues the studies in (O,G)-GVPFRSs based on overlap and grouping functions instead of t-norm and t-conorm. It should be pointed out that the present paper further enriches the application of overlap and grouping functions. In addition, it makes the research on fuzzy rough sets more complete. The rest of this paper is arranged as follows. Section 2 enumerates some fundamental concepts that are necessary to understand this paper. Section 3 proposes the (O,G)-GVPFRSs with general fuzzy relations and gives an alternative expression for eﬃcient computation of the approximation operators. Furthermore, we study the (O,G)-GVPFRSs under the conditions of crisp relations and crisp sets and draw the corresponding conclusions. Section 4 represents the (O,G)-GVPFRSs on diverse fuzzy relations. In particular, some special conclusions are given under some additional conditions. Section 5, conclusions on our research are given.
Bridging the gap between paired and unpaired medical image translation<|sep|>Each medical imaging modality captures speciﬁc characteristics of the patient. In many medical applications, complimentary information from multiple modalities can be combined for better diagnosis or treatment. However, due to limited time, cost and patient safety, not all desired modalities are captured for every patient. Medical image translation can play a vital role in many of these scenarios as it can be used to generate non-critical (i.e. the ones which are not needed for ﬁne pattern matching) missing modalities. One such clinical application is in MR-based radiotherapy, where MR images are used for delineating targets (e.g. tumors) and organs-at-risk (OARs). However, the clinicians still need CT scans for calculating the dose delivered to OARs and targets. Since this CT scan is used to compute dose delivered to various structures, the alignment of the patient
Enhancing the German Transmission Grid Through Dynamic Line Rating<|sep|>The deployment of new renewable energy sources far from load centers is leading to an increasing need for grid capacity. A notable example can be found in Germany, where most of the wind energy is generated in the North while there is signiﬁcant industrial demand in the South [1]. The current federal government plans to add 70 GW wind power capacity until 2030, at the same time the transmission line capacities need to be expanded [2]. However, in the last decade only few grid expansion projects have been realized. Most of these are delayed due to administrative problems and protest activities [3], [4]. This trend will likely exacerbate transmission system congestion, which may lead to more grid instability and curtailment [5]. There is therefore a strong incentive to make better use of the existing grid infrastructure. To overcome the mismatch between rapid installation of renewable generation and slow installation of transmission lines, several, complementing measures can be taken that must be reconciled: (1) large scale implementation of storage facilities to ﬂatten power feed-ins and power demands [6]; (2) usage of alternative energy carrier networks like hydrogen to relieve the electricity grid [7]; (3) leveraging the existing grid infrastructure to increase the operating capacity. In the following, we will focus on the last point, which in comparison to (1) and (2) stands out through a fast, low-cost and noninvasive implementation [8], [9]. Traditionally, transmission line capacities are calculated assuming unfavorable, static weather conditions such as 40◦C ambient temperature and 0.6 m/s wind speed [4]. This is referred to as Static Line Rating (SLR). By design, SLR underestimates the capacity of a transmission line and, when implemented in practice, leads to an underutilization of the transmission infrastructure. In contrast, Dynamic Line Rating (DLR) calculates the line capacity taking into account the prevailing weather conditions. Cold weather and wind cool overheated transmission lines, enabling the thermal rating to be raised. This results in key beneﬁts in cost-efﬁciency, congestion reduction and better wind power integration [5]. Transmission System Operators often adjust line ratings based on the current season but do not exploit the full potential of DLR monitoring local cooling effects [10]. The literature provides case studies with DLR applied in small scale Energy System Optimization Models (ESOMs) [11]–[13]. However, it is lacking of an extensive evaluation and assessment of DLR in higher scale ESOMs. This paper addresses this gap. We present the ﬁrst capacity optimization of the German power system with DLR being subject to high CO2 emission targets. The article is structured as follows. Section II describes the methodology of the DLR implementation (II-A) and the underlying power system modelling (II-B). In Section III, we present two case studies for the German power system used throughout the paper. Section IV comprises the main results of the analysis with limitations outlined in Section V. A conclusion is presented in Section VI.
Atmospheric gas dynamics in the Perseus cluster observed with Hitomi<|sep|>example, a LOS bulk velocity of 500 km s−1 produces a Doppler shift of 11 eV for the Fe XXV Heα
Bounded Cohomology of Groups acting on Cantor sets<|sep|>1.1. Motivation and Origins of Bounded Cohomology . . . . . . . . . . . . . 1 1.3. Present work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3.1. Homeomorphisms of the Cantor set . . . . . . . . . . . . . . . . 5 1.3.2. Thompson’s Group V . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4. Organisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.5. Notations and Conventions . . . . . . . . . . . . . . . . . . . . . . . . . 7
Protein Function Prediction Based on Kernel Logistic Regression with 2-order Graphic Neighbor Information<|sep|>Proteins are composed of sequences of amino acids and participate in nearly every  vital process within cells. With the development of gene chip technique, the  interaction of proteins can be correctly measured out, however, many of the functions  of proteins are still unknown. Predicting the protein function helps not only to  discover the protein’s unknown function, but also to detect the organism disease.  ( Shalgi, st al. 2007; Varnholt st al. 2008; Hu, st al. 2008 ) As novel feature extraction  and classifiers development are two key techniques in protein–protein interaction (PPI)  function prediction, we propose a 2-order graphic neighbor feature extraction method  based on graph theory and apply the RBF kernel logistic regression to protein–protein  interaction prediction in this paper.   To extract the feature of PPI data and predict the protein function, Schwikowski, et al. (2000) proposed to assign the possible functions to a protein based on the known  functions of its interacting partners. Hishigaki, et al. (2001) proposed the chi–square  method for n-neighbor protein interactions. Deng, et al.(2003) developed a Markov  random field (MRF) method for protein function prediction using multiple PPI data.  Lanckriet, et al. (2004) proposed a diffusion kernel based support vector machine  (SVM) approach for predicting protein functions on a protein interaction network  with 5-types data, the prediction accuracy of the SVM approach is higher than that  ofthe MRF approach. Lee, et al.(2006) developed a diffusion kernel logistic  regression (DKLR) method for protein interaction networks, which incorporates all  neighbors of proteins in the network. The result showed that the DKLR approach of  incorporating all protein neighbors significantly improved the accuracy of protein  function predictions over the MRF model. And the prediction accuracy was  comparable to another protein function classifier based on SVM with a diffusion  kernel. Gao et al. (2007) developed a method to PPI data by introducing several  methods to filter the neighbors in protein interaction networks for a protein of  unknown functions. However, DKLR method can be expressed as a features  extraction method and be calculated with traditional LR classifier for protein  interaction prediction, and it has the different learning algorithm with the traditional  KLR (Zhu, et al. 2002; Cawlay, et al. 2005; Birkenes, , et al. 2007; Tenenhaus, et al.  2007; Jaakkola, et al. 1999; Keerthi, et al. 2005; Karmakers, et al. 2007; Muller , et al.  2001 ).   From the pattern recognition point of view, the description of PPI information,  namely PPI feature extraction and selection, takes crucial role in bio-informatics  analysis (Saeys et al. 2007; Liu et al. 2009 ). To improve the prediction accuracy, we  propose a 2-order graphic neighbor information extraction method for PPI prediction,  which is different from Lee, et al.(2006) . We investigate the prediction performances  of four classifiers, LR, DKLR, PKLR and RBF KLR on the 1-order graphic neighbor  feature sets and 2-order graphic neighbor feature sets. Furthermore, the chi-square test  method is also employed for multiple category feature combination. Since the first top  chi-square value of one function may not be itself, we propose a new L-top chi-square  feature combination method in section 4.3.3 and section 4.3.4 to overcome the  insufficiency, that the own function feature vector should be list No.1and the feature  vectors calculated by L-top chi-square values are attached behind to construct the new  combination feature. The experimental results of the average overall percentage  criterion on MIPS data ( Lu, et al. 2006; Ashburner, et al. 2000; Mewes, et al. 2002 )  of YPD demonstrate the effectiveness of 2-order neighbor graphic information and  the performance of RBF KLR model.   The rest of the paper is organized as follows. Logistic regression and Kernel Logistic  regression for a single function are reviewed in Section 2., and the diffusion kernel  logistic model is also reviewed in Section 2. Kernel logistic regression with steepest  descend Newton-Raphson method, 2-order graphic neighbor feature extraction  method and feature combination with chi-square test are proposed and discussed in  Section 3. The experimental results on MIPS database of YPD are given in Section 4.  Finally, the conclusion is given in Section 5.
Learning Permutations with Sinkhorn Policy Gradient<|sep|>Learning to solve combinatorial optimization problems from data has applications in many ﬁelds. As a motivating example, consider planar minimum/maximum weight matching. Given a bipartite graph with vertices represented as points in the plane, the objective is to ﬁnd a permutation matrix that matches vertices such that the sum of Euclidean distances amongst all match pairs is minimized/maximized. Other related combinatorial problems include graph matching [4, 27], ranking [5, 1], and data association in multi-object tracking [37, 23, 24]. Data association can be cast as a minimum weight matching problem; given a sequence of raw images containing object detections, the objective is to ﬁnd the optimal matching between detections in adjacent images to form object tracks. In this paper, we propose a data-driven algorithm for the task of learning permutations based on a policy gradient method from the reinforcement learning (RL) literature. Data-driven approaches to solving combinatorial problems involve training a model on a dataset of problem instances drawn from a distribution, so that the model is able to score highly with respect to the task-speciﬁc objective on a test set of instances drawn from the same distribution. Models trained with supervised learning can perform well by relying on traditional loss functions such as mean squared error [34, 35], crossentropy error [27], or mean squared error augmented with task-speciﬁc objectives [24]. However, obtaining large quantities of solved problem instances to build a labeled dataset is not always feasible. Recent empirical results indicate that a task-speciﬁc objective can be used as the sole learning signal. Most notably, [2] uses an encoder and decoder with attention, trained with REINFORCE [36], to solve combinatorial problems that have a sequential nature (e.g., the Euclidean TSP). This was recently extended to take advantage of the graph structure of the inputs by replacing the encoder with a graph attention layer [16]. A DQN [25]-inspired algorithm proposed in [8] also learns graph embeddings of problem instances, for the Euclidean TSP, Maximum Cut, and Minimum Vertex Cover problems. We have developed SPG, a policy gradient method designed for the class of combinatorial problems involving permutations. Indeed, the action space of a policy trained with SPG is the discrete set of N × N permutation matrices (PN). In contrast to similar learning-based approaches [2, 8, 16], SPG is not restricted to learning policies that emulate a greedy heuristic. We demonstrate that SPG is able to learn to sort integers, produce near-optimal matchings for maximum weight matching (MWM), and ﬁnd tours of competitive length on the NP-Hard Euclidean Traveling Salesman Problem (TSP). On the MWM task, SPG is more data-efﬁcient and outperforms baseline methods when scaling up to larger problem sizes.
Testing the Manifold Hypothesis<|sep|>We are increasingly confronted with very high dimensional data from speech, images, and genomes and other sources. A collection of methodologies for analyzing high dimensional data based on the hypothesis that data tend to lie near a low dimensional manifold is now called "Manifold Learning". (see Figure 1.1) We refer to the underlying hypothesis as the "manifold hypothesis." Manifold Learning has been an area of intense activity over the past two decades. We refer the interested reader to a limited set of papers associated with this ﬁeld; see [1, 4, 5, 6, 9, 14, 16, 17, 26, 27, 28, 30, 32, 34, 38] and the references therein. The goal of this paper is to develop an algorithm that tests the manifold hypothesis. Examples of low-dimensional manifolds embedded in high-dimensional spaces include: image vectors representing 3D objects under diﬀerent illumination conditions, and camera views and phonemes in speech signals. The low-dimensional structure typically arises due to constraints arising from physical laws. A recent empirical study [4] of a large number of 3 × 3 images represented as points in R9 revealed that they approximately lie on a two-dimensional manifold knows as the Klein bottle. One of the characteristics of high-dimensional data of the type mentioned earlier is that the number of dimensions is comparable, or larger than, the number of samples. This has the consequence that the sample complexity of function approximation can grow exponentially. On the positive side, the data exhibits the phenomenon of “concentration of measure” [8, 18] and asymptotic analysis of statistical techniques is possible. Standard dimensional reduction techniques such as Principal Component Analysis and Factor Analysis, work well when the data lies near a linear subspace of high-dimensional space. They do not work well when the data lies near a nonlinear manifold embedded in the high-dimensional space. Recently, there has been considerable interest in ﬁtting low-dimensional nonlinear manifolds from sampled data points in high-dimensional spaces. These problems have been viewed as optimization problems generalizing the projection theorem in Hilbert Space. One line of research starts with principal curves/surfaces [14] and topology preserving networks [21]. The main ideas is that information about the global structure of a manifold can be obtained by analyzing the “interactions” between overlapping local linear structures. The so-called Local Linear Embedding method (local PCA) constructs a local geometric structure that is invariant to translation and rotation in the neighborhood of each data point [29]. In another line of investigation [35], pairwise geodesic distances of data points with respect to the underlying manifold are estimated and multi-dimensional scaling is used to project the data points on a lowdimensional space which best preserves the estimated geodesics. The tangent space in the neighborhood of a data point can be used to represent the local geometry and then these local tangent spaces can be aligned to construct the global coordinate system of the nonlinear manifold [39]. A comprehensive review of Manifold Learning can be found in the recent book [20]. In this paper, we take a “worst case” viewpoint of the Manifold Learning problem. Let H be a separable Hilbert space, and let P be a probability measure supported on the unit ball BH of H. Let | · | denote the Hilbert space norm of H and for any x, y ∈ H let d(x, y) = |x − y|. For any x ∈ BH and any M ⊂ BH, a closed subset, let d(x, M) = infy∈M |x − y| and L(M, P) = � d(x, M)2dP(x). We assume that i.i.d data is generated from sampling P, which is ﬁxed but unknown. This is a worst-case view in the sense that no prior information about the data generating mechanism is assumed to be available or used for the subsequent development. This is the viewpoint of modern Statistical Learning Theory [37]. In order to state the problem more precisely, we need to describe the class of manifolds within which we will search for the existence of a manifold which satisﬁes the manifold hypothesis. Let M be a submanifold of H. The reach τ > 0 of M is the largest number such that for any 0 < r < τ, any point at a distance r of M has a unique nearest point on M. Let Ge = Ge(d, V, τ) be the family of d-dimensional C2−submanifolds of the unit ball in H with volume ≤ V and reach ≥ τ. Let P be an unknown probability distribution supported in the unit ball of a separable (possibly inﬁnitedimensional) Hilbert space and let (x1, x2, . . .) be i.i.d random samples sampled from P. The test for the Manifold Hypothesis answers the following aﬃrmatively: Given error ε, dimension d, volume V, reach τ and conﬁdence 1 − δ, is there an algorithm that takes a number of samples depending on these parameters and with probability 1 − δ distinguishes between the following two cases (as least one must hold): (a) Whether there is a M ∈ Ge = Ge(d, CV, τ/C)
Communication-Efficient Network-Distributed Optimization with Differential-Coded Compressors<|sep|>Network-distributed optimization, a canonical topic dating back to [1], has received signiﬁcant interests in recent years thanks to its ever-increasing applications, e.g., distributed learning [2]–[4], multi-agent systems [5], resource allocation [6], localization [7], etc. All these applications involve geographically dispersed datasets that are too big to aggregate due to high communication costs or privacy/security risks, hence necessitating distributed optimization over the network. A notable feature in network-distributed optimization is that there is a lack of shared memory due to the absence of a dedicated parameter server – a key component in the hierarchical distributed master/slave architecture. As a result, every node can only exchange and aggregate information with its local neighbors to reach a consensus on a global optimal decision. In the literature, a classic algorithm for solving networkdistributed optimization problems is the decentralized gradient descent method (DGD) proposed by Nedic and Ozdaglar [8]. The enduring popularity DGD lies in its simple gossiplike structure, which can be easily implemented in networks. Speciﬁcally, in each iteration, the update at each node combines a weighted average of the state information from its local neighbors (obtained by gossiping) and a gradient step based on its own local objective function and state information. Further, DGD achieves the same convergence rate as the centralized gradient descent method, implying that distributed computation does not sacriﬁce convergence rate. However, despite the aforementioned salient features, a major limitation of the DGD method is that it requires full information exchanges of the state variables between nodes. Hence, the DGD algorithm is communication-inefﬁcient when solving large-size high-dimensional optimization problems in networks with low-speed communication links. For example, consider a distributed image regression problem over a satellite network, where each satellite has images of typical resolution 2048 × 2048 [9]. In this case, the parameter dimension is 2048×2048 ≈ 4×106 and the communication load per DGD iteration is 134 MB (32-bit ﬂoating-point). This is problematic for many satellite networks with low-speed RF (radio frequency) links (typically in the range of hundreds Mbps [10]). To improve DGD’s communication efﬁciency, recent years have seen a line of research based on exchanging compressed information between nodes (see, e.g., [11]–[14]). Speciﬁcally, by leveraging various compression techniques (e.g., quantization/rounding [15], sparsiﬁcation [16]), a high-dimensional state space can be represented by a small codebook, hence alleviating the communication load in the network. However, although progress has been made to various extents, most of the existing works on compressed DGD algorithms suffer from the following key limitations (see Section II for more in-depth discussions): 1) extra parameter tunings resulted from far more complex algorithmic structures compared to DGD; 2) restricted assumptions on compressors having bounded compression noise power; 3) convergence speed is slow and sensitive to problem structure; 4) strong i.i.d. (independently identically distributed) distribution assumptions on datasets at different locations, which often do not hold in practice. In addition, most of the existing works simply treat compressors as “blackbox operators” and do not consider how to minimize communication load with speciﬁc compression coding scheme designs. In light of the ever-increasing demand for large-scale network-distributed data analytics, the above limitations motivate us to develop new compression-based algorithms for communication-efﬁcient network-distributed optimization. The major contribution of this paper is that we propose a differential-coded compression-based DGD algorithmic framework (DC-DGD), which overcomes the above limitations and offers signiﬁcant improvements over the existing works. Moreover, based on the proposed DC-DCD framework, we propose a hybrid compression scheme that integrates gradient sparsiﬁcation and ternary operators, which enables dynamic communication load minimization. Our main technical results and their signiﬁcance are summarized as follows: • We propose a new differential-coded DC-DGD algorithmic framework, where “differential-coded” means that the information exchanged between nodes is the differential between two successive iterations of the variables, rather than the variables themselves. We show that DC-DGD allows us to work with a wide range of general compressors that are only constrained by SNR (signal-to-noise-ratio) and thus could have unbounded noise power. The use of SNR-constrained compressors relaxes the commonly adopted assumption on bounded compression noise power in the literature [11]– [13]. More speciﬁcally, we show that if a compressor’s SNR is greater than (1−λN)/(1+λN), where λN is the smallest eigenvalue of the consensus matrix used in all DGD-type algorithms, then our DC-DGD algorithm achieves the same O(1/t) convergence rate as the original DGD method. • Not only does the use of SNR-constrained compressors make our DC-DGD framework more general and practical, it also induces a nice “self-compression-noise-powerreduction effect” that keeps the algorithmic structure of DCDGD simple. More speciﬁcally, based on a quadratic Lyapunov function of the consensus form of the optimization problem, we show that the accumulated compression noise under DC-DGD shrinks to zero under SNR-constrained compressors and differential-coded information exchange. Hence, there is no need to introduce extra mechanisms or parameters to tame the accumulated compression noise for ensuring convergence. As a result, DC-DGD enjoys the same low-complexity and efﬁcient convergence rate as the original DGD method. • The insights on the relationship between DC-DCD and SNR-constrained compressors further inspires us to develop a hybrid compression scheme that integrates gradient sparsiﬁcation and ternary operators to obtain controllable SNR and a high compression ratio simultaneously. The proposed hybrid compression scheme achieves the best of both worlds through a meticulously designed mechanism to minimize the communication load. Speciﬁcally, under the hybrid compressor, the communication load minimization can be formulated as an integer programming problem. Based on the special problem structure, we show that the problem can be solved efﬁciently by a greedy algorithm. Our results in this paper contribute to the state of the art of theories and algorithm design for communication-efﬁcient network-distributed optimization. The rest of the paper is organized as follows. In Section II, we further review related works on the state of the art of compressed DGD-based optimization algorithms. In Section III, we ﬁrst present our DCDGD algorithm and then analyze its convergence gaurantees. In Section IV, we developed a family of hybrid operators and a greedy algorithm is proposed to choose the optimal hybrid
Dialectics of Knowledge Representation in a Granular Rough Set Theory<|sep|>General transitive relations are of much interest in a wide variety of application scenarios including vagueness and preference. But the semantics for the former cases have not been considered in the literature prior to this and [3] by the present author. Here we develop the KI over a granular representation of rough objects. We also show the gaping holes in meaning that would be left by omitting granularity in the considerations. Rough objects as explained in [5,1] are collections of objects in a classical domain (Meta-C) that appear to be indistinguishable among themselves in another rough semantic domain (Meta-R). But their representation in most RSTs in purely order theoretic terms is not known. In this research paper, we do this for a speciﬁc type of RST over proto-transitive approximation spaces (PRAS) developed recently by the present author in [3]. The method can be directly extended to many other types of RST. In [3], ﬁve diﬀerent algebraic semantics in slightly diﬀerent rough semantic domains are also developed by the present author. In general, rough objects correspond to concepts in the Pawlak-sense KI as they correspond to the objects that can be perceived in the rough semantic domain . Here we focus on the generalized approach to granular KI in general RST initiated in [2]. Relative the extended KI in a semantic domain placed between the classical (Meta-C) and Meta-R (of rough objects), the problem of KR is solved for reﬂexive PRAS (PRAX). From a simpliﬁed view, rough objects may be represented by pairs or tuples of deﬁnite objects under some conditions. In classical RST, deﬁnite objects are precisely those that satisfy xl = xu = x. Pairs of deﬁnite objects of the form (a, b) satisfying a ⊆ b necessarily represent rough objects and every rough object can be so represented. Further intervals of the form ]a, b[ = {c : a ⊂ c ⊂ b, } represent rough objects if and only if b covers a. However not all intervals of the form correspond to rough objects without the covering condition. This relation becomes more complicated in more general RSTs as in the case of PRAX. We characterize this in this research. Classical RST is RST starting from an approximation space of the form ⟨S
Characterizing (Un)moderated Textual Data in Social Systems<|sep|>The Web has changed the way our society communicates, giving rise to social platforms where users can share different types of content and freely express themselves through posts containing personal opinions. Unfortunately, with the popularization of this new ﬂavor of communication, toxic behaviors enacted by some users have been gaining prominence through online harassment and hate speech. These platforms have become the stage for numerous cases of online hate speech, a type of discourse that aims at attacking a person or a group on the basis of race, religion, ethnic origin, sexual orientation, disability, or gender [1]. Recently, to prevent the proliferation of toxic content, most online social networks prohibited hate speech in their user policies and enforced this rule by deleting posts and banning users who violate it. Particularly, Twitter, Facebook, and Google (YouTube) have largely increased removals of hate speech content [2] by making available their hate policies so users can actively report content that might violate their policies. Reddit also deleted some communities related to fatshaming and hate against immigrants [3]. This scenario has motivated the emergence of a new social network system, called Gab. In essence, Gab is very similar to Twitter, but barely moderates any of the content shared by its users. According to Gab guidelines, the website promotes freedom of expression and states that “the only valid form of censorship is an individual’s own choice to opt-out”. They, however, do not allow illegal activity, spam, or form of illegal pornography, promotion of violence and terrorism. Despite existing recent efforts that attempt to understand the content shared in Gab [4, 5], there is still a need for an understanding regarding the amount and forms of hate speech in an unmoderated system such as Gab. In this article, we provide a diagnostic of hate in the unmoderated content from Gab, by categorizing the different forms of hate speech in that system and comparing it with Twitter, a proxy for a moderated system. Speciﬁcally, we identify textual characteristics of a set of unmoderated (or barely moderated) data, in this work represented by Gab, and compare them with characteristics of moderated data, here represented by Twitter. Our study is based on the analysis of 7, 794, 990 Gab posts and 9, 118, 006 tweets from August 2016 to August 2017. At a high level, our analysis is centered around the following research questions: Research Question 1: What are the distinguishing characteristics of moderated content in Twitter and unmoderated content in Gab in terms of linguistic features, sentiment, and toxicity? Research Question 2: What are the most common types of hate in an unmoderated and moderated environment? To answer our ﬁrst research question we quantify the linguistic differences across moderated and unmoderated social systems by applying established language processing techniques. Our analysis shows that content in Gab and Twitter have different linguistics patterns, with higher toxicity and a more negative overall sentiment score in the unmoderated Gab content. Additionally, we ﬁnd that, in general, Gab has more hate posts than Twitter. We show that Gender and Class types of hate are more frequent on Twitter, whereas Disability, Ethnicity, Sexual Orientation, Religion, and Nationality types tend to appear proportionality more in Gab. These ﬁndings highlight the importance of creating moderation policies as an effort to ﬁght online hate speech in social systems, and also point out possible points for improvement in the design of content policies for social media systems. Additionally, our ﬁndings suggest that the unmoderated content found in Gab might be an appropriate data source for the development of learning approaches to detect hate speech. Thus, as a ﬁnal contribution, we make our hate-labeled Gab posts available for the research community1 can foster the development of future hate speech detection systems.
Direct QR factorizations for tall-and-skinny matrices in MapReduce architectures<|sep|>where Q is an m × n orthogonal matrix and R is an n × n upper triangular matrix. We call a matrix talland-skinny if it has many more rows than columns (m ≫ n). In this paper, we study algorithms to compute a QR factorization of a tall-and-skinny matrix for nearlyterabyte sized matrices on MapReduce architectures [6]. Current tall-and-skinny QR methods for MapReduce provide only a fast way to compute R [5]. (The details of these are described further in Sec. II.) In order to For R to be invertible, A must be full-rank, and we assume A is full-rank throughout this paper. The indirect formulation is known to be numerically unstable, although, a step of iterative reﬁnement can sometimes be used to produce a Q factor with acceptable accuracy [15]. (Iterative reﬁnement is the process of repeating the QR decomposition on the computed Q factor.) However, if a matrix is suﬃciently ill-conditioned, iterative reﬁnement will still result in a large error measured by ∥QT Q − I∥2 (see Sec. IV). We shall describe a numerically stable method (Sec. III) that computes Q and R directly and faster than performing the reﬁnement of the indirect computation for some matrices. Sec. V-A describes a performance model for our algorithms, which allows us to compute lower bounds on running times. The algorithms are almost always within a factor of two of the lower bounds (Sec. V-B). The data in a MapReduce computation is deﬁned by a collection of key-value pairs. When we use MapReduce to analyze tall-and-skinny matrix data, a key represents the identity of a row and a value represents the elements in that row. Thus, the matrix is a collection of keyvalue pairs. We assume that each row has a distinct key for simplicity; although we note that our methods also handle cases where each key represents a set of rows. There are a growing number of MapReduce frameworks that implement the same computational engine: ﬁrst, map applies a function to each key-value pair which outputs a transformed key-value pair; second, shuﬄe rearranges the data to ensure that all values with the same key are together; ﬁnally, reduce applies a function to all values with the same key. The most popular MapReduce implementation – Hadoop [20] – stores all data and intermediate computations on disk. Thus, we do not expect numerical linear algebra algorithms for MapReduce to be faster than state-of-the-art in-memory Table I The performance improvement of C++ over Python for our Direct TSQR on a 10-node MapReduce cluster is only mild. MPI implementations running on clusters with highperformance interconnects. However, the MapReduce model oﬀers several advantages that make the platform attractive for large-scale, large-data computations (see also [21] for information on tradeoﬀs). First, many large datasets are already warehoused in MapReduce clusters. With the availability of algorithms, such as QR, on a MapReduce cluster, these data do not need to be transferred to another cluster for analysis. Second, MapReduce systems like Hadoop provide transparent fault-tolerance, which is a major beneﬁt over standard MPI systems. Other MapReduce implementations, such as Twister [9], Phoenix++ [18], LEMOMR [10], and MRMPI [16], often store data in memory and may be a great deal faster; although, they usually lack the automatic fault tolerance. Third, the Hadoop computation engine handles all details of the distributed input-output routines, which greatly simpliﬁes the resulting programs. For the majority of our implementations, we use Hadoop streaming and the Python-based Dumbo MapReduce interface [2]. These programs are concise, straightforward, and easy-to-adapt to new applications. We have also investigated C++ and Java implementations, but these programs oﬀered only mild speedups (around 2-fold), if any. See Table I for a comparison against C++. The Python implementation uses about 70 lines of code, while the C++ implementation uses about 600 lines of code. Our two success metrics are speed and stability. The diﬀerences in speed are examined in Sec. V-B. To analyze the performance, we construct a performance model for the MapReduce cluster. After ﬁtting two parameters to the performance of the cluster, it predicts the runtime to within a factor of two. For stability, we use the metric ∥A − QR∥2/∥R∥2 to measure the accuracy of the decomposition and ∥QT Q − I∥2 to measure the orthogonality of the computed Q factor. Small scale simulations of the MapReduce algorithms show that, regardless of the algorithm, ∥A − QR∥2/∥R∥2 is O(ϵ) where ϵ is the machine precision. However, ∥QT Q − I∥2 varies dramatically based on the algorithm, but is always
Gamma-ray Flaring Emission in Blazar OJ287 Located in the Jet >14 pc from the Black Hole<|sep|>The Large Area Telescope (LAT) onboard the Fermi Gamma-ray Space Telescope has suﬃcient sensitivity to study the location of the γ-ray emission site in blazars through well-sampled light curves that allow detailed studies of the timing of γ-ray ﬂares relative to those at other spectral ranges [e.g., Abdo et al. 2010d, Jorstad et al. 2010, Marscher et al. 2010]. The technique developed by Marscher et al. [2010] and Jorstad et al. [2010] uses ultra-high angular-resolution monitoring with VLBI to resolve the innermost jet regions and monitor changes in jet structure. Monthly observations, provide time sequences of total and polarized intensity images of the parsec-scale jet that can be related to variations of the ﬂux and polarization at higher frequencies. In Agudo et al. [2011a], we employed this technique to investigate the location of the ﬂaring γ-ray emission in the BL Lacertae (BL Lac) object OJ287 (z = 0.306). In this paper we reproduce the results in Agudo et al. [2011a]. We adopt a cosmology with H0=71 km s−1 Mpc−1, ΩM = 0.27, and ΩΛ = 0.73, so that 1 mas corresponds to a projected distance of 4.48 pc, and a proper motion of 1 mas/yr corresponds to a superluminal speed of 19 c.
Evidence for Warped Disks of Young Stars in the Galactic Center<|sep|>The Galactic Center (GC) is a uniquely accessible laboratory for studying the properties and evolution of galactic nuclei (for reviews see e.g. Genzel & Townes 1987; Morris & Serabyn 1996; Mezger et al. 1996; Alexander 2005). At a distance of about 8 kpc (Eisenhauer et al. 2003a; Gillessen et al. 2009; Ghez et al. 2008; Gronewegen et al. 2008; Trippe et al. 2008), processes in the Galactic Center can be studied at much higher resolution compared to any other galactic nucleus. Stellar orbits show that the gravitational potential to a scale of a few light hours is dominated by a concentrated mass of about 4 × 106M⊙. It is associated with the compact radio source Sgr A*, which must be a massive black hole, beyond any reasonable doubt (Sch¨odel et al. 2002; Ghez et al. 2005; Gillessen et al. 2009). We will adopt a distance and a mass of Sgr A* of R0 = 8 kpc and MSgr A∗ = 4.0 × 106 M⊙ for all analyses presented in this paper. The evolution and the star-formation history in the central pc of the Galaxy may also be used as a probe of star formation processes near supermassive black holes in general, also relevant to other galactic nuclei (Collin & Zahn 2008; Levin 2007). The central parsec of the Galaxy contains about a hundred massive young stars. The majority are O-type supergiants and Wolf-Rayet (WR) stars (Forrest et al. 1987; Allen et al. 1990; Krabbe et al. 1991; Najarro et al. 1994; Krabbe et al. 1995; Blum et al. 1995; Tamblyn et al. 1996; Najarro et al. 1997; Genzel et al. 2003; Ghez et al. 2003; Paumard et al. 2006; Martins et al. 2007) with an estimated age of about 6 × 106 years. Genzel et al. (1996, 2000, 2003); Levin & Beloborodov (2003); Beloborodov et al. (2006); Paumard et al. (2006) inferred that most of the dynamical properties of the WR/O stars (located at projected distances to SgrA* between 0.8” and 12”) are compatible with belonging to either of two moderately thick counter-rotating stellar disks. However, Tanner et al. (2006) were only able to assign 15 out of 30 early-type stars near the Galactic Center as disk members. Lu et al. (2006, 2008) conﬁrm one stellar disk but do not observe a signiﬁcant number of stars in the other one. The existence of these young massive stars indicates that star formation must have recently taken place at or near the Galactic Center within the last few million years. This is surprising, since regular star formation processes are likely to be suppressed by the tidal forces from the massive black hole. Many scenarios have been suggested for the origin of these stars (see Alexander (2005); Paumard et al. (2006); Paumard (2008) for recent reviews). These include in situ star formation through gravitational fragmentation of gas in disk(s) formed from infalling molecular cloud(s); transport of stars from far out by an infalling young stellar cluster, or through disruption of binary stars on highly elliptical orbits by the massive black hole; and rejuvenation of old stars due to stellar collisions and tidal stripping. The young stars observed in the inner R ∼ 1′′ are less massive B-stars (so called the ’S-stars’) and are likely to originate from a diﬀerent scenario then the O and WR stars (e.g. Perets et al. (2007), but see Levin (2007); L¨ockmann et al. (2008)). Here we discuss only our observations of the O and WR stars outside the central 0.8” (other populations of young stars in the GC are discussed elsewhere; Gillessen et al. 2009; Martins et al. 2009), and interpret them in the context of the two leading formation scenarios, the infalling cluster (Gerhard 2001; McMillan & Portegies Zwart 2003; Portegies Zwart et al. 2003; Kim & Morris 2003; Kim et al. 2004; G¨urkan & Rasio 2005) and the in situ formation (Levin & Beloborodov 2003; Genzel et al. 2003; Goodman 2003; Milosavljevi´c & Loeb 2004; Nayakshin & Cuadra 2005; Paumard et al. 2006) scenarios. The infalling cluster and the in situ formation scenarios can be distinguished by diﬀerent phase space distributions of the stars (see also Paumard et al. 2006; Lu et al. 2008). Key observables are the number of disks, the fractions of disk and isotropic stars, the disk orientation, thickness, eccentricity and warp as well as the radial density of the stars and the stellar mass function. In the following we present the results of new observations of the Galactic Center with the adaptive optics (AO) assisted near-infrared imager NACO and the integral ﬁeld spectrograph SINFONI on the ESO/VLT. These include the detection of 27 new reliably measured WR/O stars in the central 12” and improved measurements of previously detected stars, with proper motion uncertainties reduced by a factor of four compared to our earlier work. Based on a sample of 90 well measured WR/O stars, we develop a detailed statistical analysis of their orbital properties and orientations. To this end, we use a Monte-Carlo technique to simulate observations of a large number of isotropic stars with the same measurement uncertainties as present in the data. From these simulated measurements, we determine the probability of ﬁnding coherent dynamical structures against isotropic stars. We ﬁnd strong evidence for the existence of a warped disk in the distribution of the clockwise rotating stars and a non-random structure among the counter-clockwise rotating stars, which is possibly an additional disk. We then analyze the properties of the stellar disks using both the 3D velocity information and the stellar positions. We discuss the implications of our observational results for models for the origin of the O and WR stellar population in the GC. This paper is structured as follows: First, we describe our observations, the data selection criteria and present the properties of our data set in section 2. Thereafter, in section 3, we describe our simulations of the observations of isotropically distributed stars and disk stars. In section 4 we introduce our analysis method to search for features in the star distribution. In section 5 we present our results, including a thorough study of the signiﬁcance of the counter-clockwise system, the determination of the orbital properties of the disk stars and a comparison to previous work. After a discussion of our results in section 6 we summarize our conclusions in section 7.
Quasinormal mode spectra for odd parity perturbations in spacetimes with smeared matter sources<|sep|>The recent detection of Gravitational Waves (GWs), from binary Black Hole (BH) mergers and Neutron stars by LIGO and Virgo collaboration [1, 2, 3, 4], has provided us with a new window to study and understand physical processes at extreme conditions, where the role of gravity by far dominates the other known forms of interactions in nature. Since the metric to describe the gravitational collapse of a binary merger is unknown we fall back to numerical relativity simulations from which we gain a fairly reasonable understanding of such realistic phenomena. For the present work we are assuming that these collapse events are generally consistent with the theoretical predictions of general relativity (GR) [5], although later works [6] have shown that there are signiﬁcant deviations. These observations now ﬁrmly suggest studying the possibility of having alternative sources that can account for such deviations in GW characteristic frequencies. Although an ultra-compact system of binaries is probably too exotic a system for producing strong GW signal to be detected at large distances, more common objects nevertheless can also produce GWs with frequencies that could be highly relevant for the next generation space-based GW detectors. The popularly known GWs as observed by LIGO/Virgo collaboration are actually the Quasi Normal Modes or QNMs [7, 8, 9, 10]. The waveforms of these GW signals consist of three parts: 1) inspiral, 2) merger and 3) ring-down. The ring-down phase shows characteristic frequencies of oscillation corresponding to damped resonances of the remnant BH. These damped oscillations or QNMs encode information about the BH source. Applying the linear perturbation results, the ring-down portion of the signal may be used to discriminate between BHs and other possible sources. The damped modes in turn possess a complex frequency whose real part corresponds to the oscillation frequency and whose imaginary part gives the lifetime. It is important to note that the QNM spectrum of a BH is completely characterized by the BH parameters, and does not depend on the initial conditions of the perturbations. In this work our aim is to investigate the QNM frequencies for a spherically symmetric geometry having a smeared matter source. An interesting approach was pioneered by Nicolini, Smailagic and Spallucci [11]. These authors introduced a (spherically symmetric) smeared source in the matter sector and solved the Einstein equation thereby obtaining a generalized form of Schwarzschild (black hole) metric that successfully cured the black hole singularity problem. It was also tentatively proposed to identify the smearing scale with Planck length so that the metric can play a role in the context of quantum gravity. (For an exhaustive review see [12].) Various aspects of this generalized black hole have been studied: its thermodynamics [13], eﬀect of the smearing on AdS/CFT correspondence [14], among others. As mentioned above, the conventional Dirac delta source term for matter is replaced by a new type of matter source with the energy density given by a Gaussian distribution function. The resulting geometry will be helpful to understand the dynamics of objects, which have approximately a Gaussian mass proﬁle. From astrophysical point of view, such a Gaussian proﬁle can be applied to study the dynamics of GWs for elliptical galaxies (e.g. globular clusters having dense matter core in the center [15]). Besides that, this distribution is also relevant for the Dark matter proﬁles within galaxies (e.g. Press-Schechter mass distribution that is extensively used in the context of dark matter distribution proﬁle [16]). Therefore, the mathematical formulation of this study with smeared matter source will be particularly interesting for astrophysical objects, where the length scales would be O(Ly) (1Ly ∼ 3 × 10−7Mpc). In this paper, we can tentatively identify this length scale with Θ the smearing parameter. Let us point out the proper perspective of our work in view of the recent works of Liang [17, 18] who has also made an exhaustive study of smearing eﬀect on QNM. The results of Liang are valid up to 3’rd order in WKB. On the other hand we have used the framework of [19] yielding results valid up to 6’th order in WKB. We explicitly demonstrate that there are appreciable modiﬁcations when the latter are taken in to account. The organization of our paper is as follows — in Sec. 2 we have brieﬂy reviewed the basic aspects of QNMs and an elementary method to obtain them for static spherically symmetric Schwarschild spacetime. Sec. 3 deals with the gravitational perturbation of a spherically symmetric QG-inspired spacetime. Here the analysis is made in four segments. In Sec 3.1 we have computed the equations for the odd parity gravitational perturbation of this QG-inspired spacetime. Then in Sec. 3.2 and in Sec. 3.3 we have obtained the QNM frequencies for this spacetime using the Ferrari-Mashoon formula [20] and also the WKB 6th order formula [19]. Here we discuss our results for the new QNMs by comparing them with the standard Schwarzschild QNMs and also with the QNMs for this QG-induced spactime, obtained earlier with 3rd order WKB method. Finally, in Sec. 3.4 we discuss the relevance of the results from observational perspetives and in Sec. 4 we conclude.
Magnetic properties and domain structure of ultrathin yttrium iron garnet/Pt bilayers<|sep|>Yttrium iron garnet (YIG) thin ﬁlms have attracted considerable interest in the ﬁeld of spintronics due to the possibility of converting magnon excitations into spin and charge currents ﬂowing in an adjacent nonmagnetic metal (NM) layer. Spin currents in YIG/NM bilayers have been excited thermally (spin Seebeck eﬀect)1–4, dynamically (spin-pumping)5–8 or by means of the spin Hall eﬀect9–13. In the latter case, a charge current in the NM generates a transverse spin current that is either absorbed or reﬂected at the interface with YIG. This leads to a variety of interesting eﬀects such as the spin Hall magnetoresistance14–17 (SMR) and current-induced spinorbit torques18–20, which can be used to sense and manipulate the magnetization. For the latter purpose, it is desirable to work with thin magnetic ﬁlms in order to achieve the largest eﬀect from the interfacial torques. For a long time, the growth of YIG has been accomplished by liquid phase epitaxy, which oﬀers excellent epitaxial quality and dynamic properties such as low damping and a rich spin-wave spectrum. The magnetic properties of these bulk-like samples, including the magnetocrystalline anisotropy21,22 and magnetic domain structure23–25, have been extensively characterized in the past. However, with rare exceptions26, samples grown by liquid phase epitaxy usually have thicknesses in the μm to mm range. Recent developments in oxide thin ﬁlm growth give access to the sub-μm range by employing techniques such as laser molecular beam epitaxy27, sputtering28,29, and pulsed laser deposition (PLD)30–37, which allow for growing good quality ﬁlms with thickness down to the sub-100-nm range28,29,35–37 and even below 10 nm27,33,34. Since the thickness as well as structural and compositional eﬀects have a large inﬂuence on the magnetic behavior, these developments call for a detailed characterization of the magnetic properties of ultrathin YIG ﬁlms. Several characteristic quantities, which are of high relevance for YIG-based spintronics, have been found to vary in ﬁlms thinner than 100 nm. For instance, a reduction of the saturation magnetization29,35–40 is typically observed in YIG ﬁlms with thickness down to 10 nm, which has been ascribed to either thermallyinduced stress38, lack of exchange interaction partners at the interface36, or stoichiometric variations29,37,39,40. Furthermore, an increase of the damping33–35,41 as well as a decreased spin mixing conductance42,43 have been found in ultrathin YIG. Finally, the emergence of unexpected magnetocrystalline anisotropy was reported for ﬁlms of diﬀerent orientations grown on gadolinium gallium garnet (GGG) and yttrium aluminium garnet (YAG). The magnetic anisotropy was investigated by the magneto-optical Kerr eﬀect in GGG/YIG(111)27,36 and by ferromagnetic resonance in GGG/YIG(111)31, GGG/YIG(001)32, and YAG/YIG(001)28. Whereas all these studies address important magnetic characteristics in the sub-100 nm range, only few studies33,34 explore the ultrathin ﬁlm regime below 10 nm. This thickness range is highly relevant for eﬃcient magnetization manipulation using current-induced interfacial eﬀects as well as for strain engineering, since strain and its gradients relax after 10 to 20 nm. Finally, a comprehensive knowledge of the domain and domain wall structure in the thin ﬁlm regime is lacking. Recent studies on magnetic domains address only bulk25, several micrometers44 or hundreds of nanometers45 thick YIG ﬁlms. In this work, we present a systematic investigation of the structure, saturation magnetization, magnetic anisotropy, and magnetic domains of YIG/Pt ﬁlms grown on GGG substrates by PLD as a function of YIG thickness from tYIG = 3.4 to tYIG = 90 nm. By combining x-ray diﬀraction (XRD), transmission electron microscopy (TEM), atomic force microscopy (AFM) and xray absorption spectroscopy, we show that our ﬁlms possess high crystalline quality and smooth surfaces with no detectable interface mixing throughout the entire thickness range. The saturation magnetization, investigated using a superconducting quantum interference device (SQUID), shows values close to bulk for thick ﬁlms and a gradual reduction towards lower thicknesses. We probe the magnetic anisotropy electrically by means of SMR and ﬁnd an easy plane and uniaxial in-plane anisotropy with a non-monotonic variation of the in-plane orientation of the easy axis and the magnitude of the eﬀective anisotropy ﬁeld. Finally, we investigate the domain structure using x-ray photoelectron emission microscopy (XPEEM), evidencing signiﬁcant changes in the domain structure above and below tYIG = 10 nm. Our results provide a basis for understanding the behavior of spintronic devices based on YIG/Pt with diﬀerent YIG thickness.
An Edge-Cloud Integrated Framework for Flexible and Dynamic Stream Analytics<|sep|>Stream analytics has become a major data analytics area due to hardware and software advances in Internet of Things (IoT), edge computing and cloud computing. It is now much easier to obtain sensing data from IoT devices, which leads to more and more stream analytics applications including realtime trend prediction [1, 2, 3] and real-time object detection [4, 5] on top of IoT sensing data. Diﬀerent from traditional analytics that assumes data to be processed are available ahead of time and will not change, stream analytics processes data that are being generated on the ﬂy and continuously. A well-known ∗Corresponding author Email addresses: xinwang11@umbc.edu (Xin Wang), azimkhan22@umbc.edu (Azim Khan), jianwu@umbc.edu (Jianwu Wang), gangopad@umbc.edu (Aryya Gangopadhyay), carl.e.busart.civ@army.mil (Carl Busart), jade.l.freeman2.civ@army.mil (Jade Freeman) challenge of stream analytics is concept drift [6, 7] which describes changes in the concept or distribution of stream data. There is a growing number of studies on how to conduct stream analytics by leveraging IoT, edge and cloud resources. Edge computing in an IoT environment brings computation and data storage closer to data sources. It operates on “instant data” that is usually time sensitive. Besides the latency beneﬁt, edge computing is normally designed for remote locations, where there is limited or no connectivity to a centralized computation location. However, resources on edges are constrained and limited in their capacity/capability and can only support relatively simple data processing like inference/prediction based on a pre-trained model. So it often relies on additional resources, such as storage or memory optimized devices, for more complex processing. Cloud computing [8] provides on-demand computational resources for data analytics over the Internet and becomes a major approach for supporting complex and highperformance computation. Besides IoT environments, stream ing data could also be delivered directly to the cloud and be computed with enough computing power and storage capacity. However, considering its distance to the data source, it is hard to have a quick response when data injection for some time-sensitive applications like earthquake warnings and automatic driving. Since both edge and cloud resources have their advantages and disadvantages, a related computing paradigm like Edge-to-Cloud Continuum [9] has been proposed to integrate edge with cloud. In an edge-cloud integrated framework, the computation involves both front-end on-premise edge resources like Raspberry Pi and NVIDIA Jetson Nano, and backend computing resources like big data and GPU clusters in cloud. Deep learning has been widely used in stream analytics in IoT, edge or cloud environments. As a recent survey paper [10] shows, about one third of studies surveyed in the paper employs recurrent neural network (RNN) based deep learning models for time series or sequence data prediction and forecasting. RNN models can help learn temporal dependence and structures like trends and seasonality. Most existing studies and systems, such as [10] and [5], only support deep learning based inference on IoT/edge devices. A new research area is how to best integrate both edge resources and cloud resources for deep learning applications. Several researchers [9, 11, 12, 13, 14, 15, 16, 17] have proposed solutions and frameworks for streaming data analytics that leverage the capabilities of cloud services. However, to integrate edge with cloud, we need to achieve a proper tradeoﬀ between latency and accuracy for stream analytics between edge and cloud resources. Accuracy and latency are two common metrics in stream analytics and many studies have how to balance them or make trade-oﬀs. In the paper, we focus on how to achieve good accuracy and latency for the RNN-based deep learning model in an edge-cloud integrated environment by addressing the following two challenges. First, while the existing studies like [9, 11, 12] provided promising direction, it is still not clear how to best deploy RNN-based deep learning models in edge and cloud resources for stream analytics to achieve better latency. Second, even though there have been many studies [18, 1] on how to deal with unknown or changing data distributions in stream data, a.k.a. concept drift, it is still an open question how to balance accuracy and latency for RNN based stream analytics in an edge-cloud environment. To tackle the above two challenges, we propose a novel edge-cloud integrated framework and its corresponding opensource modules [19] for stream analytics. To the best of our knowledge, our work is the ﬁrst to achieve hybrid RNN-based deep learning for stream data in an edge-cloud integrated environment. Our contributions are summarized as follows. • We propose a novel edge-cloud integrated framework for stream analytics that supports low latency inference on the edge and high capacity training on the cloud. Tasks like data injection, model inference and synchronization are encapsulated as modules and can be ﬂexibly deployed on either an edge device like Raspberry Pi or a cloud resource like AWS. deployment modalities for our hybrid learning framework: edge-centric, cloud-centric and edge-cloud integrated. Based on a modular design, the hybrid learning framework can still work even if parts of the cloud services or edge analytics are unavailable. We further measured the latency diﬀerences between the three deployments using a realworld stream analytics application. Our experiments show the proposed edge-cloud deployment is among the best in terms of latency for inference, also will not run into capacity limitation for training. • To adapt the concept drift challenge of stream data in edge-cloud integrated environments, we propose an adaptive hybrid learning framework that combines and beneﬁts from both cloud resources’ high capacity and edge resources’ low latency. Our hybrid learning framework contains batch learning by employing a pre-trained RNN model from large historical data, speed learning by periodically re-training an RNN model from most recent data and hybrid learning by combining predictions from batch and speed learning. We also study a new hybrid learning algorithm that can combine results dynamically. Our experiments show our hybrid learning approaches can have better RMSE than cloud-based batch learning and edge-based speed learning in most cases and our dynamic learning approach performs the best among all learning approaches for all three concept drift scenarios. The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the related background our work is built on. Section 3 provides an overview of the proposed edge-cloud integrated hybrid learning framework. Section 4 introduces our three ﬂexible deployment modalities of hybrid learning framework, including edge-centric, cloud-centric and edge-cloud integrated deployments. The adaptive hybrid stream analytics and its two weight combinations, namely static and dynamic weighting algorithms, are explained in Section 5. Evaluations and benchmarking results are next discussed in Section 6. We summarize related studies and compare them with our work in 7 and conclude in Section 8.
Arctic curves of the six-vertex model on generic domains: the Tangent Method<|sep|>Statistical mechanics models in two dimensions with a discrete symmetry group, within a pure phase, usually show a spatially-homogeneous order parameter and independence from the boundary conditions [1]. This can be understood by simple entropic arguments on local excitations. The prototype example is the Ising Model, where the broken symmetry group is just Z2. Nonetheless, certain models, characterised by the presence of conservation laws, under particular conditions may break this paradigm and show phase-separation phenomena and the emergence of a limit shape [2, 3]. In this case we may have spatial dependence of the order parameter, a strong dependence from the boundary conditions, and even frozen regions, in which the local entropy vanishes. This is now possible because the conservation law forbids local excitations on frozen-region vacua, the smallest perturbations taking the form of a directed path which, in each direction, shall either reach the boundary, or a non-frozen (liquid) region. The interface between frozen and liquid regions, for a given model in a given domain, is called Arctic curve. The challenge of its determination is the subject of the present paper. Among the models presenting phase separation and limit shape phenomena, those amenable to discrete free fermions are the most widely studied. Early examples include Young diagrams with the Plancherel measure [4], the evaporation of a cubic crystal [5–7], domino tilings of the Aztec diamond, [8], boxed plane partitions [9], Schur processes [10]. These examples may all be viewed as dimer models on planar bipartite graphs, for which a general theory has been constructed [2,11,12]; other approaches exist for certain subclasses of models [13–16]. An interesting connection between limit shape phenomena in such models and the out-of-equilibrium evolution of one-dimensional quantum free-fermion models has been recently unveiled [17]. Other free-fermionic models presenting a similar phenomenology are deﬁned in terms of iterated transformations applied to a deterministic initial conﬁguration. Examples of such models include ‘groves’ on the triangle [18, 19] (see also [20] for
How Does Metallicity Affect the Gas and Dust Properties of Galaxies?<|sep|>Dwarf galaxies, often carrying signatures of chemical youth, are unique laboratories to obtain insight on star formation and interstellar medium (ISM) properties in lowmetallicity environments that may be relevant to understanding early universe conditions, when metal abundances were very low. The decrease of metals reduces shielding eﬀects which have important consequences, for example, on the molecular gas reservoir, which, in turn, should aﬀect how we observe the ISM properties. Our knowledge in this subject has been growing at a fast pace due to the increasingly sensitive infrared (IR) space missions IRAS, ISO, Spitzer, AKARI and Herschel, as well as ever-growing powerful ground-based millimeter (mm) and submillimeter (submm) telescopes. The Dwarf Galaxy Survey (DGS; Madden et al. 2013), has compiled a large observational database of 48 low-metallicity galaxies, motivated by Herschel PACS (Poglitsch et al. 2010) and SPIRE (Griﬃn et al. 2010) photometric and spectroscopic observations covering 55 to 500 µm. These data, together with the ancillary database, create a rich legacy for dust and gas analyses in unprecedented detail in low-metallicity galaxies from ultraviolet (UV) to mm wavelengths in environments covering the largest metallicity range achievable in the local Universe, as low as ∼1/40Z⊙(R´emy-Ruyer et al. 2013; 2015). Here we describe enigmatic issues surrounding the nature of the dust and gas properties in low-metallicity galaxies.
segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection<|sep|>In the past two years, Convolutional Neural Networks (CNNs) have revolutionized computer vision. They have been applied to a variety of general vision problems, such as recognition [15, 9], segmentation [11], stereo [18], ﬂow [24], and even text-from-image generation [13], consistently outperforming past work. This is mainly due to their high generalization power achieved by learning complex, non-linear dependencies across millions of labelled examples. It has recently been shown that increasing the depth of the network increases the performance by an additional impressive margin on the ImageNet challenge [21, 22]. It remains to be seen whether recognition can be solved by simply pushing the limits of computation (the size of the networks) and increasing the amount of the training data. We believe that the main challenge in the next few years will be to design computationally simpler and more efﬁcient models that can achieve a similar or better performance compared to the very deep networks. For object detection, a successful approach has been to generate a large pool of candidate boxes [23] and classify them using CNNs [9]. The quality of such a detector thus largely depends on the quality of the object hypotheses. Interestingly, however, using much better proposals obtained via a high-end bottom-up segmentation approach [11] has resulted only in small improvements in accuracy. In this paper, we show how to exploit a small number of accurate object segment proposals in order to signiﬁcantly improve object detection performance. We frame the detection problem as inference in a Markov Random Field as in Figure 1, in which each detection hypothesis scores object appearance as well as contextual information using Convolutional Neural Networks. Each hypothesis can choose and score a segment out of a small pool of accurate object segmentation proposals. This enables our approach to place more accurate object bounding boxes in parts of the image where an object segmentation hypothesis [2] exists or where strong contextual cues are available. We additionally show that a signiﬁcant performance boost can be obtained by a sequential approach, where the network iterates between adjusting its spatial scope (the bounding box) and classifying its content. This strategy reduces the dependency on the initial candidate boxes obtained by [23] and enables our approach to recover from the potentially bad initial localiza tion. We show that our model, called segDeepM, outperforms the baseline R-CNN [9] approach by 3.2% with almost no extra computational cost. We get a total of 5% improvement by incorporating contextual information at the cost of doubling the running time of the method. On PASCAL VOC 2010 test, our method achieves 4.1% improvement over RCNN and 1.4% over the current state-of-the-art.
Massive Black Hole Binaries from the TNG50-3 Simulation: II. Using Dual AGNs to Predict the Rate of Black Hole Mergers<|sep|>Dual Active Galactic Nuclei (dAGNs) are two accreting massive black holes (MBHs) residing within a single host galaxy and are expected to occur following the merger of two massive galaxies. A population of dAGNs in post-merger galaxies seems to be an unavoidable prediction of hierarchal galaxy formation models (e.g. De Rosa et al. 2020). In some cases, the separation of the two MBHs that comprise a dAGN will shrink slowly over time as the orbiting MBHs interact with the gaseous and stellar backgrounds of the galaxy (e.g., Begelman et al. 1980; Volonteri et al. 2003; Barausse 2012; Valiante et al. 2016; Bonetti et al. 2019; Khan et al. 2019; Li et al. 2022), eventually leading to the emission of gravitational waves and the coalescence of the two black holes (e.g., Amaro-Seoane et al. 2017; Kelley et al. 2017, 2019). Therefore, dAGNs are ‘tracers’ of future MBH mergers and gravitational wave events. An observed sample of dAGNs, combined with a model describing their future evolution, can thus provide a prediction of the MBH merger rate, a critical parameter for the upcoming Laser Interferometer Space Antenna (LISA) gravitational wave observatory. Whether an observable dAGN results in a MBH coalescence depends on the physical processes within the remnant galaxy that drive the orbital evolution of the MBHs. When the MBHs are at separations of ∼ 1 kpc, dynamical friction (DF) by gas and stars is expected to dominate the orbital de
The Earliest Phases of Star Formation (EPoS): A Herschel Key Program - The precursors to high-mass stars and clusters<|sep|>Star formation is a critical ingredient in a broad range of astrophysical phenomena, yet there are fundamental components of the process – particularly in the early stages – that remain poorly understood. Over the past decades, a basic framework for the formation of low-mass stars has developed beginning with gravitationally bound pre-stellar cores (Ward-Thompson et al., 2002), evolving into Class 0 and Class I protostars then Class II and Class III pre-main sequence stars (Shu et al., 1987; Andr´e et al., 1993). Such a sequence for the formation of high-mass stars has not yet been established. Several good candidates for massive young cores have been identiﬁed using the 170 µm ISOPHOT Serendipity Survey (ISOSS) (Lemke et al., 1996; Krause et al., ⋆ Herschel is an ESA space observatory with science instruments provided by European-led Principal Investigator consortia and with important participation from NASA. 2003, 2004; Birkmann et al., 2006) or sensitive millimeter surveys (e.g. Klein et al., 2005; Sridharan et al., 2005). However, upon further investigation, most have been found to already host (deeply embedded) low- to intermediate-mass protostars (e.g. Motte et al., 2007; Hennemann et al., 2008; Beuther & Henning, 2009). It is the stage previous to the onset of massive protostar formation that continues to elude observers. Many gaps remain in our understanding of how massive stars and clusters form, beginning with the elusive initial conditions. Do massive stars result from the gravitational collapse of cold, very massive cores (Evans et al., 2002; Beuther et al., 2007) like their low-mass siblings? What role does the environment play in determining the ultimate fate of such cores? As a (massive) protostar evolves, how drastically do its properties change, and what impact does it have on its surroundings? In order to study these very early, embedded phases of (massive) star formation, access to the far-infrared wavelength regime, where the peak of the cold Fig. 1. Face-on schematic view of the distribution of IRDCs (green triangles) and ISOSS sources (red squares) and the HMSC 07029 (blue triangle) in the Milky Way. The background image is an artist’s impression of the Milky Way based on the GLIMPSE survey, credit R. Hurt [SSC-Caltech], adapted by MPIA graphics department. The kinematic distances to each object are derived using the Reid et al. (2009) model. dust radiation (Tdust ∼ 10-20 K) is located, is critical. In addition, high angular resolution is needed to study massive star-forming regions which usually reside several kiloparsecs from the Sun. The Herschel far-infrared satellite (Pilbratt et al., 2010) drastically improves our ability to peer deep into the dense regions where such young cores are embedded. The Earliest Phases of Star formation (EPoS) Guaranteed Time Key Program (P.I. O. Krause; Henning et al., 2010; Linz et al., 2010; Beuther et al., 2010; Stutz et al., 2010) is a PACS and SPIRE photometric mapping survey which targets objects known to be in the cold early phases of star formation. There are two main components to the EPoS sample: 15 isolated, low-mass globules at various evolutionary stages, from starless to Class I, and 45 high-mass regions, which are mostly larger, high density molecular cloud complexes containing a range of objects within their boundaries. In Stutz et al. (2010), Nielbock et al. (2012, submitted), and a forthcoming comprehensive study by Launhardt et al. (in prep.), the low-mass part of the EPoS sam Our sample is comprised of objects known as infrared-dark clouds (IRDCs), which were ﬁrst discovered in silhouette against the bright Galactic background in the mid-infrared with the ISOCAM instrument (Perault et al., 1996) and the MSX satellite (Egan et al., 1998) at 15 and 8 µm, respectively. Several surveys in millimeter and sub-millimeter continuum and spectral lines have followed (e.g. Carey et al., 1998, 2000; Teyssier et al., 2002; Pillai et al., 2006a; Rathborne et al., 2006; Ragan et al., 2006; Vasyunina et al., 2009) and have established that massive IRDCs harbor the precursors and early phases of massive star and cluster formation. We select 29 IRDCs, most of which are in the inner quadrants of the Galaxy (see Figure 1). Assuming the IRDCs lie on the near side of the Milky Way (see Section 2.2), they coincide with the Scutum-Centraurus spiral arm, which (in the ﬁrst quadrant) overlaps with the Molecular Ring (Jackson et al., 2008). Also part of our sample are sources discovered in Fig. 2. Histogram of distances of the 45 clouds in the sample in 0.5 kpc bins. The full sample is plotted in the solid gray histogram, the histogram for just the IRDCs is plotted in the green dot-dashed line, and the ISOSS targets are plotted in the red histogram. The median distance of the full sample, 3.2 kpc, is plotted in the solid gray line, and the median for the IRDC-only (3.1 kpc) and ISOSS-only (3.5 kpc) are shown with solid green and red vertical lines. the ISO Serendipity Survey (ISOSS) at 170 µm, which are seen to harbor cold, massive clumps at large Galactocentric distances. We present the ﬁrst comprehensive results of the Herschel PACS and SPIRE imaging survey of 45 massive targets as part of the EPoS survey. The goals of this study are as follows: (1) give a general characterization of the sample based on Herschel data in concert with existing complementary datasets; (2) characterize point sources embedded within the targeted clouds; and (3) connect the point source properties to the overall cloud structure and environment.
Training Data Independent Image Registration With GANs Using Transfer Learning And Segmentation Information<|sep|>Important medical image analysis tasks such as atlas building, and monitoring pathological changes over multiple patient visits have deformable image registration as an essential step. Iterative gradient descent methods used in conventional registration methods are slow in practice while deep learning (DL) methods can be very fast at test time. Most DL based methods rely on large datasets for training. Since it is difﬁcult to obtain ground truth data for registration it restricts the method’s efﬁcacy on new image types and in real world scenarios. A network trained to register a pair of chest xray images does not perform equally well on a pair of brain magnetic resonance (MR) images, or Xray images from other scanners. Although conventional registration methods are time consuming, their performance is consistent across different image types. Thus DL based methods have to be retrained for novel images. In this paper we address this challenge by proposing a DL based method that, once trained on a particular dataset, can be easily used on other image pairs without extensive retraining. A comprehensive review of conventional medical image registration methods can be found in [1]. Previous approaches to DL based image registration involve the use of convolutional stacked autoencoders (CAE) [2], convolutional neural network (CNN) regressors [3, 4, 5], and CNNs with reinforcement learning [6]. These approaches use a conventional model to generate the transformed image from the predicted deformation ﬁeld which increases computation time and does not fully utilize the generative capabilities of DL methods. CNNs trained on simulated deformations were used in [7] while in [8] a parameterized registration function is learned from training data, and does not require ground truth. The above methods are limited by the need of spatially corresponding patches or being too dependent on training data. Generative models can overcome some of these limitations by generating the registered image and the deformation ﬁeld. In previous work [9] we used generative adversarial networks (GANs) for multimodal retinal image registration, and in [10] show the advantages of including segmentation for registration compared to conventional registration. In this paper we build on our previous works and show how segmentation information can be leveraged to design a DL registration method that does not require extensive retraining when used with different datasets. Our primary contribution is in using principles of transfer learning for achieving dataset independent registration. We show that our method, despite being trained on chest Xray images, achieves high performance levels with test images of brain MRI.
A Case for Practical Configuration Management Using Hardware-based Security Tokens<|sep|>Industry 4.0 will change the landscape of factory networks. Various new use cases will require new technologies resulting in more heterogeneity. The amount of networking will increase within the factory as well as with cloud services outside of the factory. Additionally, the long life cycles of industrial machinery will lead to a side-by-side of new and legacy equipment [11]. Factory networks will consist of devices and smart services serving different use cases being provided by many different vendors. The security of these networks will become a top priority, as these systems get connected to the Internet and therefore reachable to malicious actors. Security incidents of the past may serve as warning signs of this general trend. The NotPetya attack may serve as a powerful example [5]. Figure 1: An industrial network with a heterogeneous set of devices. The green encryption gateways protect machine traffic by establishing a secure channel. One approach to improve the security of these heterogeneous networks is deploying encryption gateways or middleboxes in the network [1, 6, 13]. These are put between an industrial machine and the network, as shown in Fig. 1. Secure channels can then be configured between the gateways so that all network traffic between the respective machines is encrypted and protected from the rest of the potentially insecure or even malicious network. However, these gateways then also represent an additional infrastructure that adds another layer of complexity, which has to be maintained and managed. Security management is typically rather complex and therefore error-prone. Configuration errors even happen to expert staff leading to vulnerable networks [2]. Additionally, security measures are mostly seen by non experts as overheads, typically leading to setups, where security measures are configured in such a way that they interrupt the productive work the least. This often leads to configuration rules that do not restrict behavior in any way and hence offer no practical increase in security. This is especially true in the industrial environment, where staff is classically often not trained in IT security. As factory networks will become more dynamic through Industry 4.0related shifts in technology, these problems will only increase in the future. For example, the vision of mass customization down to a lot size of one will demand highly flexible networks, where reconfiguration will be frequent. Any future security scheme must consider this. Consequently, understandability and usability will be key factors to the successful implementation of additional security measures in general and encryption gateways in particular. This paper presents a novel approach on how to securely configure said encryption boxes. We aim for a solution that is very understandable and actionable by staff that is not trained in IT security matters. To this end, we use hardwarebased security tokens, that reduce the configuration of a secure channel to one physical action that does not require further interaction with any software user interface. The remainder of this work is structured as follows. Sec. 2 gives more insights into the importance of usability for security related mechanisms in industrial environment and details our general approach. Sec. 3 surveys some related work. Sec. 4 first motivates our design by presenting some goals, we based our design on, and then presents our scheme. A prototypical implementation of our scheme is presented in Sec. 5. Sec. 6 evaluates the design as well as the implementation. Sec. 7 concludes the paper.
GYES, a multifibre spectrograph for the CFHT<|sep|>The HIPPARCOS mission was highly successful in providing parallaxes and proper motions, but right from the beginning of the project it was realised that the lack of matching radial velocities seriously hampered the study of Galactic structure and dynamics. Unfortunately the proposals to build dedicated ground based telescopes for the radial velocity measurements were not ﬁnanced. Long term programmes on existing facilities have strived to obtain the radial velocities (Fehrenbach et al. 1997 and references therein, Grenier et al. 1999a, 1999b, Nordstr¨om et al. 2004), however a compilation of these and other sources, only amounts to radial velocities for 35 495 stars (Gontcharov 2006), which represents 30% of the Hipparcos catalogue. Building upon this experience, from the beginning of the study of Gaia it was planned that it should include an instrument capable of measuring radial velocities. Since to measure a radial velocity one needs a spectrum, there is a lot more information than radial velocity that can be be extracted from a spectrum: eﬀective temperatures, surface gravities, chemical abundances and rotational velocities. The clever phase A design of the Radial Velocity Spectrometer (RVS, Katz et al. 2004) allowed to obtain this and provide spectroscopic information essentially for all stars down to magnitude 17.5 (Wilkinson et al. 2005). As the design of Gaia became more detailed in phases B and C, the performances of the RVS degraded signiﬁcantly as a result of the result of the trade-oﬀs made by the industrial consortium Astrium and ESA. Under these conditions it is of high interest to perform ground-based obser vations which may complement the informations provided by Gaia, at least for a limited sample of stars. With this in mind we started a study for a high resolution multi-ﬁbre spectrograph, to be placed at the prime focus of the Canada-FranceHawaii telescope. In this respect, GYES is truly a son of Gaia.
A comparative study of attention mechanism and generative adversarial network in facade damage segmentation<|sep|>The deep learning based computer version (CV) concept implies the ultimate expectation of technology development, i.e., to completely replace or even exceed the human eyes’ functionality. Nowadays, state-of-the-art is still far from the goal yet, but CV technology develops by handling tasks step by step. The current CV technology can bring advantages in processing complex images and massive graphical data. Generally, CV usage can be categorized into four categories: classiﬁcation object detection, semantic segmentation, and instance segmentation. The detected object can be rendered in a speciﬁc color in the prediction results from semantic segmentation. This kind of labeling presents more precise information of objects’ sizes positions than object detection. Instance segmentation is a more advanced application to detect each object instance in a photo. 0Abbreviations: CV, Computer Version; AEC, Architecture, Engineering and Construction; GAN, Generative Adversarial Network; CNN, Convolutional Neural Network; ANN, Artiﬁcial Neural Networks; FCN, Fully Convolution Network Due to the outstanding eﬀects of CV, the Architecture, Engineering, and Construction (AEC) industry explores the possibilities of the related applications, especially in the ﬁeld of Structural Health Monitoring. The visible damage in buildings and constructions is supposed to be detected, recognized, and classiﬁed by means of the deep learning based CV. As the ﬁrst step, on-site inspection is executed to collect a structure’s one-hand real-time data. Hence, visual inspection, a conventional, laborious but practical approach, is unavoidable. Nevertheless, the manual inspection approach still has to face several obstacles. The amount of manual data collection could be restricted. The inspectors’ life might be under threat, for instance, if they step into a building damaged in an earthquake or they stand under a sea-crossing bridge. Broader utilization of drones breaks through the limitation of humans and yields an eﬃcient approach to damage data acquisition. A suﬃcient amount of image data can be acquired from on-site inspection using drones. This convenient and eﬃcient approach improves the productivity of data collection productivity and changes the conventional processing methods of inspection results.
Structure of Superheavy Nuclei Along Element 115 Decay Chains<|sep|>Superheavy nuclei at the limit of nuclear mass and atomic number pose a formidable challenge to both experiment and theory. The low cross sections for production of these nuclei, in the picobarn range or less, oﬀer limited structural information. Moreover, the α-decay chains of nuclei synthesized in experiments using a 48Ca beam with actinide targets [1–9] terminate by spontaneous ﬁssion before reaching the known region of the nuclear chart. This poses a problem with the unambiguous identiﬁcation of the new isotopes, and more direct techniques to determine Z and A must be employed [9]. Theoretical predictions of the shell structure of superheavy nuclei are also diﬃcult, as the interplay between the electrostatic repulsion and nuclear attraction, combined with a very high density of single-particle (s.p.) states, make the results of calculations extremely sensitive to model details [10–15]. In a recent experimental study [9, 16], unique structural information on low-lying states in superheavy nuclei below 288115 has been obtained. Of particular interest is the ﬁnding that some of the measured transitions in the nucleus assigned to be 276Mt have E1 character, thus suggesting opposite parities of the connected states. The new data oﬀer an exciting opportunity to constraint theoretical models in this region for the ﬁrst time. Indeed, previous macroscopic-microscopic [17–19] and self-consistent studies [20, 21] have shown that the number of opposite-parity s.p. orbitals around the Fermi level is fairly limited, and this is consistent with the Nilsson model analysis of Ref. [9]. Because of the above-mentioned sensitivity to model details, robust predictions in this region are diﬃcult to make as one is dealing with large extrapolations. To this end, when aiming at reliable predictions, it is advisable to use a model that performs well in the neighboring region where experimental information is more abundant. Furthermore, since the quadrupole deformations of αdecay daughters of 288115 are expected to increase gradually with decreasing Z and A along the α-decay chain [13, 14, 17–21], shape polarization is going to play a role when determining the energies of low-lying states. In this work, we study the low-lying states in the superheavy nuclei below 288115, using the locallyoptimized self-consistent Skyrme Energy Density Functional (SEDF) and Nilsson-Strutinsky (NS) frameworks. To assess the robustness of these results, we also carry out calculations using a globally-optimized SEDF model.
Is the Policy Gradient a Gradient?<|sep|>Reinforcement learning (RL) is a subfield of machine learning in which computational agents learn to maximize a numerical reward signal through interaction with their environment. Policy gradient methods encode an agent’s behavior as a parameterized stochastic policy and update the policy parameters according to an estimate of the gradient of the expected sum of rewards (the expected return) with respect to those parameters. In practice, estimating the effect of a particular action on rewards received far in the future can be difficult, so almost all state-of-the-art implementations instead consider an exponentially discounted sum of rewards (the discounted return), which shortens the effective horizon considered when selecting actions. The policy gradient theorem [25] describes the appropriate update direction for this discounted setting. However, almost all modern policy gradient algorithms deviate from the original theorem by dropping one of the two instances of the discount factor that appears in the theorem. It has been an open question for several years as to whether these algorithms are unbiased with respect to a different, related objective [26]. In this paper, we answer this question and prove that most policy gradient algorithms, including state-of-the-art algorithms, do not follow the gradient of any function. Further, we show that for some tasks, the fixed point of the update direction followed by these algorithms Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020), B. An, N. Yorke-Smith, A. El Fallah Seghrouchni, G. Sukthankar (eds.), May 2020, Auckland, New Zealand © 2020 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved. is pessimal, regardless of whether the discounted or undiscounted objective is considered. The analysis in this paper applies to nearly all state-of-the-art policy gradient methods. In Section 6, we review all of the policy gradient algorithms included in the popular stable-baselines repository [9] and their associated papers, including A2C/A3C [13], ACER [28], ACKTR [30], DDPG [11], PPO [18], TD3 [6], TRPO [16], and SAC [8]. We motivate this choice in Section 6, but we note that all of these papers were published at top conferences1 and have received hundreds or thousands of citations. We found that all of the implementations of the algorithms used the “incorrect” policy gradient that we discuss in this paper. While this is a valid algorithmic choice if properly acknowledged, we found that only one of the eight papers acknowledged this choice, while three of the papers made erroneous claims regarding the discounted policy gradient and others made claims that were misleading. The purpose of identifying these errors is not to criticize the authors or the algorithms, but to draw attention to the fact that confusion regarding the behavior of policy gradient algorithm exists at the very core of the RL community and has gone largely unnoticed by reviewers. This has led to a proliferation of errors in the literature. We hope that by providing definitive answers to the questions associated with these errors we are able to improve the technical precision of the literature and contribute to the development of a better theoretical understanding of the behavior of reinforcement learning algorithms.
Spark Parameter Tuning via Trial-and-Error<|sep|>Spark [10,11] has emerged as one of the most widely used frameworks for massively parallel data analytics. In summary, it improves upon Hadoop MapReduce in terms of ﬂexibility in the programming model and performance [7], especially for iterative applications. It can accommodate both batch and streaming applications, while providing interfaces to other established big data technologies, especially regarding storage, such as HDFS and NoSQL databases. Finally, it includes components for SQL-like processing, graph processing, machine learning and data mining. However, its key feature is that it manages to hide the complexities related to parallelism, fault-tolerance and cluster setting from end users and application developers. This feature renders Spark practical for use in real-life data science and big data processing applications. To support all the features mentioned above, Spark execution engine has been evolved to an eﬃcient albeit complex system with more than 150 conﬁgurable parameters. The default values are usually suﬃcient for a Spark program to run, e.g., not to run out of memory without having the option to spill data on the disk and thus crash. But this gives rise to the following research question: “Can the default conﬁguration be improved?” The aim of this work is to answer the above question in an eﬃcient manner. Clearly, it is practically impossible to check all the diﬀerent combinations of parameter values for all tunable parameters. Therefore, tuning arbitrary Spark applications by inexpensively navigating through the vast search space of all possible conﬁgurations in a principled manner is a challenging task. Very few research endeavors focus on issues related to understanding the performance of Spark applications and the role of tunable parameters [6,1,8]. For the latter, Spark’s oﬃcial conﬁguration3 and tuning4 guides and tutorial book [5] provide a valuable asset in understanding the role of every single parameter. Nevertheless, understanding the role of a parameter does not necessarily mean that the impact of each parameter on the performance of arbitrary applications is understood as well. Moreover, such an understanding does not imply that tuning is straightforward. An added complexity stems from the fact that most parameters are correlated and the impact of parameters may vary from application to application and it will also vary from cluster to cluster. In this work, we experiment with the MareNostrum petascale supercomputer at the Barcelona Supercomputing Center in Spain. After conﬁguring the cluster in an application-independent way according to the results in [8], we examine the impact of conﬁgurable parameters on a range of applications and derive a simple trial-and-error tuning methodology that can be applied to each Spark application separately. We test our methodology using three case studies with particularly encouraging results. The summary of our contributions is as follows. (i) We provide an overview of the known results to date on conﬁguring Spark applications. (ii) We identify the most important parameters in terms of their potential impact on performance and we assess this impact in real runs on the Marenostrum petascale supercomputer. The number of these parameters is 12. (iii) Based on our results, we propose a novel tuning methodology to be applied on an individual application basis (summarized in Fig. 4). The methodology treats applications as black boxes, follows an eﬃcient trial-and-error approach that involves a low number of experimental runs for just 10 diﬀerent conﬁgurations at most, and takes into account the correlation between diﬀerent parameters. (iv) We evaluate our methodology in practice using three case studies and we show that we can achieve signiﬁcant speedups (up to more than 10 times). The remainder of this work is structured as follows. The next section provides an overview of the known results to date with regards to Spark tuning. In Sec. 3, we explain the chosen parameters. The experiments that assess the sensitivity of performance to the parameters under investigation are presented in Sec. 4. Sec. 5 deals with our proposed tuning methodology and the evaluation of its eﬀectiveness. We discuss conclude in Sec. 6.
Random Tight Frames<|sep|>Frames are basis-like systems that span a vector space but allow for linear dependency, which can be used to reduce noise, ﬁnd sparse representations, or obtain other desirable features unavailable with orthonormal bases. They have proven useful in ﬁelds like spherical codes, compressed sensing, signal processing, and wavelet analysis [6, 7, 8, 10, 11, 12, 14, 15, 17, 16, 19, 25]. Tight frames even provide a Parseval type formula similar to orthonormal bases. However, characterizations and constructions of ﬁnite tight frames and ﬁnite unit norm tight frames (FUNTFs) were needed [6]. A general characterization of all FUNTFs was given by Benedetto and Fickus in [2], where they proved that the FUNTFs are exactly the minimizers of a functional called the frame potential. This was extended to ﬁnite tight frames in [30]. Casazza and Fickus have considered the frame potential in the framework of fusion frames [5]. To approximate a FUNTF, Goyal, Vetterli, and Thao considered in [18] n random points on the sphere. In fact, they showed that independent, identically distributed (i.i.d.) points according to the uniform distribution on the sphere asymptotically (as n → ∞) become a FUNTF. The present paper is concerned with frames in a probabilistic setting and the generalization of the results of Goyal, Vetterli, and Thao. Our aim is to allow for a more ﬂexible choice of n points while still preserving the asymptotical tight frame property. We ﬁrst introduce probabilistic frames and adopt many concepts and properties from ﬁnite frames to the probabilistic setting. Probabilistic versions of frames, tight frames, Parseval frames, and FUNTFs are developed. After observing that the uniform distribution on the sphere is a probabilistic unit norm tight frame, we extend the results about the random choice of n points on the sphere as follows: in comparison to [18], we are not limited to the uniform distribution and allow for any probabilistic tight frame. Moreover, the points do not have to be identically distributed nor must they lie on a sphere. This means a signiﬁcant weakening of
Yangian Symmetry for the Action of Planar N=4 Super Yang-Mills and N=6 Super Chern-Simons Theories<|sep|>2 N = 4 sYM and superconformal symmetry 5 2.1 N = 4 supersymmetric Yang–Mills theory . . . . . . . . . . . . . . . . . . 5 2.2 Superconformal algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Notions of symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3 Yangian symmetry of the equations of motion 11 3.1 Algebra and representations . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 Weak invariance of the equations of motion . . . . . . . . . . . . . . . . . . 15 3.4 Strong invariance of the equations of motion . . . . . . . . . . . . . . . . . 17 4 Yangian invariance of the action 19 4.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.2 Level zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.3 Level one . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5 N = 6 supersymmetric Chern–Simons theory 29 5.1 Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.2 Symmetries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.3 Yangian invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
New parallel programming language design: a bridge between brain models and multi-core/many-core computers?<|sep|>We live in a paradox. On the one hand, recent technological advances suggest the possible transition to powerful multi-core/many-core computers in the near future. However, in order to be economically viable, such a major shift must be accompanied with a similar shift in software, where parallel programming should enter the mainstream of programming practice becoming the rule rather than the exception. Brieﬂy, programs eager for more computing power are badly needed. On the other hand, there is a critical view that the promises of AI (Artiﬁcial Intelligence) are still to be fulﬁlled. No matter how much computer power we would have, the critics say, the advances in key AI areas as image recognition or understanding spoken languages will be slow. This means that the current AI approach is, according to critics, faulty. AI already had a major restructuring in the nineties, by adopting the agent-oriented paradigm as a major research topic.1 Jeﬀ Hawkins [6] proposes another restructuring of AI by taking a closer look to the human brain. According to Hawkins, the eﬃcient modelling of the human brain is of crucial importance if we really want to understand why human beings are so powerful on recognizing images or performing other similar tasks for which computers are still notoriously weak. Following suggestions provided by the anatomical structure of the brain, he proposes to use HTMs (Hierarchical Temporal Memories), hierarchical networks of nodes which work together processing continuous ﬂows of data. While most of the pieces have been already used by other approaches (neural networks, associative memories, learning machines, interactive computing models, etc.), Hawkins stresses out the importance of having a unitary, coherent approach. In his view, the interplay between a systematic study of the brain and a creative design approach for developing intelligent machines is the solution to the current AI crisis. Our paper brieﬂy presents HTMs and other key elements of Hawkins model of the brain [6]. Furthermore, it describes the speciﬁc features of a particular architecture called TRIPS (Tera-op, Reliable, Intelligently adaptive Processing System) for multi-core/many-core computers [1]. The main contribution of the paper might be the suggestion that Agapia, a recently proposed language for massively parallel, interactive programs [4, 7], or similar languages, can potentially be a bridge between brain models, such as those using HTMs, and multi-core/many-core computers, particularly those using the TRIPS architectures. To strengthen the suggestion, the paper shows how Agapia programs for modeling HTMs can be developed and sketches an approach for compiling and running Agapia programs on TRIPS computers.
Magnetic Transformations in the Organic Conductor kappa-(BETS)2Mn[N(CN)2]3 at the Metal-Insulator Transition<|sep|>Quasi-two-dimensional organic charge transfer complexes can be visualized as sheets of organic donor/acceptor molecules sandwiched between insulating anion/cation layers.1 Conductivity in such materials is associated with organic layers. For years they have been in focus of extensive research activities because these high-purity materials with relatively simple Fermi surfaces oﬀer rich pressure-temperature-ﬁeld (P–T –B) phase diagrams. As a result of the interplay between electron correlations, the electron-phonon interaction, the electron kinetic energy, and characteristics of the Fermi surface topology, the phase diagram can include metal-insulator (MI) and superconducting (SC) transitions as well as diﬀerent kinds of charge and spin ordering: charge and spin density waves, long-range antiferromagnetic (AF) order, spin glass etc.1,2 Synthesis of radical cation salts of organic π-donors with paramagnetic metal complex anions3–7 has added a new dimension to the physics of organic conductors due to the implications of the interaction between the conduction electrons of the π band with localized d electrons. For example, interaction between localized spins in insulating magnetic layers and itinerant spins in conducting organic layers was found to lead to new fascinating phenomena such as ﬁeld-induced superconductivity observed in λ-(BETS)2FeCl48 and κ-(BETS)2FeBr4,9 where BETS stands for C10S4Se4H8, bis(ethylenedithio)tetraselenafulvalene. The recently synthesized layered conductor κ(BETS)2Mn[N(CN)2]310 is expected to give a thrilling combination of potentially non-trivial magnetic properties arising from the nearly triangular network of Mn2+ ions in the anion layer, with strong electron correlations characteristic of the narrow half-ﬁlled conducting band of organic layers. At ambient pressure, this material undergoes a MI transition at TMI ≃ 25 K, which however can be suppressed by applying a relatively low external pressure, giving way to a superconducting state with maximum Tc = 5.75 K at P = (0.6 − 1.0) kbar.11 Resistivity measurements combined with X-ray studies and electronic band structure calculations11 have suggested the electronic ground state to be a Mott insulator. However, the question about the role of the interaction between itinerant spins in the donor layers and localized spins of Mn2+ in the formation of the insulating ground state is still open. For example, it was believed that in λ-(BETS)2FeCl4 where the MI and the AF transitions coexist,12 the magnetic ordering in Fe3+ subsystem leads to localization of π-electrons of BETS.13 However, recent speciﬁc heat14 and M¨ossbauer15 studies have casted doubt on this viewpoint, suggesting that the magnetic order only exists among the localized π-electrons while the Fe3+ moments stay paramagnetic. To clarify the issue of the π−d interaction, its inﬂuence on the phase diagram of κ-(BETS)2Mn[N(CN)2]3, and to understand the driving force of the MI transition, we have studied its magnetic properties revealed by dc magnetization, magnetic torque, and NMR measurements. In this paper we report results of this investigation, arguing for a magnetic transition caused apparently by the MI transition.
What ignites on the neutron star of 4U 0614+091?<|sep|>Type I X-ray bursts (Grindlay et al. 1975, Belian et al. 1976, Hoﬀman et al. 1978; hereafter bursts) result from thermonuclear shell ﬂashes on a neutron star, which is caused by the ignition of either He and/or H-rich material supplied by a binary companion star (Hansen & van Horn 1975, Woosley & Taam 1976, Maraschi & Cavaliere 1977, Lamb & Lamb 1978; for reviews see Lewin et al. 1993, Strohmayer & Bildsten 2006). Bursts generally appear as short transient events wherein the X-ray intensity rises rapidly on a time scale of seconds, and decays in an exponential fashion back to the pre-burst level. The decay lasts almost always longer than the rise. Burst durations range from several seconds up to half an hour. The burst spectra generally harden during the rise and soften during the decay. This has been attributed to the heating and cooling of the uppermost layers of the neutron star. The spectra can be satisfactorily described by black-body emission from spherical regions with radii of around 10 km at inferred temperatures up to kT≃3 keV. The burst-toburst time intervals are typically of the order of hours to days. They are thought to be determined by the time for the neutron star to accumulate enough fuel to power another burst. Under certain conditions, the local luminosity may reach or exceed the Eddington limit and matter may be pushed outward. As a result, the neutron star photosphere also moves outward. Consequently, the emitting area increases and the observed inferred black-body temperature drops. When the surge of energy release is over, the photosphere gradually returns to its pre-burst radius. During this phase, the emitting area decreases and the inferred temperature increases. During the expansion and con
Control of Optical Dynamic Memory Capacity of an Atomic Bose-Einstein Condensate<|sep|>Soon after the generation of Bose Einstein condensate in an ultracold gas of trapped Alkali atoms [1], a promising utilization of it for the dramatic slowing down of a light pulse was demonstrated [2]. This feat is accomplished with the help of a quantum coherent eﬀect called electromagnetically induced transparency (EIT) [3,4]. Ultraslow light pulses can be used for storage of coherent optical information [5]. Mutual conversion of classical coherent information (phase and amplitude) of the light pulse and the quantum information (quantum state) of the atomic system can allow for quantum information processing via ultraslow light [6]. A quite recent experiment provides strong hope towards this direction [7]. In the experiment, coherent optical information is ﬁrst encoded in one condensate. Quantum information in the condensate is then carried to another condensate by a matter wave. Finally, coherent optical information is revived in an optical pulse, generated out of the new condensate on demand. Due to such impressive developments in experimental ability to control of light and matter waves in light storage experiments, more practical quantum information processing applications can be expected to occur in near future. On the other hand, it is necessary ﬁrst of all, to increase our capability to control the amount of information stored in the condensate. For that aim, we should be able to inject more than one pulse into the condensate during the storage time. This is a basic requirement to realize practical logic gates in atomic condensates using slow light set ups [5,6]. To investigate how to make more than one pulse simultaneously present in an atomic condensate eﬃciently, we have performed a series of studies. This paper reviews some of our earlier results and in addition it reports our new results where we have found optimum conditions for dispersion compensation via nonlinearity. In our earlier works, we show that optical dynamic memory capacity of the condensate can be optimized by choosing a certain set of experimental control parameters, such as coupling laser Rabi frequency and temporal width of the probe pulse for a given atomic condensate [8]. It has been shown that axial density proﬁle of the condensate helps to preserve the probe pulse shape against group velocity dispersion [8]. Further enhancement of the memory capacity can be possible by taking into account radial conﬁnement of the probe pulse. We have demonstrated that a particular radial density proﬁle of the condensate
General risk measures for robust machine learning<|sep|>In machine learning, the robustness of the solutions obtained for classiﬁcation and prediction tasks remains a main issue. In Papernot, McDaniel, and Goodfellow (2016) and Kurakin, Goodfellow, and Bengio (2016), some examples are provided where small modiﬁcations of the input data can completely alter the resulting solution. In Feng, Xu, Mannor, and Yan (2014) and Plan and Vershynin (2013), poor out-of-sample performances are displayed when training data is sparse. This kind of problems also occurs in optimal control when there exist uncertainties on parameters. In Ben-Tal and Nemirovski (2000), the authors showed that a small perturbation on the parameters can turn a feasible solution into an infeasible one. In this context, robust approaches appear as a way of controlling out-of-sample performance. There is an extensive literature dealing with robust problems and the reader is refered to Ben-Tal, El Ghaoui, and Nemirovski (2009) for a survey. One of the main approaches consists of introducing constraints on the probability distribution of the unknown data. Under some conditions, this approach is equivalent to deal with ambiguity sets or a modiﬁed loss function. The works in Ben-Tal, Den Hertog, De Waegenaere, Melenberg, and Rennen (2013); Hu and Hong (2013); Duchi, Glynn, and Namkoong (2016); Moghaddam and Mahlooji (2016) and Namkoong and Duchi (2016) have brought more insight on ambiguity sets. In Esfahani and Kuhn (2015) and Esfahani, Shaﬁeezadeh-Abadeh, Hanasusanto, and Kuhn (2017), the authors present a distributionally robust optimization framework based on the Wasserstein distance. A set of probability distributions is deﬁned as a ball centered on the reference probability with respect to the Wasserstein distance, then the optimization is carried out for the worst cost over this probability set. This idea of minimizing the worst cost over a given probability set is well-known in quantitative ﬁnance. The robust representation of risk measures provides a theoretical framework to do so. A rich class of risk measures is the class of coherent ones which were introduced in the seminal paper by Artzner, Delbaen, Eber, and Heath (1999). In Föllmer and Schied (2016), a broader class of so-called convex risk measures was investigated, for which a large number of results were established. In this paper, we follow the line of Esfahani and Kuhn (2015), which aims at reformulating robust problems using ambiguity sets as convex minimization problems. Our contribution is threefold. First we clarify the links existing between risk measures and robust optimization. This allows us to transpose results from ﬁnance to machine learning. Second, we propose a unifying convex optimization setting for dealing with various risk measures, including those based on ϕ-divergences or the Wasserstein distance. Finally, we propose an accelerated algorithm grounded on the subgradient projection method proposed in Combettes (2003). We show that the proposed algorithm is able to solve eﬃciently large-scale robust problems. The organization of the paper is as follows. In Section 2, we state the general mathematical problem we investigate in the context of machine learning. In Section 3, we ﬁrst draw a parallel between this problem and convex monetary risk measures. We then provide a convex reformulation of the problem. In Section 4, we discuss some important classes of risk measures by revisiting some of the results in the literature. In Section 5, we describe our algorithm for solving convex formulations of robust problems. Then, in Section 6, we illustrate the good performance of the proposed algorithm through numerical experiments on real datasets. Finally, short concluding remarks are made in Section 7.
Lorentz factor distribution of blazars from the optical Fundamental plane of black hole activity<|sep|>Blazars, including both BL Lac objects and ﬂat-spectrum radio quasars (FSRQs), represent the most powerful and highly variable class of active galactic nuclei (AGNs), with ﬂat radio spectra, core-dominated radio morphology and radiation dominated by a relativistic jet oriented close to our line of sight (eg. Blandford & K¨onigl 1979; Urry & Padovani 1995, etc.). Blazar jets can be strongly beamed depending on the relativistic Doppler factor of the source. The relativistic jet of a blazar, at ﬁrst approximation, can be deﬁned by just two intrinsic parameters - the Lorentz factor Γ and the viewing angle θ. Studying the radio jets of these sources is crucial to understand the kinematics of black holes and the laws of physics under extreme conditions. Specially, having information on the fundamental and intrinsic properties of the relativistic jets, like Lorentz factors and viewing angles, will help to constrain the physics of jet launching region, shed light in the formation, collimation, acceleration, and propagation of the jets, kinematically characterize the sources and estimate the underlying intrinsic physical properties of the sources, like their luminosity function (eg. Ajello et al. 2012). But owing to diﬃculties involved in measuring the intrinsic luminosity of the blazar jet, not much is known regarding these intrinsic parameters of blazars, neither individually nor as a population. In this paper, we present a novel method to obtain the distribution of these parameters for a population of blazars. The most common way to calculate the blazar Lorentz factor is by observing the apparent speed of the jet and estimating the Doppler beaming factor. The apparent speed of the jets can be found directly by using Very Long Baseline Interferometry (VLBI) observations (eg. Jorstad et al. 2001; Kellermann et al. 2004; Britzen et al. 2007, etc.). Estimating the Doppler beaming factor is more complicated and L¨ahteenm¨aki & Valtaoja (1999) has compared the diﬀerent methods currently used to estimate it and have found that a typical radio-loud quasar has a Lorentz factor ∼ 10 and a viewing angle < 5◦, while a typical BL Lac object has a Lorentz factor ∼ 5 and a viewing angle < 10◦. Doppler beaming factors can be estimated by calculating the decline of ﬂux with time of a jet component and comparing it to the measured size of the VLBI component (eg. Jorstad et al. 2005, etc.); by combining X-ray observations with radio ﬂuxes at turnover frequencies (eg. Ghisellini et al. 1993; Britzen et al. 2007, etc.) or by observing the
A Type System for the Vectorial Aspect of the Linear-Algebraic Lambda-Calculus<|sep|>A number of recent works seek to endow the λ-calculus with a structure of vector space; this agenda has emerged simultaneously in two different contexts (albeit related [5]). A ﬁrst line of work forked from the study of relational models of linear logic. In [7, 11, 17], various algebraic lambda-calculi, that is, languages with vectorial structures, are considered. These languages are based on an interpretation of intuitionistic logic by linear logic. A second line of work [2, 3, 6] considers linear combinations of terms as a sort of “quantum superposition”. This paper stems from this second approach. In quantum computation, data is encoded on normalised vectors in Hilbert spaces. For our purpose, it is enough to say that a Hilbert space is a vector space over the ﬁeld of complex numbers. The smallest space usually considered is the space of qubits. This space is the two-dimensional vector space C2, and comes with a chosen orthonormal basis denoted by {|0⟩,|1⟩}. A general quantum bit (or qubit) is a normalised vector α|0⟩+β|1⟩, where |α|2 +|β|2 = 1. The operations on qubits that we consider are the quantum gates, i.e. unitary operations. For our purpose, their interesting property is to be linear. The language we consider in this paper will be called the vectorial lambda-calculus, denoted by λvec. It is inspired from Lineal [3]. This language admits the regular constructs of lambda-calculus: variables x,y,..., lambda-abstractions λx.s and application (s)t. It also admits linear combinations of terms: 0, s+t and α ·s are terms. The scalar α ranges over the ring of complex numbers. As in [3], it behaves in
Self-Rule to Multi-Adapt: Generalized Multi-source Feature Learning Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Detection<|sep|>Colorectal cancer (CRC) is one of the most common cancers worldwide, and its understanding through computational pathology techniques can signiﬁcantly improve the chances of effective treatment [Geessink et al., 2019, Smit and Mesker, 2020] by reﬁning disease prognosis and assisting pathologists in their daily routine. The data used in computational pathology most often consists of Hematoxylin and Eosin (H&E) stained whole slide images (WSIs) [Hegde et al., 2019, Lu et al., 2021] and tissue microarrays (TMAs) [Arvaniti et al., 2018, Nguyen et al., 2021] Although fully supervised deep learning models have been widely used for a variety of tasks, including tissue classiﬁcation [Kather et al., 2019] and semantic segmentation [Qaiser et al., 2019, Chan et al., 2019], in practice, it is time-consuming and expensive to obtain fully-labeled data as it involves expert pathologists. This hinders the applicability of supervised machine learning models to real-world scenarios. Weakly supervised learning is a less demanding approach that does not depend on large labeled cohorts. Examples of this approach applied to digital pathology include WSIs classiﬁcation [Tellez et al., 2018, Silva-Rodríguez et al., 2021] and Multiple-Instance Learning (MIL) based on diagnostic reports [Campanella et al., 2019]. However, these methods still need an adequate training set to initialize the learning process, limiting the gain that can be achieved from using unlabeled samples. Self-supervised learning was proposed to address limitations linked to labeled data availability. It involves a training scheme where "the data creates its own supervision"[Pieter et al., 2020] to learn rich features from structured unlabeled data. Applications of this approach in computational pathology include multiple tasks such as survival analysis [Abbet et al., 2020], WSIs classiﬁcation [Li et al., 2021], and image retrieval [Gildenblat and Klaiman, 2019]. Over the years, various large data banks have been made available online containing samples from a variety of organs [Weinstein et al., 2013, Litjens et al., 2018, Veta et al., 2019], such as the colon and rectum [Kather et al., 2016, Shanah et al., 2016a,b, Kather et al., 2019]. This opens up possibilities for transfer learning and domain adaptation. Yet, using these data banks to develop computational pathology-based models for real-world scenarios remains challenging because of the domain gap, as these images were created under different imaging scenarios. A tissue sample’s visual appearance can be heavily affected by the staining procedure [Otálora et al., 2019], the type of scanner used [Cheng et al., 2019], or other artifacts such as folded tissues [Komura and Ishikawa, 2018]. To tackle this issue, color normalization techniques [Macenko et al., 2009, Zanjani et al., 2018, Anand et al., 2019] have been widely adopted. Nevertheless, these techniques solely rely on image color information, while the morphological structure of the tissue is not taken into account [Tam et al., 2016, Zarella et al., 2017]. This could lead to unpredictable results in the presence of substantial staining variations and dark staining due to densely clustered tumor cells. Another ﬁeld of research that aims to improve the classiﬁcation of heterogeneous WSIs is unsupervised domain adaptation (UDA). These methods work by learning from a rich source domain together with the label-free target domain to have a well-performing model on the target domain at inference time. UDA allows models to include a large variety of constraints to match relevant morphological features across the source and target domains. DANN [Ganin and Lempitsky, 2015], for example, uses gradient reversal layers to learn domain-invariant features. Self-Path [Koohbanani et al., 2021] combines the DANN approach with self-supervised auxiliary tasks. The selected tasks reﬂect the structure of the tissue and are assumed to improve the stability of the framework when working with histopathological images. Such auxiliary tasks include hematoxylin channel prediction, Jigsaw puzzle-solving, and magniﬁcation prediction. Another example is CycleGAN [Zhu et al., 2017], which takes advantage of adversarial learning to map images between the source and target domain cyclically. However, adversarial approaches can fall short because they do not consider task-speciﬁc decision boundaries and only try to distinguish the features as either coming from the source or target domain [Saito et al., 2018a]. A further issue is that most UDA methods consider fully-labeled source datasets [Dou et al., 2019] for domain adaptation. However, digital pathology mainly relies on unlabeled or partly-labeled data as the acquisition of fully labeled cohorts is often unfeasible. In addition, recent approaches tend to treat domain adaptation as a closed-set scenario [Carlucci et al., 2019], which assumes that all target samples belong to classes present in the source domain, even though this is often not the case in a real-world scenario. To overcome this, OSDA [Saito et al., 2018b] proposes an adversarial open-set domain adaptation approach, where the feature generator has the option to reject mistrusted or unknown target samples as an additional class. In another recent work, SSDA [Xu et al., 2019] uses self-supervised domain adaptation methods that combine auxiliary tasks, adversarial loss, and batch normalization calibration across the source and target domains. Another domain adaptation framework DANCE [Saito et al., 2020] proposes a universal domain adaptation method to address arbitrary category shifts based on neighborhood clustering on the unlabeled target domain in a self-supervised way. Then, entropy-based optimization is utilized for feature alignment of known categories and unknown ones are rejected, based on their entropy. The recently proposed method SENTRY [Prabhu et al., 2021] uses unsupervised domain adaptation based on selective entropy optimization, in which the target domain samples are selected based on their predictive consistency under a set of randomly augmented views. Then, SENTRY selectively optimizes the model’s entropy on these samples based on their consistency to induce the domain alignment. Finally, some approaches take advantage of multiple source datasets to learn features that are discriminant under varying modalities. In Matsuura and Harada [2020], domain-agnostic features are generated by combining a domain discriminator as well as a hard clustering approach. In this work, we propose a label-efﬁcient framework called Self-Rule to Multi-Adapt (SRMA) for tissue type recognition in histological images and attempt to overcome the issues mentioned above by combining self-supervised learning approaches with UDA. We present an entropy-based approach that progressively learns domain invariant features, thus making our model more robust to class deﬁnition inconsistencies as well as the presence of unseen tissue classes when performing domain adaptation. SRMA is able to accurately identify tissue types in H&E stained images, which is an important step for many downstream tasks. Our proposed method achieves this by using few labeled opensource datasets and unlabeled data which are abundant in digital pathology, thus reducing the annotation workload for pathologists. We show that our method outperforms previous domain adaptation approaches in a few-label setting and highlight the potential use for clinical application in the diagnostics of CRC. This study is an extension of the work we presented at the Medical Imaging with Deep Learning (MIDL) 2021 conference [Abbet et al., 2021]. Here, we provide a more in-depth explanation and analysis of our proposed entropybased easy-to-hard (E2H) learning strategy. Additionally, we reformulate the entropy-based cross-domain matching used by the E2H learning strategy which improves the prediction robustness when dealing with complex tissue structures. Moreover, we also provide the generalization of the previously proposed Self-Rule to Adapt (SRA) framework to multi-source domain adaptation by including an additional public dataset and performing further experiments to assess the model’s performance. Thus, we name this improved framework Self-Rule to Multi-Adapt (SRMA).
A Converse for Fault-tolerant Quantum Computation<|sep|>The idea of a quantum computer was proposed in [2]. The computational advantage of a quantum computer over its classical counterpart was ﬁrst proved in [3], followed by [4]. However, this initial excitement for quantum computers was confronted with a practical issue, noise in quantum gates and buffers. To tackle noise in quantum circuits, the exciting area of quantum fault-tolerance emerged, thanks to the work in [5], followed by [6]–[8] and others. The seminal works in [5]–[8] essentially showed that if the strength of the noise at the gates is below a threshold, an almost accurate quantum computation can be realized. However, this is achieved at the cost of a poly-logarithmic increase in the size of the circuit with respect to the ideal circuit made of noiseless gates. In subsequent works, this polylogarithmic redundancy has been improved. In [9], [10], it was shown that a constant redundancy can be achieved if the noise strength is below a threshold. This naturally raises the question: what is the minimum redundancy requirement? For answering this question, it is necessary to obtain a tight lower bound on the redundancy required for almost accurate computation using noisy quantum gates. Relatively fewer works have been carried along this line compared to error correction schemes. However, as better and better codes are found, the interest in understanding the ultimate limit on redundancy is increasing [1], [11], [12]. In this paper, we obtain a lower bound on the required redundancy for a broad class of noise models. In the practically likely regime of sub-exponential (in input size) depth and sub-linear gate size, [9], [10], our bound is strictly better than the existing bounds in [1], [11]–[13]. This bound is obtained by connecting the fault-tolerant computation problem to a set of ﬁnite blocklength communication problems whose accuracy requirements satisfy a joint constraint. Inﬂuenced by the gradual improvements in the redundancy of the error correcting schemes for quantum circuits [6], [7], [9], [10], improved lower bounds on redundancy were sought. Harrow and Nielsen obtained a threshold of 0.74 for depolarizing noise [13]. In [11], Razborov obtained an improved gate size dependent threshold 1 − 1 g , where g is the gate size, i.e., the maximum number of inputs to a gate. Kempe et al. [12]
Strong Baselines for Complex Word Identification across Multiple Languages<|sep|>Complex Word Identiﬁcation (CWI) consists of deciding which words (or phrases) in a text could be difﬁcult to understand by a speciﬁc type of reader. In this work, we follow the CWI Shared Tasks (Paetzold and Specia, 2016; Yimam et al., 2018) and assume that a target word or multi-word expression (MWE1) in a sentence is given, and our goal is to determine if it is complex or not (an example is shown in Table 1). Under this setting, CWI is normally treated using supervised learning and feature engineering to build monolingual models (Paetzold and Specia, 2016; Yimam et al., 2018). Unfortunately, this approach is infeasible for languages with scarce resources of annotated data. In this paper, we are interested in both monolingual and cross-lingual CWI; in the latter, we build models to make predictions for languages not seen during training. While monolingual CWI has been studied extensively (see a survey in Paetzold and Specia (2017)), the cross-lingual setup of the task was introduced only recently by Yimam et al. (2017b), who collected human annotations from native and non-native speakers of Spanish and German, and integrated them with similar data previously produced for three English domains (Yimam et al., 2017a): News, WikiNews and Wikipedia. For the Second CWI Shared Task (Yimam et al., 2018), participants built monolingual models using the datasets previously described, and also tested their cross-lingual capabilities on newly collected French data. In the monolingual track, the best systems for English (Gooding and Kochmar, 2018) differed signiﬁcantly in terms of feature set size and the model’s complexity, to the best systems for German and Spanish (Kajiwara and Komachi, 2018). The latter used Random Forests with eight features, whilst the former used AdaBoost with 5000 estimators or ensemble voting combining AdaBoost and Random Forest classiﬁers, with about 20 features. In the cross-lingual track, only two teams achieved better scores than the baseline: Kajiwara and Komachi (2018) who used length and frequency based features with Random Forests, and Bingel and Bjerva (2018) who implemented an ensemble of Random Forests and feed-forward neural networks in a multi-task learning architecture. Our approach to CWI differs from previous work in that we begin by building competitive monolingual models, but using the same set of features and learning algorithm across languages. This reduces the possibility of getting high scores due to modelling annotation artifacts present in the dataset of one language. Our monolingual models achieve better scores for Spanish and German than the best systems in the Second CWI Shared Task. After that, we focus on language-independent features, and keep those that achieve good performance in cross-lingual experiments across all possible combinations of languages. This results in a small set of ﬁve language-independent features, which achieve a score as high as the top models in the French test set. Finally, we analyse the annotation of the datasets and ﬁnd some inconsistencies that could explain some of our results. Code for all our models can be found at:
Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks<|sep|>Identifying vehicles through non-overlapping cameras is an important task to assist surveillance activities such as travel time estimation, enforcement of speed limits, criminal investigations, and trafﬁc ﬂow. The vehicle identiﬁcation problem can be formally deﬁned as the process of assigning the same label to distinct instances of the same object as it moves over time in a network of non-overlapping cameras [1]. The remarkable progress of emerging technologies in producing low-cost cameras, capable of acquiring high-deﬁnition images, has made the infrastructure to tackle this problem become pervasive in many cities. Although extensively investigated [2–7], this research problem is far from being solved since several challenges come from the high inter-class similarity, caused by vehicles of the same make, model and/or color that often look exactly the same, see Figure 1(a), vehicles with similar license plate identiﬁers, see Figure 1(b), and from the high intra-class dissimilarity, caused by abrupt illumination changes or camera viewpoints, that makes two instances of the same vehicle have differences, see Figure 1(c). In the remainder of this section, we detail our research problem and the main contributions of this work. The main issue of existing datasets for vehicle identiﬁcation [8–10] is the fact that the authors intentionally redacted the license plate identiﬁer in all images to respect privacy restrictions, and, as explained later, the knowledge extracted from this unique identiﬁer is essential for solving certain difﬁcult matching problems, e.g., the correct identiﬁcation of distinct but visually similar vehicles, as shown in Figure 1(a). However, in some regions/countries, the license plates are linked/related to the vehicle and not to the respective drivers/owners; in other words, in such cases it is not possible to obtain any public information about the vehicle owner based on the license plate. One of the countries where this occurs is Brazil [11], where we collected images to create a novel dataset for vehicle identiﬁcation that contains labeled license plate information. In this work, we consider a road network topology structured as shown in Figure 2, where the rear license plate is legible in most cases – it is worth noting that in some countries/regions, e.g. several states in the United States, the li Fig. 1. Examples of challenging scenarios for vehicle identiﬁcation: (a) similar vehicles with different license plates; (b) similar license plate strings and distinct vehicles; and (c) same vehicle under different lighting conditions. The combination of attributes, e.g. vehicle appearance and textual information from the license plate region, can help to improve the recognition since two similar vehicles may have considerably different license plates and vice versa. cense plate is attached only to the vehicle’s rear. The images are taken from an elevated surveillance camera that records simultaneously multiple road lanes. Each vehicle of interest typically enters the ﬁeld of view through the bottom part of the frame and leaves through the top side. As can be noted, not every vehicle seen in one camera appears in the other. • We introduce a novel dataset, called Vehicle-Rear, composed of high-resolution videos, that, to the best of our knowledge, is the ﬁrst to consider the same camera view of most city systems used to enforce speed limits – i.e., rear view of the vehicles with their license plates legible in most cases; Vehicle-Rear is associated with accurate information about each vehicle: make, model, color and year, as well as the image coordinates of each license plate region and its corresponding ASCII string; Fig. 2. Illustration of the experimental environment setup: a pair of low-cost Full-HD cameras, depicted by red dots, properly calibrated and time synchronized are monitoring two distinct trafﬁc lights on the same street, 546 ft away. The road network is structured in such a way that some vehicles are monitored only by Camera 1, see route B; only by Camera 2, see route C; or by both cameras, see route A. uses the most distinctive and persistent features for vehicle identiﬁcation: coarse-resolution image patches, containing the vehicle shape, feed one stream, while high-resolution license plate patches, with string identiﬁers easily readable by humans (as present in the Vehicle-Rear dataset), feed the other stream. Such multi-resolution strategy helps to minimize the computational effort while it makes possible to capture the essential details for vehicle identiﬁcation; We believe that the creation of a publicly available dataset containing images captured in real-world scenarios and labeled information about both the vehicle and its license plate represents a step forward in designing different approaches to vehicle identiﬁcation, since state-of-the-art algorithms for vehicle identiﬁcation take advantage of only one of these attributes [2, 12, 13]. We hope that our dataset and deep architecture can also be useful for other machine learning problems such as vehicle model identiﬁcation, time-travel estimation, among others. The remainder of this paper is organized as follows. In Section 2, we review the literature on vehicle identiﬁcation. The proposed Vehicle-Rear dataset is described in Section 3. The two-stream architecture is described in Section 4, and the experimental evaluation is reported in Section 5. In Section 6 we discuss some alternative architectures, and in Section 7 we state the conclusions.
Narrow-band anisotropic electronic structure of ReS$_2$<|sep|>The semiconducting transition-metal dichalcogenides (TMDCs) with formula MX2 (M=transition metal, X=chalcogen) have recently attracted much attention for their intriguing optical and electronic properties [1]. These are typically found to depend sensitively on material thickness, where thinning down prototypical compounds such as MoS2 to a single layer drives a crossover from an indirect to a direct band gap semiconductor, accompanied with a dramatic increase in the photoluminescence yield [2–4]. In contrast, recent studies of the group VII TMDCs ReX2 (X=Se, S) revealed a striking resilience of their measured photoluminescence to changing material thickness [5, 6], attributed to a particularlyweak interlayer coupling leading to a monolayer-like electronic structure in the bulk. This opens the exciting prospect to achieve optoelectronic functionality from bulk ReX2 of the form which can only be realised by complex fabrication of single-layer samples and devices in group VI semiconducting TMDCs. Coupled with a pronounced anisotropy in their measured optical and electrical properties [7–10], ReX2 materials are therefore important compounds for expanding the functionality of the TMDCs class, and hold potential for next-generation technologies [11, 12]. The electronic structure underpinning their striking optoelectronic properties, however, remains almost completely unexplored experimentally to date. The crystal structure of ReS2 (space group: P1) is shown in Fig. 1. This can be described via a structural distortion away from the 1T structure that is found for some group V TMDCs such as TaS2 and group X TMDCs such as PdTe2. In the undistorted structure, the transition-metal sits at the centre of edge-sharing octahedra formed by the chalcogen atoms (Fig. 1(a)), with FIG. 1: (a,b) Top and side views of a single layer (one unit cell in height) of an (a) un-distorted and (b) distorted 1T crystal structure, respectively. (c) Overview of the valence band structure as measured by ARPES, showing strong in-plane anisotropy. The surface Brillouin zone is shown as red lines. The bulk and projected surface Brillouin zones are shown in the inset. sition metal site [13]. A Jahn-Teller-like structural distortion causes the formation of zig-zag Re chain-like structures as well as a pronounced out-of-plane buckling of the chalcogen layers (Fig. 1(b)) [5, 14]. The unit cell is approximately doubled in the in-plane directions, and the three-fold rotational symmetry of the parent 1T structure is lost. While the surface Brillouin zone remains almost hexagonal, here we show how the electronic structure becomes strongly anisotropic along diﬀerent in-plane directions, with stripe-like band contours observed in angleresolved photoemission (ARPES) measurements of the valence band constant energy contours (Fig. 1(c)). We will discuss this anisotropy in detail below, and show how it exhibits a striking binding-energy dependence tied to the underlying orbital character of the electronic states.
Deep Learning Based MAC via Joint Channel Access and Rate Adaptation<|sep|>The rapid growing of short-range communication techniques, such as Wi-Fi, Bluetooth, ZigBee, MulteFire and New Radio Unlicensed (NR-U), poses huge challenges to the user experiences due to the coexistence of dense devices on the unmannered unlicensed bands. In such crowed and heterogeneous wireless environments, the conventional design paradigm of medium access control (MAC) protocols, which were originally targeted for homogeneous networks where all the nodes follow a same rule, has been inefﬁcient. For example, observed from [1], [2], the popularly-used carriersense multiple access with collision avoidance (CSMA/CA) MAC protocol of Wi-Fi networks fails to coexist with other MAC protocols, even though with the simplest Time Division Multiple Access (TDMA) MAC. Another issue of traditional MAC design paradigm is that the algorithm associated with some certain function is designed independently of other functions. Taking the rate adaptation concerned in this work as an example, the algorithms of the rate adaptation function are usually designed to adapt to the path loss of wireless channels, and they are working independently of the function of channel access protocol (e.g., CSMA/CA in Wi-Fi). If we shift the paradigm and devise a novel MAC algorithm in a holistic way that joins channel access and rate adaptation, much performance gain could be achieved. To illustrate this point, we present a toy example in Fig. 1, where the green packets are transmitted by the nodes of interest and the red packets are transmitted by other coexisting nodes. Fig. 1 shows the coexistence behavior when the node of interest is a Wi-Fi station (STA) or it is an intelligent STA that uses a learning-based MAC protocol. For the Wi-Fi STA, the channel access protocol is CSMA/CA and the rate adaptation algorithm is IWL [3]. At time t0, the Wi-Fi STA determines to transmit after a period of carrier sensing until the backoff counter decreases to zero. 1Suppose that the IWL selects a modulation and coding scheme (MCS) indexed by 2 for the Wi-Fi STA to transmit. We can see that the collision occurs between the Wi-Fi STA and the coexisting node. By contrast, for the intelligent STA, the collision can be avoided by learning the wireless environment, predicting the transmit behavior of coexisting nodes, and determining when to access the channel along with a properly selected MCS. In Fig. 1, the intelligent STA leans to select the MCS indexed by 5, which has a higher transmission rate than MCS 2, to transmit at time t1. As illustrated in Fig. 1, the key to designing an intelligent MAC algorithm is the capability of learning the transmission behavior of coexisting nodes via observing the wireless environment. This can be achieved by employing the state-of-theart data-driven deep learning (DL) framework. Speciﬁcally, the MAC algorithm is implemented by a deep neural network 1Note that rate adaptation and MCS selection both refer to the task of regulating transmission rate based on the wireless channel quality. Hence, we use them interchangeably in this paper. (DNN) that takes a time sequence of channel sensing results as inputs and outputs a decision of joint channel access and rate adaptation. Fortunately, channel sensing results are always available for free from energy detection modules commonly employed in wireless devices. Therefore, the DNN can be used to infer whether to access the channel and which transmission rate can be used based on the historical channel sensing results. Recently, learning-based approaches were introduced to adjust the parameters of CSMA/CA in [4], [5] and enable channel access prediction in [6]–[9]. Moreover, the authors of [10], [11] proposed learning-based rata adaptation approaches for wireless networks. However, it is worth noting that these works are either merely designed for channel access or just for rate adaptation attempting to improve their respective performances rather than considering them jointly. To ﬁll this gap, we propose a learning-based MAC protocol that achieves joint channel access and rate adaptation. An inherent question arises that how much gain can be achieved from the joint design in realistic scenarios. To answer this question, we evaluate our learning-based MAC protocol using real wireless data sampled from actual wireless environments. We sampled wireless data from Shenzhen University and Shenzhen Baoan International Airport, and open-sourced the data set on github [12]. We then design a DNN and train it using the data from [12]. To the best of our knowledge, we are the ﬁrst to propose a DL-based joint channel access and rate adaptation scheme, and to evaluate its practical performance using real wireless data. The contributions of this work are summarized as follows. i) We propose a DL-based MAC protocol, referred to as DL-MAC, aiming at determining both the channel access and rate adaptation actions. Notably, the proposed DL-MAC exploits the real wireless data captured in the actual wireless environments. ii) As a benchmark for evaluating performance, we develop a global optimal MAC that has a global perspective of future channel conditions to optimize its MAC behaviors. We trained the DNN using the sampled data and experimentally compare its performance with the global optimal MAC and the traditional Wi-Fi MAC. The experimental results show that the performances of our DL-MAC can approach that of the global optimal MAC and outperform that of the CSMA/CA with traditional rate adaptation schemes [3], [13]. Moreover, we perform generalization experiments for DL-MAC, i.e., we train the DNN using the data from a speciﬁc channel and test its performance over other Wi-Fi channels. And the results of the generalization experiments also show that the DL-MAC still outperforms the traditional MAC and approach the global optimal MAC. iii) To deeply analyze the respective gains yielded by channel access and rate adaptation in the joint design, we also develop the DL-based channel access and DL-based MCS selection schemes as the benchmarks. The experimental results reveal that the performance of the joint design outperforms both the performance of DL-based channel access and DLbased MCS selection. In addition, the respective gains of MCS selection and channel access show different behavior in different environments. Speciﬁcally, on a gentle channel condition, i.e., the interference on the channel is relatively small (as in our lab at Shenzhen University), MCS selection has a higher performance gain than channel access does, thus MCS selection is of vital importance to achieve a high throughput in such wireless environments. On the contrary, on a severe channel condition, i.e., the interference on the channel is relatively strong (as in the departure hall of Shenzhen Baoan International Airport), channel access has more signiﬁcant impact on the throughout improvement than the MCS selection does.
Sound-Triggered Collapse of Stably Oscillating Low-Mass Cores in a Two-Phase Interstellar Medium<|sep|>Interactions of molecular clouds in the interstellar medium (ISM) with shock waves (Stone & Norman 1992; Klein, McKee & Colella 1994; Xu & Stone 1995; Nakamura et al. 2006) and turbulence (V´azquez-Semadeni, Gazol & Scalo 2000; Mac Low & Klessen 2004; Li & Nakamura 2006; Nakamura & Li 2007) have been extensively studied in the past decades. The general consensus from these studies is that clouds are most likely to be destroyed by non-radiative shocks due to Kelvin-Helmholtz and Rayleigh-Taylor instabilities, but can be severely compressed leading to gravitational collapse by weak radiative shocks; moreover turbulence generated by proto-stellar winds is capable of self-regulation for star formation. While these results are valid in highly active regions of ISM, such as in giant molecular clouds where nearby proto-stellar winds, supernova explosion and ionization bubbles are at work, the majority part of ISM may on the other hand experience only mild disturbances originated from distant active regions. Here small starless cores of few solar masses in relatively quiescent environments are likely much more abundant in the ISM to contribute to the low-mass end of the stellar initial mass function. In particular, B68, a Bok globule, has been identiﬁed to undergo oscillations (Lada et al. 2003; Redman, Keto & Rawlings 2006), and it is for this population of perturbed low-mass cores that the present work aims to focus. Theoretical studies of low-mass star formation in quiescent environments have historically been analyzed with two diﬀerent model conﬁgurations — the outside-in collapse (Larson 1969; Penston 1969; Hunter 1977) and the insideout collapse (Shu 1977). The outside-in collapse model considers an initial uniform cloud that just exceeds the Jeans mass and the collapse is initiated near the cloud boundary where the collapse front propagates from outside toward the center. The inside-out collapse model on the con
PAC-Bayesian Bounds for Deep Gaussian Processes<|sep|>The Bayesian viewpoint for probabilistic inference is very popular in the statistics and the machine learning communities Rasmussen (2006); Neal (2012); Kingma and Welling (2013). Its ﬂexibility and simple framework are important factors for their success. On the one hand, regarding many applications, Bayesian approaches represent state of the art benchmark methods Al-Shedivat et al. (2017); Salimbeni and Deisenroth (2017), on the other hand, the PAC-Bayesian approach is a powerful way to derive risk bounds for probabilistic models generated from Bayesian modeling and inference. It originates from Shawe-Taylor and Williamson (1997); Catoni (2004, 2007); McAllester (1999b,a). In this paper, the focus is on PAC-Bayesian investigations, that use variational approximations and inference instead of Bayesian inference. Variational approximation and infer ence is a promising approach in the last few years in many ﬁelds. We will focus on recent work of Alquier et al. (2016); Germain et al. (2017); Sheth and Khardon (2017) to derive PAC-Bayesian results for a class of variational stochastic models. More precisely, we derive PAC-Bayesian statements for the DGP models of Damianou (2015); Mattos et al. (2016); Cutajar et al. (2017); F¨oll et al. (2019), which use variational approximations and inference instead of Bayesian inference. Unlike simple GPs, DGPs have proven to be capable of capturing non-stationarity and heteroscedasticity inherent to many modeling problems and applications in practice. The theoretical aspect, regarding generalization properties of DGPs, is until now rather less understood. We can use PAC-Bayesian theory to provide precise answers by a guaranteed upper bound on the generalization error in an unspeciﬁed data-distribution setting with high probability. Therefore, using these statements in practice, we are able to design stochastic models, here DGPs, that have good generalization properties with high probability. In our setting for the supervised learning case of regression, we assume, that the output-data is coming from a multi-variate distribution given the input-data. Moreover, we assume a ﬁxed design scenario, where the input-variables are set by an experimenter. It occurs in many practical applications like in the controlling or the prediction/simulation case and is the standard in regression Deisenroth et al. (2013); Al-Shedivat et al. (2017).
Correlated density-dependent chiral forces for infinite matter calculations within the Green's function approach<|sep|>State-of-the-art ab initio nuclear many-body theories necessarily include both two-body (2B) and three-body (3B) interactions. The importance of three-nucleon interactions was ﬁrst recognized with the pioneering work of Fujita and Miyazawa, which identiﬁed two-pion exchange (TPE) as an essential underlying process [1]. The inclusion of three-nucleon forces (3NFs) becomes mandatory to avoid the underbinding of light nuclei and to elude the saturation of nuclear matter at too-high densities [2]. Several models of 3NFs have been devised in the past few decades [3]. In some of these models, the TPE part of the 3NF has been complemented with additional phenomenological terms, for instance, the repulsive terms appearing in the 3NF Urbana interactions [3]. While their inclusion is empirically reasonable, it can be hard to justify from a theoretical point of view. Consequently, systematic errors become diﬃcult to quantify and the predictive power of the theory is adversely aﬀected. Furthermore, ab initio methods should be constructed considering a uniﬁed description of 2B and 3B interactions, avoiding if at all possible ad hoc terms. In this context, we exploit the consistent framework provided by chiral nuclear forces to study many-body systems with a nonperturbative selfconsistent Green’s function (SCGF) approach [4]. Chiral eﬀective ﬁeld theory (EFT) provides a consistent picture of nuclear interactions; see Refs. [5, 6] for ∗ Present address: Institut f¨ur Kernphysik, Technische Universit¨at Darmstadt, 64289 Darmstadt, Germany and ExtreMe Matter Institute EMMI, GSI Helmholtzzentrum f¨ur Schwerionenforschung GmbH, 64291 Darmstadt, Germany; email address: arianna@theorie.ikp.physik.tu-darmstadt.de † Email address: a.rios@surrey.ac.uk ‡ Email address: artur@ecm.ub.edu recent reviews. The approach is based on chiral perturbation theory, i.e., a low-energy expansion of quantum chromodynamics (QCD) [7]. The expansion is based on a power counting, which generally involves a lowmomentum scale over a high-energy scale, usually taken as the chiral symmetry breaking scale. The expansion is constructed as to be consistent with the symmetries of the underlying quantum theory, QCD. The high-energy physics is incorporated in low-energy constants (LECs), which need to be ﬁt to low-energy hadronic and nuclear structure properties [5, 6]. A tremendous advantage of the resulting expansion is the fact that two- and many-body nuclear forces are organized according to the same consistent power counting. 3NFs, for instance, appear at next-to-next-to-leading order (N2LO) in the chiral expansion, whereas four-nucleon forces only appear at fourth order (N3LO). In this sense, a full calculation using chiral forces should be performed considering all the n-body forces which are present at a given order in the chiral expansion. The inclusion of many-body forces, other than the 2B ones, is unavoidable when dealing with chiral nuclear interactions [8, 9]. Most nuclear many-body formalisms were originally employed with two-nucleon forces (2NFs) alone. The majority of these methods have been extended, in one way or another, to include 3N interactions. For ﬁnite systems, the SCGF approach [10], the Gorkov-Green’s function method [11], the no-core shell model [12, 13], the in-medium similarity renormalization group [14] or the coupled-cluster formalisms [15, 16], have recently been developed to treat 3B forces. One of the motivations underlying all these developments is the modiﬁed shellmodel results of Ref. [17], which demonstrated the importance of 3NFs in the reproduction of the drip line in oxygen isotopes. Recently, nuclear EFT calculations on the lattice have provided solid benchmark results for ab initio calculations in the light to medium mass range [18]. For inﬁnite matter, similar extensions have been devised over the years. In bulk matter the 3B forces have been mostly included in a density-dependent 2B form [8, 19–24]. In particular, calculations using the SCGF method for both symmetric nuclear matter (SNM) and pure neutron matter (PNM), have already been presented in Refs. [25]. The practical advantages of using a densitydependent 2B force constructed from 3NFs are numerous. First, the matrix elements can essentially be precomputed at the start of the calculation if they have been built from uncorrelated averages. Second, and most important, the use of the density dependent force avoids the problem of solving the corresponding Faddeev-type equations. In principle the density-dependent force can be added directly to the original 2NF before performing many-body calculations. However, in diagrammatic calculations, this is not a correct prescription, as we have discussed previously [26, 27]. Chiral forces have been implemented at the 2B and 3B level in perturbative as well as in nonperturbative inﬁnite matter calculations. Perturbative calculations have been performed in both SNM [28] and PNM [21], up to third-order in the ladder diagrams for the former and second order for the latter, using an evolved N3LO 2NF, via renormalization group techniques [9], plus a densitydependent N2LO 2NF. Calculations up to N3LO in the chiral expansion with 2B, 3B and four-body forces have been presented for PNM in Ref. [29]. We note, however, that pure N3LO many-body terms have only been included at the Hartree-Fock level. Up-to-third-order calculations in the energy expansion were presented for PNM in Ref. [30], and later for SNM including particlehole diagrams [31]. Furthermore, second order calculations in many-body perturbation theory have been recently presented at ﬁnite temperature [32]. Nonperturbative calculations using 2B and 3B chiral forces have been carried out with several manybody approaches. Initial Brueckner-Hartree-Fock calculations with chiral interactions found anomalously attractive results [33]. In contrast, satisfactory results in the Brueckner-Hartree-Fock approach have been presented in Ref. [34]. These agree qualitatively with our SCGF analysis, presented in Ref. [26]. The Fermi-hypernetted-chain method as well as the auxiliary ﬁeld diﬀusion Monte Carlo method have been complemented in the 3B sector using a new generation of chirally inspired 3NFs in coordinate space [23]. Coupled-cluster calculations in inﬁnite nucleonic matter have also been performed, exploiting a newly optimized version of the 2NF at third order in the chiral expansion (N2LOopt) [35, 36]. Promising quantum Monte Carlo calculations have been obtained with chiral local 2NF at N2LO [37] and with chiral non-local N2LOopt forces [38]. Furthermore, quantum Monte Carlo simulations of neutrons interacting on the lattice have been recently performed including N3LO 2NFs together with 3NFs at N2LO [39]. We have recently extended the SCGF formalism [4] to incorporate 3B forces from the outset [40]. In short, the extension involves a new diagrammatic expansion, for which several structures can be subsumed using eﬀective interactions. Nonperturbative resummation schemes can be developed, and those employed with 2B interactions can, in general, be extended to account for 3NFs. The many-body method is self-consistent, in that an iterative solution of the Dyson’s equation is needed to ﬁnd a solution for the in-medium propagator. In other words, the Green’s function (GF) which describes a propagating particle through the medium is determined by an interaction which takes into account the medium itself. Conversely, for the construction of the in-medium interaction, the knowledge of the propagator is required. The successful extension of the SCGF formalism to include 3B forces has been implemented in a variety of systems. Recent calculations have found good agreement between theoretical and experimental ground state properties of nitrogen, oxygen and ﬂuorine isotopes [10]. Moreover, the extension to 3B forces has been implemented in the Gorkov-Green’s function method [41] to access midmass open-shell nuclei [11]. Results in ﬁnite nuclei have been obtained using a local version of the N2LO 3B force [42]. In inﬁnite matter, we have presented calculations for SNM [26], analyzing the modiﬁcations introduced by 3B forces on both microscopic and macroscopic properties. In our study, we have conﬁrmed the important role played by 3NFs in the saturation mechanism of nuclear matter. In fact, 3NF-induced repulsion provides more realistic values for the saturation energy and density. Throughout our calculations a non-local 3NF at N2LO is implemented [43, 44]. In our previous publication, however, 3B interactions were incorporated via a density-dependent two-body force based on an uncorrelated average over the third particle. In the following, we explore a natural improvement of this average, calculating a correlated average consistent with the many-body calculation. We provide comparisons with our previous results and with other groups’ calculations. Furthermore, and for the ﬁrst time within the extended SCGF formalism, results for PNM are presented. We will exploit a density-dependent 2NF constructed from a correlated contraction of diagrams appearing at N2LO in the 3B sector [43, 44]. We point out that this average diﬀers with respect to previous approaches [21, 45] in that it takes into account the correlations characterizing the system. In other words, the correlated momentum distribution, obtained at each stage of the iterative many-body calculation, is consistently used to calculate the averaged 3B potential. This density-dependent force will be complemented with the 2NF N3LO EntemMachleidt potential [46]. We are well aware that, because we are using diﬀerent orders in the chiral expansion for the 2B and 3B sector, our calculations are inconsistent in the way they treat chiral nuclear forces. In consequence, we will also present an exploratory study implementing the N2LOopt interaction at the 2B level [47]. This paper is divided as follows. Formal aspects are discussed in Sec. II A, where a brief revision of the extended ladder approximation to perform calculations in inﬁnite matter is presented. Section II B will be mainly dedicated to the analysis of the correlated density-dependent force. Furthermore, we will explore diﬀerent manners of performing the third-particle average. Section III will focus on the study of SNM, to complement the work already presented in Ref. [26]. Section IV will then discuss the case of PNM. We will ﬁnally provide a short summary of the work presented and draw conclusions in the last section. The explicit expressions for the correlated density-dependent 2B force are given in the Appendix.
High-precision molecular dynamics simulation of UO2-PuO2: pair potentials comparison in UO2<|sep|>technology places high demand for the nuclear reactor  materials. One of the most critical parts is the fuel  element, which is made on the basis of actinide-oxide  compounds: UO2 (~95% of all fuel), PuO2, ThO2, etc. [1].  Of great interest is mixed oxide (MOX) fuel, with regard  to the closed fuel cycle, nuclear proliferation and  effective  utilization  of  weapon-grade  plutonium (Pu-239). MOX-fuel of (U,Pu)O2 type have been already  used in some commercial light water reactors (LWR) and  experimental fast breeder reactors (FBR). (~3000K), pressures (~1–10 GPa) and radiation levels  (especially for toxic plutonium) are not always possible,  because all instruments have their limits of durability. In  such extreme conditions, molecular dynamics (MD)  simulation is often the best way to obtain the necessary  information. simulation of such many-electron elements as actinides is  usually performed in the approximations of rigid ions and  pair interactions (RIPI) instead of first principles. In this  case all structural and transport properties of the model  are completely determined by the chosen set of pair  potentials (SPP). (from first principles) calculations or the known  experimental data (empirical potentials). At present, ab  initio calculations of actinide-oxide fuel are performed in  static approximations of density functional theory (see  review in [3]) without particle dynamics and so without taking into account the kinetic (thermal) properties of the  model. Therefore, they do not allow analysis of the  temperature dependences of characteristics. In addition,  the work [4] notes that even with the same  approximations and close numerical parameters a  discrepancy between the results of ab initio calculations  has been quite large so far. parameterization of SPP is improving with the  development of computational tools and refinement of  experimental data. The parameterization has evolved  from the simplest calculations of binding energy,  dielectric, elastic properties [5] [6] and phonon spectra  [7] in the harmonic oscillators approximation to the  lattice statics calculations of the point defects formation  energies [8] and, finally, to self-consistent MDsimulation of temperature dependences, which takes into  account the kinetic (thermal) effects [9] [10] [11] [12]  [13]. suitable SPP lacks of comparative surveys, because  authors of new potentials tend to compare them only with  experimental data and one or two predecessors. The first  (and, to our knowledge, the only) broad comparative  review is the work of Govers et al. in two parts: with  static [14] and MD calculations [15], covering more than  20 SPPs for uranium dioxide (UO2), proposed over the  past 40 years. Unfortunately, in the MD part authors did  not simulate diffusion and the temperature dependences  were measured with a too coarse step of 100–200K and  only up to 3000K (which, in particular, cannot be used as  a source of premelting and melting characteristics  comparison). Besides, the parameters of Yamada’s,  Basak’s and Arima’s potentials were incorrectly  translated by authors from kJ/mol to eV (per molecule) –  with a coefficient of 96.441 instead of the standard  96.485 [16], which also decreased the accuracy (because SPPs are sensitive even to small changes of parameters).  In particular, MD simulations in the review [15] were  performed on a system of 768 ions, for which using for  example Basak-03 potentials with parameters from [14]  instead of the original causes displacement of λ-peaks by  ~100K and deviation up to 0.4*106/K and up to 0.026  kJ/(mol*K) for linear thermal expansion coefficient  (LTEC) and heat capacity temperature dependences. we are aware of only two works [9] [11] (during the  preparation of this article another SPP of Tiwary et al.  was published [17], which are going to be considered in  our future survey of PuO2 and MOX potentials), in which  authors proposed SPPs in RIPI approximation, and there  are no any comparative reviews on this compound. accurate comparison of thermophysical properties of the  most recent and widely used SPPs, giving special  attention to phase transitions (Bredig superionic  transition and melting), mass transport mechanisms  (including self-diffusion of slow-moving cations) and  nanoscopic crystals with surface. Due to the large amount  of interesting results we have divided them into several  publications, and this article focuses on simulation of the  UO2 solid phase. to clarify the general behavior of RIPI model and its  dependence on SPP, system size and boundary conditions  (periodic vs. isolated, i.e. finite crystals with surface  surrounded by vacuum).
Politics of Adversarial Machine Learning<|sep|>All technological work has some political dimension. As Langdon Winner illustrated 40 years ago, a technology’s design, systems, or arrangements can pave the way for certain social or political relations or foreclose speciﬁc possibilities (Winner, 1980). Winner’s most often cited example was the low bridges that crossed over the parkways that went between New York City and Long Island. Although it may seem that bridges with low clearance are not political, Winner explains that there was evidence suggesting that the underpasses were built this way to preclude public buses from using the roads — denying those who relied on public transit, predominantly low-income New Yorkers of color, access to certain public spaces (Winner, 1980; Woolgar & Cooper, 1999). Today, we can speak of the politics of algorithms, which can automate decisions in discriminatory ways (Noble, 2018; Eubanks, 2018; Benjamin, 2019) and spyware, which is used and abused by authoritarian governments to track, suppress, and harm human rights activists and dissidents (Harkin et al., 2019; Penney et al., 2018). Technology has the potential to reinforce or undermine existing power relationships in the context in which it is used, and even technologies or related practices that appear neutral, benign, or even benevolent have potential impacts on civil liberties and human rights. In cryptography, signiﬁcant attention has been devoted to unpacking how particular research directions within the ﬁeld may have implications for who can deploy cryptographic technologies (Rogaway, 2015). There is also signiﬁcant work that takes on these questions in the context of data science, Machine Learning (ML), and algorithmic decision-making more generally (Green, 2019; MacCarthy, 2018). We turn a similar lens on the development of defenses against adversarial attacks on machine learning, exploring how efforts to secure machine learning systems against attacks can have real-world harms that disproportionately fall on those who wish to resist the use of such systems. Our conclusion is that those engaged in security work must understand that securing machine learning systems has consequences for the human rights and civil liberties of the subjects of those systems, consequences that Winner would describe as political. In this paper, we ﬁrst discuss how machine learning system deployments increase the chance that adversarial attacks will be used by the subjects of the systems, who are unlikely to have a full say in their construction or deployment. Second, we provide examples of how adversarial attacks could be used for desirable aims. Securing systems against attack may inadvertently suppress dissent, or foreclose research that aims to shed light on how ML systems harm particular populations. We also discuss how the adversarial arms race may lead to the development and deployment of more invasive forms of surveillance. Finally, we conclude by suggesting directions forward. We draw an explicit connection to spyware, where activists, researchers, and civil society organizations have come together to recommend methods to reduce the harm of surveillance technologies. In this paper, we use facial recognition as a case study to demonstrate the politics of adversarial machine learning. Facial recognition technologies (FRT) have been widely critiqued by scholars and activists (Garvie, 2016; Cyril, 2018), and several municipalities within the United States have banned the use of FRT by local law enforcement (Montgomery, 2019). Most salient to our argument, the harms of FRT usage by authoritarian governments are not theoretical, and the scope and usages of FRT are expanding rapidly (Mozur, 2019; Balaban, 2015). We note that the politics of securing machine learning systems extend beyond FRT discussed in this paper.
Transposing Noninvertible Polynomials<|sep|>Physicists conjectured some time ago that that to each quasihomogeneous (weighted homogeneous) polynomial W with an isolated singularity at the origin, and to each admissible group of symmetries G of W, there should exist two diﬀerent physical “theories,” (called the Landau-Ginzburg A and B models, respectively) consisting of graded Frobenius algebras (algebras with a nondegenerate pairing that is compatible with the multiplication). The B-model theories have been constructed [6, 7, 8, 9, 10] and correspond to an “orbifolded Milnor ring.” The A-model theories have also been constructed [4] and are a special case of what is often called “FJRW theory.” We will not address these in this paper, but in many cases, these theories can be extended to whole families of Frobenius algebras, called Frobenius manifolds. For a large class of these polynomials (called invertible) Berglund-H¨ubsch [3], Henningson [2], and Krawitz [10] described the construction of a dual (or transpose) polynomial W T and a dual group GT . The Landau-Ginzburg mirror symmetry conjecture states that the A-model of a pair W, G should be isomorphic to the B-model of the dual pair W T , GT . This conjecture has been proved in many cases in papers such as [10] and [5], although the proof of the full conjecture remains open. It has been further conjectured that the Berglund-H¨ubsch-Henningson-Krawitz duality transform should extend to large classes of noninvertible polynomials and that Landau-Ginzburg mirror symmetry should also hold for these polynomials. In this paper we investigate some candidate mirror pairs of noninvertible polynomials and show that many obvious candidates for mirror duality cannot satisfy mirror symmetry. To approach this problem, we study the A and B models as graded vector spaces and inspect how the symmetry groups act on these spaces. Insight into this problem will not only generate new results for Landau-Ginzburg mirror symmetry, but will also be interesting from a purely algebraic standpoint as a result about groups acting on graded algebras. One case of mirror symmetry that has been veriﬁed for all invertible polynomials is when the A-model is constructed from an invertible polynomial W with its maximal group of symmetries and the B-model is constructed from the corresponding transpose polynomial with the trivial group of symmetries. This is sometimes denoted AW,Gmax W ∼= BW T ,{0}. This intuition stemming from invertible polynomials motivated two conjectures about isomorphisms between A and B models built from noninvertible polynomials. We often refer to polynomials for which the A and B models exist as admissible. Conjecture 1. For any admissible (not necessarily invertible) polynomial W in n variables, there exists a corresponding admissible polynomial W T in n variables satisfying AW,Gmax W ∼= BW T ,{0}. Note that this conjecture includes the collection of noninvertible polynomials, which are allowed to have more monomials than variables. In Section 3.1 we show that this conjecture is false. By relaxing the restriction on the number of variables that W T is allowed to have, we obtain a second conjecture. In Section 3.2 we look at an example of a particular noninvertible polynomial, and expand our search space for ﬁnding a suitable W T . We develop some formulas and show that they rule out the existence of W T in a few more cases that were not considered in Conjecture 1. Thereby we also establish that Conjecture 2 is unlikely to be true in general.
A proposed atom interferometry determination of $G$ at $10^{-5}$ using a cold atomic fountain<|sep|>The Newtonian gravity constant G can be considered the Mt. Everest of precision measurement science [1]. After more than two centuries since the original determination performed by H. Cavendish, experiments based on a torsion balance or torsion pendulum still provide the values with the lowest degree of uncertainty (∼ 20 ppm). However discrepancies of several standard deviations between independent measurements are still present [2, 3]. One of the reasons that could explain such situation lies in the inherent diﬃculty to take fully under control mechanical inﬂuences on the so far employed macroscopic probes. Cold atom interferometry has proven to be a powerful and alternative tool for measuring inertial forces [4]. The success of this method relies on the fact that the atomic probe is a microscopic quantum object in free fall that can be precisely controlled and manipulated through frequency-stabilized laser radiation. These features are essential to identify the systematic errors that have proved elusive in previous experiments. However, no metrologically signiﬁcant G values have been produced until now. The explanation for this lies in the fact that in a G determination the gravitational force probed by the atoms is not uniform over the interferometric region. Therefore, a deep characterization of the atomic sample size, trajectory and temperature is required, placing a limit on the ﬁnal accuracy of the measurement. To partially solve the problem, a viable option is to employ a set of dense source masses to create stationary points along the vertical acceleration proﬁle. In this case it is possible to identify a position ¯z of the atomic cloud apogee such as where φ is the phase shift of a single interferometer. Thanks to this strategy, a determination of G at the 150 ppm level has been realized [5] using an 87Rb cold atom gravity gradiometer, which consists of two vertically displaced, simultaneous Raman Mach-Zehnder interferometers. According to the error budget reported in [6], the systematic uncertainty on G due to an error of 0.1 mm on clouds vertical positions is 5 ppm, while the same error on the cloud vertical size (∼ 6 mm of diameter) produces a much larger shift of 56 ppm. To improve the accuracy by one order of magnitude, the clouds needs to be enclosed in a volume of ∼ 1 mm3 during the ballistic ﬂight. In principle, this can be done using an ultra-cold atomic source [7]. However, bearing in mind the typical gradiometer scheme, it is technically challenging to produce a pair of ultra-cold samples and routinely place them with a spatial resolution below 100 µm. Towards this purpose, schemes based on interferometers trapped in optical lattices have been proposed and experimentally demonstrated [8]. However, they have not yet reached the required maturity for metrological applications. An alternative way to overcome this type of geometric limitation with a traditional cold-atom gravity-gradiometer can be found following the method suggested in a recent work of A. Roura [9]. Here, it has been demonstrated that changing the module of the Raman keﬀ vector by a certain amount ∆keﬀ at the central π pulse of a Mach-Zehnder sequence, a phase shift equivalent to the one induced by a ﬁctitious gravity gradient Γ∗ zz is generated. In particular the following relation holds: where T is the free evolution time between the central π pulse and the π/2 pulses. Therefore, with a proper choice of ∆keﬀ we can compensate the real gravity gradient Γzz probed by the atoms. This matching condition can be easily found by experimentally requiring where Φ is deﬁned as the diﬀerence between the upper and lower interferometer phase. Notably, such zero phase shift condition holds regardless of the distance between the atomic samples d and their diﬀerential velocity ∆vz. This method can also be used to control the systematics in high precision tests of the equivalence principle with atoms in free fall and, more generally, to measure the average gravity gradient experienced by two atomic clouds independently of their relative distance and velocity. Strictly speaking, a geometry-free gravity gradient determination can be realized when the probed acceleration proﬁle a(z) is perfectly linear, or, more generally, if it is possible to ﬁnd a condition where z2 and z1 are the coordinates of the clouds apogee. When applied to a determination of G, this method completely redeﬁnes the experimental strategy. In this paper a novel experiment for measuring G with a relative precision of 10 ppm with atom interferometry is proposed and discussed. It will be shown that the target uncertainty can be easily achieved using a cold atomic fountain of alkali atoms, a technology widely implemented in several metrological institutes worldwide. The manuscript is organized as follows: in section 2, a principle scheme of the experiment is brieﬂy described. In section 3 requirements in term of statistical uncertainty and sensitivity are reported. In section 4 the main systematic eﬀects are quantitatively discussed. Finally, in section 5, conclusions and prospects for an experimental determination of G at a level of 10−6 are presented.
One-Dimensional Traps, Two-Body Interactions, Few-Body Symmetries: I. One, Two, and Three Particles<|sep|>the trap dependence of these amplitudes: (a) t/u = 2.9; (b) t/u = 1; (c) t/u = 0.3. The idea is that (a) for double wells tunneling in the middle is suppressed so t > u; (b) for inﬁnite square well the potential is uniform inside the trap and so (for low particle density) t and u are approximately the same; (c) for softer wells, there is more phase space in the middle of the well so u > t. For harmonic wells, t/u ≈ 0.762 (c.f. [29; 30; 37; 39]). any N given the one-particle spectrum [9; 10; 12; 13; 14; 16; 17; 18; 19; 32; 34]. The question of how
The singularity theorems of General Relativity and their low regularity extensions<|sep|>The singularity theorems of General Relativity (GR) are commonly counted among the 20th century milestones in mathematical physics. They comprise a body of rigorous results in Lorentzian diﬀerential geometry that, under physically reasonable conditions, imply the occurrence of a “singularity” in the sense of causal geodesic incompleteness of the spacetime manifold. At the time of its appearance, the ﬁrst singularity theorem proved by Roger Penrose in [Pen65] came as a surprise to the community, debunking the widely held belief that the singularities encountered in many of the exact solutions of GR were a mere artefact of their high degree of symmetry. It has even been argued e.g. in [Sen12, Sec. 15.1] that Penrose’s theorem, that appeared exactly 50 years after the birth of GR, was the ﬁrst true post-Einsteinian contribution in the sense that it was not foreseen by its founding father. And indeed, it is this paper that won Roger Penrose (one half of) the 2020 Nobel Prize in Physics. Despite its brevity of less than three pages, it not only shaped the way we now primarily think of singularities—via geodesic incompleteness—but also put forward the fundamental new idea of a trapped surface, which has stimulated mathematical GR up to the present day. The theorem itself roughly says that, in a situation that physically amounts to the formation of a black hole—intuitively a region in spacetime of so strong gravity that not even light can escape—there necessarily exists an incomplete null geodesic, i.e., a light ray that ends suddenly1. A short time later, Stephen Hawking realised that one could apply a similar reasoning to an everywhere expanding universe and showed that in such a spacetime there has to be a timelike geodesic that is past incomplete. This theorem, known as the Hawking singularity theorem [Haw67], is generally thought of as mathematical evidence for the occurrence of a big bang. Then, during the next couple of years, Robert Geroch, George Ellis and others helped to shape a body of results which we now call the classical singularity theorems—for some historical details see e.g. [SG15, Sec. 5]. Notably, in 1970 Hawking and Penrose, in their only joint paper [HP70], proved the most reﬁned of these results which is known as the Hawking-Penrose singularity theorem. Over the decades the singularity theorems have not only become an integral part of GR, but are still an area of active research, cf. e.g. [SG15, Sec. 8]. There is, of course, an extensive textbook coverage available and we only mention the early classics [Pen72, HE73], and the more mathematically oriented standard accounts [BEE96, O’N83, Cla93, Kri99], as well as the review articles [Sen98, SG15, MAS15, Sen21, Daf21, Lan21], the latest three of which have appeared on the occasion of the 2020 Nobel Prize. In this work we oﬀer a guided tour for the mathematically oriented reader to the classical singularity theorems and to their recent extensions for Lorentzian metrics of low regularity. The study of the latter is motivated by the discussion of the “character of the singularities” (cf. [HE73, Sec. 8.4]) predicted by the theorems, which we will detail after presenting the classical results. In some more detail we brieﬂy collect the necessary preliminaries on GR and Lorentzian geometry in Section 2, in a way especially suited to the thread of this account. In Section 3 we discuss the main classical theorems, namely the one of Penrose, the one of Hawking, and the one of Hawking-Penrose, together with the main arguments that enter their proofs. In particular, we will discuss the focusing of geodesics under certain curvature and other geometric conditions as well as the relevant parts of Lorentzian causality theory. Then, in a brief analysis of the conditions and statements of the theorems, we motivate the quest for their low regularity extensions. In Section 4 we then present an overview of these results which were obtained in the last couple of years, and, again provide some key insights on the techniques and arguments employed in their proofs. Finally, in Section 5 we provide a summary, some conclusions, and an outlook to current and future lines of research in the direction at hand. 1It has to be pointed out, however, that the theorem—quite contrary to its widespread folklore transcript—does not say that black holes form in gravitational collapse, see e.g.[Sen21, Sec. 4a] for details. In fact, it is here where Penrose’s cosmic censorship hypotheses originate, for more details see e.g. [Lan21].
Global hot-star wind models for stars from Magellanic Clouds<|sep|>The radiative force inﬂuences various types of astrophysical objects on diﬀerent spatial scales. Because the radiative force acts selectively on individual species, it depends on their abundances, and consequently also on metallicity. The metallicity dependence of radiatively driven hot-star winds leads to the absence of line-driven outﬂows in metal-free Pop III stars (Krtička & Kubát 2006), while Galactic stars lose a signiﬁcant part of their mass due to the winds (e.g., Keszthelyi et al. 2017). Besides metallicity, hot-star wind mass-loss rates depend also on other basic stellar parameters (luminosity, mass, and radius). While dependence of hot-star wind on most of the stellar parameters can be observationally studied using a local stellar sample (e.g., Puls at al. 2006), similar study of metallicity dependence is more complicated. Besides ultraviolet satellites, observational study of mass loss at a fraction of the solar metallicity in the Magellanic Clouds requires large telescopes. It was possible to extend the range of studied metallicities using spectroscopic observations of stars residing in galaxies of the Local Group (Tramper et al. 2011, Herrero et al. 2012). However, hot stars with metallicity below about one tenth of the solar metallicity are still observationally unattainable for a detailed wind study. The most complete picture of the dependence of wind properties on the metallicity can be therefore obtained from theoretical models. Such models are able to predict the dependence of basic wind properties on metallicity. The most important wind parameter is the mass-loss rate ˙M, that is the amount of mass lost by the star per unit of time. Vink et al. (2001) predicted that for a broad range of metallicities Z (given by the mass fraction of heavier elements) the mass-loss rate varies as ˙M ∼ Z0.69 in O and early B stars. A more complex dependence of the massloss rate on metallicity was found by Gräfener & Hamann (2008) for late-type WN stars. Petrov et al. (2016) calculated metallicity-dependent wind mass-loss rates from the balance between the radiative energy deposited in the wind and the energy required to lift up the wind material. The latter calculations based on the CMFGEN code predicted that the metallicity dependence of the massloss rate in luminous B stars is a complicated function of stellar eﬀective temperature and is the strongest around Teﬀ = 17 500 − 20 000 K. Study of stellar winds at very low metallicities is important for understanding stellar evolution and stellar feedback in the early Universe and in dwarf galaxies from the Local Group. The gravitational-wave source GW150914 is also expected to originate from low-metallicity environment (Abbott et al. 2016). Because observations of stars in such low-metallicity environments are not always possible, the numerical models may provide missing information about the wind physics. To reach this goal, the current models have to be tested against observations for as broad a range of metallicities as possible. However, testing of wind models at low metallicities is complicated by the mismatch between individual massloss rate determinations. Even at Galactic metallicity the estimates based on the X-ray spectroscopy (Cohen et al. 2014, Rauw et al. 2015), combined optical and UV analysis (Bouret et al. 2012, Šurlan et al. 2013), and near-infrared line spectroscopy (Najarro et al. 2011) seem to point to lower mass-loss rates than predicted by Vink et al. (2001). On the other hand, the results based purely on the Hα emission lines (Mokiem et al. 2007b) can be reconciled with theoretical predictions of Vink et al. (2001). Moreover, a multiwavelength analysis of Shenar et al. (2015) shows good agreement with theoretical models. Consequently, the existence of the discrepancy between empirical and theoretical mass-loss rate determinations is not unanimously accepted. The discrepancy may be even nonmonotonic, because massive stars at the top of the main sequence show enhanced mass-loss rates with respect to theoretical models (Bestenlehner et al. 2014). Part of these discrepancies may be connected with the inﬂuence of inhomogeneities on the observational mass-loss rate diagnostics (Sundqvist et al. 2010, 2011, Šurlan et al. 2012, 2013) and on theoretical predictions (Muijres et al. 2011, Sundqvist et al. 2014). On the other hand, wind blocking in global (uniﬁed) wind models leads to reduction of predicted mass-loss rates in agreement with some observational estimates (Krtička & Kubát 2017). These massloss rates taken from global wind models are typically lower than Vink et al. (2001) and Pauldrach et al. (2012) massloss rates by a factor of 2–5, however they are consistent with predictions based on CMFGEN (Bouret et al. 2012, Puebla et al. 2016, Petrov et al. 2016) and PoWR codes (Gräfener 2003, Sander et al. 2017). Here we apply our global wind models (Krtička & Kubát 2017) to stars from Magellanic Clouds.
State selective detection of hyperfine qubits<|sep|>Quantum information processing can be divided into three steps: (i) the preparation of the system in a well deﬁned state, (ii) the controlled time evolution of the system to carry out a desired algorithm, simulation or precision measurement, and, (iii) the readout of the quantum system. State selective detection is a key ingredient for quantum information processing, not only necessary for readout, but also to verify the preparation of a system, to characterize the performance of quantum gates, or to perform an error correction algorithm. In ion traps state dependend scattering of resonance ﬂuorescence is used for state selective detection. In this article we consider state selective detection of two internal ionic states labeled |d⟩ and |b⟩). Laser light drives a transition between one state of the qubit (the so called bright state) and a third fast decaying energy level. This leads to resonance ﬂuorescence, if the ion was initially in the bright state. If the ion was initially in the other qubit state (dark state), no light, or only a small number of ﬂuorescence photons is measured. This method for state selective detection can give rise to quantum jumps [3, 4, 5]. The simplest way to discriminate between the bright and dark states of an ion is the threshold method: if more than nc photons were registered during the measurement time tb we assume that the ion is bright, otherwise we assume it is dark. Due to the fact that a bright ion scatters photons only with a certain probability and dark states are not perfectly dark, due to background light not scattered by the ion and dark counts, statistical errors occur. This statistical error can be reduced by longer measurement times. However, the ion can change its state during the measurement which leads to additional systematic errors. These errors usually increase with longer measurement times. In the context of this article we refer to a measurement of a qubit’s state when resonant light is directed at the ion and an attempt is made to register ﬂuorescence. The detection of the qubit state may, however, involve more than one measurement and also additional coherent manipulations of the ionic internal states. When using the threshold method outlined above, the words “measurement” and “detection” have identical meaning. Several detection schemes were proposed and implemented to improve qubit detection by state selective resonance ﬂuorescence. For example, Myerson et al. [1] divided the total measurement time tb into several sub-bins of duration ts and calculated the probabilities pB (pD) that the measurement sequence is the result of an initially bright (dark) ion. A comparison of both probabilities reveals the more probable one, which determines the detection outcome. We call this method the time-resolved detection method. It can also be applied to read out multi-qubits [6]. Another detection scheme was proposed and implemented by Hemmerling et al [2] . They apply a π-pulse to the qubit states inverting their population after a ﬁrst measurement followed then by a second measurement. Only results are kept with opposite results of the ﬁrst and second measurement, all other results are discarded. Diﬀerent methods to readout a single measurement can be used when applying this detection technique. However, here the threshold method seems to provide an advantage compared to the time-resolved method. In general, detection methods that discard doubtful results, such as this π-pulse method, have a higher failing probability (probability to get no answer or an incorrect answer) than detection methods that always provide an answer. Note that, by assigning randomly a state to measurement results that were discarded, we obtain by chance some correct answers, which increases the overall probability to get a correct answer (success probability). Nevertheless, for some scenarios, the probability that the given answer is right is more important than the average probability of success. Both methods were designed for qubits in the optical regime such as 40Ca+where the dark state can be transferred to the bright state via spontaneous decay, and the bright state is stable. As a consequence, only a single state change (from dark to bright) is possible. Therefore, the state of an initially bright state is ﬁxed and the time dependent state of an initially dark ion can be described with a single parameter: the time t at which the ion changes its state. The present study was done in view of the widespread use of hyperﬁne qubits (for example, 9Be+ [7], 43Ca+ [8, 9], 137Ba+ [10],171Y b+ [11, 12, 13, 14, 15, 16]) where the analysis of the measurement process is more complicated. Hyperﬁne qubits can change during the detection process from the bright state (for 171Y b+ the state S1/2, F = 1 as shown in ﬁgure 1) to the dark state (S1/2, F = 0) and vice versa via oﬀ-resonant excitation and subsequent spontaneous decay. This leads to an increased number of parameters (times tj at which a state change takes place), due to the fact that not only one, but many state changes may occur. Furthermore, when using time resolved measurements for detection, the photon-number distributions of individual time sub-bins are not independent of each other. As a consequence, the total probability of a measurement sequence is not given by the product of the probability distributions of the single sub bins. One way to deal with this problem is to draw a decision tree and sum up the probabilities of all possible paths, which was done in [2]. However, in the case of several possible state changes, this leads to complicated formula: Assuming only a single possible state change per sub bin for M sub bins in total already leads to 2M terms. As a consequence, we will not follow the calculation in Ref. [2] directly but we will use hidden Markov models instead, similar to Ref. [17]. In this way, the calculations can be performed in an eﬃcient way. In this paper, we generalize the ideas of Ref. [1] and Ref.[2] to two-level systems that allow several state changes during the measurement. Furthermore, we give an eﬃcient expression to calculate the probability of a sequence of measurements starting with the probability distributions of single measurements. We apply this result to simulated measurement events and to experimental data obtained with trapped 171Y b+ ions. The paper is organized as follows: in section 2 we develop the mathematics necessary to generalize the time-resolved method to ions with several possible state Figure 1. Level structure of 171Y b+: levels S1/2, F = 0 and one of the Zeeman states S1/2, F = 1 form the hyperﬁne qubit. The transition from S1/2, F = 1 to P1/2, F = 0 (red arrow) is used for ﬂuorescence detection and doppler cooling. Through oﬀ-resonant excitation to P1/2, F = 1 and spontaneous decay (dashed arrows), the ion can change from the bright state S1/2, F = 1 to the dark state S1/2, F = 0 and vice versa. changes during the measurement sequence. Then, we apply the detection scheme to a simulation of the hyperﬁne qubit of the 171Y b+ ion followed by the description of the experimental realization of the improved time-resolved method to trapped 171Y b+ ions. We ﬁnish section 2 with a comparison of the improved time-resolved detection method developed in this paper with the original one. In section 3 we generalize the π-pulse method in a similar way. Then we apply the generalized π-pulse method to simulate detection of the hyperﬁne qubit in 171Y b+. We ﬁnish this section by comparing the generalized π-pulse method to a threshold method with two thresholds.
Bipartite Mixed Membership Distribution-Free Model. A novel model for community detection in overlapping bipartite weighted networks<|sep|>Complex networks are ubiquitous in our daily life. For example, food networks Dunne et al. (2002), social networks Palla et al. (2007); Pizzuti (2008); Bedi and Sharma (2016), coauthorship and citation networks Newman (2004a); Ji and Jin (2016); Ji et al. (2022), and biological networks Barabasi and Oltvai (2004); Guimera and Nunes Amaral (2005); Notebaart et al. (2006); Rubinov and Sporns (2010); Su et al. (2010). Wonderful reviews of complex networks and technique literature have been provided by Newman (2003); Goldenberg et al. (2010). Community detection is one of the most powerful tools in learning latent structure of complex networks. The main goal of community detection is to ﬁnd group of nodes. Usually, it is assumed that nodes have more links within communities than across communities for assortative networks, and nodes have more links across communities than within communities for dis-assortative networks Newman (2002). For networks in which some nodes have weak intra-connection, some nodes have strong intra-connection, some nodes have weak inter-connection, and some nodes have strong interconnection, a null statistical model is used to model such networks. Generally speaking, for static networks considered in this article, they mainly consist of four cases: un-directed un-weighted networks, directed un-weighted networks, un-directed weighted networks, and directed weighted networks. We will ﬁrst brieﬂy introduce representative statistical models for these four cases and then introduce our main contributions. For community detection of un-directed un-weighted networks, it has been widely studied for decades Fortunato (2010); Papadopoulos et al. (2012); Fortunato and Hric (2016); Javed et al. (2018); Jin et al. (2021); Fortunato and Newman (2022). The Stochastic Blockmodel (SBM) Holland et al. (1983) is one of the most popular generative models to describe community structure for such networks. SBM assumes that the probability of a link between two nodes is determined by node communities. Beneﬁting from the block structure, SBM can describe community structure for networks more than assortative and dis-assortative networks. From now on, we mainly focus on SBM-based models in previous literature, where SBM-based models mean that models are extensions of SBM. Though SBM is mathematically simple and easy to analyze, it always performs poorly on realworld networks since SBM assumes that nodes in the same community have the same expected degree. The Degree-Corrected Stochastic Blockmodels (DCSBM) Karrer and Newman (2011) was proposed to address this limitation by considering the node heterogeneity parameters. To detect communities of networks generated from SBM and DCSBM, substantial works have been proposed in recent years, including algorithms designed by maximizing the likelihood Karrer and Newman (2011); Bickel and Chen (2009); Zhao et al. (2012), methods for optimization via low-rank approximation Le et al. (2016), methods via convexiﬁed modularity maximization Chen et al. (2018), and spectral clustering algorithms Rohe et al. (2011); Qin and Rohe (2013); Jin (2015); Lei and Rinaldo (2015); Joseph and Yu (2016). For a comprehensive review of recent developments about SBM, see Abbe (2017). One main limitation of SBM and DCSBM is, they assume that each node only belongs to a sole community and does not allow overlapping scenarios. The classical Mixed Membership Stochastic Blockmodels (MMSB) Airoldi et al. (2008) was proposed as an extension of SBM by allowing nodes to belong to multiple communities. The Degree-Corrected Mixed Membership (DCMM) model introduced in Jin et al. (2017) can be seen as an extension of MMSB by considering node heterogeneity parameter, and it can also be seen as an extension of DCSBM from non-overlapping networks to overlapping networks. The Overlapping Continuous Community Assignment Model (OCCAM) Zhang et al. (2020) equals DCMM, the Stochastic Blockmodel with Overlaps (SBMO) Kaufmann et al. (2018) is a special case of DCMM, and the Overlapping Stochastic Block Models (OSBM) Latouche et al. (2011) is interesting because it assumes that the probability of generating an edge between two nodes depends on a product of exponential function and logistic sigmoid function. To estimate mixed memberships of networks generated from MMSB and DCMM, some methods are proposed, including MCMC Airoldi et al. (2008), variational approximation method Gopalan and Blei (2013), nonnegative matrix factorization inference methods Ball et al. (2011); Psorakis et al. (2011); Wang et al. (2011, 2016); Mao et al. (2017), tensorbased method Anandkumar et al. (2013), and spectral methods Jin et al. (2017); Mao et al. (2018); Zhang et al. (2020); Mao et al. (2020). For a general review on overlapping community detection, see Xie et al. (2013). For community detection of directed un-weighted networks, it has been widely studied more than ten years Leicht and Newman (2008); Malliaros and Vazirgiannis (2013). Some interesting and meaningful SBM-based statistical models have been proposed for such networks in recent years. Rohe et al. (2016) proposed Stochastic co-Blockmodel (ScBM) and Degree-Corrected Stochastic co-Blockmodel (DCScBM) to model directed un-weighted networks in which nodes have no mixed memberships. ScBM and DCScBM can be seen as direct extensions of SBM and DCSBM from un-directed un-weighted networks to directed un-weighted networks, respectively. Especially, both ScBM and DCScBM allow row nodes to be diﬀerent from column nodes, so they can model bipartite un-weighted networks. Spectral algorithms with theoretical guarantees on consistent estimation and one algorithm designed by co-clustering via NMF have been designed to estimate groups of nodes under ScBM and DCScBM, see algorithms proposed in Rohe et al. (2016); Zhou and A.Amini (2019); Wang et al. (2020). Similar to SBM, ScBM also can not model mixed membership networks. The Directed Mixed Membership Stochastic Blockmodels (DiMMSB) Qing and Wang (2021) was proposed to address this limitation by allowing both row and column nodes to belong to multiple communities in a bipartite un-weighted network. DiMMSB can be seen as a direct extension of MMSB from un-directed un-weighted networks to directed un-weighted networks, and it can also be seen as an extension of ScBM from non-overlapping directed un-weighted networks to overlapping directed un-weighted networks. To estimate memberships under DiMMSB, Qing and Wang (2021) designed a spectral algorithm with a theoretical guarantee of estimation consistency. For community detection of un-directed weighted networks, it has been an interesting topic in recent years. Edge weights are important and meaningful in a network since they can improve community detection Newman (2004b); Barrat et al. (2004). Newman (2004b) studied a weighted network in which edge weights are nonnegative integers. To study a weighted network in which edge weights are more than nonnegative integers, many models extend SBM from un-directed un-weighted networks to un-directed weighted networks. Some Weighted Stochastic Blockmodels (WSBM) are developed recent years Aicher et al. (2015); Ahn et al. (2018); Palowitch et al. (2017); Xu et al. (2020); Ng and Murphy (2021). However, these WSBMs always limit the block matrix to having nonnegative entries or require edge weights to follow certain distributions, and it is a challenge to design spectral algorithms to ﬁt these models. The Distribution-Free models (DFM)Qing (2021) was proposed to overcome these limitations. DFM allows the adjacency matrix to come from any distribution as long as the expectation adjacency matrix has a block structure reﬂecting community structure. Another advantage of DFM is, similar to SBM, spectral algorithms are allowed to ﬁt DFM. The distribution-free idea introduced in Qing (2021) is signiﬁcant and it is closely related to the model developed in this paper. DCDFM Qing (2022b) extends DFM by considering node heterogeneity parameters and it can also be seen as a direct extension of DCSBM from undirected un-weighted networks to un-directed weighted networks. However, all these WSBM, DFM and DCDFM have one main limitation, each node only belongs to one community under these models. To model overlapping un-directed weighted networks, the Weighted version of the MMSB (WMMSB) model Dulac et al. (2020) was proposed as an extension of MMSB by allowing edge weights to come from Poisson distribution. The Mixed Membership Distribution-Free (MMDF) model takes the advantage of distribution-free idea and extends MMSB by allowing edge weights to come from any distribution. Compared to WMMSB, MMDF is more applicable since it has no constraint on distribution. For community detection of directed weighted networks, this problem is more general than the above three cases since un-directed un-weighted networks, directed un-weighted networks and un-directed weighted networks can be seen as sub-networks of directed weighted networks. This problem does not receive suﬃcient attention up to now. However, directed weighted networks are common in our daily life. For example, in a WeChat network, nodes denote users, and the weight of a directional edge from user a to user b means the number of messages sent from a to b; in a friendship evaluating network, nodes denote member in a social system (like school, government, oﬃce, and club), and the weight of a directional edge from member a to member b means the grade of friendship that a thinks b as its friend; in a bipartite fans-actors Sina Weibo network, the weight of a directional edge from fans a to actor b means the number of fans a gives thumbs-up on the weblog of actor b in a period time. Perhaps the main reason that previous works ignore community detection in directed weighted networks is its complexity. To model directed weighted networks, beneﬁted from the distribution-free idea of Qing (2021), the Directed Distribution-Free models (DiDFM) and its extension DiDCDFM were proposed by Qing and Wang (2022). DiDFM and DiDCDFM extend DFM and DCDFM from un-directed weighted networks to directed-weighted networks, respectively. SBM, DCSBM, ScBM, DCScBM, DFM, DCDFM and DiDFM can be seen as sub-models of DiDCDFM. However, though DiDFM and DiDCDFM are beautiful models, they can not model directed weighted networks in which nodes can belong to multiple communities. For overlapping directed weighted networks, the two-way blockmodels proposed in Airoldi et al. (2013) models directed weighted networks by allowing nodes to belong to more than one community and allowing edge weights to follow Normal or Bernoulli distribution. However, one main limitation of the two-way blockmodels is, it limits edge weights to follow Normal or Bernoulli distribution. Since bipartite networks allow row nodes to be diﬀerent from column nodes and they contain directed networks, we focus on modeling bipartite weighted networks. In this article, our goal is to close these gaps for bipartite weighted networks. Main contributions of this paper are summarized as follows: a) We propose a novel model for bipartite weighted networks, the Bipartite Mixed Membership Distribution-Free (BiMMDF for short) model. BiMMDF can model a bipartite weighted network in which nodes can belong to multiple communities and edge weights can be any ﬁnite real values. In particular, bipartite signed networks in which nodes have mixed memberships can also be modeled by our BiMMDF. BiMMDF achieves its advantages by allowing elements of the adjacency matrix to follow arbitrary distribution as long as the expectation adjacency matrix has a block structure. MMSB, MMDF, DiMMSB and the two-way blockmodels are sub-models of BiMMDF. Some typical overlapping (and non-overlapping) networks can be modeled by our BiMMDF are sketched in Figure 1. We also conclude a brief comparison of previous models with our BiMMDF in Table 2. b) An eﬃcient spectral algorithm is used to ﬁt BiMMDF. This algorithm can exactly return true node memberships when applying the expectation adjacency matrix to replace the adjacency matrix as input, and this guarantees the identiﬁability of BiMMDF. A general theoretical guarantee on uniform rates of convergence for estimated node memberships returned by the algorithm is built under BiMMDF when the adjacency matrix comes from any distribution, where we also consider network sparsity parameter for our theoretical analysis. Meanwhile, theoretical upper bounds of error rates for a speciﬁc distribution can be immediately obtained from the general results. c) Separation conditions of a standard bipartite weighted network with two row (and column) communities for distributions like Bernoulli, Poisson, Binomial, Normal, Exponential, Uniform, Logistic, and bipartite signed network are obtained by delicate analysis under BiMMDF. For detail, see Examples 1-8. In particular, the separation condition of the Bernoulli distribution case is consistent with the classical results, and this guarantees the optimality of our theoretical results. d) The inﬂuence of the sparsity parameter on the algorithm’s performance and the behavior diﬀerence phenomenon on separation conditions of diﬀerent distributions are veriﬁed by substantial computer-generated bipartite weighted networks. Numerical results of real-world directed weighted networks demonstrate the advantages of the algorithm in estimating node memberships, ﬁnding highly mixed nodes, and studying asymmetry between row and column communities.
Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models<|sep|>In reinforcement learning (RL), agents acting in unknown environments face the exploration versus exploitation tradeoff. Without adequate exploration, the agent might fail to discover effective control strategies, particularly in complex domains. Both PAC-MDP algorithms, such as MBIE-EB [1], and Bayesian algorithms such as Bayesian Exploration Bonuses (BEB) [2] have managed this tradeoff by assigning exploration bonuses to novel states. In these methods, the novelty of a state-action pair is derived from the number of times an agent has visited that pair. While these approaches offer strong formal guarantees, their requirement of an enumerable representation of the agent’s environment renders them impractical for large-scale tasks. As such, exploration in large RL tasks is still most often performed using simple heuristics, such as the epsilon-greedy strategy [3], which can be inadequate in more complex settings. In this paper, we evaluate several exploration strategies that can be scaled up to complex tasks with high-dimensional inputs. Our results show that Boltzman exploration and Thompson sampling signiﬁcantly improve on the na¨ıve epsilon-greedy strategy. However, we show that the biggest and most consistent improvement can be achieved by assigning exploration bonuses based on a learned model of the system dynamics with learned representations. To that end, we describe a method that learns a state representation from observations, trains a dynamics model using this representation concurrently with the policy, and uses the misprediction error in this model to asses the novelty of each state. Novel states are expected to disagree more strongly with the model than those states that have been visited frequently in the past, and assigning exploration bonuses based on this disagreement can produce rapid and effective exploration. Using learned model dynamics to assess a state’s novelty presents several challenges. Capturing an adequate representation of the agent’s environment for use in dynamics predictions can be accom plished by training a model to predict the next state from the previous ground-truth state-action pair. However, one would not expect pixel intensity values to adequately capture the salient features of a given state-space. To provide a more suitable representation of the system’s state space, we propose a method for encoding the state space into lower dimensional domains. To achieve sufﬁcient generality and scalability, we modeled the system’s dynamics with a deep neural network. This allows for on-the-ﬂy learning of a model representation that can easily be trained in parallel to learning a policy. Our main contribution is a scalable and efﬁcient method for assigning exploration bonuses in large RL problems with complex observations, as well as an extensive empirical evaluation of this approach and other simple alternative strategies, such as Boltzman exploration and Thompson sampling. Our approach assigns model-based exploration bonuses from learned representations and dynamics, using only the observations and actions. It can scale to large problems where Bayesian approaches to exploration become impractical, and we show that it achieves signiﬁcant improvement in learning speed on the task of learning to play Atari games from raw images [24]. Our approach achieves state-of-the-art results on a number of games, and achieves particularly large improvements for games on which human players strongly outperform prior methods. Aside from achieving a high ﬁnal score, our method also achieves substantially faster learning. To evaluate the speed of the learning process, we propose the AUC-100 benchmark to evaluate learning progress on the Atari domain.
Secret Key Agreement Using Conferencing in State- Dependent Multiple Access Channels with An Eavesdropper<|sep|>Secure communication in a network is possible when legitimate users have access to some secret keys. In [1], Shannon demonstrated that the perfect secrecy condition can be satisﬁed if: where, H(K) and H(M) are the entropies of the message and the key, respectively. Secret key generation in a network requires the existence of common randomness between users. A simple model for common randomness, in the information theory context, is distributed correlated sources. This model was ﬁrst studied by Ahlswede and Csiszar [2], where legitimate users utilize two correlated sources as common randomness to share a secret key in a noiseless network that must be concealed from an eavesdropper. In [3], new bounds on the secret key capacity over a multiterminal network with public channel were established by Gohari and Anantharam. In their model, there are M legitimate terminals and an eavesdropper that have access to correlated sources. Only some of the legitimate terminals can transmit over the channel. All the legitimate terminals intend to agree on a common key that must be kept secret from the eavesdropper. In a noisy channel, common randomness can be obtained by implementing the channel distribution. This model is useful when illegal users have no access to the common randomness or a part of it. But if the legitimate users do not have any advantages compared to the illegal users, this common randomness is not beneﬁcial for secret key sharing any more. Maurer solved this problem using a backward public channel in the wiretap model [4]. The backward public channel is a noiseless channel used by the receivers to transmit messages to the transmitters where the messages can be observed by an eavesdropper. In [2], Ahlswede and Csiszar showed that a forward noiseless channel does not help to solve the problem. In addition to the Maurer’s solution, the problem can be solved when correlated sources are distributed between legitimate users in a noisy network. This idea was recently developed by Khisti et al. in the wiretap channel where the transmitter and the legitimate receiver have access to correlated sources [5]. Salimi and Skoglund, in another recent work, investigated the problem of secret key agreement over generalized multiple access channels using correlated sources [6]. In this channel, each of transmitters intends to agree on a private key with a receiver. Furthermore, when a forward public channel is available, the secret key sharing problem was studied by Salimi et al. [7]. In their model, the transmitters intend to share private keys over the generalized multiple access channel with the receiver using the public channel. The authors established examples to show that the forward public channel can improve the secret key capacity. In addition, they showed that using the forward public channel for key sharing is more effective than compress and forward strategy which was proposed in [8]. In state-dependent noisy networks, the Channel State Information (CSI) can be used as common randomness when illegal users have limited access to the CSI. In these networks, the CSI may be available causally or non-causally at the legitimate users. Khisti et al. studied the problem of secret key agreement over 2-receiver broadcast channels with causal or non-causal CSI where the transmitter upon observing the CSI generates a secret key and sends the required information over the channel and the legitimate receiver estimates the secret key [9]-[11]. Cooperation can be effective for common key sharing in a network where there are more than one transmitter. Conferencing is one of the schemes that can be utilized to provide cooperation. In most cases, a noiseless channel with a limited rate is used to establish the conferencing scheme. In [12], Willems used the conferencing scheme in a multiple access channel where there is an interactive noiseless channel with a limited rate between the transmitters. Upon receiving sequences from the noiseless channel, each transmitter determines the channel input as a function of its message and the observed sequences. Consider a multiterminal network with n + 1 users, where one of them acts as a Trusted Center (TC) and others act as End Nodes (ENs). In addition, there is an illegal user in the network which wishes to eavesdrop. In this network, the ENs try to establish a conﬁdential connection with the TC. Therefore, they ﬁrst need to agree on some keys with the TC to announce themselves as trusted users. For transmitting the conﬁdential message, the TC needs to generate n independent private keys and share them with each of the ENs. These private keys provide an ability of multiplexing in the network. The eavesdropper tries to ﬁnd the keys and attack the network. Motivated by the above scenario, we deﬁne our system model. As Fig. 1 illustrates, we consider a three-user network with an eavesdropper, in which two ENs and a TC are modeled as two transmitters and a legitimate receiver, respectively. The transmitters and the legitimate receiver are connected by a State-Dependent Multiple Access Channel (SD-MAC) where a conferencing link is available between the transmitters. In addition, the eavesdropper observes the channel. An insecure backward public channel with an unlimited capacity is available between all the terminals. In order to achieve a secure connection; at ﬁrst, the transmitters intend to share a common key over the SD-MAC with the legitimate receiver using the conferencing scheme. Then, the legitimate receiver shares an independent private key with each of the transmitters over the public channel. In this model, we investigate the problem of secret key agreement in two rounds. In the ﬁrst round, we establish the lower and upper bounds on the common key capacity. The intuition behind the lower bound comes from the superposition coding and random binning. The state is utilized to generate the common key by means of the hybrid joint source channel coding. In the second round, the inner and outer bounds are derived on the private key capacity region. The double random binning is used to satisfy the secrecy constrains. In this round, the private key capacity is obtained for some special cases. Different systems can be modeled as the SD-MAC with an eavesdropper. For example, we consider a binary memory with stuck at faults in which two end nodes utilize this memory to share a common key
Background and Imaging Simulations for the Hard X-Ray Camera of the MIRAX Mission<|sep|>Estimation of the energy spectrum of the background signal and its spatial distribution over the detector plane is crucial for the design of hard X-ray and gamma-ray astronomy telescopes. In the case of an observation of a point source from an orbital space platform (i.e. a satellite), the background consists in the diﬀuse electromagnetic radiation coming through the telescope aperture, emission from other sources in the ﬁeld of view (FoV) and the instrumental background, which arises from interaction of high-energy particles with the detectors and surrounding material. In the case of observations carried out at stratospheric balloon altitudes, we also need to take into account the atmospheric radiation produced by the interaction of cosmic particles with atmospheric constituents, which will generate secondary particles and photons. In any case, the observations of cosmic X- and gamma-ray sources are always hindered by intense and complex background radiation measured by the detector system. In this paper we consider the contribution of photons, protons, electrons and neutrons to the total background of a hard X-ray coded-aperture imaging camera to be mounted In order to estimate the background, it is important to have an accurate knowledge of its origin. To achieve this, one needs to include detailed descriptions of the energy spectra and angular distribution of the particle ﬁelds that surround the instrument. In this work we describe the procedures we used in order to model the background of a hard X-ray imaging camera being developed at the National Institute for Space Research (INPE), Brazil, in the context of the MIRAX (Monitor e Imageador de RAios X) space astronomy mission (Braga et al. 2004; Braga 2006). Since the instrument is supposed to ﬂy on-board stratospheric balloons over Brazil, at an altitude of ∼ 42 km and a latitude of ∼ −23◦ S, and as part of a satellite experiment in a near-equatorial circular LEO, we have included both environments in our simulations. We have developed a detailed mass model of the camera and simulated all the interactions in the instrument components and materials using the GEANT4 (GEometry ANd Tracking) code developed at CERN (Agostinelli et al. 2003). The results of these simulations have allowed us to estimate with good precision the spectral response of the CdZnTe (CZT) detectors of the camera and the spatial distribution of counts over the detector plane. Also, by running simulations with diﬀerent conﬁgu
Dynamical localization of chaotic eigenstates in the mixed-type systems: spectral statistics in a billiard system after separation of regular and chaotic eigenstates<|sep|>In quantum chaos [1, 2, 3] of general (generic) time-independent (autonomous) Hamilton systems in the strict semiclassical limit we can conceptually separate regular and chaotic eigenstates. This picture goes back to the work by Percival in 1973 [4] and Berry and Robnik in 1984 [5]. One of the main results in quantum chaos is the fact that in classically fully chaotic (ergodic, autonomous Hamilton) systems with the purely discrete spectrum the ﬂuctuations of the energy spectrum around its mean behaviour obey the statistical laws described by the Gaussian Random Matrix Theory (RMT) [6, 7], provided that we are in the suﬃciently deep semiclassical limit. The latter condition means that all relevant classical transport times, like the typical ergodic time, or diﬀusion time, are smaller than the so-called Heisenberg time, or break time, given by tH = 2πℏ/∆E, where h = 2πℏ is the Planck constant and ∆E is the mean energy level spacing, such that the mean energy level density is ρ(E) = 1/∆E. This statement is known as the Bohigas - Giannoni - Schmit (BGS) conjecture and goes back to their pioneering paper in 1984 [8], although some preliminary ideas were published in [9]. Since ∆E ∝ ℏd, where d is the number of degrees of freedom (= the dimension of the conﬁguration space), we see that for suﬃciently small ℏ the stated condition will always be satisﬁed. Alternatively, ﬁxing the ℏ, we can go to high energies such that the classical transport times become smaller than tH. The role of the antiunitary symmetries that classify the statistics in terms of GOE, GUE or GSE (ensembles of RMT) has been explained in [10], see also [11] and [1, 2, 3, 6]. The theoretical foundation for the BGS conjecture has been initiated ﬁrst by Berry [12], using the Gutzwiller periodic orbit theory (trace formula) [13] (for an excellent exposition see [1]) and later further developed by Richter and Sieber [14], arriving ﬁnally in the almost-ﬁnal proof proposed by the group of F. Haake [15, 16, 17, 18]. On the other hand, if the system is classically integrable, Poisson statistics applies, as is well known and goes back to the work by Berry and Tabor in 1977 (see [1, 2, 3] and the references therein, and for the recent advances [19]). In the mixed-type regime, where classical regular regions coexist in the classical phase space with the chaotic regions, being a typical KAM-scenario which is the generic situation, the so-called Principle of Uniform Semiclassical Condensation (of the Wigner functions of the eigenstates; PUSC) applies, based on the ideas by Berry [20], and further extended by Robnik [3]. If the stated semiclassical condition is satisﬁed, the chaotic eigenstates are uniformly extended, and consequently the Berry-Robnik statistics [5, 21] is observed - see also [3]. If the semiclassical condition stated above requiring that tH is larger than all classical transport times is not satisﬁed, the chaotic eigenstates will not be extended but localized and the Berry-Robnik statistics must be generalized as explained in [22, 23, 3, 24, 25] and in this paper. The relevant papers dealing with the mixed-type regime after the work [5] are [21] - [31] and the most recent advance was published in [24]. If the couplings between the regular eigenstates and chaotic eigenstates become important, due to the dynamical tunneling, we can use the ensembles of random matrices that capture these eﬀects [32, 24]. As the tunneling strengths typically decrease exponentially with the inverse eﬀective Planck constant, they rapidly disappear with increasing energy, or by decreasing the value of the Planck constant. In this work we shall deal only with high-lying eigenstates, and therefore we can neglect the eﬀects of tunneling. However, quite generally, if the semiclassical condition is not satisﬁed, such that tH is no longer larger than the relevant classical transport time, like e.g. the diﬀusion time in fully chaotic but slowly ergodic systems, we ﬁnd the so-called dynamical localization, or Chirikov localization. Dynamical localization was discovered in time dependent systems [33]. It was intensely studied since then in particular by Chirikov, Casati, Izrailev, Shepelyanski and Guarneri, in the case of the kicked rotator as reviewed in [34]. See also the references [35]-[38], and the most recent work [39]. For a general overview of the time dependent Floquet systems see also [1, 2]. It has been observed that in parallel with the localization of the eigenstates one observes the fractional power law level repulsion (of the quasienergies) even in fully chaotic regime (of the ﬁnite dimensional kicked rotator), and it is believed that this picture applies also to time independent (autonomous) Hamilton systems and their eigenstates [39]. (See the excellent review of localization in time independent billiards by Prosen in [40].) Indeed, this has been analyzed with unprecedented precision and statistical signiﬁcance recently by Batisti´c and Robnik [24] in case of mixed-type systems, and the present work is being extended in the analysis of separated regular and chaotic eigenstates. An early attempt of separation of eigenstates in the billiard system has been published in [41], using a diﬀerent approach at much lower energies and with much smaller statistical signiﬁcance. In [42] mushroom billiards were studied at much lower energies and with much smaller statistical signiﬁcance, where the aspects of tunneling were investigated in the ﬁrst place, but not the dynamical localization, although a clear deviation from the GOE statistics was found in the chaotic eigenstates. In this paper we introduce a criterion for classifying eigenstates as regular and chaotic, and moreover, we show that the regular levels obey the Poisson statistics, whilst the chaotic dynamically localized eigenenergies obey exceedingly well the Brody distribution [43], with the Brody parameter values β within the interval [0, 1], where β = 0 yields the Poisson distribution in case of the strongest localization, and β = 1 gives the Wigner surmise (2D GOE, as an excellent approximation of the inﬁnite dimensional GOE), which describes the extended chaotic eigenstates. It turns out that the Brody distribution introduced in [43], see also [44], ﬁts the empirical data much better than e.g. the distribution function proposed by F. Izrailev (see [35, 34] and the references therein). It is well known that Brody distribution so far has no theoretical foundation, but our empirical results show that we have to consider it seriously in dynamically localized chaotic eigenstates, thereby being motivated for seeking its physical foundation, and an analogous result was obtained in the recent work of Manos and Robnik (2013) [39] in the analysis of the quantum kicked rotator, where the object of study are the eigenstates of the Floquet operator and the statistical properties of the spectrum of eigenphases (quasienergies) in classically fully chaotic regime. In the Hamilton systems with classically mixed-type dynamics, which is the generic case, we have classically regular quasi-periodic motion on d-dim invariant tori (d is the number of freedoms) for some initial conditions (with the fractional Liouville volume ρ1) and chaotic motion for the complementary initial conditions (with the fractional Liouville volume ρ2 = 1−ρ1). The chaotic set might be further decomposed into several chaotic regions (invariant components) in case d = 2, whilst for d > 2 it is strictly speaking always just one chaotic set due to the Arnold diﬀusion on the Arnold web, which pervades the entire phase space. In suﬃciently deep semiclassical limit the BerryRobnik picture [5] is established, based on the statistically independent superposition of the regular (Poissonian) and chaotic level sequences, based on PUSC, as explained above. In this paper we shall consider only the case of just one chaotic component, although the results can be easily generalized for more than one chaotic component. This picture gives an excellent approximation for the statistics of spectral ﬂuctuations of the mixedtype systems, if the largest chaotic component is much larger than the next largest one, which typically indeed is the case e.g. in 2D billiards. The energy spectrum of the mixed-type system with one regular and one chaotic component can be described in the Berry-Robnik (BR) regime of suﬃciently small eﬀective Planck constant ℏeff by the following formula for the gap probability E(S), and the level spacing distribution P(S) (see e.g. [3]) is of course always given as the second derivative of the gap probability, namely P(S) = d2E(S)/dS2, so that we have This factorization formula (1) is a direct consequence of the statistical independence, justiﬁed by PUSC. Here by Er(S) = exp(−S) we denote the gap probability for the Poissonian sequence with the mean level density one. By Ec(S) we denote the gap probability for the chaotic level sequence with the mean level density (and spacing) one. Note that the classical parameter ρ1 and its complement ρ2 = 1−ρ1 enter the expression as weights in the arguments of the gap probabilities. Using the Bohigas-Giannoni-Schmit conjecture we conclude that in the suﬃciently deep semiclassical limit Ec(S) is given by the RMT, and can be well approximated by the Wigner surmise
Code Properties of the Holographic Sierpinski Triangle<|sep|>The exploration of gravity and quantum ﬁeld theories from an information theory perspective has had a long and fruitful history. The study of holographic entanglement entropy [1] eventually led to the study of holographic quantum error correction, ﬁrst deﬁned in [2]. This development led to a series of inﬂuential work in AdS/CFT, in particular [3][4][5]. Quantum information science has been one of the most active areas of research in the last decades. A plethora of literature has been produced in pursuit of the physical realization of quantum computers. The necessity for robust information encoding that can withstand errors makes the development of quantum error correction very important. In this ﬁeld, tremendous progress has been made in both the development of eﬃcient quantum error correcting codes (QECC) and the creation of appropriate hardware candidates for their physical realisation.[6] The area of topological phases of matter[7] has also been an area of active research over the past few decades. The combination of developments in that area and that in quantum error correction has led to the development of topological quantum error correcting codes[8]. While such codes are quite eﬃcient and powerful, a recent no-go theorem has revealed limitations of topological quantum error correction with regard to fractal noise [9]. Fractal noise is a quite reasonable model for real-world experimental noise, due to defects on the lattice and percolation; see [10][11] for a detailed discussion. One of the main theorems of [9] states: Theorem: ZN topological order cannot survive on a fractal embedded in a 2D Euclidean space R2. It is therefore a natural question to ask if holographic quantum error correction suﬀers from the same limitation against fractal noise. In this paper, we answer this question in the negative. In this article, we discuss extension of so-called uberholography [16], a prescient study of robustness of holographic QECC to fractal erasure noise in AdS3/CFT2 to AdS4/CFT3, in particular considering the quantum error correction properties of a boundary subregion in the shape of a Sierpinski triangle. In particular, by taking a time-slice in (2+1)d CFT, we essentially have a R2 surface, and if one is able to demonstrate bulk reconstructability of operators deep within the bulk, then this would show that holographic QECC’s do not obey an analog to the topological QECC no-go theorem. The organisation of the paper is as follows: In Section 2, we give a brief background of holographic QECC, in Section 3 we will discuss the code properties of the Sierpinski triangle subregion, and we will conclude with some discussion and potential future work in Section 4.
Multilinear Subspace Clustering<|sep|>Most clustering algorithms seek to detect disjoint clouds of data. However, in high-dimensional statistics, data can become very sparse, and these types of methods have trouble dealing with noise. In fact, a completely new approach to the geometry of clustering has recently made headway in the analysis of high-dimensional data sets. Called subspace clustering, this approach assumes that data come from subspaces offset at angles, rather than from clouds offset by gaps, the so called Union of Subspaces (UOS) model [1, 2, 3]. Applications have included detection of tightly correlated gene clusters in genomics [4], patient-speciﬁc seizure detection from EEG data [5], and image segmentation [6]. All subspace clustering methods must embed data in Rn. However, in some of the high-dimensional data sets where subspace clustering has been applied, the initial structure of the data is not a vector but rather a matrix or tensor (multiway array). Examples include the auditory temporal modulation features in [7], the image patches in [6], and raw EEG data under the “sliding-window approach” [5]. We seek to develop a clustering method that incorporates the geometric innovation of subspace clustering without vectorizing these higher-order arrays. To do this, we formulate an algebraic generative model for the data, along with methods for inference. This work is part of the ﬁrst author’s senior honors thesis at Tufts University. Nathan Majumder, Shuchin Aeron and Misha Kilmer were supported by NSF grant 1319653. The Subspace Clustering Problem and a Multilinear Variant - Mathematically, the subspace clustering problem is described as follows. Given a set of points xn, n = 1...N, suppose each point is an element of one of the K subspaces. The problem is to decide membership for each of the N points. For simplicity, we treat K as known. In order to take advantage of patterns in two-way data, we modify the assumptions of the subspace clustering problem. Rather than modeling the data as a union of subspaces, we assume they come from a union of tensor products of subspaces [8]. Given subspaces U ⊂ Rn and V ⊂ Rm, suppose the columns of U form a basis of U and likewise for V and V. The tensor product U ⊗ V is the set {A|A = UYVT }, where Y is a dim(U)× dim(V) matrix. In other words, this is a set of matrices with (column/row) space conﬁned to (U/V). We refer to this model as the union of multilinear subspaces (UOMS) model and we call this the multilinear subspace clustering (MSC) problem. Note that while U ⊗ V is a tensorsubspace of the tensor space Rn ⊗ Rm, not all subspaces of the tensor space is a tensor subspace [8]. Therefore we are assuming a tensor-subspace structure on the clusters under the UOMS model. The difference between the generative models for UOS and UOMS is clariﬁed in Algorithms 1, 2. Given {U1, ..., UK} ∈ RD×d, Repeat N times: Draw k from {1, ..., K} Draw a random length-d vector yn Compute datum xn = Ukyn Given {U1, ..., UK} ∈ RDu×du, {V1, ..., VK} ∈ RDv×dv Repeat N times: Draw k from {1, ..., K} Draw a random du by dv matrix Yn Compute datum An = UkYnVT k Relation of UOMS to existing subspace models - Note that the UOS model with single subspace (one cluster) is related to the Principal Component Analysis (PCA). Similarly the UOMS with one cluster is closely related to separable covariance models [9] and also 2D-PCA[10]. Further in [9], extensions of this idea to 3-D, 4-D,... data is shown to be equivalent to HOSVD and Tucker decompositions [11] that have been useful for dimensionality reduction for image ensembles [12]. Further such multilinear subspace models have been used in machine learning [13, 14]. In this paper we study an extension of these models by considering a union of such structured subspaces. Input X ∈ RD×N holding data vectors xi, number of clusters K, thershold (1 ≤ q ≤ N) Procedure Normalize xi, Compute adjacency matrix C = |X⊤X| Set all but q highest values of each row of C to zero Ci,j = exp � −2 ∗ cos−1(Cij) � Input X ∈ RD×N holding data vectors xi, λ1, λ2 ≥ 0 the number of clusters K Procedure Solve arg minC,E ∥X − XC − E∥2 F + λ1||C||1 + λ2∥E∥1 s.t. Cii = 0 Form W = |C| + |C|⊤ Clustering under the UOS model - There are many algorithms exploiting the UOS model for clustering, but we focus on two general methods that form an afﬁnity matrix among data points followed by spectral clustering [15]. The ﬁrst, called Thresholded Subspace Clustering (TSC), is introduced in [2]. This provably reliable and robust subspace clustering algorithm involves constructing a weighted graph where nodes represent the data points and edges represent the connectivity of any two points. The inner product between the data points is used as the edge weight with the idea that points in the same subspace will generally have a higher inner product than points in different subspaces. The symmetric adjacency matrix for the graph is then thresholded, setting all but the q highest weights in each row to zero, in order to ﬁlter out noise. The second method, called Sparse Subspace Clustering (SSC) [16] involves expressing each point as a linear combination of the other points in the dataset. The algorithm ﬁnds the sparsest possible linear representation of each data point in terms of other data points – achieved by minimizing the ℓ1norm – with the idea that the points used will come from the same subspace as the point in question. A weighted graph is then formed with an adjacency matrix found using the sparse representations of each point. Both the TSC and SSC algorithms, taken from [2] and [16] respectively, are detailed in Algorithms 3 and 4. 2. CLUSTERING UNDER THE UOMS MODEL In the case of two-way data, our data points would be a collection of N matrices An ∈ RDU ×DV such that the columns come from a union of subspaces U1 ∪ . . . ∪ UK and the rows come from a union of subspaces V1 ∪ . . . ∪ VK. To take advantage of this fact and ﬁnd these Ui and Vi subspaces, one method would be to cluster all DVN columns and all DUN rows separately; however, this is an expensive solution. Instead, we randomly select a single column and a single row from each matrix and cluster these. We stack the random columns side by side to form a DU × N matrix Xcols and transpose and stack our random rows side by side to form a DV × N matrix Xrows. The ith column of each of these matrices comes from the ith (i = 1, ..., N) data matrix Ai. We then perform a clustering algorithm on Xrows and Xcols separately, but pause after obtaining the symmetric adjacency matrix C in each case. We repeat this process for T trials, ending up with 2T adjacency matrices, which we then combine in one of a few possible ways. Possible combination methods are detailed subsequetly. Combining these adjacency matrices can be thought of as condensing multiple graph realizations to obtain a single weighted graph representing our clustering problem. Once we have our condensed graph, we perform spectral clustering on it to achieve the segmentation of our original points. Algorithm 5 outlines the steps described above. Input: Data A1, ..., AN ∈ RDc×Dr, number of clusters K Clustering Method (TSC or SSC), number of trials T Procedure: For T trials form Xcols ∈ RDc×N with i-th column randomly selected from Ai form Xrows ∈ RDr×N with i-th column randomly selected from A⊤ i Run clustering (TSC or SSC) on Xcols and Xrows to get Ccols ∈ RN×N and Crows ∈ RN×N Combining the Graph Realizations - We now discuss several (heuristic) methods for combining the adjacency matrices obtained at each iteration of MSC. 1. Addition: One simple method is to add the 2T adjacency matrices together. 2. Thresholding: Add the matrices together followed by thresholding, setting all but the q highest edges per row to zero. A possible choice of threshold for this method would be the average number of data points per cluster – if this number is known – minus one (to count out the point itself). 3. Filtering by Quantile: A ”quantile” method involves choosing a parameter l and taking the l-th highest weight at each edge out of all the adjacency matrices. The choice of l poses an obstacle, as there is not a given value that will be optimal for all graphs. 4. Projection: Project each individual adjacency matrix’s columns onto its leading K singular vectors (corresponding to largest singular values) before adding the instances. However, the fact that each matrix is projected onto its leading singular vectors before sharing any information with other graph realizations could lead to loss of quality. Remark - These methods are by no means exhaustive. In particular, the problem of combining various graph realizations for the same problem instance by itself is an interesting avenue of research. Algorithmic Complexity - For N data points of dimension D, TSC has algorithmic complexity O(DN 2). Therefore, if we are comparing TSC on vectorized 2-way data against MSC using TSC on the same data in matrix form, the MSC data points will be matrices of size Dc × Dr where D = DcDr. At each iteration of MSC, we form the matrices of size Dc × N and Dr × N for the column space and row space respectively. Since we then perform TSC on these matrices, the algorithmic complexity at each iteration will be O(DcN 2 + DrN 2) or approximately O( √ DN 2) when Dc ≈ Dr. For the projection method, which is the computationally most expensive, when K << N, using randomized SVD a computational cost of O(N 2 log K) [17] is incurred. Therefore, for T iterations of MSC, we have O(T √ DN 2) compared to O(DN 2) for TSC. Therefore if we can pick a number of trials T such that T ≪ √ D, MSC will be cheaper. This obviously leads to a possible conclusion that MSC will be a better choice for large data while TSC will be more realistic for data of smaller dimensions. The computational complexity of the SSC algorithm is O(DN 3), which for large D and N becomes more prohibitive compared to TSC as well as MSC.
Type 1 low z AGN. I. Emission properties<|sep|>The ﬁrst systematic study, based on optical spectroscopy, of a complete and well deﬁned sample of Broad Line AGN was conducted by Boroson & Green (1992, hereafter BG92). It included 87 z < 0.5 AGN from the Bright Quasar Survey (BQS, Green et al. 1986), with spectra taken over the 4100 ˚A–5900 ˚A range, at a spectral resolution of ∼ 700. This survey, and additional systematic studies at other wavelengths, led to major new understandings of the emission and absorption properties of AGN (BG92, and citations thereafter). The BQS survey was followed by the Large Bright Quasar Survey, which yielded spectra of 1055 quasars (Hewett et al. 1995), and by the 2dF QSO Redshift Survey which yielded spectra for 23 338 quasars (Croom et al. 2004). The next major step in AGN optical spectroscopy came with the Sloan Digital Sky Survey (SDSS) Quasar Catalog (latest release QCV, Schneider et al. 2010), which produced a sample of quasars a factor of 1000 larger, compared to BG92, with a factor of 10 more spectroscopic data per object (a factor of three larger wavelength coverage and a factor of three larger spectral resolution). This sample is now a prime resource for studies of AGN (see Schneider et al. 2002, and citations thereafter). The QCV sample is mainly based on objects selected for spectroscopy due to their non-stellar colours, supplemented by SDSS spectra selected based on other surveys. The QCV sample includes luminous objects (Mi < −22.0) which exhibit at least one emission line with FWHM > 1000 km s−1. Various studies used the SDSS to produce samples of lower luminosity AGN. At Mi > −22.0 the AGN are generally dominated by the host light, which needs to be subtracted to detect the AGN continuum and broad line emission. Hao et al. (2005a), who used the SDSS second data release (DR2), subtracted the host light based on a principal component analysis method. They then modeled the Hα proﬁle using one or two Gaussians, and derived the FWHM of the broader component. Their type 1 AGN sample includes a total of 1317 objects with FWHM > 1200 km s−1, and was used to study the extension of the luminosity function of type 1 AGN to low luminosities. The later study of Vanden Berk et al. (2006), based on DR3, extended the earlier SDSS quasar sample (Schneider et al. 2005) to lower luminosity by removing the absolute magnitude criterion, which yielded 4 666 low-luminosity type 1 AGN, used to study the relation between the AGN luminosity and the host prop
A new zero-knowledge code based identification scheme with reduced communication<|sep|>The use of coding theory for public key cryptography was initiated by McEliece more than 30 years ago, although the system has often be considered as too costly and impractical because of the size of the public key, code-based cryptography has received much more attention in recent years. Besides the fact that code-based cryptography can possibly resist to a quantum computer, code-based systems have also inherent interests: they are very fast and are usually easy to implement compared to number theory based systems. Such features make code-based systems good candidates for low-cost cryptography. There are two main types of code-based cryptosystems: systems with hidden structure like the McEliece cryptosystems (analogous to RSA) and systems with no hidden structure (analogous to discrete log -based cryptosystems) like for instance the Stern code-based authentication scheme ([Ste93]). This second type of system is not vulnerable to structural attacks which are the main cause of attacks on McEliece-like cryptosystems. In practice as for the Stern scheme, they have not been attacked beneath the usual improvement on the attack of the underlying hard problem. In the case of coding theory the underlying hard problem (the Syndrome decoding problem SD) is now well studied and considered as very secure. Code-based Zero-knowledge authentication schemes are very interesting since their security is directly related to a hard problem, moreover they can be turned into signature schemes through the Fiat-Shamir paradigm. Meanwhile there are two strong drawbacks for these schemes. The ﬁrst drawback is the size of the public key which can attain several hundred thousand bits and the second drawback is the size of the communication induced by the cheating probability, more than 150kb in practice for a 280 security level. The ﬁrst drawback was resolved in part by Gaborit and Girault [GG07] who proposed to use structured matrices like double-circulant matrices (matrices of the form (IA) for A a random circulant matrix) to reduce the size of the public key to only a few hundred bits. The second drawback, the high cost of communications, largely remains. In this paper we make a step further to obtain a small communication cost, our new algorithm, with the same type of security than previous algorithm and small size of keys, permits to reduce the size of communications by 40%. We propose two diﬀerent improvements, a ﬁrst improvement relies on using the double-circulant structure to increase the number of possible challenges, and the generic second improvement consists in a better use of commitment by compressing them. In practice it is now possible to sign for a security level of 280 with a signature of size 93kb rather than 155kb, and to get identiﬁed for a security level of 2−16 with 20kb rather than 31kb. II. Background on code-based authentication schemes A. Previous work There are severals protocols based on the syndrome decoding problem, we quickly survey the main advances in this area The ﬁrst eﬃcient protocol was proposed by Stern [Ste93]: his idea was a new way to prove the knowledge of a word with small weight and ﬁxed syndrome. The idea consist of revealing one of the three statements, the adequate weight with a masked syndrome, the adequate syndrome with a wrong weight or a way the weight and the syndrome can be masked. The 3 challenges structure implies a cheating probability equal to 2/3 instead of 1/2 for the well known scheme of Fiat-Shamir. The Stern protocol is also uncommon by the use of hash functions. In [Ste93] Stern presents another protocol which aims at reducing the cheating probability to 1/2 by cutting the challenge step into 2 parts. Indeed, adding this challenge in the scheme prevents the prover to reveal the third statement and reduce the probability close to 1/2. The next improvement was a reduction of communication due to V´eron in [V´er96], the reduction is due to a diﬀerent formulation of the secret, which decreases the cost of communication but increases the size of the key. In [GG07], GaboritGirault proposed to use particular compact matrices (doubly circulant matrices) in order to obtain a very short public matrix. The last improvement appeared with the protocol of Cayrel-V´eron-El Yousﬁ where the aim was to reduce the cheating probability to 1/2 as well as in the second protocol of Stern but using ﬁelds with cardinality higher than 2. Our protocol uses the V´eron variation that we recall here. B. Scheme of Veron private key : (e, m) with e of weight w and of length n and m a random element of Fk 2. public key : (G, x, w)) with G a random matrix of size k × n and x = e + mG. 1) [Commitment Step] P randomly chooses u ∈ Fk 2 and a permutation σ of {1, 2, . . ., n}. Then P sends to V the commitments c1, c2 and c3 such that :
Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image Segmentation using Partially Labeled Data<|sep|>The recent advances in digital pathology, and particularly the approach of whole slide imaging (WSI), have led to a paradigm shift in pathology (Bandari et al., 2016). Computerassisted tumor segmentation has been broadly used in cancer pathology (Nguyen et al., 2021). However, there are increasing demands in analyzing more comprehensive tissue types beyond the tumor, including investigating tumor micro-environments and non-cancer pathology (Rangan and Tesch, 2007) rather than cancer pathology. Brieﬂy, the learning algorithms can be asked to examine more comprehensive tissue types simultaneously, as a multi-label setting. Moreover, the recent advances in data sharing are providing increasingly large amounts of publicly available data, which requires a more robust computer-assisted analytics tool for large-scale multi-center and multi-stain WSIs (Ginley et al., 2019). Ideally, Figure 1: Multi-label pathology dataset – Due to being labor-intensive and timeconsuming, there is only one manual annotation for one type of tissue on pathological images. A majority of prior arts in renal pathology trained multiple independent networks for diﬀerent tissue types, increasing computational complexity. More recently, multi-head networks were proposed for the partial label dataset but were inﬂexible for a new task. In this paper, we proposed a single network with a single dynamic head using a class-aware controller and dynamic head mapping to achieve multi-label pathological segmentation eﬃciently. the computer-assisted segmentation method should be able to segment all types of tissue on large-scale pathological images in order to alleviate the labor-intensive, time-consuming manual annotation (Barisoni et al., 2017; Wernick et al., 1993). One major hurdle in achieving automated multi-label pathological image segmentation is the so-called partially labeling issue. This issue is caused by intensive labor and resource cost in pixel-level manual annotations on Giga-pixel pathological images, so that many datasets contain annotations of only one type of tissue. Mainstream approaches tackle the partial label issue by splitting the partially labeled cohorts into several fully labeled subsets and training “multiple networks” for diﬀerent tasks (Yu et al., 2019; Isensee et al., 2019; Zhang et al.; Myronenko and Hatamizadeh, 2019; Zhu et al., 2019). This intuitive strategy, however, increases the computational complexity dramatically. Another more recent family of solutions is to employ multi-head networks. A multi-head network typically consists of a shared encoder, and several task-speciﬁc decoders (heads) (Chen et al., 2019; Fang and Yan, 2020; Shi et al., 2021). Moreover, the multi-class segmentation method with a single network (Gonz´alez et al., 2018; Cerrolaza et al., 2019) shows the promising performance for partially labeled datasets. However, the redundant implementation of heads is not only wasteful but also inﬂexible, especially when it is extended to a new task. To mitigate the redundancy, Zhang et al. (Zhang et al., 2021) proposed a dynamic on-demand network (DoDNet) for radiological image analysis, which is an encoder-decoder network with a dynamic head that segments multiple organs and tumors as previously done by multiple
Self-gravitating systems of ideal gases in the 1PN approximation<|sep|>Galaxies are composed of many diﬀerent kinds of astrophysical objects such as stars, interstellar gas, dust, and dark matter amongst other things. Such system holds together due to gravitational interaction amongst their constituents. In fact, they can be organized in bigger astrophysical structures called clusters or super-clusters [1]. It is generally believed that the amount of dark matter in a cluster is ten times bigger than the total amount of gas and stars [2]. However, the recent analysis based on both optical and radio data in spiral galaxies reveals that the amount of dark matter is even less than previously thought [3]; where the gravitational ﬁeld associated with spiral galaxies (such as NGC 7793, 1365, 6946 and UGC 6446) is assumed to follow a global disk-like proﬁle rather than spheroidal one [3]. For a detailed analysis of the universal rotational curve of spiral galaxies and its connection with dark matter see [4]. If the physical scale of interest is much bigger than the size of these objects then the latter ones can be considered as pointlike particles interacting among them, the gravity being the most relevant interaction. Hence, one possible route to study this system may be to solve its dynamical equation and ﬁnd the trajectories of each one of the “particles”. Nevertheless, the number of particles involved in these systems is enormously big so the aforesaid approach becomes unsuitable. An alternate view of galaxies is as a system of particles in six dimensional phase space. The galaxies are then instantaneously described in terms of a distribution function f(x, p) over the phase space, where x represents the position vector and p stands for the momentum. In this way, galaxies can be described in terms of ensemble of particles along with a distribution function which satisﬁes a kinetic equation [5]. If one obtains the distribution function associated with the kinetic equation then one can extract the main traits of the system under consideration, for instance, one can be able to reconstruct the average square velocity and several other moments of the distribution function[5]. If the collision between two particles is a very rare event then the collision operator of the Boltzmann equation can be neglected. In the latter case, one can work with the collisionless Boltzmann equation (sometimes this is also known as Vlasov equation) whose solution turns out to be an equilibrium distribution function [5]. As an example, one thinks in a gas with a Boltzmann distribution at equilibrium whose gravity center follows a circular geodesic in Schwarzschild ﬁeld [6]. Or the case of dark matter halos, dark matter only interacts gravitationally and there are hardly any encounters, so that one can describe the dark matter as a collisionless system and use the Vlasov equation in order to describe its evolution [7]. Besides, a self-consistent rederivation of collisionless Boltzmann equation for self-gravitating gases with post-Newtonian corrections [8] was obtained recently; the case of post-Newtonian polytropes solutions was examined numerically, focusing in the role played by the relativistic correction to the rotation curves [8]. As is well-known the equations of general relativity reduce to those of Newtonian gravity in the limit of slow motions along with weak gravitational ﬁelds. Newton’s theory of gravity is good enough to describe all the physics in the solar system, but it is incomplete and requires some corrections to properly account for the shift in the perihelion of Mercury [9]. In order to describe physical phenomena like the latter one it becomes essential to include post-Newtonian (PN) corrections to the standard gravitational physics. A straightforward manner to include all the relativistic corrections to New tonian gravity is by means of post-Newtonian formalism. The post-Newtonian method relies on the idea that one perturbs around a Minkowski background. In doing so, one considers that the perturbations of the metric tensor along with those corresponding to the energymomentum tensor are both small ﬂuctuations, which can be expanded in power of v/c, being v a typical velocity in a system associated with matter and c is the speed of light [9]. One of the main reasons to examine relativistic corrections in galactic dynamics concerns to the issue of the rotation curves predicted by Newtonian theory. In Newtonian theory, rotation curves typically increase linearly near the origin up to a maximum and then vanish for large radii, while the measured circular velocity curve for the galaxies leads to small value near the center, increases linearly, then exhibits a small cusp and tends to a ﬁnite nonzero value at large radii. One can clearly recognize the contribution of the dark matter halo in order to generate the ﬂat circular velocity curve [7]. Indeed, some authors pointed out that the inclusion of post-Newtonian corrections in astrophysical models can really help to reduce the amount of dark matter needed to explain rotation curves which ﬂatten at large radii [10]. Of course, these corrections by their own cannot overcome the whole problem of generating ﬂat rotation curves, however, can reduce the amount of dark matter in relation with the Newtonian models. For the reasons mentioned above, the study of relativistic corrections within the context of kinetic theory seems to be a very promising route. Bearing this in mind, many authors devoted some eﬀorts to apply kinetic theory plus post-Newtonian method to describe astrophysical models. They explored the kinetic theory of self-gravitating gases with axial symmetry within the framework of post-Newtonian formalism applied to the case of razor thin disks, focusing on axially symmetric galaxy models[11]. In particular, they obtained the rotation curves and mass proﬁles for the post-Newtonian version of Morgan-Morgan disks and derived the virial theorem in 1PN approximation as well [11]. Continuing with this line of research, Nguyen and Pedraza studied selfgravitating system with polytropic equation of state in the PN approximation of general relativity, the physical motivation is that such equation can be used to describe many astrophysical models such as white dwarfs, neutron stars, galactic halos, and globular clusters amongst others. By solving the Einstein-Vlasov system of equations, they found a family of star clusters with anisotropic in velocity space within PN scheme. In addition, they analyzed the stability of circular orbits for radial perturbations [12]. Here, we are going to use the relativistic collisionless Boltzmann equation along with the Maxwell-J¨uttner distribution function to derive the Maxwell-J¨uttner distribution function in the 1PN approximation. In doing so, we introduce a peculiar velocity associated with the particle velocity in the gas frame, use the Tolman’s law and integrate over the peculiar velocity space. We calculate the particle ﬂow at 1PN order while energy–momentum tensor components are derived at diﬀerent PN orders, for instance, the diagonal temporal part is calculated at 1PN order but the diagonal spatial components are reported at 2PN order. In this way, we derive the macroscopic energy momentum tensor from the Maxwell-J¨uttner distribution function at 1PN order. Our result coincides with the one obtained from ﬂuids description [9]. We numerically solve the nonlinear equation associated to the gravitational potential ﬁelds. Our analysis diﬀers from the one reported for post-Newtonian polytropes in diverse manners [8]. We contrast the Newtonian with the PN proﬁles of matter density, circular velocity and gravitational potential energy. As was expected, the pressure proﬁle only includes PN corrections. For certain value radius, say rc, the gravitational ﬁelds become complex and therefore we must match the latter solution with a physical one. We show the procedure to obtain physical gravitational potentials by joining with other solutions at rc. In addition, we notice that such method leads to circular velocity proﬁles that ﬂatten at large radii. Our paper is outlined as follows. In Sec. II, we give the general derivation of the Maxwell-J¨uttner distribution function at 1PN order. We devote Sec. III to obtain the particle four-ﬂow and energy-momentum tensor within the PN approximation from the Maxwell-J¨uttner distribution function at 1PN order. We seek static solution for the gravitational potentials associated with nonlinear Poisson-like equations in Sec. IV. In Sec. V, we solve numerically the aforesaid equations, present the density, pressure and circular velocity proﬁles. In Sec. VI, we ﬁnally reexamine the issue of generating ﬂat rotation curves by gluing two diﬀerent solutions at certain radius. In Sec. VII the conclusions are stated. Throughout the article we adopt the metric signature (−, +, +, +) and we do not set the speed of light c equal to the unity for practical reasons.
Vacuum stability from vector dark matter<|sep|>The Standard Model of particle physics (SM) is an extremely successful theory, which describes the properties of all known elementary particles, notably also the characteristics of the Higgs boson. Nevertheless, the cosmology requires also the existence of the dark matter (DM), which cannot be built out of the ﬁelds included in the SM, therefore this is the major motivation to explore its extensions. In this paper we study a model of a Higgs portal with a complex scalar ﬁeld responsible for the mass generation of an abelian vector dark matter. We check, if it can explain the value of the DM relic abundance obtained from the measurements of the Planck satellite and fulﬁll constraints coming from colliders and DM direct detection experiments. We also consider the issue of the vacuum stability and discuss whether the metastability of the scalar potential, which is a feature of the SM [1], can be avoided in the extended framework. In particular, we present the modiﬁcations to the issue of stability due to the existence of the vector dark matter.
A classification of transitive links and periodic links<|sep|>Symmetry is one of the oldest and richest subjects not only in mathematics but also in many diﬀerent disciplines including engineering, designs, network models. Even though each discipline has diﬀerent prospectives of symmetry, a common interest is to realize the highest symmetry possible. In geometry and topology, symmetry plays a key role in modern research. In the present article, we will study the links of the highest symmetry :“transitive links”. A link L is an embedding of n copies of S1 into S3. Since we may consider S3 as R3 ∪ {∞}, we will assume all links are in S3 or R3 depending on our convenience. If a link has only one copy of S1, the link is called a knot. Two links are equivalent if there is an isotopy between them. In the case of prime knots, this equivalence is the same as the existence of an orientation preserving homeomorphism on S3, which sends a knot to the other knot. Although the equivalent class of a link L is called a link type, throughout the article, a link really means the equivalent class of link L. Additional terms in the knot theory can be found in [3]. One classical invariant in knot theory is the periodicity. A link L in S3 is p-periodic if there exists an orientation preserving periodic homeomorphism h of order p such that fix(h) is homeomorphic to S1, h(L) = L and fix(h) ∩ L = ∅ where fix(h) is the set of ﬁxed points of h. By the positive solution of Smith conjecture, fix(h) is unknotted. Thus, if we consider S3 as R3 ∪ {∞}, we can assume that h is a rotation by 2π/p angle around the z−axis. If L is a periodic link, we denote its factor link (S3, L)/h by L. Murasugi [16] found a strong relation between the Alexander polynomials of a periodic link and its factor link. Murasugi also found a similar relation for the Jones polynomials of L and L [17]. There are various result to decide periodicity of links [9,11,20,24,26,27]. These are all necessary conditions for periodic links using polynomial invariants of links. There is no complete classiﬁcation for periodic links yet. For the periodicity of links, the homeomorphism are all rotations. But, some of non periodic links are invariant under some homeomorphisms which are not necessary rotations. This motivates us to enlarge our interest for links of non rotational symmetries.
Detecting Majorana Bound States by Nanomechanics<|sep|>Over the past few years, nanomechanics and the ﬁeld of topological condensed matter systems have been pushing limits in their respective area of research. Nanomechanical systems have proven to be exceptional measurement devices for, e.g., mass, force, and position1–5, as well as a unique platform for studying fundamental questions concerning the quantum nature of macroscopic objects, theoretically as well as experimentally6–9. Majorana fermions are among the most intriguing features of topological states of matter. They are their own anti-particles, i.e., γ = γ†, and satisfy fermionic anticommutation relations {γi, γj} = 2 δij. A Majorana fermion has half the degrees of freedom of a Dirac fermion. This can be seen, for instance, by expressing two Majorana fermions γL,R as γR = c† + c and γL = −i(c† − c), where c and c† are the annihilation and creation operators, respectively, of a single Dirac fermion and satisfy � c†, c � = 1. Various proposals have been made about how to generate Majorana states10–17. In view of future applications, these states might be particularly useful in the ﬁeld of topological quantum computation18. Unfortunately, to date no experimental realization of Majorana fermions has been achieved. Proposed detection schemes are based on tunnel setups19–23, interferometer setups24–28 and the Josephson eﬀect12,13,15. However it is fair to say that true qualitative experimental signatures of Majorana fermions that persist in realistic systems are rather diﬃcult to predict. In this paper, we present a novel scheme for detecting Majorana bound states (MBS) at the edges of topological superconductors. Our proposal involves an oscillating electrode tunnel coupled to a topological superconductor (TS) hosting MBS (see Fig. 1). We show that the interplay between the two weakly coupled MBS, located at the edges of the TS, and the oscillating electrode gives rise to unique features in the diﬀerential conductance of the setup. We ﬁnd that in the presence of the resonator, satellite peaks appear in the diﬀerential conductance. We identify the underlying transport processes giving rise to the rich structure in the diﬀerential conductance. Furthermore, we study the dependence of the diﬀerential conductance on the eﬀective temperature of the oscillator and ﬁnd for an oscillator close to its quantum mechanical ground state a qualitative transport signature which is due to the interplay between the resonator and the Majorana bound states. The paper is organized as follows. We introduce the setup under investigation and the underlying model in Sec. II, followed by details on the calculation of the current in the setup in Sec. III. In Sec. IV, we present our results with subsections focusing on a setup with and without the resonator. Finally, we conclude in Sec. V.
Indian Language Wordnets and their Linkages with Princeton WordNet<|sep|>Wordnets (Fellbaum, 1998) have been useful in different Natural Language Processing applications such as Word Sense Disambiguation (TuﬁS¸ et al., 2004; Sinha et al., 2006), Machine Translation (Knight and Luk, 1994) etc. Linked Wordnets are extensions of wordnets. In addition to language-speciﬁc information captured in constituent wordnets, linked wordnets have a notion of an interlingual index, which connects similar concepts in different languages. Such linked wordnets have found their application in machine translation (Hovy, 1998), cross-lingual information retrieval (Gonzalo et al., 1998), etc. Given the extensive application of wordnets in different NLP applications, creation and maintenance of wordnets involve expert involvement. Such involvement is costly both in terms of time and resources. This is further ampliﬁed in case of linked wordnets, where experts need to have knowledge of multiple languages. India is a vast country with massive language diversity. According to a census in 2001, there are 122 major languages 1, out of which, 29 have more than a million native speakers. The IndoWordNet project contains wordnets of 18 of these languages. These wordnets were created using expansion approach with Hindi Wordnet as the pivot. This paper makes the following contributions: • Using mappings between Princeton WordNet and Hindi wordnet, we create and release mappings between Princeton WordNet and these 18 languages wordnet. The rest of the paper is organized as follows: Section 2. covers some background and related work needed for further discussions. Section 3. describes the released resources. Section 4. discusses different issues encountered in the creation of these datasets, followed by the conclusion and future work.
CAIXA: a Catalogue of AGN In the XMM-Newton Archive I. Spectral analysis<|sep|>Despite the unquestionable progress made in our understanding of the physics of Active Galactic Nuclei (AGN), there are still many open issues related to the X-ray properties of these objects. Some of the most discussed ones in the last years are: the highly debated nature of the soft X-ray excess in unobscured sources; the different spectral and timing properties for different classes of sources; the nature of radio emission in radio-quiet objects and its relation to the X-ray nuclear emission; the origin of the reprocessing of the primary emission from highly ionized material; the correlation of the above-mentioned phenomenology with fundamental properties of AGN, such as the Black Hole (BH) mass and the accretion rate. The most effective way to address these questions is to analyze large numbers of AGN with good-quality X-ray spectra and to perform statistical studies, taking into account data in other wavelengths and other basic properties of the objects. The European Space Agency’s (ESA) X-ray Multi-Mirror Mission (XMM-Newton) was launched on December 10th 1999. The XMM-Newton public archive has since become the repository of an enormous amount of high-quality X-ray data and a precious legacy for future missions. In particular, our knowledge of the physics of AGN has dramatically improved thanks to the large sensitivity of the European Photon Imaging Camera (EPIC) pn charge-coupled device (CCD) arrays. At this time, a systematic and homogeneous study of the EPIC pn spectra of AGN represents a necessary step to fully take advantage of this highly successful X-ray mission. In this paper, we present CAIXA, a Catalogue of AGN In the XMM-Newton Archive. This catalogue was already used by Bianchi et al. (2007) to conﬁrm with much higher conﬁdence the ‘Iwasawa-Taniguchi’ effect (i.e. the anti-correlation between the Equivalent Width of the neutral iron narrow emission line and the X-ray luminosity) and by Guainazzi et al. (2006a, a smaller catalogue was used at that period) and Longinotti et al. (2007), to assess the frequency of the relativistic component of the iron line. Here we describe our homogeneous spectral analysis of the X-ray data in CAIXA and present all the results on the parameters adopted in our best-ﬁt models. In following papers, we will present the timing analysis and properties of CAIXA and we will investigate the correlations between the X-ray and the multiwavelength properties of the sources in the catalogue.
Simultaneous Partial Inverses and Decoding Interleaved Reed-Solomon Codes<|sep|>This paper revolves around the following problem and develops its application to decoding interleaved Reed–Solomon codes beyond half the minimum distance. Simultaneous Partial-Inverse (SPI) Problem: For i = 1, . . . , L, let b(i)(x) and m(i)(x) be polynomials over some ﬁeld F with deg m(i)(x) ≥ 1 and deg b(i)(x) < deg m(i)(x). For ﬁxed τ (i) ∈ Z with 0 ≤ τ (i) ≤ deg m(i)(x), ﬁnd a nonzero polynomial Λ(x) ∈ F[x] of the smallest degree such that Jiun-Hung Yu is with the Dept. of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan 300, ROC. H.-A. Loeliger is with the Dept. of Information Technology and Electrical Engineering, ETH Zurich, 8092 Z¨urich, Switzerland. This paper was presented in part at the 2014 Allerton Conf. on Communication, Control, and Computing [1] and at the 2015 IEEE Int. Symposium on Information Theory (ISIT) [2]. Copyright (c) 2017 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org. Moreover, we will see that the SPI problem for general moduli m(i)(x) can efﬁciently be transformed (“monomialized”) into an equivalent SPI problem with monomial moduli m(i)(x) = xνi. The special case1 L = 1 was extensively discussed in [3]. In this paper, we address the generalization from L = 1 to L > 1, which is not obvious. For L > 1, the SPI problem appears to be new, but it is closely related to a number of well-researched problems in coding and computer science including “key equations” for interleaved Reed–Solomon codes [4], [5], the multi-sequence linear-feedback shift-register (MLFSR) problem of [6]–[8], and generalizations of Pad´e approximation problems [9]– [11]. However, none of these related problems shares all the mentioned properties (unique solution, degree bound, monomialization) of the SPI problem. By developing the decoding of interleaved Reed–Solomon codes around the SPI problem, we generalize and harmonize a number of key ideas from the literature, as will be detailed below. We will consider codes as follows. Let F = Fq be a ﬁnite ﬁeld with q elements. The codewords are L × n arrays over F such that every row is a codeword in some Reed–Solomon code over F. We will only consider column errors, and we will not distinguish between columns with a single error and columns with many errors. The Reed–Solomon codes (for each row) consist of the codewords �� a(β0), . . . , a(βn−1) � : a(x) ∈ F[x] with deg a(x) < k � , (3) where β0, . . . , βn−1 are n different elements of F. Note that punctured Reed–Solomon codes are included and βℓ = 0 (for a single index ℓ) is allowed. The dimension k will be allowed to depend on the row. However, for the further discussion in this section, we will assume that all row codes have the same dimension k. Such interleaved Reed–Solomon codes can equivalently be viewed as punctured Reed–Solomon codes over FqL simply by replacing F[x] = Fq[x] in (3) by FqL[x] while the evaluation points β0, . . . , βn−1 remain in Fq [4], [12], [13]. Note that symbol errors in FqL correspond to column errors in the array code. Decoding such array codes (or subﬁeld-evaluation codes) beyond the Guruswami-Sudan decoding radius [14] was studied in [4], [12], [13], [15]–[20]. Following [14], some of these papers use list-decoding algorithms [13], [16] while others use unique-decoding algorithms that return at most one codeword [4], [12], [15], [17]–[20]. The best unique-decoding algorithms can now correct t errors (column errors or FqL-symbol errors) up to with high probability if q is large [4], [12], [15]. For L ≥ n − k − 1, the bound (4) becomes which cannot be improved. (For small L, however, improvements over (4) have been demonstrated, cf. [11] and the references therein.) Speciﬁcally, for t errors with random error values (uniformly distributed over all nonzero columns), the best bound on the probability Pf of a decoding failure (due to Schmidt et al. [4]) is (Note that γ > 1, but γ ≈ 1 for any t of interest.) The bound (6) implies that the decoding algorithm of [4] decodes up to (4) errors with high probability if q is large. Another type of bound, not relying on randomness, uses the rank of the error matrix E ∈ F L×n that corrupts the transmitted (array-) codeword [20]. The decoding algorithm by Roth and Vontobel [20] corrects any t (column) errors provided that which beats the guarantee in [4] by a margin of rank(E)/2. Note that [4] and [20] use different decoding algorithms, and the decoding algorithm of [4] assumes cyclic Reed–Solomon codes (as row codes) where m(x) = xn − 1. The bound (8) can also be used with random error values. For t ≤ L, rank(E) is then likely to equal t, in which case (8) reduces to (5); for t = n − k − 1 ≤ L, (8) (by (117)) yields the same bound as (6) with γ = 1, which agrees with the bounds in [17], [19], where different decoding algorithms are used. In this paper, we deﬁne a partial-inverse condition (Deﬁnition 2) for the error pattern, which is always satisﬁed up to half the minimum distance and almost always satisﬁed almost up to the full minimum distance. If that condition is satisﬁed, then the (true) error locator polynomial is the unique solution of a standard key equation and can thus be computed in several different ways. Speciﬁcally, we will show that (8) guarantees the partialinverse condition to be satisﬁed. For random error values (as above), the probability for this condition not to hold will be shown to be bounded by (6), with the minor improvement that (7) is replaced by γ = 1. In this way, the scope of both (6) and (8) is widened. The primary decoding algorithms for interleaved Reed– Solomon codes are based on the MLFSR algorithm by Feng and Tzeng [7] with corrections by Schmidt and Sidorenko [4], [8] (but see also [21]). The complexity of this algorithm is O(L(n − k)2) additions and/or multiplications in F. (Asymptotically faster algorithms have been proposed [22] and will be discussed below.) However, the MLFSR algorithm is restricted to monomial moduli, which arise naturally from cyclic Reed– Solomon codes. Beyond cyclic codes, for L = 1, it is a classical result that decoding general Reed–Solomon codes can be reduced to a key equation with monomial modulus [28], which is amenable to the MLFSR algorithm. (However, standard accounts of that method do not allow an evaluation point βℓ to equal zero.) For L > 1, such a transformation was used in [20]. In this paper, the same effect (without any constraints) is achieved by the monomialization of SPI problems, with the additional beneﬁt that the partial-inverse condition is preserved. This transformation can be carried out, either by the Euclidean algorithm or by the partial-inverse algorithm of [3], with complexity O(L(n − k)2). Finally, we propose algorithms to solve the SPI problem. The basic SPI algorithm is of the Berlekamp–Massey type. In the special case where m(i)(x) = xνi, it looks very much like, and has the same complexity as, the MLFSR algorithm [7], [8]. However, the two algorithms are different: for L = 1, the MLFSR algorithm of [7] and [8] reduces to the Berlekamp– Massey algorithm [23] while the proposed SPI algorithm (Algorithm 3 of this paper) reduces to the reverse Berlekamp– Massey algorithm of [3]. As shown in [3], the reverse Berlekamp–Massey algorithm is easily translated into two other algorithms, one of them being a variation of the Euclidean algorithm by Sugiyama et al. [24]. The (reverse) Berlekamp–Massey algorithm and the Euclidean algorithm may thus be viewed as two versions of the same algorithm. In this paper, we extend this to L > 1: by easy translations of Algorithm 3, we obtain two other algorithms (Algorithms 10 and 11), one of which is of the Euclidean type and reminiscent of [6] rather than of [7]. (Yet another, quite different, “Euclidean” algorithm was proposed in [25].) For L > 1, no such connection between the (different) approaches of [6] and [7] has been described in the literature. However, the (reverse) Berlekamp–Massey version for monomial (or monomialized) SPI problems stands out by having the lowest complexity. As mentioned, asymptotically faster algorithms for the MLFSR problem have been presented in [22] for cyclic row codes and in [5] for general row codes, both achieving O(L3(n − k) log2(n − k) log log(n − k)). (Note that the asymptotic speed-up in n−k is bought with the factor L3.) It seems likely that such asymptotically fast algorithms can also be developed for the SPI problem, but this is not addressed in the present paper. In any case, the algorithms from [22] and [5] also proﬁt from the performance bounds and the monomialization scheme of this paper. In summary, we demonstrate that the SPI problem allows to harmonize and to generalize a number of ideas from the literature on interleaved Reed–Solomon codes. Speciﬁcally: 1) A general SPI problem can be transformed into an equivalent SPI problem with m(i)(x) = xνi. When applied to decoding, this monomialization preserves the partial-inverse condition (with the associated guarantees). We also show how the error evaluator polynomial (which is used, e.g., in Forney’s formula, cf. Section III-A3) can be transformed accordingly. 2) We show that the SPI problem can be solved by an efﬁcient algorithm of the Berlekamp–Massey type. In the Appendix, we also show that this algorithm is easily translated into two other algorithms, one of which is of the Euclidean type. For the MLFSR problem with L > 1, no such connection between the Berlekamp– Massey approach [7] and the Euclidean approach [6] has been demonstrated. However, for L > 1, the (reverse) Berlekamp–Massey version has lower complexity than the other versions. 3) Using the partial-inverse condition, we prove the Schmidt–Sidorenko–Bossert bound (6) (with γ = 1) and the Roth–Vontobel bound (8) for a range of algorithms including MLFSR algorithms (such as, e.g., [4] and [5]) and the SPI decoding algorithms of this paper. The paper is structured as follows. In Section II, we discuss the SPI problem without regard to any algorithms or applications. In particular, we prove the degree bound (2) and we discuss the monomialization of the SPI problem. In Section III, we consider the decoding of interleaved Reed–Solomon codes. The pivotal concept in this section is a partial-inverse condition for the error pattern, which guarantees that the (correct) error locator polynomial can be computed by many different (wellknown and new) algorithms. In Section IV, the bounds (6) and (8) are shown to apply to the partial-inverse condition. In Section V, we return to the problem of actually solving SPI problems, for which we propose the reverse Berlekamp– Massey algorithm. In Section VI, we adapt and apply this algorithm to decoding interleaved Reed–Solomon codes. The proof of the reverse Berlekamp–Massey algorithm is given in Appendix A. The other two versions (including the Euclidean version) of the algorithm are given in Appendix B. Section VII concludes the paper. The reader is assumed to be familiar with the basics of algebraic coding theory [26]–[28] as well as with the notion of a ring homomorphism and its application to F[x] [29]. We will use the following notation. The coefﬁcient of xd of a polynomial b(x) ∈ F[x] will be denoted by bd, and the leading coefﬁcient (i.e., the coefﬁcient of xdeg b(x)) of a nonzero polynomial b(x) will be denoted by lcf b(x). We will use “mod” both as in r(x) = b(x) mod m(x) (the remainder of a division) and as in b(x) ≡ r(x) mod m(x) (a congruence modulo m(x)). We will also use “ div” for
Composition Properties of Bayesian Differential Privacy<|sep|>Differential privacy by Dwork et al. [1], [2] is a robust privacy standard that has been used in a range of data analysis tasks, since it provides a rigorous The author Jun Zhao obtained his PhD from Carnegie Mellon University, Pittsburgh, PA 15213, USA, where he was with the Cybersecurity Lab (CyLab). He was a postdoctoral scholar with Arizona State University, Tempe, AZ 85281, USA. He is now a research fellow at Nanyang Technological University in Singapore. Email: junzhao@alumni.cmu.edu 2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC): Special Session SP-04 on “Resource-Efﬁcient, Reliable and Secure Internet of Things in the 5G Era” 978-1-5386-3531-5/17/$31.00 c⃝ 2017 IEEE foundation for deﬁning and preserving privacy. Differential privacy has received considerable attention in the literature [3]–[8]. Apple has incorporated differential privacy into its mobile operating system iOS 10 [9]. Google has implemented a differentially private tool called RAPPOR in the Chrome browser to collect information about clients [10]. A randomized algorithm Y satisﬁes ϵ-differential privacy if for any adjacent databases x and x′ differing in one record, and for any event E, it holds that P[Y (x) ∈ E] ≤ eϵP[Y (x′) ∈ E], where P[·] denotes the probability throughout this paper. Intuitively, under differential privacy, an adversary given access to the output does not have much conﬁdence to determine whether it was sampled from the probability distribution generated by the algorithm when the database is x or when the database is x′. Despite the powerfulness of differential privacy, it has recently been observed by Kifer and Machanavajjhala [11] (see also [12]–[18]) that differential privacy may not work as expected when the data tuples have dependencies. To extend differential privacy when data tuples have dependencies, Yang et al. [19] introduce the notion of Bayesian differential privacy as follows. For a database x with n tuples, let i ∈ {1, 2, . . . , n} be a tuple index in the database and K ⊆ {1, 2, . . . , n}\{i} be a tuple index set. An adversary denoted by A(i, K) knows the values of all tuples in K (denoted by xK) and attempts to attack the value of tuple i (denoted by xi). For a randomized mechanism Y = P[y ∈ Y | x] on database x, the Bayesian differential privacy leakage (BDPL) of Y with respect to the adversary A(i, K) is BDPLA(i,K)(Y ) = supxi,x′ i,xK,Y ln P[y∈Y|xi,xK] In this paper, we formally show that similar to differential privacy, Bayesian differential privacy has the following nice properties: sequential composability, parallel composability, and post-processing, as detailed below [1], [21]. The idea behind sequential composability is that if we have m algorithms Y1, Y2, . . . , Ym, where Yℓ is independently ϵℓ-Bayesian differential private for ℓ = 1, 2, . . . , m, then by feeding the re sult of Y1 into Y2, the result of Y2 into Y3, and so on, we will ﬁnally have an �m ℓ=1 ϵℓ-Bayesian differential private algorithm. For parallel composability, we consider the situation where a database is partitioned into m disjoint subsets. The ℓ-th subset is input to a Bayesian differential private algorithm Yℓ, for ℓ = 1, 2, . . . , m. Then the parallel composition of Y1, Y2, . . ., Ym will be maxm ℓ=1 ϵℓ-Bayesian differential private. The post-processing property means that a data analyst, without additional knowledge about the private database, cannot compute a function of the output of a Bayesian differential private algorithm and reduce its privacy guarantee. The rest of the paper is organized as follows. Section II presents the results on the sequential composability, parallel composability, and post-processing properties of Bayesian differential privacy. We elaborate their proofs in Sections III. Section IV surveys related work, and Section V concludes the paper.
Working Principles of Binary Differential Evolution<|sep|>The family of diﬀerential evolution (DE) heuristics, ﬁrst proposed by Storn and Price in 1995 [SP97], has become one of the most successful branches of evolutionary computation in continuous optimization and has been applied with great success to many real world problems, see, e.g., the survey [DMS16]. However, compared to the abundance of results in continuous optimization, DE for discrete search spaces is much less understood. The diﬃculties start with how to implement the inherently continuous working principles of DE in discrete search spaces. One approach is to embed the discrete optimization problem into a continuous setting and then utilize continuous DE. For instance, Pampar´a, Engelbrecht, and Franken [PEF06] employ angle modulation to generate binary strings from ﬂoating-point individuals. Engelbrecht and Pampar´a [EP07] further use the sigmoid value of the individual as the probability to generate the bit value, and also propose a normalization mapping. Much less eﬀort has been put into the design of truly discrete DE algorithms. Historically the ﬁrst to do so, to the best of our knowledge, are Gong and Tuson [GT07]. They apply the rigorous forma analysis method to derive in a generic way a DE variant for binary search spaces. Moraglio and Togelius as well as Moraglio, Togelius and Silva [MT09, MTS13] deﬁne discrete versions of DE via another generic approach, namely by requiring that certain geometric properties of the operators should be maintained. They demonstrate the usefulness of this approach not only for binary representations, but also for permutations and vectors of permutations. Recently, Santucci, Baioletti and Milani [SBM16] propose another diﬀerential mutation for permutation. To the best of our knowledge, apart from the axiomatic deﬁnitions of the different binary DE algorithms, there are no theoretical analyses of these methods so far. This contrasts the increasing theoretical understandings on other evolutionary algorithms like simple mutation-based algorithms [DJW02], the compact Genetic Algorithm (cGA) [Dro06], ant colony optimizers [Gut08, NW09], and the univariate marginal distribution algorithm (UMDA) [CTCY10]. The lack of theoretical work on binary DE could be caused by the relatively complicated dependencies in the stochastic process of a run of a DE heuristic. There are two types of the stochastic dependencies in DE, one from the reusing the same individuals when generating the mutant, and the other from the selection operator. As we shall see in this work, these dependencies lead to diﬃculties not seen in the analysis of the other evolutionary algorithms, which often treat the diﬀerent bit positions independently (apart from the ﬁtness-based selection). Our results: Since a theoretical understanding of an evolutionary algorithm can be very useful for its future use, this paper conducts a ﬁrst fundamental analysis of the working principles of the binary diﬀerential evolution (BDE) algorithm proposed by Gong and Tuson [GT07]. We concentrate on this BDE, since it is the historically ﬁrst and because we feel that its derivation via forma analysis makes it most likely that it inherits the true nature of DE from the continuous world. However, we expect that our results in a similar manner hold for other variants of BDE. We show that the stochastic dependencies discussed above lead to a behavior signiﬁcantly diﬀerent from what is observed with many other nature-inspired optimization heuristics, in particular those, for which a solid theoretical understanding exists. For example, many heuristics have the property that at any time any point of the search space can be generated (possibly with a small probability). For BDE, this is substantially diﬀerent. We show that from the random initial population, only an exponentially small fraction of all individuals can be generated in one iteration (see Theorem 1). In a similar vein, we present an objective function f and a population P such that BDE from this population with probability 1 never ﬁnds the optimum of f. Here P can be chosen exponentially large in the dimension and for each bit position each value may occur exponentially often (Theorem 3). Unlike most other optimization paradigms for bit-string representations, we show that BDE is stable in the sense of Friedrich et al. [FKK16], that is, neutral bit values are sampled with probability close to 1 2 for a long time. We prove that BDE is stable when optimizing the Needle function, in which all bits are neutral before the optimum is found. Here, precisely, we show that for a time exponential in the population size all bit values are sampled with frequencies in [ 1 The inherent dependencies in BDE prevent us from mathematically extending this stability result to arbitrary neutral bits. Therefore, similar to the mean-ﬁeld approach in statistical physics, we analyze a simpler but similar model called iBDE in which each bit position is treated independently when generating the mutant. We experimentally show the similarity of the behavior between BDE and iBDE in neutral bits and theoretically show the stability of iBDE (Theorem 11). As a contrast, extending and sharpening results from [FKK16] (partially also mentioned without proof in [SW16]), we show that in the algorithms UMDA and cGA, the sampling frequency of a neutral bit hits the absorbing boundaries 0 and 1 (or the artiﬁcial boundaries 1
Synthetically Trained Neural Networks for Learning Human-Readable Plans from Real-World Demonstrations<|sep|>In order for robots to perform useful tasks in real-world settings, it must be easy to communicate the task to the robot; this includes both the desired end result and any hints as to the best means to achieve that result. In addition, the robot must be able to perform the task robustly with respect to changes in the state of the world, uncertainty in sensory input, and imprecision in control output. Teaching a robot by demonstration is a powerful approach to solve these problems. With demonstrations, a user can communicate a task to the robot and provide clues as to how to best perform the task. Ideally, only a single demonstration should be needed to show the robot how to do a new task. Unfortunately, a fundamental limitation of demonstrations is that they are concrete. If someone pours water into a glass, the intent of the demonstration remains ambiguous. Should the robot also pour water? If so, then into which glass? Should it also pour water into an adjacent mug? When should it do so? How much water should it pour? What should it do if there is no water? And so forth. Concrete actions themselves are insufﬁcient to answer such questions. Rather, abstract concepts must be inferred from the actions. We believe that language, with its ability to capture abstract universal concepts, is a natural solution to this problem of ambiguity. By inferring a human-readable description of the task from the demonstration, such a system allows the user to debug the output and verify whether the demonstration was interpreted correctly by the system. A humanreadable description of the task can then be edited by the user The authors are afﬁliated with NVIDIA. Email: {jtremblay, thangt, styree, jkautz, sbirchfield}@nvidia.com †Work was performed as an intern with NVIDIA. Also afﬁliated with the University of Southern California. Email: molchano@usc.edu 1Video is at https://youtu.be/B7ZT5oSnRys . Fig. 1. In this work, a human stacks colored cubes either vertically or in a pyramid. A sequence of neural networks learns a human-readable program to be executed by a robot to recreate the demonstration. to ﬁx any errors in the interpretation. Such a description also provides qualitative insight into the expected ability of the system to leverage previous experience on novel tasks and scenarios. In this paper we take a step in this direction by proposing a system that learns a human-readable program from a single demonstration in the real world. The learned program can then be executed in the environment with different initial conditions. The program is learned by a series of neural networks that are trained entirely in simulation, thus yielding inexpensive training data. To make the problem tractable, we focus in this work on stacking colored cubes either vertically or in pyramids, as illustrated in Fig. 1. Our system contains the following contributions: • The system learns from a single demonstration in the real world. We believe that real-world demonstrations are more natural, being applicable to a wider set of scenarios due to the reduced system complexity required, as compared to AR/VR systems (e.g., [7]). • The system generates human-readable plans. As demonstrated in [8], this enables the resulting plan to be veriﬁed by a human user before execution. • The paper introduces image-centric domain randomization for training the perception network. In contrast with a world-centric approach (e.g., [27]), an image-centric network makes fewer assumptions about the camera’s position within the environment or the presence and visibility of ﬁxed objects (such as a table), and is therefore portable to new situations without retraining.2 2Recalibrating to determine a camera’s exterior orientation is arguably easier than creating a virtual environment to match a new actual environment, generating new training images, and retraining a network.
From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification<|sep|>The softmax transformation is a key component of several statistical learning models, encompassing multinomial logistic regression (McCullagh & Nelder, 1989), action selection in reinforcement learning (Sutton & Barto, 1998), and neural networks for multi-class classiﬁcation (Bridle, 1990; Goodfellow et al., 2016). Recently, it has also been used to design attention mechanisms in neural networks, with important achievements in machine translation (Bahdanau et al., 2015), image caption generation (Xu et al., 2015), speech recognition (Chorowski et al., 2015), memory networks (Sukhbaatar et al., 2015), and various tasks in natural language understanding (Hermann et al., 2015; Rockt¨aschel et al., 2015; Rush et al., 2015) and computation learning (Graves et al., 2014; Grefenstette et al., 2015). There are a number of reasons why the softmax transformation is so appealing. It is simple to evaluate and differentiate, and it can be turned into the (convex) negative log-likelihood loss function by taking the logarithm of its output. Alternatives proposed in the literature, such as the Bradley-Terry model (Bradley & Terry, 1952; Zadrozny, 2001; Menke & Martinez, 2008), the multinomial probit (Albert & Chib, 1993), the spherical softmax (Ollivier, 2013; Vincent, 2015; de Br´ebisson & Vincent, 2015), or softmax approximations (Bouchard, 2007), while theoretically or computationally advantageous for certain scenarios, lack some of the convenient properties of softmax. In this paper, we propose the sparsemax transformation. Sparsemax has the distinctive feature that it can return sparse posterior distributions, that is, it may assign exactly zero probability to some of its output variables. This property makes it appealing to be used as a ﬁlter for large output spaces, to predict multiple labels, or as a component to identify which of a group of variables are potentially relevant for a decision, making the model more interpretable. Crucially, this is done while preserving most of the attractive properties of softmax: we show that sparsemax is also simple to evaluate, it is even cheaper to differentiate, and that it can be turned into a convex loss function. • We formalize the new sparsemax transformation, derive its properties, and show how it can be efﬁciently computed (§2.1–2.3). We show that in the binary case sparsemax reduces to a hard sigmoid (§2.4). • We derive the Jacobian of sparsemax, comparing it to the softmax case, and show that it can lead to faster gradient backpropagation (§2.5). • We propose the sparsemax loss, a new loss function that is the sparsemax analogue of logistic regression (§3). We show that it is convex, everywhere differentiable, and can be regarded as a multi-class generalization of the Huber classiﬁcation loss, an important tool in robust statistics (Huber, 1964; Zhang, 2004). • We apply the sparsemax loss to train multi-label linear classiﬁers (which predict a set of labels instead of a single label) on benchmark datasets (§4.1–4.2). • Finally, we devise a neural selective attention mechanism using the sparsemax transformation, evaluating its performance on a natural language inference problem, with encouraging results (§4.3).
Mesh-TensorFlow: Deep Learning for Supercomputers<|sep|>Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from several major problems when training very large models. The memory required to store parameters and/or activations and the time necessary to synchronize parameters can make purely-data-parallel algorithms impossible or inefﬁcient. Different distribution strategies (model-parallelism [9]) can solve these issues, but specifying these strategies can be complicated, and the current MIMD implementations generate very large programs which can be difﬁcult to compile and to optimize. We solve this problem by introducing Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the "batch" dimension, in Mesh-TensorFlow, the user can specify any tensordimensions to be split across any dimensions of a multi-dimensional mesh of processors. A MeshTensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efﬁcient data-parallel, model-parallel version of the Transformer [21] sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state-of-the-art results on WMT’14 English-to-French translation task and the one-billion-word Language modeling benchmark.
The bow-shock and high-speed jet in the faint, 40 arcmin diameter, outer halo of the evolved Helix planetary nebula (NGC 7293)<|sep|>The evolved Helix (NGC 7293) planetary nebula (PN) is of particular importance because of its close proximity to the Sun and hence open to observation over an unprecedented range of spatial scales. Harris et al. (2007) measure a distance of only 219 pc to the white dwarf (WD 2226-210; Mendez et al. 1988) progenitor star which, with its surface temperature of 117,000 K radiatively ionizes the inner structure of the envelope ejected in its Asymptotic Giant Branch (AGB) phase to give the nebula a bright helical appearance at optical wavelengths. Originally a dMe late-type companion ﬂare star in a central binary system (Gruendl et al. 2001) was thought to be present but later (Su et al. 2007) this was ruled out in favour of the presence of a 35-150 AU diameter debris disk around WD2226210. Observations of the morphology and kinematics of many of the structures up to a diameter of 25 arcmin of NGC 7293 have been made on a variety of spatial scales in many wavelength domains and these are summarised in Meaburn et al. (2005b), Meixner et al. (2005), Meaburn et al. (2008), Matsuura et al. (2009), Meaburn & Boumis (2010), O’Dell, McCullough & Meixner (2004) & O’Dell, Henney & Ferland (2007) and references therein up to these dates. The consensus of opinion is that multiple eruptive events along different axes have occurred during the evolution of the central star as it passed from its AGB phase to its present WD state. For instance Huggins & Healey (1986), Forveille et al. (1986) and Young et al. (1999) found two expanding molecular tori emitting the CO lines but with their axes orthogonal to each other: one axis is aligned with the inner bi-polar lobes that form the bright helical structure the other with the fainter lobes (L1 & L2 in Meaburn et al. 2008) that project into the halo. This complexity is not unexpected when other evolved PNe are considered e.g. see Guill´en et al. (2013) for a recent example among many. Jets of collimated ejected material are also not unexpected e.g. see those in the PN Fg 1 (L´opez, Meaburn & Palmer 1993). Remarkably it was a very deep image in the light of the Hα plus [N II] 6548, 6584 ˚A nebular emission lines obtained with the 30-cm aperture Crete optical telescope that revealed (see ﬁg. 1c in Meaburn et al. 2005b) what appeared to be a bow-shock and a jet in the very outer 40 arcmin diameter faint halo of NGC 7293. Both of these morphological features were detected subsequently in the GALEX NUV (175- 280 nm) images and sketched in ﬁg. 6 of Meaburn et
Design and Analysis of Dynamic Auto Scaling Algorithm (DASA) for 5G Mobile Networks<|sep|>ELLULAR networks have been evolved to 4th generation (4G). Long Term Evolution-Advanced (LTE-A) has become a commonly used communication technology worldwide and is continuously expanding and evolving to 5th generation (5G). A recent report states that 95 operators have commercially launched LTE-A networks in 48 countries and the total smartphone trafﬁc is expected to rise 11 times from 2015 to 2021 [1]. Accordingly, operators are improving their network infrastructure to increase capacity and to meet the demand for fast-growing data trafﬁc in 5G networks. One of the most important technologies for 5G networks is to utilize Network Function Virtualization (NFV) to virtualize the network components in the core network which is called Evolved Packet Core (EPC). The virtualized EPC is commonly referred to as virtual EPC (vEPC) [2]. The emergence of NFV enables operators to manage their network equipment in a ﬁne-grained and efﬁcient way [3]. Indeed, legacy network infrastructure suffers from the nature that data trafﬁc usually has peaks during a day while having relative low utilization in the rest of time (e.g., in the midnight). To guarantee the Quality of user Experience (QoE), operators usually leave spare capacities to tackle the peak trafﬁc while deploying network equipment. Accordingly, the network equipment are under low utilization during non-busy periods. NFV enables operators to virtualize hardware resources. It also makes special-purpose network equipment toward software solutions, i.e., Virtualized Network Function (VNF) instances. A VNF instance can run on several Virtual Machines (VMs) which can be scaled-out/in to adjust the VNF’s computing and networking capabilities to save both energy and resources. Although the idea is just being applied to cellular networks, it has been used in the community of cloud computing. A classic case is Animoto, an image-processing service provider, experienced a demand of surging from 50 VM instances to 4000 VM instances (Amazon EC2 instances) in three days in April 2008. After the peak, the demand fell sharply to an average level [4]. Animoto only paid for 4000 instances for the peak time. In future 5G networks, it is expected that there will be heterogeneous types of trafﬁc, including trafﬁc from Human-to-Human (H2H) and Machine-to-Machine (M2M) communications. With such diverse trafﬁc types, it is very likely similar case as that in Animoto will also happen to future 5G networks. Given the fact that auto-scaling VNF instance can decrease operation cost while meeting the demand for VNF service, it is critical to design good strategies to allocate VNF instances adaptively to fulﬁll the demands of service requirements. However, it is not a trivial task. Speciﬁcally, the operation cost is reduced by decreasing the number of powered-on VNF instances. On the other hand, resource under-provisioning may cause Service Level Agreement (SLA) violations. Therefore, the goal of a desirable strategy is to reduce operation cost while also maintaining acceptable levels of performance. Thus, a cost-performance tradeoff is formed: The VNF performance is improved by scalingout VNF instances while the operation cost is reduced by scaling-in VNF instances. Given that legacy equipment usually is expensive in cellular networks, network operators usually power on network equipment all the time and try to use the equipment as long as they can to maximize Return On Investment (ROI). Thus, nowadays most operators operate old generation systems and new generation systems simultaneously. For example, many operators offer 3G and 4G services at the same time. In future 5G systems, virtualized resources would be added to boost the system performance of legacy 4G systems, leading to the evolution from EPC to vEPC1. It is expected that 5G and legacy systems will coexist. In this paper, we study the cost-performance tradeoff while considering both the VM setup time and legacy equipment capacity. In [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], they either ignore VM setup time or only consider virtualized resource itself while overlooking legacy (ﬁxed) resources. However, this is not practical for future 5G cellular networks as follows: • Although a scale-out request can be sent right way, a VNF instance cannot be available immediately. The lag time could be as long as 10 minutes or more to start an instance in Microsoft Azure and it could be varied from time to time [18]. It could happen that the instance is too late to serve the VNF if the lag time is not taken into consideration. • The capacity of legacy network equipment is another issue. For example, a network operator with legacy network equipment wants to increase network capacities by using NFV technique. The desired solution should consider the capacities of both legacy network equipment and VNFs. If the capacity of a legacy network equipment is only equal to that of one VNF, scaling-out from one VNF instance to two VNF instances increases 100% capacity. However, if the capacity of a legacy network equipment is equal to that of 100 VNF, its capacity only grows less than 1% when adding one more VNF instance. Current cloud auto-scaling schemes usually ignore this problem which is called non-constant issue [19]. In other words, the capacity of legacy network equipment has a signiﬁcant impact on the desired auto-scaling solution for future 5G systems. In this paper, we propose Dynamic Auto Scaling Algorithm (DASA) to solve the problems. In the proposed DASA, we consider that legacy 4G network equipment is powered on all the time as a block, while virtualized resources (VNF instances) are added to or deleted from the system dynamically. To the best of our knowledge, this has not been discussed in any previous literature. The VNF instances are scaled in and out depending on the number of jobs in the system. A critical issue is how to specify a suitable k, the number of VNF instances, for the cost-performance tradeoff. We propose detailed analytical models to answer this question. The cost-performance tradeoff is quantiﬁed as operation cost metric and performance metric of which closedform solutions are derived and validated against extensive discrete-event simulations. Moreover, we develop a recursive algorithm that reduces the computational complexity from O(k3 × K3) to O(k × K), where K is the total capacity of the system. Without our algorithm, it is difﬁcult to solve the problem in a short time. Our models enable wide applicability in various scenarios, and therefore, have important theoretical signiﬁcance. Furthermore, this work offers network operators guidelines to design an optimal
Silicon carbide absorption features: dust formation in the outflows of extreme carbon stars<|sep|>Stars between about 1 and 8 M⊙ will eventually evolve up the Asymptotic Giant Branch (AGB; Iben & Renzini 1983). Because of instabilities in their interior, AGB stars pulsate and throw oﬀ large amounts of mass from their surface (e.g., Vassiliadis & Wood 1993). This intensive mass loss produces a circumstellar shell of dust and neutral gas. Once the AGB star has exhausted its outer envelope, the AGB phase ends. At this point, the mass loss virtually stops and the circumstellar shell begins to drift away from the star. At the same time, the central star begins to shrink and heat up from ∼3000 K until it is hot enough to ionize the surrounding gas, at which point the object becomes a planetary nebula (PN). The short-lived post-AGB phase, as the star evolves toward to the PN phase, is also known as the proto- or pre-planetary nebula (PPN) phase. During the ascent of the AGB, the velocity of the outﬂowing mass appears to be fairly constant (e.g., Huggins et al. 1988; Fong, Meixner & Shah 2003). Therefore the dust furthest from the star represents the oldest mass loss, while material closer to the star represents more recent mass loss. Towards the end of the AGB phase the increasing impact of the thermal pulse cycles leads to an increasing mass-loss rate (e.g., Vassiliadis & Wood 1993; Villaver et al. 2002a). Such an increase in mass-loss rate (dubbed the superwind) is necessary to explain the densities seen in typical PNe (Renzini 1981). Since the invocation of the superwind, many observations of AGB stars and post-AGB stars have supported this hypothesis (e.g., Knapp & Morris 1985; Wood et al. 1992). The chemical composition of the atmospheres of AGB stars is expected to change as these stars evolve, due to convective dredge up of carbon produced in the He-burning shell. The amount of carbon relative to oxygen (the C/O ratio) is critical in determining which types of dust and molecules are present around an AGB star. The formation of extremely stable CO molecules will consume whichever of the two elements is less abundant, leaving only the more abundant element available for dust formation. Stars start their lives with the cosmic C/O ratio of ≈0.4 and are therefore oxygen-rich. In about a third of AGB stars, enough carbon will be dredged up to make C/O > 1 and therefore carbon will dominate the chemistry around these stars, known as carbon stars. Carbon stars are expected to have circumstellar shells dominated by amorphous or graphitic carbon grains, which do not have diagnostic infrared features. Another component of the dust shell around carbon stars, silicon carbide (SiC), does have an infrared spectral feature at ≈11µm and therefore has been of great interest to researchers seeking to understand the evolution of the dust shells and infrared features of carbon stars (Baron et al. 1987; Chan & Kwok 1990; Goebel et al. 1995; Speck et al. 1997; Sloan et al. 1998; Speck et al. 2005, 2006; Thompson et al. 2006). As carbon stars evolve, mass loss is expected to increase. Consequently, their circumstellar shells become progressively more optically thick, and eventually the central star is obscured. Volk et al. (1992, 2000) christened such stars “extreme carbon stars”. These stars have also been dubbed “infrared carbon stars” (Groenewegen 1994), and “very cold carbon stars” (Omont et al. 1993). Extreme carbon stars are expected represent that small subset of carbon-rich AGB stars which are in the superwind phase, just prior to leaving the AGB. Because the superwind phase is short-lived compared to the AGB phase the number of extreme carbon stars is intrinsically small. Consequently, few of these objects are known. At present there are ∼30 known extreme carbon stars in the Galaxy (Volk et al. 1992) compared to ∼30,000 known visible carbon stars (Skrutskie et al. 2001). van der Veen & Habing (1988) attempted to deﬁne a way to distinguish between oxygenrich and carbon-rich AGB stars using IRAS color-color space, which was divided into subsections according to the properties of the dusty shells are these stars (see Table 1 of van der Veen & Habing 1988). This was further reﬁned by Omont et al. (1993) who identiﬁed a population of very cold carbon stars using HCN and CO observations, and showed that the regions originally designated as extremely dusty O-rich AGB stars also contain a signiﬁcant fraction of C-rich stars. The reﬁnement of the van der Veen & Habing (1988) color-color diagram by Omont et al. (1993) deﬁned subdivisions of the seven zones in color-color space (see Fig. 1 in Omont et al. 1993). Cool carbon stars with high mass-loss rates (and little or no SiC emission) fall into regions III and IV, which had previously been assumed to deﬁne OH-IR stars (i.e. the oxygen-rich counterparts to extreme carbon stars). The numbered regions have been subdivided into smaller regions denoted by IIIa1, IIIa2, IIIb1 etc. A subset of the color-color space, covering parts of regions IIIa1c, IIIb2, IIIb2 and VIb is reproduced in Fig. 1 and includes our sample stars. SiC has long been predicted to be present in carbon star circumstellar shells, beginning with condensation theories (Friedman 1969; Gilman 1969) and continuing with the prediction of a characteristic SiC ∼11µm spectral feature (Gilra & Code 1971) and then the observational discovery of an ∼11µm emission feature in many carbon star spectra (Hackwell 1972; Treﬀers & Cohen 1974). The eﬀect of the evolving dust shell density structure on observed features, and particularly on the ∼11µm feature, have been discussed extensively (see review in Speck et al. 2005, and references therein). As the optical depth of the dust shell increases, self-absorption will diminish the ∼11µm feature and it will eventually be seen in net absorption. SiC self-absorption was found to be important in producing accurate radiative transfer models of extreme carbon stars (e.g. Volk et al. 1992), even though this previous work did not recognize SiC absorption features. These absorption features are rare and have mostly been ignored in discussions of evolutionary sequences in carbon star spectra. In fact the rarity of such absorption features led to the hypothesis that SiC becomes coated in carbon at high optical depths (e.g. Baron et al. 1987; Chan & Kwok 1990). However, meteoritic data and theoretical models do not support this hypothesis (see § 1.6 and § 3.4.1). A few extreme carbon stars have been shown to have an absorption feature at ∼ 11µm which has been tentatively attributed to SiC. This feature was discovered in the “prototype” extreme carbon star AFGL 3068 (hereafter referred to as IRAS 23166+1655; Jones et al. 1978), and was re-examined by Speck et al. (1997), which also identiﬁed three additional extreme carbon stars with this feature. Cl´ement et al. (2003) examined the absorption features of two of these extreme carbon stars (IRAS 23166+1655 and IRAS 02408+5458), and showed that their 11µm absorption features are consistent with β-SiC1 nanoparticles. The broad absorption features of IRAS 19548+3035 and IRAS 21318+5631 (also discovered by Speck et al. 1997) were attributed to SiC absorption with an interstellar silicate absorption contribution (see also Groenewegen et al. 1996). This will be discussed further in § 4.6. The absorption features in the spectra of IRAS 19548+3035 and IRAS 21318+5631 were revisited by Cl´ement et al. (2005) who suggested Si3N4 grains as the carrier. However, this hypothesis has been shown to be erroneous (Pitman, Speck & Hofmeister 2006). The failure of the Si3N4 hypothesis led Speck et al. (2005) to suggest that amorphous SiC grains may be able to account for the breadth, structure and barycentric position of the observed broad 10-13µm feature in IRAS 19548+3035 and IRAS 21318+5631. However, the dearth of amorphous presolar SiC grains seems to preclude this hypothesis (see § 1.6). 1Silicon carbide exists in many (>70) diﬀerent crystal structures, known as polytypes. See Speck et al. (1997); Daulton et al. (2003); Pitman et al. (2008) for a discussion of the polytypes of SiC. An alternative explanation for this feature is molecular line absorption, however, currently available line lists are not suﬃcient to properly assess this hypothesis (see Speck et al. 2006, and references therein). One molecular candidate which has transitions in the correct wavelength range is C3 (e.g. Zijlstra et al. 2006; Jørgensen et al. 2000), but the line lists are not readily available. Furthermore, C3 is expected to be photospheric, rather than circumstellar, which probably precludes its detection in optically obscured stars. Moreover, the theoretical spectrum of C3 from Jørgensen et al. (2000) shows a strong absorption close to the ∼ 5µm CO line, which is stronger than the ∼ 11µm feature. As will be seen in § 4.4, the spectrum of IRAS 17534−3030 does not show the 5µm absorption band and provides evidence that the observed absorption feature is not molecular in origin. Though previous research has included the eﬀects of SiC self-absorption (shown to be crucial to produce accurate models Volk et al. 1992; Speck et al. 1997, 2005), no work has been done to directly ﬁt the apparent SiC absorption feature in radiative transfer models of extreme carbon stars. Volk et al. (1992) performed radiative transfer modeling in order to match the Infrared Astronomical Satellite (IRAS; Neugebauer et al. 1984) Low Resolution Spectrometer (LRS) data for several of extreme carbon stars. They determined that the exact star temperature entered into the model was not important for the emerging spectra due to the very thick dust shells around extreme carbon stars (c.f. DePew et al. 2006; Speck et al. 2000). Their models used a ﬁxed composition (a mixture of graphite and SiC), and a ﬁxed dust condensation temperature. Groenewegen (1994) also performed radiative transfer modeling on a larger set of extreme carbon stars, but these models varied the dust condensation temperature. Again this was based on IRAS LRS data. Following up on this, Groenewegen (1995) modeled a large sample of carbon stars using amorphous carbon optical constants (Rouleau & Martin 1991), and assumed low dust condensation temperature in a fairly narrow range (650–900 K for the extreme carbon stars). Consequently the inner dust radius is larger than expected. Moreover the resulting models all have relatively low optical depths (τ11.3µm < 2). The optical depth for their IRAS 23166+1655 model was found to be τ11.3µm < 1, even though this star has an absorption feature at 11µm. Groenewegen et al. (1998) remodeled these stars, again assuming relatively low dust condensation temperatures, with similar results. Finally, Volk et al. (2000) used the improved spectral resolution of the the Infrared Space Observatory (ISO; Kessler et al. 1996) Short Wavelength Spectrometer (SWS; de Graauw et al. 1996) to examine ﬁve extreme carbon stars. In their modeling study, Volk et al. (2000) al lowed the optical depth and radial dust density distribution to vary; the resulting optical depths were relatively high (1.4-4.5 at 11.3µm), and the density of the dust shell was found to increase rapidly towards the center. This increase was interpreted as evidence of an increasing mass-loss rate within the last few thousand years, consistent with the identiﬁcation of extreme carbon stars as the ﬁnal stage of AGB star evolution. While these models did include SiC opacity data, the 11µm absorption feature was not recognized and consequently no attempt was made to ﬁt this feature in these models. A summary of the parameters of previous models for extreme carbon stars with 11µm absorption features in our sample can be found in Table 1. Interestingly, all previous models assume relatively low inner dust temperatures. This will be discussed further in § 3.4.1. Furthermore, the modeled dust density distributions suggest a relatively slow increase in mass-loss rates (1/rx, where x ≈ 2.25—3.0). In reality, dust shells are expected to have heterogeneities and anisotropies in their density structure as a result of pulsation-driven dust formation and the ensuing hydrodynamic turbulent eﬀects (e.g. Woitke 2006). These dust formation models suggest that carbon star mass-loss is expected to be modulated on several timescales, especially that of the pulsation cycle. Furthermore Woitke (2006) has suggested that the dynamics in the dust-forming zones around carbon stars lead to inhomogeneous dust formation, producing ﬁne scale structure in the density of¿ the dust envelope. In addition, while pulsation shocks are predicted to have a strong eﬀect on local conditions (e.g. Cherchneﬀ 2006), this is not reﬂected in temporal changes in the IR spectra of carbon stars (Corman et al. 2008). As will be seen in § 4, the spatial scale of the heterogeneities is small and the timescale for pulsations is short compared to the timescales associated with even the thinnest dust shells. Moreover, the inhomogeneities are expected to be wiped out over time by the hydrodynamic interactions (Villaver et al. 2002a,b). Consequently, we do not consider these small scale structures in our models. The isotopic compositions of certain grains found in primitive meteorites indicate that they originated outside the solar system and are thus dubbed “presolar”. Dust grains from AGB stars are found virtually unaltered in these meteorites, demonstrating that these grains become part of the next generation of stars and planets (Clayton & Nittler 2004, and references therein). The precise physical characteristics of these meteoritic dust grains (e.g. sizes, crystal structures, compositions) can be used to help constrain the nature of the dust we see in our astronomical observations. Silicon carbide was the ﬁrst presolar grain to be found in meteorites (Bernatowicz et al. 1987) and remains the best studied (Bernatowicz et al. 2006, and references therein). The most important ﬁndings of this work are (1) that most (∼ 99%) of the SiC presolar grains were formed around carbon stars; (2) of the AGB SiC grains, >∼95% appear to originate around low-mass carbon stars (<3 M⊙), based on nucleosynthesis models of isotopic compositions; (3) that all the SiC grains are crystalline (not amorphous); (4) that nearly all ( >∼80%) are of the cubic β polytype, with the remainder comprising the lower temperature 2H polytype; (5) that with one exception, SiC grains have not been found in the cores of carbon presolar grains (unlike other carbides: TiC, ZrC, and MoC); and (6) that the grain size distribution includes both very small and very large grains (1.5 nm → 26 µm), with most grains in the 0.1–1µm range. Single-crystal grains can exceed 20µm in size. Observations of the 11µm feature have been compared with laboratory spectra of various forms of SiC, and after some false starts it has now been attributed to β-SiC, matching the information retrieved from meteoritic samples (Speck et al. 1999; Cl´ement et al. 2003). However, there are still some discrepancies between observational and meteoritic evidence (most notably related to grain size). Prombo et al. (1993) found a correlation between grain size and the concentration of s-process elements in SiC grains taken from the Murchison meteorites. The Indarch meteorite presolar SiC grains yielded similar results (Jennings et al. 2002). In both cases, the smaller grains have higher relative abundances of s-process elements. This observation may be a result of diﬀerent metallicity sources yielding diﬀerent grain-size distributions (Lagadec et al. 2007, 2008). Alternatively, it may reﬂect an evolution in grain-size with dredge-up (Speck et al. 2005). In addition to SiC presolar grains, carbon grains are also relatively abundant and well studied (see Bernatowicz et al. 2006, and references therein). Presolar carbon grains are usually referred to as “graphite” grains, but their structures are more complex than this name infers. Presolar graphite is found in two types of spherules classiﬁed according to their external morphologies as “onion-like” and “cauliﬂower-like”. In general the graphite spherules follow a similar size distribution to the SiC grains. However, the high-density grains (ρ ≈ 2.15 − 2.20 g cm3) associated with AGB stars have a mean size of 2µm. In addition, the AGB presolar graphite spherules span a larger range of isotopic compositions than the SiC grains, possibly suggesting that they form at a wider range of times during the While the presolar SiC grains tend to be single crystals, the graphite grains regularly contain carbide grains. These carbides are enriched in s-process elements, indicative of formation around late-stage AGB stars. Many of the “onion-like” graphite grains have a core mantle structure in which the core contains disordered agglomerations of graphene2 sheets and PAH3-like products, while the mantle is composed of well-ordered graphitic concentric shells. The graphene particles have a typical size of 3-4nm. The “cauliﬂower-like” graphite grains also have a concentric shell structure, but it is less well ordered, and is composed primarily of the disordered graphene. Whether “onion” and “cauliﬂower” graphites are formed in the same outﬂows is not known. Both types of grain contain the refractory carbides and both span the same range of isotopic compositions. Whether the “onion” or “cauliﬂower” grains are more representative of grains in the outﬂows of extreme carbon stars is not known. However, even the most disordered “cauliﬂowers” or “onion”-cores are still closer to graphite than glassy carbon in structure. The least ordered grains are still considered to be agglommerations of nano-crystalline grains, rather than truly amorphous (pers. comm. K. Croat). As discussed in § 1.6.1 and § 1.6.2, refractory carbides are found inside “graphite” grains but not in SiC grains. Furthermore, SiC is not one of the carbides found in “graphite” grains. The refractory carbides (TiC, ZrC, MoC and RuC) provide more constraints on the dust formation processes around carbon stars. In particular, the formation of “graphite” spherules with TiC nuclei limits the range of C/O ratios in which these grains could form to 1 <∼C/O <∼1.2. Meanwhile, the ZrC can form nuclei at higher C/O, but the value still needs to be less than two. This is consistent with the measured C/O ratios of Galactic carbon stars, which have an average of 1.15 and a maximum of 1.8 (Lambert et al. 1986) 2Graphene is basically a single sheet of graphitic material. If it is disordered, there are some heptagons and pentagons in place of the regular hexagonal carbon structure. Graphite is the 3-d structure. In the present work, we investigate a subset of extreme carbon stars, those which exhibit the 11µm absorption feature. Through radiative transfer modeling, we investigate the nature of these dust shells. We use theoretical models and meteoritic data to limit the parameter space and thus reduce the degeneracy within the model results. In addition, we look for correlations between observed parameters, such as those that deﬁne the 11µm feature (strength, position, etc) as well as mass-loss rates and expansion velocities associated with the dust shells. Finally we determine timescales associated with the dust shells.
Neural Template: Topology-aware Reconstruction and Disentangled Generation of 3D Meshes<|sep|>Polygonal meshes, as a compact 3D shape representation, are widely used in many applications, such as modeling, rendering, and animation. In recent years, generative modeling and reconstruction of 3D meshes has received increasing interest and we may also guide the generative process by using various forms of input, e.g., images [22,42,62] and point sets [10,14,25]. Yet, typical challenges remain— how to deal with the diverse topologies of 3D meshes, and also how to effectively provide high-level controls for new shape generation, e.g., in a topology-aware manner. To directly reconstruct a 3D mesh, one popular scheme is to learn to deform the vertices of an initial template [5, 36,42,51,55,61,62], e.g., a manually-defined skeleton or a universal sphere, into the target mesh. However, the topologies of the final reconstructed meshes are typically limited by the template model. To address this, other works learn to cover a 3D mesh with planar or curved patches [22,63]; yet, the visual quality is often tampered due to the patch mis Figure 1. Our DT-Net learns to construct a topology-aware neural template (b) adapted to the input (a) and then deform it towards an accurate 3D mesh while preserving the initial (learned) topology. This decoupled design enables a disentangled latent representation of topology (ZT ) and shape (ZS), promoting controllable 3D mesh generation, e.g., remixing codes for object re-synthesis. alignment, so the resulting meshes often have rough surface appearance. While other 3D representations such as voxels [20, 60, 64, 65, 70, 71], point clouds [2, 16, 28], and implicit functions [3,21,38,43,54] have been explored, these representations typically require conversions to meshes via a post-processing step for supporting visual applications. Another drawback is that most works focus on capturing the mesh geometry directly in a single step, without providing high-level interpretability, e.g., structure or topology. So, it is particularly hard to control the mesh generation process. Some recent works tried to address this shortcoming by generating objects using parts and parts composition, e.g., in terms of voxels [67], point clouds [39,66], and meshes [17]. While the approach allows certain part-aware generation, these works highly rely on the availability and the quality of the extra parts annotations. In this paper, we present a novel framework, namely DT-Net, for 3D mesh reconstruction and generation via disentangled topology (DT). Distinctively, DT-Net enables the reconstruction of high-quality 3D meshes with diverse topologies, well-adapting to the input, e.g., images or voxels. Also, our novel design facilitates controllability in the generative process, since DT-Net implicitly learns a disentangled latent representation for the topology and shape. Therefore, we can achieve disentangled mesh generations with separate topology and shape manipulations. Figure 1 illustrates the pipeline of DT-Net. Beyond previous works, we learn a topology-aware neural template (e.g., genus of chairs) that fits each input then deform the template to reconstruct a detailed mesh. A key insight behind our design is that we decouple the mesh reconstruction into two sub-tasks: (i) topology formation for adapting various topologies; and (ii) shape deformation for reconstructing accurate objects while respecting their initial topologies. Our decoupling scheme eases the learning process and accounts for the topology, while enhancing the reconstruction quality and enriching mesh generation with diverse topologies. Another important design is that we extract a topology code (blue) and a shape code (red) from the input, to guide the learning of the two decoupled sub-tasks, respectively. By doing so, two key aspects of 3D objects, topology and shape, can be jointly learned to ensure the reconstruction plausibility, while being disentangled in the latent space, for enabling novel disentangled controls in the mesh generation process; see Figure 1 (right). Please refer to Section 3.2 for further elaborations on our framework. Method-wise, we design an end-to-end framework with the topology-learning module to first learn to produce a topology-aware neural template composed of convexes. To decouple topology learning and shape learning, we learn a family of invertible maps [23, 72] to maintain the topology between the neural template and the final reconstructed object. Also, we propose to use a dual (implicit and explicit) representation for the neural template, so it can be trainable via the implicit functions and extractable as polygonal meshes at the inference. Importantly, our approach can directly learn the topology-aware neural template without intermediate topology annotations, while well-aligning it with an inversely-deformed version of the ground-truth mesh. Both quantitative and qualitative results show that DTNet enables the reconstruction of high-quality meshes with diverse topologies, performing favorably over the state of the arts. Further, our method supports various generative applications via disentangled controls, which cannot be achieved by existing reconstruction-based methods.
Two Higgs Doublet Model with Scalar Mediation via Yukawa Interactions<|sep|>The standard model (SM) [1, 2] has already seen its limitations as it is inadequate for complete understanding of a number of observations and the new physics. Supersymmetry [3, 4], cosmic inﬂation [5–12], dark matter [13– 18], baryon asymmetry, and naturalness problem are some of these examples. In order to study new physics, various extensions of the SM are studied [19]. Scalar sector is no exception, and the discovery of the SM Higgs [20–22] serves as a strong motivation to study scalar interactions in presence of the SM Higgs. Furthermore, scalars are among the new particles being looked for [23] on the experimental front of particle physics searches. A thorough understanding of scalar interactions will be immensely helpful in identifying the new physics. Historically, studies in Yang-Mills theory [24, 25], which is the gauge sector in the SM [1, 2], has been particularly useful in understanding QCDrelated physics [26–28]. One may harbor similar expectations from pure scalar sector [29–31], particularly for richer renormalizable quantum ﬁeld theory models. Among simpler but yet potent models are the ones with Yukawa interactions which can also create larger renormalizable quartic reactions. Discovery of the Higgs boson presents an opportunity to study the sector in order to explore the masses and correlation functions in renormalizable theories. The Wick-Cutkosky (WC) model, and its variants, are among the widely explored models in scalar sector [32–36]. It is not known apriori if the new Physics is perturbatively accessible. Hence, non-perturbative approaches [37, 38] naturally ﬁnd their place in this arena. The method of Dyson Schwinger Equations (DSEs) [37, 39–43] is one of the widely used approaches for non-perturbative studies. This paper addresses a two Higgs doublet model (2HDM) [44–46] with both Higgs ﬁelds preserving the SU(2) symmetry and a real scalar singlet mediating ﬁeld 1. The theory is essentially the case of two complex doublet scalar families interacting with each other only via a scalar particle. For such a model, there are two peculiar cases. First, it presents an opportunity to observe how a particle with mass around 1 MeV, which is the case with electron and the lightest quarks, interacts with the SM Higgs under the given vertices for various couplings, technical details are given in the next section. Second, the case of the second Higgs mass in TeVs oﬀers an opportunity to understand the physics for the case of two families of particles of masses at diﬀerent orders of magnitude interacting with each other via a real scalar mediating ﬁeld. Despite the simplicity, the model has a rich parameter space. Hence, the model is studied for diﬀerent bare masses and bare couplings with diﬀerent cutoﬀ values. During the entire study, the two vertices are represented by their bare Yukawa couplings, similar to ladder approximation [42], up to certain factors due to renormalization. The details are included in the next section. The method of DSEs [37] is used to study the model. There is a companion paper which encompasses a study of a richer model than the one studied here, using a diﬀerent approach [38]. It is assumed that the model is not trivial. The assumption is supported by the fact that Higgs interaction with gauge bosons does not render the model trivial [52, 53], despite that φ4 theory [29–31] is found trivial [47–51]. 1For simplicity. one of the Higgs is termed as the SM Higgs with its physical (renormalized) mass ﬁxed at 125.09 GeV, while the other Higgs is called the second Higgs whose physical mass is determined in the parameter space of the theory.
Inferring the star formation histories of the most massive and passive early-type galaxies at z<0.3<|sep|>Planck Collaboration et al. 2015), the complete understanding of how baryonic matter evolves leading to the formation of present-day galaxies is still unclear. The presence of cold dark matter in the ΛCDM framework implies a bottom-up or hierarchical formation of structures in which the baryonic mass growth is driven by the merging of dark matter halos which progressively assemble more massive galaxies. However, recent studies showed that merging is not the only channel to build galaxies. Depending on the redshift and dark halo mass, the baryonic mass can gradually grow also thanks to ﬁlamentary streams of cold gas (T∼ 104 K) capable to penetrate the shock heated medium in dark matter halos and feed the forming galaxies (Dekel et al. 2009). The relative importance of these two channels of mass growth as a function of cosmic time is one of the key questions in order to explain how protogalaxies evolved into the diﬀerent types of galaxies that we see today in the Hubble classiﬁcation. In this context, elliptical (E) and lenticular (S0) galaxies, collectively called early-type galaxies (ETGs), have always been considered ideal probes to investigate the cosmic history of mass assembly. Indeed, holding the major share of stellar mass in the local Universe (Renzini 2006 and references therein), massive ETGs are supposed to be the endpoints of the hierarchical evolution, thus enclosing fundamental information about the galaxy formation and mass assembly cosmic history. In this regard, several studies attempted to infer the evolutionary properties of massive ETGs at z ∼ 0 (archaeological approach), or to observe directly their progenitors at higher redshifts (look-back approach) (see again Renzini 2006 for a review). The results of these studies suggest an empirical evolutionary trend, called downsizing (see Cowie et al. (1996) for its ﬁrst definition), where massive galaxies formed earlier and faster than lower mass systems. The downsizing scenario is evident in several cases of galaxy evolution (Fontanot et al. 2009). In the case of ETGs at z ∼ 0, one of the ﬁrst observational evidences can be referred to the studies of Dressler et al. (1987), Faber et al. (1992) and Worthey et al. (1992), who found more massive elliptical galaxies to be more enriched in αelements than less massive ones. These works suggested selective mass-losses, diﬀerent initial mass functions (IMF) and/or diﬀerent star formation timescales as possible explanations of the high level of [α/Fe]. Subsequent studies found the same trend of [α/Fe] with mass (e.g Carollo et al. 1993, Davies et al. 1993, Bender et al. 1993, Thomas et al. 2005, Thomas et al. 2010, McDermid et al. 2015), and led to the dominant interpretation that in more massive ETGs, the duration of star formation was substantially shorter than in less massive ones, with timescales short enough (e.g. < 0.5 Gyr) to avoid the dilution of the α element abundance (produced by Type II supernovae) by the onset of Fe production by Type Ia supernovae. This is considered one of the main evidences of the downsizing of the star formation duration. Also the age of the ETG stellar populations at z ∼ 0 show evidence of downsizing, with more massive objects being older than less massive ones. These results have been derived both in clusters (Thomas et al. 2005, 2010; Nelan et al. 2005) and in the ﬁeld (Heavens et al. 2004, Jimenez et al. 2007, Panter et al. 2007; see also Renzini 2006). Most of these studies are based on ﬁtting individual spectral features with the Lick/IDS index approach (Burstein et al. 1984, Worthey et al. 1994) which allows to mitigate the problem of the age-metallicity degeneracy (Graves & Schiavon 2008a; Thomas et al. 2005, 2010; Johansson et al. 2012a; Worthey et al. 2013). However, more recently, other approaches based on the fullspectrum ﬁtting have been developed (e.g. STARLIGHT, Cid Fernandes et al. 2005; VESPA, Tojeiro et al. 2009, 2013; FIREFLY, Wilkinson et al. 2015), and applied to samples of ETGs at z ∼ 0 (Jimenez et al. 2007, Conroy et al. 2014, McDermid et al. 2015). The results based on Lick indices are in general rather consistent with those of full spectral ﬁtting within 10 − 30 % (e.g. Conroy et al. 2014) and support the downsizing evolutionary pattern. In addition to the archaeological constraints, also the look-back studies are providing complementary constraints on the evolution of the ETGs. First of all, mas sive and passive ETGs (M ∼ 1011 M⊙) have been unexpectedly discovered in substantial number up to z ∼ 3 (e.g. Cimatti et al. 2004, McCarthy et al. 2004, Kriek et al. 2006; Gobat et al. 2012, Whitaker et al. 2013; Onodera et al. 2015, Mendel et al. 2015). For a ﬁxed mass, these ETGs are on average more compact and therefore denser than present-day analogs (Daddi et al. 2005, Cimatti et al. 2008), especially at z > 1.5 (e.g. Cimatti et al. 2012). The mere existence of these massive systems with absent star formation and old stellar ages (∼ 1 − 3 Gyr) implies that their star formation ceased at z > 2 − 3 (consistently with the downsizing scenario) and that their formation mechanism was necessarily fast and leading to a rapid assembly of compact and dense systems. It is relevant to recall here that at the time of their ﬁrst discovery, these massive, passive and old galaxies at z > 1.5 were not expected at all in theoretical models of galaxy formation (Cimatti et al. 2002, 2004). More constraints come also from the evolutionary trends emerging from statistical samples of ETGs at z > 0.5 and support the downsizing picture. For instance, the evolution of the fundamental plane indicates a decreasing formation redshift (zF ) for galaxies with decreasing mass both in clusters and in the ﬁeld (Treu et al. 2005). Moreover, if compared to the local one, the faint-end of the luminosity function up to z ∼ 1 is progressively depopulated going to higher redshift, contrary to its high-luminosity end, suggesting again that more massive galaxies assembled their mass earlier than less massive ones (Cimatti et al. 2006). Furthermore, it has been observed that the galaxy number density rapidly increases from z = 0 to z = 1 for M ≲ 1011M⊙, while it has a slower increase for more massive galaxies (Pozzetti et al. 2010, Moresco et al. 2013), starting to show a signiﬁcant variation only at z ≳ 1 for the most massive systems (M ≳ 1011M⊙) (Ilbert et al. 2010, see also van Dokkum 2005, 2010, Muzzin et al. 2013). The physical interpretation of these results on ETG evolution coming from the archaeological and look-back approaches is not trivial in the hierarchial merging scenario of ΛCDM cosmology because the mass downsizing evolution seems anti-hierchical. However, several progresses have been made in the last decades. For instance, the combination of N-body simulations of dark matter halos evolution (Springel et al. 2005, Boylan-Kolchin et al. 2009) with semi-analytic models for galaxy formation (White & Frenk 1991, Kauﬀmann et al. 1999, Springel et al. 2005, Lu et al. 2011, Benson 2012) have allowed a signiﬁcant advance. For example, De Lucia et al. (2006) were able to reproduce the age-downsizing of elliptical galaxies by invoking AGN feedback to quench the star formation earlier in more massive systems with respect to less massive ones within the standard framework of the hierarchical assembly for stellar mass. The role of AGNs in inﬂuencing galaxy evolution and quenching star formation is supported by several observations (Fabian 2012 and references therein, Cimatti et al. 2013, Cicone et al. 2014, Förster Schreiber et al. 2014); however, other models are capable to form rapidly ETGs without invoking the AGN feedback (e.g. Naab et al. 2006, 2009; Khochfar & Silk 2006, Johansson et al. 2012b). More recently, Henriques et al. (2013), (2015) found that the early build-up of low mass galaxies predicted by many models could in fact be prevented by assuming less massive systems to reincorporate the gas ejected by supernova-driven winds later than more massive objects. This kind of as sumption, although requiring further investigation, would also reproduce the observed mass assembly downsizing. However, despite the improvements on the theoretical side, many questions remain still open and the physics of massive galaxy formation is not fully understood. The empirical downsizing scenario implies that the starforming progenitors of massive and passive ETGs should be present at z ≳ 2. Thus, the identiﬁcation of these protoETGs is a crucial test to validate the downsizing picture and to link the evolutionary properties of ETGs inferred at z ∼ 0 with those derived directly at higher redshifts with the look-back approach. The current knowledge on the ETG precursors is still fragmentary and a coherent picture is lacking. However, star forming systems with diﬀerent levels of SF activity and sometimes with compact sizes have been identiﬁed at z ≳ 2 − 3 as potential star-forming progenitors of massive ETGs at z ∼ 0 (e.g. Daddi et al. 2004, Finkelstein et al. 2013, Williams et al. 2015). An additional piece of information comes from the host galaxies of QSOs at z ≳ 6, which have stellar masses up to M ∼ 1011 M⊙ and high metallicity (e.g. Fan et al. 2003), and also from the surprising identiﬁcation of a signiﬁcant number of galaxies at z ≳ 4−5, which have stellar masses up to M ∼ 1011 M⊙ and look already quiescent at these high redshifts (e.g. Mobasher et al. 2005, Wiklind et al. 2008, Juarez et al. 2009, Brammer et al. 2011, Marsan et al. 2015), although their properties are based on photometric redshifts and SED ﬁtting, since they are too faint for spectroscopic identiﬁcation. Should these galaxies be really ETG analogs at z ≳ 4−5, they would imply that a fraction of massive galaxies formed rapidly at very high redshifts (say z > 5) and with an intense star formation, if they are observed already quiescent at z ∼ 4 − 5. In this work, we apply the archaeological approach to study the properties of a sample of ETGs at 0.02 < z < 0.3, restricting our analysis only to the most extreme systems, i.e. to the most massive (M ≳ 5.6 × 1010 M⊙) and passive objects. The aim is to infer their star formation histories and place quantitative constraints on their precursors at high redshifts. In particular, instead of relying on the individual spectral features of these galaxies, we exploit the information contained in their full-spectrum, by means of the public code STARLIGHT (Cid Fernandes et al. 2005). Throughout this work, we adopt a ﬂat ΛCDM cosmology with ΩM = 0.27; H0 = 72 km s−1Mpc−1. Moreover, we always refer to galaxy stellar masses.
Lie-B\"acklund symmetry and non-invariant solutions of nonlinear evolution equations<|sep|>It is of common knowledge, that the most eﬀective method for constructing solutions of nonlinear ODEs of mathematical physics is the symmetry reduction method which brings a PDE with several independent variables down to another PDE with fewer independent variables, or even to ODE. The method can be both classical [1] and non-classical [2, 3, 4, 5]. In these cases the construction of a proper ansatz (by which we mean a general form of a invariant solution) boils down to solving a quasilinear ﬁrst order DE, therefore ansatz includes one arbitrary function and the initial equation reduces to a single diﬀerential equation with fewer independent variables (especially an ODE). In [6] and [7] the concept of conditional Lie-B¨acklund symmetry of evolution equations is proposed. By using this method one can reduce nonlinear evolution equations with two independent variables to system of ODEs. The approach is used to construct exact solutions of nonlinear diﬀusion equations in [8]. The relationship of generalized conditional symmetry of evolution equations to compatibility of system of diﬀerential equations is studied in [11]. Svirshchevskii [9] put forward the reduction method for evolution equations of the form ut = K[u] where u = u(t, x). The method is applicable if K[u]∂u is a Lie-B¨acklund symmetry operator of a linear homogeneous ODE. In this paper we use the method proposed in [10], which is a generalization of the Svirshchevskii’s method, meaning we can analyze symmetries of a nonlinear (or nonhomogeneous linear) equation together with ODEs which include, besides dependent and independent variables, parametric variables and derivatives with respect to them. As it was shown in [10], such generalization is important. For example the equation It is clear then, that the method is related to the inverse scattering transformation method. The idea is to use Lie-B¨acklund symmetries of ordinary diﬀerential equations (linear or nonlinear) for constructing solutions of evolutionary equations. In this paper we consider a nonlinear evolutionary equation that describes transport phenomena in inhomogeneous medium and apply a reduction method based on the symmetries of third order nonlinear ODEs. Note that the proposed method could be applied to not only evolution-type equations, but to diﬀerential equations of any type as well. We present the results obtained for the model medium with exponential and polynomial heterogeneity. Within the method applied, nonlinear transport equation is reduced to a system of three ODEs. After integrating (solving) the system of ODEs, we obtain exact solution of the initial equation. Since the method applied diﬀers from the classical Lie method, it does not enable to construct algorithms for generation of new solutions, or production of conservation laws. Its only advantage is the preservation of the reduction property. In addition, it doesn’t ensure that none of the solutions obtained could obtained within the classical method. Therefore there is a very important question of distinguishing truly new solutions obtained within the method proposed. Based on the fact that a set of point and Lie-B¨acklund symmetry operators (of the ODE) form a Lie algebra, we distinguish a class of diﬀusion equations whose solutions, obtained with the help of the aforementioned approach, cannot be obtained through the classical Lie method. Furthermore, it can be used to construct a large class of nonlinear evolution equation all of which are reduced to systems of ordinary diﬀerential equations by the same ansatz and possess solutions which are not invariant in the classical Lie sense.
Infrared scaling for a graviton condensate<|sep|>It is believed that the gravitational interaction is mediated by the spin-2 graviton, which can be canonically quantised around a weak curvature background [1]. A massless graviton in four spacetime dimensions will have both 2-on-shell and 6-oﬀ-shell degrees of freedom [2]. The former is responsible for describing independent dynamical modes such as gravitational waves, while the latter describes how the force is being mediated between the two masses. Despite all our eﬀorts the discovery of a graviton remains a challenging problem, see [3–5]. However, the quantum nature of a graviton leaves indelible mark in both classical and quantum systems [6–12]. Despite of the weakness in the gravitational interaction, gravity is unique among the other known fundamental interactions of nature that it generates a new length scale in the presence of a self-gravitating matter [13]. The gravitational radius for a given non-rotating mass, M, is given by rg = 2GM, which is known as the Schwarzschild radius or the gravitational radius, while the Planck length, which is determined solely by fundamental constants 1, is given by ℓp = √ G. In Ref. [14], the idea has been proposed from the corpuscular nature of a black hole – a black hole is a condensate of gravitons [14–17], whose occupation number can be denoted here by Ng ∼ (M/Mp)2 ≫ 1 for any mass M ≥ Mp. The large occupancy leads to not only weakening of the gravitational strength by ∼ 1/� Ng, but also leads to classicalization of a black hole, therefore, recovers the classical black hole spacetime geometry outside the Schwarzschild radius. In this regard, we might imagine that Ng would dictate how classical the space time of a black hole would 1We are working in natural units c = ℏ = 1, and ǫ0 = 1. The metric signature is given by (−, +, +, +) and Einstein summation convetion will be used in the text. Ng/Mp ≫ ℓp for Ng ≫ 1. Recently, a very similar result were obtained by us in a quantum system [18], where both matter and gravity were treated as a quantum entity in a perturbative regime. We found that by tracing out the non-relativistic self-gravitating matter of mass M, the graviton vacuum state is found to be that of a displaced vacuum, like a coherent state with the occupation number similar to that of Ng ∼ (M/Mp)2. For Ng ≫ 1, the gravitons can be thought of as a condensate of mass M. For a light subatomic particle, such as that of an electron, the number of gravitons by tracing out the electron is much less than unity, Ng ∼ (me/Mp)2 ∼ 10−44 ≪ 1 2 3. The aim of this paper will be two-fold. First of all, we will argue that gravity is unique in this regard as it provides an infrared scale. This infrared length scale may even be larger than that gravitational radius, L ≥ rg = 2GM, we will provide an example of this. A similar analysis in the quantum electrodynamics (QED) does not yield any such infrared length scale. In particular, we will show that by integrating out an electron in QED, the photon vacuum is that of a displaced vacuum 2Interestingly, the electron cannot be described by a gravitational metric, such as a Reissner-Nordstr¨om or a Kerr metric [19]. The metric is inherently a classical notion. 3There is another proposal, known as the fuzz ball paradigm [20], where it has been argued that the new scale in gravity will arise naturally from the quantum ﬂuctuations in the gravitational degrees of freedom [21], in particular by taking all microscopic states of string theory, namely the fuzz ball states [20]. The fuzz-ball paradigm is one of the popular contenders to resolve the black hole information-loss paradox. The idea here is that an astrophysical black hole can have a radius few Planck length greater than then the gravitational radius, i.e. rbh = rg(1 + ǫ), where ǫ < 0.5rg = 3Gm to avoid having an event horizon. There are already astrophysical constraints on ǫ, see [23, 24]. (similar to the case of a graviton), but the occupation number of photon in this case is always bounded below unity for foreseeable energies. In fact, the occupation number of photons is proportional to the ﬁne structure constant. Indeed, the ﬁne structure evolves with the energy in the ultraviolet, but it remains below unity for energies below the Planck energy. Moreover, we will further show that Bekenstein’s entropy bound in our case is always satisﬁed [26], i.e. Bekenstein’s entropy is always bounded by the energy and the distance scale. The second goal will be to generalise our earlier results of Ref. [18] by including both relativistic/non-relativistic eﬀects while integrating out the energy momentum tensor for the matter ﬁeld. To illustrate, we will consider an in-falling thin shell of matter, in an adiabatic approximation, where the vacuum changes slowly. We will show that up to the leading order in the Newton’s constant, G, by integrating out the energy momentum tensor of an in-falling thin shell, we will obtain a large occupation number of gravitons. In fact, we will further show that the infrared scale in this example is slightly larger than the Schwarzschild radius of the corresponding mass M, i.e., L ≥ rg = 2GM. Moreover, we will argue that surprisingly such an infrared scaling persists for higher curvature theories of gravity as well, but now the emergence of a new infrared length scale is diﬀerent from that of general relativity. This occurs due to the fact that higher curvature theories of gravity brings in a new mass scale, Ms ≤ Mp. In section II, we will show how by integrating out the electron, we will obtain that the photon vacuum to be displaced and the corresponding occupation number would scales as that of the ﬁne structure constant. In section III, we will generalise our earlier results of Ref. [18] for the relativistic energy momentum tensor for an arbitrary geometric conﬁguration, and ﬁnd the corresponding number of gravitons. In section IV, we consider an example of an in falling thin shell of matter and compute the graviton occupation number by integrating out the thin shell of matter. In section V, we will discuss the infrared length scale in theories of gravity within general relativity and in higher curvature theories of gravity.
Anomaly Detection via Self-organizing Map<|sep|>Anomaly detection is a classical problem and refers to identifying those samples in dataset that are signiﬁcantly different from normal samples. Usually, the abnormal samples are not accessible at the training phase, or they are not sufﬁcient enough to model its distribution, compared to its huge diversity. Moreover, there are some unexpected anomalies in the process of inference. Therefore, anomaly detection is still a challenging task. Traditional machine learning methods demand to design different pipelines for each type of product, thus the generalization performance is unsatisfactory. Supervised deep learning methods require a large number of labeled samples, but in practice it is often difﬁcult to satisfy. On the one hand, defective images are very rare in manufacturing lines. On the other hand, we cannot know all types of defects before they occur. On the contrary, the normal samples are almost the same and easily accessible. Therefore, modeling the normality of normal samples for abnormal detection using an unsupervised manner is a feasible idea. In re cent years, a large number of anomaly detection methods have been proposed. We provide an overview of existing unsupervised anomaly detection methods in the following. In order to better analyze this problem, we categorize these methods into either learning from scratch or leveraging pre-trained convolutional neural network (CNN). Learning from Scratch: Learning representations from scratch is a major way for image classiﬁcation and other tasks. Autoencoder-based methods are the most widely used out of a large number of learning representation approaches in unsupervised anomaly detection. Autoencoders (AE) [1, 2, 3, 4, 5, 6, 7, 8], variational autoencoders (VAE) [9, 10, 11] or generative adversarial networks (GAN) [12, 13, 14, 15] try to learn the distributions of normal samples by training models to reconstruct normal images exclusively using normal images. These methods usually ﬁrst compress input image to a low dimensional latent vector, and then the normal image is expected to be reconstructed by the latent vector. After that, anomalies can be detected by comparing the input image and its reconstruction at the pixel level. However, in the complex real-world datasets, autoencoder-based methods can yield good reconstruction results for abnormal images too [16], which results in failing to distinguish between normal and abnormal images. Leveraging Pre-trained CNN: Usually the sample scale of anomaly detection datasets is relatively smaller than ImageNet dataset [17], so it is inadequate to learning a good representation from scratch. Many methods have also been proposed for anomaly detection [18, 19, 20, 21] or segmentation [2, 22, 23, 24, 25] by using deep representations pretrained on ImageNet [17]. They compare patch features at the same position from target and normal image to obtain pixellevel anomaly map. The image-level anomaly score will be calculated by aggregating the anomaly score of pixels in the image. However, these methods are not robust to unaligned datasets, especially for large scales of translation and rotation. Cohen et al. [26] proposed a method to tackle the problem of unaligned images, they search the most similar patch from all positions of the 50 nearest training images. However, it is computationally intensive because of the procedure of retrieval from a large normal feature gallery. To alleviate the aforementioned problems, we proposed a novel and efﬁcient approach: self-organizing map for anomaly detection (SOMAD). It makes use of pre-trained CNN to extract the features of patches and leveraging the SOM to maintain the neighborhood relationship of embedding vectors in topology space. In addition, we have greatly reduced the search space by mapping the normal feature space into 2-dimensional space through SOM. Each patch position corresponds to a SOM node, and every patch in the training images is assigned to the nearest SOM node in topology space. Our major contribution can be summarized as follows:
Gravitational field of one uniformly moving extended body and N arbitrarily moving pointlike bodies in post-Minkowskian approximation<|sep|>Since exact solutions of Einstein’s ﬁeld equations are available only for highly idealized systems usually one is forced to resort to approximation schemes. One of the most powerful and most important approximation schemes is linearized gravity, where the ﬁeld equations in harmonic coordinates are simpliﬁed to an inhomogeneous wave equation [1, 2]. As it has been shown in [3–5] in the post-Newtonian approximation (weak-ﬁeld slow-motion approximation) the metric outside the matter distribution can be expanded in terms of two families of multipole moments: mass multipole moments ML and spin multipole moments SL. Later, in post-Minkowskian approximation (weak-ﬁeld approximation) such a set of multipole moments has been introduced by Damour & Iyer [6]. For many purposes, for instance for high precision astrometry or fundamental tests of relativity, the knowledge of the global metric of an N-body system in post-Minkowskian approximation is of fundamental importance. Presently the post-Minkowskian metric for arbitrarily moving celestial objects is known only for pointlike bodies with mass-monopoles and spin-dipoles. The metric of arbitrarily shaped, rotating, oscillating and moving bodies is a highly sophisticated and complex problem and is only known for the case of slowly moving bodies in the post-Newtonian approximation [7]. One reason for this complexity is, that one might want to deﬁne the multipole moments of a single body in its own rest-frame, with origin close to the body’s center of mass; however, if the acceleration of such a ’local’ co-moving system is taken into account corresponding multipole moments have been deﬁned only to post-Newtonian order [7, 8]. Thus, in order to study the global metric ﬁeld in terms of locally deﬁned multipoles of a realistic N-body system such as the solar system, one has to apply further approximations. Accordingly, this will be the strategy of this paper: we will ﬁrst consider an arbitrarily shaped, rotating and oscillating body ﬁrst in uniform motion, and then we treat the problem of N arbitrarily moving pointlike bodies with mass-monopoles and spin-dipoles. The article is organized as follows: the metric for an extended body with arbitrary Damour-Iyer moments, deﬁned in a co-moving system, in uniform motion is derived in section II in post-Minkowskian approximation. In section III we consider the post-Minkowskian metric for N arbitrarily moving pointlike bodies (mass-monopoles and spin-dipoles) and show that our results agree with corresponding results from the literature. Throughout the article we use fairly standard notation:
Analyzing the Flux Anomalies of the Large-Separation Lensed Quasar SDSS J1029+2623<|sep|>SDSS J102913.94+262317.9 (SDSS J1029+2623; Inada et al. 2006) is only the second “naked cusp” lens after APM 08279+5255 (Lewis et al. 2002) to be discovered, and with a 22.′′6 maximum separation between its components, it is the largest known quasar lens. These attributes alone would make it an interesting lens system, but it also exhibits one of the more dramatic examples of an anomalous ﬂux ratio which cannot be reproduced by a single central potential. Such anomalies can be produced by diﬀerential extinction (Lawrence et al. 1995; Falco et al. 1999; El´ıasd´ottir et al. 2006), microlensing by stars in the lens (Koopmans & de Bruyn 2000; Morgan et al. 2006; Poindexter et al. 2007; Anguita et al. 2008) or the presence of substructure (satellites) in the primary lens or along the line of site (Mao & Schneider 1998; Ros et al. 2000; Metcalf & Madau 2001; Dalal & Kochanek 2002; Kochanek & Dalal 2004; MacLeod et al. 2009). The latter case is of particular interest with regard to the “missing satellite” problem (Klypin et al. 1999; Moore et al. 1999; Brada˘c et al. 2002; Chiba et al. 2005; Miranda & Jetzer 2007). Originally discovered during the Sloan Digital Sky Survey Quasar Lens Search (SQLS; Oguri et al. 2006, 2008a; Inada et al. 2008), Inada et al. (2006) spectroscopically conﬁrmed that the A and B components of SDSS J1029+2623 (see Figure 1) captured by the Sloan Digital Sky Survey (SDSS; York et al. 2000) were lensed images of the same radio-loud quasar located at zs = 2.197. A mass model based on these two images predicted a potential that was inconsistent with the location of the observed lens galaxy/cluster, necessitating further investigation. Follow-up optical and spectral observations made by Oguri et al. (2008b) resolved the B component into two components, B and C, and the lensing galaxy, G1, into two components (G1a and G1b). Additional spectra conﬁrmed that C was indeed another lensed image and determined the lens redshift to be zl ≃ 0.60. Oguri et al. (2008b) modeled the positions of the three images using a single elliptical Navarro, Frenk, & White (NFW; 1997) density proﬁle centered near galaxy G1, consistent with the cluster center inferred from weak lensing and the presence of additional lensed arcs. These models predicted ﬂux ratios of A : B : C = 0.11 : 1.00 : 0.99 that are wildly at odds with the observed optical ﬂux ratios of 0.95 : 1:00 : 0.24. Simply tweaking the parameters of a cluster potential cannot ﬁx this problem because B and C are a fold pair on opposite sides of a critical curve – in a smooth potential, these components should be more magniﬁed than image A and have a ﬂux ratio of order unity. The optical ﬂux ratios alone cannot resolve this issue because they are inﬂuenced by so many physical eﬀects: lensing, microlensing, and extinction. In this letter we present the results from new radio observations of the system. Emission at these wavelengths is unaﬀected by dust or stellar microlensing, leaving only substructure as a potential explanation if the anomalies persist. As we discuss in §2, we successfully resolved all 3 images and ﬁnd that anomalies persist but diﬀer from those in the optical. In §3 we model and interpret these results, and in §4 we discuss their implications. Fig. 1.— The follow-up 6 cm radio map of SDSS J1029+2623. North is up and East is left. The clean beam shape is shown in the lower right corner. Note that this map resolves images B and C while also establishing that the radio source associated with galaxy G2 is made up of two distinct components, which we label G2a and G2b. The optical center of G2 is marked with a cross and is located at R.A. 10 29 12.49 and Dec. +26 23 32.12 (8.2 m Subaru observations). The solid line indicates the critical curves for our best ﬁt mass model discussed in §3.1.
Self-Organizing Maps as a Storage and Transfer Mechanism in Reinforcement Learning<|sep|>The use of off-policy algorithms [5] in reinforcement learning (RL) [15] has enabled the learning of multiple tasks in parallel. This is particularly useful for agents operating in the real-world, where a number of tasks are likely to be encountered, and may be required to be learned [6, 16, 21]. Ideally, as an agent learns more and more tasks through its interactions with the environment, it should be able to efficiently store and extract meaningful information, which could be useful for accelerating its learning on new, possibly related tasks. This area of research, which aims at addressing the issue of effectively reusing previously accumulated knowledge is referred to as transfer learning [17]. Formally, transfer learning is an approach to improve learning performance on a new ‘target’ task MT , using accumulated knowledge from a set of ‘source’ tasks, MS = {Ms1...Msi ...Msn }. Here, each task M is a Markov Decision Process (MDP) [11], such that M = {S, A, T, R}, where S is the state space, A is the action space, T is the transition function, and R is the reward function. In this work, we address the relatively simple case where tasks vary only in the reward function R, while S, A and T remain fixed across the tasks. For knowledge transfer to be effective, source tasks need to be selected appropriately. Reusing knowledge from an inappropriately selected source task could lead to negative transfer [8, 17], which is detrimental to the learning of the target task. In order to avoid such problems and ensure beneficial knowledge transfer, a number of MDP similarity metrics [3, 4] have been proposed. However, it has been shown that the utility of a particular MDP similarity metric depends on the type of transfer mechanism used [3]. In addition, these transfer mechanisms are generally not designed to handle situations involving a large number of source tasks. This could be limiting for both embodied as well as virtual agents operating in the real-world. For such an agent, the value functions pertaining to hundreds or thousands of tasks may be learned over a period of time. Some of these tasks may be very similar to each other, which could result in considerable redundancy in the stored value function information. From a continual learning perspective, a suitable mechanism may be needed to enable the storage of such information in a scalable manner. In the approach described here, the knowledge of a task is assumed to be contained in the value function (Q-function) associated with it. We assume that these value functions are represented using parameter weights, which are learned from the agent’s interactions with its environment. We define a cosine similarity metric within this value function weight (parameter) space, and use this as a basis for maintaining a scalable knowledge base, while simultaneously using it to perform knowledge transfer across tasks. The proposed mechanism enables the storage of value function weight vectors using a variant of the growing self organizing map (GSOM)[1]. The inputs to this GSOM algorithm consist of the value function weights of new tasks, along with any representative value function weights extracted from previously learned tasks. The resulting map would ideally correspond to value function weights representative of previously acquired task knowledge, topologically arranged in accordance with their relation to each other. As the agent interacts with its environment and learns the value function weights corresponding to new tasks, this new information is incorporated into the SOM, which evolves by growing to a suitable size in order to sufficiently represent all of the agent’s gathered knowledge. Each element/node of the resulting map is a variant of the input value function weights (knowledge of previously learned tasks). These variants are treated as solutions to arbitrary source tasks, each of which is related to some degree to one of the previously learned tasks. The aim of storing knowledge in this manner is not to retain the exact value function information corresponding to all the previously learned tasks, but to maintain a compressed and scalable knowledge base that can approximate the value function weights of the previously learned tasks. While learning a new target task, this knowledge base is used to identify the most relevant source task, based on the same similarity metric. The value function associated with this task is then greedily exploited to provide the agent with action advice to guide it towards achieving the target task. Due to random initialization, the agent’s initial estimates of the value function weights corresponding to the target task is poor. However, as it gathers more experience through its interactions with the environment, these estimates improve, which consequently improves its estimates of the similarities between the target and source tasks. As a result, the agent becomes more likely to receive relevant action advice from a closely related source task. This action advice can be adopted, for instance, on an ϵ-greedy basis, essentially substituting the agent’s exploration strategy. In this way, the knowledge of source tasks is used to merely guide the agent’s exploratory behavior, thereby minimizing the risk of negative transfer which could have otherwise occurred especially if value functions or representations were directly transferred between the tasks. Apart from maintaining an adaptive knowledge base of value function weights related to previously learned tasks, the proposed approach aims to leverage this knowledge base to make informed exploration decisions, which could lead to faster learning of target tasks. This could be especially useful in real-world scenarios where factors such as learning speed and sample efficiency are critical, and where several new tasks may need to be learned continuously, as and when they are encountered. The overall structure of the proposed methodology is depicted in Figure 1.
Warm molecular gas and kinematics in the disc around HD 100546<|sep|>Over the past decade our understanding of the structural and physical properties of discs around young stars has increased from basic theoretical modelling of the spectral energy distributions (SEDs) constrained by observations with no spatial information, to modelling based on not only the SEDs, but also spatially resolved dust observations, like scattered light images and interferometry (Pinte et al. 2008; Pani´c et al. 2008; Tannirkulam et al. 2008). Two decades ago, the ﬁrst submillimetre interferometer observations resolved the molecular gas emission spatially and this allowed major progress in understanding the disc kinematics, structure and chemistry (e.g., Beckwith & Sargent 1987; Koerner et al. 1993; Dutrey et al. 1994). A more recent example is a study of one of the brightest discs around a Herbig Ae star, HD 163296 shown in Isella et al. (2007). (Sub)millimetre gas and dust emission is the ideal probe of the global disc properties, like size, mass and radial distribution of disc material, because the bulk of the disc mass is located beyond 100 AU from the star, at temperatures of 1050 K that dominate this part of the spectrum. Disc models which include constraints of both dust and molecular gas observations have stressed the importance of analysing the gas and dust components simultaneously, in the context of a single disc model with a three-dimensional temperature and density struc Until recently, observations of rotational transitions of molecules in the submillimetre regime were focused primarily on the low-J emission from 12CO, up to the J =3-2 line (Greaves et al. 2000; Thi et al. 2001; Qi et al. 2004; Thi et al. 2004; Dent et al. 2005a). In two of the brightest and most studied sources, TW Hya and LkCa 15, the observations of higher-J transitions of 12CO, up to J=6–5 (Ek =116 K), were compared to the low-J lines, providing estimates of the gas temperature in the intermediate-height molecular layer (van Zadelhoﬀ et al. 2001a), crucial ingredients for chemical modelling of discs. These single-dish line spectra were ﬁtted using simplistic disc models, deriving a temperature of 20-40 K in the 12CO line emitting layers of LkCa 15, and more than 40 K in TW Hya. Qi et al. (2006) analysed submillimetric interferometer observations of TW Hya in the context of a disc structure based on an accretion disc model (Calvet et al. 2002). Based on 12CO J =6–5, 3–2 and 2–1 observations, they show that X-ray heating of the gas is eﬃcient in this source, in addition to the stellar radiation ﬁeld. Such diagnostics of gas heating and ionisation improve our understanding of how the gas content evolves in discs. and the Australia Telescope Compact Array (ATCA) are opening a window towards the star-forming regions of the Southern sky and are well suited to study circumstellar disc emission. These instruments also pave the path for future observations with the Atacama Large Millimetre / Submillimeter Array (ALMA), which will drastically improve our knowledge of disc structure and evolution (e.g. Guilloteau & Dutrey 2008). We use APEX receivers APEX-2a and CHAMP+ to observe the 12CO J =7–6, J =6–5, J =3–2, 13CO J =3–2 and [C I] 3P2–3P1 line emission towards the disc around the young intermediate-mass star HD 100546. A wealth of observations of dust in this bright disc (Waelkens et al. 1996; Malfait et al. 1998; Grady et al. 2001; Augereau et al. 2001; Bouwman et al. 2003; Acke & van den Ancker 2006a; Ardila et al. 2007) has motivated us to probe its molecular gas content and kinematics. The chosen transitions are particularly sensitive to the gas in the warm upper layers and kinematics of the outer disc. Our millimetre line observations probe the outer radius and inclination. The existing observational constraints on these parameters in the disc around HD 100546 provide an excellent basis for the analysis of our data. Our observations also provide a bridge toward even higher-J far infrared 12CO transitions to be observed with the Herschel Space Observatory. HD 100546 is a young B9V type, 2.5 M⊙ star, classiﬁed as a Herbig Be star due to its isolation, infrared excess and silicate emission (Th´e et al. 1994; Malfait et al. 1998). With a distance of 103±6 pc, measured by Hipparcos, this is one of the nearest Herbig Ae/Be stars. The age of the star is estimated to be greater than 10 Myr (van den Ancker et al. 1998). This makes the presence of circumstellar material intriguing, considering that discs are found to dissipate within 10 Myr in most young stars (e.g., Hollenbach et al. 2000; Hern´andez et al. 2007; Hillenbrand 2008). Based on SED modelling, Bouwman et al. (2003) postulate the presence of an inner hole in the disk with a 10 AU radius, possibly caused by a Jupiter-sized planet (see also Acke & van den Ancker 2006a). Direct evidence of cold disc material at larger radii is provided by ATCA observations of Wilner et al. (2003) at 89 GHz (3.4 mm) and 2′′ resolution, with the ﬂux of 36±3 mJy, values consistent with the 1.3 mm observations of Henning et al. (1998). They do not detect HCO+ J =1–0 line emission and speculate that photodissociation of its progenitor species 12CO, in the upper disc layers or an overall gas depletion may be the reason for this. Recent spectroastrometric observations of rovibrational 12CO transitions by van der Plas et al. (2009) suggest that this molecular species is missing from the inner disc, at least up to 8 AU from the star (also see Brittain et al. 2009). Scattered light imaging of HD 100546 reveals the disc extending up to 4′′ from the star viewed at an inclination of 50◦, and an interesting disc structure resembling spiral arms (Pantin et al. 2000; Augereau et al. 2001; Grady et al. 2001). This structure was interpreted as due to disc perturbation by a companion (Quillen et al. 2005) or a warped disc structure (Quillen 2006). Coronographic imaging by Augereau et al. (2001) shows steep surface brightness proﬁles in the environment of HD 100546 indicative of optically thin emission in the near-infrared, with surface densities as low as 10−3 g cm−2. Their images trace the emission of small dust (< 5 µm), extending out to 800 AU from the star. The authors suggest the presence of an optically thick disc with a 400 AU radius, and an optically thin ﬂattened halo or envelope farther from the star. The scattered light images hint at the presence of gas in the disc that supports the disc vertical structure. In this work, we detect and study the molecular gas, its kinematics and temperature, in the disc at spatial resolution from 7.′′7 to 18.9.′′. In Sect. 2 we present our observations of 12CO, 13CO and [C I] lines. All 12CO lines are detected, there is a tentative detection of the 13CO line, while [C I] emission is not detected. We model the spectra in Sect. 3 discussing the implications for the disc size, mass and kinematics. We identify the regions dominating the observed lines, and derive their temperatures. Section 4 summarises our results.
ProposalCLIP: Unsupervised Open-Category Object Proposal Generation via Exploiting CLIP Cues<|sep|>Object proposal generation aims to predict a number of category-agnostic bounding box proposals for all objects in an image. It serves as a fundamental and crucial step towards many higher-level tasks, such as object detection [12, 23, 30, 40], object segmentation [5, 13, 38] and image captioning [19, 24]. How to effectively generate as few as possible proposals to cover all objects is the key challenge in object proposal generation. Traditional proposal generation methods [1,7,36,43,46] often utilizes low-level cues (e.g., color, texture, gradient and/or edge) to select proposals from sliding window boxes. In recent years, deep-learning-based methods [18,22,30,46]
3D Numerical Simulations of f-Mode Propagation Through Magnetic Flux Tubes<|sep|>Magnetic ﬂux tubes couple the diﬀerent layers of the solar atmosphere, and waves propagating along the magnetic-ﬁeld lines are a possible source of the mechanical-energy transport required to heat the chromosphere and corona. The waves also provide a helioseismic means of constraining the properties of the magnetic tubes which is complementary to observational studies with SOHO/MDI data (Duvall, Gizon, and Birch, 2006) and Hinode (Fujimura and Tsuneta, 2009). In the latter work the authors found that the observed Poynting ﬂux is substantial when compared to the ﬂux required to heat the chromosphere and corona. Unfortunately with observations at a single height they were unable to identify what mixture of m = 0, axisymmetric sausage-modes, and m = 1, kinkmodes, they were observing. Theoretical studies of the interaction of ﬂux tubes 1 Observatory of Algiers, CRAAG, Algiers, Algeria, email: k.daiﬀallah@craag.dz 2 University of Sciences and Technology (USTHB), Faculty of Physics, Algiers, Algeria 3 Max-Planck-Institut f¨ur Sonnensystemforschung, Katlenburg-Lindau, Germany and waves have been performed in the past, for example Roberts and Webb (1979), Wilson (1980), Spruit (1981), Spruit (1982), Bogdan and Kn¨olker (1989), Solanki (1993), Bogdan et al. (1996), Hasan and Kalkofen (1999), Tirry (2000), Gizon, Hanasoge, and Birch (2006), Jain and Gordovskyy (2008), Hanasoge et al. (2008), Hanasoge and Cally (2009). It is now well established that slender magnetic ﬂux tubes permit the propagation of the two basic types of magnetohydrodynamic waves: The longitudinal tube waves (sausage modes) with azimuthal wave number m = 0, which are axisymmetric, excited by pressure ﬂuctuations. The transversal tube waves (kink modes) with m =± 1, which are nonaxisymmetric and describe the incompressible undulations of the tube, supported by magnetic tension. However, in these various papers, simplifying assumptions were used, such as the thin-ﬂux-tube approximation, isothermal or unstratiﬁed atmospheres, and the neglect of higher-m modes. Hanasoge et al. (2008) included a stratiﬁed atmosphere and used the thin-ﬂux-tube approximation to evaluate the scattering matrix associated with an f-mode wave interacting with a ﬂux tube. The thin-ﬂux-tube approximation works when the radius of the tube is smaller than it scale height which was 260 km. For this reason the largest tube that they considered was 160 km. Numerical treatment of ﬂux tubes of arbitrary size is straightforward and robust. The approach is already yielding interesting results in the context of sunspots, see e.g. Cameron, Gizon, and Daiﬀallah (2007), Hanasoge (2008), Khomenko, Collados, and Fellipe (2008), Cameron, Gizon, and Duvall (2008), Khomenko et al. (2009). In this paper we use numerical simulations to explore the response of vertical, small scale ﬂux-tubes with radii between 200 km and 3 Mm. The paper is structured as follows: In Section 2 we present the background model and the equations describing the wave propagation. In Section 3 we present the results of a series of convergence tests. These are essential since we are considering tubes with small radii. In Section 4 we study the scattered wave ﬁeld, Section 5 is devoted to mode conversion, before discussing the results in Section 6.
You Only Need One Model for Open-domain Question Answering<|sep|>Open-domain Question Answering (Open QA) is a knowledge-intensive task that ﬁnds the answer for the given question from a large-scale knowledge corpus that can easily surpass millions of documents. Thus, how to store and refer to the knowledge at such scales is important in terms of both performance and scalability for Open QA systems. Traditional systems rely on information retrieval engines such as Lucene. These score the relevance of knowledge to a given query by lexical overlaps between them in a sparse representation space based on TF-IDF or BM25 (Chen et al., 2017; Wang et al., 2018; Yang et al., 2019). However, recent advances in neural language modeling have enabled two new lines of approach; 1) referring to internal knowledge parameterized in the model (Brown et al., 2020; Petroni et al., 2019; Roberts et al., 2020), and 2) referring to external knowledge retrieved by matching query and knowledge in dense representation spaces (Karpukhin et al., 2020; Lee et al., 2019; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021b). Despite the simplicity of the approach, parametric models have limitations such as a large number of model parameters that require large compute for both training and inference and nonexpandable knowledge without re-training. Their implicit knowledge reference also makes it hard to ﬁnd supporting knowledge and often results in hallucinations (Shuster et al., 2021). The current dense retrieval models have advantages over parametric models on these issues (Karpukhin et al., 2020; Guu et al., 2020; Lewis et al., 2020). But most retrieval models only have a weak coupling between and separate parameters for the reader, reranker (if any), and retriever that limits these models from achieving optimal end-to-end training and efﬁcient use of the total model capacity. In this paper, we propose a single language model YONO (You Only Need One model) that can refer to external knowledge via its internal attention functions, which are trainable in a fully end-to-end manner. We achieve this by generalizing the retrieval and reranking as internal passagewise attentions. At the lower retrieval layers, the query and passages are separately encoded allowing pre-computation of all the passage representations. Then passage-wise hard attention is applied to retrieve initial relevant passages from the entire knowledge base. While it would be optimal to retrieve passages based on cross-attention between the query and all the passages (Khattab and Zaharia, 2020), it is computationally intractable. Hence, we approximate this attention by a passagewise hard-attention layer using decoupled query and passage representations. The representations of the initial relevant passages are further encoded jointly with the query representation to compute more expressive coupled representations. These are used to select only the more relevant passages using another passage-wise hard-attention in the reranking layer. The representations of the ﬁnal set of passages are then encoded by transformer encoders for deeper representations that are fused in the decoder to generate the answers. We train this architecture fully end-to-end by self-supervised pre-training and weakly supervised ﬁne-tuning without passage labels. Our contributions are twofold; • A single model that generalizes retrieval, reranking, and reading as internal attention functions. We show that this model trained end-to-end signiﬁcantly improves the retrieval performance by leveraging a training signal from the answer generation decoder to allow better gradient ﬂow across the whole model in Section 6.1. It also achieves better utilization of the model parameters, outperforming a stand-alone reader with the same number of parameters by 7.1% and 3.2% on NQ and TQA respectively as shown in Section 6.2. • A method to train this architecture in a fully end-to-end manner. We show our pre-training method requires 51.5% fewer pre-training tokens compare to the previous the state-of-theart approach in Section 7.2.
Efficient Cavity Searching for Gene Network of Influenza A Virus<|sep|>High order structures as the cliques and the cavities of complex networks are shown in Fig.1, which often have interpretable meanings in reality. The problem of searching for higher order structures in complex networks belongs to the topological ﬁeld [1][2]. For instance, the cavity in complex networks maps to homology groups in algebraic topology. With the increase of computational ability these days, the signiﬁcance in the reality of high order complex networks has attracted more and more attention of researchers and the theory is supplemented [3]. There are various ﬁndings relevant to the high order structure in complex networks. By exploring the brain network, humans can better understand how neurons in the brain work. By exploring animals’ genetic regulatory networks (GRNs), Sinha S et al. [4] offered new directions for human beings to understand better of animal behaviors. By exploring plants’ GRNs, De Clercq I et al. [5] predicted a series of new transcription factors (TFs) functions in Arabidopsis thaliana cells. The other vital issue for GRNs research is how to transfer genomes (text) to GRNs (graph). X Zhang et al. [6] offer a solution to infer GRNs from genomes by employing the path consistency algorithm (PCA) based on conditional mutual information (CMI). Another answer came from Huynh-Thu VA et al. [7], they developed a program named GENIE3, a procedure to recover GRNs from multifactorial expression data based on a variable selection with ensembles of regression trees. It can potentially capture high order conditional dependencies between expression patterns, remains acceptable computing resource consumption, and is easy to implement. The genome of the inﬂuenza A virus is composed of eight segments of single-strand negative RNA. With the classiﬁcation of HA and NA, the inﬂuenza A virus has 16 subtypes HA and nine subtypes NA [8][9][10]. Following fast mutation of the viral genome, antigen escaping, drug-resistance, and virulence enhancing will occur [8][9][10]. In order to understand the evolution of the inﬂuenza virus, scientists tried to search high order structures by using 0-1 programming. But we are sure those approaches cannot well-solving the problem when facing a large-size network within acceptable timeconsuming. Consequently, there is an urgent to ﬁnd a shortcut that provides a good balance between efﬁciency and precision to get high order structures in complex networks. In this work, we provide a method to ﬁnd potential cavities in the gene network of the inﬂuenza A virus named Flu-Network based on HGN[11]. The proposed method is validated on a public inﬂuenza virus dataset which consists of 4538 viruses each containing eight segments of single-strand negative RNA. Compared with baseline (Fully-connected Neural Networks [12], Convolutional Neural Networks [13], Graph Convolution Neural Networks, and Attention Convolutional Neural Networks [14]), our method signiﬁcantly reduced the computation time and achieved a precision of about 80%. Importantly, we provide an algorithm that balanced computation time and precisions, so that we can further explore the biological signiﬁcance represented by the cavities. To sum up, we make the following contributions: (1) A method is proposed to predict cavities in computable networks with features. (2) A k-clique reconstruction algorithm is proposed to ﬁlter low order features to exclude structures that obviously cannot form cavities. (3) Experimental results demonstrate that the proposed method can well-balance efﬁciency and accuracy, achieving 80% precision in minutes while ordinary algorithms should take a few days.
Strategies for spectroscopy on Extremely Large Telescopes. I - Image Slicing<|sep|>This is the first part of a study of strategies to produce instruments for Extremely  Large Telescopes (ELTs) that are both affordable and usable before the full benefits  of Adaptive Optics delivering near-diffraction-limited imaging become available. It has long been realised that ELT instruments matched to natural seeing (0.3-0.7  arcsec FWHM depending on conditions and wavelength) must be very large and equipped with unfeasibly fast cameras. This is a simple consequence of the fact that  the characteristic disperser dimension (in Littrow configurations, simply the length  along the optical axis) must scale with telescope diameter for fixed resolving power if  the slitwidth is constant; and the conservation of Etendue in classical optical systems.  To defeat the former problem requires a reduction in the slitwidth since this allows the  disperser dimension (and the beam diameter for a fixed blaze angle) to be reduced  while delivering the same resolving power. The problem is that reducing the slitwidth  also reduces the throughput by a large factor (if correctly oversampled). One solution  is to exploit recent improvements in integral field unit (IFU) construction (e.g  Dubbeldam et al. 1994, Allington-Smith et al. 1996a,b) to slice the input image  efficiently into numerous thin slices that can be reformatted into a single long pseudoslit. This requires extra detector pixels to accommodate the elongated slit and larger  optics to accommodate the increased field angles. This approach also opens up the possibility of directing the light in each slice to a  separate spectrograph which could be of simple, compact construction and thus easy  to replicate. This has synergy with instrument concepts such as MOMSI (Evans et al.  2006) and MOMFIS (Cuby et al. 2006) which divide the field into separate pieces,  each of which is processed by an independent IFU. The same multiplexed unit  spectrographs can thus be used whether it is the field or the slit that is sliced.  Furthermore such an approach allows an orderly transition between dealing with  natural seeing, in which case there is no spatial information to be recovered, and  improvements in image quality as the AO systems mature, allowing spatial  information to be recovered. From the point of view of the spectrograph, it is simply a  question of whether you sum over all the pixels along the slit or attempt to recover  spatial information from the same pixels: different software, same hardware. In this paper, the potential benefits of slicing are examined by a simple model,  described in §2, that attempts to account for all the main optical effects which might  prevent the advantages being realised in practice. This is coupled with a costing  model, in the spirit of those long used in industry, to estimate the costs of instruments  of characteristic types on ELTs. This is based on a calibration using current 8-m  telescopes of broadly similar characteristics to the ELT concepts in an attempt to  reduce the uncertainty in this considerable extrapolation. This is presented in §3. In  §4, results are obtained to determine the best strategy for designing instruments of this  type for a 40-m telescope such as the proposed European ELT.
Capacity Region of Vector Gaussian Interference Channels with Generally Strong Interference<|sep|>channel probabilities p (y1y2 |x1x2 ) of (y1, y2) ∈ Y1 × Y2 given (x1, x2) ∈ X1 × X2. The receiver i, i = 1, 2, is required to decode Xi from the received signal Yi. The capacity region of this channel is
Towards time-dependent, non-equilibrium charge-transfer force fields: Contact electrification and history-dependent dissociation limits<|sep|>As an illustration of a system with history-dependent forces consider the following thought experiment: a neutral gold cluster and a neutral sodium cluster are prepared separately and placed at a large separation in a UHV chamber. The two clusters are then moved to close proximity and separated later in such a way that no atoms have transferred between the clusters. Since sodium has the smaller work function, the gold cluster will have picked up electrons from the sodium cluster after the separation. As a result, the two ﬁnal clusters will carry opposite charge and thus attract each other through a long-range Coulomb force, which was not there initially. This implies that the atomic interactions between – but also within – the two clusters diﬀer between the initial and the ﬁnal state. This is the case even if we allow for a demon enforcing the atoms to take identical positions at the beginning and at the end. Current force ﬁelds are intrinsically unable to reﬂect such changes in the atomic interactions, because they assign to each atomic conﬁguration an unambiguous set of forces. Even charge equilibration (QE) or charge-transfer methods [1,2] fail to to describe the electrostatic ﬁelds that arise due to the contact-induced charge transfer. They determine fractional charges as single-valued (vector) functions of the instantaneous atomic coordinates just like conventional density functional theory provides a unique electronic charge densities for a given atomic conﬁguration. While attempts to incorporate time dependence into density functional theory-based approaches to the electronic state are thriving [3], virtually no attention has been paid to the question how to tackle history-dependent fractional charges with classical force ﬁelds. Being able to do so would not only proof useful to reproduce non-equilibrium charge transfer processes as they occur in our thought experiment. It might also become possible to simulate many other systems and processes which are not amenable to simulations based on current force-ﬁelds, or contain too many particles for calculations necessitating the solution of time-dependent quantum mechanics. One such example, discussed in this work, is the dissociation of an NaCl molecule in solvents, which can result in neutral or singlycharged dissociation products depending on the solvent polarity. Another example, which we discuss in detail in a separate work, is the discharge of a small-scale battery [4]. The proper description of electron transfer upon contact formation is a crucial aspect in the all-atom simulation of the discharge of a battery, since closing and opening of an electric switch involves the formation and the break
Expolring Architectures for CNN-Based Word Spotting<|sep|>Word Spotting in handwritten documents refers to the image retrieval task with the goal to obtain a list of word images based on a relevance ranking for a given user’s query. Since the introduction of word spotting in handwritten documents by Manmatha et. al. [1], many promising approaches were proposed. Over the years the performance achieved by different approaches steadily increased mainly by introducing more powerful feature representations. In [2] a Convolutional Neural Network (CNN), trained with the backpropagation algorithm, was successfully applied on a recognition task consisting of handwritten digits. Encouraged by the great success of CNNs in recognition tasks [3] [4], Sudholt et. al. [5] proposed the TPP-PHOCNet, a CNN based on the attribute representation proposed by Almazan et. al [6] namely the Pyramidal Histogram of Characters (PHOC). In 2015 He et. al. [7] introduced the Residual Network (ResNet) and achvied state-of-the-art results in image classiﬁcation. Despite of an enormous depth the ResNets can be efﬁciently trained by proposing residual modules containing skip-connections. Only one year later the Densely Connected Convolutional Network (DenseNet) [8] was introduced with an enhanced approach concerning skip-connections. In contrast to ResNets, which are based on the summation of layer output and skip-connection, the DenseNet is built upon the respective concatenation. This results in the connection of every layer to all subsequent layers and hence the name ”densely connected”. In this work, we explore the TPP-PHOCNet, ResNet, and DenseNet on three standard word spotting benchmarks. All three architectures were adjusted and speciﬁcally designed for word spotting, by using the PHOC attribute representation. We evaluate the CNNs for both the Query-by-Example (QbE) and Query-by-String (QbS) scenario. This work shows that deeper networks as well as newer architectures do not necessarily perform better. Sec. II presents segmentation-based word spotting methods and gives a brief review of the explored CNN architectures. The CNN architectures we used for this work are presented in Sec. III followed by the evaluation in Sec. IV. Finally a conclusion is drawn over the proposed exploration in Sec. V.
Exact (1+1)-dimensional flows of a perfect fluid<|sep|>The problem of solving the (1 + 1)-dimensional ﬂow of a relativistic perfect ﬂuid has a quite long history in particle physics. It has been ﬁrst investigated in the pioneering work [1] where relativistic hydrodynamics has been introduced for describing high-energy multiparticle scattering. Together with the other pioneering work of Ref.[2] they are considered as the founding papers of the modern applications of hydrodynamics to heavy-ion collisions. Ref.[1] has been followed by studies on the same guideline [3–9]. The Gaussian rapidity dependence prediction for the “Landau ﬂow” found in [1] consistent with the observed multiplicity distributions has inspired subsequent works [10–12]. It has been revived recently [13–15] in connection with the experimental results on heavy-ion collisions at ultra-high energies [16]. The other well-known pioneering work analyzing the (1 + 1)-dimensional ﬂow of a relativistic perfect ﬂuid is thus Ref.[2] (with a precursor [17]). Here, the boost-invariant solution of the (1 + 1)-dimensional ﬂow, the “Bjorken ﬂow”, allows for quantitative predictions valid for the central rapidity region of heavy-ion reactions. It provided a ﬁrm theoretical basis for the prediction of the Quark-Gluon Plasma produced in subsequent heavy-ion colliders. In fact, it is now realized that the ﬂow of relativistic particles created by the collisions can be well described by hydrodynamics, at least during some intermediate stage of the reaction where one observes the creation of a speciﬁc phase of Quantum Chromodynamics, namely the Quark-Gluon Plasma (QGP) [18]. Recent works on the hydrodynamic behavior of the QGP [19] uses numerical simulations of hydrodynamics, with the aim of solving them in a realistic way, including 4-dimensionality of space-time, initial and ﬁnal conditions of the hydrodynamic regime, viscosity and other transport coeﬃcients, realistic equation of state. However, it is useful to reconsider the initial [1, 2] problem, namely ﬁnding the exact analytic solutions of hydrodynamic equations in the simpliﬁed set-up of a perfect ﬂuid ﬂow in the longitudinal direction with constant speed of sound. As we shall see, this problem has not yet been solved. There are quite a few motivations to follow this path, besides being the missing piece of a long lasting theoretical physics problem. On the phenomenological ground, it is known that in a ﬁrst stage (important for later evolution, as discussed already in the seminal papers [1, 2]), the hydrodynamic ﬂow is mainly (1+1)-dimensional, i.e. can essentially be described in the kinematic relativistic subspace deﬁned by proper-time τ and space-time rapidity η. On a more theoretical ground, the recently found Gauge/Gravity connection [20–22] between relativistic hydrodynamics and gravity in an higher-dimensional space through the AdS/CFT correspondence motivates completing the study of exact solutions of hydrodynamical equations. For instance in (1+1) dimensions, the ‘Bjorken ﬂow” of a perfect ﬂuid in a strongly coupled gauge theory is put in one-to-one correspondence [21] with the time-dependent 5-dimensional gravity conﬁguration of a Black Hole escaping away in the ﬁfth dimension. Going beyond the “Bjorken ﬂow” is an important open question for the application of AdS/CFT correspondence to plasma physics. Hence, making progress in the exact solution of the hydrodynamic equations in (1 + 1) dimensions may be quite useful in a modern perspective. The state of the art we have to begin with is the following. The hydrodynamic equations are a priori non-linear and as such are diﬃcult to handle exactly through analytic methods. Only few particular exact solutions have been found. Apart the noticeable contributions of the pioneering studies, namely the analytic asymptotic solution of [1] (the “Landau ﬂow” solution), and the boost-invariant solution of [2] (the “Bjorken ﬂow” solution), there were only few interesting exact solutions given in the literature for speciﬁc values of the dynamical parameters (see e.g. [23–26], [27–30]). To our knowledge, a general solution for the relativistic (1+1)-dimensional ﬂow of a perfect ﬂuid is still lacking. Recently, two developments on exact solutions of the (1+1)-dimensional ﬂow appeared, which are the building blocks of the present work. On the one hand, aone-parameter family of solutions, interpolating between the “Bjorken ﬂow” and the “Landau ﬂow” was derived [31]. They were named harmonic ﬂows since they are obtained assuming that the physical rapidity y is an harmonic function of the light-cone kinematic variables, condition which is valid both for the “Bjorken ﬂow” and the “Landau ﬂow”. On the other hand, it was possible using the formalism of the Khalatnikov potential [3] to derive exact solutions of the (1+1)-dimensional entropy ﬂow as a function of rapidity [32]. The Khalatnikov potential method makes use of a hodograph transformation, allowing for a substitution of the kinematic light-cone variables by the hydrodynamic ones, namely temperature and rapidity, in order to transform the initially nonlinear mathematical problem, posed by the hydrodynamic equations, into a linear one. In the present paper we show how, by combining both approaches, i.e. the “harmonic ﬂow” and the Khalatnikov potential approach, one generates an inﬁnite-dimensional linear basis of exact solutions, making a sizable step towards the general solution of the relativistic (1+1)-dimensional ﬂow of a perfect ﬂuid. Our plan is the following: In section II, we provide a reminder on the Khalatnikov potential method [3] and recall those results obtained in Refs. [31, 32] for the harmonic ﬂow solution and its entropy ﬂow which we will use here. In section III, we introduce the notion of regular (resp. irregular) solutions obtained by integration (resp. derivation) from the “harmonic ﬂow” and give ﬁrst generic examples of solutions. Focusing in section IV on regular solutions, we derive the more general set of solutions by solving appropriate polynomial equations in two variables. Section V is devoted to a discussion of the general solution. A ﬁnal section VI provides a summary of our results and an outlook on the prospects for a complete solution of the exact (1+1)-dimensional ﬂows of a perfect ﬂuid.
Welsch Based Multiview Disparity Estimation<|sep|>Disparity estimation has been studied extensively in the literature and many different approaches have been taken for this problem. Most existing approaches focus on stereo pairs and excellent results have been achieved in this context [1] [2] [3] [4]. However, the intuitive assumption that adding more views of a given scene will make disparity estimation more accurate does not actually hold. In scenes with occlusions, it turns out that as more views are added, with larger baselines, the increased areas of occlusion pose problems for most disparity estimation algorithms [5]. This is unfortunate, since larger baselines allow for greater precision in the estimated disparity [6]. Current hand-crafted methods of multiview disparity estimation with large numbers of views generally attempt to deal with this by using explicit occlusion based reasoning. This is done to reduce the inﬂuence of occluded areas on the estimated disparity. These techniques often use heuristics such as conﬁdence based measures [7] or speciﬁc visibility reasoning [5] [8] [9]. These are often based on preliminary estimates of the disparity ﬁeld themselves, so that early errors can potentially be reinforced. Other approaches treat occluded areas simply as outliers [10] [11] [12] and reduce their effect on the solution. This is conceptually similar to the use of robust cost functions in optical ﬂow, reducing the inﬂuence of outliers on the solution without explicitly identifying them. Common robust cost functions include the Huber norm [13] and similar L1-like norms [14] [15] the Geman and McClure norm, the Lorentzian norm [16], the Tukey norm [17] and the Welsch loss function [18]. In this paper we experimentally show that occlusions are a key problem for extending stereo disparity to a large number of views. We also propose a novel method of dealing with these occlusions in a multiview context using a Welsch loss function based data term. In particular, we propose an automatic selection process for Welsch loss parameter σd, a progressive approach to including the multiple views and a warping strategy that uses a disciplined multiple hypotheses method for upsampling the disparity ﬁeld. We evaluate the proposed method using a synthetic dataset with 31 views of the same scene as well as a 4D Light Field Benchmark training dataset [19]. This paper is organised as follows. The proposed Welsch-L1 multiview disparity algorithm is detailed in Section 2. The progressive inclusion of views is detailed in Section 3. The warping strategy which uses a disciplined method for upsampling disparity ﬁelds is discussed in Section 4. The performance of the proposed algorithm is evaluated in Section 5. Conclusions are provided in Section 6.
Learning to generate one-sentence biographies from Wikidata<|sep|>Despite massive effort, Wikipedia and other collaborative knowledge bases (KBs) have coverage and quality problems. Popular topics are covered in great detail, but there is a long tail of specialist topics with little or no text. Other text can be incorrect, whether by accident or vandalism. We report on the task of generating textual summaries for people, mapping slot-value facts to onesentence encyclopaedic biographies. In addition to initialising stub articles with only structured data, the resulting model could be used to improve consistency and accuracy of existing articles. Figure 1 shows a Wikidata entry for Mathias Tuomi, with fact keys and values ﬂattened into a sequence, and the ﬁrst sentence from his Wikipedia article. Some values are in the text, others are missing Figure 1: Example Wikidata facts encoded as a ﬂat input string. The ﬁrst sentence of the Wikipedia article reads: Mathias Tuomi, (born September 30, 1985 in Espoo) is a professional squash player who represents Finland. We treat this knowlege-to-text task like translation, using a recurrent neural network (RNN) sequence-to-sequence model (Sutskever et al., 2014) that learns to select and realise the most salient facts as text. This includes an attention mechanism to focus generation on speciﬁc facts, a shared vocabulary over input and output, and a multi-task autoencoding objective for the complementary extraction task. We create a reference dataset comprising more than 400,000 knowledgetext pairs, handling the 15 most frequent slots. We also describe a simple template baseline for comparison on BLEU and crowd-sourced human preference judgements over a heldout TEST set. Our model obtains a BLEU score of 41.0, compared to 33.1 without the autoencoder and 21.1 for the template baseline. In a crowdsourced preference evaluation, the model outperforms the baseline and is preferred 40% of the time to the Wikipedia reference. Manual analysis of content selection suggests that the model can infer knowledge but also makes mistakes, and that the autoencoding objective encourages the model to select more facts without increasing sentence length. The task formulation and models are a foundation for text completion and consistency in KBs.
The Consistency of Fermi-LAT Observations of the Galactic Center with a Millisecond Pulsar Population in the Central Stellar Cluster<|sep|>The ability of gamma-ray observatories to provide a window on dark matter annihilation signals has been known for some time, (e.g. [1]). The recent manuscript by Hooper & Goodenough [2] performs a detailed analysis to extract the signal in the gamma-ray of the dynamical center of our Milky Way Galaxy as observed by the Large Area Telescope (LAT) aboard the Fermi Gamma-Ray Space Telescope. That work interpreted the morphology and spectral feature of the signal within the inner 1.25◦ (175 parsec radius) from the Galactic dynamic center to be inconsistent with any known astrophysical sources, and further claimed for it to be accounted for by the presence of annihilating light (∼8 GeV) dark matter. In this short note, I show that the signal as seen by Hooper & Goodenough (hereafter HG) is from the Galactic Central stellar cluster and is consistent with prior observations of gamma-ray emission from massive stellar globular clusters with a population of millisecond pulsars (MSP) [3]. Figure 1. Shown is the spectrum E2dN/dE of the HG source (thick black points with errors), and its best ﬁt power law with exponential cutoﬀ curve (blue). The HG spectrum power-law index is consistent with the globular cluster gamma ray emission of Omega Cen (green), NGC 6388 (cyan) and M 28 (magenta) [3]. The spectrum of the Geminga pulsar (light grey) [4] has a nearly identical peak energy and exponential cutoﬀ and a softer power-law index than the HG source. The spectra of Omega Cen and M 28 are shifted by +0.14 dex and +0.34 dex in energy, respectively, to illustrate the consistency in power-law index. NGC 6388 is not shifted in energy. The HG source is consistent, within errors, with the shape of the intrinsic spectrum of all the plotted globular cluster sources. All sources are normalized to the HG peak ﬂux.
A slow gravity compensated Atom Laser<|sep|>The conceptual similarities between the coherence of laser light and the coherence of Bose-Einstein condensed samples has been a driving force for the ﬁeld of cold quantum gases. Similarly to the laser, coherent matter waves hold the promise of improving precision measurements and fundamental tests of quantum physics. However, there is a striking diﬀerence between the laser and cold quantum gases that still has to be overcome. Current exponents with cold quantum gases rely on the technique of forced evaporative cooling to achieve the desired temperatures and densities. Since this cooling technique depends on collisions between the particles, conservative magnetic or optical potentials are used to conﬁne the samples. While many similarities still apply in this trapped case, the conﬁnement impedes the production of a bright coherent matter wave output. It was realized soon after the production of the ﬁrst Bose-Einstein condensates (BEC), that the mechanism for forced evaporation could also be used as an outcoupler for matter wave packets [1]. This technique also allowed for the production of long pulses [2,3], limited only by the size of the initial BEC. Due to the small momentum of the outcoupled atoms, this technique is however limited by the inherent interaction with the remaining BEC fraction [4]. In a diﬀerent approach Raman transitions were hence used to impart a larger momentum to the outcoupled atoms [5,6] and it was shown that these outcouplers lead to a strongly improved output beam quality [7]. All of these outcoupling techniques rely on the transfer of atoms to magnetically untrapped states. Yet many recent experiments use all optical or hybrid potentials to produce BEC. Hence outcoupling techniques were also developed for samples conﬁned in dipole potentials [8,9] and optical lattices [10]. In addition to these techniques to produce coherent matter wave beams, powerful analysis techniques have been developed to investigate their coherence properties and both ﬁrst and second-order coherence were conﬁrmed [11,12,13]. Current experimental eﬀorts focus on methods to continuously replenish a BEC while simultaneously outcoupling a coherent beam. Both the replenishment of a BEC in an optical dipole trap [14] and simultaneous pumping and outcoupling [15] of limited duration were demonstrated. However completely new experimental approaches such as continuous condensation in a beam [16] or continuous loading of a trap [17] may be necessary to achieve this ambitious goal. The similarity between a laser beam and the atom laser points at another major experimental challenge. While the internal degrees of freedom can be controlled precisely in quantum gases, control of the external degrees of freedom of an outcoupled beam poses a larger challenge. The divergence of the beam and its mode structure have been investigated intensely [18,19,4,20,
Analytic solutions for marginal deformations in open superstring field theory<|sep|>Ever since the analytic solution for tachyon condensation in open bosonic string ﬁeld theory [1] was constructed by Schnabl [2], new analytic technologies have been developed [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and analytic solutions for marginal deformations were recently constructed [16, 17].1 We believe that we are now in a new phase of research on open string ﬁeld theory.2 Extension of these new technologies to closed string ﬁeld theory, however, does not seem straightforward. The star product [1] used in open string ﬁeld theory has a simpler description in the conformal ﬁeld theory (CFT) formulation when we use a coordinate called the sliver frame which was originally introduced in [37]. It has been an important ingredient in recent developments. Closed bosonic string ﬁeld theory [38, 39, 40, 41, 42, 43] and heterotic string ﬁeld theory [44, 45], however, use inﬁnitely many non-associative string products, and we have not found any coordinate where simple descriptions of these string products are possible. On the other hand, extension to open superstring ﬁeld theory formulated by Berkovits [46] is promising because the string product used in the theory is the same as that in open bosonic string ﬁeld theory. In this paper we construct analytic solutions for marginal deformations in open superstring ﬁeld theory. We ﬁrst review the solutions for marginal deformations in open bosonic string ﬁeld theory. The solutions take the form of an expansion in terms of the deformation parameter λ, and analytic expressions to all order in λ have been derived when operator products made of the marginal operator are regular [16, 17]. When the operator product of the marginal operator with itself is singular, solutions were constructed to O(λ3) by regularizing the singularity and by adding counterterms [17]. The goal of this paper is to construct analytic solutions in open superstring ﬁeld theory when operator products made of the marginal operator and the associated superconformal primary ﬁeld of dimension 1/2 are regular. It will be a starting point for constructing analytic solutions when these operators have singular operator products. We ﬁrst simplify the equation of motion for open superstring ﬁeld theory by ﬁeld redeﬁnition. We then make an ansatz motivated by the structure of the solutions in the bosonic case and solve the equation of motion analytically. The solutions in the superstring case turn out to be remarkably simple and similar to those in the bosonic case. The ﬁnal section of the paper is devoted to conclusions and discussion. We learned that T. Erler independently found analytic solutions for marginal deformations in open superstring ﬁeld theory [47] prior to our construction. 1 For earlier study of marginal deformations in string ﬁeld theory and related work, see [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]. 2 See [33, 34, 35, 36] for reviews.
Breakdown of light transport models in photonic scattering slabs with strong absorption and anisotropy<|sep|>In photonic scattering media, such as paint, foam and tissue, the refractive index varies spatially causing incident waves to be scattered and absorbed [1–6]. Understanding the transport of light in such scattering media is crucial for many application areas, such as atmospheric and climate sciences [7–10], oceanography [11, 12], biophysics [13–16], powder technology [17, 18], and solidstate lighting [19–22]. The transport theory describes the propagation of wave in scattering media, notably in a widely-used realistic situation like a slab in three dimensions (3D), as shown in Fig. 1(a). The theory describes the transfer of intensity and neglects interference eﬀects including diﬀraction [1]. For all practical purposes the transport theory is rigorous and only in exceptional cases such as the situations of very strong elastic scattering and Anderson localization of light where interference predominates, the transport shows features beyond the predictions of transport theory [2]. The basic diﬀerential equation used in transport theory is the radiative transfer equation (RTE), which is equivalent to Boltzmann’s equation used in the kinetic theory of gases and neutron transport [1, 23, 24]. The most fundamental quantity is the speciﬁc intensity I(r,ˆs) that describes the average power ﬂux density at position r in a given direction ˆs within a unit solid angle and a unit frequency band [1]. Due to its dependency on both the position r and the direction ˆs, it is challenging to solve I(r,ˆs) directly. The most popular method to solve the RTE for light is the Monte Carlo simulation of light transport [25–33], a statistical method that converges to the exact solution of the RTE. To obtain a high accuracy, however, this method comes with the cost of extremely long computation times [34], high computational power requirements with concomitant high energy consumption. The complexity of transport theory and the tedious resource-consuming Monte Carlo simulations have stimulated the development of analytical approximations to the RTE [35–40]. These analytical approximations are sustainable alternatives to Monte Carlo simulations, since computations consume much less resources. In addition, despite impressive advances made by graphics processing unit (GPU) based Monte Carlo simulations in terms of speed [30, 31], analytical methods are significantly faster. Moreover, in certain conﬁgurations, such as the slab geometry, the results of these analytical approximations match the accuracy of simulations. A widely used analytical approximation to the RTE is the PN approximation (See Appendix A for full solution for a slab geometry), where the dependence on both variables is separated [24] by expanding the speciﬁc intensity I(r,ˆs) in products of complete sets on the domains of r and ˆs Here, ψm l (r) are the spatial components, Y m l (ˆs) are the Laplace spherical harmonics [41, 42], and N is the order of the approximation that determines the number of terms in Eq. (1). The analytical PN approximations are mostly used for simple sample geometries such as a slab and a sphere, and their accuracies depend notably on (i) the order N of the approximation and (ii) on the optical properties of the medium. As N approaches inﬁnity, the PN approximation yields exact solutions. In realistic cases, such as a 3D slab, the order is rarely higher than N = 3 as the mathematical complexity increases rapidly with increasing N and becomes computationally demanding. 1 In this work, we thoroughly validate the P1 and P3 approximations for a slab geometry [44]. FIG. 1. (a) Incident plane waves with intensity I0 are scattered by scatterers (spheres) inside a slab and leave as scattered waves (speckle) with intensity Iout. The refractive indices of the slab and of the medium outside are nslab and nout, respectively. (b) An isotropic scatterer that scatters in all directions with equal probability, corresponding to a constant phase function. (c) An anisotropic scatterer that scatters more in the forward direction. (d) An absorbing scatterer, where the loss of intensity is depicted as thinner arrows after the scattering event. In (b,c) the solid arrows are incident and scattered (ˆs) directions, the dashed arrows are other possible scattering directions, and the arrow lengths indicate the probability to scatter into that direction. The centers of the scattering spheres in (b,c,d) are at r. The ﬁrst-order analytical approximation P1 to transport theory is the diﬀusion theory [1], which is widely used to extract transport parameters from opaque media with isotropic scatterers (see Fig. 1(b)) and with negligible absorption. For a slab shown in Fig. 1(a), this opaque conﬁguration amounts to the thickness L being much larger than the transport mean free path ℓtr and much smaller than the absorption mean free path ℓabs: If the scattering is dominantly in the forward direction, ℓtr increases (see Fig. 1(c)), and ℓabs decreases if the scatterers have signiﬁcant absorption (see Fig. 1(d)). Meretska et al. [45] deﬁned a physically more informative 1 Odd positive integers are chosen for N since odd order approximations are known to be more accurate than even orders, as in the latter the angular integrands are discontinuous [43]. validity range using the three-parameter space (a,g,b) spanned by the albedo a, the anisotropy g, and the optical thickness b. A practically relevant parameter space also requires consideration of the internal reﬂection at the slab boundaries [46, 47], so we add as a 4th parameter the refractive index contrast ∆n2 Here, nslab and nout are the refractive index of the medium inside the slab that surrounds the scatterers and the index of the medium outside the slab (typically free space), respectively. Once the (a,g,b,∆n2) parameter set is known, the solution of PN approximation is fully determined. In some parts of the (a,g,b,∆n2) parameter space, the PN approximations predict unphysical behavior, such as a negative energy density. We call these regions unphysical ranges. An example of an unphysical result of the P1 approximation is shown in Fig. 2(C,D). In Fig. 2(D) the diﬀusion theory predicts an unphysical negative energy density. The diﬀuse ﬂux F given in Fig. 2(C) is unphysical, since the theory predicts erroneously an incident diﬀuse ﬂux F, whereas a reﬂected ﬂux opposite to the incident direction of light is required at the incident boundary (left boundary in Fig. 2). FIG. 2. Examples of physical and unphysical results. (A) Diﬀuse ﬂux F and (B) average intensity U computed with the P1 or diﬀusion approximation for isotropic (g = 0.001) scattering with little absorption (a = 0.999) are physically sensible. (C) Diﬀuse ﬂux F and (D) average intensity U using P1 for anisotropic scattering (g = 0.99) and strong absorption (a = 0.4). In (C) the red arrow on the left boundary indicates the unphysical direction of F and in (D) the unphysical negative average intensity U is highlighted with red hatches. Black dashed vertical lines represent the boundaries of the slab with optical thickness b = 3 and ∆n2 = 0.245 typical of a polymer slab in air. The direction of F is given by the orange arrows, and the red dashed horizontal line indicates the zero level. RTE, namely, the P1, the P3, and the P3 + δE(4) approximations. The P3 approximation is a widely used higher-order approximation to RTE [22, 48–50], especially popular in biophysics [13, 36, 38]. The P3 + δE(4) approximation uses a modiﬁed phase function within the P3 approximation to increase the accuracy of the forward scattering region [13, 36, 51] (see Appendix B). The unphysical ranges of each approximation are found by scanning the (a,g,b,∆n2) parameter space and looking for the regions where the conditions (4) are violated. In addition, the relative error maps are obtained by comparing the PN approximations to extensive Monte Carlo simulations. Our simulation code is based on the work of Prahl et al. [25], with the addition of multiple internal reﬂection similar to a Fabry-P´erot cavity [52], where only the multiply reﬂected intensities are considered but no interference, see Ref. [53].
BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification<|sep|>Recent studies have exposed the importance of biomedical NLP in the well-being of human-beings, analyzing the critical process of medical decisionmaking. However, the dialogue managing tools targeted for medical conversations (Zhang et al., 2020), (Campillos Llanos et al., 2017), (Kazi and Kahanda, 2019) between patients and healthcare providers in assisting diagnosis may generate certain insigniﬁcant perturbations (spelling errors, paraphrasing), which when fed to the classiﬁer to determine the type of diagnosis required/detecting adverse drug effects/drug recommendation, might provide unreasonable performance. Insigniﬁcant perturbations might also creep in from the casual language expressed in the tweets (Zilio et al., 2020). Thus, the classiﬁer needs to be robust towards these perturbations. Generating adversarial examples in text is challenging compared to computer vision tasks because of (i) discrete nature of input space and (ii) preservation of semantic coherence with original text. Initial works for attacking text models relied on introducing errors at the character level or manipulating words (Feng et al., 2018) to generate adversarial examples. But due to grammatical disﬂuency, these seem very unnatural. Some rule-based synonym replacement strategies (Alzantot et al., 2018), (Ren et al., 2019) have lead to more natural looking examples. (Jin et al., 2019) proposed TextFooler, as a baseline to generate adversaries for text classiﬁcation models. But, the adversarial examples created by TextFooler rely heavily on word-embedding based word similarity replacement technique, and not overall sentence semantics. Recently, (Garg and Ramakrishnan, 2020) proposed BERT-MLM-based (Devlin et al., 2019) word replacements to create adversaries to better ﬁt the overall context. Despite these advancements, there is much less attention towards making robust predictions in critical domains like biomedical, which comes with its unique challenges. (Araujo et al., 2020) has proposed two types of rule-based adversarial attacks inspired by natural spelling errors and typos made by humans and synonym replacement in the biomedical domain. Some challenges include: 1) Biomedical named entities are usually multi-word phrases such as colorectal adenoma. During token replacement, we need the entire entity to be replaced, but the MLM model (token-level replacement) fails to generate correct synonym of entity ﬁtting in the context. So, we need a BioNER+Entity Linker (Martins et al., 2019), (Mondal et al., 2019) to link entity to ontology for generating correct synonyms. 2) Due to several variations of representing medical entities such as Type I Diabetes could be expressed as ’Type One Diabetes’, we explore numeric entity expansion strategies for generating adversaries. 3) Spelling variations (keyboard swap, modiﬁcation). While we evaluate on two benchmark datasets, our method is general and is applicable for any biomedical classiﬁcation datasets. In this paper, we present BBAEG (Biomedical BERT-based Adversarial Example Generation)1, a novel black-box attack algorithm for biomedical text classiﬁcation task leveraging both the BERT-MLM model for non-named entity replacements combined with NER linked synonyms for named entities to better ﬁt the overall context. In addition to replacing words with synonyms, we explore the mechanism of generating adversarial examples using typographical variations and numeric entity modiﬁcation. Our BBAEG attack beats the existing baselines by a wide margin on both automatic and human evaluation across datasets and models. To the best of our knowledge, we are the ﬁrst to introduce a novel algorithm for generating adversarial examples for biomedical text whose success attack is higher than the existing baselines like TextFooler and BAE (Garg and Ramakrishnan, 2020). The overall contributions of the paper include: 1) We explore several challenges of biomedical adversarial example generation. 2) We propose BBAEG, a biomedical adversarial example generation technique for text classiﬁcation combining the power of several perturbation techniques. 3) We introduce 3 type of attacks for this purpose on two biomedical text classiﬁcation datasets. 4) Through human evaluation, we show that BBAEG yields adversarial examples with improved naturalness.
Enhancing or suppressing spin Hall effect of light in layered nanostructures<|sep|>Spin Hall eﬀect (SHE) is a transport phenomenon, in which an applied ﬁeld on the spin particles leads to a spin-dependent displacement perpendicular to the electric ﬁeld direction [1–3]. The SHE of light can be regarded as a direct optical analogy of SHE in electronic system where the spin electrons and electric potential are replaced by spin photons and refractive index gradient, respectively [4–6]. The SHE of light is sometimes referred to as the Fedorov-Imbert eﬀect, which was predicted theoretically by Fedorov [7], and experimentally conﬁrmed by Imbert [8]. The spin-dependent transverse shift in the SHE of light is generally believed as a result of an eﬀective spin-orbital interaction, which describes the mutual inﬂuence of the spin (polarization) and trajectory of the light beam [9]. Recently, the SHE of light has been extensively investigated in diﬀerent physical systems. In a static gravitational ﬁeld, the photon Hamiltonian shows a new kind of helicity-torsion coupling, resulting in a novel birefringence phenomenon: photons with distinct helicity follow diﬀerent geodesics [10]. In optical systems, the SHE of light is observed directly in the glass cylinder and its fundamental origin is related to the dynamical action of the topological Berry-phase monopole in the evolution of light [11]. The SHE of light can also be observed in scattering from dielectric spheres [12]. In particular, a giant SHE of light can be produced by subwavelength displacements of a nanoparticle [13]. Even in free space, the SHE of light can be observed on the direction tilted with respect to beam propagation axis [14]. In plasmonic systems, a spin-dependent splitting of the focal spot of a plasmonic focusing lens was demonstrated and explained in terms of a geometric phase [15]. In semiconductor physics, the SHE of light has been observed in silicon via free-carrier absorption. The interesting result suggests The SHE may oﬀer an eﬀective way to manipulate the spin particles, and open a promising way to some potential applications, such as in dense data storage, ultrafast information processing, and even quantum computing [17–19]. The generation, manipulation, and detection of spin-polarized electrons in semiconductors and nanostructures deﬁne the main challenges of spin-based electronics. Similar challenges also exist in spin-based photonics. The SHE of light may open new opportunities for manipulating photon spin and developing new generation of all-optical devices as counterpart of recently presented spintronics devices. In this paper, we will study the SHE of light in layered nanostructures in which the refractive indices of their constituent materials vary between highindex regions and low-index regions. Such an environment presents to photons as an analogy of semiconductor presenting potential to electrons [20], thus presenting some imaginable interesting properties of the SHE of light. The paper is organized as follows. First, we want to establish a three-dimensional propagation model to describe the SHE of light in layered nanostructure. The Fresnel coeﬃcients are no longer real in the layered nanostructures, so it is necessary for us to obtain a more general expression. Next, we attempt to reveal what roles the Fresnel reﬂection and transmission coeﬃcients play in the SHE of light. We ﬁnd that the Fresnel coeﬃcients present sine-like oscillations and the spin-dependent splitting of wave-packet centroid signiﬁcantly depends on their ratio. Finally, we want to explore the secret underlying this interesting phenomenon. The result shows that the SHE of light can be readily modulated, i.e., enhanced or suppressed, via tuning the optical resonance in layered nanostructures.
Reflexive spatial behaviour does not guarantee evolution advantage in prey--predator communities<|sep|>Modelling of the dynamics of biological communities is of great importance. The pioneering fundamental works [1, 2] started the development of mathematical modelling of the dynamics of various biological populations (see also [3–6, 13, 15]). Here populations are supposed to be a kind of chemical reactor where various “chemical” reactions run (namely, reproduction, elimination and other types of interactions of organisms). Studying spatially distributed communities, one faces the stunning prevalence of the papers based on “reaction – diﬀusion” ideology. Nonetheless, a coincidence of a solution of such system and real dynamics usually brings a kind of misconception: a diversity and abundance of the (structurally stable) solutions of such “reaction – diffusion” systems exceeds any really available trajectory families. Thus, one always is able to ﬁgure out PDE (or ODE) with the behaviour pretty close to an observed one. The key issue is that all the organisms, including microorganisms, diﬀer in their microscale behaviour[57] from the chemical compounds and relative chemical reactions. Vito Volterra, the founder of mathematical population biology, was very keen in this point, when implemented a chemically based equation system for a description of a community behaviour. Yet, the “reaction – diﬀusion” systems make the basis for modelling of spatially distributed populations and communities [5–13, 15–19]. This approach puts on a strong constraint on the properties of organisms under consideration: – organisms must transfer randomly and aimlessly; – organisms must have no memory, and ﬁnally, – the greatest majority of the transfers in space must be of a small (or even inﬁnitesimally small) scale. Obviously, none of these constraints hold true (see, e. g., [20–22]) even for microorganisms [23–26]. Another problem of the “reaction – diﬀusion” approach is that long distance transfers have extremely low probability ∼ exp{−l2} (here l is a typical distance of a transfer). A diﬀusion, in such capacity, means that changing a habitation site for another one (located inﬁnitesimally close to the original one), a being has to arrange de novo a nest (or any other type of habitation). Yet, any signiﬁcant change in environmental conditions never take place at the small distances. This fact follows in a loss of any advantages from a transfer; just contrary, a small scale transfer would result in the increased senseless wastes of all the vital resources (time, eﬀorts, increased risk of an outer attack, etc.), while a change of environmental conditions would be inﬁnitesimally small. Thus, a migration makes sense only when it is long enough. This fact makes a change of continuous spatial pattern for a discrete one quite natural and clear. Essentially new class of model must be implemented to overcome those discrepancies. The core principle of these models is that a transfer of beings in space must meet the evolution optimality conditions and result in a growth of a population viability[58]. To get that, beings must transfer in space smartly; it means that a transfer must bring a maximization of net reproduction (an average oﬀspring number per capita determined for a suﬃciently long line of generations), see details in [23, 35–37]. Here we consider the model of a two-species spatially distributed community, where the species interact following the “prey – predator” pattern. Both species are split on two subpopulations each occupying a station. Migration is a transfer from one station to another; no other movements in space (ordinary in a real situation) are assumed to aﬀect a community dynamics. Both species (in various combinations) are supposed to implement a reﬂexive behaviour. Modeling of a behaviour where few agents are to maximize the same payoﬀ function is rather sounding in a variety of science ﬁelds ranging from sociology, ethology, psychology or environmental sciences [27–29]) to quite theoretical issues in the theory of optimal control and game theory [30, 31]). A comprehensive study of the dynamics of such systems requires a straight and unambiguous problem formulation. The formulation, in turn, is mainly determined by a speciﬁc system under consideration where a conﬂict interaction (or competition) takes place. That was Vladimir Lefevre [32–34] who pioneered the studies of a conﬂict behaviour. A wide spread of mathematical modelling ideology over the studies of biological systems surprisingly brought very few eﬀorts in implementation of a conﬂict behaviour into the biological systems dynamics studies [27–31]. A study of the dynamics of a spatially distributed biological community is a good point to apply the conﬂict analysis techniques. In general, an impact of a spatial patterns on the dynamics of biological communities makes a complicated and nontrivial problem. In this paper, we present an approach to describe a conﬂict behaviour of a spatially distributed biological community including those dynamical peculiarities which result from the targeted and smart behaviour of agents, in a conﬂict. We shall concentrate on a study of classical two-species prey – predator community. It means that the organisms of the ﬁrst species (preys) exist due to an external resource, but the organisms of another species (predator) have a solely source of a food from the predation of those preys; this system is a classical object of a study in mathematical ecology [35].
Variant-based Equational Unification under Constructor Symbols<|sep|>Equational uniﬁcation of two terms is of special relevance to many areas in computer science, including logic programming, and consists of ﬁnding a substitution that, when applied to both terms, makes them equal modulo some equational properties. Several algorithms have been developed in the literature for speciﬁc equational theories, such as associative-commutative symbols, exclusive-or, Difﬁe-Hellman, or Abelian Groups (see [3]). Narrowing was proved to be complete for uniﬁcation [23] and several cases have been studied where narrowing provides a decidable uniﬁcation algorithm [1, 2]. A narrowingbased equational uniﬁcation algorithm relying on the concept of the variants of a term [11] has been developed in [22] and it is available in the most recent version of Maude, version 3.0, which provides quite sophisticated uniﬁcation features [9, 13]. Several tools and techniques rely on Maude’s advanced uniﬁcation capabilities, such as termination [14] and local conﬂuence and coherence [15, 16] proofs, narrowing-based theorem proving [35] or testing [34], and logical model checking [20, 4]. The area of cryptographic protocol analysis has also beneﬁted from advanced uniﬁcation algorithms: Maude-NPA [19], Tamarin [12] and AKISS [5] rely on the different uniﬁcation features of Maude. Furthermore, numerous decision procedures for formula satisﬁability modulo equational theories also rely on uniﬁcation, either based on narrowing [37] or by using variant generation in ﬁnite variant theories [32]. Constructor symbols are extensively used in computer science: for representing data instead of functions, for manipulating programs as data, or for reasoning in complex semantic structures. In an equational theory, constructors can be characterized in the “no junk, no confusion” style of Goguen and ∗This work has been partially supported by the EU (FEDER) and the Spanish MCIU under grant RTI2018-094403-B-C32, by the Spanish Generalitat Valenciana under grants PROMETEO/2019/098 and APOSTD/2019/127, and by the US Air Force Ofﬁce of Scientiﬁc Research under award number FA9550-17-1-0286.
Contrast stability and "stripe" formation in Scanning Tunnelling Microscopy imaging of highly oriented pyrolytic graphite: The role of STM-tip orientations<|sep|>More than thirty years after the invention of the scanning tunnelling microscope, STM is still one of the most useful tools for obtaining atomic resolution in surface imaging. However, in spite of such a long and successful history, the interpretation of STM experiments still raises some, to date unanswered, questions. The explanation of experimental results relies on long-established electron tunnelling models, which, however, invariably present some level of approximations. The ﬁrst tunnelling model presented by Bardeen [1] was based on ﬁrst-order perturbation theory. Tersoﬀ and Hamann [2, 3] derived a simpliﬁed model, where the tip is modelled by a spherically symmetric wave-function, and the electronic structure of the tip is neglected. Despite its simplicity, the method has successfully been used for the simulation of STM, and it is still the most commonly used model. However, as was pointed out by Chen [4, 5], the symmetry of the tip can have a huge eﬀect on the STM image since the tunnelling matrix elements are proportional to the derivatives of the sample wave-function depending on the tip orbital symmetries. Tip orbitals with non-zero orbital momentum (e.g. pz, d3z2−r2) can lead to an enhancement of the corrugation [4, 6]. Later, the roles of the tip orbital symmetry and electronic structure were emphasized in the STM imaging in several other studies [7, 8]. Recently, Palot´as et al. developed an orbital-dependent tunnelling model and demonstrated the eﬀect of the tip orbitals on the bias-voltageand tip-sample distance dependence of the atomic contrast inversion on the W(110) surface [9] and on the Fe(110) surface [10]. Extending this model to include arbitrary tip orientations, they found that diﬀerent tip orientations can considerably distort the STM image [11]. Accordingly, it was suggested that a sound interpretation of experimental STM images cannot, in principle, be obtained without explicitly accounting for tiporientation eﬀects. Recent interest in diﬀerent carbon allotropes (fullerenes, nanotubes, graphene, graphite) and nanostructures [12], and their potential for a wide spectrum of technological applications [13, 14, 15], for example biological and chemical sensors [16, 17], nano- and molecular electronics [18, 19], photovoltaics [20] and catalysis [21, 22], make atomically resolved investigation of carbon substrates − such as highly oriented pyrolytic graphite (HOPG) − of great relevance across many diﬀerent scientiﬁc ﬁelds. HOPG(0001) is one of the most frequently probed surface, where the tip orbital symmetries play a crucial role. The tip-dependent corrugation was discussed by Tersoﬀ and Lang, and the role of the orbital composition of the tip atom was highlighted [23]. The two nonequivalent carbon atomic sites of HOPG (α and β) are responsible for diﬀerent patterns in STM images. Depending on the applied bias voltage and tunnelling current both triangular and hexagonal honeycomb patterns can be observed. The selective imaging of the α and β atoms results in a triangular pattern [24, 25], which is mostly observed under typical tunnelling conditions, although a honeycomb pattern can be recorded as well [26, 27]. Chaika et al. showed that using a [001]-oriented tungsten tip allows for the control of the tip orbitals responsible for the imaging, hence diﬀerent patterns in the STM image can be obtained [28, 29]. Ondr´aˇcek et al. showed that multiple scattering eﬀects can also play an important role in the near contact regime, and they can result in a triangular pattern in the STM image with hollow sites appearing as bright spots, instead of the carbon atoms [30]. Teobaldi et al. rationalised the bias dependent STM contrast mechanisms observed on the HOPG(0001) surface by Figure 1. Schematic view of the STM tip above the HOPG surface. The rotation of the tip local coordinate system with respect to that of the sample surface is described by the Euler angles (ϑ0, ϕ0, ψ0). The shaded rectangle shows the considered scanning area of the HOPG surface for STM simulations in Figs. 5, 6, and 7. The positions of the characteristic h, α, and β sites of the HOPG(0001) surface are indicated in the inset. modelling a set of tungsten tips taking the eﬀects of tip electronic structure, termination, composition, and sharpness into account [31]. It is clear that the tip geometry and electronic structure cannot be neglected in an accurate STM simulation method. If the symmetry of the tip orbitals has a considerable eﬀect on the STM image, it follows naturally that so does the tip orientation. All simulation methods require a well-deﬁned tip geometry and orientation. Usually a simple geometry is chosen, e.g., a pyramid-shaped tip apex, but the local tip geometry at the apex and the relative orientation of the sample surface and the tip apex are unknown and hardly controllable in experiments. Moreover, these tip apex characteristics can even change during the experimental STM scan, see e.g. Refs. [32, 33] for magnetic surfaces. In separate electronic structure calculations of the sample surface and the tip their local coordinate systems are usually set up in such a way that they represent the corresponding crystallographic symmetries. The electronic structure data, either the single electron wave-functions or the density of states (DOS), are deﬁned in the given local coordinate systems, and they are used in the STM simulations. Thus, the relative orientation of the tip and the sample is ﬁxed, and it usually corresponds to a very symmetrical setup, which is unlikely in experiments. Hagelaar et al. studied a wide range of tip geometries and spatial orientations in the imaging of the NO adsorption on Rh(111) in combination with STM experiments [34], and their analysis is quite unique among the published STM simulations. In the present work we employ the three-dimensional (3D) Wentzel-Kramers Brillouin (WKB) electron tunnelling theory implemented in the 3D-WKB-STM code [9, 10, 11, 35, 36, 37, 38] to study the STM contrast characteristics of the HOPG(0001) surface as a function of the local orientation of a set of tungsten tips. In the tunnelling model the tip orientation, deﬁned by the local coordinate system of a crystallographically well-deﬁned tip surface with (hkl) Miller indices, can be rotated by the Euler angles (ϑ0, ϕ0, ψ0) in an arbitrary fashion [11]. A schematic view of an STM tip with rotated local coordinate system above the HOPG(0001) surface is shown in Fig. 1. The systematic eﬀect of local tip rotations is practically unexplored in experiments. The reason is that there is no direct in-situ information about the local rearrangements of the tip apex structure, e.g., manifesting as local tip rotations during scanning in an STM equipment, and the atomically precise stability of the tip apex structure is almost impossible to control. As we will demonstrate theoretically, local tip rotations can have an important eﬀect on the STM contrast. Initially, we compare the 3D-WKB and Bardeen tunnelling models with each other and with experimental results using bias dependent topography brightness correlations. We ﬁnd quantitatively good agreement for particular tips and bias voltage ranges, and discuss the identiﬁed diﬀerences. Based on the comparison with experimental data we conclude that the two tunnelling methods perform at the same quantitative reliability at both positive and negative bias voltages. The paper is organised as follows: After a brief description of computational details in Sec. 2, we deﬁne the topography brightness, and compare the 3D-WKB method with Bardeen’s approach in terms of correlations between the calculated relative brightnesses above the HOPG surface in section 3.1. Comparison with available experimental data [31] is reported in section 3.2. The simulated eﬀect of the local tip orientation on the STM image contrast is presented in section 3.3, followed by our conclusions in section 4. The 3D-WKB tunnelling theory with an arbitrary tip orientation is brieﬂy presented in Appendix.
Determining topological order from a local ground state correlation function<|sep|>The theoretical proposal and experimental discovery of topological band insulators [1] (TI) has been raising increasing interest in the condensed matter physics community. These materials form a novel topological state of matter, which does not fall into the standard classiﬁcation of broken symmetries. Instead, what distinguishes this phase from a trivial band insulator (BI) is the existence of a nontrivial Z2 topological index associated with the full band structure. This distinction is somewhat analogous to the integer quantum Hall eﬀect (IQHE), whose distinguished states can be attributed to a Z topological index associated with full Landau levels of noninteracting electrons. Broken symmetry phases are characterized by local order parameters, in what is known as the Landau paradigm [2]. It is commonly accepted that this paradigm does not apply to the above topological phases. Instead, such phases are described by the more elusive quantity, known as a topological order [3]. In IQHE the quantized Hall conductance is a direct manifestation of the topological order, and for the case of free electrons, this response function is a local bulk quantity [4]. As far as we know, for TI the implication of the topological order is only through edge eﬀects (see, for example, Refs. [5–7]) or defect-related eﬀects [8]. There is no local response function that is known to characterizes this Z2 phase, let alone an order parameter. In this work we show that both the Z2 index of the TI and the Z index of the IQHE, as well as any gapped insulator with non-zero Chern number ν, can be extracted from a local equal time ground-state correlation function. This implies that in these systems the topological order is in fact a local ground state property. It also proves a bulk-edge correspondence theorem for such local topological orders. When interactions are taken into account, the Z index remains well deﬁned [9], while it is yet unclear whether the Z2 index does [10, 11]. Our formulation, however, remains well deﬁned at least for weak interactions, and thus suggests an extension to the deﬁnition of the TI to weakly interacting systems. Beyond weak interactions, we show that either the energy gap or some “occupation gap” must be closed during the transition from a TI to a BI. The theoretical procedure that we provide can be straightforwardly adapted to form an algorithm for numerically determining the Z2 and Z indices. This algorithm is rather eﬃcient, since one only needs to diagonalize several matrices with dimensions of the order of the correlation length squared (2D) or cubed (3D). Such an algorithm may help in numerically testing a predicted topological order.
Novel Collider and Dark Matter Phenomenology of a Top-philic Z'<|sep|>Augmenting the Standard Model (SM) at the TeV scale with an additional gauged abelian group (U(1)′) provides an economical handle to address various shortcomings of the SM. Various aspects of the phenomenology of the U(1)′ and the associated gauge boson, Z′, have been discussed in the literature; see for example [1] and references therein for a review. These unbroken Abelian groups naturally arise in the low scale phenomenology of Grand Uniﬁcation Models with a GUT group of rank ﬁve or higher. They also arise from higher dimensional constructions where symmetry is broken by orbifolding. In this paper we adopt a bottom up approach within this class of models without specifying any particular UV completion. We consider the possibility that the SM has an additional exotic abelian group with its own Higgs mechanism resulting in a massive Z′ gauge boson. We will assume that only the right-handed top quark is charged under this exotic U(1)′ making the Z′ top-philic1. The Z′ can have kinetic mixing with the hypercharge gauge boson of the SM providing an additional portal for states in the SM and exotic sectors to communicate with each other. Such a top-philic Z′ has been discussed in the literature in the context of dark matter (DM) [5] and galactic γ-ray lines [6; 7] as well as in the context of vacuum stability [8], and can be motivated from models of composite/RS dark matter [6]. Such a model also requires additional chiral fermions for anomaly cancellation. We will assume that additional fermions in the exotic sector take care of this but are heavy enough to be decoupled from our eﬀective theory at low energies. Our goal in this paper is to study the possibility that a minimal extension of the SM which includes a top-philic Z′ is able to address some recent signals that are hard to explain within the SM. We will try to reconcile some observed experimental tension in the LHC data with the apparently uncorrelated issue of dark matter and the galactic γ-ray excess using such a construction. The ATLAS and CMS collaborations have recently released their combined Higgs measurements using the complete Run I dataset from the LHC [9; 10]. While overall the results show good agreement with Standard Model expectations, they observe a slight excess in the case of Higgs production in association with a pair of top quarks. This process is of particular importance since it allows, in principle, for a direct measurement of the top quark Yukawa coupling, which otherwise only enters via loops in the Higgs production and decays into massless gauge bosons. Though the SM cross-section for this process is small (σSM ∼ 130 fb at √s = 8 TeV) and thus measuring its rate challenging at the LHC Run I, interestingly the combined measurement by ATLAS and CMS gives a t¯th signal strength relative to the SM cross-section of µ = σ/σSM = 2.3+0.7 −0.6 for a 125 GeV Higgs boson. This deviation from the SM value (µ = 1) is driven by the multi-lepton channels and in particular the CMS samesign dilepton channel, which gives a best ﬁt value of µ = 5.3+2.1 −1.8. Overall, compared to the SM expectation, the observed excess is equivalent to a 2.3-standard-deviation (σ) upward ﬂuctuation. We will show that an additional contribution arising from the production of the Z′ in association with top quarks (t¯tZ′) could provide an explanation for the observed 1For models where the Z′ preferentially couples to the bottom quark see [2]; while for models where the Z′ couples to all right-handed quarks, see [3; 4]. excess. (For previous theoretical studies of this excess, see [11–15].) We stress that a more precise measurement of this Higgs production mechanism will be one of the key objectives at the LHC Run II, making our study relevant for the near future. On the other hand an intriguing excess of γ-rays from the galactic center has been observed in the Fermi-LAT data [16; 17]. The observed excess is contingent on the possible large uncertainties in the astrophysical γ-ray background [18] and may in the end have an astrophysical origin. However, it can be well ﬁtted by the continuum photon spectrum provided by DM annihilating in the center of the Milky Way, within the usual thermal weakly interacting massive particle paradigm and for the standard DM relic density proﬁles. The excess is observed in the energy range of 10 MeV to 300 GeV with a peak around 3 GeV. In the case of our Z′ extension to the SM, the Higgs mechanism in the exotic sector that breaks the U(1)′ may leave an unbroken Z2 under which all the SM states are even. Thus the lightest Z2 odd state in this exotic sector can become a viable dark matter candidate. The DM can then annihilate to SM states via the Z′, potentially leading to an observable γ-ray signal. To summarize, we consider a top-philic Z′ in the mass range 150-450 GeV and determine the phenomenologically relevant production mechanisms and decay widths. We translate the constraints from electroweak precision tests and direct searches at colliders and identify the region of parameter space that results in an improved ﬁt of the t¯th measurements. Furthermore we show that if the exotic sector contains a stabilized Dirac fermion, it can provide a simultaneous solution to the DM relic density constraints and the galactic center excess that is also compatible with the ﬁt to the t¯th data. We also present the reach of the LHC run at √s = 13 TeV to explore the relevant regions of parameter space. This paper is organized as follows: in Sections 2 and 3 we introduce the eﬀective theory for the Z′ and brieﬂy discuss the main phenomenological aspects. Then in Section 4 we discuss the improved ﬁt to the measured t¯th signal strength and present the current and projected constraints from collider searches. Finally in Section 5 we discuss the dark matter and galactic center γ-ray excess within this framework before concluding in Section 6.
Quantitative probing: Validating causal models using quantitative domain knowledge<|sep|>The topic of causal inference has been a focus of extensive research in statistics, economics and artiﬁcial intelligence. [1, 2, 3, 4, 5, 6] In contrast to more traditional statistics, methods of causal inference allow predicting the behavior of a system not only in situations where we passively observe certain evidence, but also in situations where we actively intervene in the data generating process. Prior to the rise of causal inference, ruling out confounding in such predictions required performing costly and possibly harmful randomized controlled trials [7]. Causal inference, on the contrary, provides the methodology to infer the effect of hypothetical actions from passively collected observational data and additional assumptions, which can be encoded in graphs [1] or conditional independence statements [3]. The beneﬁts of evaluating a wide range of possible actions without actually having to perform them are obvious in ﬁelds including medical treatment, industrial manufacturing or governmental policy design. However, in contrast to correlation-based prediction techniques, such as linear regression models, support vector machines or neural networks, which can all be validated using the well-known train/test split [8], the challenge of validating the causal models responsible for the predictions of interest is still unsolved. Without having validated the causal model, the promising perspective of predicting the behavior of a system under interventions from purely observational data is compromised by our uncertainty about the validity of the underlying model. This gap in the methodology needs to be ﬁlled, before decision makers in the above-mentioned application domains can leverage the powerful methods of causal inference, in order to complement, enhance or even replace the costly current method of randomized controlled trials. In this paper, we present quantitative probing, a novel approach for validating causal models, which applies the logic of scientiﬁc discovery [9] to the task of causal inference. Both, the code for integrating quantitative probing into causal analysis and the code for simulation-based studies of the effectiveness of quantitative probing are provided in two separate open-source Python packages [10, 11]. The article is structured as follows: Chapter 2 reviews the train/test split as the classical method for validating correlation-based statistical models, and explains why it is not possible to directly transfer the method to the ﬁeld of causal models. Chapter 3 presents different approaches to the validation of causal models and discusses the limitations of the currently available validation techniques. Chapter 4 brieﬂy clariﬁes the notion of a causal model and introduces causal end-to-end analysis as a model that generalizes other types of causal models. Chapter 5 synthesizes the idea of quantitative probing from the observations in the previous chapters, and relates the approach to the established method of scientiﬁc discovery. The concept is illustrated using Pearl’s well-known Sprinkler example [1] and assumptions are explicitly stated, in order to deﬁne the current scope of the quantitative probing approach. Chapter 6 provides simulation-based evidence for the effectiveness of quantitative probing as a validation technique. Special cases where the method fails to detect misspeciﬁed models are investigated in detail, in order to identify limitations and future enhancements. Chapter 7 concludes the article by summarizing the main points and proposes concrete questions for future research.
Learning to Selectively Transfer: Reinforced Transfer Learning for Deep Text Matching<|sep|>Text matching is an important problem in both information retrieval and natural language processing. Typical examples of text matching include paraphrase identification [26], natural language inference [3], document retrieval [10], question answering (QA) [37], and conversational response ranking [38]. In particular, text matching plays a key role in conversational assistant systems to answer customer questions automatically. For example, the Contact Center AI1 recently launched by Google and the AliMe [15] built by Alibaba Group are both capable of handling informational requests by retrieving potential answers from a knowledge base. We illustrate the importance of text matching by describing the role it plays in a retrieval-based QA system. Typically, for a given user query, the system measures its similarity with the questions in the knowledge base and returns the answer of the best matched question [36, 43]. The query-question matching problem can be modeled as a paraphrase identification (PI) or natural language inference (NLI) task, which are both typical tasks of text matching. Thus in this work, we focus on PI and NLI tasks to evaluate the performance of our method on text matching. We believe the improvement of text matching methods can benefit the end tasks such as question answering. PI and NLI problems have been widely studied in previous work [3, 26, 40, 41]. However, when applied to real world applications, such methods face the challenge of insufficient labeled data in different domains. For example, in the E-commerce industry, a QA system has to handle each small domain of products such as books, electronics, clothes, etc. It is unrealistic to obtain a large amount of labeled training data for every small domain. As a promising approach to bridge the domain discrepancy, Transfer Learning (TL) has become an important research direction in the past several years [17, 27, 29, 42, 43]. Due to the domain shift between the source and target domains, directly applying TL approaches may result in “negative transfer" problem. To prevent this problem, we argue that source domain data selection is necessary for the TL approaches. Table 1 gives an example of negative transfer in the PI task. “Order" typically means to place an order for a product in the E-commerce domain (target domain). However, in an open domain (source domain) dataset, “order" can be used to denote a succession or sequence. Hence in such case, TL without source domain data selection might result in negative transfer. Recently, neural architectures are employed to leverage a large amount of source domain data and a small amount of target domain data in a multi-task learning manner [20, 39], which can be described as Deep Neural Networks (DNN) based supervised transfer learning. The DNN based TL framework has been proven to be effective in deep text matching tasks for question answering systems [43]. Although various data selection methods [4, 11, 23, 27] were proposed for TL settings, most of them do not fit well with neural transfer models, because the data selector/reweighter is not jointly trained with the TL model. Specifically, the TL task model is considered as a sub-module of the data selection framework. Thus the TL task model needs to be retrained repetitively to provide sufficient updates to the data selection framework. Due to the relatively long training time of neural models, such data selection methods may suffer from long training time when applied to neural TL models. Therefore, we argue that data selection methods for transfer learning need to be revisited under the DNN based TL setting. In the setting of DNN based transfer learning, the TL model is updated with mini-batch gradient descent in an iterative manner. In order to learn a universal data selection policy in this setting, we model the problem of source domain data selection as a Markov Decision Process (MDP) [25]. Specifically, at each time step (minibatch/iteration), the TL model is at a certain state s, the decision maker (data selector) chooses an action a to select samples from the current source batch to optimize the TL model. The TL model gives the data selector a reward r and moves on to the next state s′. The state of s′ depends on the current state s and the action a made by the data selector. To solve this problem, it is intuitive to employ reinforcement learning, where the decision maker is the data selection policy that needs to be learned. In this paper, we propose a novel reinforced data selector to select high-quality source data to help the TL model. Specifically, we build our data selector based on the actor-critic framework and integrate it to a DNN based TL model, resulting in a Reinforced Transfer Learning (RTL) method. To improve the model training efficiency, the instance based decisions are made in a batch. Rewards are also generated on a batch level. Extensive experiments on PI and NLI tasks show that our RTL method significantly outperforms existing methods. Finally, we use Wasserstein distance to measure the target and source domain distances before and after the data selection. We find our method is able to select source data whose Wasserstein distance is close to the target domain data. This is reasonable and intuitive as such source domain data can provide more transferability power to the model. Our contributions can be summarized as follows. (1) To the best of our knowledge, we propose the first reinforcement learning based data selector to select high-quality source data to help the DNN based TL model. (2) In contrast to conducting data selection instance by instance, we propose a batch based strategy to sample the actions in order to improve the model training efficiency. (3) We perform thorough experimental evaluation on PI and NLI tasks that involves four benchmark datasets. We find that the proposed reinforced data selector can effectively improve the performance of the TL model and outperform several existing baseline methods. We also use Wasserstein distance to interpret the model performance.
Dynamics of a Reaction-Diffusion Benthic-Drift Model with Strong Allee Effect Growth<|sep|>Streams and rivers are characterized by a variety of physical, chemical and geomorphological features such as unidirectional ﬂow, pools and riﬄes, bends and waterfalls, ﬂoodplains, lateral inﬂow and network structure and many more. These complex structures provide a wide range of qualitatively diﬀerent habitat for aquatic species and organisms such as zooplankton, invertebrates, aquatic plant and ﬁsh. In [29], M¨uller proposed an important issue in stream ecology, the “drift paradox”, which asks how stream dwelling organisms can persist in a river/stream environment when continuously subjected to a unidirectional water ﬂow. Mathematical models, such as reaction-diﬀusion-advection equations and integro-diﬀerential equations have been established to study the population dynamic in advective environment. For species following logistic type growth, a “critical ﬂow speed” has been identiﬁed, below which can ensure the persistence of the stream population [14, 18, 19, 20, 23, 24, 28, 38]. On the other hand, when the species following Allee eﬀect type growth, population persistence for all initial conditions becomes not possible as the extinction state is always a stable state, and more delicate conditions are needed to ensure the population persistence [36, 44, 45]. The solution of stream population persistence/extinction not only leads to a better understanding of population dynamics in a stream environment, but also provides strategies for how to keep a native species persistent. Stream hydraulic characteristics is another important factor in the ecology of stream populations. Of great importance is the presence of storage zones (zones of zero or near-zero ﬂow) in stream channels. These zones are refuges for many organisms not adapted to high water velocity. And for some aquatic species, the individuals spend a proportion of their time immobile and a proportion of their time in an environment with a unidirectional current and do not reproduce there. Following [2, 5], the river can be partitioned into two zones, drift zone and benthic zone, and the population is also split into two interacting compartments: individuals residing in the benthic zone and the ones dispersing in the drift zone. Assuming that longitudinal movement occurs only in the drift zone, a system of coupled reactiondiﬀusion-advection equation of drift population and equation of benthic population can be used to model the dynamic evolution of aquatic species that reproduce on the bottom of the river and release their larval stages into the water column, such as sedentary water plant, oyster and coral [23, 30]. Assuming logistic growth for the benthic population, the population spreading, invasion and the propagation speed were studied in [23, 30]; the population persistence criteria on a ﬁnite length river based on the net reproductive rate was investigated in [12]; and the population dynamics of two competitive species in the river was studied in [17]. All these work assume logistic growth for the benthic population so the population persistence/extinction or spreading can be completely determined by a sharp threshold which is often expressed by a basic reproduction number or a critical advection rate. Benthic-drift models of algae and nutrient population have also been considered [7, 10, 11, 41]. Other studies also consider the eﬀect of river network structure [16, 33, 34, 39], the eﬀect of advection on competition [21, 47, 48, 49, 50] and meandering structure [15]. In this paper, we investigate how interactions between the benthic zone and the drift zone aﬀect the population dynamics of a benthic-drift model, when the species follows a strong Allee eﬀect population growth in the benthic zone. Our main ﬁndings on the dynamics of benthic-drift model with strong Allee eﬀect type growth in the benthic population are 1. If the benthic population release rate is large, then for all the boundary conditions, extinction will always occur regardless of the initial conditions, the diﬀusive and advective movement and the transfer rate from the drift zone to the benthic zone; 2. If the benthic population release rate is small (but not zero), then for all the boundary conditions, the population persists for large initial conditions and becomes extinct for small initial conditions. Such a bistability in the system exists also independent of the diﬀusive and advective movement and the transfer rate from the drift zone to the benthic zone; 3. If the benthic population release rate is in the intermediate range, the persistence or extinction depends on the diﬀusive and advective movement. It is shown that for the closed environment, the population can persist under small advection rate and large initial condition. These results are rigorously proved by using the theory of dynamical systems, partial diﬀerential equations, upper-lower solution methods, and numerical simulations are also included to verify or demonstrate theoretical results. Compared with the single compartment reaction-diﬀusion-advection equation with a strong Allee eﬀect growth rate [45], in which the advection rate q plays an important role in the persistence/extinction dynamics, the benthic-drift model dynamics with strong Allee eﬀect relies more critically on the strength of interacting between zones. The dynamic behavior of the single compartment reaction-diﬀusion-advection equation modeling a stream population with a strong Allee eﬀect growth rate was investigated in [45]. Compared to the well-studied logistic growth rate, the extinction state in the strong Allee eﬀect case is always locally stable. It is shown that when both the diﬀusion coeﬃcient and the advection rate are small, there exist multiple positive steady state solutions hence the dynamics is bistable so that diﬀerent initial conditions lead to diﬀerent asymptotic behavior. On the other hand, when the advection rate is large, the population becomes extinct regardless of initial condition under most boundary conditions. Corresponding dynamic behavior for weak Allee eﬀect growth rate has been considered in [44]; and the role of protection zone on species persistence or spreading for species with strong Allee eﬀect growth has [4, 6]. The benthic-drift model has the feature of a coupled partial diﬀerential equation (PDE) for the drift population and an “ordinary diﬀerential equation” (ODE) for the benthic population. Note that the benthic population equation is not really one ODE but an ODE at each point of the spatial domain, or a reaction-diﬀusion equation with zero diﬀusion coeﬃcient. Such degeneracy causes a noncompactness of the solution orbits in the function space, which brings an extra diﬃculty in analyzing the dynamics. Such PDE-ODE coupled systems have been also studied in the case of population that has a quiescent phase [46], or some species are immobile [26, 42]. In Section 2, the benthic-drift model of stream population is established, and all model parameters and growth rate conditions are set up in a general setting. Some preliminary results are stated and proved in Section 3: the basic dynamics, global attractor, and linear stability problem. The main results on the population persistence and extinction are proved in Section 4, and some numerical simulations are shown in Section 5 to provide some more quantitative information of the dynamics. A few concluding remarks are in Section 6.
Stream-orbit misalignment II: A new algorithm to constrain the Galactic potential<|sep|>The study of the Galactic halo is crucial to the understanding of the dark-matter content of the Galaxy. The dynamics of halo stars reﬂect the shape of the Galactic potential on large scales and hence the underlying dark-matter distribution. Recent large photometric surveys have revealed many stream-like structures in the halo (Belokurov et al. 2006), which should prove useful in this respect. These tidal streams are believed to be formed by stars being stripped from their progenitor by the tidal forces of the Galactic potential. The formation of the stream reﬂects the underlying Galactic potential, such that the resulting structure may be used to constrain the potential. One way of using streams to constrain the Galactic potential is to assume that a stream delineates an orbit. This assumption has been used by many authors in the development of streamﬁtting algorithms (Jin & Lynden-Bell 2007; Binney 2008; Eyre & Binney 2009a,b) and their application to real data (Willett et al. 2009; Koposov et al. 2010). However, not until recently was the assumption that a stream delineates an orbit properly questioned by Eyre & Binney (2011). These authors demonstrated that a misalignment between the progenitor orbit and stream can lead to signiﬁcant errors in the parameters of simple potentials. Sanders & Binney (xxxx, hereafter Paper I) expanded on this by discussing the misalignment in realistic Galactic potentials for real streams, and demonstrated that orbit-ﬁtting is not valid for many known streams. In particular order-one errors in the halo parameters of a realistic Galactic potential can arise if orbit-ﬁtting is applied to known Galactic streams. Clearly, an improvement over orbit-ﬁtting is required, which accounts for the stream-orbit misalignment. Johnston et al. (1999) accounted for the misalignment between the stream and progenitor orbit by calculating a progenitor-massdependent energy offset at pericentre. The observed stream stars are assigned an energy in this range, and the stream is integrated backwards for a Galactic lifetime. The quality of the trial potential is assessed by the number of ‘captured’ particles at time t = 0. Varghese et al. (2011) developed a similar method which took into account the misalignment between the stream and progenitor orbit by correcting a proposed orbit track in real-space with a progenitormass dependent term. To fully model stream formation with limited
First-order Optimization for Superquantile-based Supervised Learning<|sep|>Classical supervised learning assumes that, at training time, we have access to examples (x1, y1), . . . , (xn, yn) drawn i.i.d. from a distribution P, and that at testing time, we may face a new example, also drawn from P. The learned predictor or function can be used by humans or machines to make decisions, or used in as an intermediate component in a greater data processing and computing system. This common framework is currently challenged by important domain applications [1], in which several of the standard assumptions turn out to be unrealistic or simply incorrect. We may not face the same distribution at test time as we did at training time (train-test distribution shift). Recent failures of learning systems when operating in unknown environments [2, 3] underscore the importance of reconsidering the learning objective used to train learning machines in order to ensure robust behavior in the face of unexpected distributions at prediction time. The generalized regression framework presented in [4] provides an attractive ground to design learning machines dis playing increased robustness in the face of unexpected testing distributions. The framework hinges upon the notion of superquantile, a statistical summary of a distribution tail [5, 6, 7]. This notion of robustness is aligned with the one in distributionally robust optimization [8] and empirical likelihood estimation [9]. It is, however, different, from notions of robustness commonly considered in robust statistics [8, Sec. 12.6]. The superquantile is a risk measure, a family of statistical summaries of distribution tails, well studied in economics and ﬁnance [10, 11]. The quantity is, however, a nonsmooth function. We present here a simple approach, based on inﬁmal convolution smoothing, which allows one to easily adapt state-of-the-art gradient-based optimization algorithms for classical supervised learning to the superquantilebased learning framework. Moreover, we provide a companion software package in Python available here https: //github.com/yassine-laguel/spqr . Risk measures play a crucial role in optimization under uncertainty, involving problems with an aversion to worst-cases scenarios. Among popular convex risk measures, superquantile (also called Conditional Value at Risk) has received a special attention because of its nice convexity properties; see e.g. the textbook [12, Chap. 6]. We use here the notation and terminology of Rockafellar and Royset [13]. The p-quantile Qp(U) of a random variable U is deﬁned as the general inverse of the cumulative distribution of U. More precisely, for a random variable U (admitting a second order moment), the cumulative distribution function FU : R → [0, 1] is deﬁned as FU(x) = P(U ≤ x). For any p ∈ [0, 1], the p-quantile Qp(U) and the p-superquantile ¯Qp(U), are respectively deﬁned by The superquantile is, therefore, a measure of the upper tail. The parameter p allows one to control the sensitivity to risk. Interestingly, the dual formulation uncovers another interpretation of the superquantile learning objective relating it to the re-weighting of the terms in the empirical risk. In practice, the ambiguity on the data distribution may be formalized before training, for instance by incorporating side information (geographical and/or temporal for instance) that drives the heterogeneity of the data. Superquantile learning is expected to produce models that perform better in case of distributional shifts between the training time and the testing time, compared to models trained using standard empirical risk minimization. We are interested in a supervised machine learning setting with training data D = (xi, yi)1≤i≤n ∈ (Rp × Rq)n, a prediction function ϕ : Rd × Rp → Rq (such as a linear model or a neural network) and a loss function ℓ : Rq × Rq → R (such as the logistic loss or the least-squares loss). The classical empirical risk minimization writes A natural approach consists then in replacing the expectation in (3) by the superquantile (1) in the case of discrete distributions standing for the training data This representation is central to the implementation as we shall see in Sec. 3. Existing works on minimizing superquantiles considered linear programming or convex programming including interior point algorithms; see [15]. Our approach considers ﬁrst-order algorithms instead; although natural, this work seems to be the ﬁrst one to do so.
Quiescent X-Ray/Optical Counterparts of the Black Hole Transient H 1705-250<|sep|>Soft X-ray transients (SXTs), or X-ray novae, are a sub-class of low mass X-ray binaries which contain a neutron star or a black hole primary accreting matter from the companion donor star via Roche lobe overﬂow. SXTs spend most of their lifetime in the quiescent state, and occasionally undergo dramatic outbursts which could last from weeks to months, and in some special cases, the outburst can go on for years (e.g. GRS 1915+105). The typical maximum outburst X-ray luminosity of such systems ranges from 1036 to 1039 erg s−1, while the minimum luminosity can go as low as a few times of 1030 erg s−1. However, the true quiescent luminosities (deﬁned as the luminosities when no accretion onto the black hole occurs) are unclear due to the insuﬃcient sensitivity of current instruments. There are several theoretical attempts to explain the weak X-ray emission from X-ray binaries during their quiescent phase (see Narayan et al. 2001 for a review and references therein). The most widely used idea is perhaps through the advection dominated accretion ﬂow (ADAF) model (Narayan & Yi 1994, 1995, Narayan & McClintock 2008 and references therein). The thermal energy that is created by the mass transfer is stored in the ADAF ﬂowing towards the center of the compact object instead of being radiated away eﬃciently as it is at higher accretion rate (a thin disk is thought to be present in this case). Depending on the nature of the central compact objects, diﬀerent quiescent luminosities are expected. In the case of a neutron star with a solid surface, the energy stored in the ADAF may impact onto the neutron star surface and be reradiated away. In the case of a black hole, no solid surface is present but an event horizon. A large fraction of the energy carried by the gas in the ADAF is then transported beyond the event horizon when the gas falls into the black hole before this energy could be emitted. The consequence of this scenario is that we expect to observe the quiescent luminosity of a black hole to be much fainter than a quiescent neutron star. With the sensitivity of current X-ray observatories (i.e. Chandra, XMM-Newton), we are able to observe this discrepancy and this provides strong evidence of the existence of the black hole event horizon (see Narayan & McClintock 2008 for a review and references therein). In addition, there is also evidence showing that part of the energy could be dissipated as outﬂows moving away from the system resulting in low observed X-ray luminosities (Fender et al. 2003, Gallo et al. 2006). H 1705–250 (also Nova Ophiuchi 1977, V2107 Oph) was discovered independently by both the HEAO-1 scanning modulation collimator and the Ariel 5 all sky monitor in September 1977 (Griﬃths et al. 1978; Kaluziensku & Holt 1977), and subsequently found to be associated with a bright 16.5 mag optical nova from observations taken at the AngloAustralian Telescope and UK Schmidt Telescope(Longmore
DeepFaceEditing: Deep Face Generation and Editing with Disentangled Geometry and Appearance Control<|sep|>Generating realistic face images is an active research topic in image synthesis. Recent techniques such as StyleGAN [Karras et al. 2019, 2020] allow random generation of realistic face images. However, many of them lack direct control of facial geometry or appearance. Adding conditions to the generation process is necessary to generate specific face images of interest, since face images are results of multiple factors, including geometry, appearance, head pose, viewpoint, etc. Sketch-based conditions have been commonly used in existing image-to-image translation techniques [Isola et al. 2017; Wang et al. 2018], since sketching provides an easy and expressive means to depict desired geometry, including shape contours and salient surface edges. Multiple sketch-to-image techniques [Chen et al. 2020; Chen and Hays 2018; Li et al. 2020; Sangkloy et al. 2017] have been proposed to generate realistic face images from edge maps or even rough sketches. However, since it is difficult to infer the appearance information from sketches alone, the above techniques lack enough control of appearance during the generation process. On the other hand, there exist various techniques [Kim et al. 2020; Kolkin et al. 2019; Yoo et al. 2019] which allow the generation of realistic faces with their appearance as similar to given reference images as possible. However, such techniques focus on global appearance transfer and often do not well preserve geometric details of source images. Recent image disentangling works [Kim et al. 2020; Park et al. 2020] offer promising frameworks to have decoupled control of multiple attributes such as geometry and appearance. However, disentangling techniques for processing general image types do not provide optimal results for face generation. These methods treat images holistically and do not provide detailed, localized control, which is essential for face image editing. In this work, we present DeepFaceEditing, a novel face generation and editing framework that allows disentangled control of geometry and appearance (Fig. 1). The key enabler is a structured disentanglement framework specifically designed for face images. Observing that faces have a fixed structure of facial components, we adopt a local-to-global framework, consisting of two modules: Local Disentanglement (LD) and Global Fusion (GF). With paired images and sketches, the LD module disentangles facial components into two independent representations: geometry and appearance. The former encodes the geometry information including shapes of face and facial components as well as salient geometric details such as wrinkles. This can be specified explicitly as a sketch or implicitly as a geometry reference image. In contrast, the appearance representation encodes the information related to color, material, lighting conditions, etc. The GF module is trained to fuse local representations to obtain globally consistent face images. Unlike many other forms of disentanglement, our disentanglement makes the resulting representations easy to manipulate, and in particular, the geometry is represented using sketches, which can be edited intuitively for both overall shapes and details. To ensure reliable disentanglement, our geometry space is designed to be a shared space of both sketches and images. In this way, we can not only make use of the geometry information in a geometry reference image but also develop a sketch-based interface for intuitive geometry manipulation. Figure 1 shows representative editing examples achieved with our interface, which supports decoupled control of appearance and geometry. We will also show the applications of our technique to face generation and face image style transfer. We perform extensive qualitative and quantitative experiments, which show that disentangled control of geometry and appearance allows flexible face image generation and editing, and our approach outperforms state-of-the-art methods for various applications. The main contributions of our work are summarized as follows: • We present a structured disentanglement framework for face image representation and synthesis, which ensures geometry and appearance are well decoupled and can be manipulated separately. • To achieve this, our network architecture involves local disentanglement (LD) modules for individual facial components to make learning more efficient, and a global fusion (GF) module for effective fusion of feature maps generated by LD modules. The LD modules are designed to embed both images and sketches of local regions to a shared space to ensure appropriate disentanglement, and allow sketches to be used as an explicit representation for the geometry, which is essential for detailed editing. • We present a novel interactive system that supports real-time editing of portrait images, which allows detailed editing of the geometry through an intuitive sketch-based interface, as well as modifying the appearance by providing an appearance reference image.
Quantum Communication and Quantum Multivariate Polynomial Interpolation<|sep|>The following problem is normally considered in classical and quantum communication scenarios: Bob have to communicate some secret message to Eve. They proceed the following procedure. • Encoding message and sending: Eve prepares some text message, namely in state |ψ0⟩, encodes it into a state |ψ1⟩ and send it to Bob • Queries and Back Sending : Bob makes some queries and transforms the received message into the state |ψ2⟩ and sends it back to Eve. • Decoding and Identifying the Secret: Eve decodes it and identiﬁes the common secret share with Bob. In their paper [5] K. Nagata and T. Nakamura described this procedure of quantum communication by using a scheme with Hadamard inverse transform H⊗N, N queries Q: (xi, yi), i = 1,N −−−−−−−−−→ |ψ2⟩ H⊗N −−−→ |ψ3⟩ This scheme is applicable to the Bernstein-Vazirani algorithm for linear black boxes f for ﬁnding their coeﬃcients as the secret. In this paper we consider the Quatum Fourier and the inverse Quantum Fourier Trasnforms QFT, QFT −1 in place of the Hadamard and inverse Hadamard gates and we discover that it is applicable to nonlinear black box function f of multivariate interpolation of polynomials in higer degrees. Our scheme works for nonlinear qudits in the same way as in the linear case for qubits because for qubits, d = 2 we have for qubits (d=2) becomes the Hadamard gate. In the work [1] J. Chen, A. M. Childs and S.-H. Hung estimated the queries complexity of order kFq=pr = d n+d �n+d d � with probability 1 − O(1/q) and this complexity is optimal. Therefore we may use it to make distribution of secret scheme among k = kFq users. A novelty in the paper is that we use the multivariate polynomial interpolation in place of linear interpolation, qudits in place of qubits and apply to quantum key distribution for general interpolation as in the scheme of Shamir’s code or NSA codes. The paper is organized as follows. In the next section 2 we describe the quantum communication scheme, needed for our problem. In section 3 we describe the quantum multivariate polynomial interpolation and then use it to the problem of quantum secret sharing communication.
Initiation of the detonation in the gravitationally confined detonation model of Type Ia supernovae<|sep|>Type Ia supernovae (SNe Ia) are among the most luminous optical transients. They contribute signiﬁcantly to the non-primordial elements in the universe, especially iron (Timmes et al. 1995). A large fraction of SNe Ia exhibit a correlation between peak luminosity and rate of decline that can be used to make them standard candles, and therefore excellent distance indicators and probes of cosmology [see, e.g. the reviews by Leibundgut (2001) and Frieman et al. (2000)]. While SNe Ia are thought to be the result of the thermonuclear incineration of white dwarfs (WDs) (Hoyle & Fowler 1960), the explosion mechanism is not fully understood [see e.g., the reviews by Hillebrandt & Niemeyer (2000) and Podsiadlowski et al. (2008)]. Most astronomers favor a scenario in which the exploding star is a single, near Chandrasekharmass WD. Consequently, multi-dimensional full-star simulations of the explosion of near Chandrasekhar-mass CO white dwarfs have been performed by several research groups. These models posit the ignition of a turbulent deﬂagration ﬂame in the core of the WD (e.g. Garcia-Senz & Woosley 1995; H¨oﬂich & Stein 2002; Kuhlen et al. 2006), which eventually gives rise to a supersonic detonation wave. In the deﬂagration to detonation transition (DDT) model, the detonation is postulated to occur when the deﬂagration ﬂame reaches densities low enough that the Gibson length lG, deﬁned as the scale where the turbulent velocities equal the laminar ﬂame speed, becomes smaller than the ﬂame width δ (Khokhlov et al. 1997; Niemeyer & Woosley 1997). When this criterion is met, turbulence tears the ﬂame apart, producing distributed burning. Simulations of SNe Ia that explode via DDT have been extensively studied in multiple dimensions (e.g. Arnett & Livne 1994a,b; Livne 1999; Gamezo et al. 2004, 2005; Golombek & Niemeyer 2005; The pulsational delayed detonation model (PDD) posits a centrally ignited deﬂagration, which undergoes a deﬂagration to detonation transition after one or several pulsations (Ivanova et al. 1974; Khokhlov 1991). A variant of this model is the pulsational reverse detonation (PRD) model, in which the initial deﬂagration releases energy and expands the star but quenches before it becomes gravitationally unbound. The detonation in these models is postulated to form from an accretion shock during the re-collapse of the star after the deﬂagration ash has risen and fresh fuel occupies again the center of the WD (Dunina-Barkovskaya et al. 2001; Bravo & Garc´ıa-Senz 2005, 2006). In the gravitationally conﬁned detonation model (GCD), the deﬂagration ﬂame ignites at one or more points slightly oﬀset from the center of the WD (Plewa et al. 2004; Plewa 2007; R¨opke et al. 2007; Townsley et al. 2007; Jordan et al. 2008; Meakin et al. 2009). As the bubble grows and rises, the Rayleigh-Taylor instability causes rapid growth of the ﬂame surface resulting in a rate of nuclear burning far exceeding the rate that would occur due to laminar burning alone (e.g. Khokhlov 1995; Zhang et al. 2007). The bubble quickly accelerates, burning on the order of a few percent of the mass of the star by the time it breaks through the surface of the WD (e.g. Livne et al. 2005). Owing to the large surface gravity of the WD, the bulk of the ash in the bubble remains gravitationally bound. The cold ash ﬂows over the surface of the star and converge at the opposite point on the stellar surface from where breakout occurred. The converging ﬂow of cold ash pushes unburned fuel in the surface layers of the WD ahead of it, compressing it, and increasing its temperature and pressure enough that the carbon in it begins to burn. Inwardly and outwardly directed jets form at the stagnation point, driving the inwardly directed jet to high densities [ see the discussion in Meakin et al. (2009)]. The inwardly directed jet is unstable to shear instabilities and Kelvin-Helmholtz rolls are observed to form. Additionally, strong sound waves originating from the surface ﬂow traverse the jet. It is in this environment that the detonation in the GCD model is thought to occur. To determine whether initiation of a detonation occurs under these conditions is, however not straightforward, primarily because the length scales pertinent to the detonation initiation are typically much smaller than the highest resolution possible in 2D, and even more so in 3D, whole-star simulations. In this context, past simulations of the GCD scenario can be grouped into two categories: 1. Those that limit nuclear burning to the ﬂame model and don’t simulate the detonation phase, including initiation of the detonation (R¨opke et al. 2007; Townsley et al. 2007; Jordan et al. 2008). 2. Those in which the detonation emerges in situ through nuclear burning outside the ﬂame model (Plewa et al. 2004; Plewa 2007; Meakin et al. 2009). Simulations in the ﬁrst category only infer the successful initiation (or failure) of the detonation based on a comparison of the temperatures and densities reached in the full-star simulation with those for the “critical detonation conditions” derived from 1D reactive hydrodynamics simulations (e.g. Niemeyer & Woosley 1997) at higher resolution. This procedure is inherently uncertain, due to the sensitivity of the detonation conditions to the functional form of the unresolved and therefore unknown temperature proﬁle (Seitenzahl et al. 2009). The practice of using the temperatures and corresponding densities in a simulation without distributed burning (i.e. in which nuclear burning is not allowed to occur outside the ﬂame model) suﬀers from additional limitations. First, it ignores the feedback of nuclear burning on the hydrodynamics. The energy liberated during carbon burning in the jet alters the ﬂow and may produce a temperature structure more conducive for initiation of a detonation. Second, not keeping track of nuclear reactions takes away the ability to decide whether the temperatures and densities obtained in the purely hydrodynamical ﬂow in fact occur in detonatable fuel, or the fuel has already been depleted due to the nuclear burning that occurs during the process of compressional heating. It is our belief that direct initiation of a detonation is hard to achieve in a CO WD environment (Nomoto et al. 1976; Mazurek et al. 1977) and that spontaneous initiation via the gradient (SWACER) mechanism (Zel’dovich et al. 1970; Lee et al. 1978) is more likely (Blinnikov & Khoklov 1986; Blinnikov & Khokhlov 1987). GCD simulations in the second category allow for the detonation to form explicitly in situ, and have paid some attention to the details of the initiation mechanism. However, no GCD simulations to date have spatially resolved the temperature gradients that are key to initiation of the detonation. In section 2 we describe the setup for the simulations reported in this paper, and the method we use to resolve the temperature gradients in the region where the detonation is expected to occur and so shed more light onto the details of the detonation mechanism in GCD. In section 3 we present our results for a set of four simulations with diﬀerent maximum resolutions. Section 4 compares our results with earlier work and discusses the implications of our results. Section 5 summarizes our conclusions.
A Model of the IEEE 802.11 DCF in Presence of Non Ideal Transmission Channel and Capture Effects<|sep|>Wireless Local Area Networks (WLANs) using the IEEE802.11 series of standards have experienced an exponential growth in the recent past [1]-[13]. The Medium Access Control (MAC) layer of many wireless protocols resemble that of IEEE802.11. Hence, while we focus on this protocol, it is evident that the results easily extend to other protocols with similar MAC layer operation. The most relevant works to what is presented here are [3], [4]. In [3] the author provided an analysis of the saturation throughput of the basic 802.11 protocol assuming a two dimensional Markov model at the MAC layer, while in [4] the authors extended the underlying model in order to consider unsaturated trafﬁc conditions by introducing a new idle state, not present in the original Bianchi’s model, accounting for the case in which the station buffer is empty, after a successful completion of a packet transmission. In the modiﬁed model, however, a packet is discarded after m backoff stages, while in the Bianchi’s model, the station keeps iterating in the m-th backoff stage until the packet gets successfully transmitted. In [5], the authors look at the impact of channel induced errors and the received SNR on the achievable throughput in a system with rate adaptation whereby the transmission rate of the terminal is adapted based on either direct or indirect Accepted to IEEE Globecom 2007 This work has been supported by PRIN-2005 ICONA. F. Daneshgaran is with ECE Dept., California State University, Los Angeles, USA. M. Laddomada (laddomada@polito.it), F. Mesiti and M. Mondin are with DELEN, Politecnico di Torino, Italy. measurements of the link quality. In [6], the authors deal with the extension of Bianchi’s Markov model in order to account for channel errors. Paper [7] proposes an extension of the Bianchi’s model considering a new state for each backoff stage accounting for the absence of new packets to be transmitted, i.e., in unloaded trafﬁc conditions. In real networks, trafﬁc is mostly unsaturated, so it is important to derive a model accounting for practical network operations. In this paper [2], we extend the previous works on the subject by looking at all the three issues outlined before together, namely real channel conditions, unsaturated trafﬁc, and capture effects. Our assumptions are essentially similar to those of Bianchi [3] with the exception that we do assume the presence of both channel induced errors and capture effects due to the transmission over Rayleigh fading channel. The paper is organized as follows. Section II extends the Markov model initially proposed by Bianchi, presenting modiﬁcations that account for transmission errors and capture effects over Rayleigh fading channels employing the 2-way handshaking technique in unsaturated trafﬁc conditions. Section III provides an expression for the unsaturated throughput of the link. In section IV we present simulation results where typical MAC layer parameters for IEEE802.11b are used to obtain throughput values as a function of various system level parameters, capture probability, and SNR under typical trafﬁc conditions. Finally, Section V is devoted to conclusions.
Distribution of spectral linear statistics on random matrices beyond the large deviation function -- Wigner time delay in multichannel disordered wires<|sep|>The determination of linear statistics for eigenvalues of random matrices is an important question which has played a central role in the applications of random matrix theory to physical problems. For concreteness, let us introduce the Laguerre ensemble, which will play an important role in the paper : we consider N × N matrices with positive eigenvalues, distributed according to the measure [39, 27, 1] where DM is the Lebesgue (uniform) measure for Hermitian matrices and β is the Dyson index corresponding to orthogonal (β = 1), unitary (β = 2) or symplectic (β = 4) symmetry classes. The condition N(1 + θ) > 1 − 2/β ensures normalisability of the measure. If NL = N(1 + θ) − 1 + 2/β is an integer, Eq. (1) is the Wishart distribution for matrices of the form M = X†X, where X has size NL × N and has independent and identically distributed Gaussian entries. With some abuse, we will sometimes denote the distribution (1) for arbitrary θ the Wishart distribution. This distribution appears in many contexts. Few examples are : the distribution of the empirical covariance matrix in statistics [61], principal component analysis [36], ﬂuctuating interface models [46], random bipartite quantum states [47, 48], Wigner-Smith matrix (quantum scattering) in chaotic quantum dots [11, 12, 31, 54, 56] (θ = 1, i.e. NL ≃ 2N) and multichannel disordered semi-inﬁnite wires [7] (θ = 0, i.e. NL ≃ N). Distributions such as (1) describe a so-called “invariant ensemble” as the measure is invariant under unitary transformations. This leads to the decorrelation of the eigenvalues and the eigenvectors, hence the joint probability density function for the eigenvalues has the generic form P(λ1, · · · , λN) = CN,θ � k w(λk), where w(λ) = λβNθ/2 exp[−(β/2)λ] for the Laguerre ensemble (CN,θ is a normalisation). Many interesting physical quantities take the form of spectral linear statistics for a given function f(x) (see examples below). The analysis of the distribution of such quantity remains in general a diﬃcult task which has stimulated considerable eﬀorts. Several techniques have been developed : orthogonal polynomials, Selberg’s integrals and Coulomb gas method. Although all these techniques provide the typical ﬂuctuations of L, the Coulomb gas technique seems the most eﬃcient method to determine the large deviations (atypical ﬂuctuations). One interprets the distribution as the Gibbs measure for a one-dimensional gas of charges interacting with logarithmic interactions [22] : P(λ1, · · · , λN) = (1/ZN) exp{−βEgas}, where 1/ZN is a normalisation. In the limit N → ∞, the analysis of the distribution of the linear statistics is mapped onto the determination of the optimal conﬁguration of charges which minimizes the energy Egas under the constraint that � the gas scales as Egas ∼ N 2, one can in general write the large deviation ansatz ‡ PN(s = N −ηL) ∼ exp { − (βN 2/2) Φ(s)}, characterizing the distribution of (2) in the limit N → ∞, where the scaling exponent η depends on the matrix ensemble and the function f (e.g. : for the Laguerre ensemble and f(x) = x one has η = 1). When the moments of L exist, the large deviation function has a regular expansion § near its minimum Φ(s) ≃ (s − s∗)2/(2c2), i.e. ⟨L⟩ ≃ N η s∗ and Var(L) ≃ (2/β) c2 N 2(η−1). Interestingly, the transition from the (universal) regular behaviour for |s − s∗| ≪ 1 to (non universal) behaviours for |s − s∗| ≫ 1 can be associated in some cases to phase transitions in the Coulomb gas [19, 35, 46, 47, 48, 56, 58, 59] (see an example in § 3.2). The study of 1/N expansions in matrix integrals has stimulated considerable eﬀorts for several decades (see for instance Refs. [3, 23, 25, 14, 13, 26, 8, 45, 9]). Some motivations came from ﬁeld theory, where the study of the large number of ﬁeld components was proposed as a tool to probe the strong coupling limit : a famous example is the exploration of SU(N) invariant non Abelian gauge theories in the limit of a large number of colours by ’t Hooft, who identiﬁed that the 1/N expansion corresponds to a genus expansion in Feynman diagrams [53]. Along this line, the planar diagram approximation (N → ∞ limit) was solved by Br´ezin et al. [10] for φ3 and φ4 scalar ﬁeld theories (i.e. solving the combinatoric problem of enumeration of planar Feynman diagrams). Beyond the planar approximation, the systematic topological (genus) expansion in powers of 1/N can be provided through the analysis of loop equations, i.e. recursions between N −k-contributions to correlation functions [43, 3]. A solution of the loop equations of Ambjørn et al. [3] was later found by Eynard [25, 26] and interpreted in terms of geometric properties of algebraic curves. References and a brief review can be found in the introduction of Ref. [26], where the importance of such systematic 1/N expansions of matrix integrals in physics and mathematics is emphasized. Here, we do not provide such a rigorous and systematic analysis, usually written for the characteristic function, but concentrate ourselves on the distribution PN(s), which will be expressed in terms of simple properties of the Coulomb gas. The purpose of the present article is two-fold : ﬁrst, we conjecture in Section 2 a formula including the pre-exponential s-dependent function in front of the large deviation ansatz,
A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin<|sep|>With cryptocurrencies gaining traction among both retail and institutional users over the past few years, the market cap of the cryptocurrencies grew signiﬁcantly. Bitcoin is the most traded and largest cryptocurrency by market capitalisation. While trading activities of traditional assets are dominated by institutional investors, retail investors play a much bigger role in Bitcoin trading (see Goldman Sachs report by Nathan, Galbraith and Grimberg (2021)) . Bitcoin is also a digital asset that does not derive its value from physical demands such as coal and iron ore. This makes the Bitcoin price more susceptible to be inﬂuenced by the market sentiment. For example, the price of Bitcoin rose by as much as 5.2 percent on 24 March 2021 when Elon Musk tweeted Tesla would accept Bitcoin for payments. It also crashed as much as 9.5 percent on 13 May 2021 when Elon Musk tweeted to question the energy consumption from Bitcoin mining. In this paper we examine and propose a multimodal model that can predict extreme Bitcoin price movements based on Twitter data as well as an extensive set of price data with technical indicators and related asset prices. Research has shown that information from media sources is correlated to stock price movements (Pagolu, Reddy, Panda and Majhi, 2016). This also applies to the relationship between social media and cryptocurrency prices. There exists some research that uses sentiment information from social media to try to predict cryptocurrency prices (Mohanty, Patel, Patel and Roy, 2018). We discuss the existing studies at length in Section 2. By just using sentiment information from social media posts, a lot of potentially useful information is ignored. In this research, we therefore leverage a stateof-the-art method to embed the entire tweet contents into a BERT model and use it as input to our predictive model. We further enhance our model using historical candlestick (OHLCV) data and technical indicators, together with correlated asset prices such as Ethereum and Gold. When exploring existing studies that use social media data, we notice that most of the exiting research uses derived measures from the media data source such as sentiments from the text, number of posts, and number of comments as the model input, rather then actual word embeddings. In addition, sentiments are often extracted using pretrained models such as VADER (Elbagir and Yang, 2019), Word2vec (Acosta, Lamaute, Luo, Finkelstein and Andreea, 2017), or BERT (Sun, Huang and Qiu, 2019a). Firstly, sentiment models pretrained for general purpose may not apply to ﬁnancial language, for example, they may not accurately model or embed the words ‘chart’, ‘hold’, ‘bull’ or ‘bear’. The context of the text is also lost when only the derived statistics are used. Utilising the full text of posts in the model retains more information and improve model performance. Hence, in this paper, we use the full text embeddings in our predictive model, in combination with a dedicated ﬁnancial sentence embedding model, FinBERT (Araci, 2019). An additional challenge when doing this is that the number of words in the tweets gathered every day varies and a neural network typically requires a constant input length. We propose a solution to this problem by concatenating the tweets and splitting them into larger blocks as explained in detail in Section 4. To the best of our knowledge, only one study (Lamon, Nielsen and Redondo, 2017) has tried to use text embeddings, not just sentiment, to predict Bitcoin prices, and none of these use an embedding model pretrained on ﬁnancial texts, nor do they predict extreme movements or oﬀer a backtested trading strategy with reduced market exposure risk. This research aims to ﬁll this gap. Most Bitcoin prediction models are formulated as a regression problem (see next section), whereby the exact price in the next bar is predicted. In this research, we opt to approach this as a classiﬁcation problem, whereby we predict extreme price movement (up/down 2 or 5%) the next day. This way, the problem statement becomes directly useful for building trading strategies whereby the investor wants to either take advantage of an upcoming upwards price movement or avoid downwards volatility. We further experiment with moving the predictive threshold of our model and notice how risk exposure can be further minimised in our backtesting study. In this paper, we propose a multimodal embedded model for predicting extreme price movements of Bitcoin and evaluate the impact of diﬀerent modalities, including tweets represented through ﬁnBERT context embeddings. A new dataset is released consisting of tweets as well as candlestick data, related asset prices (Ethereum and Gold) and a selection of technical indicators from 1 January 2015 until 31 May 2021. In an ablation study, we explore the inﬂuence of diﬀerent multimodal data. Finally, we show the usefulness of the model by proposing a simple trading strategy which was backtested with diﬀerent predictive thresholds to optimally control risk exposure. The next section provides a review of related literature. In Section 3, we describe the PreBit dataset in more detail. The proposed models are explained in the next section, followed by our experimental setup in Section 5. Finally, in Section 6, the results from the experiments are discussed followed by a conclusion.
Magneto-transport of graphene and quantum phase transitions in the quantum Hall regime<|sep|>The discovery of the quantum Hall eﬀect (QHE) in two dimensional electron gas (2DEG) opened a new pathway in the study of quantum phase transitions (QPTs). In the quantum Hall regime the Hall resistivity (ρxy), measured as a function of the magnetic ﬁeld or of the charge density, exhibits plateaus while the longitudinal resistivity (ρxx) vanishes, as a consequence of the 2DEG quantum Hall localization. In the region between two adjacent plateaus, in correspondence to the 2DEG delocalization, ρxx shows Shubnikov-de Haas (SdH) peaks which has a temperature dependent width. In this region a QPT takes place, called plateau-plateau (PP) transition, which has been studied since the pioneering experimental works by Tsui and coworkers [1, 2] and the theoretical page-to-page one by Pruisken etal. [3] In the limit of zero temperature the concept of delocalization can be understood assuming that the localization length ξ diverges for some ranges of the Fermi energy. Then, there exist a singular energy Ec in the energy spectrum where ξ diverges with a power-like law ξ ∼ (E − Ec)−γ, being γ its critical index that has been theoretically calculated and supposed to be universal. The ﬁrst works dealing with the calculation of this index neglected the electron-electron (e-e) interaction and revealed a constant value γ = 2.38 [4–6] for the localization length critical index. More recent and accurate calculations, carried out taking into account the e-e interaction, give an higher value γ ≃ 2.61(1) [7, 8]. Indications emphasizing the importance of interactions for the localization phenomenon also come from recent experiments on high-mobility low-density 2D electron structures, which have given evidence for the existence of an interaction-mediated metal-insulator transition [9]. Experimental measurements have also lately revealed that electronic localization in graphene, in the quantum Hall regime, is not entirely dominated by single-particle physics, but rather a competition between the underlying disorder and the repulsive Coulomb interaction exists [10]. Indeed, the eﬀect of interactions near the QPTs, and in particular the role of multifractality, are not well understood at the moment. For instance, at the Integer Quantum Hall transition shortrange interactions seem to be irrelevant, in an renormalization group sense, at the critical point [11], (i.e. the critical exponent γ for the localization length and the multifractal spectrum remain the same as in the non-interacting problem). The 1/r Coulomb interaction is relevant, however, and should drive the system to a novel critical point [12]. Consequently, how the value of the critical exponent γ is aﬀected, is far from being solved. Experimentally it is possible to obtain γ from the magneto-transport measurement using an indirect method which relies on the calculation of the critical exponent κ. This method exploits the relation κ = p/2γ, being p the temperature exponent of the inelastic scattering length (LΦ ∝ T −p/2), widely accepted [13] to be equal to 2 within the framework of the Anderson model (when the disorder is of the alloy type an there is no electron-electron interaction). The pioneering experimental work of Wei et al. [2] gave κ = 0.42(4), a value that has been recently reproduced extending the study to temperatures down to the milli-Kelvin regime [14]. As explained above κ was accepted to be universal for PP transitions under the Anderson model but recent experiments have shown that the value of the exponent depends on the type of disorder. Indeed Wanli Li et al. [15] observed diﬀerent values for κ in AlGaAs/InGaAs heterostructures by changing the density of Al, so that it could be possible that the clustering of Al implies a change in the regime of the transition. This would mean that the Anderson model is no more applicable in systems with long range disorder. This hypothesis has been strengthened by experimental studies that provided κ values between 0.2 and 0.9 [16,17]. Indeed in presence of this type of disorder or when interactions are playing an important role, the electron transport is better described by a percolation picture through the so-called saddle points rather than by the Anderson localization so that the scaling regime of the QPTs has still to be reached and the obtained non-universal value at most represents a cross-over behavior. The magneto-transport measurements in a perfect two dimensional electron systems, such as graphene, in the quantum Hall regime, presents another QPT, referred as plateau-insulator (PI) transition, which is the transition from the last Hall resistivity plateau of the integer quantum Hall eﬀect (IQHE) (ν = 1 in semiconductor 2DEGs and ν = ±2 in the case of graphene) to an insulating phase which persists at higher magnetic ﬁelds. Magneto-transport experiments in a semiconductor 2DEG have shown that ρxx passes from a metallic phase (in the delocalized state or SdH peak) which value decreases by decreasing T , to an insulating phase where ρxx increases by decreasing T. In such transition there exists a T-independent crossing point corresponding to a critical magnetic ﬁeld Bc where all the isotherms collapse. The value obtained for the critical exponent of this transition in semiconductor samples (InGaAs/InP and InGaAs/GaAs) [18–23] was κ = 0.58, but there still exist some controversy whether this transition belongs to the same universality class of the PP one. The recent discovery of graphene has implied a revision of the 2DEG knowledge since it is the ﬁrst truly two dimensional system studied and its properties diﬀer markedly from the ones of the semiconductor-based 2DEG. Indeed, magneto-transport experiments performed in graphene have shown a new type of the IQHE [24, 25], as a consequence of the fourfold degeneracy of the charge carriers [26], and the half-ﬁlling of the n = 0 Landau level (LL) [27] . The fourfold degeneracy of the graphene carriers arises from the usual twofold spin degeneracy and from a novel twofold valley one. These two properties led to assume a special sequence of ﬁlling factor in graphene ν = ±2, ±6, . . ., in contrast with the well-established sequence of ﬁlling factors typical of the IQHE in semiconductor 2DEG (ν = 1, 2, . . .). This sequence of ﬁlling factors is no longer kept in clean graphene samples, and new integer plateaus close to the charge neutrality point (CNP) at ν = 0, ±1, ±3, ±4 appear, as well as fractional ones [28–33]. These exotic states have been observed in the presence magnetic ﬁeld strong enough to lift the spin and valley degeneracy implying symmetry breaking eﬀects and depend strongly on the homogeneity and purity of the graphene sample. SiO2 substrate-supported monolayer graphene samples are usually far to be clean or homogeneous in sharp contrast with suspended or BN supported ones. Some indicators of their quality are a high value of the mobility (> 20000cm2V −1s−1), the proximity of the charge neutrality point (CNP) to 0 V, the narrowness of the Dirac peak and its increase when applying an external magnetic ﬁeld. Moderate mobility samples (with values in the range of 103cm2V −1s−1) and a CNP far from 0 V indicate the presence of noticeable disorder like charged impurities, substrate eﬀects, corrugations and strain (see the work of Mucciolo and coworkers for a complete review [34]). Since it is not easy to control the homogeneity of the sample, it still remains unclear which type of disorder or interaction (charged impurities, alloy, etc.) is the dominant one in each sample. For this major reason the study of the eﬀect of the disorder and interactions in the QHE in graphene is at a preliminary stage yet. Previous experiments have shown that in the vicinity of the the CNP ρxx may either decrease [29], increase [31] at ν ∼ 0 with decreasing temperature or exhibit a metallic state [35] without traces of an insulating one when crossing the CNP resulting of the coexistence of electron and hole puddles [35,36]. These observations have fueled the debate on the existence and origin of an insulating phase close or at the CNP at high magnetic ﬁelds. This confusion is partially clariﬁed in the work by Zhang et al. [37] where it is clearly established that there are two diﬀerent insulating regimes on the N=0 LL in SiO2 substrate-supported monolayer graphene samples (in the vicinity of not at the CNP) and that they appear accordingly to the quality of the sample. A ﬁrst transition to a quantum Hall insulator occurs due to existence of disorder in the sample and can be followed by a second one to a bulk collective insulator state (a pinned Wigner crystal has been suggested) at half LL ﬁlling regime. This latter transition arises from the symmetry breaking and would be observable in samples that exhibits high mobility (> 20000cm2V −1s−1), then it would be hindered by the presence of the disorder in low mobility samples. As explained above, the non-interacting electrons model does not fully describe the critical phenomena of the IQHE so that the model must be improved, in order to clarify the mechanism that control the interplay between disorder and e − e interaction on the electron transport in graphene in the quantum Hall regime. In this work we try to shed more light to the case. Actually, in a previous work [38] we observed the ν = 0 plateau in a graphene sample and we reported preliminary measurements on the PI QPT obtaining the value κ = 0.58(3), in agreement with the one obtained with semiconductor alloys but in a limited range of temperatures. In the present work we present a detailed study of the quantum Hall magneto-transport in various regimes of carrier concentration (which were tuned by means of a gate voltage VG applied to the back-gate of the graphene Hall bar) in a moderate Drude mobility sample. We have carried out the measurements from liquid He temperature up to 230 K. In this way we explore the interplay between the Coulomb interaction, the thermal energy and the disorder landscape probed by the charge carriers and the overall eﬀect on the transport in the quantum regime. In particular we reveal the non universality of the critical exponents when the spin-valley degeneracy is not lifted, a fact which give new insights in the knowledge of standard quantum Hall insulator ν = 0 state.
Measurement of the solar system acceleration using the Earth scale factor<|sep|>The solar system is rotating around the Galactic centre with a period of approximately 200 million years. The solar system barycentre shows, therefore, rotational acceleration in the direction of the Galactic centre of 5-6 microarcseconds of arc per year (µas/yr) and this eﬀect, the secular aberration drift (SAD), results in a systematic proper motion of reference radio sources around the sky. The theoretical prediction of the dipole systematics (Fanselow (1983); Bastian (1995); Eubanks et al. (1995); Gwinn et al. (1997); Sovers et al. (1998); Mignard (2002); Kovalevsky (2003); Kopeikin & Makarov (2006)) was recently conﬁrmed after the analysis of a global set of very long baseline interferometry (VLBI) data since 1979 (Titov et al. (2011); Xu et al. (2012, 2013); Titov & Lambert (2013, 2016); MacMillan (2014)). The magnitude of the dipole systematics is consistent, whereas the estimate of the dipole direction varies mostly owing to the strong inﬂuence of the variability of the relativistic jets (Table 1). The individual apparent proper motion of the reference radio sources may reach 0.5 millisecond of arc/yr (i.e. 100 times of the dipole eﬀect). Therefore, the exclusion of astrometrically unstable radio sources from the analysis is a highly labour-intensive process that yields marginal results with a formal error of 1 µas/yr (e.g. Titov & Lambert (2013)). In the meantime, Mignard & Klioner (2012) showed that the Gaia mission will obtain the parameters of the Galactocentric acceleration with a formal error of 0.2 µas/yr, i.e. ﬁve times better. Therefore, we are seeking methods to improve the VLBI data results. Theoretically, the Galactocentric acceleration vector points to the centre of the Galaxy, which is supposed to be the location of the compact radio source Sagittarius A* with the estimated coordinates αG = 267◦for right ascension and δG = −29◦for declination having the absolute position error of 0′′.12 (Reid & Brunthaler 2004). A comprehensive instruction to calculate the acceleration amplitude using the International Astronomical Unionrecommended Galactic centre distance (Rgal = 8.5 kpc) with the Galaxy rotation velocity at Rgal (Vgal = 220 km/s) and assuming the circular type of motion is given, for example in Kovalevsky (2003) or Titov et al. (2011). This results in a value Rgal = 1.85 · 10−13 km/s2, which can be converted to the acceleration vector a with the magnitude of 3.8 µas/yr. There are dozens of papers devoted to revision of the parameters Rgal and Vgal based on diﬀerent modern observation data and analysis strategies. For instance, Reid et al. (2009) provided new meanings for Rgal = 8.4 ± 0.6 kpc and Vgal = 254 ± 16 km/s. The corresponding acceleration amplitude is 2.49 · 10−13 km/s2 and the amplitude A of the dipole proper motion equals to 5.1 µas/yr. In one of the most recent publications, de Grijs & Bono (2016) recommended a downward adjustment of the Rgal to 8.3 kpc and an upward revision of Vgal in the range from 232 km/s to 266 km/s. The corresponding amplitude of the Galactocentric acceleration vector lies therefore within the range (2.10 ÷ 2.76) · 10−13 km/s2 followed by the expected amplitude of the dipole proper motion between 4.3 and 5.7 µas/yr. Titov (2011) noted that the conventional equation of the geodetic VLBI group delay (Petit & Luzum 2010, chap. 11) could be altered to accommodate the Galactocentric acceleration. This modiﬁcation provides a way to estimate the SAD either as a dipole systematics in proper motions or as a systematic eﬀect in the VLBI scale. Section 2 recalls the corresponding mathematical equations and demonstrates the advantage of the latter option. In Section 3, we present the results obtained by applying the new method to analysis of VLBI data during 1979.72016.5 together with the SAD estimates from global VLBI adjustments.
Structural correlations and phase separation in binary mixtures of charged and uncharged colloids<|sep|>There are, in principle, two mechanisms to stabilize colloidal suspensions against irreversible ﬂocculation, namely charge-stabilization and steric stabilization [1–3]. In the former case, the colloidal particles are highly charged releasing counterions into the solvent such that they repel each other by electrostatics which is traditionally described by a screened Coulomb interaction. In the latter situation of steric stabilization, the colloidal particles are typically coated with polymer brushes causing repulsive entropic interaction forces which stabilize against ﬂocculation. While the eﬀective interactions and structural correlations in strongly interacting colloidal ﬂuids are by now wellunderstood in one-component or even polydisperse systems of either charged [4–10] or uncharged colloidal spheres [11–13], much less is known about binary mixtures of charged and uncharged particles. Such binary neutral-charged systems occur frequently in mixtures of colloids with nanoparticles culminating in the charged nanoparticle-halo eﬀect around neutral colloids which provides colloidal stabilization [14–28]. Moreover, in ordinary mixtures of charged colloids the particle charge can be tuned by the pH of the solution [29–31] such that one component stays charged and the other can become neutral close to the isoelectric point realizing a charged-uncharged colloidal system. Further examples are uncharged spherical vesicles exposed to charged colloids [32] and mixtures of charged nanoparticles and neutral spherical bacteria [33]. About 30 years ago, ﬁrst theoretical calculations were performed for a binary mixture of charged and uncharged spheres [34]. The results were based on a Yukawa model for the interaction between charged particles and a simple excluded hard sphere interaction between the charged-neutral and neutral-neutral spheres as predicted by standard DLVO-theory of linear screening applied to such a mixture. Liquid integral equation closures were then used to compute the partial pair correlation functions [23,31,34–36]. These studies with eﬀective pairwise interactions give some ﬁrst insight into the structural correlations but neglect nonlinear eﬀects [37–39] beyond linear screening. In general the latter cause eﬀective many-body interactions between the colloids [40, 41]. While earlier studies have employed approximate Poisson-Boltzmann theory (see, e.g. Ref. [38]) and local classical density functional theory for the inhomogeneous counterions in the ﬁeld created by the charged colloids [37,39,42–44], it has become possible by now to calculate eﬀective interactions and pair correlations with explicit counterions based on the primitive model (PM) approach of electrolytes [45–53] which includes full nonlinear screening and Coulomb correlations. Though there are simulations for charged mixtures [54–57] and even for oppositely charged colloids [55,58], to the best of our knowledge a mixture of charged and neutral colloids has not yet been simulated using the primitive model approach with explicit microions on a large scale. The binary colloidal mixture considered in this work can be assumed as a particular case of the charge regulated macroion system investigated in Ref. [59] using mean-ﬁeld formulation. In this paper we close this gap and present computer simulations data for charged and uncharged colloidal mixtures and extract their partial pair correlation functions. Complementary to earlier studies of the colloidal halo eﬀect [15–20, 22], we focus on the case of comparable hard core radii of the two colloidal spheres. Here, we ﬁnd that the traditional Yukawa-hard sphere model is suﬃcient to describe the pair correlations in aqueous suspensions where the dielectric constant (or relative permittivity) ǫ of the solvent is pretty high (about 80 for water at room temperature). However, in less polar solvents when ǫ is reduced by an order of magnitude, the Coulomb coupling without screening between the charge species is getting much stronger resulting in nonlinear screening eﬀects. In this study we show that for ǫ = 8, the standard Yukawa-hardcore interaction model as proposed in [34] cannot be applied any longer to a chargeduncharged mixture. There are signiﬁcant deviations in the pair correlations. Moreover we predict the existence of ﬂuid-ﬂuid phase separation as documented by divergence in the partial static structure factors at small wave vector. There are two regions either rich with charged or with uncharged colloidal particles. This phase separation is absent within the traditional DLVO-model when combined with excluded volume interactions. The paper is organized as follows. In section 2 we describe the details of our primitive model simulations for the binary colloidal system. The description of the parameters used in the diﬀerent runs is presented in section 3. The results obtained for the partial pair correlation functions, as well as the structure factors, are discussed in section 4 with supplementary snapshots from the simulation boxes documenting phase separation for high Coulomb couplings without screening. In section 5 we explore the role of the core size of neutral colloids on the correlations and we conclude in section 6.
Particle generation through restrictive planes in GEANT4 simulations for potential applications of cosmic ray muon tomography<|sep|>The wide angular distribution [1] of the incoming cosmic ray muons in connection with either incident angle or azimuthal angle is a challenging trait that leads to a drastic particle loss in the course of parametric computations through the GEANT4 [2] simulations associated with the muon tomography [3–5] since the tomographic conﬁgurations as well as the target geometries also inﬂuence the processable number of the detected particles apart from the generation strategies. To further detail, the basic parameters such as the scattering angle, the particle displacement, and the particle absorption owing to the volume-of-interest (VOI) de facto dictate the particle penetration through the multiple sections of the tomographic setup in addition to the VOI. Hence, a number of the loss cases notably come into eﬀect unless the calculation conditions are fulﬁlled, and not only the computation statistics as well as the numerical outcomes but the initial assumptions like the energy spectrum are also perturbed since the VOI accepts a signiﬁcantly lower number of particles in the instance of the substantial particle loss. While a number of source biasing techniques [6] are oﬀered by MCNP6 [7,8] in the black box format under the class of non-analogue Monte Carlo simulations, the GEANT4 simulations are usually constrained to the existing particle generators or the general particle source (GPS) unless G4ParticleGun is favored. Motivated by the excessive particle loss and its eﬀect on the computation time as well as the characteristic parameters identiﬁed in the muon tomography, we set forth in the present study a scheme that is hinged on the particle generation through the planar restriction by means of the vectorial construction over our tomographic setup consisting of plastic scintillators manufactured from polyvinyl toluene with the dimensions of 100×0.4×100 cm3. This study is organized as follows. In section 2, we elucidate our methodology based on the restrictive planes and we express our characteristic parameters as well as our simulation features in section 3. While we disclose our simulation outcomes in section 4, we draw our conclusions in section 5.
Robust Tracking Control for Constrained Robots<|sep|>Robust tracking control under unknown constrained environment is one of the main important issues in robotic field. A  number of significant papers are proposed in this area. They include fuzzy logic based controllers [1, 2], neural networks  based controllers [3, 4] and sliding mode controllers [5, 6]. In the most cases, these research papers are not concerned by the  compromise between robustness and safety for the tracking control problem. Only few papers are devoted to such subject [7,  8]. This paper is placed in this context. Furthermore, the controller proposed in this paper is simpler than those proposed in  [7] and [8]. The stability and the robustness of the proposed approach are shown, by simulation results, on a robotic  manipulator constrained to a circular trajectory.
Lowest-ID with Adaptive ID Reassignment: A Novel Mobile Ad-Hoc Networks Clustering Algorithm<|sep|>Hierarchical organization of networks is a well-known and  studied problem of distributed computing. It has been proved  an effective solution for problems such as, minimizing the  amount of storage communication (e.g. routing and multicast  tables), thus reducing information update overhead, optimizing  the use of network bandwidth and distributed resources  throughout the network, etc [1]. While the hierarchical  organization fits well in wired infrastructured networks, its  suitability in Mobile Ad-Hoc Networks (MANETs) remains an  open research issue.  MANETs represent dynamic wireless environments that  have been intensively researched within the last years. Unlike  wireless cellular networks which rely on a wired backbone 1  The research work presented herein has been co-funded by 75%  from EU and 25% from the Greek government under the framework  of the Education and Initial Vocational Training II, Programme  Archimedes. connecting base stations, MANETs are self-organizing and  self-configuring multi-hop networks where the network  structure changes dynamically due to node mobility [4].  MANETs are expected to play a critical role in cases where a  wired (central) backbone is neither available nor economical to  build, such as law enforcement operations, battle field  communications, disaster recovery situations, and so on [11].  Such situations require a dynamic network topology where all  nodes, including routers, are mobile and communication  between two end nodes may be supported by intermediate  nodes.  Similarly to wired networks, flat MANET structures  encounter scalability problems with increased network size,  especially in the face of node mobility due to MANETs  intrinsic characteristics. Hence, the need for partitioning  MANET nodes among virtual groups is imperative. Virtual  grouping would create hierarchies of nodes, such that the  network topology can be abstracted. This process is commonly  referred to as clustering� and the substructures that are  collapsed in higher levels are called clusters�[3]. Clustering is  also crucial for controlling the spatial reuse of the shared  channel (e.g. in terms of time division and frequency division  schemes), for minimizing the amount of data to be exchanged  in order to maintain routing and control information in a  mobile environment, as well as for building and maintaining  cluster-based virtual network architectures.   In clustering procedure, a representative of each cluster is  ‘elected’ as a cluster head (CH) and a node that belongs to  more than two clusters is called gateway. Remaining members  are called ordinary nodes. CHs hold routing and topology  information, relaxing ordinary mobile hosts (MHs) from such  requirement; however, they represent network bottleneck  points and -being engaged in packet forwarding activities- are  prone to fast battery exhaustion. The boundaries of a cluster are  defined by the transmission area of its CH.  The concept of clustering in MANETs is not new; many  algorithms that consider different metrics and focus on diverse  objectives have been proposed [2][6][7][9]. Most existing  schemes separate clustering into two phases, cluster formation  and cluster maintenance (where initial cluster configurations  may be modified, depending on nodes movement). During  cluster maintenance phase those algorithms typically involve  increased exchange of control messages and fail to preserve  valuable energy resources of CHs. In this article, we introduce  a distributed algorithm for efficient and scalable clustering of  MANETs that corrects the two aforementioned problems. The  main contributions of the algorithm are: fast and inexpensive completion of clustering procedure; incorporation of both  mobility and battery power metrics in cluster formation;  fairness in cumulative time of serving as CHs among network  nodes; minimization of control traffic volume during clustering  maintenance phase.  The remainder of the paper is organized as follows: Section  II provides an overview of related work in the field of clusterbased mobile ad-hoc networks. Section III describes the details  of our proposed algorithm, while Section IV discusses  simulation results. Finally, Section V concludes the paper and  draws directions for future work.
Interferometric probe of paired states<|sep|>Interference experiments are the primary tool of detecting and characterizing cold atom systems1,2. While original experiments focused on demonstrating macroscopic coherence of large BEC’s [3], subsequent work used interference experiments to explore more interesting phases and phenomena. For example, interference in the time of ﬂight (TOF) experiments was used for observation of the superﬂuid to Mott insulator transition in optical lattices4, analysis of ﬂuctuations in low dimensional systems5,6, and studies of phase diﬀusion and decoherence in dynamically split condensates3,7,8. Interference can also give rise to interesting patterns in second order coherence9. This approach was used to demonstrate that Hanburry Brown Twiss experiments with both bosons and fermions10,11,12,13,14,15,16,17 and to observe pairing of fermions17. A series of recent theoretical and experimental papers explored the idea that one can use interference between two or more low-dimensional systems to probe their non-trivial correlation functions5,6,18. Partially using this ideas Hadzibabic et. al. were able to detect Berezinskii-Kosterlits-Thouless transition in twodimensional bosonic systems which is associated with vortex proliferation5. One of the novel feature of this approach was the idea to use not only the average contrast but the full distribution functions6,19,20. Distribution functions are determined by high order correlation functions and contain a wealth of information about underlying systems. Distribution functions of interference fringe amplitudes were recently analyzed for onedimensional quasi-condensates and provided direct probe of long wavelength phase ﬂuctuations in the system of either quantum or thermal origin [6]. However there is another source of ﬂuctuations of the fringe amplitude which is purely quantum in nature. Namely, this is shot noise coming from the discreteness of particles60. Shot noise is especially strong in systems with short range single particle correlations, in particular in fermionic systems. Thus interferometric probes in such systems are intrinsically more diﬃcult than in the systems with long or quasi long range order for which shot noise is less impor tant than the low wavelength thermal and/or quantum ﬂuctuations20,21. Our emphasis on low-dimensional systems has two main reasons: they exhibit exotic phases more often and it is easy to perform interference experiments with them. In this paper we focus on fermionic and bosonic systems in low dimensions and continue to study the possibility of using interferometry to probe stronglycorrelated many-body states. In interacting systems one is often interested in states which do not have coherence of individual particles but exhibit a coherence (or slowly decaying correlations) of particle pairs. For example, fermionic paired states are characterized by the pairing amplitude Here r is the center of mass position of Cooper pairs and f(η) is the Cooper pair wave function23. The ordered state corresponds to the condensation of pairs of particles and should be analyzed using correlation functions of the form ⟨∆†(r1)∆(r2)⟩. Correlation functions of this type which we will refer to as anomalous correlation functions also arise in the context of exotic states of interacting bosons such as condensates of pairs of bosons24 and polar condensates in two dimensional systems25,26. In principle one can extract anomalous correlation functions analyzing higher order moments of the interference amplitude. However, as we will show below, this might be a very diﬃcult task in practice because of eﬀects of shot noise20,21 and because such anomalous correlation functions can appear as small corrections on top of normal correlation functions. In this paper we suggest an alternative method for identifying paired states and for measuring directly their anomalous correlation functions using interference experiments with two (or more) systems. This paper extends earlier work on the analysis of interference experiments with pairs of independent condensates of single component bosons5,6,18,19,20. Our main purpose here is to show that one can probe fermionic superﬂuidity in low dimensional systems. In particular, we deﬁne a new observable, which we refer to in the text as anomalous interference amplitude, which should vanish when there is no pair ing between fermions and which is nonzero when there is paring in the system. We suggest two methods to detect this anomalous amplitude. The ﬁrst approach relies on detecting interferometric signal in two disjoint parts of the system RI,II and averaging appropriate observable over these disjoint regions. This way of detecting pairing correlations relies on the existence of the long range (or quasi long range) order in the pairing channel and thus requires phase ordering in the fermionic superﬂuids. Note that averaging over two disjoint regions is necessary to cancel the eﬀects of an undeﬁned relative phase of the superﬂuid order parameter in two independent layers. One can straightforwardly extend this idea and split the system to a larger number of disjoint regions improving the signal to noise ratio but other than that not aﬀecting our analysis. In the second method we introduce a weak tunneling coupling between the systems to lock the relative phase. We show that in suﬃciently large systems there is always a broad range of parameters, where the coherence is established but the correlation functions are still not aﬀected by the presence of this weak tunneling term. Because the phase locking transition does not require long range order in each superﬂuid, this method is more sensitive to the formation of the local pairing amplitude. We further argue that in lattice fermionic systems one can measure the symmetry of the pairing gap and thus distinguish, for example, d− wave from s− wave superﬂuidity. This can be achieved by aligning the probing laser beam along diﬀerent axes of the lattice. The ideas presented in this paper can be further extended to low-dimensional Bose systems. We show that in a similar setup one can measure anomalous correlators in bosonic superﬂuids. These correlators have an unusual property that they grow with the separation between the particles showing eﬀective “anti-bunching” behavior for bosons. Usually anomalous correlations are not easy to detect, since they are not gauge invariant, i.e. they are sensitive to the global superﬂuid phase. The two setups considered here eliminate eﬀects of this phase and make such measurements possible. Carusotto and Castin have previously suggested an experiment which relies on particle interference to detect paired states27. While there is some conceptual connection between their work and our approach, our method has an advantage that it does not require Bragg outcoupling of atoms, splitting and mixing of atom beams, and using single atom detectors to measure coincidences. As we demonstrate below, interference of two ballistically expanding independent clouds does all of this work itself! The paper is organized as follows. In Sec. II we ﬁrst analyze the basic structure of anomalous correlators and the interference amplitude between two independent fermionic superﬂuids. We then introduce the new observable, the anomalous interference amplitude, which probes the pairing amplitude. In Sec. III we show how this anomalous amplitude can be detected performing simultaneous measurements in disjoint parts of the time of ﬂight image. Using this scheme we discuss possible set ups for observing the d-wave superﬂuid and the FFLO phases. We suggest how one can detect not only the amplitude, but also a phase of the pairing function. We perform explicit quantitative analysis of the anomalous amplitude for two-dimensional superﬂuids with s− and d− wave pairing based on BCS-theory. Then in Sec. IV we discuss the second way of detecting anomalous interference amplitude by introducing a weak interlayer tunneling. We show that on the one hand its presence introduces corrections to the results of Sec. II, which are not related to the superﬂuidity. On the other hand the presence of this tunneling establishes the interlayer phase coherence. We show that by decreasing the imaging area and increasing the system size one can always achieve the regime where the coherence between the superﬂuids is established and yet the eﬀect of the tunneling on the correlation functions is negligible. In Sec. V we extend our analysis to bosonic superﬂuids. In particular, we show that in the superﬂuids with quasi long range order the anomalous interference amplitude grows superlinearly with the imaging size A. In turn this implies that the corresponding interference contrast increases with A. This behavior is opposite to that of the normal interference amplitude, which always decreases with A. And ﬁnally in Sec. VI we summarize our results. Throughout the paper we use BCS approximation to perform explicit calculations. This approximation is only reliable in the weak coupling regime; at strong coupling one has to do more elaborate calculations. However, we do not expect any qualitative diﬀerence between BCS and exact results.
Radio-Optical Galaxy Shape Correlations in the COSMOS Field<|sep|>Weak gravitational lensing analyses exploit the coherent distortion of galaxy shapes induced by the gravitational potential along the line of sight to constrain the distribution and evolution of massive structures in the Universe, providing an excellent probe of cosmology (see e.g. Kilbinger 2015, for a review). Conducting precision cosmology tests with weak lensing (e.g. selecting between competing models of dark energy) requires high number densities (ngal ≳ 1 arcmin−2) of high redshift (z ≳ 1) sources and analysis tools to measure their shapes to extraordinary accuracy. These attributes have been available to surveys at optical wavelengths for a number of years, with useful cosmological results beginning to emerge from CFHTLens (K¨ohlinger et al. 2016), DES-SV (The Dark Energy Survey Collaboration et al. 2015) and DLS (Jee et al. 2015). As source number densities increase in these and future experiments (and statistical uncertainties decrease) it is the systematic uncertainties which will come to dominate. One way of tackling the problem of such systematics is through multi-wavelength investigations, which have the potential to address both instrumental and astrophysical contamination. In particular, telescopes operating at radio wavelengths are about to undergo a signiﬁcant leap in survey speed and sensitivity and, ultimately, with the Square Kilometre Array (SKA)1 will be capable of worldleading weak lensing cosmology alone (Brown et al. 2015; Bonaldi et al. 2016). In addition, cross-correlation weak lensing studies between radio and optical wavebands appear highly promising, providing comparable cosmological constraints as traditional single-waveband experiments, but with the signiﬁcant advantage of being much more robust to wavelength-dependent systematic eﬀects (Demetroullas & Brown 2016; Harrison et al. 2016). In this paper we focus on a particular aspect of these cross-correlation studies: the correlation between the optical and radio shapes of those galaxies which will be common to both catalogues. As discussed in Harrison et al. (2016) this
Automated One-Loop Calculations with GoSam<|sep|>The Standard Model is currently being re-discovered at the LHC, and new exclusion limits on Beyond the Standard Model particles – and on the Higgs mass – are being delivered by the experimental collaborations with an impressive speed. Higher order corrections play an important role in obtaining bounds on the Higgs boson and New Physics. In particular, the exclusion limits for the Higgs boson would look very diﬀerent if we only had leading order tools at hand. Further, it will be very important to have precise theory predictions to constrain model parameters once a signal of New Physics has been established. Therefore it is of major importance to provide tools for next-to-leading order (NLO) predictions which are largely automated, such that signal and background rates for a multitude of processes can be estimated reliably. The need for an automation of NLO calculations has been noticed some time ago and lead to public programs like FeynArts [1] and QGraf [2] for diagram generation and FormCalc/LoopTools[3] and GRACE [4] for the automated calculation of NLO corrections, primarily in the electroweak sector. However, the calculation of one-loop amplitudes with more than four external legs were still tedious case-by-case calculations. Only very recently, conceptual and technical advances in multi-leg one-loop calculations allowed the calculation of six-point [5,6,7,8,9,10,11,12,13,14,15,16,17, 18,19,20,21,22,23,24] and even seven-point[25,26] processes at all, and opened the door to the possibility of an automated generation and evaluation of multi-leg one-loop amplitudes. As a consequence, already existing excellent public tools, each containing a collection of hard-coded individual processes, like e.g. MCFM [27, 28], VBFNLO [29,30], MC@NLO [31,32], POWHEG-Box [33,34], POWHEL [35,36,37], can be ﬂanked by ﬂexible automated tools such that basically any process which may turn out to be important for the comparison of LHC ﬁndings to theory can be evaluated at NLO accuracy. We have recently experienced major advances in the activity of constructing packages for fully automated one-loop calculations, see e.g. [38,39,40,41,42, 43]. The concepts that lead to these advances have been recently reviewed in [44]. Among the most important developments are the integrand-reduction technique [45,46] and the generalized n-dimensional unitarity [47]. Their main outcome is a numerical reconstruction of a representation of the tensor structure of any one-loop integrand where the multi-particle pole conﬁguration is manifest. As a consequence, decomposing one-loop amplitudes in terms of basic integrals becomes equivalent to reconstructing the polynomial forms of the residues to all multi-particle cuts. Within this algorithm, the integrand of a given scattering amplitude, carrying complete and explicit information on the chosen dimensional-regularisation scheme, is the only input required to accomplish the task of its evaluation. In fact, the integration is substituted by a much simpler operation, namely by polynomial ﬁtting, which requires the sampling of the integrand on the solutions of generalised on-shell conditions. In this article, we present the program package GoSam which allows the automated calculation of oneloop amplitudes for multi-particle processes. Amplitudes are expressed in terms of Feynman diagrams, where the integrand is generated analytically using QGRAF [2], FORM [48], spinney [49] and haggies [50]. The individual program tasks are steered via python scripts, while the user only needs to edit an “input card” to specify the details of the process to be calculated, and launch the generation of the source code and its compilation, without having to worry about internal details of the code generation. The program oﬀers the option to use diﬀerent reduction techniques: either the unitarity-based integrand reduction as implemented in Samurai [40] or traditional tensor reduction as implemented in Golem95C [51,52] interfaced through tensorial reconstruction at the integrand level [53], or a combination of both. It can be used to calculate one-loop corrections within both QCD and electroweak theory. Beyond the Standard Model theories can be interfaced using FeynRules [54] or LanHEP [55]. The Binoth-Les Houches-interface[56] to programs providing the real radiation contributions is also included. The advantage of generating analytic expressions for the integrand of each diagram gives the user the ﬂexibility to organize the computation according to his own eﬃciency preferences. For instance, the computing algorithm can proceed either diagram-by-diagram or by grouping diagrams that share a common set of denominators (suitable for a unitarity-based reduction), and it can deal with the evaluation of the rational terms either on the same footing as the rest of the amplitude, or through an independent routine which evaluates them analytically. These options and the other features of GoSam will be discussed in detail in the following. In Section 2, after giving an overview on the diagram generation and on processing gauge-group and Lorentz algebra, we discuss the code generation and the reduction strategies. The installation requirements are given in Section 3, while Section 4 describes the usage of GoSam, containing all the set-up options which can be activated by editing the input card. In Section 5 we show results for processes of various complexity. The release of GoSam is accompanied by the generated code for some example processes, listed in Appendix A.
Conductivity of two-dimensional narrow gap semiconductors subjected to strong Coulomb disorder<|sep|>In a band gap insulator, charged impurities often play a decisive role in determining the properties of the insulating state. Due to the long-range nature of the Coulomb potential that they create, such impurities produce large band bending that changes qualitatively the nature of electron conduction relative to the ideal disorder-free situation. An illustrative case is that of a three-dimensional completely-compensated semiconductor, for which positively charged donors and negatively charged acceptors are equally abundant and randomly distributed in space. In this case, the impurity potential has large random ﬂuctuations, which can be screened only when the amplitude of this potential Γ reaches ∆, where 2∆ is the band gap. This screening is produced by sparse electron and hole droplets, concentrated in spatially alternating electron and hole puddles [1–3]. At high enough temperatures, the electrical conductivity is due to activation of electrons and holes from the chemical potential to the energy associated with classical percolation across the sample. At lower temperatures, the conductivity is due to hopping between nearestneighbor puddles. At even smaller temperatures, it is due to variable-range hopping between puddles. Crucially, in each of these temperature regimes, the naive relation Ea = ∆ is lost, where Ea is the activation energy for conductivity. Only in the highest-temperature regime is there a direct proportionality between Ea and ∆ (with a nontrivial small numeric prefactor) [3, 4]; at lower temperatures, the observed activation energy is nonuniversal and disorder dependent [1, 2]. In this paper, we consider a similar problem in two dimensions, focusing on the case of strong disorder, Γ ≫ ∆. Speciﬁcally, we consider a two-dimensional small band-gap semiconductor resting on a thick substrate with a three-dimensional concentration of randomly positioned impurities. We derive the temperature dependence of the electrical conductivity across all temperature regimes and show that observed activation energy can be very small. Understanding the relation between the energy gap and the observed activation energy for transport is of crucial importance for studying a variety of two-dimensional (2D) electron systems. For example, recent studies of 2D topological insulators (TIs) [5–7], ﬁlms of 3D TIs [8–20], bilayer graphene (BLG) with an orthogonal electric ﬁeld [21, 22], and twisted bilayer graphene (TBG) [23–27] use the transport activation energy as a way of characterizing small energy gaps. In all these cases, the observed activation energy is much smaller than the energy gap that is expected theoretically or measured through local probes such as optical absorption or scanning tunneling microscopy. Here, we show that there is indeed no simple proportionality between the energy gap and the activation energy except at the highest-temperature regime, which is likely irrelevant for many experimental contexts. Instead, we ﬁnd a wide regime of temperature and disorder strength for which the activation energy is parametrically smaller than the energy gap. At the lowest temperatures the conductivity follows a EfrosShklovskii (ES) law [28] rather than an Arrhenius law, and this dependence can give the appearance of a small activation energy. Let us dwell on two likely applications of our theory. First, our results may be especially relevant for ongoing efforts to understand the energy gaps arising in TBG at certain commensurate ﬁllings of the moir´e superlattice [23–27]. Such gaps apparently arise from electron-electron interactions, but the observed activation energies of the maximally insulating state are typically an order of magnitude smaller than the naive interaction scale (see, e.g., Refs. 24 and 25), and they vary signiﬁcantly from one sample to another. Scanning tunneling microscopy studies also suggest a gap of the order of ten times larger than the observed activation energy [29, 30]. The theory Figure 1. Schematic picture of a cross section of puddles in the case Γ ≫ ∆. The wavy lines show the conduction-band bottom and the valence-band ceiling separated by the gap 2∆ . The red shaded region above the chemical potential µ = 0 represents a hole puddle, while the blue shaded region below µ represents an electron puddle; Γ is the amplitude of the disorder potential, λ is the screening length, and w is the width of the barrier between neighboring puddles. we present here offers a natural way to interpret this discrepancy. Second, our theory can be applied to the huge body of experimental work on thin ﬁlms of a 3D TI, where the surface electrons have a small gap 2∆ due to hybridization of the surface states of two surfaces [8, 9] or due to intentionally introduced magnetic impurities [10–20]. Understanding the origin of the small apparent activation energy Ea ≪ ∆ is crucial for achieving metrological precision of the quantum anomalous Hall effect [11, 13, 16, 19, 20, 31, 32] and the quantum spin Hall effect [9, 33–35]. The model we consider of is a two-dimensional semiconductor with band gap 2∆ atop a substrate with a threedimensional concentration N of random sign charged impurities. We assume that the semiconductor has a gapped Dirac dispersion law, where ǫ is the electron energy, k is the 2D wave vector, v is the Dirac velocity, and ℏ is the reduced Planck constant. We are interested in the case when the amplitude Γ of spatial ﬂuctuations of the random potential satisﬁes Γ ≫ ∆, so that electron and hole puddles occupy almost half of the space each and are separated by a small insulating gap which occupies only a small fraction of space (see Fig. 1). This system is an insulator because in 2D neither electron nor hole puddles percolate, and they are disconnected from each other (neglecting, for now, the possibility of quantum tunneling between puddles). Throughout this paper, we focus on the case of zero chemical potential, for which electron and hole puddles are equally abundant and the system achieves its maximally insulating state. We argue that this situation is likely realized in the experiments of Refs. 5–27, 29, and 30. The remainder of this paper is organized as follows. In the following section, we ﬁrst summarize our main results for the temperature-dependent conductivity. In Sec. III, we review the fractal geometry of two-dimensional puddles for the case Γ ≫ ∆. In Sec. IV, we calculate the action accumulated by electrons tunneling across the gap between two neighboring puddles, the corresponding localization length, and the critical value (Γ/∆)c of the ratio Γ/∆, at which crossover to “almost metallic conductivity” takes place. In Sec. V, we calculate the hopping conductivity at 1 ≪ Γ/∆ ≪ (Γ/∆)c . Section VI deals with the generalization of our results to thin TI ﬁlms. Because of the intense recent interest in such ﬁlms [8–10, 12– 20, 31–45], in this section we add a fair amount of numerical estimates. We close in Sec. VII with a summary and conclusion.
Reflection and Rotation Symmetry Detection via Equivariant Learning<|sep|>From molecules to galaxies, from nature to man-made environments, symmetry is everywhere. Comprehensive perception and exploitation of real-world symmetry are the instinctive abilities of humans and animals that have the potential to take intelligent systems to the next level. The focus of this paper is on the two most primitive symmetries, reﬂection and rotation symmetries. The goal of reﬂection and rotation symmetry detection is to ﬁnd a reﬂection axis and a rotation center that remain invariant under reﬂection and rotation, respectively. Despite decades of efforts [29, 46], symmetry detection methods have been limited to the well-deﬁned symmetry patterns, and the remedy for realworld symmetry is still yet to be thoroughly explored. The simplicity of mathematical concepts of symmetry encouraged early approaches to ﬁnd keypoint pairs that satisfy pre-deﬁned constraints for symmetry [1, 3, 32, 37, 39, 43], Figure 1. Symmetry detection examples of our method EquiSym. (a) an input image, (b) a score map of reﬂection symmetry axes, and (c) that of rotation symmetry centers. Best viewed in color. which leverage hand-crafted local feature descriptors to detect sparse symmetry patterns. Recently, convolutional neural networks (CNNs) have been successfully applied to detect reﬂection symmetry and have surpassed the previous methods by learning score map regression [13] or symmetric matching [38] from data. The primary challenge in detecting symmetry patterns lies in the fact that a symmetry manifests itself with an arbitrary orientation and perceiving the pattern requires an analysis based on the orientation; a reﬂection symmetry mirrors itself against an axis with a speciﬁc orientation and a rotation symmetry matches its rotated copy with a speciﬁc orientation. Most methods for symmetry detection thus involve searching over the space of candidate orientations of symmetry patterns and also developing a robust representation that is either invariant or equivariant with respect to rotation and reﬂection. The early approaches leverage an equivariant representation by extracting oriented keypoints and performing orientation normalization [1,32,37,39,43]. While this technique has proven effective for shallow gradient-based features, it cannot be applied to deep feature maps from standard neural networks, where rotation and reﬂection induce unpredictable variations in representation. To address the challenge, we propose to learn a group equivariant convolutional neural network for reﬂection and rotation symmetry detection, dubbed EquiSym. Recently, there has been active research on equivariant networks to incorporate equivariance explicitly for robust and sampleefﬁcient representation learning [7, 9, 19, 40, 44, 47]. Unlike standard neural networks, they induce predictable and structure-preserving representation with respect to the geometric transformations, e.g., rotation or reﬂection, which is eminently suitable for symmetry detection. To detect consistent symmetry patterns over different orientations, we build a dihedrally-equivariant convolutional network [44], which is designed to be end-to-end equivariant to a group of reﬂection and rotation. The network effectively learns to output a score map of reﬂection axes for reﬂection symmetry or that of rotation centers for rotation symmetry. We also present a new dataset, DENse and DIverse symmetry (DENDI), for reﬂection and rotation symmetry detection. DENDI contains real-world images with accurate and clean annotations for reﬂection and rotation symmetries and mitigates limitations of existing benchmarks [4,12,13,27,38]. First, the reﬂection symmetry axes are diverse in scale and orientation, while previous datasets mostly focus on the dominant axes of the vertical or horizontal ones. Second, the rotation centers are annotated to the objects in polygon and ellipse shape, not limited to the circular objects. Third, the number of the rotation folds for each rotation center is annotated, which is the ﬁrst in a large-scale dataset. Finally, the number of images is 1.7x and 2.0x larger than the second-largest reﬂection and rotation symmetry detection datasets, respectively. The contribution of our work can be summarized as: • We propose a novel group-equivariant symmetry detection network, EquiSym, which outputs groupequivariant score maps for reﬂection axes or rotation centers via end-to-end reﬂection- and rotationequivariant feature maps. • We present a new dataset, DENse and DIverse symmetry dataset (DENDI), containing images of reﬂection and rotation symmetries annotated in a broader range of typical real-world objects. • We show the outstanding performance of EquiSym in reﬂection symmetry detection on SDRW [27], LDRS [38], and DENDI, and in rotation symmetry detection on DENDI.
Feature Guided Search for Creative Problem Solving Through Tool Construction<|sep|>Humans often show remarkable improvisation capabilities, particularly in times of crises. The makeshift carbon dioxide ﬁlter constructed on board the Apollo 13 [8], and the jury-rigged ventilators built to combat equipment shortages during COVID-19 [52], are examples of human ingenuity in the face of uncertainty. In addition to humans, other primates and certain species of birds have also been shown to creatively accomplish tasks by constructing tools from available objects, such as sticks and stones [45, 51, 22]. While the capability to construct tools is often regarded as a hallmark of sophisticated intelligence, similar improvisation capabilities are currently beyond the scope of existing robotic systems. The ability to improvise and construct necessary tools can greatly increase robot adaptability to unforeseen circumstances, enabling robots to handle any uncertainties or equipment failures that may arise [5]. In this paper, we focus on the problem of tool construction in the context of task planning. Speciﬁcally, we address the scenario in which a robot is provided with a task that requires certain tools that are missing or unavailable. The robot must then derive a task plan that involves constructing an appropriate replacement tool from objects that are available to it, and use the constructed tool to accomplish the task. Existing work that addresses the problem of planning in the case of missing tools focuses on directly substituting the missing tool with available objects [7, 3, 32]. In contrast, this is the ﬁrst work to address the problem through the construction of replacement tools, by introducing a novel approach called Feature Guided Search (FGS). FGS enables efﬁcient application of existing heuristic search algorithms in the context of task planning in order to perform tool construction by accounting for physical attributes of objects (e.g., shape, material) during the search for a valid task plan. Heuristic search algorithms, such as A∗ and enforced hill-climbing (EHC), have been successfully applied to planning problems in conjunction with heuristics such as cost-optimal landmarks [23] and fast-forward [21] respectively. However, the application of heuristic search algorithms to perform tool construction in the context of task planning can be challenging. For example, consider a task where the goal of the robot is to hang a painting on the wall. In the absence of a hammer that is required for hammering a nail to complete the task, the robot may choose to construct a replacement for the hammer using the objects available to it. How does the robot know which objects should be combined to construct the replacement tool? One possible solution is for the user to manually encode the correct object combination in the goal deﬁnition, and the search procedure would ﬁnd it. However, it is impractical for the user to know and encode the correct object combination to use, for all the objects that the robot could possibly encounter. Alternatively, the robot can autonomously attempt every possible object combination until it ﬁnds an appropriate tool construction for completing the task. However, this would require a prohibitive number of tool construction attempts. Further, what if the robot cannot construct a good replacement for a hammer using the available objects, but can instead construct a makeshift screwdriver to tighten a screw and complete the task? In this case, the task plan would also have to be adapted to appropriately use the constructed tool, i.e., “tighten” a screw with the screwdriver instead of “hammering” the nail. In order to address these challenges, FGS combines existing planning heuristics with a score that is computed from input point clouds of objects indicating the best object combination to use for constructing a replacement tool. The chosen replacement tool then in turn guides the correct action(s) to be executed for completing the task (e.g., “tighten” vs. “hammering”). Hence, our algorithm seeks to: a) eliminate the need for the user to specify the correct object combination, thus enabling the robot to autonomously choose the right tool construction based on the available objects and the task goal, b) minimize the number of failed tool construction attempts in ﬁnding the correct solution, and c) adapt the task plan to appropriately use the constructed replacement tool. Prior work by Nair et al. introduced a novel computational framework for performing tool construction, in which the approach takes an input action, e.g., “ﬂip”, in order to output a ranking of different object combinations for constructing a tool that can perform the speciﬁed action, e.g., constructing a spatula [29, 31]. For performing the ranking, the approach scored object combinations based on the shape and material properties of the objects, and whether the objects could be attached appropriately to construct the desired tool. In contrast, this work focuses on the application of heuristic search algorithms such as A∗, to the problem of tool construction in the context of task planning. In this case, the robot is provided an input task, e.g., “make pancakes”, that requires tools that are inaccessible to the robot, e.g., a missing spatula. The robot must then output a task plan for making pancakes, that involves constructing an appropriate replacement tool from available objects, and adapting the task plan to use the constructed tool for completing the task. Thus, prior work takes an action as input, and outputs a ranking of object combinations. In contrast, our work takes a task as input, and outputs a task plan that involves constructing and using an appropriate replacement tool. Hence, our work relaxes a key assumption of the prior work that requires the input action to be speciﬁed. Our approach directly uses the score computation methodology described in prior work [29, 31], but combines it with planning heuristics to integrate tool construction within a task planning framework. Our core contributions in this paper include: • Introducing the Feature Guided Search (FGS) approach that integrates reasoning about physical attributes of objects with existing heuristic search algorithms for efﬁciently performing tool construction in the context of task planning. • Improving upon prior work by enabling the robot to automatically choose the correct tool construction and the appropriate action based on the task and available objects, thus eliminating the need to explicitly specify an input action as assumed in prior work. We evaluate our approach in comparison to standard heuristic search baselines, on the construction of six different tool types (hammer, screwdriver, ladle, spatula, rake, and squeegee), in three task domains (wood-working, cooking, and cleaning). Our results show that FGS outperforms the baselines by signiﬁcantly reducing computational effort in terms of number of failed construction attempts. We also demonstrate the adaptability of the task plans generated by FGS based on the objects available in the environment, in terms of executing the correct action with the constructed tool.
A new approach for numerical simulation of the time-dependent Ginzburg-Landau equations<|sep|>Based on the Ginzburg–Landau theory of superconductivity [16], the macroscopic state of a superconductor is described by the complex-valued order parameter ψ, the real scalar-valued electric potential φ, and the real vector-valued magnetic potential A. In the nondimensionalization form, the order parameter satisﬁes that 0 ≤ |ψ|2 ≤ 1, where |ψ|2 = 0 corresponds to the normal state and |ψ|2 = 1 corresponds to the superconducting state, and 0 < |ψ|2 < 1 represents an intermediate state between the normal and superconducting states. If the superconductor occupies a long cylinder in the x3-direction with a ﬁnite cross section and the external magnetic ﬁeld is H = (0, 0, H), then the order parameter ψ and the magnetic potential A = (A1, A2) are governed by the time-dependent Ginzburg–Landau equations (TDGL) This work was supported in part by the National Natural Science Foundation of China (NSFC) under grants No. 11301262, No. 11471031, No. 91430216, and the US National Science Foundation (NSF) through grants DMS-1115530 and DMS-1419040. †Department of Mathematics, Nanjing University, Nanjing, China. (buyangli@nju.edu.cn) ‡Beijing Computational Science Research Center, Beijing, China. ‡Department of Mathematics, Wayne State University, Detroit, USA (ag7761@wayne.edu). 1 in the two-dimensional cross sectional domain Ω, where η is the normalized conductivity, κ is the Ginzburg-Landau parameter, and ψ∗ denotes the complex conjugate of ψ. Discovered by Schmid [24] and derived by Gor’kov and Eliashberg [18] from the microscopic principles, the TDGL was widely accepted for simulation of transient behaviors and vortex motions of superconductors [14, 21]. Variables of physical interest in this model are the superconducting density |ψ|2, the magnetic induction ﬁeld B = ∇ × A, and the electric ﬁeld E = ∂tA + ∇φ. The boundary conditions are � i where n denotes the unit outward normal vector on the boundary ∂Ω. Detailed description of the physics of superconductivity phenomena can be found in the review articles [5, 11] and the books [10, 25]. Here, in a two-dimensional domain, we use the notations the two solutions (ψ, A, φ) and ( �ψ, �A, �φ) are equivalent in producing the physical variables, e.g. superconducting density, magnetic induction and electric ﬁeld. As a consequence, solving the TDGL under diﬀerent gauges is theoretically equivalent in calculating the quantities of physical interest. However, solving the TDGL under diﬀerent gauges is not equivalent computationally. It is important to use a gauge under which the numerical solution is stable and accurate. A widely used gauge in numerical simulations is the temporal gauge φ = 0; see [14, 21]. Numerical simulations of the TDGL under the temporal gauge have been done in many works with either ﬁnite element or ﬁnite diﬀerence methods; see [2, 17, 23, 26, 27, 28]. Under the temporal gauge, (1.1)-(1.2) reduce to Ω |∇A|2 dx, the equation (1.8) is degenerate parabolic. Due to the degeneracy and the nonlinear structure, both theoretical analysis and numerical approximation of (1.7)-(1.11) are diﬃcult. In a smooth domain Ω, existence and uniqueness of a solution for this system were proved in [12]. Finite element approximations of (1.7)-(1.11) and convergence of the numerical solutions have been reviewed in [13] and an alternating Crank–Nicolson schemes was proposed in [23]. Some implicit, explicit and implicit-explicit time discretization schemes were studied in [19]. For the ﬁnite element approximations, error estimates were carried out for a regularized problem, by adding a term −ϵ∇(∇·A) to the equation (1.8). Depending on the parameter ϵ, convergence rate of the numerical solution to the exact solution cannot be expressed explicitly. Although an explicit convergence rate was proved in [29], the strong regularity assumption on the solution restrict the problem to a smooth domain without corners. In a domain with reentrant corners, well-posedness of (1.7)-(1.11) remains open and convergence of the numerical solution is not known yet. To overcome the diﬃculties caused by degeneracy, the Lorentz gauge φ = −∇·A was introduced in [7] for the simulation of TDGL. Under the Lorentz gauge, (1.1)(1.2) reduce to The equation (1.13) is parabolic without degeneracy, as ∥∇ × A∥2 L2 + ∥∇ · A∥2 L2 is equivalent to ∥∇A∥2 L2 for any A ∈ H1 n(Ω) := {a ∈ H1(Ω)2 : a · n = 0 on ∂Ω}. In a bounded smooth domain, existence and uniqueness of solution for (1.12)(1.16) were proved by Chen et al. [8]. Error estimates of the FEM were presented in [6] with a backward Euler scheme and presented in [15] with a linearized Crank–Nicolson scheme. Besides, the regularized TDGL under temporal gauge are approximately in the form of (1.12)-(1.13); see [22]. If the domain contains a reentrant corner, then the magnetic potential may not be in L2(0, T; H1 n(Ω)) and well-posedness of the TDGL remains open in this case. Overall, convergence of the numerical solution is not guaranteed under either gauge if the domain contains reentrant corners. Meanwhile, correct numerical approximation of the TDGL in domains with reentrant corners are important for physicists to study the eﬀects of surface defects in superconductivity [2, 3, 27], which was often done by solving (1.7)-(1.11) or (1.12)-(1.16) with the ﬁnite element method (FEM). We believe that the magnetic potential A may not be in L2(0, T; H1 n(Ω)) in a domain with reentrant corners, and the ﬁnite element solutions of (1.12)-(1.16) may converge to an incorrect solution. Moreover, the incorrect numerical solution of A may pollutes the numerical solution of ψ through the coupling of the equations and lead to wrong approximation of the physical quantity |ψ|. In this paper, we introduce a new approach to simulate the TDGL in a curved polygon which may contain reentrant corners. Speciﬁcally, we reformulate the TDGL into an equivalent system of equations whose solutions are in L2(0, T; H1(Ω)), and propose a simple numerical scheme to solve the reformulated system. We shall demonstrate the eﬃciency of the new approach via numerical simulations, comparing the numerical results with the numerical solutions of (1.7)-(1.11) and (1.12)-(1.16) by using the same triangulation and ﬁnite element space. We will see that, in a domain with reentrant corners, the numerical solution of (1.7)-(1.11) is unstable and the numerical solution of (1.12)-(1.16) is incorrect, while our new approach leads to stable and accurate numerical solutions. Existence and uniqueness of solutions for the reformulated system and its equivalence to the original TDGL system are proved in a separate paper [20].
An Improved Traffic Matrix Decomposition Method with Frequency-Domain Regularization<|sep|>The network traﬃc matrix has been applied to many signiﬁcant application problems such as capacity planning, trafﬁc engineering and anomaly detection. A traﬃc matrix combines diverse traﬃc components with distinct temporal properties. Therefore, it is necessary to decompose them efﬁciently, and this problem is named traﬃc matrix structural analysis [1]. We presented the traﬃc matrix decomposition model in [2] and decomposed a traﬃc matrix into three sub-matrices, which is equivalent to the generalized Robust Principal Component Analysis (RPCA) problem [3]. The results in [2] were achieved by applying the Stable Principal Component Pursuit (SPCP) method in [3]. In this study, we improve the traﬃc matrix decomposition method by using frequency-domainregularization. This method is a variation of SPCP, and is named Stable Principal Component Pursuit with Frequency-Domain Regularization (SPCP-FDR). We design the numerical algorithm for SPCP-FDR, evaluate its decomposition results on the Abilene dataset [7], and show that SPCP-FDR achieves more rational traﬃc decompositions compared with SPCP.
Penalized Push-Sum Algorithm for Constrained Distributed Optimization with Application to Energy Management in Smart Grid<|sep|>Due to emergence of large-scaled networked systems with limited information, distributed multi-agent optimization problems have gained a lot of attention recently. In such systems a number of agents, represented by nodes over some communication graph, aim to optimize a global objective by taking only the local information into account. Beside the various applications of distributed optimization such as robust sensor network control [8], signal processing [14], network routing [5], and machine learning [15], [16], an important and promising area of applicability is energy management of future smart grid [3], [9], [17]. Smart grid is equipped with advanced communication technologies enabling efﬁcient and distributed energy management between the grid’s users [7], [17]. However, from technical point of view, it is important to keep communication costs limited and choose a communication protocol that would require minimal coordination between the agents and stays robust against changes in the network topology [11]. That is why, in this paper, we develop a communication-based optimization algorithm with the desired features mentioned above. For this purpose we utilize the push-sum communication protocol. This protocol was initially introduced in [2] and used in [15] for distributed optimization. The push-sum protocol is applicable to time-dependent network topology and it can overcome the restrictive assumptions on the The authors are with the Control Methods and Robotics Lab at TU Darmstadt, Germany. The work was gratefully supported by the German Research Foundation (DFG) within the SPP 1984 “Hybrid and multimodal energy systems: System theoretical methods for the transformation and operation of complex networks”. communication graph structure such as double stochastic communication matrices [2], [15]. The work [4] studied this algorithm over directed and time-varying communication in the case of well-behaved convex functions. The authors in [13] extended the results to a broader class of non-convex functions. However, all works on the push-sum algorithm presented in the literature so far dealt with unconstrained optimization. As in many applications, including energy management in smart grid, agents face a number of constraints. In this paper, we adapt the push-sum algorithm to the case of convex constrained optimization by introducing an appropriate choice of penalty functions and penalty parameters. Under some standard technical assumptions, we prove convergence of the resulting procedure to the optimal value of the system’s objective function. Another contribution of this paper consists in the application of the proposed procedure to the problem of energy management in smart grid. In contrast to the recent communication-based procedure proposed in [17], the penalty-based push-sum algorithm presented in this work is based on a time-dependent directed communication topology with column-stochastic matrices, where each agent merely needs to know the current number of its out-neighbors to deﬁne the elements of the communication matrix at each iteration. Moreover, it uses only one communication step per iteration, whereas the procedure in [17] requires two communication steps per iteration, with a row-stochastic and a column-stochastic communication matrix at the consequent communication iterations. Thus, the proposed penalty-based push-sum algorithm keeps communication costs cheaper and is able to adapt to the changes in the communication topology. The paper is organized as follows. In Section II we introduce the penalty-based push-sum algorithm and prove its convergence. Section III deals with formulation of the general non-convex energy management problem in smart grid, presents its convex reformulation for which the penaltybased push-sum procedure can be applied, and demonstrates some simulation results. Section IV concludes the paper. Notations. We will use the following notations throughout this paper: We denote the set of integers by Z and the set of non-negative integers by Z+. For the metric ρ of a metric space (X, ρ(·)) and two subsets B1 ⊂ X and B2 ⊂ X, we let ρ(B1, B2) = max{supx∈B1 infy∈B2 ρ(x, y), supy∈B2 infx∈B1 ρ(x, y)}. We denote the set {1, . . . , n} by [n]. We use boldface to distinguish between the vectors in a multi-dimensional space and scalars. We denote the dot product of two vectors a and b by ⟨a, b⟩. ∥·∥ denotes the standard Euclidean norm, whereas ∥·∥l1 is used to denote l1-norm in the vector space. Throughout this work, all time indices such as t belong to Z+. For vectors vi ∈ Xd, i ∈ [n], of elements in some vector space X (over R), we let ¯v = 1 n �n i=1 vi. We say the function F : Rd → R be inf-compact, if the set {x ∈ Rd : F(x) ≤ A} is compact for all A ∈ R. The function 1{A}(x) denotes the indicator of the set A (1{A}(x) = 1, if x ∈ A and 1{A}(x) = 0, otherwise). The notation o(x) as x → x0 is for some function f(x) such that limx→x0 f(x)
Tracking without bells and whistles<|sep|>Scene understanding from video remains one of the big challenges of computer vision. Humans are often the center of attention in a scene, which leads to the fundamental problem of detecting and tracking them in a video. Tracking-bydetection has emerged as the preferred paradigm to solve the problem of tracking multiple objects as it simpliﬁes the task by breaking it into two steps: (i) detecting object locations independently in each frame, (ii) form tracks by linking corresponding detections across time. The linking step, or data association, is a challenging task on its own, due to missing and spurious detections, occlusions, and target interactions in crowded environments. To address these issues, research in this area has produced increasingly complex models achieving only marginally better results, e.g., multiple object tracking accuracy has only improved 2.4% in the last two years on the MOT16 [45] benchmark. In this paper, we push tracking-by-detection to the limit by using only an object detection method to perform tracking. We show that one can achieve state-of-the-art tracking results by training a neural network only on the task of detection. As indicated by the blue arrows in Figure 1, the regressor of an object detector such as Faster-RCNN [52] is sufﬁcient to construct object trajectories in a multitude of challenging tracking scenarios. This raises an interesting question that we discuss in this paper: If a detector can solve most of the tracking problems, what are the real situations where a dedicated tracking algorithm is necessary? We hope our work and the presented Tracktor allows researchers to focus on the still unsolved critical challenges of multi-object tracking. This paper presents four main contributions: • We introduce the Tracktor which tackles multi-object tracking by exploiting the regression head of a detector to perform temporal realignment of object bounding boxes. • We present two simple extensions to Tracktor, a reidentiﬁcation Siamese network and a motion model. The resulting tracker yields state-of-the-art performance in three challenging multi-object tracking benchmarks. • We conduct a detailed analysis on failure cases and challenging tracking scenarios, and show none of the dedicated tracking methods perform substantially better than our regression approach. • We propose our method as a new tracking paradigm which exploits the detector and allows researchers to focus on the remaining complex tracking challenges. This includes an extensive study on promising future research directions. c⃝ 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Figure 1: The presented Tracktor accomplishes multi-object tracking only with an object detector and consists of two primary processing steps, indicated in blue and red, for a given frame t. First, the regression of the object detector aligns already existing track bounding boxes bk t−1 of frame t − 1 to the object’s new position at frame t. The corresponding object classiﬁcation scores sk t of the new bounding box positions are then used to kill potentially occluded tracks. Second, the object detector (or a given set of public detections) provides a set of detections Dt of frame t. Finally, a new track is initialized if a detection has no substantial Intersection over Union with any bounding box of the set of active tracks Bt = {bk1 t , bk2 t , · · · }. Several computer vision tasks such as surveillance, activity recognition or autonomous driving rely on object trajectories as input. Despite the vast literature on multiobject tracking [42, 38], it still remains a challenging problem, especially in crowded environments where occlusions and false detections are common. Most state-of-the-art works follow the tracking-by-detection paradigm which heavily relies on the performance of the underlying detection method. Recently, neural network based detectors have clearly outperformed all other methods for detection [33, 52, 50]. The family of detectors that evolved to Faster-RCNN [52], and further detectors such as SDP [63], rely on object proposals which are passed to an object classiﬁcation and a bounding box regression head of a neural network. The latter reﬁnes bounding boxes to ﬁt tightly around the object. In this paper, we show that one can rethink the use of this regressor for tracking purposes. Tracking as a graph problem. The data association problem deals with keeping the identity of the tracked objects given the available detections. This can be done on a frame by frame basis for online applications [5, 15, 48] or track-by-track [3]. Since video analysis can be done ofﬂine, batch methods are preferred since they are more robust to occlusions. A common formalism is to represent the problem as a graph, where each detection is a node, and edges indicate a possible link. The data association can then be formulated as maximum ﬂow [4] or, equivalently, minimum cost problem with either ﬁxed costs based on distance [26, 49, 66], including motion models [39], or learned costs [36]. Alternative formulations typically lead to more involved optimization problems, including minimum cliques [65], general-purpose solvers like MCMC [64] or multi-cuts [59]. A recent trend is to design ever more complex models which include other vision input such as reconstruction for multi-camera sequences [40, 60], activity recognition [12], segmentation [46], keypoint trajectories [10] or joint detection [59]. In general, the signiﬁcantly higher computational costs do not translate to significantly higher accuracy. In fact, in this work, we show that we can outperform all graph-based trackers signiﬁcantly while keeping the tracker online. Even within a graphical model optimization, one needs to deﬁne a measure to identify whether two bounding boxes belong to the same person or not. This can be done by analyzing either the appearance of the pedestrian, or its motion. Appearance models and re-identiﬁcation. Discriminating and re-identifying (reID) objects by appearance is in particular a problem in crowded scenes with many object-object occlusions. In the exhaustive literature that uses appearance models or reID methods to improve multi-object tracking, color-based models are very common [31]. However, these are not always reliable for pedestrian tracking, since people can wear very similar clothes, and color statistics are often contaminated by background pixels and illumination changes. The authors of [34] borrow ideas from person re identiﬁcation and adapt them to “re-identify” targets during tracking. In [62], a CRF model is learned to better distinguish pedestrians with similar appearance. Both appearance and short-term motion in the form of optical ﬂow can be used as input to a Siamese neural network to decide whether two boxes belong to the same track or not [35]. Recently, [54] showed the importance of learned reID features for multi-object tracking. We conﬁrm this view in our experiments. Motion models and trajectory prediction. Several works resort to motion to discriminate between pedestrians, especially in highly crowded scenes. The most common assumption is the one of constant velocity (CVA) [11, 2], but pedestrian motion gets more complex in crowded scenarios for which researchers have turned to the more expressive Social Force Model [57, 48, 61, 39]. Such a model can also be learned from data [36]. Deep Learning has been extensively used to learn social etiquette in crowded scenarios for trajectory prediction [39, 1, 55]. [67] use single object tracking trained networks to create tracklets for further postprocessing into trajectories. Recently, [7, 51] proposed to use reinforcement learning to predict the position of an object in the next frame. While [7] focuses on single object tracking, the authors of [51] train a multi-object pedestrian tracker composed of a bounding box predictor and a decision network for collaborative decision making between tracked objects. Video object detection. Multi-object tracking without frame-to-frame identity prediction is a subproblem usually referred to as video object detection. In order to improve detections, many methods exploit spatio-temporal consistencies of object positions. Both [28] and [27] generate multiframe bounding box tuplet proposals and extract detection scores and features with a CNN and LSTM, respectively. Recently, the authors of [47] improve object detections by applying optical ﬂow to propagate scores between frames. Eventually, [18] proposes to solve the tracking and detection problem jointly. They propose a network which processes two consecutive frames and exploits tracking ground truth data to improve detection regression, thereby, generating two-frame tracklets. With a subsequent ofﬂine method, these tracklets are combined to multi-frame tracks. However, we show that our regression tracker is not only online, but superior in dealing with object occlusions. In particular, we do not only temporally align detections, but preserve their identity.
SenTion: A framework for Sensing Facial Expressions<|sep|>Facial expressions play a very important role for sensing the perceived human emotion and intention in people. Driven by advancements in machine learning, image processing and human cognition they have found a wide variety of applications in interpreting user reactions, human computer interaction systems, emotion based interactive games, creation of virtual characters, surveillance, driver-less car, etc. Facial expression analysis consist of three major stages: feature extraction, feature selection and classiﬁer construction. In the ﬁrst stage, features based on geometry or appearance textures of the face are extracted from a image or from a sequence of images. These facial features are extracted from face by using various feature extraction method like HOG, LBP, Gabor-Wavelet, AMM models etc. From these extracted set feature a subset feature is selected in second stage which best ﬁtted to distinguish between different expression. Then in third stage classiﬁer is constructed using these subset of features. Although various methods has been proposed for facial expression recognition, robust recognition that performs well across databases and subjects is still a challenging task, as most of the methods are person dependent, intolerant to scale changes and have low rate of accuracy across emotions and high computational overhead. In this paper, we propose SenTion: a person independent and scale invariant framework for sensing emotions from facial expressions. Accuracy of the proposed system has been tested on the Cohn-Kanade Plus and JAFFE database, where it shows considerable improvement with low computational overhead making is suitable for realtime application. We validate our results with a user study, that compares the recognition accuracy of our system across different expressions and overlays it with the corresponding human perception associated with it. The speciﬁc contributions of our work is threefold. We ﬁrst describe in detail our novel techniques of fusing geometrical and appearance based features. We then demonstrate the goodness of our system by evaluating it on benchmark databases. Finally, we validate the practicality of facial expression recognition systems via a ﬁrst of it’s kind user study that measures the ease of expressing and understanding of various facial expressions by humans.
Sharp recovery bounds for convex demixing, with applications<|sep|>In modern data-intensive science, it is common to observe a superposition of multiple information-bearing signals. Demixing refers to the challenge of separating out the constituent signals from the observation. A fundamental computational question is to understand when a tractable algorithm can successfully complete the demixing. Problems of this sort arise in ﬁelds as diverse as acoustics [1], astronomy [70], communications [7], [8], geophysics [77], image processing [71], [35], machine learning [14], and statistics [12]. Some well-known examples of convex methods for demixing include morphological component analysis [70], robust principal component analysis [17], [12], and inpainting [35]. This work presents a general framework for demixing based on convex optimization. We study the geometry of the optimization problem, and we develop conditions that describe precisely when our method succeeds. Let us illustrate the major aspects of our approach through a concrete example. ∗Corresponding author. †The authors are with the Department of Computing & Mathematical Sciences, California Institute of Technology, 1200 E California Blvd., Pasadena, CA 91125. Email: {mccoy,jtropp}@cms.caltech.edu. Tel.: (626) 395-4059 Fax: (626) 578-0124. Research supported by ONR awards N00014-08-1-0883 and N00014-11-1002, AFOSR award FA9550-09-1-0643, DARPA award N66001-08-12065, and a Sloan Research Fellowship.
Full $\mathcal{O}(\alpha)$ electroweak radiative corrections to $e^+e^- \rightarrow t \bar{t} \gamma$ with GRACE-Loop<|sep|>The experimental results of CDF [1] and D0 [2] on the measurement of top pair production at the Tevatron show an unexpected large top quark forward-backward asymmetry. The precise theoretical calculations of the top pair production play an important role in explaining the experimental data. QCD radiative corrections to top pair production from proton-proton collisions were calculated by several authors [3], [4], [5], [6], [7]. However, the measurement is aﬀected by a huge background from QCD. A good example is the gg → t¯t reaction. In the future, the measurement will be performed at the ILC without QCD background. Therefore, we consider the precise calculations of top pair production and top pair with photon production in e+e− collisions. A completed full one-loop electroweak correction calculation to the process e+e− → t¯t has already been presented in refs [8], [9], [10]. In this paper, we calculate the full O(α) electroweak radiative corrections to both the process e+e− → t¯t and e+e− → t¯tγ at the ILC. The data of the ATLAS [11] and CMS [12] experiments prove the existence of a new boson with mass around 126GeV. It is assumed to be the standard-model Higgs particle. Once the discovery of the Higgs boson is conﬁrmed, the next important task is to measure its properties. However, it is clear that such a measurement is much easier at the cleaner environment of the ILC than at the LHC with its large QCD backgrounds. To measure the properties of the new boson it is important that also the radiative correction calculations for the ILC take the complete standard model into account. The experiments at the ILC require a precise determination of the luminosity which will be based on higher order theoretical calculations of the Bhabha scattering crosssection. Thus, the computation of electroweak radiative corrections to the process e+e− → e+e−γ is mandatory. This will be our eventual target. However, as a ﬁrst step, we are going to calculate the process e+e− → t¯tγ which is easier in several respects: there are fewer diagrams and the numerical cancellations between the diagrams are less severe. It will provide a framework for our target calculation. In the scope of this paper, we discuss the full O(α) electroweak radiative corrections to the process e+e− → t¯tγ at ILC. We then examine the numerical results of the top quark forward-backward asymmetry as well as the genuine weak corrections in both the α scheme and the Gµ scheme [13] as compared to the process e+e− → t¯t. The paper is organized as follows. In section 2 we introduce the GRACE-Loop system and set up the calculation. In section 3 we discuss the numerical results of the calculation. Future plans and conclusions of our paper are presented in section 4.
Variance Based Algorithm for Grouped-Subcarrier Allocation in OFDMA Wireless Systems<|sep|>Optimal and suboptimal algorithms for radio resource management in wireless high speed networks based on orthogonal frequency division multiplexing (OFDM) are widely studied in the literature. Several systems are considered in these studies: single or multiple antennas at the transmitter and at the receiver. The complexity of resource allocation algorithms can be reduced in the case of downlink environment of OFDM access (OFDMA) systems by dividing the available subcarriers into groups (partitions) based on the channel coherence bandwidth and users compete for these groups. This system generalizes the independent subcarrier per subcarrier allocation. Recent published studies have considered this system model as [1] [2] [3] [4] and references therein. The groups are composed of continuous or non-continuous subcarriers, based on how the channel fading is fast or slow as described in [5]. Multiuser diversity (MUD), if used optimally, enhances the system sum capacity by mitigating the fading effect, while providing some quality of service (QoS) to users. For a given user QoS requirement, optimal algorithms have to optimize one between two major goals: (i) maximize the data rate under power constraints [2] or (ii) minimize the overall transmit power under data rate constraints [6]. Some papers considered the ﬁrst optimization problem for the radio resource management of OFDMA systems. Therefore, an adaptive subcarrier allocation algorithm has been proposed in [3], where the available subcarriers are divided into blocks (groups) and the algorithm assigns blocks to each user according to its required data rate and BER constraint. This approach is a two-step method that ﬁrstly, adopted an adaptive block allocation to increase the system capacity by using channel state information (CSI) of all users and assigning a subset of blocks with the highest average channel gains to the corresponding user. Secondly, an iterative improvement procedure is employed to minimize the total required transmit power while satisfying multiuser data rates and BER requirements. This algorithm enhances the throughput and the power efﬁciency but doesn’t provide a capacity close to the optimal. To increase the system capacity a decentralized subcarrier allocation algorithm [2] provided an interesting solution. All users divide all the subcarriers into a number of partitions (groups) in parallel and each user selects the partition with the highest average channel gain independently. Since each user attempts to select the partition with the highest average channel gain, more than one user may conﬂict in the selection of a partition, if it is the best one to them simultaneously. The important contribution of this algorithm is to resolve the conﬂict selection. An iterative algorithm is used for partition with conﬂicting selection by using a usage value of each partition for each user. This iterative step enhances the system performance but increases signiﬁcantly the algorithmic complexity of the system. A solution to this problem has been provided in [1], where an adaptive grouped-subcarrier allocation algorithm is proposed for increasing the system broadcast capacity of OFDM systems and reducing their complexity. Instead of the usage value, the known average channel gain of each group is used to resolve the conﬂict problem among users. The ﬁrst step of the allocation algorithm is followed by a swap procedure used to enhance the system capacity. This swap procedure is its still drawback. Therefore, it enhances the system capacity but for the cost of a signiﬁcant increase of the complexity. As a solution to this problem, we propose an algorithm based on the variance of the transmissible rates on user groups. The groups without conﬂict selection are assigned ﬁrst. Then, the scheduler computes, for each active user, the variance of transmissible rates on all the reported groups (the variance of the reported gains). These variances are then sorted in a descending order. To optimally share the available groups, the proposed algorithm allows ﬁrst, the user that has the max variance to select his best group. The assigned group is removed from the set of the available groups for the allocation. The same processing is done for the set of the remainder groups. If a user reaches his requested rate, he would be removed from the set of users allowed to compete for group allocation. After this ﬁrst step, the subset of the remainder unassigned groups is allocated adaptively to users. Accordingly, while providing a system capacity close to that obtained in [1], the complexity is reduced efﬁciently. This algorithm would be analyzed in section 3. The remainder of this paper is organized as follows: While section 2 presents the system model, section 3 analyses the operation mode of the proposed scheduling scheme and section 4 presents a selection of simulation results and performance analysis.
Y(9.46 GeV) and the gluon discovery (a critical recollection of PLUTO results)<|sep|>messenger of the strong (“color”) force, were proposed (after early papers by Gell-Mann in 1962  [1] and 1964 [2] in which the gluon is mentioned for the first time as a neutral vector meson field)  in the years 1970-1980 [3,4] (see also reviews [5,6]), in parallel and after the quark parton model  was stabilized. A laboratory for studying QCD and gluons was proposed to be the next heavy  narrow hadronic resonance [7-15].  In 1977 the (9.5 GeV) resonance was discovered at Fermilab  [16,17] and its very narrow width (≈50 KeV) was found at DORIS (3-10 GeV e+e- storage ring at  DESY) by the experiments PLUTO [18] and DASP2 [19,20] in May 1978 and later by DHHM [21]. The  first evidence for the abundant decay of  into 3 gluons was reported by the PLUTO Collaboration,  at Seminar, Schools and Conferences in Summer 1978 [23-31], as well as in publications [32-34].  These results were already quoted to be a strong hint toward the existence of the gluon ([27-29],  the same repeated later in [44-46]). Further presentations followed at Winter [35,36] and Spring  [37-40] Schools and meetings, and the cross sections were given in the thesis [41]. In June 1979 at  the Geneva International Conference [42] the evidence for the  decay into 3 gluons (with the  partonic matrix element) was presented by PLUTO [43], mentioned also in [44,45], and the first  evidence at PETRA (the new 10-48 GeV e+e- storage ring at DESY) for quark jet broadening by  gluon radiation was shown by TASSO [46] and also with more results by PLUTO [47]. At the  following Lepton-Photon Symposium at FermiLab [48] PLUTO showed the step in R due to the  production of the new quark b and confirmed the jet broadening [49] and the 3-gluon  interpretation of the  decay [50]. At this conference the evidence for three jet events  (interpreted as gluon radiation by a   ̅ pair) was shown by the TASSO, PLUTO, MARK-J and JADE  experiments at PETRA [51-57] again confirming the existence of gluon jets now at a factor three  larger energies. A review of the latest results from DESY, summarizing the evidence for gluons,  both from 3-gluon decay and gluon bremsstrahlung, was presented by H. Schopper, director of  the laboratory, at the Goa International Symposium in September 1979 [58]. worthwhile to recollect and recall in this article, and for a wider public, what PLUTO did in relation  to the gluon discovery in the years 1978 and first half of 1979 and the confirmations obtained both  at DORIS and at PETRA.1 experiment at DORIS; in Chapter 3 we sketch the PLUTO detector and the properties of the DORIS  and PETRA storage rings, with a brief history of the machines and the detector; in Chapter 4 we  outline the model simulations of the physical processes. In the main Chapter 5 we recollect the  elements for the discovery of the 3-gluon decay: the  resonance; inclusive dynamics;  geometry (topology); exclusion of alternative models; exclusive 3-gluon dynamics and gluon  hadronization (the first study of gluon jets). All with the aim to single out the sufficient and the  necessary conditions to demonstrate the validity of the 3-gluon hypothesis (QCD). In Chapter 6 we  cover the confirmations found at DORIS, especially by a more sophisticated detector (ARGUS), as  well by CLEO at the CESR storage ring, Cornell, USA, the jet broadening found at PETRA and the                                                                1 While writing this article a recent related historical review of P. Söding in this journal  [138] came to our  knowledge. As that review mainly concentrated on the contribution to the gluon discovery by a different  process, at a different collider (PETRA, at substantially higher c.m.s. energies), at a different (later)  time  and with a different experiment, we consider the two articles to be complementary. Our view on the value  of the  decays in the search for evidence for gluons is different, as it is motivated also in Fig. 11 and  footnote 12. most important confirmation for the gluon: the discovery of gluon bremsstrahlung. Finally we give  a summary and draw the conclusions in Chapter 7.
Temperature dependence of low-energy phonons in magnetic nonsuperconducting TbNi2B2C<|sep|>In some rare-earth nickel borocarbides RNi2B2C, superconductivity coexists with magnetic order [1, 2]. Extensive neutron and x-ray scattering experiments revealed an incommensurate magnetic structure below approximately 15 K in both superconducting Er, Ho [3– 6] and nonsuperconducting Tb, Gd [7, 8] compounds. This observation was interpreted in terms of common Fermi-surface nesting features along a∗, which cause magnetic ordering of the rare-earth moments via the Ruderman-Kittel-Kasuya-Yosida (RKKY) mechanism [7]. 57Fe M¨ossbauer spectroscopy and muon- spin relaxation (µSR) studies of polycrystalline TbNi2B2C [9] conﬁrmed the presence of a small ferromagnetic component below about 8 K previously observed via neutron diﬀraction [7] and magnetization measurements [10, 11]. In addition to magnetic eﬀects, strong phonon softening has been observed in superconducting RNi2B2C single crystals with R = Lu, Y, Er, and Ho [12–15], while no signiﬁcant temperature dependence of the phonon spectra was detected for the nonsuperconducting TbNi2B2C [16]. The superconducting transition temperature Tc systematically decreases for RNi2B2C (R=Lu, Y, Tm, Er, and Ho) upon going from Lu (Tc = 16.6 K) to Ho (Tc = 7.5 K). This observation was interpreted by H. Eisaki et al. in terms of increasing coupling between the rareearth magnetic moments and the conduction electrons [1], which suppressed superconductivity. For TbNi2B2C, this pair breaking could be strong enough to completely destroy superconductivity. An alternative possibility for the absence of superconductivity in this system is that electron-phonon coupling is weaker than in the superconducting compounds [16]. We investigated the strength of electron-phonon cou
Semantic Hierarchy Preserving Deep Hashing for Large-scale Image Retrieval<|sep|>The past few years have witnessed a signiﬁcant improvement in the quality of content-based image retrieval (CBIR) [1, 2, 3] due to the emergence of deep learning. With the explosive growth of online visual data, there is an urgent need to develop more eﬃcient deep learning models. Recently, deep hashing has been proposed as a promising method for large-scale image retrieval. It directly projects images to binary codes for approximate nearest neighbor search, which considerably reduces the storage and computation cost. In the real world, the classiﬁcation and description of things often follow a hierarchy structure. One typical example is taxonomy, as shown in Fig. 1. However, most existing deep hashing models [4, 5, 6, 7] only utilize single-level semantic labels for training. Their training processes are either supervised with the ﬁnelevel labels or similar/dissimilar pairs of labels that are converted from ﬁne-level labels. With such supervision, the deep hashing model can only learn partly class Figure 1: A tree visualization of three-level hierarchy comprising six kinds of mammals. Note that we refer the Species level to ﬁne-level and the Order level to the highest level, which is analogous to the leaf node and the root node in the hierarchy tree, respectively. similarity within the lowest hierarchy while the class similarity between the upper-level labels is not well preserved as the semantic hierarchy structure. Consequently, it hinders the deep hashing model from learning a better semantic hashing space for hierarchical retrieval. Taking Fig. 1 as an example. Without hierarchy, bears’ feature embeddings are not necessarily closer to giant pandas belonging to the same superclass (Ursidae) than that to other species belonging to diﬀerent super-class. In this work, we propose a novel deep hashing method called Hierarchy Preserving Deep Hashing (HPDH). Fig. 2 illustrates a comparison of the ﬁne-level labels-based Deep Class-Wise Hashing (DCWH) [10] and the proposed method. It is clear that the hashing codes generated by our method fall into an obvious hierarchy structure. The main contribution of HPDH can be summarized in three folds: • We propose a hierarchical loss function that directly uses class labels for hashing learning. The proposed loss function preserves intra-class compactness and inter-class separability in each hierarchy level. Figure 2: Visualization of learned hashing codes using t-SNE [8]. The samples in CIFAR-100 [9] are labeled with both ﬁne-level and coarse-level labels. We illustrate samples by their coarse-level labels. Each color indicates one coarse-level class. Note each coarse-level class contains exactly ﬁve ﬁne-level classes. • Experiments on two benchmark datasets: CIFAR100 and NABirds show that our method consistently outperforms other state-of-the-art baselines with a distinct margin on both general ﬁne-level retrieval and hierarchical retrieval.
Sector coupling via hydrogen to lower the cost of energy system decarbonization<|sep|>As the greenhouse gas (GHG) emission intensity of electricity generation in various regions has declined with continued adoption of wind and solar generation, there is growing interest to pursue electriﬁcation-centric decarbonization strategies for other end-use sectors where emissions reduction has been sluggish. Yet, direct electriﬁcation may be practically challenged for some of these end-uses, such as in the case of heavy-duty transport where volumetric energy density and refueling time are key drivers for fuel choice. In this context, there is renewed interest in hydrogen (H2) and H2 derived carriers for their role in the decarbonization of difﬁcult-to-electrify end-uses in transport, building and industrial sectors. In addition to the plurality of its end-uses, the multiple technology choices across *Corresponding author: gnhe@mit.edu †MIT Energy Initiative, Massachusetts Institute of Technology, Cambridge, MA, USA. ‡Shell Global Solutions International B.V., Shell Technology Centre Amsterdam, 1031 HW Amsterdam, Netherlands. the H2 supply chain, from production, storage, transport and end-use, make its assessment a complex systems problem. Here, we propose a scalable decision-support framework for assessing the impact of technology and policy choices on the decarbonization of power sector in conjunction with other end-use sectors. This framework provides a systematic way to study the role and impact of H2-based technology pathways in a future low-carbon, integrated energy system at a regional/national scale. Recent renewed interest in H2 has been partially intrigued by expectations of a future renewablesdominant electric grid and cost declines for water electrolyzers1, both of which raise the prospect of electrolytic H2 becoming cost-competitive with fossil fuel-based pathways, such as natural gas reforming2,3. Besides the economics of electrolytic H2 production4,5, many studies have focused on evaluating the economics of H2-based energy storage (power-to-gas-to-power, P2G2P), which relies on electrolysis for H2 production, under deep decarbonization scenarios. Some of the studies in this area focus on: 1) comparing the cost-effectiveness of P2G2P with other types of long-duration energy storage options like pumped hydro and compressed air energy storage for VRE integration, from a marginal deployment perspective (i.e. electricity price taker)6–8, 2) assessing least-cost investment and operation of H2 storage and short-term energy storage like lithium-ion batteries in the context of a VRE dominant power systems8–11, and 3) the operational scheduling of H2 storage in power markets12,13. Although these studies provide useful insights to compare different energy storage technologies from the perspective of the power sector, they overlook the multiple potential uses of H2 (or H2 derived carriers) outside the power sector and the associated cost-savings resulting from sharing infrastructure costs across these uses. Consequently, in the absence of modelling sectorcoupling interactions, the role of H2 storage may be under-valued as compared to other long-duration storage technologies in future low-carbon power grids14. With the above motivation, a number of studies have expanded the scope of traditional power sector capacity expansion models (CEM) to endogenize investment decisions in end-use technologies, which includes some parts of the H2 supply chain, notably electrolytic H2 production. These studies highlight the potential for ﬂexible electricity consumption in other end-uses to partially substitute the need for energy storage in the electricity sector and alter generation mix in the power sector towards increasing VRE deployment15–18. While these studies are inspiring, the interactions between the H2 supply chain and the power sector, in many of the studies, exclude critical components in the H2 supply chain. For example, some studies ignore the possibility of natural gas-based H2 production from steam methane reformer (SMR) with or without carbon capture and storage (CCS)19–21, which is the dominant mode of H2 production today. Second, most literature either do not consider some modes of H2 transmission19,22,23 or when it is included, the modelling of H2 transmission is oversimpliﬁed by setting ﬁxed lower and upper H2 ﬂow limits for each route20–22,24. These approaches may not capture the potential beneﬁts of both H2 pipeline and trucks serving as transmission and storage assets simultaneously. Notably, H2 trucks can function as mobile storage, which has been shown to provide greater operational ﬂexibility than stationary storage25. Moreover, the existing literature does not reveal a clear evolution of the role of H2 in energy systems as the costs of H2 infrastructure decline with increased adoption or technology innovation. This paper develops a high-ﬁdelity electricity-H2 (e-H2) capacity planning model to study the role of H2 in low-carbon energy systems, the sector-coupling effects, and the trade-offs between various technology options across the entire bulk supply chain* of both energy carriers. For a pre-deﬁned set of electricity and H2 demand scenarios, the model determines the least-cost technology mix across the power and H2 sectors while adhering to operational constraints of the power and H2 supply chains at an hourly resolution along with the spatiotemporal variations in VRE supply and energy demands. Applying the e-H2 model to the U.S. Northeast energy system for a range of CO2 price (up to $1000/tonne CO2), H2 demand and technology cost scenarios, we ﬁnd electrolytic H2 supply to be cost-effective under moderate carbon policy ($50/tonne or greater) and/or electrolyzer capital cost of $500/kW or lower. The interactions between the power and H2 supply chains increase investments in VRE generation and reduce investments in dispatchable resources like battery storage and natural gas generation in the power sector, which results in reducing the total system cost by up to 14% in the most carbon constrained scenarios analyzed here. Notably, as opposed to most literature that emphasizes the role for H2 in the power sector as grid-scale energy storage, i.e. P2G2P6,6–9,12,13, we ﬁnd a greater role for H2 to serve as a ﬂexible demand response resource based on the use of electrolysis in conjunction with H2 storage. This ﬁnding, stemming from the reduced capital cost and energy efﬁciency of the P2G path, is found to be valid across a range of CO2 price scenarios as well as capital cost assumptions for electrolysis and G2P generation. Finally, we ﬁnd that the role for natural gas in a future energy system is predominantly in the H2 supply chain and this role remains robust to increasing CO2 prices because of the relative cost-competitiveness of CCS-equipped natural gas based H2 production vs. electrolytic H2 production.
Demand forecasting techniques for build-to-order lean manufacturing supply chains<|sep|>Supply chain management (SCM) represents the managerial backbone of the logistics and production sector. Due to its relevance, new methodologies appear regularly in the literature. One of them is Build-To-Order Supply Chain Management (BTO-SCM). This technique has seen signiﬁcant adoption. Motivated by the lack of work in the demand forecasting literature on BTO-SCM problems, this research develops and presents Diagonal Feeding, a data transformation technique, specially tailored for this setting together with a relevant and novel data set from an electronics manufacturer. Data set and implementations of all methods are available for download 3. An accurate demand forecasting is essential for the global economy with stockpiled or in-transit inventories representing 17% of the world’s Gross Domestic Product (GDP), [8]. Yet, imprecise demand planning is still pervasive
Deeply Exploit Depth Information for Object Detection<|sep|>The RGB-D images have been provided in the real-world visual analysis systems thanks to the wide availability of affordable RGB-D sensors, e.g. the Microsoft Kinect. Compared with the primitive RGB, the RGB-D can bring remarkable performanceimprovement for various visual tasks due to the access to the depth information complementary to RGB [28, 11, 20]. Actually, the depth has some profitable attributes for visual analysis, e.g. being invariant to lighting or color variations, and providing geometrical cues for image structures [29]. For object detection, which is one of typical complex visual tasks, the acquisition of RGB-D images is applicable and beneﬁcial. However, how to effectively utilize the provided depth information of RGB-D Figure 1. Illustration of learning rich features for RGB-D object detection. Various property maps are derived to describe the object from different perspectives. The features for these maps are learned independently and then fused for the ﬁnal classiﬁcation. Speciﬁcally, the derived maps include geometry contour from the color/depth pairs, and horizontal disparity, height above ground, angle with gravity from the depth data. These maps, as well as the RGB image, are sent into different CNNs for feature learning. And the features are joint before being fed into the classiﬁer. images is still an open question. In recent years, Convolutional Neural Network(CNN) has achieved great success in computer vision and obtained the best performance in various visual tasks [30, 17, 23]. CNN is generally considered as an end-to-end feature extractor to automatically learn discriminative features from millions of input images [22]. In this paper, we also adopt CNN to extract rich features from the RGB-D images, i.e. we are under the CNN model to investigate the exploitation of the depth information. For the RGB-D object detection with CNN, the key is how to elegantly coordinate the RGB with depth information in feature learning. In the previous literatures, some intuitive methods have been proposed [3, 16]. Roughly, we can divide them into two broad categories according to the strategy the depth is treated. The ﬁrst one is to straightforwardly add the depth map to CNN as the fourth channel along with the RGB [3]. That is, the depth is processed in the same way as the RGB, and they are together convolved for granted. However, it makes no semantic sense to directly merge the depth and color maps, since they contain disparate information. The second is to process the color and depth separately, and they are combined before being fed into the ﬁnal classiﬁer, where the extracted features are joint. Speciﬁcally, two independent CNN networks are learned: one for RGB and one for depth [16]. As for the depth network, the input can be the original depth data or encoded data from the depth, e.g. height above ground, and angle with gravity [16]. It has been empirically shown that the second way usually outperforms the ﬁrst one. In this paper, we further investigate how to deeply exploit the depth information with aims of boosting the detection performance. Before introducing our proposed method, we review the primary mechanism of human visual systems. First, multiple visual properties are always used together to describe one object when people try to recognize it, e.g. geometry contour, color, and contrast [6]. And it is usually thought that exploiting more properties is much helpful. Second, the primary visual cortex (V1), which consists of six functionally distinct layers and is highly specialized in pattern recognition [12], abstracts different visual properties independently in the low layers and integrates in the relatively high layers. Inspired by the working mechanism of V1 area, we propose a novel method to deeply exploit the depth information for object detection. Figure. 1 illustrates the main ideas of our method. Firstly, various visual property maps are derived through analyzing the provided color and depth pairs. It is believed that more properties can contribute to the accurate description of the object and thus help boost the detection performance. Speciﬁcally, the derived properties include the contour, height, and angle maps1. Secondly, we systematically investigate the method to fuse different visual properties under the CNN model, i.e. how to represent a property, and from which layer the properties need to be fused together. The result of our analysis shows that the multiple properties should have complete and independent semantics in accordance with the human cognition, e.g. RGB channels should be treated as a whole to represent the color property rather than separate them from each other, and it is better to fuse the different properties after they have been explicitly transformed into the high-level features. We evaluate the proposed method on the challenging dataset NYUD2, and the experimental results show that our method outperforms all the baselines and achieves state-ofthe-art performance. 1Indeed, other properties can be also adopted, which may be obtained by speciﬁc sensors or more advanced derivation methods. Considering the simplicity, only several directly computable properties are employed here.
Fast B-spline Curve Fitting by L-BFGS<|sep|>Curve ﬁtting is a fundamental problem in many ﬁelds, such as computer graphics, image processing, shape modeling and data mining. Depending on applications, diﬀerent types of curves such as parametric curves, implicit curves and subdivision curves are used for ﬁtting. In this paper, we study the problem of ﬁtting planar B-spline curves to unorganized data points. Given a set of unorganized data points {Xi}N i=1 ⊂ R2 sampled from the outline of a planar shape, the aim of curve ﬁtting is to ﬁnd a B-spline curve P(t) = �n i=1 PiNi(t) that best approximates the shape’s outline. The outline is called a target shape, and the B-spline curve is called a ﬁtting curve. Here, P := {Pi}n i=1 ⊂ R2 is the set of B-spline control points, {Ni(t)}n i=1 are B-spline basis functions. We suppose that knots of the B-spline curve are ﬁxed and therefore not subject to optimization, and all the basis functions are thus deﬁned on ﬁxed, uniform spaced knots throughout the curve ﬁtting process. For a data point Xk, let P(tk) denote the nearest point of Xk on the ﬁtting curve. Then, the distance between data point Xk and the ﬁtting curve is ∥P(tk)−Xk∥. Here, tk is called the location parameter of Xk, P(tk) is called the foot point corresponding to Xk. Denote T = {t1, ..., tk}, i.e. the collection of the location parameters of all the data points. The ﬁtting problem is then formulated as: Since the objective function in Eqn. 1 is nonlinear, it is natural to apply iterative minimization methods to solve it. Most prevailing methods for solving this problem in CAGD are not standard optimization methods in the sense that they separately optimize location parameters T and control points P, making the problem much simpler to handle. However, these methods are time-consuming because they need to compute foot points on the ﬁtting curve and to formulate and solve linear systems in every iteration. We observe that these time-consuming operations can be avoided by employing a L-BFGS optimization method that solves T and P simultaneously. We show that the resulting algorithm is very eﬃcient because in every iteration it does not need to perform foot point projection or solve a linear system of equations. The remainder of this paper is organized as follows. In section 2, we review some previous work. Section 3 introduces the standard L-BFGS optimization method. Section 4 presents our new algorithm. Section 5 shows experimental results and comparisons with existing methods. Then we conclude the paper in Section 6 with discussions of future work.
ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data<|sep|>Much of the success of Big data today comes from predictive or descriptive analytics: statistical models or data mining algorithms applied to data to predict new or future observations, e.g., we observe how users click on ads, then build a model and predict how future users will click. Predictive analysis/modeling is central to many scientiﬁc ﬁelds, such as bioinformatics and natural language processing, in other ﬁelds - such as social economics, psychology, education and environmental science - researchers are focused on testing and evaluating causal hypotheses. While the distinction between causal and predictive analysis has been recognized, the conﬂation between the two is common. Causal inference has been studied extensively in statistics and computer science [9,30,13,24,25]. Many tools perform causal inference using statistical software such as SAS, SPSS, or R project. However, these toolkits do not scale to large datasets. Furthermore, in many of the most interesting Big Data settings, the data is highly relational (e.g, social networks, biological networks, sensor networks and more) and likely to pour into SQL systems. There is a rich ecosystem of tools and organizational requirements that encourage this. Transferring data from DBMS to statistical softwares or connecting these softwares to DBMS can be error prone, diﬃcult, time consuming and ineﬃcient. For these cases, it would be helpful to push statistical methods for causal inference into the DBMS. Both predictive and causal analysis are needed to generate and test theories, policy and decision making and to evaluate hypotheses, yet each plays a diﬀerent role in doing so. In fact, performing predictive analysis to address questions that are causal in nature could lead to a ﬂood of false discovery claims. In many cases, researchers who want to discover causality from data analysis settle for predictive analysis either because they think it is causal or lack of available alternatives. This work introduces ZaliQL,1 a SQL-based framework for drawing causal inference that circumvents the scalability issue with the existing tools. ZaliQL supports state-of-the-art methods for causal inference and runs at scale within a database engine. We show how to express the existing advanced causal inference methods in SQL, and develop a series of optimization techniques allowing our system to scale to billions of records. We evaluate our system on a real dataset. Before describing the contributions of this paper, we illustrate causal inference on the following real example. Example 1. FlightDelay. Flight delays pose a serious and widespread problem in the United States and signiﬁcantly strain on the national air travel system, costing society many billions of dollars each year [3]. According to FAA statistics,2 weather causes approximately 70% of the delays in the US National Airspace System (NAS). The upsetting impact of weather conditions on aviation is well known, however quantifying the causal impact of diﬀerent weather types on ﬂight delays at diﬀerent airports is essential for evaluating approaches to reduce these delays. Even though predictive analysis, in this context, might help make certain policies, this problem is causal. We conduct this causal analysis as a running example through this paper. To this end, we acquired ﬂight departure 1 The preﬁx Zali refers to al-Ghzali (1058-1111), a medieval Persian philosopher. It is known that David Hume (1711-1776), a Scottish philosopher, who gave the ﬁrst explicit deﬁnition of causation in terms of counterfactuals, was heavily inﬂuenced by al-Ghzali’s conception of causality [36]. 2 National Aviation Statistic http://www.faa.gov/ details for all commercial ﬂights within the US from 2000 to 2015 (105M entries) and integrated it with the relevant historical weather data (35M entries) (see Section 5.1). These are relatively large data sets for causal inference that can not be handseled by the existing tools. Table 1 presents the list of attributes from each data set that is relevant to our analysis. When we make predictive analysis, whether we predict E[Y |X = x] or Pr(Y |X = x) or something more complicated, we essentially want to know the conditional distribution of Y given X. On the other hand, when we make a causal analysis, we want to understand the distribution of Y , if the usual mechanisms controlling X were intervened and set to x. In other words, in causal analysis we are interested in interventional conditional distribution, e.g., the distribution obtained by (hypothetically) enforcing X = x uniformly over the population. In causal analysis, the diﬃculty arises from the fact that here the objective is to estimate (unobserved) counterfactuals from the (observed) factual premises. Example 2. FlightDelay (Cont.). Suppose we want to explore the eﬀect of low-pressure on ﬂight departure delays. High pressure is generally associated with clear weather, while low-pressure is associated with unsettled weather, e.g., cloudy, rainy, or snowy weather. Therefore, conducting any sort of predictive analysis identiﬁes low-pressure as a predictor for ﬂight delays. However, lowpressure does not have any causal impact on departure delay (low-pressure only requires longer takeoﬀ distance) [39]. That is, low-pressure is most highly a correlated attribute with ﬂight delays, however ZaliQL found that other attributes such as thunder, low-visibility, high-wind-speed and snow have the largest causal eﬀect on ﬂight delays (see Sec. 5.2); this is conﬁrmed by the results reported by the FAA and [1]. This paper describes novel techniques implementing and optimizing state-ofthe-art causal inference methods (reviewed in Section 2) in relational databases. We make three contributions. First, in Section 3 we describe the basic relational implementation of the main causal inference methods: matching and subclassiﬁcation. Second, in Section 4 we describe a suite of optimization techniques for subclassﬁcation, both in the online and oﬄine setting. Finally, in Section 5 we conduct an extensive empirical evaluation of ZaliQL, our system that implements these techniques, on real data from the U.S. DOT and Weather Underground [6,38].
PGPE theory of finite temperature collective modes for a trapped Bose gas<|sep|>The response of a manybody system to an external perturbation, particularly its collective mode response, forms an important method of analysis in condensed matter physics. Since the experimental realisation of dilute gas Bose-Einstein condensation (BEC) there have been several collective mode experiments [1, 2, 3, 4, 5], in which perturbations of the conﬁning potential were used to excite the system. Of particular interest is the 1997 experiment of Jin et al. at JILA [2], which determined the excitation frequencies for the lowest energy quadrupolar collective modes over a temperature range spanning the condensation transition. At low temperatures, where the system was mainly condensate, the results were accurately described by simple meanﬁeld theory [6, 7, 8]. However, the behavior of the collective mode frequencies at higher temperature, where a signiﬁcant thermal fraction was present, proved much more difﬁcult to describe. Indeed, the description of these experiments has become the de facto standard for testing ﬁnite temperature quantum ﬁeld theories of BEC, and has been largely responsible for the development of gapless [9] and second order [10] theories of the trapped Bose gas. To date, the only fully quantitative theoretical descriptions of these results have been provided by the Zaremba-NikuniGrifﬁn (ZNG) formalism calculations of Jackson et al. [11] in 2002 and the second order theory of Morgan et al. [12] in 2003. In Fig. 1(a) and (b) we show a schematic representation of the quadupolar modes excited in the JILA experiment, characterized by the projection, m, of their angular momentum onto the z axis. A large body of theoretical work has been conducted on the subject of the JILA experiments [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23] and is nicely summarized in a recent review by Proukakis and Jackson [24]. The temperature dependence of the m = 2 mode was accounted for by gapless Hartree-Fock Bogoliubov (HFB) theory calculations in 1998 [13], which included anomalous average and manybody effects in the system description (also see Refs. [25, 26, 27, 28]). However, gapless HFB failed to account for the rather sudden upward shift in the m = 0 mode frequency observed in experiments at T ≈ 0.65Tc. An explanation for the unexplained behavior of the m = 0 mode was ﬁrst provided by Stoof and coworkers [19, 29] (also see [30]), who suggested that it arose from Figure 1: Schematic view of the quadrupolar modes of an oblate condensate. (a) The m = 0 excitation where the axial and radial widths of the condensate oscillate out of phase and (b) the m = 2 excitations and the widths of the condensate in the x and y directions oscillate out of phase. the coupling of in-phase and out-of-phase oscillations of the condensate and thermal cloud. This hypothesis suggested that an adequate theoretical description would require a dynamic treatment of both the condensate and noncondensate parts of the system. The ﬁrst such formalism was the ZNG ﬁnite temperature theory [31, 32, 33] in which the system description takes the form of a Gross-Pitaevksii equation for the condensate, coupled to a Boltzmann equation for the noncondensate. Jackson and Zaremba [11] applied the ZNG theory to model the JILA experiment and found relatively good agreement with the experimental results. The following year, Morgan et al. [12] reported the results of a second order theory that were also in good agreement with the experimental results. That theory, the culmination of seven years of work by Burnett, Hutchinson, Morgan, Proukakis and coworkers [9, 10, 12, 13, 14, 15, 16, 17, 18, 34], consistently included the dynamical interactions between the condensate and noncondensate atoms. In this paper we develop the projected Gross-Pitaevskii equation (PGPE) formalism to model the experiment of Jin et al. [2]. The PGPE method is a c-ﬁeld technique [35] applicable to the study of ﬁnite temperature degenerate Bose gases. It includes interactions between low energy modes of the gas non-perturbatively and is applicable in the critical region, e.g. see [36, 37, 38, 39]. Indeed, PGPE predictions for the shifts in critical temperature [40] are in good agreement with experi Figure 2: Schematic view of the c-ﬁeld and the incoherent regions for a Bose gas in a harmonic trap potential, and the approximations we employ in our treatment of the collective mode dynamics. mental measurements [41]. While this formalism has successfully predicted equilibrium properties for a degenerate Bose cloud, there have been no quantitative comparisons to dynamical experiments, so our comparison to the experiments of Jin et al. [2] forms an important test of this theory. A central feature of the PGPE approach is the formal division of the system modes into the classical region C [42], which is simulated using the PGPE, and an incoherent region I, for which a classical ﬁeld treatment is inappropriate (see Fig. 2). The C region dynamics are accounted for in the PGPE description, and the I region dynamics could be treated using, e.g., a Boltzmann description. As previous theoretical work has has shown, the full dynamical treatment of the noncondensate is crucially important in providing a correct description of the JILA experiments. However, a full dynamical treatment of the I region is a rather complex addition to the theory that we do not consider here. Instead, we simply ignore the dynamics of the I region, with the justiﬁcation that many of the noncondensate modes exist in the C region and their dynamical effect is included in the PGPE. We critically analyze this approximation by quantifying the dependence of equilibrium and dynamic properties on the energy cutoff, ϵcut, which sets the division between the C and I regions. The organisation of the paper is as follows. In Sec. II we review the PGPE formalism for equilibrium properties of a trapped Bose gas, before outlining our extensions to the theory to model the JILA collective mode experiments. The results of our calculations for the equilibrium states, collective mode frequencies, and damping rates are presented in Sec. III. In that section we also consider the relative phase between the condensate and noncondensate modes, and the cutoff sensitivity of our predictions, before we conclude in Sec. IV. The data used to prepare our initial states is summarized in Appendix A.
Tale of tails using rule augmented sequence labeling for event extraction<|sep|>Event occurrences involve several entities such as time, date, place, reason, etc. Event extraction recovers structured representations from the text, often characterised by complex argument and nested events, involving several entities. Entities have associated properties and attributes such as reason of the happening, after-effects of the events, etc. Event extraction has several applications in tasks that include text summarization, knowledge-base construction, machine translation, etc. Entity extraction is a pre-requisite for building any event extractor. Additionally, event extraction (EE) often requires the discovery of relations between entities and dependencies between relations from the text. Typically, EE systems ﬁnd triggers and their associated arguments. The discovery of triggers and arguments poses several challenges. Firstly, event detection is highly contextual driven. The same event may appear with different trigger ‘On 29 December 2017 a massive ﬁre broke in Kamala Mills, Mumbai the capital of Maharashtra, killed at least 14 people and injured several’. The trigger word killed may co-occur with ﬁre accident or attack or any other keyword associated with an incident involving such a trigger word. Similarly, the trigger word killed can evoke different events depending upon the context. Secondly, the presence of a large number of entities requires large annotated data focused on deep learning based systems. Typical deep learning systems require a sufﬁciently large amount of data for training (Sun et al., 2017). Named entity recognition (NER) systems are not effective for extracting a large number of tags given less data. Third, of particular concerns are the labels with very few instances in the dataset, hereafter referred to as ‘tail labels’. Most event extraction methods are not designed to be fair to tail labels and land up being biased by the more frequent labels. Additionally, the absence of embeddings that capture language models effectively is especially pronounced for low resource languages. This can lead to propagation of errors resulting from the improper initialisation of embeddings for out-ofvocabulary words. In this paper, we address these challenges to create an end-to-end EE system in the disaster domain in 5 languages, namely, Marathi, English, Hindi, Bengali and Tamil. We introduce rule augmented deep learning methods and demonstrate the effectiveness of our approach with extensive experiments. We model EE as a sequence labelling task and investigate ways of enhancing state-of-the-art entity extractors by augmenting using rule-based approaches. Our contributions are summarised as follows : • We re-purpose existing rule augmented deep learning models for learning event structures that captures event arguments and their interdependencies on disaster domain in lowresource languages. • We conduct extensive experiments on our dataset and demonstrate the importance of rule-augmented deep learning models in improving performance on tail labels. • We release a new dataset named InDEE2019 that consists of tagged event extraction data in the disaster domain covering ﬁve languages: Marathi, English, Hindi, Bengali and Tamil.
Nonequilibrium Green's function theory for predicting device-to-device variability<|sep|>A very important yet difﬁcult issue of electronic device physics is to be able to predict ﬂuctuations in quantum transport properties due to atomic disorder1,2. In existing and emerging ﬁeld-effect transistors with a channel length of ∼ 10 nm or so, a serious source of property unpredictability is the random dopant ﬂuctuation (RDF). RDF comes from the particular microscopic arrangement of the small number of dopant atoms inside the device channel. Experimentally it is extremely difﬁcult – if not absolutely impossible, to control the precise location of each dopant atom, therefore transport properties vary from one device to another. It was even pointed out that nanowire transistors can suffer from RDF in the source/drain extension region even if the channel is dopant free3,4. The device-to-device variability is in fact a general phenomenon for device structures in the nano-meter scale which compromises device performance and circuit functionality. From the theoretical point of view, incorporating disorder and randomness in nano-electronics modeling is of great importance1,5. In particular, one is interested in predicting not only the average value of the transport property (e.g. conductance) but also the variance of it. The device-to-device variability has so far been investigated by statistical analysis of large number of simulations. For instance Reil et al carried out classical drift-diffusion simulations for an ensemble of 105 dopant conﬁgurations under the combined inﬂuence of RDF and line edge roughness6. Martinez et al did effective-mass nonequilibrium Green’s function (NEGF) simulations of an ensemble of 30 dopant conﬁgurations to analyze statistical variability of quantum transport in gate-all-around silicon nanowires7. The contrast of the size of the statistical ensemble clearly shows the difﬁculty of quantum simulations. The difﬁculty in brute force computation becomes much more severe in full self-consistent atomistic modeling (as opposed to effective-mass modeling) such as the NEGF based density functional theory (DFT)8. There is an urgent need to develop viable theoretical methods that does not rely on brute force computation for predicting the device to-device variability. It is the purpose of this work to present such a formalism. We shall report a new theoretical approach to directly calculate statistical variations of quantum transport due to RDF without individually computing each and every impurity conﬁguration by brute force. Our theory is composed of two formalisms: one is general but more complicated and the other is specialized but much simpler. The two formalisms are based on the Green’s function technique and the multiple scattering theory. The ﬁrst formalism builds on coherent potential approximation (CPA) and can be applied to a wide range of impurity concentrations. The second formalism builds on the low concentration approximation (LCA) and is extremely useful for situations involving low impurity concentration which is often the case for realistic semiconductor devices. Our theory and implementation have been checked by both analytical and numerical veriﬁcation. The basic physical model of a two-probe quantum coherent nanoelectronic device is schematically shown in any one of the sub-ﬁgures of Fig.1, which consists of a central channel region sandwiched by the left and right semi-inﬁnite electrodes9. The electrodes extend to reservoirs at z = ±∞ where bias voltages are applied and electric current measured. We assume that the RDF occurs in the channel region of the system and each sub-ﬁgure in Fig.1 represents one dopant conﬁguration. Clearly, due to different locations of the dopant atoms, every device exhibits slightly different transport behavior leading to the device-to-device variability. The transport current ﬂowing through the device can be expressed9 in terms of the transmission coefﬁcient T (hereafter atomic units are assumed, e = ℏ = 1), where E is the electron energy, fL (E) and fR (E) are the Fermi functions of the left/right electrodes. Without RDF, the electric current I is a deﬁnite number for a given bias voltage. In the presence of RDF, I depends on the particular impurity conﬁguration thus varies from one conﬁguration to another. FIG. 1: Ensemble of two-probe devices with various disorder conﬁgurations. In each sub-ﬁgure, the left and right electrodes extend to z = ±∞, respectively. The black dots are pure sites in the electrodes, the empty circles are pure sites in the central channel region, and the crossed empty circles are disorder sites (dopant or impurity) in the channel region. By calculating a large ensemble of conﬁgurations one can determine an average current and its associated variance δI. For our device model where RDF occurs inside the channel region, δI is obtained in term of transmission ﬂuctuation, δT, as follows where · · · refers to averaging over the disorder conﬁgurations. Notice that the transmission coefﬁcient T can be expressed in terms of Green’s functions. As a result the calculation of T involves evaluating a 2-Green’s function correlator G · G. The calculation of transmission ﬂuctuation δT (E) which needs the quantity T 2 involve a 4-Green’s function correlator G · G · G · G. In the literature, a well known technique called coherent potential approximation (CPA)10,11 is available to evaluate disorder average of a single Green’s function G. The CPA technique was generalized12,13 to evaluate 2-Green’s function correlators G · G and 3-Green’s function correlators G · G · G (albeit in other contexts). More recently, the generalized CPA technique for calculating the 2-Green’s function correlator has been applied to study transmission14 and nonequilibrium quantum transport15 in disordered systems. This work will address how to evaluate 4-Green’s function correlator and apply the technique to study device variability. In a very recent manuscript16, Zhuang and Wang carried out an analysis of conductance ﬂuctuation and shot noise in graphene by using a direct expansion approach. To some extent, their approach is complementary to the methods presented in this work with respect to accuracy and efﬁciency. Finally, there are large bodies of literature in mesoscopic physics to analyze such issues as the universal conductance ﬂuctuation in bulk systems using the Kubo formula and δ-like short range impurity potentials17. In contrast, the goal of this work is to formulate a theoretical approach for calculating the transmission ﬂuctuation of two-probe systems where the disorder scattering is due to impurity atoms as opposed to δ-like models. The rest of the paper is organized as follows. Section II reviews the multiple scattering theory of the t-matrix formalism. Section III presents the ﬁrst formalism, i.e. the CPA diagrammatic technique for calculating transmission ﬂuctuation. Section IV presents the second formalism, i.e. the LCA perturbative expansion technique for calculating transmission ﬂuctuation. Section V discusses a special but important situation where the device structure is periodic in transverse dimensions. Section VI presents some miscellaneous technical issues of the theory. Section VII presents three examples as applications of the CPA and LCA formalisms. Finally, the paper is concluded with a brief summary in Section VIII. Some technical details are enclosed in the two appendices.
Behaviour of light transmission channels in random media with inhomogeneous disorder<|sep|>Coherent transport of waves in random media at the the mesoscopic scale showing extraordinary characteristics has attracted much attention in recent decades. Wave interference during the multiple scattering leads to many amazing physical phenomena, such as Anderson localization, enhanced backscattering and universal conductance ﬂuctuations [1–5]. In the theoretical framework of quantum transport, an incident wave (outgoing wave) can be decomposed into several transport channels, which correspond to the quantized directions in which the wave enters (exits) the random medium. The transmission behaviour of the channels is governed by the N × N ﬁeld transmission matrix t, where N is the number of the channels. With t one can obtain the transmitted intensity, the total transmission for diﬀerent incident channels as well as the transmittance. From the statistics of these quantities one can extract rich information about the transmitted wave [6]. Eigenvalues {τn} of the Hermitian matrix t†t together with the corresponding eigenchannels can also be extracted from t [7–9], and researches on the so-called “open channels” with τn ≃ 1 has realized focusing and imaging of light through turbid media [10–15]. The transmission matrix t is determined by the conﬁguration of disorder when the dimensions of the sample are ﬁxed, and previous researches mostly focused on homogeneous disorder. However, inhomogeneous disorder exists widely in natural and artiﬁcial materials. The inhomogeneity may results from multilayer conﬁgurations or inhomogeneous doping, which are common in real experiments and can not be eliminated by ensemble averaging. The additional degree of freedom introduced by the inhomogeneity of disorder will cause diﬃculties for theoretical investigations on transport properties and the behaviour of channels in such random media. Thus two fundamental questions are proposed. The ﬁrst question is that, is there any equivalent treating method, which is analogous to the eﬀective medium theory, and can be applied to deal with the inhomogeneity? And the subsequent question is that, if there is no such method, then how can we consider the inﬂuence of such inhomogeneity? In this work, we investigate the inﬂuence of inhomogeneous disorder by numerical simulations and try to provide basic comprehension for the questions proposed above.
Determining the outcome of cosmic bubble collisions in full General Relativity<|sep|>Accelerated expansion of the universe and spontaneous symmetry breaking, two powerful ideas in modern physics, conspire in many theories to give rise to a phenomenon known as eternal inﬂation. This phenomenon occurs when a region of the universe is in a metastable vacuum with positive energy density. Classically, these vacua are stable, and drive inﬂation (exponential expansion). Quantum mechanically, they can decay via the nucleation of expanding bubbles [1, 2] containing a new phase. Unless the rate of bubble formation outpaces the expansion of the inﬂating background, the original phase is never completely consumed, and inﬂation becomes eternal. In this picture, many diﬀerent phases can be seeded from an eternally inﬂating “parent” vacuum, leading to a patchwork-universe with diverse spacetime-dependent physical properties. For a review of eternal inﬂation, see e.g. Ref. [3]. Surprisingly, there may be directly observable consequences of living in an eternally-inﬂating multiverse. In eternal inﬂation, our observable universe resides inside one member of an ensemble of bubbles. These bubbles necessarily collide, perturbing the homogeneity and isotropy of our own bubble interior, and providing a direct observational test of eternal inﬂation [4]. Assessing the likelihood of seeing bubble collisions, determining their eﬀects on cosmology, and predicting their observational signatures has been the subject of a large body of work [5–22]. This body of literature has established the plausibility of models where collisions are likely, compatible with our observed cosmology, and leave observable signatures. However, an existence proof does not yet exist. For a review of much of this work, see Ref. [23]. The primordial inhomogeneities left by bubble collisions are imprinted in the cosmic microwave background (CMB) radiation. The signature of an individual collision is an azimuthally symmetric modulation of the CMB temperature in a localized region. In general, the theory predicts a set of observable collisions, with the properties of each drawn from an in-principle calculable probability distribution. A search for the signatures of bubble collisions in the Wilkinson Microwave Anisotropy Probe [24] (WMAP) 7-year CMB data [25] was performed in Refs. [26, 27], which yielded an upper bound on the average number of observable collision signatures on the CMB sky. A key element in the analysis of Refs. [26, 27] is the fact that each bubble collision is expected to leave a fairly generic set of signatures in the CMB [14, 15, 19, 26], described by a phenomenological template with only a few free parameters. However, there is currently no way to connect the parameters in this phenomenological model to the parameters in the fundamental theory. In order to go from the Lagrangian to the signatures of bubble collisions, one must determine the properties of the colliding bubbles, the immediate outcome of the collision, and the cosmological evolution of the perturbed bubble interior. Because of the non-linear nature of the ﬁeld equations, a full and unambiguous determination of the cosmological signatures of bubble collisions requires numerical relativity. See e.g. Ref. [28] for a modern review of this ﬁeld. In this paper, we present the results of fully-relativistic simulations of the collision between Coleman-de Luccia (CDL) bubbles containing a realistic inﬂationary cosmology. Although any given bubble undergoes many collisions, the future domains of inﬂuence of each of the collision events typically do not overlap until relatively late times inside the bubbles. It is therefore a good approximation to treat each collision as occurring in isolation, and we restrict ourselves to the study of collisions between two bubbles. The spacetime resulting from the collision between two CDL vacuum bubbles has SO(2,1) symmetry [7, 8], allowing us to treat the problem of bubble collisions in full generality using a 1+1D simulation. This symmetry is expected to be mildly broken by ﬂuctuations on the walls of the colliding bubbles [29–32]. However, expanding bubbles become more and more spherical (the perturbations remain much smaller than the overall size of the bubbles), and so our truncation to 1+1 dimensions should capture the most important dynamics. Einstein’s equations in 1+1D are fully constrained by the scalar ﬁeld conﬁguration, allowing for a straightforward and eﬃcient numerical implementation. The goal of the present paper is to study the outcome of bubble collisions and identify the properties that models must have in order to be consistent with our observed cosmology. This has been addressed to a certain extent in previous work using the Israel Junction Condition formalism [7, 8, 10, 11, 13, 22], as well as in numerical simulations that neglected gravitational eﬀects [12, 17, 18]. In addition, Ref. [33] studied collisions in full numerical relativity for the collision of vacuum bubbles (see also Ref. [34] for a relativistic treatment of single bubbles). Our fully relativistic treatment allows us to go beyond these previous studies and completely determine the phenomenology of bubble collisions given a particular model of the scalar ﬁeld theory driving eternal inﬂation. In addition, we determine how well the approximations used in previous work hold up when compared against the fully relativistic solutions. In a forthcoming paper, we will extend this numerical framework to extract the observational signatures of bubble collisions. The remainder of the paper is organized as follows. We begin in Sec. II by reviewing conclusions obtained from the Israel Junction Condition formalism and summarizing the set of questions we attempt to address with our simulations. In Sec. III, we perform the truncation to the 1+1D system of equations to be solved numerically, and discuss the generation of initial data. We then brieﬂy outline the numerical implementation and tests of the code in Sec. V, and present the results of our simulations in Sec. VI. More detailed treatments of a few technical aspects of the setup are presented in a set of appendices. We work in units where MPl = 1 unless otherwise noted; Newton’s constant is given by GN = M −2 Pl .
Multi-granularity Item-based Contrastive Recommendation<|sep|>Real-world recommender systems aim to provide items according to user preferences from million-level item candidates [6]. It is challenging to directly conduct complicated ranking algorithms that involve user-item interactions on all items, for even the linear complexity with million-level corpus size is unacceptable [29,18]. Hence, practical recommender systems usually adopt the classical two-stage architecture [3] to balance eﬀectiveness and eﬃciency, which consists of both ranking and matching modules. The matching module [21] (i.e., candidate generation [3]) tries to retrieve a small subset of (usually hundreds of) item candidates from the million-level large corpora. In contrast, the ranking module focuses on the speciﬁc ranks of top items retrieved by matching for the ﬁnal display.
Polaritonic Rabi and Josephson Oscillations<|sep|>A superconductor can be described by an order parameter, that reduces in the simplest formulation the dynamics of such a complex object to a simple complex number [1]. The question of what happens with the phases of two superconductors put in contact through an insulating barrier led Josephson to predict in 1962 with elementary equations that a supercurrent should ﬂow between them, driven by their phase diﬀerence [2]. The phenomenon was quickly observed [3] and became emblematic of broken symmetries and quantum eﬀects at the macroscopic scale. It was soon speculated that a similar phenomenology should be observed with other macroscopically degenerated quantum phases, such as superﬂuids or Bose– Einstein condensates, even before the latter were experimentally realized [4]. The role of the phase as the driving agent of quantum ﬂuids was brought to the fore by Anderson [5] who identiﬁed “phase slippage” as a source of dissipation [6]. Notably, in the case of BECs, the ﬁrst transposition of this physics was considering noninteracting particles [4] and the role of the phase diﬀerence as a drive for the superﬂow was the focus of attention. The question of the phase of macroscopically degenerate quantum states remained anchored in the phenomenon but also took a separate route of its own [7–9], that is still actively investigated to this day [10, 11]. The Josephson eﬀect itself, on the other hand, was put on its theoretical foothold by Leggett who deﬁnes it as the dynamics of N bosons “restricted to occupy the same two-dimensional single particle Hilbert space” [12]. Leggett introduced three regimes for such systems depending on the relationship between tunelling and interactions, namely the Rabi (non-interacting), Josephson (weakly-interacting) and Fock (strongly-interacting) regimes [13]. “Tunneling” refers to linear coupling between the condensates (quadratic in operators) while “interactions” refer to a nonlinear self-particle quartic term. In this sense, Josephson’s physics is a limiting case of the Bose–Hubbard model [14], although the name retained a strong bond with superconductors [15], possibly due to the important applications it found as a quantum interference device [16, 17] or merely for historical reasons (the Josephson–Bardeen debate on the existence of the eﬀect is one highlight of scientiﬁc controversies [18]). To mark this diﬀerence, one speaks of “Bosonic Josephson junctions” (BJJ) for bosonic implementations of the Josephson dynamics [19]. This typically relates to condensates trapped in two wells, but due to its fundamental and universal character as formulated by Leggett, numerous platforms exhibit the eﬀect. A pioneering report came from superﬂuids [20]. For proper BECs, a so-called “internal” Josephson eﬀect was deemed “more promising” with alkali gases by involving diﬀerent hyperﬁne Zeeman states rather than a straightforward coupling between two spatially separated condensates [13]. Eventually, the Josephson oscillation was observed in a single junction of BEC [21]. In this text, we consider another platform that can host Bose condensates: microcavity polaritons [22]. These systems having demonstrated Bose–Einstein condensation [23] and superﬂuid behaviour [24], are natural candidates to implement the Josephson physics of coupled condensates—furthermore, in strongly out-ofequilibrium open systems—and several theoretical pro
Multivariate Chebyshev Inequality with Estimated Mean and Variance<|sep|>The Chebyshev inequality (1867) is a fundamental result from probability theory and has been studied extensively for more than a century in a wide range of sciences. The most common version of this result asserts that the probability that a scalar random variable ξ with distribution P diﬀers from its mean µ ∈ R by more than λ ∈ R>0 standard deviations σ ∈ R>0 satisﬁes the relation Recent works by Chen (2011) and Navarro (2013) provide a closed form extension of (1) to the multivariate case where the conﬁdence intervals are ellipsoids centered at the population mean. Moreover, Navarro (2014b) shows that the derived extension is tight. Another extension of (1) to more general ellipsoidal and polyhedral sets has been described in (Vandenberghe et al., 2007) where a multivariate version of the Chebyshev bound is computed as a solution to a semideﬁnite program (SDP) (Vandenberghe and Boyd, 1996). Although these results provide means to explicitly compute distribution-free probability bounds based only on the ﬁrst two moments of P, they are of limited practical value since one often does not know µ and σ exactly. In practice, a common approach is to compute empirical estimates of µ and σ via sampling and to substitute these estimated values into (1), although it can be shown that this method can lead to unreliable results in the event of poor estimates of the moments. There is an extensive literature about empirical processes where the quality of the estimates is investigated, see e.g. Dudley (1978) and van de Geer (2010). However, these approaches suﬀer from two main problems. They either assume that the underlying distribution P has bounded support (e.g. Hoeﬀding’s inequality), or they provide asymptotic results on the convergence rate that are valid only as the sample size tends to inﬁnity. Unfortunately, neither of these two cases turns out to be helpful when we make no assumption on the support of the distribution and the number of samples is limited. In the univariate case, Saw et al. (1984) approached the problem of formulating an empirical Chebyshev inequality from a diﬀerent direction. Given N i.i.d samples ξ(i), . . . , ξ(N) ∈ R from an unknown distribution P, and their empirical mean µN and empirical standard deviation (SD) σN, Saw derives a Chebyshev inequality with respect to the (N + 1)th sample. The bound derived is remarkably simple and requires only a modiﬁcation of the right-hand side of the theoretical bound in (1), i.e. PN+1 ���ξ(N+1) − µN �� ≥ λσN � Currently, there exists no counterpart of (2) for the multivariate case. There have been only limited eﬀorts to extend these results to multiple dimensions by making strong assumptions on the population. In (Navarro, 2014a) the author derives a multivariate equivalent by assuming that the true distribution of the population is the empirical distribution over a given data set. In this paper we derive a multivariate version of the inequality in (2) using the Euclidean norm, without requiring any further assumptions on the distrubution. In addition, we show that the result converges to the multivariate Chebyshev inequality as computed in (Vandenberghe et al., 2007) for an ellipsoidal set centered at the mean.
Iteration Complexity of Variational Quantum Algorithms<|sep|>There has been a great deal of recent interest in near-term applications of quantum computers. Variational quantum algorithms [38, 62, 11] are the most popular approach by a large margin, as evidenced by the quantity of papers and citations thereof studying their performance on a wide range of practical and theoretical problems. Recent studies show that they can potentially outperform existing approaches in a number of important tasks, such as combinatorial optimization [19], quantum chemistry [5, 36], particle physics [28, 8] and many others. In variational quantum algorithms (VQAs), a parametric quantum circuit is run and “evaluated”, or endowed with measurement that collapses the state to some noisy eigenvalue of an observable operator. This measurement is in turn used to classically guide the estimate of the parameters, which are adjusted according to the evaluation, and then used to prepare the (updated) parametric quantum circuit again. Thus, it is a scheme to simultaneously use classical and quantum computing in an alternating manner, developed as a means of utilizing the potential speedup properties of quantum computing despite the inability to obtain useful output from circuits beyond a relatively shallow depth in the so-called near-term devices [11]. The classical computation component can be viewed as an unconstrained optimization problem, wherein a function of the so-called variational parameters is minimized with respect to these variables. This objective function has certain speciﬁc properties, which we shall discuss in detail in the following. However, one important and immediate point is that the evaluations of the function (and its gradients and any higher-order information) are ultimately noisy, meaning that one evaluation with a certain set of parameters may yield a diﬀerent value than another evaluation with the same set of variational parameters. We can distinguish two classes of algorithms used to compute the steps of the classical optimization procedure in VQA. One popular option [27], inspired by automatic diﬀerentiation, evaluates closed-form expressions for the gradient [20, 22]. Alternatively, one can use “zero order” or “derivative free optimization” methods, which only make use of function evaluations rather than seeking to compute gradients. A popular choice is the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm, a classic method introduced by Spall et al. [51] to solve stochastic root-ﬁnding problems using noisy function evaluations to approximate a gradient. There it was found that a careful evaluation of the problem function at distinct points yielded a more reliable estimate of the gradient than ﬁnite diﬀerence methods, relative to the number of evaluations required, and it became a standard tool of zeroth-order stochastic approximation. More commonly in the optimization literature, an approach described (ambiguously, since SPSA also used two evaluations) as the “two-point function approximation” rule is the most extensively analyzed in the zero-order optimization community, so although it is not commonly used in quantum circuits at the moment, we will also analyze its performance. The third popular procedure, which is unique to the functional form of parametrized quantum circuits, is called the “parameter shift rule”. It can be understood as an intermediate step between zero’th- and ﬁrst-order methods. In particular, although by strict deﬁnition the procedure is zero-order in that it estimates a gradient using function evaluations, it is based on the original observation that for certain classes of gates and choices of perturbation, the approximation is exact. Based on select choices of the direction and magnitude of the perturbation around the base point, the parameter shift rule can be calculated to produce the exact gradient for certain circuits [41, 48]. The class of circuits for this applicability, however, is limited, especially when one considers the entire pipeline through an ultimate evaluation of an observable. Recent work has expanded the scope of procedures that appear equivalent to analytical expressions as well as the estimation accuracy of those that are not [4]. As a consequence, given that the parameter shift rule computes analytically exact in certain contexts, and approximations in many others, it is best considered as a method in between zero- and ﬁrst-order, and is best analyzed in a manner attuned to the problem at hand. In regards to claiming analytical results on these methods, while there are plenty of studies on the iteration complexity of classical optimization algorithms, their applicability to VQA is suspect due to the presence and nature of the errors arising in comptuing near-term circuits. That is: beyond the potential stochastic nature of a quantum circuit evaluation (i.e., for a number of problems, the theoretical output can be one of several eigenstates of an observable operator), the gates in near-term devices are far from implemented with perfect ﬁdelity. The errors or unintended operations in the quantum state [6] are diﬃcult to model. Common models consider rotation (dephasing) and dampening of an amplitude (dissipation). While there is active research in developing hardware and error-correcting codes to mitigate the scale and impact of these errors, for the time being, it is clear that such systematic errors in the evaluation of quantum circuits shall remain endemic for some time. Furthermore, even in principle, the currently proposed error mitigation strategies appear to have an exponential increase in hardware and computation relative to their increased accuracy [57], suggesting that even error mitigation would come at a cost that fundamentally limits the potential for quantum advantage. Whereas there is thorough theoretical understanding of the performance of many variants of SGD and zero-order optimization methods, the analysis is typically inspired by statistical and machine learning applications. In this case, the function being minimized is a loss function in the form of a large sum of discrepancies from actual to predicted estimates across a large sum of data samples. The stochasticity comes from the necessity of sampling one or multiple (minibatch) samples in evaluation of the function or gradient during the course of the optimization procedure, due to the frequently large sample size and unrealizability of storing an entire function or gradient in memory. A key feature of this class of problems is that a function or gradient evaluation can be performed in such a way that it is unbiased, i.e., while it is noisy, the statistical expectation of the evaluation is equal to a desired deterministic quantity. By contrast, as we shall see below, the errors that appear in the evaluation of quantum circuits in VQA tend to exhibit “bias”. To formalize this notion, consider an optimization problem of the form, min θ f(θ) := Eξ∼Ξ(θ)[F(θ, ξ)], (1) where θ ∈ Rp are the deterministic VQA parameters and ξ ∈ R is the stochastic sample. We consider that Ξ is the underlying distribution on which we want to minimize this ﬁrst moment quantity. However, in practice, we may not have access to unbiased samples of the quantity in (1), but instead each physical evaluation of F(θ, ·) satisﬁes a diﬀerence distribution, formally, where in general the state-dependent distributions over ξ are not the same nor do they have the same moments, i.e., Eξ∼Ξ(θ)[F(θ, ξ)] ̸= Eξ∼χ(θ)[F(θ, ξ)]. Thus, in this work the desired minimization is with respect to the distribution denoted by Ξ(θ), but only estimates associated with the distribution χ(θ) are available. We assume that there is a reasonable amount of resemblance in the sense that supp (∪θΞ(θ)) ⊆ supp (∪θχ(θ)). In the sequel, for parsimony we will drop the state-dependence of the distributions, i.e., write Ξ(θ) as just Ξ, observing that the notation limitation does not hinder the clarity of the presented results. In standard Stochastic Approximation (SA) Theory, one iteratively computes, i.e., forming some possibly approximate estimate of a sample of ∇F from a distribution, written formally as the exact gradient of the (hidden) function f plus an approximation error denoted as n(θk, ξk). This error represents the deviation from the exact gradient and can be due to systemic stochastic noise and algorithmic inexact approximation of the gradient. We consider the general case of state-dependent noise, where θk is the state. SA then involves iterating, θk+1 = θk − αkgk (4) Standard SGD methods assume that such an evaluation satisﬁes Eξ[n(θk, ξ)] = 0, that is, they assume the noise is unbiased. The convergence properties of SA under asymptotically vanishing biased noise are considered throughout many classic texts [10]. Several recent works do consider systemic bias that remains throughout the iterative process, i.e., Ek[n(θk, ξk)] ̸= 0, ∀k, and, even stronger, a stationary lim supk ∥Ek[n(θk, ξk)]∥ = b ̸= 0, ∀k, has been considered in works such as [52, 30] and most recently [1]. Of course, with systematic bias, one cannot hope to reach, even asymptotically, a solution (even a local minimizer). Instead, guarantees are in the form of achieving iterations that are in some neighborhood of a stationary point. Analyses of biased zeroth-order optimization methods in the literature are scant, to the best of the authors’ knowledge. In [12] this problem is claimed to be uninteresting since one can simply minimize f(θk) − Eξ[n(θk, ξk)]. However, we believe this is misguided as one typically does not have a closed-form expression for the noise, and its estimation may add signiﬁcant computational expense. However, this indicates that one cannot hope to achieve, even asymptotically, a stationary point of minθ Eξ[F(θ, ξ)]. Instead, there are notions of approximation and robustness that become relevant: given a bound for the noise, for a zero-order algorithm, 1) how close can we guarantee that iterates get to stationary points and 2) how can we guarantee an estimate from the algorithm that is the best choice under consideration of the worst case outcome of the noise? We should also mention that there exists one work studying the asymptotic guarantees of SPSA under bounded noise [24] with subsequent application to Gaussian mixture models [9]. In this work the authors established a quantitative bound on the asymptotic bias associated with the mean squared error of the gradient for SPSA and other zero-order optimization procedures. Our contribution In this paper, we are interested in studying the properties of classical optimization algorithms as used for variational quantum algorithms (VQA) under non-trivial noise in the evaluation of a quantum circuit. The bias in zeroth-order methods would consist of function evaluations yielding an error, i.e., ξ stochastic sample Eq. (1) Ξ(θ) ideal (noise) distribution Eq. (1) χ(θ) physical (noise) distribution Eq. (2) n(θ, ξ) random variable describing mean zero and noisy bias Eq. (6) gk (stochastic) gradient at step k Eq. (3) αk gain in the negative gradient direction k Eq. (4) and Eq. (5) U(θ) parametrized variational ansatz Eq. (8) and Eq. (8) O, B observable that deﬁnes the cost function Eq. (12), Sec. 2.2 and Fig. 1 resp. E(ρ) quantum noise channel Eq.(47) and Sec. B ∆k k-th simultaneous perturbation vector Sec. 2.3 and Eq. (18) µ smoothing parameter > 0 Eq. (20) fµ Gaussian smoothing function Eq. (20) N number of SA iterations Sec. 2.5
Searching for Intermediate Mass Black Holes in the Milky Way's galactic halo<|sep|>During the last years of XVIII century, the English pastor John Michell and the mathematician Pierre Simon de Laplace imagined the existence of some dark objects, which were called dark stars 1. The given name derived from the fact that if the radius of an object with mass M turns out to be R = 2GM/c2 (where G is Newton’s gravitational constant and c is the light speed), it cannot be visible from outside and therefore it would “appear” as dark. The ﬁrst actual mathematical description was provided in 1916 by Karl Schwarzschild with his solution of Einstein’s ﬁeld equations, representing the gravitational ﬁeld around a non-rotating, spherical, and electrically neutral mass. Half a century later, in 1967, John Wheeler coined the term black hole (BH) to indicate an object whose gravitational ﬁeld is so large that does not let anything, even light, escape from its gravitational attraction. Unlike the classical formalization, General Relativity predicts the existence of an event horizon located at r = rS = 2GM/c2, which is called the Schwarzschild radius.
Interaction between magnetic vortex cores in a pair of nonidentical nanodisks<|sep|>Magnetic objects of nano- and mesoscopic dimensions of diﬀerent shapes - squares, ellipses or disks - may have, as their ground state, a vortex structure1–5. This state is characterized by magnetization in the plane of the nanostructure, tangential to concentric circles, and a small core where the magnetization is perpendicular to the plane. One can deﬁne the circulation c = +1 for counterclockwise (CCW) in-plane magnetization direction, or c = −1 for CW direction; the polarity is p = +1 for magnetization of the vortex core along the +z axis, and p = −1 for the opposite direction (−z). The physical description of the vortex properties is usually made within two analytical models: rigid vortex model6 and the two-vortex model7,8. Magnetic structures with vortices have many potential applications, e.g., as spin-torque nano-oscillators (STNO’s)9, magnetic random memories (MRAM’s)10,11, or logic gates12. The applications usually require magnetic elements arranged in a regular array where the characterization of the interaction between them is required: in some cases it is necessary for the functioning of the device, in other cases it has to be minimized. This interaction allows the coupling of the nanoobjects13 and the loss-less transmission of energy14. When a magnetic vortex structure, for example, a magnetic nanodisk, is in equilibrium, its vortex core rests at its center, and the structure has magnetic ﬂux-closure. In this conﬁguration the coupling with nearby nanoelements is minimum. Conversely, when the vortex structure is out of equilibrium, with its core displaced e.g., by an external magnetic ﬁeld or a spin-polarized current, magnetostatic coupling with the neighbor elements results. The dependence with distance of this coupling has been the subject of several studies in recent years14–17. Once the excitation of the vortex cores through an ex ternal agent is over, they return to their equilibrium position, performing a periodic spiral-like trajectory. This motion, called gyrotropic motion, has been described analytically through Thiele’s equation, that is derived from Landau-Lifshitz equation18, and has also been obtained from micromagnetic simulations. The angular frequency of this motion depends on the saturation magnetization of the material and on the aspect ratio of the disks. It is typically in the range of hundreds of MHz. Also, several experiments using diﬀerent techniques have expanded our knowledge of this phenomenon, e.g., [19]. FIG. 1. Coupled disks with vortex magnetic conﬁguration and diﬀerent radii R1 and R2, separated by a center to center distance d. All these considerations justify why vortex core dynamics, and vortex core interactions, have recently attracted the interest of researchers in the area of Nanomagnetism. Recent studies, both theoretical and experimental, explore the vortex core interaction and vortex dynamics in identical disks pairs13–16,20. The simplest system where one can study interacting nanodisks is, of course, a pair of such magnetic structures; it is therefore the ideal system for the investigation of the properties of the interaction, its dependence with distance, etc. Some devices were proposed using nanodisks with different diameters, e.g., in magnonic devices21 or in nanooscillators22, although there are few studies of the interactions in more complex structures, such as nonidentical disk arrays in which each element interacts with all the others17. In the present work, the problem of the interaction between pairs of magnetic nanodisks with magnetic vortex structures and diﬀerent gyrotropic frequencies, is analyzed both analytically and through micromagnetic simulation. The present discussion is applicable to pairs of magnetic nanodisks that have diﬀerent gyrotropic frequencies, arising either from diﬀerent radii, or diﬀerent materials, or diﬀerent thicknesses. This may be relevant to the study of fabricated pairs of magnetic nanodisks, where a distribution of frequencies is inherent in the actual samples. We will choose as illustration the diﬀerence in radii, as shown in Fig. 1. Here we generalized the analytical treatment of the formulation of disk interaction, for any pair of disks. Our results can be well described by interaction intensities that are a multipole expansion with terms of the form d−n, with n =3, 5, 7 and 9, i.e., dipole-dipole, dipoleoctupole, octupole-octupole and dipole-triacontadipole interactions, respectively, as recently demonstrated by Sukhostavets et al.23. Finally, we used the recently reported magnetic vortex echo method17, in order to obtain information on the interaction between the disks, using the magnetization M(t) given by two diﬀerent approaches, on the one hand using Thiele’s equation, and on another using micromagnetic simulation.
On travelling wave solutions of a model of a liquid film flowing down a fibre<|sep|>Unstable thin viscous liquid ﬁlms coating a vertical ﬁbre display complex interfacial dynamics and various ﬂow regimes. Driven by Rayleigh-Plateau instability and gravity eﬀects, a rich variety of dynamics can occur including the formation of droplets, regular travelling wave patterns, and irregular droplet coalescence. Such dynamics has attracted a lot of attention from researchers in recent years due to its widespread applications in heat and mass exchangers [28], desalination [22,29], and particle capturing systems [1]. With proper choices of ﬂow rate, liquid, ﬁbre radius, and inlet geometry, three typical ﬂow regimes are observed in experiments [12,15,23]: a convective instability regime where bead coalescence happens repeatedly, a travelling wave regime where a steady strain of beads ﬂow down the ﬁbre at a constant speed, and an isolated droplet regime where widely spaced large droplets are separated by small wave patterns. When other system parameters are ﬁxed, varying the ﬂow rate from high to low can lead to a ﬂow regime transition from the convective to the travelling wave regime, and eventually to the isolated droplet regime. A better understanding of the travelling wave pattern is expected to
Analyzing Whole-Body Pose Transitions in Multi-Contact Motions<|sep|>While efﬁcient solutions have been found for walking in different scenarios [1], [2], including rough terrain and going up/down stairs, humanoid robots are still not able to robustly use their arms to gain stability, robustness and safety while executing locomotion tasks. Robotics has approached this problem from a computational point of view ([3], [4], [5], [6], [7]). However, due to the complexity of the problem, these methods are still not completely successful. In this work, we propose to take a step back to analyze human motion in order to gain understanding of the processes humans make when using multi-contacts. Robotics in general, but particularly humanoid robotics, has always been inspired by biological human experience and the anatomy of the human body. However, human motions involving support contacts have almost not been studied [8] and even less how healthy subjects choose to make use of contacts with support surfaces. Works like [9] show that in a standing posture, reaching for a support contact provides augmented sensory information, reducing sway even if it is just through a ”light touch”. This shows that the ability of reaching for supports can be crucial to increase robustness in tasks that require balance like walking or general locomotion, but also for increasing maneuverability in complex manipulation tasks. Nevertheless, to execute such tasks in an autonomous way, we need to better understand The research leading to these results has received funding from the European Union Seventh Framework Programme under grant agreement no 611832 (WALK-MAN) and grant agreement no 611909 (KoroiBot). The authors are with the Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany. {mandery, julia.borrassol, asfour}@kit.edu Fig. 1. When performing locomotion (a), manipulation (b), balancing (c) or kneeling (d) tasks, the human body can use a great variety of support poses to enhance its stability. Automatically detecting such support contacts allows for an automatic identiﬁcation of the visited support poses and their transitions. the principles of whole-body coordination in humans, the variety of supporting whole-body postures available and how to transition between them. In this work, we analyze real human motion data captured with a marker-based motion capture system and postprocessed using our unifying Master Motor Map (MMM) framework [10], [11], to gain information about the poses that are used while executing different locomotion and manipulation tasks like those shown in Fig. 1. The analysis presented allows us to quantify the amount of time spent in each pose, classify transitions depending on their duration, and build a graph of pose transitions that can enlighten the difﬁcult problem of ﬁnding motions that utilize multicontacts to balance. This paper is organized as follows. Section II brieﬂy reviews related works. In Section III, we introduce our methodology to detect support poses. In Section IV, we apply our method to a large set of motions and analyze the resulting data. Finally, Section V summarizes our contributions and gives prospects of future work.
Thick smectic shells<|sep|>Thin liquid crystal ﬁlms, coating spherical surfaces, exhibit chevron-like periodic instability patterns, as in Fig. 1 [1, 2, 3], when observed under an optical polarizing microscope. The mechanism of this pattern formation is two-fold. On the one hand, cooling the nematic 8CB (4-n-octyl-4-cyanobiphenyl) phase towards the smectic phase with the transition temperature TN−SmA = 33.5◦C results in formation of smectic layers, orthogonal to the spherical surface, which locally tend to maintain constant spacing. On the other hand, because of the underlying curvature of a sphere and ﬁnite thickness of the liquid crystal ﬁlm, this constraint cannot be satisﬁed globally, leading to geometric frustration [4, 5]. In smectic-like systems geometric frustration can be relieved by introducing topological defects, such as focal conics, dislocations or curvature wall defects [6, 7] or forming spatially periodic texture, for example, during embryogenesis in presence of intestinal tissue growth [8]. However, a quantitative description of energy minimizing space-ﬁlling smectic structures still remains challenging, because of the intricate connection between curvature of the embedded space, local energetic constraint of equally spaced layers, global topological constraints as well as the boundary conditions. In this paper we propose a simple but realistic approach to study the eﬀects of thickness, curvature and elastic properties of smectic shells on the characteristic wavelength of instability texture. Nematic shells with in-plane two-dimensional (2D) order have been extensively studied before both experimentally and theoretically [9, 10, 11]. Smectic shells can be treated as the limiting case of nematic shells when the bend elastic constant K3 is much larger than splay elastic constant K1 in the Frank free energy [12]. The anisotropy of elastic constants K3 � K1 inﬂuences the equilibrium arrangement of topological defects in spherical nematics [13] as well as the equilibrium shape of closed vesicles with in-plane orientational order [14]. On a curved Figure 1: Birefringence texture observed in experimental smectic shells [1, 2] with radius R = 92 µm and thickness h = 3.4 µm. Image courtesy of Teresa Lopez-Leon and Alberto Fernandez-Nieves. substrate, an inﬁnitely thin (2D) liquid crystal ﬁlm with the nematic director n aligned along geodesics (n · ∇)n = 0 [5, 13, 15] contains no bend distortions, and is thus the ground state in the limit K1 ≪ K3. For a sphere the geodesics coincide with great circles, and thus the director n is preserved under parallel transport along meridians, resulting in splay-rich equilibrium textures [13]. The question we pose here is how this texture changes when we allow the smectic ﬁlm on a sphere to have a ﬁnite constant thickness. The increase of dimensionality of the system from 2D to 3D results in the dilation of smectic layers at the outer spherical shell, which can be compensated by the insertion of extra layers (dislocations) or by bending of the layers (curvature walls). Experimental data [1, 2, 3] suggests that geometric frustration in thick smectic shells is resolved by curvature walls rather than dislocations (see Fig. 1). In this paper we distinguish (i) ‘ultrathin’ smectic ﬁlms with director ﬁeld following the meridians of an inner sphere, which remain stable under extension into the third dimension along the radius and (ii) ‘thick’ smectic shells when the tilt of the director out of the tangent plane of the spherical surface (bending of the layers) is energetically favoured. Similar to the Barbero– Barberi criterion for thin nematic ﬁlms [16], here we ﬁnd that the conﬁguration (i) becomes unstable when K3 > RWa, where R is the radius of an inner sphere and Wa is the anchoring strength of the outer spherical shell. Increasing the thickness h of the smectic ﬁlm above the critical value (∝ K3/Wa − R) one might expect periodic undulations of smectic layers resulting in the formation of crescent-like domains as in Fig. 1 and in Refs. [1, 2, 3]. Thus the relationship between thickness h and the curvature R of smectic shell and bend elastic constant K3 play the role of the magnetic ﬁeld or dilative strain in the Helfrich–Hurault instability in planar geometry [6, 17]. By adopting a perturbation approach with h/R being a small parameter of our system, we explore the instability of the ground state (i) for h → 0 with respect to the periodic solution in thick smectic shells. Our prediction of the critical period and the critical thickness accounting for chevron-like texture can be relevant to analyze experimental data and extract values of elastic constants and anchoring strength in the vicinity of the nematic–smectic phase transition. In the following we ﬁrst reconsider the ground state of thin smectic shells accounting for splay-rich topological defects. Next, we consider the breaking down of this solution state for thick smectic shells, assuming inﬁnitely strong planar anchoring at the inner shell and neglecting the role of splay rich topological defects (K1 ≪ K3). In the last section we hypothesize periodic
Contour Detection Using Contrast Formulas in the Framework of Logarithmic Models<|sep|>The contour or edge detection is an essential  operation in any chain of image processing  aiming at the segmentation of the image. Very  often the methods using the classical image  representation – i.e. when the pixel value is  considered a real number and the addition and  scalar multiplication are the classical ones in the  field of real numbers – don’t end at satisfactory  results. This is the case especially for images  with regions of low or high luminosity. A  correct (edge detection) contouring for these  images  asks  for  many  intermediate work in an algebraic structure of logarithmic  type.    In this paper we shall concisely present an  algebraic structure based on a logarithmic  model developed in [1,2]. In  this framework we  shall define the relative and the absolute  contrast between two pixels and the contrast of  pixel ( in an arbitrary neighborhood). With these  notions we shall obtain an efficient procedure  for contour detection: in fact the contour image  is the image of the contrast of the pixels.    The remainder of the paper is structured as  follows: the section 2 shows the notion of  logarithmic function of gray levels which  mathematically defines an achromatic image,  section 3 shows the vector space of gray levels  in the context of logarithmic model, section 4  shows the fundamental isomorphism between  the vector space of logarithmic gray levels and  the vector spaces of real numbers, section 5  shows the contrast formulas in the logarithmic  model, section 6 defines the contour operation  in the sense of this paper and shows the contour  images using the contrast defined in the  logarithmic model elaborated in [1,2] and as a  comparation , the same images contoured by  classical methods. Finally, section 7 is for short  conclusions. The 8th International Conference, Exhibition on Optimization of Electrical and Electronic Equipment, OPTIM 2002, Vol III,  16 - 17 May 2002, Braşov, România
Assessing the Performance of Leja and Clenshaw-Curtis Collocation for Computational Electromagnetics with Random Input Data<|sep|>More often than ever before, the design phase of electric and electronic devices, e.g. waveguides or accelerator magnets, incorporates parameter studies in order to predict the device’s behavior under uncertainty. This uncertainty, e.g. with respect to the device’s geometry or material properties, often stems from tolerances during the manufacturing process. As part of those uncertainty quantiﬁcation (UQ) studies, one typically investigates a speciﬁc output of the device, called the quantity of interest (QoI), and tries to estimate statistical moments or sensitivities, with the goal of reducing the risk of malfunction, misﬁre or other type of failure. Most commonly, UQ studies rely on sampling methods. Monte Carlo (MC) sampling [10] converges irrespective of the number of random variables (RVs) or the regularity of the given problem, albeit with a slow convergence rate of O � M −0.5� in the mean-square-error sense, where M denotes the number of samples, equivalently, costs. Improved cost-error ratios can be achieved with multilevel MC [18] methods. Spectral UQ approaches [17, 49] converge much faster, exponentially in the best case, for a small to moderate number of random inputs and smooth input-to-output map. Typical methods of this type are stochastic Galerkin [2, 17, 29], stochastic collocation [1, 3, 7, 50], or point collocation [6, 31, 32] methods. The stochastic Galerkin method is often labeled as “intrusive”, due to the fact that dedicated solvers have to be developed in order to tackle the stochastic problem at hand. The additional programming eﬀort is usually regarded as a major disadvantage, especially in the case of complex computational models whose software and underlying solvers are diﬃcult to be accessed, modiﬁed or otherwise manipulated. Therefore, and despite the fact that stochastic Galerkin methods have nice properties for error analysis and estimation, collocation methods are generally preferred, as they allow for a non-intrusive, black-box use of the original computational models. It must be noted that the separation of methods into intrusive and non-intrusive is an ongoing topic of discussion, see e.g. [19]. In the context of the present paper we shall retain the usual distinction. Comparisons between stochastic and point collocation methods, see e.g. [14], indicate that the former tends to provide superior accuracies and convergence rates for smooth QoIs. However, since these approaches diﬀer signiﬁcantly, a fair comparison between the two is still an open research topic, as also indicated in [31]. A common bottleneck of all aforementioned methods is the so-called “curse of dimensionality” [4], i.e. convergence rates deteriorate and computational costs increase with the number of considered input parameters, by deﬁnition, exponentially. As a possible remedy, state-of-the-art methods employ sparse, adaptively constructed polynomial approximations, see e.g. [11, 33, 36, 41] for adaptive stochastic collocation methods, [6, 30] for adaptive point collocation methods, and [13] for an adaptive stochastic Galerkin method. While generally not free of the curse of dimensionality, adaptive methods exploit possible anisotropies among the input parameters regarding their impact upon the QoI. Assuming that such anisotropies exist, adaptivity may enable studies with a comparably large number of input parameters. More recently, tensor decompositions (see [20] and the references therein) have been used to exploit possible low-rank structures of parametric problems in order to tackle the curse of dimensionality. In several cases, again relying on high regularity, superior asymptotic convergence rates have been obtained compared to sparse grid methods [48]. However, comparisons between these methods remains an active ﬁeld of research, as break-evens have not yet been fully determined. Here we will only consider stochastic collocation methods, in which case, dimension-adaptive algorithms [16, 25] constitute the current state-of-the-art. In the search for an acceptable compromise between computational work and approximation accuracy, such approaches are receiving increasing attention in uncertainty quantiﬁcation. Dimension-adaptive methods are based on nested univariate collocation points, e.g. Clenshaw-Curtis and Genz-Keister nodes are typical choices for uniform and normal input distributions, respectively. The extension of dimension-adaptive schemes to cases where the input distributions do not fall into the two aforementioned categories is desirable and an active ﬁeld of interest, as well as one of the main considerations of the present paper. In this work we consider univariate and medium to high-dimensional multivariate UQ, in the context of electromagnetic ﬁeld (EMF) problems with random inputs. The probability distributions of the inputs are assumed to be bounded, but not necessarily uniform, e.g. beta distributions are considered in this work. The stochastic collocation method is used for the UQ studies. When multiple inputs are considered, we employ a dimension-adaptive algorithm based on nested univariate collocation points. We investigate the performance of the method for diﬀerent choices of nested collocation points, in particular provided from either Clenshaw-Curtis [12] or Leja rules [26]. In the case of non-uniform input distributions, we use weighted Leja rules based on [33], while the Clenshaw Curtis rules are modiﬁed as in [44]. For the case of uniform inputs only and in a purely mathematical context, comparisons between the two rules can be found in [11, 33, 34]. We are unaware of such comparisons for the case of bounded, non-uniform inputs, such as the ones considered here. The available literature also lacks works considering concrete engineering applications, such as the EMF problems presented in this work. The rest of the paper is organized as follows. In Section 2 we oﬀer a general description of the UQ problem at hand. In Section 3 we describe the UQ method of choice, namely the stochastic collocation method. The univariate collocation is presented in Section 3.1, while multivariate collocation schemes are presented in Sections 3.2 and 3.3 for the cases of tensor grids and sparse grids, respectively. The latter are further separated into isotropic and adaptive-anisotropic sparse grids, respectively discussed in Sections 3.3.1 and 3.3.2. Post-processing and collocation-based quadrature schemes are presented in Section 3.4. The two choices of collocation points considered in this work are presented in Section 3.5. A number of numerical experiments are given in Section 4. An analytical, academic EMF model is considered in Section 4.1. Results for a real-world application are available in Section 4.2.
Asteroseismology of solar-type stars<|sep|>Helio- and asteroseismology allow us to study the internal structure and dynamics of the Sun and other stars by means of their resonant oscillations (e.g. Gough, 1985; Turck-Chi`eze et al., 1993; Christensen-Dalsgaard, 2002; Aerts et al., 2010; Basu, 2016, and references therein). These vibrations manifest themselves by motions of the stellar photosphere and by temperature and density changes implying modulations of the positions of the Fraunhofer lines and of the stellar luminosity respectively. Repeated sequences of stochastic excitation and damping by turbulent motions in the external convective layers lead to a suite of resonant modes in the Sun (Goldreich and Keeley, 1977; Goldreich and Kumar, 1988; Balmforth, 1992; Goldreich et al., 1994; Samadi and Goupil, 2001; Belkacem et al., 2008). The stars where their modes are excited in this way are usually called “solar-like pulsators” or simply “solar-like” stars even though their structure and dynamics could be diﬀerent compared to the actual Sun, covering main-sequence (MS), sub-giant and red-giant stars (e.g. De Ridder et al., 2009; Bedding et al., 2010a; Garc´ıa and Stello, 2015; Hekker and Christensen-Dalsgaard, 2017). The oscillation periods of solar-like stars range from minutes to years (e.g. Mosser et al., 2010, 2013; Stello et al., 2013, 2014; Chaplin et al., 2014a). In this review, we focus on “Solar-type” stars deﬁned as cool main-sequence dwarfs located below the red edge of the classical instability strip (spectral types from late F, G and K dwarfs, see the zone encircled by the red circle in Figure 1) with a near-surface convective zone that excites the oscillation modes. However, to put these stars in context, we will sometimes discuss sub-giants and early red giants. In such a way, the continuity towards more evolved stars will be ensured. There are other mechanisms exciting stellar oscillations in more massive and luminous mainsequence stars: a) the heat-engine mechanism (also known as κ or opacity-driven mechanism), related to the changes in the opacity proﬁle due to temperature variations, and responsible for the pulsation in the instability strip and white dwarfs (e.g. Eddington, 1926); b) the ϵ mechanism, where the nuclear reaction rate changes as a consequence of the contraction and expansion of the star (e.g. Lenain et al., 2006).; c) tidal eﬀects, where non-radial oscillations can be forced in stars belonging to multiple systems (e.g. Welsh et al., 2011). All of these pulsating stars are usually referred to by the generic term “classical pulsators” (e.g. δ Scuti, γ Doradus, RR Lyrae, Cepheids, etc) and their study is out of the scope of this review (for more information on these variable stars see, for example, Aerts et al., 2010).
Strong Optimistic Solving for Dynamic Symbolic Execution<|sep|>Modern software developers invest a huge amount of efforts in increasing the programs quality. Companies employ security development lifecycle (SDL) [1–3] to ﬁnd bugs in their products and defend programs from dangerous interventions. Various automated testing tools provide thorough code exploration. Hybrid fuzzing [4] is the state-of-the-art solution for ﬁnding bugs. Its efﬁciency comes from fast and lightweight fuzzing [5, 6], that allows to discover new paths quickly, and more accurate dynamic symbolic execution [7–12], that helps reaching difﬁcult program parts by inverting complicated branches along some execution path. Moreover, dynamic symbolic execution (on initially valid paths) enables new seeds generation that trigger memory and undeﬁned behavior errors [13]. Dynamic symbolic execution [14, 15] tools execute the target program and construct the path predicate by symbolic interpretation of program instructions. The path predicate contains the branch conditions met during analysis. Symbolic engines try to invert each branch from the path predicate to discover new execution paths that are hardly reached with fuzzing. The predicate for the branch inversion conjuncts all the preceding branch constraints (i.e. constraints from branches executed before the target branch) and the negation of the target branch constraint. The majority of symbolic engines frequently face over- and underconstraint problems (similar to taint analysis [16]) that prevent them from exploring more program paths. Overconstraint denotes the situation when there are many redundant constraints in the path predicate that may cause its complication or even unsatisﬁability. Overconstraint is increasing the symbolized instructions number during analysis. Underconstraint, on the contrary, means that some variable is not treated as symbolic, though it should be. Non-trivial branch conditions or symbolic pointers (that depend on user data) may cause underconstraint. Symbolic executors apply various techniques to cope with these problems (Section II). We propose the strong optimistic solving method that generates input data for branch inversion even when the constructed predicate is unsatisﬁable. We eliminate some constraints from the solver query based on the branches nesting. Thus, we can bypass over- and underconstraint. As an illustration, consider the code example below: 1 void func(const char* buf) { 2 if (buf[3] == ’6’ & buf[0] == ’5’) //c_2 3 printf("Success!\n"); 4 else 5 printf("Fail\n"); 6 } 7 8 int main() {
Voronoi Density Estimator for High-Dimensional Data: Computation, Compactification and Convergence<|sep|>Given a discrete set of data sampled from an unknown probability distribution, the aim of density estimation is to recover the underlying Probability Density Function (PDF) (Diggle 2013; Scott 2015). Non-parametric methods achieve this by directly computing the PDF through a closed formula, avoiding the potentially expensive need of searching for optimal parameters. One of the most common non-parametric density estimation techniques is the Kernel Density Estimator (KDE; Gramacki 2018). The resulting PDF is a convolution between a ﬁxed kernel and the discrete distribution of samples. In case of the Gaussian kernel, this corresponds to a mixture density with a Gaussian distribution centered at each sample. Another popular density estimator, more commonly used for visualization purposes is given by histograms (Freedman and Diaconis 1981), which depend on a prior tessellation of the ambient space (typically, a grid). The estimation is piecewise constant and is obtained by the number of samples falling in each cell normalised by its volume. A common limitation of the aforementioned methods is a bias towards a ﬁxed local geometry. Namely, estimates through KDE near a sample are governed by the level sets of the chosen kernel. In the Gaussian case, such level sets are ellipsoids of high estimated probability. Histograms suffer from an analogous bias towards the geometry of the cells of the tessellation (i.e., the bins of the histograms), on which the estimated PDF is constant. The issue of geometrical bias severely manifests when considering real-world highdimensional data. Indeed, one cannot expect to approximate the rich local geometries of complex data with a simple ﬁxed one. Both the estimators come with hyperparameters controlling the scale of the local geometries which require tuning. This amounts to the bandwidth for KDE and the diameter of the cells for histograms. sidering the Voronoi tessellation generated by data (Okabe et al. 2009), the estimated PDF is piece-wise constant on the cells and proportional to their inverse volume. The Voronoi tessellation adapts local polytopes so that each datapoint is equally likely to be the closest when sampling from the resulting PDF. This has enabled successful application of the VDE to geometrically articulated real-world distributions in lower dimensions (Duyckaerts, Godefroy, and Hauw 1994; Ebeling and Wiedenmann 1993; Vavilova et al. 2021). The goal of the present work is to enable the VDE for high-dimensional scenarios. Although the VDE constitutes a promising candidate due to its local adaptivity, the following aspects have to be addressed: Computation. The Voronoi cells are arbitrary convex polytopes and their volume is thus challenging to compute explicitly, which yields the necessity for fast approximate computations. Compactiﬁcation. Data is often concentrated around lowdimensional submanifolds, which makes most of the ambient space empty and several Voronoi cells unbounded, i.e. of inﬁnite volume (see Figure 3). One still needs to produce a ﬁnite estimate on those cells, a process we refer to as ’compactiﬁcation’. We propose solutions to the problems above. First, we present efﬁcient algorithmic procedures for volume computation and sampling from the estimated density. We formulate the cell volumes as integrals over a sphere, which can then be approximated by Monte Carlo methods. Furthermore, we propose a sampling procedure for the distribution estimated by the VDE. This consists in randomly traversing the Voronoi cells via a ’hit-and-run’ Markov chain (M.-H. Chen and Schmeiser 1996). The proposed algorithms are highly parallelizable, allowing efﬁcient computations on the GPU. In order to compactify the cells, we place a ﬁnite measure on each of them by means of a ﬁxed kernel (typically, a Gaussian one), leading to an altered version of the VDE which we refer to as Compactiﬁed Voronoi Density Estimator (CVDE). Figure 1 shows an example of an estimate by the CVDE on a simple two-dimensional dataset. All the computational and sampling procedures naturally extend to the CVDE. A further contribution of the present work is a theoretical proof of convergence for the CVDE. Assuming the original density has support in the whole ambient space, we show that the PDF estimated by the CVDE converges (with respect to an appropriate notion for random measures) to the ground-truth one as the number of datapoints increases. The convergence holds without any continuity assumptions on the ground-truth PDF nor on the kernel and does not require the kernel bandwidth to vanish asymptotically. This is in contrast with the convergence properties of the KDE. Due to the aforementioned local geometric bias of the KDE, the bandwidth has to decrease at an appropriate rate in order to amend for the local inﬂuence of the kernel and guarantee convergence to the underlying distribution (LP Devroye and Wagner 1979; Jiang 2017). Finally, we implement the CVDE in C++ and parallelize computations via the OpenCL framework. Our code, with a provided Python interface, is publicly available at https://github.com/vlpolyansky/cvde.
Testing the isotropy of the Universe by using the JLA compilation of type-Ia supernovae<|sep|>The cosmological principle, which is one of the foundations in modern cosmology, says that the Universe is homogeneous and isotropic at large enough scales (Dodelson 2003). It is well consistent with the present observational data, such as the cosmic microwave background (CMB) radiation from Wilkinson Microwave Anisotropy Probe (WMAP) (Bennett et al. 2013; Hinshaw et al. 2013) and Planck satellite (Ade et al. 2015a,b). Until now, the cosmological observations are still in accordance with the cosmological constant plus cold dark matter (ΛCDM) model which is based on the cosmological principle. Thus, the ΛCDM model becomes the leading model in modern cosmology. Despite the great successes it achieved, the ΛCDM model still faces certain challenges (Perivolaropoulos 2008, 2011; Mariano & Perivolaropoulos 2013). As the improvements of accuracy, it is found from a large amount of observations that the Universe might deviate from statistical isotropy. These include the alignment of low multipoles in the angular power spectrum of CMB temperature ﬂuctuations (Tegmark, de Oliveira-Costa & Hamilton 2003; Bielewicz, Gorski & Banday 2004; Copi et al. 2010; Frommert & Enßlin 2010), the hemispherical power asymmetry of CMB temperature anisotropy (Bennett et al. 2013; Ade et al. 2014; Eriksen et al. 2004; Hansen, Banday & Gorski 2004; Akrami et al. 2014; Quartin & Notari 2015), the spatial variation of the electromagnetic ﬁne-structure constant (Webb et al. 2011; King et al. 2012; Molaro et al. 2013), the largescale alignment of the quasar polarization vectors (Hutsemekers & Lamy 2001; Hutsemekers et al. 2005), the large-scale bulk ﬂow beyond the prediction of ΛCDM model (Kashlinsky et al. 2009, 2010; Watkins, Feldman & Hudson 2009), and so on. All of these phenomena arouse us to rethink the validity of the cosmological principle. If the cosmological principle is proven to be failed, the modern cosmology should be rewritten. Due to their consistent absolute magnitudes, type-Ia supernovae (SNe Ia) are regarded as the ideal distance indicators to trace the accelerated expansion of the Universe. In fact, they have been widely used to search for the anisotropic signals in the Universe. Especially, a statistic based on the extreme value theory shows that the gold data set is consistent with the isotropy (Gupta, Saini & Laskar 2008). The study (Blomqvist, Enander & Mortsell 2010) on the angular covariance function of supernova magnitude ﬂuctuations is consistent with zero dark energy ﬂuctuations by using the Union2 compilation (Amanullah et al. 2010). A “ residual” statistic shows that the isotropic ΛCDM model is not consistent with the Union2 data with z < 0.05 at 2 − 3σ (Colin et al. 2011). There are no signiﬁcant evidence for deviations from the isotropy in the anisotropic Bianchi-I cosmology (Campanelli et al. 2011; Schucker, Tilquin & Valent 2014), Bianchi-III and Kantowski-Sachs metrics (Koivisto et al. 2011),
OwlEyes-Online: A Fully Automated Platform for Detecting and Localizing UI Display Issues<|sep|>GUI is widely used among modern mobile apps, making it practical and easy to use. However, with the development of visual effects of GUI, five categories of UI display issues [27] such as component occlusion, text overlap, missing image, null value and blurred screen always occur during the UI display process, especially on different mobile devices. Detecting those issues is a hard problem because most of those UI display issues are caused by many factors, especially for Android, such as different Android OS versions, device models, and screen resolutions [25]. Nowadays, some practical automated testing tools like Monkey [11, 26], Dynodroid [17] are also widely used in industry. However, these automated tools can only spot critical crash bugs, rather than UI display issues that cannot be captured by common tools. Inspired by the fact that display bugs can be easily spotted by human eyes, we develop an automated online tool OwlEyes-Online1, which provides quick detection and localization of UI display issues from apps or GUI screenshots. The OwlEyes-Online is a user-friendly web app. Developers can upload GUI screenshots or apps and receive accurate UI display issue detection results. When a developer uploads an APK, it will automatically run the app and get its screenshots, and then we use computer vision technologies to detect the UI display issues. OwlEyes-Online builds on the CNN to identify the screenshots with issues and Grad-CAM to localize the regions with UI display issues in the screenshots for further reminding developers. Finally, it summarizes the detection and localization results, automatically generates the test report and sends it to users. Considering that the CNN needs lots of training data, we adopt a heuristic data generation method to generate the training data. 1OwlEyes-Online is named as our approach is like the owl’s eyes to effectively spot UI display issues. And our model (nocturnal like an owl) can complement conventional automated GUI testing (diurnal like an eagle) for ensuring the robustness of the UI. OwlEyes-Online provides a dashboard for users to upload the screenshots or apps. After analyzing an uploaded screenshot, it displays detection results in real-time. As for an app, it automatically generates a test report (issue screenshots, localization, etc.) and sends the report to the user in an email. This paper makes the following contributions: • We implement a CNN based issue detection method and a Grad-CAM based issue localization method to detect UI display issues from GUI screenshots. • We develop a fully automated web app. Users only need to upload an APK file, and OwlEyes-Online will automatically generate test reports and send them to users. We release the implementation of OwlEyes-Online on Github [5].
Trading Strategies Generated by Lyapunov Functions<|sep|>Back in 1999, E.R. Fernholz introduced a construction that was both remarkable and remarkably easy to establish. He showed that for a certain class of so-called “functionally-generated” portfolios, it is possible to express the wealth these portfolios generate, discounted by (that is, denominated in terms of) the total market capitalization, solely in terms of the individual companies’ market weights – and to do so in a pathwise manner, that does not involve stochastic integration. This fact can be proved by a somewhat determined application of Itˆo’s rule. Once the result is known, its proof becomes a moderate exercise in stochastic calculus. ∗We are grateful to Robert Fernholz for initiating this line of research and for encouraging us to think about the issues studied here. Many discussions with Kostas Kardaras helped us sharpen our thoughts. We are also deeply indebted to Adrian Banner, Christa Cuchiero, Freddy Delbaen, David Hobson, Tomoyuki Ichiba, Philip Protter, Mathieu Rosenbaum, Walter Schachermayer, Konrad Swanepoel, Kangjia’Nan Xie, and Hao Xing for helpful comments, and Alexander Vervuurt and Minghan Yan for their detailed reading and suggestions on successive versions of this paper. I.K. acknowledges the support of the National Science Foundation under grant NSF-DMS-14-05210. J.R. acknowledges generous support from the Oxford-Man Institute of Quantitative Finance, University of Oxford. †Department of Mathematics, Columbia University, New York, NY 10027 (E-mail: ik@math.columbia.edu), and INTECH Investment Management, One Palmer Square, Suite 441, Princeton, NJ 08542 (E-mail: ikaratzas@intechjanus.com). ‡Department of Mathematics, University College London, Gower Street, London WC1E 6BT, United Kingdom (E-mail: j.ruf@ucl.ac.uk). The discovery paved the way for ﬁnding simple and very general structural conditions on large equity markets – that involve more than one stock, and typically thousands – under which it is possible strictly to outperform the market portfolio. Put a little differently: conditions under which strong relative arbitrage with respect to the market portfolio is possible, at least over sufﬁciently long time-horizons. Fernholz (1999, 2001, 2002) showed also how to implement this strong relative arbitrage, or “outperformance,” using portfolios that can be constructed solely in terms of observable quantities, and without any need for estimation or optimization. Pal and Wong (2015) related functional generation to optimal transport in discrete time. Although well-known, celebrated, and quite easy to prove, Fernholz’s construction has been viewed over the past 15 years as somewhat “mysterious.” In this paper we hope to help make the result a bit more celebrated and a bit less mysterious, via an interpretation of portfolio-generating functions G as Lyapunov functions for the vector process µ(·) of relative market weights. Namely, via the property that G(µ(·)) is a supermartingale under an appropriate change of measure; see Remark 3.3 for elaboration. We generalize this functional generation from portfolios to trading strategies, as well as to situations where some, but not all, of the market weights can vanish; along the way we simplify the underlying arguments considerably, and answer an old question of Fernholz (2002), Problem 4.2.3. Conditions for strong outperformance of the market over appropriate time horizons become extremely simple via this interpretation, as do the strategies that implement such outperformance and the accompanying proofs that establish such results; see Theorems 5.1 and 5.2. We have cast all our results in the framework of continuous semimartingales for the market weights; this seems to us a very good compromise between generality on the one hand, and conciseness, unity and readability on the other. The reader will easily decide which of the results can be extended to general semimartingales, and which cannot. Here is an outline of the paper. Section 2 presents the market model and recalls the ﬁnancial concepts of trading strategies, outperformance, and deﬂators. Section 3 then introduces the notions of regular and Lyapunov functions. Section 4 discusses how such functions generate trading strategies, and Section 5 uses these observations to formulate conditions that guarantee trading strategies which outperform the market over sufﬁciently long time horizons. Section 6 contains several relevant examples for regular and Lyapunov functions and the corresponding generated strategies. Section 7 proves that concave functions satisfying certain additional assumptions are indeed Lyapunov and provides counterexamples if those additional assumptions are not satisﬁed. Finally, Section 8 concludes.
A Probabilistic Framework for Moving-Horizon Estimation: Stability and Privacy Guarantees<|sep|>Moving-horizon estimation (MHE) is an optimization-based state estimation method that uses the most recent measurements within a moving-time horizon to recursively update state estimates. In principle, its optimization-based formulation enables it to handle nonlinearities and state constraints much more eﬀectively than other known methods. This, coupled with the adoption of increasingly powerful, inexpensive computing platforms has brought new impetus to the adoption of moving-horizon estimation in various data-driven applications. In many cases, data is acquired from particular individuals or users, which introduces new ethical concerns about data collection and manipulation, highlighting an increasing need for data privacy. Such is the case in home monitoring and traﬃc estimation (with vehicle GPS data) applications, to name a few. Motivated by this, here we design and analyze a new class of moving-horizon estimation ﬁlters that can guarantee the diﬀerential-privacy of the data. ∗The authors are with the Department of Mechanical and Aerospace Engineering, University of California at San Diego, La Jolla CA 92093 USA (email: v6krishn@ucsd.edu; soniamd@ucsd.edu). The origins of MHE can be traced back to the limited memory optimal ﬁlters introduced in [19]. Theoretical investigations on MHE have broadly been directed at their asymptotic stability [2, 29, 33] and robustness [18, 20, 24] properties. These properties have primarily been built upon underlying assumptions of input/output-to-state (IOSS) stability, which is adopted as the notion of detectability, wherein the norm of the state is bounded given the sequences of inputs and outputs. However, alternative foundations for the stability results in other classical notions of observability, such as strong observability [25], have remained unexplored. The connection between nonlinear observability theory and estimation problems runs deep, see [22] and more recently [32], and it is worthwhile to explore this connection in the context of optimization-based estimation methods such as moving-horizon estimation. The problem of state estimation is fundamentally about dealing with uncertainty, manifested as uncertainty in the initial conditions and/or in the evolution of the system in the presence of unknown disturbances. This is appropriately formulated in the space of probability measures over the state space of the system. Recent advances in gradient ﬂows in the space of probability measures [5], [30], and the corresponding discrete-time movementminimizing schemes [28] present powerful theoretical tools that can be applied to recursive optimization-based estimation methods such as moving-horizon estimation, and can serve as a unifying framework for their design and analysis. Another important consideration in the MHE problem is the cost of computation. The problem formulation more commonly involves solving an optimization problem at every time instant, with the state estimate and disturbances as decision variables in the optimization, where the dimension of the problem scales with the size of the horizon. This approach, in general, tends to be computationally intensive, which poses a hurdle for implementation in real-time. This has motivated the search for fast MHE that implement one or more iterations of the optimization at every time instant. Recently, in [3], [4], the authors develop such a method for noiseless systems and provide theoretical guarantees on convergence. However, these works assume the convexity of the cost function, which is restrictive for general nonlinear systems, and not well connected to notions of observability. None of these works has considered the additional question of privacy. Diﬀerential privacy [11] has emerged over the past decade as a benchmark in data privacy. The typical setting assumes independence between the records in static databases; however, basic existing mechanisms fail to provide guarantees when correlations exist between the records in the database. This is the case when data is employed by a state estimation process whose output is then released: there is a dynamic system from which a time series of sensor measurements is obtained, and the measurement data and the released estimates are correlated. In [8,9], the authors generalize the deﬁnition of diﬀerential privacy to include general notions of distance between datasets and design diﬀerentially private mechanisms for Bayesian inference. In [23, 31], the authors investigate privacy-preserving mechanisms for the case where correlations exist between database records. Privacy-preserving mechanisms for functions and functional data were investigated in [15]. The work [27] studies the problem of diﬀerentially-private state estimation, introducing the formal notion of diﬀerential privacy into the framework of Kalman ﬁlter design for dynamic systems. The authors of [13] consider the problem of optimal state estimation for linear discrete-time systems with measurements corrupted by Laplacian noise. A ﬁnite-dimensional distributed convex optimization is consid ered in [26], where diﬀerential privacy is achieved by perturbation of the objective function. We refer the reader to [7] for a broad overview of the systems and control-theoretic perspective on diﬀerential privacy. Contributions: The contributions of this work are two-fold: establishing the robust asymptotic stability of the proposed moving-horizon estimator in a probabilitstic framework, founded on the notion of strong local observability; and incorporating diﬀerential privacy in moving-horizon estimation. We begin with the well-studied notion of strong local observability of nonlinear, discrete-time systems and investigate its relationship to the optimizationbased state estimation problem. To handle uncertain initial conditions and the possible non-uniqueness of solutions to the estimation problem, we adopt a generalized problem formulation over the space of probability measures over the state space. More precisely, we deﬁne the MHE as a proximal gradient descent in the space of probability measures, with a nonconvex, time-varying cost function. This probabilistic setting serves as a unifying framework for moving-horizon estimation and allows us to develop diﬀerent classes of moving-horizon estimators by simply varying the metric used to deﬁne the proximal operator, and to obtain implementable ﬁlters by Monte Carlo methods. We then consider the Wasserstein metric and the KL-divergence, which yield the more familiar MHE and a particle ﬁlter, respectively. Following this, we present an analysis of the convergence and robustness properties of these estimators in the probabilistic setting, under assumptions of strong local observability. Further, we modify the optimization problem (in the space of probability measures) by an entropy regularization to derive conditions that guarantee a desired level of diﬀerential privacy for these ﬁlters. Paper organization: The rest of the paper is organized as follows. In Section 2, we introduce the notation and mathematical preliminaries used in the paper. We present the optimization-based state estimation problem in Section 4, where Section 4.1 deals with the Full Information Estimation (FIE) problem and the Moving-horizon Estimation (MHE) problem is introduced in Section 4.2. We present the MHE method based on proximal gradient descent with the Wasserstein metric in Section 5, and with the KL-divergence in Section 6. In Section 7, we address the diﬀerential privacy considerations for the moving-horizon estimators designed. The results from numerical experiments are presented in Section 8, with the conclusions in Section 9.
Modelling Concurrent Behaviors in the Process Specification Language<|sep|>In Process Speciﬁcation Language (PSL), the ordering of event (activity) occurrences is modelled using occurrence trees, which are restricted forms of partial orders. Although partial orders can sufﬁciently model the “earlier than” relationship, they cannot explicitly model the “not later than” relationship [7]. For instance, if an event a is performed “not later than” an event b, then this “not later than” relationship can be modelled by the following set of two step sequences x = {{a}{b},{a,b}}, where the step {a,b} models the simultaneous performance of a and b. But the set x can not be represented by any partial order. To provide a uniﬁed framework for analyzing “earlier than” and “not later than” relationships, we proposed to interpret the generalized stratiﬁed order structure (gsostructure) theory within PSL. The gso-structure theory is originated from causal partial order theory and stratiﬁed order structure (so-structure) theory. A so-structure [1,6,8,9] is a triple (X,≺,⊏), where ≺ and ⊏ are binary relations on X. They were invented to model both “earlier than” (the relation ≺) and “not later than” (the relation ⊏) relationships, under the assumption that all system runs (also called observations) are modelled by stratiﬁed orders, i.e., step sequences. They have been successfully applied to model inhibitor and priority systems, asynchronous races, synthesis problems, etc. (see for example [8,11,14] and others). However, so-structures can adequately model concurrent histories only when the paradigm π3 of [7,9] is satisﬁed. Paradigm π3 says that if two event occurrences are observed in both orders of execution, then they will also be observed executing simultaneously. Without this assumption, we need gso-structures, which were introduced and analyzed in [2]. The comprehensive theory for gso-structures has been developed in [5,15]. A gso-structure is a triple (X,<>,⊏), where <> and ⊏ are binary relations on X modelling “never simultaneously” and “not later than” relationships respectively under the assumption that all system runs are modelled by stratiﬁed orders. Intuitively, gso-structures can model even the situation when we have the mixture of “true concurrency” and interleaving semantics. The only disadvantage is that gso-structures are more complex to conceptualize than so-structures. Since the works of Janicki et al. [7,5] focus on the algebraic properties of gsostructures, the number of axioms are kept to minimal and some of the assumptions are made implicit. Furthermore, the theorems of gso-structure theory frequently involve quantifying over relations, which requires the use of higher-order language. Hence, to apply ﬁrst-order ontology and model-theoretic techniques in the manner as in [4], we will ﬁrst deﬁne a formal ontology for gso-structure in ﬁrst-order logic and characterize all possible models of gso-structure theory up to isomorphism. After that we can proceed to investigate to what extend the theorems of gso-structure theory hold within the ﬁrst-order setting of PSL by studying possible ontological mappings from gso-structure theory to PSL. The organization of this paper is as follows. In Section 2, we will give a ﬁrst-order axiomatization of the gso-structure theory and end the section will a result showing that our theory is consistent. In Section 3, we will classify all possible models of the gsostructure theory from Section 2 using more natural and intuitive concepts from graph theory. In Section 4, we study a semantic mapping from our theory to PSL-core theory. Section 5 contains our concluding remarks.
Robust Benchmarking for Machine Learning of Clinical Entity Extraction<|sep|>Free text clinical notes contain a rich narrative of a patients interactions with the healthcare system. However, the strengths and weaknesses of clinical notes lie in the diversity of natural language. Due to the ﬂexibility of documentation, notes contain information omitted from the structured clinical record, but the language itself is often hard to parse into a consistent structured format. For example, Cold can refer to the temperature, a temperament, the viral infection, or Chronic Obstructive Lung Disease. By matter of preference, a doctor can refer to a patient with a 101◦ F temperature as running a fever, being febrile, or having pyrexia. In order to transform text into a uniﬁed structured format useful for downstream applications, EHR text mining often involves recognizing spans representing concepts (named entity recognition) and mapping these spans to a common vocabulary (named entity normalization/linkage). We will refer to both steps together as clinical entity extraction. An example of this two-step process is shown below in Figure 1. Typically, in medicine, concepts are mapped to terms in the Uniﬁed Medical Language System (UMLS), each term denoted by a Concept Unique Identiﬁer (CUI) (Bodenreider, 2004). Figure 1: Pipeline of extracting UMLS concepts from text. Typically, this is broken down into two steps as shown: recognition (detecting relevant spans) and normalization (mapping spans to a vocabulary). Normalized clinical entities can be used as direct input for retrospective analyses and as an additional source of information when addressing confounders. For example, clinical entities can help learn adverse drug events or a health knowledge graph (Jensen et al., 2012; Rotmensch et al., 2017; Nadkarni et al., 2011); or at point-of-care, e.g. as a step in matching patients to relevant clinical trials (Demner-Fushman et al., 2009). Additionally, causal inference and reinforcement learning must both account for unobserved confounders aﬀecting both medical intervention and patient outcome (Gottesman et al., 2019), information often contained in normalized clinical entities. Structured text data could reveal relevant patient history or additional interventions omitted from the structured clinical record. Because normalizing entities condenses high-dimensional raw text into a lower-dimensional space by collapsing synonyms, relevant factors from normalized entities could be plausibly integrated into a model’s state space. In a similar vein, Oakden-Rayner et al. (2020) found that machine learning models trained on medical images suﬀer from hidden stratiﬁcation, where performance depends on unknown confounders. In particular, the authors found that models for predicting pneumothorax from X-rays were partially picking up on previously placed chest drains. The presence of these chest drains is often noted in the accompanying radiology notes. As before, ﬁnding clinical entities associated with abnormally high or low performance could identify unexpected confounders. Such multimodal sanity checks are particularly important given the high-stakes nature of machine learning for healthcare. Despite the importance of clinical entity extraction for a range of tasks, it still remains largely an open problem, and clinical research often has to resort to manual chart review for high-ﬁdelity data, which is tedious and diﬃcult to scale. While there has been a signiﬁcant, concerted eﬀort from the clinical natural language processing (NLP) community to bridge this gap, performance falters due to (i) the lack of suﬃcient annotated training data, (ii) the huge space of clinical entities to map to, and (iii) known issues with medical vocabularies. In this work, we investigate, quantify performance of, and indicate areas of improvements for the current state of normalization by analyzing performance on the Medical Concept Normalization (MCN) Corpus (Luo et al., 2019). We focus on a subset of CUIs in the UMLS: the SNOMED Clinical Terms, which consist of over 400,000 common clinical concepts; and RxNorm, a comprehensive clinical drug vocabulary (Luo et al., 2019; NIH-NLM, 2015; Nelson et al., 2011). We examine top systems for clinical entity normalization from the 2019 National NLP Clinical Challenges (n2c2) Shared Task as well as two widely used end-to-end open-source systems, cTAKES and MetaMap (Aronson and Lang, 2010; Savova et al., 2010). These analyses reveal holes in performance, underscore the need for future dataset creation and method development in this space, and highlight the need for a new evaluation framework. In their description of the MCN corpus, Luo et al. (2019) recounted how their corpus annotation eﬀort was hindered by several issues including imperfections of UMLS. Therefore, we additionally develop a new annotation framework to address these and other identiﬁed issues. We demonstrate how our new schema adjusts to these issues and allows for more ﬂexible evaluation of end-to-end systems. Due to its utility, clinical entity extraction is widely used as an initial step in many machine learning for healthcare pipelines. As the ﬁeld increasingly turns to multimodal data, clinical text will become a continuously more important component of ML systems. In this work, we show that performance across normalization and end-to-end systems diﬀer signiﬁcantly on various subsets of the data. Given this observed heterogeneity in performance, it is incredibly important for users of such systems to consider how well they extract concepts relevant to their speciﬁc use cases. Unfortunately, a full evaluation of these systems is currently unachievable since a publicly available reference standard for end-to-end benchmarking does not exist. Such reference standards have been crucial to progress in other ﬁelds in machine learning, e.g. computer vision through ImageNet (Deng et al., 2010). In this work, we propose recommendations to the clinical NLP community regarding a path forward for the creation of that reference standard. We detail a robust annotation framework that by construction allows for end-to-end evaluation and reduces ambiguity for annotators.
Large-N behavior of three-dimensional lattice CP(N-1) models<|sep|>Models that are invariant under a local U(1) gauge symmetry have been systematically studied in condensed-matter and in high-energy physics. The simplest model with a local U(1) gauge symmetry is the CPN−1 model. In three dimensions it emerges as an eﬀective theory describing several condensed-matter systems [1, 2, 3, 4, 5, 6, 7], while in two dimensions it is an interesting theoretical laboratory to study quantum ﬁeld theories of fundamental interactions as it shares several features with quantum chromodynamics (QCD), the theory that describes the hadronic strong interactions [8, 9]. A lattice formulation of the CPN−1 model is obtained by associating complex Ncomponent unit vectors zx with the sites x of a cubic lattice, and U(1) variables λx,µ with each link connecting the site x with the site x + ˆµ (where ˆµ = ˆ1, ˆ2, . . . are unit vectors along the lattice directions). The partition function of the system reads and the sum runs over all lattice links. Such a model can also be seen as a limiting case of the lattice abelian Higgs model in which the gauge ﬁelds become dynamical, see [10] and references therein. In three dimensions CPN−1 models are expected to undergo a ﬁnite-temperature transition. The associated order parameter is the gauge-invariant quantity In the high-temperature (HT) phase, the system is disordered and ⟨Qab⟩ = 0, while in the low-temperature (LT) phase the parameter Qab magnetizes. In spite of extensive ﬁeld-theoretical and numerical studies for N = 2, 3, 4 and N → ∞, the nature of the transition is still controversial [9, 11, 12, 13], and in particular, it is not clear whether CPN−1 universality classes exist for N > 2. Model (2) was numerically studied in [13] for N = 2, 3, 4. It was shown that the system undergoes a continuous transition for N = 2, in the O(3) universality class. For N = 3 and 4, it undergoes a ﬁrst-order transition. No numerical results are available for larger values of N, although both existing lattice and continuum analytic computations predict a continuous transition for N = ∞. Consistency between the large-N prediction and the N = 3, 4 numerical results requires the existence of a critical number of components Nc, such that the transition is of ﬁrst order for 2 < N < Nc and continuous for N > Nc. The universality class of the critical transition for N > Nc would then be naturally identiﬁed with that associated with the large-N ﬁxed point occurring in the continuum abelian-Higgs model [14, 15, 16] and which exists for N ≥ Nc0. The estimate Nc0 = 12.2(3.9) was obtained by an analysis of the ǫ expansion up to four loops [16]. Of course, the critical number Nc should be larger than or equal to Nc0. Therefore, a numerical study of the model for quite large values of N is required. The purpose of this paper is that of determining the nature of the transition in the model with Hamiltonian (2) for large values of N. We report results for 7 ≤ N ≤ 100, which are all consistent with a simple scenario in which the transition is always of ﬁrst order, even for N → ∞. This conclusion contradicts the analytic calculations for N = ∞, forcing us to review the assumptions that are generally made in the standard large-N approach [9, 17, 18, 13]. In particular, we verify that one crucial assumption in these calculations is not correct. All calculations assume that the gauge ﬁelds order for N → ∞, i.e., that one can set λx,µ = 1 in this limit. We ﬁnd that this assumption is correct in the low-temperature phase, but not in the high-temperature phase. In the latter one, gauge ﬁelds as well as gauge-invariant observables remain uncorrelated up to the transition point. The transition is therefore of ﬁrst order. The paper is organized as follows. In Sec. 2 we present our numerical large-N results. In Sec. 2.1 we give some details on the numerical simulations and deﬁne the observables we measure in the Monte Carlo (MC) simulations. In Sec. 2.2 we present the numerical results at the transition, while in Sec. 2.3 we discuss the nature of the two phases. In Sec. 3 we review the large-N analytic calculations. Finally, in Sec. 4 we summarize and present our conclusions.
Closed-loop robots driven by short-term synaptic plasticity: Emergent explorative vs. limit-cycle locomotion<|sep|>It has been argued (Pfeifer et al., 2007; Aguilar et al., 2016) that ‘robophysics’, deﬁned as the pursuit of the discovery of biologically inspired principles of self generated motion, may constitute a promising road for eventually achieving life-like locomotor abilities. Distinct principles such as predictive information (Ay et al., 2008), surprise minimization (Friston, 2011), chaos control (Steingrube et al., 2010), empowerment (Salge et al., 2014), homeokinesis (Der and Martius, 2012), cheap design (Mont´ufar et al., 2015), and curiosity (Frank et al., 2014) have been studied in this context. Behavior, resulting from guided self organization (Prokopenko, 2009) or autonomous adaption (Chiel and Beer, 1997), may be generated in addition through suitable synaptic (Der and Martius, 2015; Der, 2016) and intrinsic (S´andor et al., 2015) plasticity rules. Here we point out, that complex dynamics may be generated through a transient plasticity mechanism widely present in the brain. Short-term synaptic plasticity (STSP) (Fioravante and Regehr, 2011; Regehr, 2012) is an activity induced transient modulation of the synaptic efﬁciency, which may lead either to facilitating or to depressing behavior lasting from a few hundred to a few thousand milliseconds. STSP has been argued, besides others, to be relevant or causal for working memory (Barak and Tsodyks, 2014), for the facilitation of time sequences of alternating neural populations (Carrillo-Reid et al., 2015), for motor control in general (Nadim and Manor, 2000), and for the sculpting of rhythmic motor patterns (Jia and Parker, 2016) in particular. Plasticity mechanisms similar to STSP have also been shown to allow for stable gaits (Toutounji and Pasemann, 2014) in neural networks which are distinctively simpler than the ones used conventionally for bio-inspired controllers (Schilling et al., 2013). In this study we use the LPZRobots physics simulation package (Der and Martius, 2012) for the investigation of the spherical three-axis robot illustrated in Fig. 1. This robot is driven exclusively by STSP, with locomotion coming to a stillstand both in the absence of synaptic plasticity and when the feedback from the environment is cut off, e.g. when the gravitational constant is set to zero. We ﬁnd a surprisingly large palette of self-organized motion primitives, which includes a chaotic phase. The locomotion observed is ﬂexible, in all modes, readjusting seamlessly to disturbances like the collision of the robot with obstacles. The capability of STSP to have a large impact on locomotion can be traced back in our analysis to the destabilizing effect short-term synaptic plasticity may have on attracting states of the controlling network, inducing attractor-to-attractor transitions within timescales of the order of a few hundred milliseconds. We corroborate this ﬁndings by short-circuiting the sensori-motor loop, viz by taking out the environment. Transitions between distinct limit cycles within the full sensori-motor loop are found in addition in the chaotic mode.
QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding<|sep|>The surge of massive data has led to signiﬁcant interest in distributed algorithms for scaling computations in the context of machine learning and optimization. In this context, much attention has been devoted to scaling large-scale stochastic gradient descent (SGD) algorithms [33], which can be brieﬂy deﬁned as follows. Let f : Rn → R be a function which we want to minimize. We have access to stochastic gradients �g such that E[�g(x)] = ∇f(x). A standard instance of SGD will converge towards the minimum by iterating the procedure where xt is the current candidate, and ηt is a variable step-size parameter. Notably, this arises if we are given i.i.d. data points X1, . . . , Xm generated from an unknown distribution D, and a loss function ℓ(X, θ), which measures the loss of the model θ at data point X. We wish to ﬁnd a model θ∗ which minimizes f(θ) = EX∼D[ℓ(X, θ)], the expected loss to the data. This framework captures many fundamental tasks, such as neural network training. In this paper, we focus on parallel SGD methods, which have received considerable attention recently due to their high scalability [6, 8, 13, 32]. Speciﬁcally, we consider a setting where a large dataset is partitioned among K processors, which collectively minimize a function f. Each processor maintains a local copy of the parameter vector xt; in each iteration, it obtains a new stochastic gradient update (corresponding to its local data). Processors then broadcast their gradient updates to their peers, and aggregate the gradients to compute the new iterate xt+1. In most current implementations of parallel SGD, in each iteration, each processor must communicate its entire gradient update to all other processors. If the gradient vector is dense, each processor will need to send and receive n ﬂoating-point numbers per iteration to/from each peer to communicate the gradients and maintain the parameter vector x. In practical applications, communicating the gradients in each iteration has been observed to be a signiﬁcant performance bottleneck [8, 35, 37]. One popular way to reduce this cost has been to perform lossy compression of the gradients [1, 3, 10, 11, 41]. A simple implementation is to simply reduce precision of the representation, which has been shown to converge under convexity and sparsity assumptions [10]. A more drastic quantization technique is 1BitSGD [35, 37], which reduces each component of the gradient to just its sign (one bit), scaled by the average over the coordinates of �g, accumulating errors locally. 1BitSGD was experimentally observed to preserve convergence [35], under certain conditions; thanks to the reduction in communication, it enabled state-of-the-art scaling of deep neural networks (DNNs) for acoustic modelling [37]. However, it is currently not known if 1BitSGD provides any guarantees, even under strong assumptions, and it is not clear if higher compression is achievable. Contributions. Our focus is understanding the trade-offs between the communication cost of dataparallel SGD, and its convergence guarantees. We propose a family of algorithms allowing for lossy compression of gradients called Quantized SGD (QSGD), by which processors can trade-off the number of bits communicated per iteration with the variance added to the process. QSGD is built on two algorithmic ideas. The ﬁrst is an intuitive stochastic quantization scheme: given the gradient vector at a processor, we quantize each component by randomized rounding to a discrete set of values, in a principled way which preserves the statistical properties of the original. The second step is an efﬁcient lossless code for quantized gradients, which exploits their statistical properties to generate efﬁcient encodings. Our analysis gives tight bounds on the precision-variance trade-off induced by QSGD. At one extreme of this trade-off, we can guarantee that each processor transmits at most √n(log n + O(1)) expected bits per iteration, while increasing variance by at most a √n multiplicative factor. At the other extreme, we show that each processor can transmit ≤ 2.8n + 32 bits per iteration in expectation, while increasing variance by a only a factor of 2. In particular, in the latter regime, compared to full precision SGD, we use ≈ 2.8n bits of communication per iteration as opposed to 32n bits, and guarantee at most 2× more iterations, leading to bandwidth savings of ≈ 5.7×. QSGD is fairly general: it can also be shown to converge, under assumptions, to local minima for nonconvex objectives, as well as under asynchronous iterations. One non-trivial extension we develop is a stochastic variance-reduced [23] variant of QSGD, called QSVRG, which has exponential convergence rate. One key question is whether QSGD’s compression-variance trade-off is inherent: for instance, does any algorithm guaranteeing at most constant variance blowup need to transmit Ω(n) bits per iteration? The answer is positive: improving asymptotically upon this trade-off would break the communication complexity lower bound of distributed mean estimation (see [44, Proposition 2] and [38]). Experiments. The crucial question is whether, in practice, QSGD can reduce communication cost by enough to offset the overhead of any additional iterations to convergence. The answer is yes. We explore the practicality of QSGD on a variety of state-of-the-art datasets and machine learning models: we examine its performance in training networks for image classiﬁcation tasks (AlexNet, Inception, ResNet, and VGG) on the ImageNet [12] and CIFAR-10 [25] datasets, as well as on LSTMs [19] for speech recognition. We implement QSGD in Microsoft CNTK [3]. Experiments show that all these models can signiﬁcantly beneﬁt from reduced communication when doing multi-GPU training, with virtually no accuracy loss, and under standard parameters. For example, when training AlexNet on 16 GPUs with standard parameters, the reduction in communication time is 4×, and the reduction in training to the network’s top accuracy is 2.5×. When training an LSTM on two GPUs, the reduction in communication time is 6.8×, while the reduction in training time to the same target accuracy is 2.7×. Further, even computationally-heavy architectures such as Inception and ResNet can beneﬁt from the reduction in communication: on 16GPUs, QSGD reduces the end-to-end convergence time of ResNet152 by approximately 2×. Networks trained with QSGD can converge to virtually the same accuracy as full-precision variants, and that gradient quantization may even slightly improve accuracy in some settings. Related Work. One line of related research studies the communication complexity of convex optimization. In particular, [40] studied two-processor convex minimization in the same model, provided a lower bound of Ω(n(log n + log(1/ϵ))) bits on the communication cost of n-dimensional convex problems, and proposed a non-stochastic algorithm for strongly convex problems, whose communication cost is within a log factor of the lower bound. By contrast, our focus is on stochastic gradient methods. Recent work [5] focused on round complexity lower bounds on the number of communication rounds necessary for convex learning. Buckwild! [10] was the ﬁrst to consider the convergence guarantees of low-precision SGD. It gave upper bounds on the error probability of SGD, assuming unbiased stochastic quantization, convexity, and gradient sparsity, and showed signiﬁcant speedup when solving convex problems on CPUs. QSGD reﬁnes these results by focusing on the trade-off between communication and convergence. We view quantization as an independent source of variance for SGD, which allows us to employ standard convergence results [7]. The main differences from Buckwild! are that 1) we focus on the variance-precision trade-off; 2) our results apply to the quantized non-convex case; 3) we validate the practicality of our scheme on neural network training on GPUs. Concurrent work proposes TernGrad [41], which starts from a similar stochastic quantization, but focuses on the case where individual gradient components can have only three possible values. They show that signiﬁcant speedups can be achieved on TensorFlow [1], while maintaining accuracy within a few percentage points relative to full precision. The main differences to our work are: 1) our implementation guarantees convergence under standard assumptions; 2) we strive to provide a black-box compression technique, with no additional hyperparameters to tune; 3) experimentally, QSGD maintains the same accuracy within the same target number of epochs; for this, we allow gradients to have larger bit width; 4) our experiments focus on the single-machine multi-GPU case. We note that QSGD can be applied to solve the distributed mean estimation problem [24, 38] with an optimal error-communication trade-off in some regimes. In contrast to the elegant random rotation solution presented in [38], QSGD employs quantization and Elias coding. Our use case is different from the federated learning application of [24, 38], and has the advantage of being more efﬁcient to compute on a GPU. There is an extremely rich area studying algorithms and systems for efﬁcient distributed large-scale learning, e.g. [1, 3, 6, 10, 11, 21, 32, 39, 43]. Signiﬁcant interest has recently been dedicated to quantized frameworks, both for inference, e.g., [1, 17] and training [10, 16, 20, 35, 37, 42, 45]. In this context, [35] proposed 1BitSGD, a heuristic for compressing gradients in SGD, inspired by delta-sigma modulation [34]. It is implemented in Microsoft CNTK, and has a cost of n bits and two ﬂoats per iteration. Variants of it were shown to perform well on large-scale Amazon datasets by [37]. Compared to 1BitSGD, QSGD can achieve asymptotically higher compression, provably converges under standard assumptions, and shows superior practical performance in some cases.
A Bayesian Foundation for Physical Theories<|sep|>Science is fundamentally based on the processing of information accumulated as we humans continually observe the endless spectrum of natural phenomena occurring in our universe. The philosophical foundations of the task of trying to make sense of all this collected information in a rational way, which received the name of scientiﬁc method, were constructed based on the principle that scientiﬁc knowledge should rely entirely on the empirical evidence collected from these observed phenomena and that the theories elaborated to explain them should not make any unnecessary assumption beyond what is required to account for the facts. Constructing a scientiﬁc theory would then be equivalent to inferring patterns, or an underlying connecting structure, from this acquired information. The very hypothesis of this possibility relies on the also empirical observation of the regularity and predictability of nature. The above paragraph is full of ill-deﬁned concepts, but they contain the essence of what we would require from an ideal scientiﬁc theory. One of our aims in this work will be to give more precise deﬁnitions not only to the above presented ideas, but also to many others that appear in connection with physics in particular and with science in general. It is important to clarify at this point what the theory to be developed here is not. The present work is not a sociological theory of how scientists develop the scientiﬁc knowledge. It is not concerned with historical details of how theories were discovered or developed and how were the mental processes that resulted in their ﬁnal formulations. A theory like that would have to take into consideration not only sociological and psychological aspects, but also their complex relations with the economical and political environment in which the theories appeared. The theory, although a better word would be framework, to be developed here will be concerned with what could be considered the ideal mathematical methods that should be used by the scientiﬁc inquiry and, if not, will lead to wrong or suboptimal results. The author thinks that he does not have the proper training to analyse the historical contingencies of the development of theories by humans and that, in fact, social scientists and historians are better suited to this task. That said, let us point out that physics can be considered the best example of the application of the scientiﬁc method, providing us with descriptions of nature with astonishing quantitative accuracy as in the well-known case of quantum electrodynamics. The success of physics is indisputable as can be easily attested by the improvement of quality of life brought by our present technology. This high eﬃciency of physics is a direct result of taking seriously the basic tenets of the scientiﬁc method to discern between what should be considered science and what should not. One of the most fundamental of these criteria is the concept of falsiﬁability, which is a central point in the Popperian description of science [1]. Falsiﬁability can be a diﬃcult concept to identify in theoretical physics, specially when the theoretical advancements run ahead of the experimental technology, as is the present situation with respect for instance to quantum gravity, where the diﬃculty of falsifying tentative theories is well recognised [2]. Partly because of that, physicists rely also on other characteristics of physical theories, like mathematical simplicity and elegance, as guides in the development of theories even if they are not rigorously deﬁned concepts. Obviously it must be always stressed that these criteria are only guidelines that must be discarded if experiments turn out to disprove them. A rather illus trative example, although apparently nonsensical for today scientists, is the idea that every substance is composed of a mixture of only four elements: water, ﬁre, air and earth. This can be considered a rather elegant theory, as it requires only four basic elements in comparison with the more than one hundred elements known today, but nonetheless it was proved wrong beyond doubt by the overwhelming experimental evidence accumulated since its formulation. Given that what tells science apart from mysticism and other non-scientiﬁc human activities is the strict reliance on rational thought, many attempts were made to formalise the scientiﬁc method using inductive logic, with deductive logic as a particular case (see text and references in [3]). Salmon [4] is a good example of one of ﬁrst authors to analyse concepts involved in inductive logic applied to scientiﬁc explanations with the introduction of his idea of statistical relevance. Although since the very early stages it was recognised that inductive, probabilistic reasoning should underpin these efforts, it is fair to say that the correct formulation was only achieved with the modern development of Bayesian inference [5]. The work of Salmon, for instance, can be neatly mapped to Bayesian language with appropriate care as he himself suggests in the above cited book. The application of Bayesian inference to the problem of formalisation of the scientiﬁc method however has not yet made use of the full power of the theory, although there exists at least one very developed tentative framework, called Minimum Description Length (MDL) [6]. MDL fuses concepts of information theory, computation and probability theory in an attempt to develop a method to decide between theories given some data. The main idea of MDL is to choose the best hypothesis by calculating the length of the smallest description in a certain code simultaneously of the hypothesis and the data when compressed using the hypothesis itself. We will not give any detailed account of MDL here, but a comparison of its main points with the work we develop here will be made in section 9. It can be shown that Bayesian inference is the only way to deal with information that comply with a sensible set of requirements for inductive and logical reasoning [5,7,8]. Although many misconceptions about the Bayesian interpretation of probability are still used against it on fundamental grounds, with strong attacks against its use and the use of probabilistic reasoning in general within the scientiﬁc method [9], the success of Bayesian methods in machine learning applications, which are ultimately an attempt to understand and formalise an ideal process of thinking, is an experimental fact [10, 11]. These fundamental questions will not be discussed here and the interested reader can refer to Jaynes’ book [5] for a thorough study of these and other fundamental points. A huge supporting bibliography containing all the relevant works can also be found there and, in order to save space, will not be presented here. In this work, we will ﬁrst use the Bayesian inference framework to introduce our reasoning that will lead to a formalisation of a concept of scientiﬁc method concept, which is done in section 2. The basic processes that compose the scientiﬁc methodology according to this framework, which will be called information acquisition, modeling and testing/selection, are there introduced and brieﬂy commented. The precise meaning of information acquisition is explained in section 3. Section 4 then analyses the modeling process of theories in a more detailed way. Testing, which we will use as a short word for testing/selection, is studied in section 5, after which follows a discussion about the important related concept of falsiﬁability in section 6. In particular, this latter section will contain the two most important deﬁnitions of this paper, those of a scientiﬁc theory and of the scientiﬁc method. These sections contain the core of our framework and the next sections will be concerned with applications. In section 7 the formalism developed here is used to analyse three current cosmological issues in theoretical physics, namely the question of typicality of human observers, the preference for a multiverse description and a brief analysis of the meaning of the (weak) anthropic principle. Section 8 will introduce the isolated worlds problem and show how the present framework yields a solution that extends the applicability of scientiﬁc method beyond today’s range. A discussion and the ﬁnal conclusions are given in section 9.
Probabilistic Handshake in All-to-all Broadcast Coded Slotted ALOHA<|sep|>Vehicular communications (VCs) is presently one of the most challenging problems of communication engineering. Its deployment will enable numerous applications, such as intelligent transportation systems, autonomous driving, and, most importantly, trafﬁc safety. VCs entails a number of challenges, such as all-to-all communication, high mobility networks with rapidly changing topologies and a large number of users, and poor channel quality. These challenges require new ideas and designs at the physical and the medium access control (MAC) layers. The main requirements for VCs are high reliability and low latency. Furthermore, the aforementioned challenges prohibit the classical use of acknowledgements in the form of additional signaling. A novel MAC protocol called all-to-all broadcast coded slotted ALOHA (B-CSA) was proposed by the authors in [1], which was shown to be able to satisfy the reliability and latency requirements in rough conditions of vehicular networks under a set of idealized assumptions, such as perfect interference cancellation. Originally proposed for a unicast scenario, coded slotted ALOHA (CSA) can provide large throughputs close to those of coordinated schemes [2], [3]. Different versions of CSA have been proposed (see [4] for the most recent review). All of them share a slotted structure borrowed from the original slotted ALOHA [5] and the use of This research was supported by the Swedish Research Council, Sweden, under Grant No. 2011-5950, the Ericsson’s Research Foundation, Sweden, Chalmers Antenna Systems Excellence Center in the project ‘Antenna Systems for V2X Communication’, and the Danish Council for Independent Research, under Grants No. 11-105159 and No. 4005-00281. successive interference cancellation. The contending users introduce redundancy by encoding their messages into multiple packets, which are transmitted in randomly chosen slots. In the unicast scenario, the base station (BS) buffers the received signal, decodes the packets from the slots with no collision and attempts to reconstruct the packets in collision exploiting the introduced redundancy. A packet that is reconstructed is subtracted from the buffered signal and the BS proceeds with another decoding round. In contrast to classical CSA, where a BS is the intended recipient of the messages, in B-CSA each user acts as both receiver and transmitter. Every user is equipped with a halfduplex transceiver, so that a user cannot receive packets in the slots it uses for transmission. This can be modeled as a packet erasure channel [6] and it affects the design and the performance analysis as compared to classical CSA. Whereas B-CSA can provide high reliability, the rear communication failure events may be extremely costly in safety applications. Since providing error-free communication under the described conditions of VCs is not possible, one may attempt to detect communication failure events to use this information in the application level. For instance, consider the scenario where two users A and B are heading towards each other. If A receives a message from B and obtains the information that B failed to receive its message, A can take extra precautions to avoid collision with B. In this paper, we propose an algorithm to obtain this information based on the by-product of decoding, i.e., no extra signaling is used by the users. In particular, A uses the knowledge of B’s transmissions to detect that B failed to resolve A. The problem studied in this paper resembles the one of handshake used for establishing connections in connectionoriented protocols. In transmission control protocol (TCP), a three-way handshake is used by a pair of users to exchange their messages and to acknowledge that the messages were received [7]. If the messages in the TCP level are exchanged, then the handshake is always performed successfully. In the proposed algorithm, however, the decision about successful handshake can be in error, which indicates its probabilistic nature.
A New Automatic Method to Identify Galaxy Mergers I. Description and Application to the STAGES Survey<|sep|>Mergers are the most extreme type of galaxy interaction, as the ﬁnal product of a merger event can be totally different from the original objects involved. Considerable efforts have been devoted towards the understanding of the physical processes that regulate galaxy mergers starting from the very early work of Spitzer & Baade (1951) or the seminal simulational works presented in Toomre & Toomre (1972); Toomre (1977). These work made it clear that, even though the stars rarely collide with each other during a merger process, such episodes can have dramatic consequences for the gaseous component of the galaxies involved. Later works such as those of Barnes & Hernquist (1991, 1992, 1996); Barnes (2002); Bournaud et al. (2005); Wetzstein, Naab, & Burkert (2007); Springel & Hernquist (2005); Bournaud et al. (2008); Hopkins et al. (2009); Stewart et al. (2009); Chilingarian et al. (2010) have helped address speciﬁc issues of merger processes such as the internal structure of the remnants, the relevance of the orbital parameters or the impact of the gas fraction on the possible regeneration of galactic discs after a merger episode. However, the study of mergers is not only relevant because of the physics involved. The evolution of the massive early type galaxies that populate the red sequence cannot be explained using passive evolution models only, and mergers have been found to play a key role in their evolution. In particular, the evolution of the luminosity function and colours of galaxies since z ≃ 1.0 observed in the COMBO-17 (Classifying Objects by Medium-Band Observations in 17 Filters) (Wolf et al. 2003; Bell et al. 2004) and the phase 2 of the DEEP (Deep Extragalactic Evolutionary Probe) survey (Faber et al. 2007) suggest that the merger episodes have a huge impact on the evolution of early-type galaxies, increasing the stellar mass by a factor of two over the last 8Gyr. More recently, the importance of mergers has been highlighted in Robaina et al. (2010), who conclude that the evolution of massive, red galaxies depends strongly on their merging history. Further studies have helped ascertain the impact of mergers in speciﬁc aspects of the evolution of red galaxies, including masses (van Dokkum et al. 2010), sizes (see Trujillo et al. 2006, 2007; Giavalisco, Ravindranath, & Daddi 2007; Buitrago et al. 2008; van Dokkum et al. 2008, among others) and velocity dispersions (Cenarro & Trujillo 2009). The identiﬁcation of mergers in deep astronomical images is thus a very important issue for galaxy evolution studies, and the huge number of galaxies observed in modern surveys creates the need of reliable automated merger detection mechanisms. A reliable merger identiﬁcation technique is the key element in the calculation of the merger fraction. The merger fraction is deﬁned as the fraction of galaxies with a recognizable ongoing merger episode that is found in any given (often mass-limited) sample. It is the ﬁrst step towards the comoving merger rate, which is the number of merger events per Mpc−3Gyr−1 . Several automatic identiﬁcation techniques have been developed to single out mergers from non interacting galaxies. These methods use morphological criteria (CAS and G− M20 systems, Conselice 2003; Conselice, Rajgor, & Myers 2008; Lavery et al. 2004; Cassata et al. 2005; Lotz et al. 2004, 2008; Jogee et al. 2009), kinematical and spatial close pairs (Patton et al. 2000, 2002; Lin et al. 2004; De Propris et al. 2007; Lin et al. 2008; Ellison et al. 2010; Robaina et al. 2010), or even the correlation function (Bell et al. 2006; Masjedi et al. 2006). The morphological techniques are based on the fact that the objects involved in a merger episode will be gravitationally disturbed. The CAS (Concentration, Asymmetry, clumpineSs) system measures these speciﬁc aspects of the surface brightness distribution of galaxies in order to identify mergers. Objects with high asymmetries are usually taken to be mergers by this method. On the other hand, the G − M20 system measures whether the galaxies appear to be shredded or not, since both the Gini and the M20 numbers measure whether and how is the light concentrated in any given object. In this system, the more shredded galaxies are selected as mergers. The pairing techniques look for pairs of galaxies whose relative positions and velocities should be conducive to strong interactions in a relatively short timescale after observation. Each of these methodologies are sensitive to diﬀerent time scales, mass ratios, orbital parameters, and gaseous content of the galaxies involved. For instance, Conselice (2006); Lotz et al. (2008) conclude that the CAS parameters are sensitive to roughly a timescale of 0.4 → 1.0 × 109 years, while the time sensitivity of the pairing techniques depend on the projected separation between the galaxies. This work contributes to the morphological automated detection of mergers. Here, the morphological parameters of the residual images after the subtraction of a smooth S´ersic model (see S´ersic 1963, for a deﬁnition of this proﬁle) are explored. This should, at least in principle, better reveal the impact of the gravitational interaction on the morphology of galaxies. This was done with the aspiration of detecting minor mergers. An isolated galaxy will, with time, adopt an approximately symmetric proﬁle whereas an interacting galaxy will appear to be more asymmetric. The removal of an intrinsically symmetric proﬁle such as the S´ersic model, which could be regarded as the quiescent, underlying galaxy, will more clearly expose the asymmetric signature of the light from an interacting galaxy. Thus, the structure of the residuals is investigated with the aim of ﬁnding the combination of structural parameters that produces merger samples of better statistical quality. This optimization step is done in an unambiguous way, by using an objective criterion to grade the performance of the diagnostics tried. The speciﬁc criterion used here encourages completeness at the expense of a fairly high contamination by non-mergers, and the resulting merger sample needs to be cleaned afterwards. In this paper, §2 presents the observational data used together with the galaxy samples selected to derive and then test the proposed method. Section §3 describes the data processing techniques employed, and the structural parameters used. Section §4 presents the objective method introduced here to determine what combination of structural parameters produces the merger sample of highest statistical quality. The precise deﬁnition of the “statistical quality” of a sample that is selected from a parent population is given in §4.1. Section §5 presents a visual analysis of the objects selected as potential mergers by the method presented here, focusing on the contamination of the resulting sets of merger galaxies by non-mergers (§5.1) and on the ability to detect minor mergers (§5.2). Finally, §6 presents the conclusions of this work.
Josephson effect in topological superconducting rings coupled to a microwave cavity<|sep|>Majorana fermions are zero energy quasiparticles that are their own antiparticles. In condensed matter physics, they emerge as zero energy excitations in the so called topological superconductors, such as genuine p-wave superconductors1–4, topological insulators in the presence of conventional superconductors5–7, nanowire in proximity to s-wave superconductor and subject to a magnetic ﬁeld8–11, in chains of magnetic atoms12–18. Majorana fermions in condensed matter physics were ﬁrst predicted by Kitaev in Ref. 3 (see Ref. 19 for a review). They obey non-abelian statistics2 in two dimensions, and they are robust against local perturbations that do not close the superconducting gap, which make them promising candidates as qubits for a functional topological quantum computer. There are several physical manifestations associated with the presence of Majorana fermions, such as the zero bias anomaly in the topological region, the dependence of the Majorana energy splitting on the chemical potential, Zeeman splitting, or the size of the system, as well as the fractional Josephson eﬀect and the non-abelian statistics pertaining to braiding of the Majoranas19. The fractional Josephson eﬀect is one of the hallmarks of the Majorana fermions3,4,6,9,20. It corresponds to 4π periodicity of the supercurrent in the superconductor phase diﬀerence across the weak link, as opposed to 2π periodicity for a usual Josephson eﬀect in conventional s-wave superconductors, and it arises because of the degeneracy of the zero energy Majorana modes. In other words, because of such degeneracy, there is a coherent charge e transfer across the weak link, instead of 2e as in the normal Josephson eﬀect. There are various ways to reveal the fractional Josephson eﬀect, such as the measurement of the Shapiro steps in the I − V characteristics of a voltage biased junction21,22, by employing a resistive shunted SQUID setup23, or by performing microwave spectroscopy of the junction. The latter method is particularly interesting as it allows to access also the exited states of the junction. There are several theoretical works that study the interplay of the Majorana fermions physics and microwaves in a superconducting cavity QED setup 24–34. In contrast to the usual electronic methods, this approach is unique in that it can be totally non-invasive, i.e. it does not alter the electronic system, a crucial feature in view of using these excitations for quantum computing. However, almost all of these studies deal with eﬀective models that consider only the Majorana fermions interacting with the cavity ﬁeld, which cannot account for the evolution of the Majorana fermions through the topological transition from a non-trivial to a trivial topological superconductor. More speciﬁcally, the Majorana fermions emerge as edge modes in a topological superconductor: below the phase transition the system is non-trivial, and shows edge modes, while above the phase transition the system is trivial, and is has no edge modes. Majorana fermions are thus intimately related to the bulk physics, and are not simply impurity states in the superconductor. The fractional Josephson eﬀect disappears above the phase transition, where it becomes the normal 2π periodic one. In order to account for the evolution of the system from topological to trivial, associated with the fractional and normal Josephson eﬀect, respectively, one needs to consider the Majorana and the bulk states on the same footing. In this paper, we do precisely that in the context of the cavity QED setup that was ﬁrst proposed in Ref . 35 and experimentally implemented in Ref. 36. The recent progress in engineering sensitive superconducting cavities makes it possible to implement on-chip mesoscopic circuits coupled to high ﬁnesse resonators37. In this paper, we study a mesoscopic superconducting ring with a weak link threaded by a magnetic ﬂux and coupled to the cavity. We consider an inductive coupling between the mesoscopic ring and the cavity38. We show that the electronic susceptibility has a diﬀerent periodicity as a function of the magnetic ﬂux in the topological and non-topological regions, thus allowing us to probe whether the wire is in topological or non
A CRC-aided Hybrid Decoding for Turbo Codes<|sep|>Channel codes with iterative decoding methods, such as turbo codes and LDPC codes, are extensively utilized in wireless communication systems. Turbo codes [1], as in the long-term evolution (LTE) protocols, can not only achieve high throughput with its parallel architecture, but also support almost any code rate and arbitrary block length from 40 bits to 6144 bits to cover various services in LTE systems. For the voice over internet protocol (VoIP) service in LTE, the transmission latency and quality of turbo coded data are particular critical. However, turbo codes with short information length usually suffer from severe performance degradation and can not guarantee the quality of the VoIP service, where the information length are limited from 40 bits to about 328 bits. On the other hand, the cyclic redundancy check (CRC) codes, conventionally for error detection, have no further ability to correct the transmission errors. But they can improve the power consumption and throughput of turbo decoder by early termination, and give indication for retransmission [2]–[5]. There are always 24 CRC bits attached after the information bits in LTE physical layer, where the mixed bit stream to be encoded is called a code block (CB). The CRC coding has considerable redundancy which can not be ignored, when the length of a CB is sufﬁciently short for some speciﬁc services like VoIP, especially when the user equipment (UE) is located at the edge of the cell and only small size of CBs can be scheduled due to limited signal to noise ratio [7]. As we can see, due to the CRC redundancy for short length of CBs, there exists an inherent performance gap between turbo decoding Y. Wei is with the Department of Wireless Research, Huawei Technologies, Shanghai, China (email: weiyuejun@huawei.com); M. Jiang is with National Mobile Communications Research Laboratory, Southeast University, Nanjing, China (e-mail: jiang ming@seu.edu.cn); W. Chen and Y. Yang are with the Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China (e-mail: {wenchen;yhyang}@sjtu.edu.cn). and maximum likelihood decoding (MLD) for concatenated turbo-CRC codes. Besides error detection, CRC codes can also give some help in decoding. Soft list Viterbi algorithm (SLVA) aided convolutional decoding [6] or turbo decoding [8] introduces an efﬁcient way to take the advantage of CRC error detection to narrow the performance gap between conventional decoding and MLD. However, the main contribution of SLVA in turbo decoding is limited to the improvement on the error ﬂoor. CRC codes can also be decoded by iterative decoding with soft output [9], which are considered as one component code serially concatenated with convolutional codes. Unfortunately, the parity check matrices (PCM) of the classic CRC codes are not appropriate for iterative decoding, since the density of PCM is not sparse enough and four-cycle-free can not be guaranteed at all. In this letter, we propose a CRC-aided hybrid approach for turbo-CRC codes, where the decoding scheme is hybrid with iterative-based standard turbo decoding (STD) and ordered statistics decoding (OSD) [10], [11]. Moreover, the hybrid decoding of concatenated turbo-CRC codes incorporates the CRC bits into the OSD process to further lower the error rate. Since the CRC bits participate in the error correction process, which makes them lose the error detection ability, we introduce an alternative method for error detection based on the normalized Euclidean distance (NED). Simulation results show that the proposed CRC-aided hybrid scheme can signiﬁcantly improve the decoding performance of turbo codes with short information length.
Shock Dynamics In Relativistic Jets<|sep|>Collimated outﬂows (with jet-like geometry) moving at relativistic speeds are characteristic of active galactic nuclei. It is commonly accepted that an extragalactic jet is produced in the neighborhood of a massive black hole in the center of an active galaxy (e.g. Rees 1984; Istomin 2010). These relativistic jets are subject to the development of shock waves. Rees (1978) proposed that the observed knots in the extragalactic jet M87 correspond to the locations of internal shocks which arise owing to variations in the outﬂow velocity of a beam generated in the nucleus. Later, Rees & M´esz´aros (1994) pointed out that ﬂuctuations of the Lorentz factor around its mean value in a relativistic outﬂow, that give rise to internal shocks, can dissipate a substantial fraction of the outﬂow energy into non-thermal radiation. They proposed that this mechanism is operating in the so-called gamma-ray bursts (GRBs). Several authors have studied internal shocks in ultrarelativistic outﬂows to explain the observed variability of GRBs (e.g., Mochkovitch, Maitia & Marques 1995; Kobayashi, Piran & Sari 1997; Daigne & Mochkovitch 1998; 2000). In these models the ﬂow is represented by a succession of shells with diﬀerent values of the Lorentz factor. This models reproduce the burst proﬁles and their short-time scale variability. Kobayashi et al. pointed out that variations of the relativistic ﬂow velocity are strongly correlated with the temporal variations observed in the GRBs. Daigne & Mochkovitch (1998) studied the detailed radiation processes to calculate the fraction of the kinetic energy dissipated in the shocks that can be emitted in the form of gamma rays and obtained that the total eﬃciency is of the order of only a few percent. In addition, Spada et al. (2001) proposed that the internal shock scenario can also be used for blazars. For comprehensive reviews of the physical processes and observations of GRBs see, e.g., M´esz´aros (2002); Piran (2004) ; and Gehrels et al. (2009). Using mass and momentum conservation, Cant´o et al. (2000) solved the dynamics of internal shocks of non rela
A Probabilistic Characterization of Random Proximity Catch Digraphs and the Associated Tools<|sep|>The proximity catch digraphs (PCDs) are a special type of proximity graphs which are based on proximity maps and are used in disciplines where shape and structure are crucial. Examples include computer vision (dot patterns), image analysis, pattern recognition (prototype selection), geography and cartography, visual perception, biology, etc. Proximity graphs were ﬁrst introduced by Toussaint (1980), who called them relative neighborhood graphs. The notion of relative neighborhood graph has been generalized in several directions and all of these graphs are now called proximity graphs. From a mathematical and algorithmic point of view, proximity graphs fall under the category of computational geometry. In recent years, a new classiﬁcation and spatial pattern analysis approach which is based on the relative positions of the data points from various classes has been developed. Priebe et al. (2001) introduced the class cover catch digraphs (CCCDs) and gave the exact and the asymptotic distribution of the domination number of the CCCD based on two data sets Xn and Ym both of which are random samples from uniform distribution on a compact interval in R. DeVinney et al. (2002), Marchette and Priebe (2003), Priebe et al. (2003b), Priebe et al. (2003a), and DeVinney and Priebe (2006) applied the concept in higher dimensions and demonstrated relatively good performance of CCCD in classiﬁcation. The employed methods involve data reduction (condensing) by using approximate minimum dominating sets as prototype sets, since ﬁnding the exact minimum dominating set is in general an NP-hard problem — in particular, for CCCD — (see DeVinney (2003)). Furthermore, the exact and the asymptotic distribution of the domination number of the CCCDs are not analytically tractable in higher dimensions. Ceyhan (2004) extended the concept of CCCDs by introducing PCDs, which do not suﬀer from some of the shortcomings of CCCDs in higher dimensions. In particular, two new types of PCDs (namely, proportional-edge and central similarity PCDs) are introduced; distribution of the domination number of proportional-edge PCDs is calculated, and is applied in testing spatial patterns of segregation and association (Ceyhan and Priebe (2005, 2007)). The distributions of the relative arc density of these PCD families are also derived and used for the same purpose (Ceyhan et al. (2006) and Ceyhan et al. (2007)). A general deﬁnition of proximity graphs is as follows: Let V be any ﬁnite or inﬁnite set of points in Rd. Each (unordered) pair of points (p, q) ∈ V × V is associated with a neighborhood N(p, q) ⊆ Rd. Let P be a property deﬁned on N = {N(p, q) : (p, q) ∈ V ×V }. A proximity (or neighborhood) graph GN,P(V, E) deﬁned by the property P is a graph with the set of vertices V and the set of edges E such that (p, q) ∈ E iﬀ N(p, q) satisﬁes property P. Examples of most commonly used proximity graphs are the Delaunay tessellation, the boundary of the convex hull, the Gabriel graph, relative neighborhood graph, Euclidean minimum spanning tree, and sphere of inﬂuence graph of a ﬁnite data set. See, e.g., Jaromczyk and Toussaint (1992). The relative allocation of the data points are used to construct a proximity digraph. A digraph is a directed graph, i.e., a graph with directed edges from one vertex to another based on a binary relation. Then the pair (p, q) ∈ V × V is an ordered pair and (p, q) is an arc (directed edge) denoted pq to reﬂect its diﬀerence from an edge. For example, the nearest neighbor (di)graph in Paterson and Yao (1992) is a proximity digraph. The nearest neighbor digraph, denoted NND(V ), has the vertex set V and pq an arc iﬀ d(p, q) = minv∈V \{p} d(p, v). That is, pq is an arc of NND(V ) iﬀ q is a nearest neighbor of p. Note that if pq is an arc in NND(V ), then (p, q) is an edge in RNG(V ). Our PCDs are based on the property P that is determined by the following mapping which is deﬁned in a more general space than Rd. Let (Ω, M) be a measurable space. The proximity map N(·) is given by N : Ω → ℘(Ω), where ℘(·) is the power set functional, and the proximity region of x ∈ Ω, denoted N(x), is the image of x ∈ Ω under N(·). The points in N(x) are thought of as being “closer” to x ∈ Ω than are the points in Ω \ N(x). Proximity maps are the building blocks of the proximity graphs of Toussaint (1980); an extensive survey is available in Jaromczyk and Toussaint (1992). The PCD D = (V, A) has the vertex set V = � p1, p2, . . . , pn � and the arc set A is deﬁned by pipj ∈ A iﬀ pj ∈ N(pi) for i ̸= j. Notice that D depends on the proximity map N(·), and if pj ∈ N(pi), then N(pi) is said to catch pj. Hence the name proximity catch digraph. If arcs of the form pipi (i.e., loops) were allowed, D would have been called a pseudodigraph according to some authors (see, e.g., Chartrand and Lesniak In this article, we provide a probabilistic characterization of the proximity maps, and the associated regions and PCDs, and introduce auxiliary tools for the PCDs. We deﬁne the proximity maps and datarandom PCDs in Section 2, describe the auxiliary tools (such as edge and vertex regions) for the construction of PCDs in Section 3, provide Γ1-regions and the related concepts for proximity maps in Section 4, discuss the examples of proximity maps in Delaunay triangles in Section 6, and provide the transformations preserving uniformity on triangles in R2 in Section 5. We investigate the characterization of proximity regions and the associated PCDs in Section 7, introduce Γk-regions for proximity maps in T (Y3) in Section 8, κ-values for the proximity maps in T (Y3) in Section 9, and provide discussion and conclusions in Section 10.
Filtering Procedures for Sensor Data in Basketball<|sep|>Professional team sports’ managers, more and more in recent years, are becoming aware of the potential of Data Analytics in order to better manage their team. In team sports, continuous interactions among three agents - coaches, single players and the whole team - produce an high level of complexity. This complexity has been studied, among others, in the new domain of ecological dynamics ([14, 1]). Nowadays, Information Technologies (IT) make large amounts of real-time information on teams and players available. Most of the results from the interaction among these three agents could be captured by two kind of data: (i) play-by-play data (also called event − log), which report a sequence of relevant events that occur during a match, related to either the team or the single player, such as shots or fouls; (ii) the positioning, the velocity and the acceleration of players or the ball, also called sensor data, which is captured through Global Positioning System (GPS) techniques. There is high potential in jointly using these two kind of data to cope with the intrinsic complexity in team sports and with the aim of producing advanced statistics for team managers. Several aspects are already taken into account in the scientiﬁc literature, [4] being a nice and quite complete review. For example,[2, 3] used data mining techniques in order to identify the drivers that mostly aﬀect the probability to win a football match. Social network analysis has also been used to capture the interactions between players ([15]); [10] used centrality measures with the aim of identifying central players in water polo. A necessary condition to produce statistics is to correctly understand data, by collecting, storing and processing them in a proper way. A review on this regard has been made by [10]. This paper is about basketball data processing. Basketball is a sport played by two teams of ﬁve players each on a rectangular court. The objective is to shoot a ball through a hoop 46 centimetres in diameter and mounted at a height of 3.05 meters to backboards at each end of the court. According to International Basketball Federation (FIBA) rules, the match lasts 40 minutes, divided into four periods of 10 minutes each. There is a 2-minute break after the ﬁrst quarter and after the third quarter of the match. After the ﬁrst half, there is an half-time break. The manuscript focuses on the processing of players’ movements data. In particular, the aim of this manuscript is to i) automatically drop all the inactive moments from a data matrix that tracks the movements of the players on the court at diﬀerent moments in the game, ii) automatically split the game into sorted actions, labelling them as oﬀensive or defensive. To do that we make use of available sensor data tracked during a game. This work is similar to that by [16], as they provide a procedure to process ball’s and players’ trajectories. However, our procedure diﬀers, since it works even if data on ball’s movement is missing. We place this piece of research within the domain of Human Activity Recognition (HAR). HAR aims to recognize the actions of an agent from a series of observations on the agents’ actions and the environmental conditions, [13] being a representative article on such a topic. In this work, the agents are considered to be the players of a team as a whole moving inside the court, and the action to be recognized concerns whether the game is active or inactive. In this vein,[5] apply data automation algorithm in sports, by using sensor data to categorize golf swing trajectories. [6] propose a game segmentation algorithm that suits in diﬀerent sports. [7] propose a model based on design of experiments and response surface methodology. In this paper we propose and discuss a multiple-stage algorithm which aims to drop inactive moments of a basketball data matrix tracking players’ movements. We apply this algorithm to diﬀerent real case studies (CS) in order to calibrate the algorithm’s parameters by means of a data-driven approach. We then provide some descriptive statistics related to the CS for a validation check of the algorithm and of the robustness of the parameters. In Section 2 we present and explain the algorithm. In Section 3 we validate the algorithm using real data. Section 4 concludes the paper and suggests further analysis.
Incorporating Monitors in Reactive Synthesis without Paying the Price<|sep|>Synthesis of programs from declarative speciﬁcations is an attractive prospect. Although thought prohibitive due to the theoretical hardness of LTL synthesis, recent improvements have made it a more reasonable endeavour, e.g. the identiﬁcation of GR(1) [25], for which synthesis is easier, and development of tools such as Strix [21,23] whose decomposition method allows for practical synthesis of full LTL. Limitations remain in the context of LTL, due to the inherent hardness of the problem. Beyond LTL there are also directions where the practicality of synthesis is not clear. ⋆ This research is funded by the ERC consolidator grant D-SynMA under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 772459).
Dark Matter Signals from Cascade Annihilations<|sep|>Recent observations by PAMELA [1] and ATIC [2] strongly suggest a new primary source of galactic electrons and positrons. Three leading interpretations of the PAMELA/ATIC excesses are astrophysical sources [3], decay of dark matter [4], and annihilation of dark matter [5, 6, 7, 8, 9]. While the current PAMELA/ATIC data cannot distinguish between these possibilities, one expects that the correct scenario will ultimately be determined with the help of complementary data from synchrotron, gamma ray, and neutrino telescopes, as well as collider and direct detection experiments. One piece of data that points toward an annihilation interpretation is the WMAP Haze [10], an apparent excess of synchrotron radiation coming from the galactic center. Dark matter annihilation into charged particles is uniquely positioned to explain the Haze [11, 12]. If n is the dark matter number density near the galactic center, then the synchrotron signal for dark matter annihilation scales like n2, while the signal for dark matter decay scales only as n. (Astrophysical signals also roughly scale like n.) Given the normalization of the PAMELA/ATIC excess, the n2 scaling is favored to explain the size of the Haze anomaly [13]. On the other hand, the same n2 versus n logic implies that the dark matter annihilation interpretation is more strongly constrained by the absence of gamma ray or neutrino excesses from the galactic center. While these constraints are dependent on the Milky Way dark matter halo proﬁle, there are already strong bounds on the annihilation interpretation for strongly peaked halos [14, 15, 16, 17, 18]. Therefore, it is worth exploring dark matter annihilation scenarios in detail to understand how robust the tension is between explaining PAMELA/ATIC/Haze and satisfying other bounds. Given the absence of anti-proton [19] or gamma ray [20, 21, 22] excesses, the dark matter annihilation scenarios favored to explain PAMELA/ATIC involve annihilation into electrons and muons. However, dark matter need not annihilate into leptons directly. There are a variety of “cascade annihilation” models where dark matter annihilates into light resonances which in turn decay into electrons or muons. These light resonances can lead to nonperturbative enhancements [23, 24] of the dark matter annihilation rate in the galactic halo, providing the large boost factors necessary to explain PAMELA/ATIC [5, 6, 7]. Also, annihilation into light ﬁelds gives a kinematic explanation for why dark matter annihilation preferentially yields light leptons [12, 6, 7]. Previous studies of cascade annihilation models appear in Refs. [8, 16]. In the present context, these cascade annihilation scenarios are interesting because they have the potential to explain PAMELA/ATIC while weakening constraints from gamma rays, as measured by atmospheric Cerenkov telescopes like H.E.S.S. [20, 21, 22]. The reason is that gamma ray experiments are directly sensitive to the primary injection spectra, and cascade annihilations yield softer and smaller injection spectra of gamma rays from ﬁnal state radiation (FSR). PAMELA/ATIC sees electrons and positrons through the ﬁlter of charged cosmic ray transport, a process which introduces large uncertainties. Considering also the uncertainties in the highest energy ATIC data, we ﬁnd that softer spectra of primary leptons can still explain the PAMELA/ATIC excesses. For cascade annihilations that terminate in muons, there is also an irreducible source of galactic neutrinos, which can be observed as an upward-going muon ﬂux on earth, for example, by water Cerenkov detectors like Super-Kamiokande (Super-K) [25]. While cascades soften the neutrino spectrum, we will see that the ﬁnal constraints from neutrinos are rather insensitive to the number of cascade steps, and may provide the most robust bound on muon cascade scenarios. The organization of this paper is as follows. In the next section, we deﬁne our framework for analyzing signals of dark matter through cascade annihilations, with details of the cascade energy spectra given in Appendix A. In Section 3, we ﬁnd the best ﬁt dark matter masses and annihilation cross sections for various cascade scenarios given the PAMELA/ATIC data. We consider H.E.S.S. gamma ray bounds from FSR in Section 4 and Super-K neutrino bounds in Section 5. In Section 6, we study a particular cascade annihilation scenario called the axion portal [7], and present a less constrained “leptonic” version in Appendix B. Conclusions are given in Section 7.
Optical counterparts of undetermined type $\gamma$-ray Active Galactic Nuclei with blazar-like Spectral Energy Distributions<|sep|>The observation of the sky with space-born instruments, equipped with detectors working at those electro-magnetic frequencies that cannot be accessed from the ground, revealed the existence of several classes of high energy radiation sources. With their location in distant galaxies, Active Galactic Nuclei (AGNs, in brief) turned out to be the most powerful non transient sources of such radiation. Although AGNs appear with many diﬀerent properties, they share an extreme intrinsic luminosity, ranging between 1041 erg s−1 and 1046 erg s−1, comparable to or greater than the energy output of large galaxies, but released from a region that is smaller than 1 pc in radius. To explain this property we assume that large amounts of matter, in the order of some solar masses per year, are conveyed to the nuclear regions of active galaxies, where they are accreted by a Super Massive Black Hole (SMBH). It is now well established that SMBHs with masses between 106 M⊙ and some 109 M⊙ reside in the nuclei of every massive galaxy (Ferrarese & Merrit, 2000; Shankar, 2009). In addition, we know that accretion of fuel into their gravitational ﬁeld leads to the conversion of gravitational binding energy into radiation with very high eﬃciency (Blandford & Znajek, 1977; Shields, 1978). Due to the presence of relativistic plasmas and strong magnetic ﬁelds in the vicinity of the black hole’s accretion ﬂow, the spectrum of the emitted radiation results from a combination of thermal and non-thermal components that cover several orders of magnitude in frequency, sometimes extending from radio wavelengths all the way up to γ-ray energies. The Fermi Large Area Telescope (Fermi-LAT; see Atwood et al., 2009) gave new life to the study of the γ-ray sky, producing, after 4 years of scientiﬁc observations, a map of γ-ray detections with unprecedented resolution and sensitivity in the energy range between 100 MeV and 300 GeV (Acero et al., 2015). Thanks to this result, a large number of γ-ray sources can now be associated with lower energy counterparts. In the extra-Galactic environment, it turned out that the most commonly detected objects are AGNs belonging to the blazar class. Blazars are extremely variable, highly polarized, radio-loud sources, dominated by power-law continuum spectra of type Fν ∝ ν−α, with a typical radio spectral slope α ≤ 0.5. Their properties are the consequence of the relativistic beaming of the synchrotron radiation produced by a jet that is collimated and accelerated close to our line of sight (Blandford & K¨onigl, 1979). They are classically separated into BL Lac objects (BLL), whose optical spectra show a nearly featureless power-law continuum, and Flat Spectrum Radio Quasars (FSRQs), which, instead, are characterized by strong emission lines. In addition to the blazars that dominate the extra-Galactic γ-ray population, other types of AGNs, together with a large number of sources without a ﬁrm classiﬁcation, are detected as well. In this contribution we describe our investigation on the nature of γ-ray AGNs of undetermined type, through the observation of their optical spectra. We focus our attention on targets whose spectral energy distributions (SED) are consistent with those of blazars, although they still lack ﬁrm spectroscopic classiﬁcation, and they are therefore called Blazar Candidates of Undetermined type (BCU in 3FGL terminology, Acero et al., 2015). In this report, we discuss the spectral classiﬁcation of some BCUs, with respect to the associated SEDs, rather than giving a full list of observations. In the following sections, we describe the techniques used in the attempt to identify the low energy counterparts of the γ-ray sources, the details of how we collected the optical spectra and their classiﬁcation criteria. Finally, we draw a sketch of the γ-ray emission in the diﬀerent classes of objects that we observe, compared with the multiple wavelength properties of their SEDs.
Stability of concentrated suspensions under Couette and Poiseuille flow<|sep|>It is well-known since the work by Orszag [1] that two-dimensional Poiseuille ﬂow of Newtonian ﬂuids have a critical Reynolds number of Re ≈ 5772.22 beyond ∗Institut f¨ur Mathematik, Technische Universit¨at Berlin, Straße des 17. Juni 136, 10623 Berlin, Germany †Mathematical Institute, University of Oxford, Andrew Wiles Building, Woodstock Road, Oxford OX2 6GG, UK ‡Hausdorﬀ Center for Mathematics, Villa Maria, Endenicher Allee 62, 53115 Bonn, Germany §Weierstrass Institute (WIAS), Mohrenstrasse 39, 10117 Berlin, Germany which point the ﬂow becomes linearly unstable. The linear stability analysis of parallel shear ﬂows, such as Poiseuille and Couette ﬂow is based on the study of the spectrum of the associated initial boundary value problem for the Orr-Sommerfeld equation, described in detail for example in Drazin and Reid [2]. This analysis does not reveal all the unstable behavior seen in experiments as some nonlinear instabilities do seem to be initiated by linear transient growth of certain modes, which is possible since the eigenfunctions of the Orr-Sommerfeld boundary value problem are not orthogonal as discussed in Trefethen et al. [3]. Some of these modes have time to grow large enough to serve as ﬁnite amplitude perturbation and eventually lead to a nonlinear, possibly three-dimensional, instability. While the literature on these fundamental hydrodynamic instabilities as well as their route to turbulence is quite extensive, much less is known if non-Newtonian ﬂuids or multiphase liquids are considered [4, 5, 6]. For the two-phase model equations for concentrated suspensions, which is the focus of this study, it has been shown in Ahnert et al. [7] that as the maximum packing fraction is approached, plane Poiseuille ﬂow gives rise to jammed and unyielded regions of the solid (particulate) phase. This ﬂow structure is a result of shear-induced migration, a phenomenon ﬁrst discovered by Leighton and Acrivos [8], and of a yield stress condition for the solid phase. Hence, understanding the eﬀect of yield stress on the stability properties of the ﬂow is of particular interest. One of the ﬁrst studies of the eﬀect of the yield stress on the stability properties can be found in Frigaard et al. [9]. In their analysis for the plane Poiseuille ﬂow of a Bingham ﬂuid (one of the simplest cases of a one-phase ﬂuid model with a yield stress) they derived a boundary value problem analogon of the Orr-Sommerfeld problem for Newtonian ﬂow. Further discussions by Frigaard et al. [10] and more recently by Metivier et al. [11] and Georgievskii [5] showed that the stability properties for plane Poiseuille ﬂow depend critically on the choice of boundary conditions at the yield surface for the associated eigenvalue problem. Using symmetric boundary conditions for the velocity at the yield surface the well-known critical Reynolds number Re = 5772.22 is approached as the Bingham number B → 0. On the other hand M´etivier et al. [11] noted that for their nonsymmetric boundary conditions all modes are stable, also as B → 0. This indicates that the Orr-Sommerfeld-Bingham equation is not a canonical generalization of the standard Orr-Sommerfeld equation. Guided by these investigations, we revisit the formulation of the boundary value problem for the Orr-Sommerfeld-Bingham equations and discuss its implications for the derivation to the eigenvalue problem for the two-phase ﬂow of plane Couette and Poiseuille ﬂow. In particular we show that for the two-phase Poiseuille ﬂow model for concentrated suspensions the conditions at the yield surface of the corresponding eigenvalue problem are non-symmetric. The stability analysis of the resulting boundary value problem carried out in this study thus constitutes a next step in complexity for the investigation of the dynamical behavior of two-phase ﬂow models with yield-stress. The analysis will moreover serve to assess the necessary conditions to address the problem of well-posedness of the two-phase ﬂow model. The problem of well-posedness is in fact an inherent property of even the simplest multiphase model equations for suspension ﬂow and many other applications, since its ﬁrst derivations from an averaging method pioneered by Drew and Passmann [12] and Ishii [13]. Nevertheless, such models have found widespread applications and using various forms of regularizations their study started the development of a number of numerical schemes described for example in Stewart and Wendroﬀ [14]. The problem of ill-posedness has recently been reviewed by Lhuillier et al. [15]. In a series of articles, Keyﬁtz et al. [16, 17, 18] showed for simple cases of two-phase ﬂows that the ill-posedness of the initial boundary value problem is connected to a loss of hyperbolicity in the principal part of the equations. They have begun to generalize the theory for conservation laws in order to connect the arising singular behavior with the existence of a so-called singular shock. The present study is intended to lay the groundwork for future studies concerning the existence of singular shocks in concentrated suspensions. After the formulation of the two-phase ﬂow model and the derivation of the eigenvalue problem in Section 2, our investigations will focus on the stability analysis of the Couette ﬂow problem in Section 3. This problem is instructive since we can simplify the resulting eigenvalue problem considerably and derive criteria for an ill-posedness in the system that is related to the competition between the solid phase viscosity and the collision pressure. The study of these special cases is then used for the design of a reliable numerical scheme for the general eigenvalue problem. In addition to the ill-posedness we also ﬁnd a convection induced instability via a Kelvin-mode ansatz and show that in general, the growth of the unstable mode is transient. However, as the particle volume fraction of the suspension increases the growth rates of the unstable modes increase as well, so that it can become strong enough to possibly trigger ﬁnite-amplitude, nonlinear instabilities. For the two-dimensional Poiseuille ﬂow, considered in Section 4, simpliﬁcations of the resulting eigenvalue problem, that allow analytical work are not possible. Here, our numerical parameter studies show that the ill-posedness as well as the transient growth property occur again, however for diﬀerent parameter values. The main diﬀerence to the Couette ﬂow is that for Poiseuille ﬂow there are volume fractions for which unyielded region emerge. The stability of the corresponding yielding surface is the ﬁnal topic of our investigations. For the derivation of the associated boundary value problem we found it helpful to revisit the formulation of the eigenvalue problem for the Orr-Sommerfeld-Bingham equation. We conclude in Section 5 with an outlook.
A Ku-Band Novel Micromachined Bandpass Filter with Two Transmission Zeros<|sep|>applications to reject spurious signals and to separate  different channels in a multichannel communication  system. So the characteristics of compact size, high  selectivity, and low insertion loss for microwave filters  are highly required. To meet these objectives, some key  techniques are used in this paper, such as tapped-line  interdigital  structure,  micromachined  via-hole  and coupling sections are eliminated. Therefore, filters with  tapped-line feed can offer space and cost saving  advantages. Furthermore, the realizable bandwidth could  be much greater [1]. MEMS technique. It is used for the grounding of the  microwave resonators in the filter and therefore makes  the filter much more compact [2]. remarkably improve the selectivity of the filters. Various  methods have been introduced for the implementation of  the transmission zeros at finite frequencies. Using cross  couplings between nonadjacent resonators is the most  common way [3]. But it usually suffers more complex  structures in design. To avoid cross coupling structures, open-loop and hairpin transmission-line resonator filters  using tapped-line feed topology to create transmission  zeros have also been reported [4], [5]. However, their size  is usually not as compact as the one with interdigital  structure. interdigital bandpass filter with two transmission zeros is  presented. It is easy to realize in the physical structure.  Each transmission zero can be freely placed at desired  frequency to achieve demand attenuation in the stopband.  The locations of the transmission zeros can be calculated.  All the grounded sections of the filter are realrized by  micromachined via-hole. Finally, an eight-order bandpass  filter is fabricated and measured. The measured  performance shows good agreement with the desin theory.  The chip size is only 9.6mm×4mm.
Quantum Model-Discovery<|sep|>Recent advances in automatic differentiation in classical computers have led to novel machine learning based approaches to scientiﬁc computing, often referred to as scientiﬁc machine learning. For example, incorporating neural networks into classical ordinary differential equation solvers [1], or manipulating neural network derivatives towards prescribed differential equations [2]. The applications of such techniques have included inference of unobserved states [3], inference of differential equation parameters [4] and approaches to solve differential equations [2], or discover differential equations from observated data [5]. Such methods depend on machine learning techniques and are often end-to-end differentiable. Recent progress in automatic differentiation on quantum computers [6–14] introduce the prospect of extending recent classical results in scientiﬁc machine learning to a quantum setting. These techniques have already led to quantum methods of solving differential equations [15] which are analogous to methods involving classical neural networks [2]. However, the existence of quantum automatic differentiation opens the door to further cross pollination between classical scientiﬁc machine learning and quantum computing. In this work, we target an important task in scientiﬁc machine learning and modelling in general; while many systems can be modelled using differential equations, and their solving constitutes an important task in phenomenological understanding, in many practical settings these equations are not (fully) known. In such cases, one may have access to observations on some target system, and some initial idea of the dynamics. Parameter inference in differential equations is an essential part of combining theoretical models with empirical observations. Oftentimes a given mechanism might be speculated for an observed process in the form of a differential equation, but whose coefﬁcients must be inferred from empirical observations. Model discovery (often referred to as equation discovery) tries to ﬁnd a mathematical expression that describes a given dataset. We speciﬁcally tackle the problem of discovering a differential equation only from observations of a given system. By discovery we mean an interpretable (i.e. human readable) model in symbolic form, which can be converted directly into a differential equation. This task is generally not suited for treatment by ﬁnite-differencing techniques, since such applications require many repeated simulations. Instead, to treat such a case of mixture of equation solving and model discovery efﬁciently, we generalize two techniques from the realm of classical scientiﬁc machine learning to a quantum setting: differential equation parameter inference and differential equation model discovery. We also highlight conceptual similarities between scientiﬁc machine learning and quantum computing that might point to further research avenues.
Reduced Physics Model of the Tokamak Scrape-off-Layer for Pulse Design<|sep|>The dynamic interplay between the core and the edge plasma has important consequences in the conﬁnement and heating of fusion plasmas. The eﬀects of the ScrapeOﬀ-Layer (SOL) can be signiﬁcant in the contexts of neutral beam deposition [1, 2], radio-frequency heating and current drive [3, 4, 5], and impurity transport [6, 7]. To systematically study these problems and enable controlroom applications, we need to be able to simulate the entire tokamak plasma self-consistently and time-dependently with reasonable turn-around time. Reduced physics models are well-suited for this purpose because of their lighter computational cost and the ability to reasonably reproduce physical eﬀects within their validated parameter spaces. The Two-Point Model of SOL plasma is a widely used 0-D stationary model of SOL transport. It consists simply of conservation equations that relate the quantities found at “upstream”, typically deﬁned as the outer mid-plane (OMP) of the tokamak or sometimes the divertor entrance, to those found at the divertor targets. However, the model requires the upstream density as an input parameter so that the other plasma parameters can be calculated, thus limiting its ability as a predictive model. Various eﬀorts have been made in calculating this upstream density via other known quantities to achieve a predictive Two-Point Model, adopting either a database approach [8, 9, 10] or a diﬀusive ansatz in the cross-ﬁeld direction with chosen scale-lengths [11]. While achieving reasonable success, the former approach is inherently device-speciﬁc, while the latter approach can over-estimate the upstream density since the slow cross-ﬁeld transport is the only particle sink. We will show in this paper that by introducing a new picture for particle balance we can avoid this problem of an undetermined upstream density and instead calculate all SOL parameters from core power and particle ﬂuxes only. Speciﬁcally, by balancing the cross-ﬁeld transport of particles from core to SOL with the pumping at the targets, the plasma density can be calculated at both upstream and the target. Recycling coeﬃcients are required as additional control parameters (Sec. 2). When coupled to a transport solver like TRANSP [12], the ﬂuxes are calculated by the core transport solver and the recycling coeﬃcients can be constrained using divertor measurements, as will be demonstrated in Sec. 3. This approach will oﬀer new opportunities for eﬃcient core-edge coupled predictive modeling as well as interpretive analysis of target recycling processes.
A New Representation of Successor Features for Transfer across Dissimilar Environments<|sep|>Reinforcement learning (RL) is a computational approach that learns how to attain a complex goal by maximising rewards over time. Successful applications range from Atari games (Mnih et al., 2015), to robotics (Zhang et al., 2017), and self-driving cars (Liang et al., 2018). However, this success is based on solving each task from scratch, and thus training these agents requires vast amounts of data. Several solutions have been proposed to address this problem. Most works in transfer RL such as Progressive Neural Networks (Rusu et al., 2016) and Inter-Task Mapping setups (Ammar & Taylor, 2011; Gupta et al., 2017; Konidaris & Barto, 2006; Yin & Pan, 2017) assume that the state-action space, or reward distribution space can be disentangled into independent sub-domains. However, learning interpretable, disentangled representations is challenging (Zhu et al., 2020). The dynamics and the reward was decoupled for the ﬁrst time in (Barreto et al., 2017). Building upon an elegant formulation called the successor function (Dayan, 1993), the method allowed ﬂexible transfer learning across tasks that differ in their reward structure. The underlying assumption was that the environmental dynamics remains unchanged and using a Generalised Policy Improvement (GPI) method, the optimal policy is determined. Such successor feature based methods have been shown to efﬁciently transfer knowledge across RL tasks (Barreto et al., 2018; 2019; 2020). Other works that have built upon successor features include generalised policy updates on successor features (Barreto et al., 2020), a universal type of successor feature based on the temporal difference method (Ma et al., 2020), and Variational Universal Successor Features (Siriwardhana et al., 2019) that perform target driven navigation. Option Keyboard (Barreto et al., 2019) leveraged successor features to combine skills to deﬁne and manipulate options. This allows for change in reward, but a slight change of the environment can deteriorate the performance of a new task. If however both the environmental dynamics and the reward differs across tasks, these methods are unable to handle this challenge. In real-world problems when environment dynamics and rewards change across tasks, both these aspects need careful modelling. Failing to do so can lead to negative transfer of knowledge from the previously seen tasks. Thus RL methods using successor features need to be extended to handle the changes in environmental dynamics. Such work is limited. Zhang et al. (Zhang et al., 2017) aim to address this problem by considering a linear relationship between the source and target successor features. This modelling is restrictive and may fall short in capturing the complexity of the changed environment dynamics. Thus, the problem of designing a method using a successor feature based approach to transfer knowledge from source to target environment where the dynamics are dissimilar, is still open. Our new approach enables the efﬁcient learning of novel successor features to cater to the new target environmental dynamics. This is done by using the distribution of the previous (source) task successor features as a prior for the new tar get task. We model both the target and source distributions through Gaussian Processes (GPs). The target distribution is modeled as a noisy version of the source distribution. This approach assumes that the source and target environments lie within some proximity to each other i.e. they are similar within some noisy envelope. However, this adjustable noisy envelope impacts the upper bounded error on the modelling of optimal policy in the target environment. The advantage of this approach is that the source observations provide a head-start for the learning process and additional explorations in the target will provide efﬁcient convergence to the optimal policy. We use a GPI method to estimate the target action value function. We provide theoretical analysis and upper bounds (1) on the difference of action-value functions when the optimal policy derived from environment i is replaced by the optimal policy derived from environment j; (2) on the estimation error of the action-value function of an optimal policy learned in a source environment when executed in the target environment; (3) on the difference of the optimal action-value function in the target environment and our GPI-derived action value function. We evaluate this approach in a variety of benchmark environments with different levels of complexity. Our key contributions are: • A new method based on successor features and Gaussian Processes that enhances transfer from source to target tasks when the dynamics of the environment are dissimilar;
Approximation of Search Times for On-street Parking Based on Supply and Demand<|sep|>Long parking search time is a perpetual problem of every big city, and quantitative estimation of cruising  time is a long-standing challenge for transportation research. Given only a moment’s thought, it is clear  that a parking search may take very long time or even fail when the parking occupation rate is very high  and when, during the period of the search, the number of arriving vehicles is similar to or higher than  the number of vehicles departing their parking spots. This imbalance between the occupation rate and  the rates of arrivals and departures is typical for the central office/commercial/residential areas of city  centers, and results in the notorious estimate of 30% of urban traffic being caused by parking search  [Shoup, 1997]. Let us recall that, in reality, urban parking conditions are highly heterogeneous in space and vary in time.  Parking demand is defined by the size and use of buildings and other attractions in the area, while  supply is defined by the parking capacity of street links and off-street facilities, as well as by parking  regulations and pricing. The high spatio-temporal heterogeneity of urban parking availability inspires  drivers to hope in vain that they will find a spot “just around the corner.” At the same time, drivers want  to minimize their walk time and confine their search to the area nearby their destination. The rate of  arrivals to an area is affected by the attractiveness of its destinations, such as workplaces, offices, shops  and residences; while the rate of departures by the activities of the drivers who are parked in the area,  such as working, visiting destinations or residential parking. The state of the area of a driver’s parking  search reflects the net outcome of the arrivals and departures to that area and its immediate  surroundings. Parking search habits of visitors and residents are a reflection of a city’s parking regulations and pricing.  Let us consider, for example, Tel Aviv city center, where the overall on-street demand-to-supply ratio is  above 100% throughout the entire day. On-street parking is free for Tel Aviv residents and inexpensive  for non-residents, significantly cheaper than parking in one of the city’s numerous parking facilities. In  the short run, the parking choices of the drivers in the Tel Aviv center are either (1) cruise until a free  spot is found; (2) use an off-street parking facility that is expensive and possibly far from the destination;  (3) park illegally, accepting the risk of a high fine; (4) leave the area. As the last three options are very  unattractive for residents parking overnight, they often cruise for a quarter of an hour or more when  returning home from work [Levy et al., 2015] and yet their final parking spot may be very far from their  home. In the long run, frequent long cruising may result in a change of transportation habits and  decrease car ownership levels [Weinberger et al., 2009], but as far as Tel Aviv is concerned, there is not  yet any meaningful decrease in the demand-to-supply ratio, and long cruising for on-street parking  remains one of major issues of the Tel Aviv municipal elections. Quantitative understanding of the parking search that accounts for the inherent heterogeneity of the  urban parking space thus remains one of the basic issues of urban transportation planning and  management. In this paper, we propose a simple algorithm for estimating cruising time curves – the  probability p(, n) of searching for parking longer than  for a given destination n in the city. The  algorithm is based on high-resolution maps of urban parking demand and supply that are available for  the majority of large Western cities. We validate the algorithm with PARKGRID, an agent-based model of urban parking search. As a practical example we construct cruising time curves for the Israeli city of Bat  Yam, with a population of 120,000.
Grain Surface Classification via Machine Learning Methods<|sep|>Silos are primary storage tools for grain. Grain is a so important food source because it contains various nutrients. So, the main task of silos is to protect and increase the storage life of grain[1]. For these reasons, determining the amount of grain is important in many  situations such as seed planning, trades, and inventory tracking. That's why, different level measurement systems having own specific  characteristics have been used[2]. The most important issue in silo level measurement is the conical grain stack structures inside the  silo. These conical structures occur after the grain is unloaded or filled from the silo as seen from Figure 1. Single-point level  determination systems such as ultrasonic, and laser, which provide a perfect solution for level measurement of liquids. Determination  of the silo level with single-measurement point cannot produce a good solution due to the conical grain structures inside silo. Errors of  these systems can be reduced by placing more sensor[3]. In addition, laser and ultrasonic systems are affected by dust, especially after  grain filling and unloading. On the other hand, radar-based level measurement system is not affected by harsh environmental conditions  and so it can be preferred in dusty environments such as silos. Besides, the whole surface of the grain can be illuminated  electromagnetically by a radar system consisting of a wide beam width antenna. Thus, the level information of the whole grain surface  can be obtained. However, it is very difficult to determine the grain level information from the complex reflection signal caused by the  metallic walls of the silo and the surface structures of the grain. In recent years, in order to overcome these problems, combination of  signal processing and machine learning algorithm has become popular[1], [4]. In this study, Support Vector Machine (SVM) algorithm  is proposed to determine the grain surface type in silo by using radar backscattering signal dataset. The learning and test dataset were  constituted using the radar-based experimental system. A total of 5681 measurements were performed for different grain surface  conditions and different amounts of grain. The content of this paper is as follows: in the next chapter, the experimental measurement setup and proposed method are mentioned.
CrossFill: Foam Structures with Graded Density for Continuous Material Extrusion<|sep|>3D printing enables the fabrication of complex structures with unprecedented geometric detail. This creates the opportunity to realize 3D shapes with complex internal structures. Physical properties of these inﬁll structures are determined by their geometry and the constitutive material by which they are made. Even with a single constitutive material, 3D printing allows to achieve graded physical properties (e.g. density and stiﬀness) by spatially varying the geometry of inﬁll structures. This enables functionally graded materials (FGM) at a manufacturable scale. Precise realization of graded physical properties can lead to many applications, such as customized insoles, comfort cushioning and medical phantoms. Fused deposition modeling (FDM) is one of the most widely used 3D printing processes as it has a comparatively low running cost and supports a wide variety of materials. FDM systems Fig. 2. Microscopic photos of top and side views of printing results with a 0.38 mm wide extrusion path: (a) without versus (b) with overlapping by 0.36 mm respectively. Overlapping extrusion paths exhibit over-extrusion of material at the overlapping region, which results in unwanted blobs on the surface of the print. work by extruding melted streams of material from a moving nozzle to form a quickly solidiﬁed path. However, there is limited study on using FDM to reliably fabricate FGM. This task is challenging since the complicated geometry of a functionally graded inﬁll structure is diﬃcult to meet the diﬀerent manufacturing constraints required by FDM to ensure printing quality: • Overhanging geometry: If a geometric feature is not properly supported by lower layers, it is said to be overhanging. While overhanging geometry can be printed using support structures, the complexity of inﬁll structures inside a 3D model does not allow an easy removal of support structures. Therefore, inﬁll structures are expected to be self-supporting. • Discontinuous material extrusion: In extrusion-based fabrication, frequent restarting and stopping extrusion creates defects. Simply stopping the extrusion motor will lead to material leakage. One common way to prevent that is to retract the ﬁlament backward a bit before starting a rapid travel move, but that in turn introduces bulges where the extrusion paths start and end. See Fig. 1. In order to fabricate inﬁll structures reliably, it is desired that each layer of the structures is fabricated by continuous extrusion along a single toolpath without any in • Overlapping extrusion paths: Since material is extruded along a toolpath typically with a constant width, there is an excess of material in a layer when extrusion paths are too close to each other. The overlap of extrusion paths causes blobs and wider lines, which make it diﬃcult to control the density of inﬁll structures in corresponding regions (see Fig. 2). It is preferred to solve this problem intrinsically by generating inﬁll structures without overlapping toolpaths. In order to use FDM for fabricating foam structures with graded density inside a given model, a method needs to be developed for generating inﬁll structures according to a user-speciﬁed density distribution, which should also avoid the above manufacturing problems. In this paper, we propose a novel type of foam structure that can achieve the aforementioned objective. Speciﬁcally, we develop a space-ﬁlling surface, called CrossFill, an FDM printable foam structure as inﬁll for a 3D model. Each layer of CrossFill is a space-ﬁlling curve that can be continuously extruded along a single overlap-free toolpath. The space-ﬁlling surface consists of surface patches which are embedded in prism-shaped cells, which can be adaptively subdivided to match the user-speciﬁed density distribution. The adaptive subdivision level results in graded mechanical properties throughout the foam structure. Our method consists of a step to determine a lower bound for the subdivision levels at each location and a dithering step to reﬁne the local average densities, so that we can generate CrossFill that closely matches the required density distribution. A simple and eﬀective algorithm is developed to merge a space-ﬁlling curve of CrossFill of a layer into the closed polygonal areas sliced from the input model. Physical printing tests have been conducted to verify the performance of the CrossFill structures. • An algorithm for merging the toolpath of an inﬁll structure with the input model’s boundary so as to retain continuity.
Diffractive mechanisms in $pp \to pp \pi^{0}$ reaction at high energies<|sep|>We discuss the exclusive pp → ppπ0 process at high energies, see [1]. 1 This reaction was measured in detail only near to the pion threshold. A nice summary of the intermediate energy data can be found in Ref.[4]. In this region of energy the corresponding cross section systematically decreases which is consistent with the meson exchange picture. The pp → p(nπ+) and np → (pπ−)p reactions were measured at ISR and Fermilab [5]. As discussed in [6] the dominant hadronic bremsstrahlung-type mechanism is the Drell-Hiida-Deck mechanism for diffractive production of πN ﬁnal states in NN collisions [7]. The pp → ppπ0 process can be measured with the help of the forward detectors at the LHC. 2 s = 29.1 GeV was performed. The results show that the η and η′ mesons appear to have a similar production mechanism which considerably differs from that for the π0 production [8]. The WA102 Collaboration concentrated on central production of mesons and eliminates contribution comes from diffractive mechanisms discussed in [1]. Reactions of this type pp → pMp are expected to be mediated by double exchange processes. For instance, the η and η′ mesons are produced dominantly by a mixture of pomeron-pomeron, reggeon-pomeron, and reggeon-reggeon exchanges (see [10]). For the central exclusive π0 production at intermediate energies the ρ-ω, ρ-a2 exchanges may be the dominant mechanism. The π0 can be also produced by γγ, γω, and γO exchanges. The search performed at HERA [11] was negative and found only an upper limit for this process σγ p→π0 p < 49 nb. In Refs. [12, 13] the authors discussed some results of exclusive pseudoscalar meson production in high energy ep scattering. As shown in Ref.[13, 14] the photon exchange is larger than the odderon exchange only at very small transverse momenta of π0. Here we shall consider the odderon exchange processes using a simple phenomenological approach and estimate their contributions in the pp-collisions.
Flatland Competition 2020: MAPF and MARL for Efficient Train Coordination on a Grid World<|sep|>Modern railway networks such as the one operated by the Swiss Federal Railways Company (SBB) are becoming increasingly complex, and the eﬃcient coordination of the traﬃc on the network poses a signiﬁcant challenge. In a system that registers more than 10,000 train runs a day1, even small disturbances such as a train malfunction can have a huge impact on the service quality and stability of the entire network. Therefore, not only the initial scheduling but also the eﬃcient, dynamic rescheduling of trains is essential. The challenge of scheduling vehicles dates back several decades and was formally expressed as the “vehicle scheduling problem” (VSP) (Bodin and Golden, 1981). Finding solutions to the VSP has been an active area of operations research (OR) ever since (Foster and Ryan, 1976; Potvin and Rousseau, 1993). Due to the increasing complexity and the need for eﬃcient rescheduling in case of disruption, Li et al. (2007) proposed an extension of the VSP to the broader “vehicle re-scheduling problem” (VRSP) which takes disruptions like vehicle breakdowns into account and represents a dynamic version of the VSP. However, solving the VRSP incorporating all aspects given in a real world railway network is an NP-complete problem and does not allow for fast experimentation of automated traﬃc management, let alone the real-time rescheduling of trains. Therefore, the research team at SBB started to explore new approaches, including multi-agent reinforcement learning (MARL) (Egli et al., 2018). In recent years, MARL algorithms have achieved remarkable successes on challenging multiplayer video game benchmarks such as StarCraft II (Vinyals et al., 2019; Samvelyan et al., 2019), Dota 2 (OpenAI et al., 2019), hide-and-seek (Baker et al., 2020) and Capture the Flag (Jaderberg et al., 2019). Besides idealized video game environments, cooperative multi-agent reinforcement learning also shows promise for many real-life applications such as network traﬃc signal control (Arel et al., 2010; Tan et al., 2020) and real-time bidding (Jin et al., 2018). However, scalability to large number of agents, partial observability and communication constraints of individual agents remain major challenges. Many recent approaches address these by learning decentralized policies in a centralized manner (Foerster et al., 2016; Gupta et al., 2017; Rashid et al., 2018). With decentralized policies, communication becomes important. Foerster et al. (2016), Sukhbaatar et al. (2016), Jiang and Lu (2018) and Das et al. (2019) addressed this by explicitly modelling communication between agents. To cope with large joint action spaces, value function decomposition methods learn centralized but factored global Q-functions, which handle coordination dependencies implicitly (Sunehag et al., 2018; Rashid et al., 2018; Son et al., 2019; Wang et al., 2020, 2021a; Rashid et al., 2020; Wang et al., 2021b). Foerster et al. (2018) and Lowe et al. (2017) proposed the paradigm of centralized critic with decentralized actors for multiagent policy gradient algorithms. Extending these works, Iqbal and Sha (2019) proposed a shared attention mechanism and Wen et al. (2019) introduced a recursive reasoning mechanism. To address the computational complexity of a full system simulation and in light of the recent development in reinforcement learning (RL) research, SBB in collaboration with AIcrowd, developed a framework that provides a simpliﬁed yet representative environment to study dynamic train (re-)scheduling (Mohanty et al., 2020). A ﬁrst edition of the Flatland competition was held in 2019 with the aim of discovering novel approaches to the VRSP with a special emphasis on RL-based solutions. In this paper, we present the Flatland competition run at NeurIPS 2020 and the main insights gained from it. We outline the competition format in the next section. We then present common approaches taken by top submissions in Section 3 and proceed with detailed descriptions of outstanding solutions – provided by the teams themselves – in Section 4. In Section 5, we present a comparative analysis of these solutions, and discuss our ﬁndings in the ﬁnal section.
Stateless multicast switching in software defined networks<|sep|>Multicast routing and switching is generally more complex than its unicast counterpart, predominantly because: optimal multicast routing is an NP-complete problem (a Steiner tree optimisation) [1]; and, IP routers/switches have to maintain speciﬁc multicast state [2]. Although multicast is not, truly, deployed as an inter-domain service at Internet scale [3], it is widely deployed at intra-domain level for applications such as IPTV. While multicast has problems at the IP layer, these difﬁculties are somewhat greater at lower layers that do not natively support multicast, such as multi-protocol label switching (MPLS). RFC6513 [4] has added multicast support to MPLS; however, at operator scale, a trade-off between optimality of routing and scalability of state is needed, as described by Martinez-Yelmo et al. [2]. The problems with deploying multicast at operator scale has motivated stateless solutions, such as multi-protocol stateless switching (MPSS) [5] and line speed publish/subscribe internetworking (LIPSIN), that use the Bloom ﬁlter (BF) as the forwarding identiﬁer (FID) in the packet-header. For brevity, this form of switching will be described as BF switching. Indeed BF switching has been utilised by architectures such as PSIRP and PURSUIT [6], [7] that form the basis of a clean-slate information centric network (ICN) [8]. These This work was carried out within the project POINT, which has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 643990 efforts demonstrate that BF switching can deliver very efﬁcient unicast/multicast forwarding with minimal state, however, they require either software-based switching or custom hardware. This paper follows a BF approach, implemented natively in SDN, as a new technique for providing stateless multicast switching at operator scale. While a BF has been previously proposed for stateless multicast switching [5], [9], this paper is the ﬁrst to show that it can be implemented directly in contemporary SDN switches, without reactive controller intervention. We focus on an SDN solution as it is a frequent proposal for next generation networks at operator scale [10]. More speciﬁcally, SDN is proposed as a “clean-slate” network upgrade, replacing technologies such as spanning-tree or OSPF in data-centre or operator networks respectively. The term “software deﬁned networks” can encompass a number of different software controlled network technologies; however, in this paper the term SDN will be used, speciﬁcally, to describe contemporary layer-2/3 Ethernet switches controlled by a centralised controller employing the OpenFlow protocol [11]. There have been previous attempts at using SDN for cleanslate proposals such as ICN. Chanda and Westphal proposed ContentFlow, which maps application layer content information onto a legacy IP transport mechanism using extensions to an SDN controller [12]. Alternatively, Syrivelis et al. described in [13] how SDN can be used for forwarding in the PURSUIT architecture by using the BF switching header within the SDN controller to route packets to the appropriate destination on a packet-by-packet basis. Notably, neither of these propositions have provided scalable solutions at operator network sizes, as the controller has to react to each application session or packet, respectively. In contrast, the proposition here directly implements the BF switching in the SDN switches. The rest of this paper is structured as follows: Section II will describe the proposal together, with Section III providing an analytical approach to the bounds of the SDN ﬂow sizes; Section IV demonstrates the scalability advantages of the solution by modelling and comparing the ﬂow entry requirements in realistic network topologies; ﬁnally, Section V brieﬂy concludes and outlines future work.
High-Performance Neural Networks for Visual Object Classification<|sep|>The human visual system eﬃciently recognizes and localizes objects within cluttered scenes. For artiﬁcial systems, however, this is still diﬃcult, due to viewpoint-dependent object variability, and the high in-class variability of many object types. Deep hierarchical neural models roughly mimick the nature of mammalian visual cortex, and by community consensus are among the most promising architectures for such tasks. The most successful hierarchical object recognition systems all extract localized features from input images, convolving image patches with ﬁlters. Filter responses are then repeatedly sub-sampled and re-ﬁltered, resulting in a deep feed-forward network architecture whose output feature vectors are eventually classiﬁed. One of the ﬁrst hierarchical neural systems was the Neocognitron (Fukushima, 1980) which inspired many of the more recent variants. Unsupervised learning methods applied to patches of natural images tend to produce localized ﬁlters that resemble oﬀ-center-on-surround ﬁlters, orientation-sensitive bar detectors, Gabor ﬁlters (Schmidhuber et al., 1996; Olshausen and Field, 1997; Hoyer and Hyv¨arinen, 2000). These ﬁndings in conjunction with experimental studies of the visual cortex justify the use of such ﬁlters in the so-called standard model for object recognition (Riesenhuber and Poggio, 1999; Serre et al., 2007; Mutch and Lowe, 2008), whose ﬁlters are ﬁxed, in contrast to those of Convolutional Neural Networks (CNNs) (LeCun et al., 1998; Behnke, 2003; Simard et al., 2003), whose weights (ﬁlters) are randomly initialized and changed in a supervised way using back-propagation (BP). Despite the hardware progress of the past decades, computational speed is still a limiting factor for CNN architectures characterized by many building blocks typically set by trial and error. To systematically test the impact of various architectures on classiﬁcation performance, we present a fast CNN implementation on Graphics Processing Units (GPUs). Previous GPU implementations of CNNs (Chellapilla et al., 2006; Uetz and Behnke, 2009) were hard-coded to satisfy GPU hardware constraints, whereas our implementation is ﬂexible and fully online (i.e., weight updates after each image). It allows for training large CNNs within days instead of months, such that we can investigate the inﬂuence of various structural parameters by exploring large parameter spaces (Pinto et al., 2009) and performing error analysis on repeated experiments. We evaluate various networks on the handwritten digit benchmark MNIST (LeCun et al., 1998) and two image classiﬁcation benchmarks: NORB (LeCun et al., 2004) and CIFAR10 (Krizhevsky, 2009).
Bidirectional deep learning of polarization transfer in liquid crystals with application to quantum state preparation<|sep|>A vast number of applications require accurate control of the polarization state of light, such as in display technology, ellipsometry, polarization microscopy, and optical communications. The polarization can be manipulated by mechanically adjusting birefringent elements or using electro-optic modulators. The former approach can reach high accuracy but is slow, producing vibrations, and prone to malfunction. The latter one is ultra-fast and vibration-free, however, its accuracy is limited. Freespace electro-optic modulators are bulky and require high voltage drivers, which tend to ﬂuctuate and decrease the overall accuracy even further. Low-voltage integrated modulators show inherent polarization instabilities due to ﬁber coupling. Liquid crystals represent a middle ground between the stability of birefringent elements and the response speed of electro-optic modulators. Voltage-controlled nematic liquid crystals are particularly convenient and widely available, as they are commonly used in the display industry. We distinguish two main types of nematic liquid crystals based on the alignment of the crystals in a device, namely parallel and twisted conﬁgurations. The former acts as a polarization retarder and is typically custom-made for specialized applications; the latter is used in displays. Parallel nematic liquid crystals were utilized as polarization retarders for polarization modulation [1], polarization state preparation and tomography [2, 3], remote state preparation and imaging [4, 5], entangled-photons generation [6, 7], and implementation of quantum channels [8, 9] and quantum communication protocols [10, 11]. Recently, fully reconﬁgurable topological photonic devices were proposed employing nematic liquid crystals [12]. Despite the wide utilization of nematic liquid crystal (LC) devices, we lack an accurate theoretical model of LCs. The available models are particularly inaccurate for twisted LCs, which prevents them from entering a more extensive range of applications. The response of LC to control voltage(s) is aﬀected by various imperfections such as alignment layers dragging, multiple reﬂections, and inhomogeneity-induced depolarization. These eﬀects represent a serious setback to the modeling of the LC response and, particularly, to the inverse task of ﬁnding the optimum control voltages to prepare the target polarization state. The theoretical model of twisted LCs [13] was modiﬁed to include the boundary eﬀects [14] and yet further adjusted [15–17] to achieve better results. Unfortunately, even with these improvements, the theoretical model of polarization transformation is not suﬃciently accurate. The inaccuracy is especially apparent when considering a device consisting of multiple LC cells [1, 18]. When aiming for a discrete set of polarization states, such a complex LC device can be calibrated and even used as a highly accurate polarimeter [18]. However, modeling of a continuous polarization response of an LC device to analog control voltages represents an open problem. Here we model a complex twisted LC device using deep neural networks (DNNs). The model is ﬁtted to a training dataset obtained by measuring the polarization states prepared by a device for diﬀerent control voltages. We employ the mesh adaptive direct search (MADS) algorithm for black-box optimization of the deep learning model hyperparameters. The model is optimized and tested using separate datasets not involved in the training process. We demonstrate an unprecedented ﬁdelity and repeatability between the polarization state predicted by the model and the measured state. We achieve the aver age inﬁdelity of 4 × 10−4 of the polarization preparation for a three-cell twisted LC device. The DNN approach outperforms other models based on radial basis functions (RBF) and linear interpolation. We study the eﬀect of the training dataset size over several orders of magnitude and found that errors of the DNN model decrease faster than the other numerical methods with an increasing number of training data. In other words, the DNN approach is more eﬃcient with respect to the dataset size than other methods. Furthermore, we analyze the DNN model size and its overparametrization and scalability. Our main result is solving the inverse task of ﬁnding control voltages optimal for preparing a target polarization state. We utilize the trained DNN direct model as a part of the compound autoencoder-like network to ﬁnd the inverse model. This allows using physics metrics, e.g., the ﬁdelity, consistently in the whole framework and also avoids ambiguous mapping from polarization state to control voltages. The compound model outperforms other approaches by orders of magnitude. We verify the predictive strength of the DNN model by preparing over a thousand of single-photon polarization states and performing their independent characterization using full quantum tomography. Finally, we demonstrate a remote preparation of quantum states using of entangled photons. The reported local and remote preparation of polarization-encoded quantum bits (qubits) certiﬁes the use of twisted LC devices in quantum technology. Besides the imminent application of our approach to accurate polarization qubit manipulation, we may think of it as a use case of a more general problem of optimal control of quantum devices, see Fig. 1 (a). Various implementations of quantum devices share the common aspect of being controlled by classical analog signals, related non-trivially to the device operation [19–21]. The control signals need to be optimally adjusted to provide high-ﬁdelity operation of the device [22–28]. Photonic circuits on optical chips include voltage-controlled phase modulators with a complex response and crosstalk [29– 31]. Superconducting circuits are controlled by radiofrequency signals with variable amplitudes and complex timing, which are subject of optimization [32, 33]. Also, semiconductor quantum dots need to be optimally tuned to produce target states [34–40]. The approach developed in this work can be directly applied to the learning of the steady-state response of quantum devices to classical control signals and, consequently, their optimal control.
Reconfiguration Dynamics in folded and intrinsically disordered protein with internal friction: Effect of solvent quality and denaturant<|sep|>required. SDCRIF reduces to Rouse model in the limit ν = 1/2, kc = 0, ξint = 0 and to “
Informational Substitutes<|sep|>1.1 Motivation and challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 This paper: summary and contributions . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
End-to-End Memristive HTM System for Pattern Recognition and Sequence Prediction<|sep|>VER the course of the last decade, there has been a profound shift in artiﬁcial intelligence (AI) research, where biologically inspired computing systems are being actively studied to address the demand for energy-efﬁcient intelligent devices. Biologically inspired systems, such as hierarchical temporal memory (HTM) [1], [2], have demonstrated strong capability in processing spatial and temporal information with a high degree of plasticity while learning models of the world. HTM also exhibits natural compatibility for continuous online learning [3], noise and fault tolerance [4], and low power consumption achieved through sparse neuronal activity [5], [6]. These properties make the algorithm attractive for a wide range of applications such as visual object recognition and classiﬁcation [7], [8], prediction of data streams [9], natural language processing and anomaly detection [10]. Despite the fact that HTM is an attractive algorithm, it demands high computational power that cannot be fulﬁlled by conventional von Neumann architectures. This is because the innate HTM architecture, which is composed of thousands of neuronal circuits, requires a high-level parallelism in information processing. One may map the HTM algorithm onto a GPU. A GPU can provide the necessary parallelism, but it fails to provide satisfactory performance and demands a large power budget [11]. To this end, several research groups have attempted to develop specialized custom hardware designs to run the HTM algorithm • Abdullah M. Zyarah is with the Neuromorphic AI Lab, Department of Computer Engineering, Rochester Institute of Technology, NY 14623, USA (E-mail: amz6011@rit.edu). • Dhireesha Kudithipudi is with the Neuromorphic AI Lab, Department of Electrical and Computer Engineering, University of Texas at San Antonio, TX 78249 USA (E-mail: dk@utsa.edu). efﬁciently and affordably [12]. While some of the previous designs focused only on the spatial aspect of the HTM [13]– [15], other endeavors incorporated both the spatial and temporal models in the same design. For instance, in 2015, Zyarah et al. implemented the HTM algorithm including the spatial and temporal aspects [16]. The implemented network incorporates 100 mini-columns with 3 cells each, and is veriﬁed for image classiﬁcation and sequence prediction. Furthermore, it supports synthetic synapses, which are realized with distributed memory blocks, to enable synaptic pathway dynamics. The authors also optimized their design further in [17]. Weifu Li et al. [18], proposed a full architecture of the HTM algorithm in 2016. The proposed architecture is composed of 400 mini-columns (2 cells in each minicolumn) connected in point-to-point format to the HTM input space, which eventually causes the mini-column to be in an active mode even when there is insigniﬁcant activity (noise) in the input space. When it comes to HTM memristorbased analog and mixed-signal designs, in 2016, Fan et al. implemented the ﬁrst generation of HTM, HTM-Zeta. The authors proposed RCN (resistive-crossbar networks) pattern matching modules with core processing unit named, spinneurons [19]. The network operation is veriﬁed for image classiﬁcation in an ofﬂine fashion as the proposed design does not support online learning. In 2018, Krestinskaya et al. presented the full analog design of the HTM, but the temporal aspect of the implementation does not match that in the HTM sequence memory as it depends on the class map concept which matches the stored patterns with the test ones (unseen input samples) [20]. To the best of our knowledge, there is no full custom mixed-signal design of the HTM algorithm in literature with underlying digital communication scheme and analog computational modules. Such a design should include the necessary reconﬁgurability, low energy-delay product, and a robust communication scheme, in one platform. It is important to mention here that such architectures have been explored in the context of spiking neural networks (SNNs) [21], where the communication scheme is realized with address event representation (AER), developed by Mahowald in 1992 [22]. AER takes advantage of sparse neuronal activity and high-bandwidth VLSI to enable time-multiplexed communication. Hence, it reduces the number of connections between sending and receiving neuronal arrays from n to log2 n [23]. It turns out that AER is considered as an effective approach for point-topoint connections, but not for complex networks with sparse connections. The complex network connectivity is solved through the enhanced AER proposed by Goldberg et al. [24]. The enhanced AER uses look-up tables (LUTs) to describe the connectivity network between two sets of neuronal arrays. The LUT contains the sender address, destination address, and the probability of connectivity. Thus, complex networks even sparse ones can easily be implemented. However, the enhanced AER demands a large amount of memory and this makes it unsuitable for power and area constrained devices. Therefore, this paper also proposes a synthetic synapses representation (SSA) communication scheme, which leverages the linear feedback shift registers (LFSR)s to describe the sparse connections among neurons. Using the LFSRs eliminates the need for memory-based address description as the addresses between neurons are generated rather than stored. This results in a considerable reduction in the network area and power consumption. Speciﬁc key contributions of this paper are as follows: • Synthetic synapses representation (SSR) communication scheme is proposed to virtually formulate and prune the physical synaptic connections in the HTM network.
Universal network deployment model for universal connectivity<|sep|>The issue is simple: to allow and regulate the deployment of private networking infrastructures (such as private cables, towers) over public areas, that literally or conceptually belong to everyone and everything in this planet, in a way that generates a return to everyone, which preserves and directly contributes to universal connectivity. That return is in the form of paths of minimal or no cost. This way any privative investments in connectivity infrastructure for private beneﬁt, always results in an added value infrastructure for everyone. Instead of an “abstract” monetary tax return for private deployments, land and submarine cables should generate a mandatory return in terms of a portion of infrastructure sharing. In general terms, this return will be as open-access ﬁber managed collectively, as a commons. Many stakeholders may be interested in it, but unlike unlicensed radio-spectrum bands, a single/pair ﬁber has virtually unlimited potential for open-access communication for many under a commons governance. These ideas build on the proposal of guiﬁ.net for municipal deployments as a template for “ordinance for the deployment of access networks to new generation telecommunications services in universal format”. This template document is suitable for local administrations interested in promoting the deployment of access networks to broadband telecommunications services. The result is new data paths for public and community shared usage. We extend the concept of universal deployment deﬁned for the municipal scope, to the state level, and multi-state in the case of undersea cables. We ﬁrst describe the idea of universality of participation in the Internet from the recent UNESCO Universality Indicators. We then describe the universal deployment model proposed by the guiﬁ.net Foundation in the municipal context, and ﬁnally we describe the general principles of the model for any other deployment including regional and transnational deployments including undersea cables.
Control Barrier Functions With Unmodeled Dynamics Using Integral Quadratic Constraints<|sep|>This paper focuses on the design of control barrier functions (CBFs) for systems with unmodeled dynamics at the plant input. CBFs are used to design controllers that ensure the system remains within a safe set [1], [2]. Simpliﬁed, loworder models are often used for design. However, unmodeled dynamics (e.g. actuator lags, time delays, etc) can lead to safety violations as shown in Section II-B. This paper presents a method to design CBFs while accounting for unmodeled dynamics. The approach uses the integral quadratic constraint (IQC) framework for analysis of uncertain systems [3]. The main IQC result in [3] provides frequency-domain conditions for stability of uncertain linear time-invariant (LTI) systems. Related results have been formulated using time-domain dissipation inequalities [4], [5]. The speciﬁc IQC formulation used in this paper involves a time-domain integral with an exponential weighting (see Section IV-A). This is called an α-IQC1 and was introduced in [6], [8] for analysis of discrete-time optimization algorithms. A continuous-time formulation was given in [7]. Finally, αIQCs were used in [9] to bound the effect of unmodeled dynamics in the design of model predictive controllers. There is a large literature on CBFs with a good overview in [2]. The most closely related work on robust CBFs is brieﬂy summarized. Robust control barrier functions have been developed for guaranteeing safety in the presence of L∞ bounded disturbances [10], [11], [12], [13] or stochastic disturbances [14]. The work in [15] and [16] considers robust CBFs to account for variations in the model (changes to the vector ﬁelds) and input (sector-bounded) nonlinearities, respectively. A distinguishing feature of our paper is the ability Peter Seiler is with the Electrical Engineering and Computer Science Department, University of Michigan, Email: pseiler@umich.edu. Mrdjan Jankovic and Erik Hellstrom are with Ford Research and Advanced Engineering, P.O. Box 2053, MD 2036 SRL, Dearborn, MI 48121, USA, Emails: {mjankov1,jhells11}@ford.com 1The terminology “ρ-IQC” was ﬁrst used in [6] for the discrete-time case. Later the term “α-IQC” was used in [7] for the continuous-time formulation. to handle the effect unmodeled dynamics using α-IQCs. The implication is that the true state of the plant dynamics is only partially observed, i.e. the state of the unmodeled dynamics is not measured. Finally, we note that [17] provides a method to design CBFs for systems with known time delays. Our proposed method can handle unknown (but bounded) delays although with more conservatism than the approach in [17].
Isoscaling in statistical fragment emission in an extended compound nucleus model<|sep|>The search for a nuclear equation of state (EOS) has been one of the dominant factors driving interest in heavy-ion collisions at intermediate and high energies, ever since such beams have become available. Exploration of the isospin dependence of the EOS is an inseparable part of the task. Originally, the latter studies were conﬁned to use stable projectile beams with only a moderately wide range of isotopes between neutron-poor and neutron-rich nuclei. However, exotic secondary beams of projectile nuclei with extreme neutronto-proton ratios that have become available recently oﬀer new and intriguing opportunities to study isospin physics in heavy-ion collisions [1,2,3]. These studies have already resulted in the discovery of several interesting isospinrelated systematics [4,5,6,7], prompting further eﬀorts both, in experiments and in theoretical modeling. In particular, a so-called isoscaling analysis of light fragment emission in the decay of very hot nuclear systems produced in heavy-ion collisions has attracted attention as a possible tool for deducing the nuclear symmetry energy from the relative fragment yields [7,8,9,11,10]. For strongly damped collisions fragment isotopic yields were found to follow a “Qgg systematics” [12,13,14]. Isotopic scaling, termed “isoscaling” [7,8,9], is observed for other types of reactions such as evaporation [7,15], ﬁssion [16,17], projectile fragmentation [18,19], and multifragmentation [7,5,6]. This scaling law refers to a general exponential relation between the ratios of yields Yi(N, Z) for fragments (N, Z) emitted from systems which diﬀer only in their isospin or ( Ns Zs )i. In particular, if two reactions lead to primary systems i = 1 and 2 having approximately the same temperature but diﬀerent isospin, the ratio R21(N, Z) of the experimental yields of a given fragment (N, Z) emitted from these systems exhibits an exponential dependence on the fragment neutron number N and atomic number Z of the form, On the theoretical side, isoscaling has been extensively examined in the antisymmetrical molecular dynamics model [20], a Boltzmann-Uehling-Uhlenbeck model [5], a lattice gas model [21,22], the expanding evaporating source model [23] and in statistical multifragmentation models [8,9,11,24]. In the present paper it will be shown how the essential features of isoscaling observed in experiment are related to the nuclear symmetry energy in an extended compound nucleus (ECN) model. This model is closely related to that known from ﬁssion studies, and its central notion is a relatively high entropy (particularly at moderately high excitation energies) associated with the diﬀuse nuclear surface region (as opposed to bulk matter). Application of this ECN model reveals a new mechanism of nuclear fragmentation caused by the softening of the diﬀuse nuclear surface at high excitations [25]. The ECN model is equivalent to conventional compound nucleus models at low excitation energy as represented, e.g., by GEMINI [26], but greatly extends the validity of the compound nucleus concept towards high excitations [27]. Therefore, the ECN model provides a uniﬁed description of statistical emission of light particles and fragments in a wide range of excitation energies. While the present model is still somewhat schematic, it is based on a physically transparent picture and thus provides direct insight into the phenomena of interest.
Cosmic microwave background anomalies viewed via Gumbel Statistics<|sep|>Although the primary source of information in the cosmic microwave background (CMB) is the angular power spectrum, tests of other statistics provide an important consistency check for cosmological models. The key assumptions of Gaussianity and isotropy of the CMB, particularly of the WMAP data (Limon et. al. 2008), have both been challenged by more complex tests. The isotropy of the CMB has been strongly challenged with respect to power distribution (Hansen et. al. 2004) and multipole alignment (Land & Magueijo 2007; Copi et. al. 2004), and some evidence for non-Gaussianty has been noted among many statistical tests (Vielva et. al. 2004; Erikesen et. al. 2004; Mukherjee et. al. 2004; McEwen et. al. 2008). Many such tests use higher order statistics, moving beyond the correlation of pairs of sky pixels and using more complex comparisons. The statistics of CMB extrema have previously been studied by looking at the statistics of hot and cold spots (Larson & Wandelt 2005; Hou, Banday & Gorski 2009); here we address the inverse question: given patches of ﬁxed area, what are the statistics of the extremes within those patches? We develop and demonstrate the use of a new statistical approach, based on the statistics of extreme events. We ﬁnd that the test has weak discriminatory power but shows some suggestion of non-Gaussianity at low ℓ. Gumbel statistics describe the behaviour of sample extrema in the same way that Gaussian statistics are used to describe the behaviour of sample means. For example, if we draw n independent values from a Gaussian distribution N(µ, σ), the average of those values will also be Gaussiandistributed with mean µ and standard deviation σ/√ ilarly, the maximum and minimum of that sample will, in the limit, follow a Gumbel distribution (Castillo 2005). Just as the central limit theorem implies that sample means from any distribution will, in the limit, tend to a Gaussian proﬁle, all sample extrema will tend to a Gumbel distribution for suﬃciently large sample size. The distribution is also known in literature as the von Mises family of distributions (Castillo 2005). Gumbel statistics have found application in a number of ﬁelds, primarily ﬁnancial and meteorological. In this paper we apply Gumbel statistics to CMB-type maps and hence discuss cosmological applications of Gumbel distributions. Section 2 presents the mathematical basis of Gumbel statistics. In Section 3 we apply the formalism to the CMB and simulated maps to demonstrate our method. Section 4 compares the ﬁndings for the CMB to those for simulated Gaussian maps. Sections 5 applies Gumbel statistics to searching for potential non-Gaussianities in simulated maps.
Template based Graph Neural Network with Optimal Transport Distances<|sep|>Attributed graphs are characterized by i) the relationships between the nodes of the graph (structural or topological information) and ii) some speciﬁc features or attributes endowing the nodes themselves. Learning from those data is ubiquitous in many research areas [3], e.g. image analysis [21, 8], brain connectivity [30], biological compounds [23] or social networks [68], to name a few. Various methodologies approach the inherent complexity of those data, such as signal processing [54], Bayesian and kernel methods on graphs [41, 29] or more recently Graph Neural Networks (GNN) [63] in the framework of the geometric deep learning [8, 7]. We are interested in this work in the classiﬁcation of attributed graphs at the instance level. One existing approach consists in designing kernels that leverage topological properties of the observed graphs [5, 17, 19, 53]. For instance, the popular Weisfeiler-Lehman (WL) kernel [52] iteratively aggregates for each node the features of its k-hop neighborhood. Alternative approaches aim at learning vectorial representations of the graphs that can encode the graph structure (i.e. graph representation learning [10]). In this domain, GNN lead to state-of-the-art performances with endto-end learnable embeddings [63]. At a given layer, these architectures typically learn the node embeddings via local permutation-invariant transformations aggregating its neighbour features [37,
Effects of vertex corrections on diagrammatic approximations applied to the study of transport through a quantum dot<|sep|>Large-N expansions, with N representing the angular momentum degeneracy, are commonly used for solving the Anderson Impurity model (AIM) and to study the Kondo physics. In particular, the so-called non-crossing approximation (NCA) in its inﬁnite Coulomb repulsion limit captures the formation of the Kondo resonance at ﬁnite temperatures.1 When compared with the numerical renormalization group (NRG), the NCA scheme also provides the correct Kondo temperature (TK).2 This successful match results from the fact that NCA collects, in a self-consistent way, all diagrams up to order 1/N. However, it is well known that the Fermi liquid properties are not properly described. To deal with this deﬁciency, inﬁnite resummations of spin ﬂip terms are required.3–5 The leading crossing diagrams that restore the proper energy scale were introduced in the early work by Pruschke and Grewe within the framework of the enhancedNCA (ENCA) also called One Crossing Approximation (OCA).7 Unlike NRG and Quantum Monte Carlo, which are actually more robust techniques, the OCA method can be extended to multi orbital systems without much numerical eﬀort. It also works on the real axis and can go to temperatures far below the Kondo one. These features make OCA a powerful impurity solver when combined with electronic structure calculations within the context of the Dynamical Mean Field Theory (DMFT).8 Due to the interesting advantages mentioned above, it is important to check the role of vertex corrections on diagrammatic techniques in the calculations of diﬀerent properties. Recently, the inﬂuence of vertex functions incorporated by OCA has already been studied for general lattice problems,9 multi-orbital Anderson models,10 and dynamic susceptibilities of the AIM,11 among others. In this work, we study the eﬀect of vertex corrections on transport properties, in particular transport through a quantum dot (QD) weakly coupled to metallic contacts. The NCA results for the conductance, in the linear-response regime, were previously analyzed by Gerace et al..12 The aim of our contribution is to compare the equilibrium conductance, as a function of temperature and gate voltage, as calculated by NCA and OCA methods. Furthermore, we analyze the scaling properties of the conductance by comparison with the empirical formula extracted from NRG calculations. We observe that the diagrams omitted within NCA are really important for appropriately describing transport phenomena. Furthermore, the vertex corrections introduced by OCA partially recover the universal scaling behavior of the conductance as a function of temperature. The paper is organized as follows. In Sec. II we introduce the model and the calculation methods. In Sec. III we present and discuss numerical calculations. Finally, in Sec. IV some conclusions are drawn.
Dark Energy Coupled with Dark Matter in Viscous Fluid Cosmology<|sep|>The appearance of new cosmological models is connected with the discovery of the accelerated expansion of the universe. Cosmic acceleration can be introduced via dark energy or via modiﬁcation of gravity; cf. Nojiri and Odintsov (2011). Recently, a general review of dark energy was given in Bamba et al.
A frequency quintupled laser at 308 nm for spectroscopy of intercombination lines in zinc<|sep|>High-power, continuous-wave (cw), and narrowlinewidth laser sources are available only at visible and infrared (IR) wavelengths. To generate such radiation at UV wavelengths, nonlinear frequency conversion has been tremendously successful. Fiber lasers and ﬁber ampliﬁers combine high power output with very low intensity and phase noise, which are highly desired properties for spectroscopy and atomic physics experiments, including laser cooling and optical clocks. The output wavelength of ﬁber laser is limited to the IR spectrum: roughly 1030 to 1120 nm for Yb-doped ﬁbers, 1535 to 1580 nm for Er-doped models, and 1900 to 2100 nm for Tm-doped models. Most relevant atomic transitions, however, appear at UV and visible wavelengths. Here, we report on the development of a frequencyquintupled Er-doped ﬁber laser to access the wavelength range of about 307 to 316 nm. This wavelength range covers a range of transitions that are relevant for laser cooling and optical clocks, including the cooling transitions of Be+ ions (313 nm) and neutral Zn atoms (308 nm), the clock transition in Zn (310 nm), and the direct Rydberg excitation in Cs (318 nm). It also includes the well-established 308-nm emission wavelength of XeCl excimer lasers. Our setup does not involve optical cavities, which would increase the conversion eﬃciencies, but also add considerable complexity to the system. Instead, we start with a high-power fundamental laser source and employ single-pass frequency conversion. As a result, the setup described here requires only a small footprint and is highly robust. These assets, combined with the performance of well-established ﬁber laser technology, are highly beneﬁcial for the applications mentioned above. This applies especially to transportable laser systems. Our scheme of generating cw output at the odd harmonics could readily be adapted to ﬁber laser models with other fundamental wavelengths [1].
On Reliability-Aware Server Consolidation in Cloud Datacenters<|sep|>The role of Cloud computing and its applications in our daily life are growing exponentially. In this way, users can use these applications (e.g. search engines, email, ﬁle storage) without the need to own the service or infrastructure. These clouds provide wide range of services hosted by DCs in a ”pay-as-you-go” manner, which helps organizations to reduce the CAPEX and OPEX costs and focus on their core business. Due to unpredictable and growing demand for Internet-based services and resources, DCs computing and storage capacities has been increased signiﬁcantly. Consequently, there has been a rapid rise in energy consumption and carbon dioxide (CO2) footprints of these DCs, which are a major challenge in both industry and academia [1]. Physical resources, along with networking and cooling devices are the main power consumers in DCs. However, the average utilization of physical resources in cloud DCs is relatively low and it is between 10% and 50% [2]. This could lead to massive energy wastage, because an idle server consumes at about 70% of its peak draw [3], [4]. To cope with this challenge, server consolidation technique is widely used in cloud DCs. This technique, which is working based on virtualization technology, pack DC virtual machines (VMs) on minimum number of Physical Machines (PMs) to improve resource utilization and decrease the energy consumption by shutting down idle servers [5]. On the other hand, because of over-aggressive consolidation methods, combined with DC workload ﬂuctuations, the turned off servers would be turned on to serve the incoming workload. These repeated on-off cycles have several negative impacts on servers wear-and-tear and reliability (i.e. aging), and hence replacement and procurement costs: 1) Repeated high transition frequency and on-off cycles are recognized as the most crucial factor impairing disk reliability [6], [7]. 2) On-off thermal cycle of CPU, which is another factor causing server failures [8], [9]. Therefore, repeated on-off cycles of PMs in consolidation approaches, increase the wear-and-tear of server components, incurring replacement and procurement costs and also partial or complete service(s) downtime that costs $5,000 per minute [10]. Thus, in addition to short-term energy savings, long-term reliability and maintenance costs are also an important issue that needs to address. Hence, in this paper, the key question to answer is: ”Considering DC energy consumption (PMs, cooling, and network), reliability, and migration costs, how server consolidation should be performed to minimize the total DC costs” In response, this paper presents a mathematical model with the objective of minimizing total DC costs. We analyze and characterize the energy and reliability costs in a DC. Using these costs, we formulate the above problem as a Mixed Integer Linear Programming (MILP) mathematical model which is in form of NP-complete. Moreover, we simulate the proposed approach in MATLAB software and then evaluate the performance of the presented approach through extensive simulation experiments. Therefore, the main points of this paper could be summarized as follows: 1) Providing a mathematical model for reliability-aware server consolidation in cloud DCs. 2) Taking disk and CPU reliability impacts on PMs into account to provide a reliability-friendly server consolidation approach. 3) Considering rack structure (including network and cooling devices) In addition to PMs and VM migration costs for V Set of VMs, V = {v1, v2, ..., vV} P Set of PMs, P = {p1, p2, ..., pP} R Set of Racks, R = {r1, r2, ..., rR} t Time-slot index τ Time-slot duration Cene Total server consolidation energy cost Crel Total server consolidation reliability cost Grel Total server consolidation reliability gain cpm i Energy cost of pi crack i Energy cost of ri cmig Total energy cost of VM migrations S′, S VM-to-PM mapping matrices for time-slot t and t + 1 cdisk i Disk reliability cost for pi ccpu i CPU reliability cost for pi cT oR ToR switch energy consumption ccooling Rack cooling device energy consumption Ru i ith VM requirement for resource type u ∈ U ¯Cu i ith PM total capacity of resource type u ∈ U The rest of this paper is organized as follows: we start by discussing related work in literature (section II). In sections III, we present the system model and formulations. Then, we describe the proposed mathematical model in section III. The performance of the presented approach is evaluated in section V, and ﬁnally, section VI concludes the paper along with some future directions.
Efficiency of Exponentiality Tests Based on a Special Property of Exponential Distribution<|sep|>The general problem of exponentiality testing is stated as follows. Let X1, . . . , Xn be nonnegative independent observations having a continuous distribution function (df) F and a density f. We wish to test the composite null-hypothesis H0 : F(x) is a df of an exponential law with density f(x) = λe−λx, x ≥ 0, where λ > 0 is an unknown scale parameter, against the following alternative: F is a df of a nonexponential law. There exist numerous tests of exponentiality based on various ideas [2], [4], [6], [8], [13], [29]. Among them a good few tests are based on characterizations. This is a relatively fresh idea which manifests growing popularity in goodness-of-ﬁt testing, and in particular, in exponentiality testing, see, e.g., [3], [7], [12], [16], [18], [27], [31], [37], [40], [42]. Recently Noughabi and Arghami proved and used in [40, Theor.1] the following ”characterization” of exponential law for testing of exponentiality: Let X1, X2 be two independent identically distributed nonnegative rv’s having a continuous df F. Then Y = X1/X2 has the df F(2,2) if and only if F is exponential. Here F(2,2) is the df of Fisher distribution with 2 and 2 degrees of freedom so that In fact this property is not the proper characterization of exponential law. This is known since the paper of Kotlarski [23] which was preceded by the work of Mauldon [26]. In particular, Kotlarski gave three examples of non-exponential densities for X1 and X2 under which the distribution of Y is still F2,2. These three densities are Presumably Noughabi and Arghami got the erroneous result because of inaccurate application of the characterization result of Kotz and Steutel [24]. The same concerns item iii) of their Theorem 1 in [40]. However, one can build the statistical tests based on properties of distributions which are not the proper characterizations as well. Of course, this will lead to inconsistency of such tests against certain alternatives. But many famous tests well known in statistical practice are inconsistent against certain special alternatives, for instance, the chi-square test, the Wilcoxon test (and many other rank tests), the Gini test, and even the likelihood ratio test. Moreover, according to the usual concepts of testing statistical hypotheses, the evidence can be suﬃcient only for the rejection of the null-hypothesis H0. On the contrary, its deﬁnitive acceptance is hardly possible but any new test ”failing to reject” H0 gradually brings the statistician to the perception of the validity of H0. The aim of the present paper is to test the hypothesis H0 using the same property of exponential law as used in [40] and formulated above. We will construct two test statistics which turn out to be quite sensitive and eﬃcient. We justify it by calculation of their local Bahadur eﬃciency against common alternatives and by simulation of their power. Consider instead of the standard empirical df It is known that the properties of U-empirical df’s are similar to the properties of usual empirical df’s, see [11], [15]. Hence for large n and under H0 the df Hn should be close to Fisher’s df F(2,2), and we can measure their closeness using some test statistics. We suggest two scale-invariant statistics assuming that their large absolute values are critical. We have inserted the exponential weight with some indeﬁnite value of µ > 0 under the sign of integral in order to guarantee its convergence but for brevity we omit µ in the notation of statistic. We discuss the limiting distributions of these statistics under the null hypothesis and calculate their eﬃciencies against common alternatives. We use the notion of local exact Bahadur eﬃciency (BE) [5], [30], as the statistic Dn has the nonnormal limiting distribution, and hence the Pitman approach to the calculation of eﬃciency is not applicable. However, it is known that the local BE and the limiting Pitman eﬃciency usually coincide, see [43], [30]. The large deviation asymptotics is the key tool for the evaluation of the exact BE, and we address this question using the results of [34] and [32]. Finally, we study the conditions of local optimality of our tests and describe the ”most favorable” alternatives for them. We present the simulated powers of new tests and enlarge the paper by the example of application to real data. Namely, as an application of new exponentiality tests, we examine the interesting question on the durations of reigns for Roman emperors discussed by Khmaladze and his coauthors [19], [20]. Our tests ﬁrmly reject the hypothesis of exponentiality, and this contradicts the ﬁndings of Khmaladze and his team, see also [9] and [38]. We stress that usually in the papers on testing based on characterizations one uses the equality in distribution of two statistics T1 and T2: which characterizes the family of distributions or some speciﬁc property, e.g., symmetry of distribution. But in our paper we use a diﬀerent relation when a certain statistic has the prescribed distribution, and this characterizes or strongly restraints the distribution of the sample. It seems probable that other tests of ﬁt can be build on the ground of this apparently new approach.
Free Lunch for Few-shot Learning: Distribution Calibration<|sep|>white wolf 97% 97% malamute 85% 78% lion 81% 70% meerkat 78% 70% jellyﬁsh 46% 26% orange 40% 19% beer bottle 34% 11% Learning from a limited number of training samples has drawn increasing attention due to the high cost of collecting and annotating a large amount of data. Researchers have developed algorithms to improve the performance of models that have been trained with very few data. Finn et al. (2017); Snell et al. (2017) train models in a meta-learning fashion so that the model can adapt quickly on tasks with only a few training samples available. Hariharan & Girshick (2017); Wang et al. (2018) try to synthesize data or features by learning a generative model to alleviate the data insufﬁciency problem. Ren et al. (2018) propose to leverage unlabeled data and predict pseudo labels to improve the performance of fewshot learning. While most previous works focus on developing stronger models, scant attention has been paid to the property of the data itself. It is natural that when the number of data grows, the ground truth distribution can be more accurately uncovered. Models trained with a wide coverage of data can generalize well during evaluation. On the other hand, when training a model with only a few training data, the model tends to overﬁt on these few samples by minimizing the training loss over these samples. These phenomena are illustrated in Figure 1. This biased distribution based on a few examples can damage the generalization ability of the model since it is far from mirroring the ground truth distribution from which test cases are sampled during evaluation. Figure 1: Training a classiﬁer from few-shot features makes the classiﬁer overﬁt to the few examples (Left). Classiﬁer trained with features sampled from calibrated distribution has better generalization ability (Right). Here, we consider calibrating this biased distribution into a more accurate approximation of the ground truth distribution. In this way, a model trained with inputs sampled from the calibrated distribution can generalize over a broader range of data from a more accurate distribution rather than only ﬁtting itself to those few samples. Instead of calibrating the distribution of the original data space, we try to calibrate the distribution in the feature space, which has much lower dimensions and is easier to calibrate (Xian et al. (2018)). We assume every dimension in the feature vectors follows a Gaussian distribution and observe that similar classes usually have similar mean and variance of the feature representations, as shown in Table 1. Thus, the mean and variance of the Gaussian distribution can be transferred across similar classes (Salakhutdinov et al. (2012)). Meanwhile, the statistics can be estimated more accurately when there are adequate samples for this class. Based on these observations, we reuse the statistics from manyshot classes and transfer them to better estimate the distribution of the few-shot classes according to their class similarity. More samples can be generated according to the estimated distribution which provides sufﬁcient supervision for training the classiﬁcation model. In the experiments, we show that a simple logistic regression classiﬁer trained with our strategy can achieve state-of-the-art accuracy on three datasets. Our distribution calibration strategy can be paired with any classiﬁer and feature extractor with no extra learnable parameters. Training with samples selected from the calibrated distribution can achieve 12% accuracy gain compared to the baseline which is only trained with the few samples given in a 5way1shot task. We also visualize the calibrated distribution and show that it is an accurate approximation of the ground truth that can better cover the test cases.
Applicability of Large Corporate Credit Models to Small Business Risk Assessment<|sep|>A small business is one that has fewer than 1,500 employees and a maximum of $38.5 million in average annual receipts, according to the Small Business Administration. Small businesses, though, often lack the size, assets (for collateral), ﬁnancial history, or data that are typically used by traditional ﬁnancial institutions to assess credit worthiness. While lenders extended nearly $650B worth of loans to small businesses in 2019 [1], the Federal Reserve reports this represents less than half of these businesses credit needs [2]. Large public businesses, on the other hand, provide a rich credit dataset. SEC rules require these businesses to publicly report detailed ﬁnancial data in the form of proﬁt and loss, balance sheet, and cash ﬂow statements. Three main ratings agencies publish credit ratings for public companies: Standard & Poor’s (S&P), Moody’s, and Fitch. We study the applicability of deep learning-based credit models derived from large public corporate data to small businesses. We create a DL-based model to predict the credit ratings of large public companies from their ﬁnancial statement data. We then explore the applicability of the model in forecasting adverse events or the probability of default of a small business on loan payments.
FormuLog: Datalog for static analysis involving logical formulae<|sep|>The logic programming language Datalog has become a popular domain-speciﬁc language for writing static analyses (Reps 1995; Whaley et al. 2005; Bravenboer and Smaragdakis 2009; Scholz et al. 2016; Madsen et al. 2016). As a declarative language that embodies Kolwaski’s principle of separating a program’s “logic” (what it computes) from its “control” (how it computes it) (Kowalski 1979), Datalog is a good ﬁt for programming a static analysis, where often the logic of the analysis is substantially simpler than the control necessary to efﬁciently compute it. In particular, many analyses logically consist of mutually recursive sub-analyses that in theory interact elegantly, but in practice can be tricky to coordinate. Datalog makes it easy to state the dependencies found in these types of analyses, enabling analysis designers to focus on perfecting the logic of their analyses without worrying about the low-level control details. Because Datalog is a very restricted language and there are many analyses that cannot easily be expressed in pure Datalog, implementations of Datalog for static analysis have extended the language with additional features. For instance, Soufﬂ´e adds a form of n-ary constructors (Scholz et al. 2016), and Flix adds user-deﬁned functions and support for reasoning over lattices (Madsen et al. 2016). However, there are still common analyses that cannot be naturally expressed even
Contrastive Entropy: A new evaluation metric for unnormalized language models<|sep|>There are two standard evaluation metrics for language models: perplexity and word error rate (WER). The simpler of these two, WER, is the percentage of erroneously recognized words E (deletions, insertions, substitutions) to the total number of words N in a speech recognition task i.e. The second metric, perplexity (per word), is an information theoretic measure that evaluates the similarity between the proposed probability distribution m and the original distribution p. It can be computed as an inverse of the (geometric) average probability of test set T where n is the number of words in the test set T. In many ways, WER is a better metric. Any improvements on language modeling benchmarks is meaningful only if they translate to improvements in Automatic Speech Recognition (ASR) or Machine Translation. The problem with WER is that it needs a complete ASR pipeline to evaluate. Also, almost all benchmarking datasets are behind a pay-wall, hence not readily available for evaluation. Perplexity, on the other hand, is a theoretically elegant and easy to compute metric which correlates well with WER for simpler n-gram models. This makes PPL a good substitute for WER when evaluating n-grams models, but for more complex language models the correlation is not so strong [1]. In addition to this, due to its reliance on exact probabilities, perplexity is an unsuitable metric to evaluate unnormalized models for which the partition function is intractable. Also, when comparing two models using perplexity, they must share the same vocabulary. Most of the previous work done to improve upon perplexity has been focused on achieving better correlation with WER. Iyer et al. [1] proposed a decision tree based metric that uses additional features like word length, POS tags and phonetic length of words to improve the WER correlation. Chen et al. [2] proposed a new metric M-ref which attempts to learn the likelihood curve between WER and perplexity. Clarkson et al. [3] attempted to use entropy in conjunction with perplexity— empirically learning the mixing coefﬁcients. In this paper we focus on a different problem, the problem of extending perplexity for unnormalized language models evaluation. We do so by introducing a discriminative approach to language model evaluation. Our approach is inspired by Contrastive Estimation [4] and stems from the philosophical starting point that a superior language model should be able to distinguish better between the sentence from the test set and its deformed version. While we use an unnormalized sentence level model as an example in this paper this technique should work for all models where partition function is intractable, for example unnormalized Model M and feed forward neural network language model (NNLM) from [5] or sentence level models like [6], [7] and [8]. In the next section, we give a sketch derivation of perplexity that highlights its word level model assumption. As we will be using a sentence level language model for evaluation, we then move the probability space to sentences and derive an expression for cross entropy rate for sentence level models. In Section 3, we introduce our new discriminative metric, Contrastive Entropy, which removes the normalization requirement associated with perplexity. In Section 4, we formulate recurrent neural networks as sentence level language models that we use for validation and in Section 5 we analyze this new metric across various models on the Pen-TreeBank section of the WSJ dataset. We conclude this paper by hypothesizing a better correlation between WER and contrastive entropy based on the fact they share the same goal of minimizing errors in prediction.
Variational Depth from Focus Reconstruction<|sep|>The basic idea for depth from focus (DFF) approaches is to assume that the distance of an object to the camera at a certain pixel corresponds to the focal setting at which the pixel is maximally sharp. Thus, for a given data set of diﬀerently focused images, one typically ﬁrst ﬁnds a suitable contrast measure at each pixel. Subsequently, the depth at each pixel is determined by ﬁnding the focal distance at which the contrast measure is maximal. Figure 1 illustrates this approach. DFF diﬀers from depth from defocus (cf. [1–3]) in the sense that many images are given and depth clues are obtained from the sharpness at each pixel. Depth from defocus on the other hand tries to estimate the variance of a spatially varying blur based on a physical model and uses only very few images. Generally, the measurements of diﬀerently focused images do not necessarily determine the depth of a scene uniquely, such that the estimation of a depth map is an ill-posed problem. The ambiguity of the depth map is particularly strong in textureless areas. The literature dealing with the shape from focus problem has very diﬀerent contributions. A lot of work has targeted the development of diﬀerent measures for sharpness and focus (cf. [4] for an overview). Other works deal with diﬀerent methods of ﬁltering the contrast coeﬃcients before determining the maximum, i.e. the depth. The ideas range from windowed averaging (e.g. [5]), over diﬀerently shaped kernels, to nonlinear ﬁltering as proposed in [6]. In [7,8], the authors proposed to detect regions with a high variance in the depth map and suggested to smooth the depth in these parts by interpolation. Pertuz, Puig, and Garcia analyzed the behavior of the contrast curve in order to identify and remove low-reliability regions of the depth map in [9]. Ideas for using diﬀerent focus measures and fusing the resulting depth estimates in a variational approach using the total variation (TV) regularization have been proposed in [10]. However, very little work has been done on ﬁnding a noisefree depth map in a single variational framework. Formulating the shape from focus problem in a variational framework has the advantage that one clearly deﬁnes a cost function and tries to ﬁnd a solution which is optimal with respect to the costs. More importantly, regularity can be imposed on the depth estimate itself, e.g. by favoring piecewise constant solutions. Additionally, our framework is robust to noise and controls the ill-posedness of the shape from focus problem. ∗M.M. (corresponding author, email michael.moeller@in.tum.de) is with the Department of Mathematics, Technische Universit¨at M¨unchen, Boltzmannstrasse 3, 85748 Garching. M.B. is with the Institute of Mathematics and Image Computing (MIC), Universit¨at zu L¨ubeck, Maria-Goeppert-Str. 3, 23562 L¨ubeck, Germany. C.S. is with the Department of Applied Mathematics and Theoretical Physics, Centre for Mathematical Sciences, Wilberforce Road, Cambridge, CB3 0WA, United Kingdom. D.C. are with the Department of Computer Science, Technische Universit¨at M¨unchen, Boltzmannstrasse 3, 85748 Garching. Figure 1: Example of a simple DFF reconstruction: The data set of images can be visualized as a data cube (a), where the z-direction corresponds to a change of focus from the front to the back. In order to determine the focal setting at which the contrast is maximal, one picks a contrast measure, applies a windowed ﬁltering to the contrast coeﬃcients and selects the focal setting for which the coeﬃcients are maximal. By knowing the distance at which a region appears maximally sharp in a focal setting, one reconstructs the depth. Figure (b) shows an example of the depth map, where red values indicate being far away from the camera and blue values correspond to pixels close to the camera. The result was obtained by using the modiﬁed Laplacian contrast measure with 9 × 9 windowed ﬁltering as used in the comparison in [4]. While variational methods belong to the most successful class of methods for general image reconstruction tasks and have successfully been used in several depth from defocus approaches (c.f. [3, 11–13]), very little work has been done on using them for the DFF problem. The only work the authors are aware of is the method proposed in [14], where the framework of Markov random ﬁelds was used to derive an energy minimization method consisting of two truncated quadratic functionals. However, using two nonconvex functionals in a setting where the dependency of the depth on the contrast is already nonconvex, results in a great risk of only ﬁnding poor local minima. For instance, any initialization with pixels belonging to both truncated parts is a critical point of such an approach. This paper has two contributions: First, we propose a variational framework for the shape from focus problem using the total variation (TV) regularization [15]. Secondly, we discuss the problem of minimizing the resulting nonconvex energy eﬃciently. While schemes that can guarantee the convergence to critical points are computationally expensive, we propose to tackle the minimization problem by an alternating minimization method of multipliers (ADMM) with an additional linearization of the nonconvex part.
Distortion of Magnetic Fields in a Starless Core IV: Magnetic Field Scaling on Density and Mass-to-flux Ratio Distribution in FeSt 1-457<|sep|>Magnetic ﬁelds are believed to play an important role in controlling the formation and contraction of dense cores in molecular clouds. The determination of the relationships between the magnetic ﬁeld strength |B| and the gas volume density ρ, usually expressed in a power law form as |B| ∝ ρκ, is important because they are related to the accumulation history of both the magnetic ﬂux and the cloud material (e.g., Crutcher 1999). The |B|–ρ relationship is also crucial in order to compare the magnetic ﬁeld and internal density structure observations with theory. If an initially uniform magnetic ﬁeld pervading a diﬀuse medium is assumed as a starting condition of the mass accumulation to form dense cores, the |B|–ρ relationship of the core depends on 1) the shape of the progenitor cloud (e.g., spherical, cylindrical), 2) the magnetic ﬁeld geometry (i.e., parallel or perpendicular or inclined geometry with respect to the elongation axis of the core), and 3) the direction of contraction (i.e., isotropic contraction or contraction toward a speciﬁc direction). In the case of (spherical) isotropic contraction, the conservation of magnetic ﬂux (Φ = πR2|B|) yields |B| ∝ R−2 (R is the radius of the core) and the conservation of mass (M = (4/3)πR3ρ) yields ρ2/3 ∝ R−2, providing the |B|–ρ relationship as |B| ∝ ρ2/3. This corresponds to the prediction of the relatively weak magnetic ﬁeld case (Mestel 1966). Note that isotropic contraction does not necessarily mean spherical cloud shape, merely that the shape be conserved during the contraction. However, if the initial axial ratio of the cloud is large, the shape of the cloud becomes more elongated during the contraction by the eﬀect of gravity. In the case of the plane-parallel or inﬁnite thin disk geometry, the conservation of magnetic ﬂux (Φ = πR2|B|) and mass (M = πR2zρ) yields ρz/|B| = constant, where z is the distance perpendicular to the plane. In this geometry, the force balance between self-gravity and internal thermal pressure along the symmetry axis is 2πGρz2 ≈ C2 s (Spitzer 1942), where Cs is the sound speed. Therefore, |B| ∝ (ρT)1/2 (T is the gas temperature), and in the isothermal case, |B| ∝ ρ1/2 (see, Crutcher 1999). On the basis of large samples with Zeeman measurements of the line-of-sight magnetic ﬁeld strength Blos and Bayesian statistical analysis, Crutcher et al. (2010) concluded that the data prefer κ ≈ 2/3 (|B| ∝ ρ0.65±0.05 for ρ > 300 cm−3) and reject κ ≈ 1/2. They also showed the existence of two distinct branches on the B versus ρ diagram, a ﬂat region at low densities (|B| independent of ρ, i.e., κ ≈ 0) and a power-law scaling region at high densities (κ ≈ 2/3). A recent study reported results contrary to those reported by Crutcher et al. (2010) based on the re-analysis of the same observational data (κ ≈ 1/2 is preferred; Tritsis et al. 2015). Note that Crutcher et al. (2010) analyzed the full set of Zeeman data including non-detections, whereas Tritsis et al. (2015) only analyzed the observational data with Zeeman detection (this may cause the biased results with stronger magnetic ﬁeld strength and smaller κ). Several κ measurements with smaller samples have been conducted. Li et al. (2015) obtained κ = 0.41±0.04 toward the clouds and cores in the NGC 6334 complex based on the measurements of Bpos by comparing the curvature of the plane-of-sky magnetic ﬁeld lines with self-gravity. Ching et al. (2017) obtained κ = 0.54 ± 0.30 toward the cores in the dense ﬁlamentary cloud DR21 based on the submillimeter (submm) dust emission polarimetry and the Chandrasekhar– Fermi method (Chandrasekhar & Fermi 1953). Hoq et al. (2017) obtained κ = 0.73 ± 0.06 toward the ﬁlamentary infrared dark cloud (IRDC) G28.23-00.19 based on near-infrared (NIR) dust extinction polarimetry and the Chandrasekhar–Fermi method. Observations show a variety of κ values ranging from κ ≈ 1/2 to κ ≈ 2/3. Therefore, it is important for observational studies to provide the deﬁnite value of κ through much larger samples or much more accurate measurements, although it is possible that the value of κ varies from region to region, depending on the shape of objects or the type of contractions or other characteristics. Note that there is no observation of the |B|–ρ relationship determined using a single molecular cloud core. From a theoretical point of view, Mouschovias (1976a,b) showed that the ratio of magnetic and gas pressure (B2/8πP) tends to remain constant, ≈ 1, inside the magnetized cloud during collapse. This yields |B| ∝ ρ1/2 for the isothermal case (i.e., P = ρC2 s , where Cs is the isothermal sound speed). Numerical simulation of the ambipolar diﬀusion driven core contraction (Fiedler & Mouschovias 1993) provided κ ≈ 0.47 which is consistent with a κ value of 1/2. Ciolek & Mouschovias (1994) obtained relatively smaller values of κ = 0.38 − 0.43. Mouschovias (1991) suggested that the magnetic ﬁeld in molecular clouds depends on both the density and the velocity dispersion σv as |B| ∝ ρ1/2σv. Basu (2000) showed that there is a good correlation between Blos and ρσv in observations, providing Blos/σv ∝ ρ0.50±0.12. If the velocity dispersion does not depend on the density, this is consistent with the relation of |B| ∝ ρ1/2. In contrast, recently Li, McKee & Klein (2015) conducted a large-scale magneto-hydrodynamic (MHD) simulation of isothermal, self-gravitating gas with a slightly magnetically supercritical initial magnetic ﬁeld. A κ value of 0.70 ± 0.06 was obtained, and the result is consistent with the value obtained by Crutcher et al. (2010) of κ ≈ 2/3 (Btot ∝ ρ0.65±0.05). The ﬂat low density region and the high density region following a power law relation (κ = 0.65) on the |B| vs. ρ diagram are reproduced in their simulation. Furthermore, it was found that the velocity dispersion scales weakly with density as σv ∝ ρ0.14±0.05, which is also consistent with the result of κ ≈ 2/3. Theoretical studies have revealed a variety of κ values ranging from κ ≈ 1/2 to κ ≈ 2/3. Further theoretical studies are desirable in this ﬁeld. Another critical parameter for magnetic ﬁeld theories is the ratio of the mass M in the ﬂux tube to the magnitudes of magnetic ﬂux Φ, which is often expressed as the observational parameter normalized by the theoretical critical value, λ = (M/Φ)obs/(M/Φ)critical. Since the magnetic support and the gravity have same radial dependence, the collapse of dense cores cannot be stopped by magnetic ﬁelds once gravity overcomes magnetic ﬁelds. Theoretical determination of the critical value is thus important. The critical value suggested by theory can be written as (M/Φ)critical = cΦ/ √ G, and Mouschovias & Spitzer (1976) found cΦ ≈ 0.126 for disks with support along magnetic ﬁeld lines. Tomisaka et al. (1988) found a consistent value based on extensive numerical calculations as cΦ ≈ 0.12. Nakano & Nakamura (1978) derived cΦ = 1/2π with a linear perturbation analysis for the magnetized isothermal gaseous disk. Note that the mass-to-ﬂux ratio depends on cloud geometries, and (M/Φ)critical = [3π � G/5]−1 can be obtained for a uniform sphere under virial equilibrium between gravity and the magnetic ﬁeld, 3GM2/5R = B2R3/3 (Crutcher 2004). Thus, cΦ ≈ 2/3π for the spherical case. Molecular cloud cores in various regions tend to show projected aspect ratios of 2:1 (e.g., Myers et al. 1991; Jijina et al. 1999), and de-projection analyses for revealing the intrinsic shape of dense cores were reported (e.g., Jones et al. 2001: triaxial shape, Tassis et al. 2007: oblate shape with ﬁnite thickness). Therefore, in general, observational studies need assumption on the shape of the core when choose and use the theoretical critical value, although the value of cΦ = 1/2π (Nakano & Nakamura 1978) has been widely used. Without information of line-of-sight inclination angle of magnetic ﬁeld direction, λ was statistically estimated assuming random orientation of the inclination angle for many target cores. After statistical geometric correction, Crutcher (1999) and Troland & Crutcher (2008) obtained λ ≈ 2 based on the OH Zeeman observations of dark cloud cores, and the CN Zeeman observations by Falgarone et al. (2008) showed consistent results. Thus, typical dense cores seem to be in a state of slightly magnetically supercritical condition. However, these results have a problem that the statistical analysis eliminates the information of the diversity of the magnetic ﬁelds for each core. In order to know λ for each core and discuss the magnetic ﬁeld condition of the core in detail, it is necessary to obtain the information of the magnetic inclination angle θinc. If θinc is known in addition to ρ and κ, the distribution of λ can be obtained from the center of the core to its envelope. As stated by Crutcher (2004), the λ value at the cloud envelope provides a crucial test for magnetic support models of star formation. In the present study, the |B|–ρ relationship was constructed for the starless dense core FeSt 1-457 based on the NIR polarimetric observations of the dichroic polarization of dust toward the background stars. A modiﬁed form of the Chandrasekhar–Fermi method, which enables the determination of the value of κ, was used. With information of the magnetic ﬁelds (κ and θinc) and the cloud density distribution, the distribution of mass-to-magnetic ﬂux was obtained, and physical status of FeSt 1-457 was discussed. The mass-to-ﬂux ratio distribution for the case of critical Bonnor–Ebert sphere with λ = 2 was calculated in order to evaluate the behavior of the distribution for typical dense cores. FeSt 1-457 is known to be accompanied by an hourglass-shaped magnetic ﬁeld (Kandori et al. 2017a, hereafter Paper I), and the three dimensional (3D) modeling of the ﬁeld provides the magnetic ﬁeld curvature and the line-of-sight inclination angle of the magnetic ﬁeld direction θinc (Kandori et al. 2017b, Paper II). The total magnetic ﬁeld strength of the core is 33.7 ± 18.0 µG with a ratio of the observed mass-to-magnetic ﬂux to a critical value of λ = 1.41 ± 0.38 (magnetically supercritical, Paper II). These analyses seem reliable, because observed NIR polarizations of stars show linear relationship with respect to the dust extinction, indicating that magnetic ﬁelds inside FeSt 1-457 is traced by the NIR polarimetry (Kandori et al. 2018, Paper III). The fundamental physical parameters of FeSt 1-457 have been well deﬁned in an internal density structure study based on NIR extinction measurements of the background stars and ﬁtting with the Bonnor–Ebert sphere model (Ebert 1955; Bonnor 1956). The radius, mass, and central density of the core are 18,500 AU (144′′), 3.55 M⊙, and 3.5 × 105 cm−3 (Kandori et al. 2005), respectively, at a distance of 130+24 −58 pc (Lombardi et al. 2006). Throughout this paper, the spherical shape was assumed for the core geometry, and (M/Φ)critical = 1/2π √ G (for disk geometry: Nakano & Nakamura 1978) was used for the theoretical critical mass-to-ﬂux ratio. Though FeSt 1-457 was well ﬁtted using the Bonnor–Ebert sphere model, the elongation in column density structure appears around the core center, which may be the existence of disk-like structure around center. The theoretical critical value for spherical geometry is larger than that for disk geometry, and we thus use the value of 1/2π √
Shape and Angular Distribution of the 4.438-MeV Line from Proton Inelastic Scattering off 12C<|sep|>The shape of the 4.438-MeV line of 12C emitted during solar ﬂares is an important observable that may be used to determine the composition, energy spectra and directional distribution of the accelerated particles interacting in the solar atmosphere. Up to now, its line shape could be studied in several ﬂares observed by SMM [1], Rhessi [2] and INTEGRAL [3, 4]. In particular observations with the high-energy resolution Ge detectors onboard the Rhessi and INTEGRAL satellites permitted ﬁne spectroscopy of the line and resulted in interesting conclusions about the accelerated proton and α-particle directional distributions in the impulsive ﬂare phases. Emission of this line from the inner Galaxy due to interactions of low-energy cosmic rays, is also one of the primary science objectifs of next-generation γ-ray observatories [5, 6]. A good knowledge of the line shape there should facilitate its extraction in the presence of multiple other emissions and the determination of its intensity. In astrophysical sites in general, the line is mainly produced by energetic-proton and α-particle induced reactions with 12C and 16O of the interaction medium and in inverse kinematics by reactions of accelerated 12C and 16O with helium and hydrogen. Furthermore, there is signiﬁcant interest in the monitoring of the dose deposition in radiotherapy with particle beams [7]. An example are devices that detect the prompt γ rays resulting from proton (or carbon) interactions in human tissue, rising the demand of detailed and reliable nuclear cross sections [8, 9]. In this context, carbon and oxygen make up more than 3/4 of the human body mass, making the 4.438-MeV line the strongest prompt emission line produced. In all described scenarios, inelastic scattering of protons oﬀ 12C makes up a signiﬁcant part of the 4.438-MeV line intensity. The line-shape calculations for this component in the solar ﬂare studies [1, 3, 4] were based on a relatively simple parametrization of the magnetic substate population of the 2+ 4.439-MeV state of 12C at low proton energies Ep <∼ 15 MeV, and coupled-channels calculations at higher energies adjusted to reproduce an abundant data set of measured line shapes and γ-ray angular distributions [10]. This parametrization reproduced fairly well the meaured line shapes and γ-ray angular distributions above Ep = 15 MeV, but reproduced only approximately the experimental data at lower proton energies. Since then, new data for proton inelastic scattering oﬀ 12C became available, resulting in a now full data set of line shapes and angular distributions for the 4.438-MeV line in the proton energy range from threshold to Ep = 25 MeV. There are now, in particular, line-shape data close to the energy of each CN resonance that shows up as a distinct peak in the γ-ray production cross section in proton reactions with 12C (see Figure 1). With these data, a new method has been developed that improves speciﬁcally the description of line shapes and γ-ray angular distributions in the region of dominating CN resonances, i.e. from threshold to about Ep = 12 MeV. At higher proton energies, coupled-channel calculations with a deformed potential for nucleon scattering oﬀ 12C [11] reproduced fairly well the measured data. In the following section, a short outline of the formalism for the calculations will be given. After that, the extended parameter search for these calculations to reproduce measured γ-ray line shapes and angular distributions is described and the results are discussed.
Application of Coherent State Approach for the cancellation of Infrared divergences to all orders in LFQED<|sep|>In the LSZ formalism, the asymptotic states are taken as free states and the S–matrix elements are the residues of the poles that arise in the Fourier transform of the correlation functions when four-momenta of the external particles are put on-shell. Therefore, the initial and ﬁnal states used to calculate the transition matrix elements are taken to be the Fock states. However, in Quantum Electrodynamics (QED) the asymptotic states are not free states and the fermion is accompanied by soft photons i.e. photons with very low momentum. In an actual experiment, due to the ﬁnite size of the detector, the charged particle can be accompanied by any number of such photons, which are source of Infrared (IR) divergences. In the soft photon limit, the virtual and real emission are indistinguishable. So when we are dealing with a virtual photon correction in a process, we need to take into account emission of an inﬁnite number of real soft photons also. Hence, the physical state should be deﬁned as a set of states with an inﬁnite number of soft photons. A method of asymptotic dynamics was developed by Kulish and Faddeev (KF) to address the issue of cancellation of IR divergences at amplitude level [1]. They were the ﬁrst to show that in QED, the asymptotic Hamiltonian does not coincide with the free Hamiltonian. KF constructed the asymptotic Hamiltonian Vas for QED thus modifying the asymptotic condition to introduce a new space of asymptotic states. KF further modiﬁed the deﬁnition of S−matrix and showed that it is free of IR divergences. Thus the method of asymptotic dynamics proposed by KF replaces, the free Hamiltonian by an asymptotic Hamiltonian which takes into account the long range interaction between incoming and outgoing states and can be used to construct a set of coherent states as the asymptotic states. The transition matrix elements formed by using these states are then IR ﬁnite. In Light Front Field Theory (LFFT), there are two kinds of IR divergences viz. the true IR divergences which are the bona-ﬁde divergences of equal-time ﬁeld theory and which appear when both k+ and k⊥ approach zero and the spurious IR divergences that are just a manifestation of ultra-violet divergences of equal-time theory. The KF method has been used by various authors [2; 5; 6; 7] in the context of equal time theories. In LFFT, a coherent state formalism was developed in Ref. [8; 9; 10] as a possible method to deal with the true IR divergences in one loop vertex correction for LFQED and LFQCD. It has also been shown that the IR divergences in the fermion self-energy correction at the two-loop order cancel in LFQED [11; 12] if one uses the coherent state approach. In QED, a general approach to treat IR divergences was given by Yennie et al. [13]. Following the work of Yennie et al., Chung [14] showed that IR divergences indeed cancel to all orders in perturbation theory at the level of amplitude. KF approach was applied by Greco etal[6] to study the IR behavior of non abelian gauge theories using coherent states of deﬁnite color and factorized in ﬁxed angle regime. We shall present a general proof of all order cancellation of IR divergences in fermion mass renormalization in LFQED by using the coherent state method. The details of the calculation can be found in Ref.[15].
Constraining the formation of black-holes in short-period Black-Hole Low-Mass X-ray Binaries<|sep|>Since the discovery of the ﬁrst stellar-mass black hole (BH) in the Galactic X-ray source Cygnus X-1 (Bowyer et al. 1965), other BHs have been found. Stellar-mass† BHs have a mass between ∼ 5 and 30 times the mass of the Sun, while the peak of the Galactic distribution is centered at 8 M⊙ ( ¨Ozel et al. 2010). Their Galactic population currently amounts to 17 dynamically conﬁrmed BHs (Casares & Jonker 2014), and 31 BH candidates (Tetarenko, B.E. et al. 2015, in prep). So far, the best way to detect BHs is when they are actively accreting from a stellar companion, whose matter falls in the gravitational ﬁeld of the BH while forming an accretion disc. We wish to mention the recent discovery of a BH with a Be-type star as companion (Casares et al. 2014). This system, in which the accretion ﬂow onto the BH is radiatively inefﬁcient, opens-up the possibility of a new detection-window for BHs. In black-hole low-mass X-ray binaries (BH-LMXBs), a BH accretes matter from a star similar in mass to our Sun. At some point in the evolution of the progenitor of a BH-LMXB, the companion overﬁlls its Roche lobe, either because of its own nuclear expansion, or due to shrinking of the orbit caused by angular momentum losses. The material hence escapes the gravitational pull from the star and forms an accretion disc around the BH, which is detectable in the X-ray band (Shakura & Sunyaev 1973). Despite the strong evidence for the existence of BHs, the way they actually form is still a matter of great debate. It is generally accepted that BHs are formed from the gravitational collapse of a massive star, a star with a mass on the main sequence (MS) greater than 20 − 25 M⊙ , and/or a core mass greater than 8 M⊙ , e.g. Fryer (1999), MacFadyen et al. (2001), Tauris & van den Heuvel (2006). In one scenario, massive BHs are thought to form by direct collapse, whereas the lightest ones via fallback onto the nascent neutron star (NS). Two main uncertainties are the size of the velocity the BH receives at birth (natal kick, NK), if any, and the amount of mass ejected at the moment of BH formation, if any. These are the questions we address in this Paper. Measuring BH natal kicks is of great importance for a number of reasons. First of all, the magnitude of the NK is dependent upon the physical mechanism driving the kick. For instance, if BHs were discovered to receive high kicks, this would have important implications for the supernova (SN) mechanism. Theoretical calculations by Janka (2013) show that the momentum of the BH can
Gravitational instability of filamentary molecular clouds, including ambipolar diffusion<|sep|>Finding a reliable description of star formation has been one of the pivotal challenges of astrophysics for decades. Having such a model for star formation which involves many diverse physical phenomena would provide invaluable information to understand the evolution of galactic structures, star clusters, binary stars and even the formation of planets. It has been long established that stars form from collapsed dense molecular clouds cores (see Klessen et al. 1998; Dib et al. 2007; Padoan et al. 2014; McKee & Ostriker 2007). These cores are substantially denser than their parents so they collapse faster. Nevertheless, once these fragments turn into stars they start to aﬀect the surrounding (by radiation, winds or supernova explosions) preventing it from collapsing and forming stars. Generally it should be possible to derive a model that follows it from the scale of the largest self gravitating clouds, the giant molecular clouds (GMCs) (∼ 100 pc), to the scale of protostars (∼ 10−5 pc); while this is not possible in direct hydrodynamic simulations due to resolution limits, it can be studied approximately in analytic and semi-analytic models. Filamentary structures are quite commonly found in the interstellar medium. Recent high angular resolution and signal to noise of dust imaging with the Herschel Space Observatory have revealed the ﬁlamentary nature of the dense MCs in unprecedented details (Andr´e et al. 2010; Molinari et al. 2010). Consequently it has been suggested such ﬁlamentary structure likely play a key role in the star-formation process. Filaments are also ubiquitously observed in numerical simulations of supersonic turbulence in molecular clouds and of the interstellar medium (e.g. Klessen et al. 1998; Dib & Burkert 2005; Ntormousi et al. 2016). These ﬁlamentary structures exhibit a large degree of sub-structures of their own (such as clumps and cores). Andr´e et al. (2010, 2014) proposed that turbulence-driven ﬁlaments are maybe the ﬁrst stage of star formation. Given the widespread interest in the ﬁlamentary clouds structure and evolution and their key role in star formation, it is worth to examine the stability properties of the ﬁlaments in their realistic setting. It is now accepted that magnetic ﬁeld plays signiﬁcant role on the dynamics of molecular clouds (MCs) and consequently on the star formation process (McKee et al. 1993; Heiles et al. 1993; Dib et al. 2010). Large scale magnetic ﬁeld can limit compression in interstellar shocks that form cores and ﬁlaments, while the local magnetic ﬁeld within cores and ﬁlaments can prevent collapse if it is large enough (Mouschovias & Spitzer 1976). We know that magnetic ﬁelds are coupled with ionised particles, while the gas in the GMCs and their substructures such as ﬁlaments is mostly neutral or partially ionised. Mestel & Spitzer (1956) proposed a process, ambipolar diﬀusion, that allows charged particles to drift relative to the neutrals, with a drag force proportional to the collision rate. Also ambipolar diﬀusion leads to the dissipation of magnetic ﬁeld as well as to driving the system into force-free states (Zweibel & Brandenburg 1997). Conse
Tetrahedron in F-theory Compactification<|sep|>With the advent of the Large Hadron Collider (LHC) at CERN, theoretical studies around the Minimal Supersymmetric Standard model (MSSM ) and Grand Uniﬁed Theories (GUT) have known intense activities. Among these research activities, the studies of TeV- scale decoupled gravity scenarios aiming the embedding of MSSM and GUT models into superstrings and M- theory [1, 2, 3, 4]; see also [5, 6, 7, 8]. Recently BeasleyHeckman-Vafa made a proposal, to which we refer here below as the BHV model, for embedding MSSM and GUT into the 12D F-theory compactiﬁed on Calabi-Yau fourfolds [9, 10, 11]. In this proposal, the visible supersymmetric gauge theory in 4D space time including chiral matter and Yukawa couplings is given by an eﬀective ﬁeld model following from the supersymmetric gauge theory on a seven brane wrapping 4- cycles in the F-theory compactiﬁcation down to 4D Minkowski space time. In the engineering of the supersymmetric GUT models in the framework of the BHV theory [10, 11], see also [12, 13], one has to specify, amongst others, the geometric nature of the complex base surface S of the elliptically K3 ﬁbered Calabi-Yau four -folds X4: In this relation Y is a complex two dimension ﬁber where live ADE singularities giving rise to the rank r gauge symmetry Gr that we observe in 4D space time and S is a complex base surface whose cycle homology captures important data on matter ﬁelds representations and their tri- ﬁelds couplings. If the singular ﬁber Y of the local CalabiYau four-folds (CY4) is ﬁxed by the targeted 4D space time invariance Gr, one may a priori imagine several kinds of compact complex surfaces S as its base manifold. The choice of S depends on the eﬀective 4D space time physics; in particular the number of conserved supersymmetric charges and chiral matter ﬁelds as well as their couplings. Generally speaking, the simplest surfaces one may consider are likely those given by the so called Hizerbruch surfaces Fn = P 1 ×n P 1 generated by ﬁbration of a complex projective line over a second projective line. Other examples of surfaces are given by the complex projective plane CP 2 and its del Pezzo dPn cousins; or in general non planar complex surfaces D embedded in higher dimensional complex Kahler manifolds. Typical examples of adequate surfaces S that have been explicitly studied in the BHV model are given by the family of del Pezzo surfaces dPn with n = 0, 1, ..., 8. These complex surfaces are obtained by preforming up to eight blow ups at isolated points of the projective plane CP 2 = dP0 by complex projective lines [14, 15, 9, 16, 17]; see also section 2 for technical details. Motivated by the study of the geometric engineering of the F-theory GUT models building `a la BHV, we aim in this paper to contribute to this matter by constructing a family of backgrounds for F-theory compactiﬁcation based on the tetrahedron geometry T and its blow ups. This study sets up the basis for developing a class of F-theory GUT- like models building and uses the power of toric geometry of complex surfaces to geometrically engineer chiral matter and the Yukawa couplings. Recall that the tetrahedron T viewed as a toric surfaces with the following toric ﬁbration has toric singularities generated by shrinking cycles of T 2 on the edges of the tetrahedral base ∆T and at its vertices. In our approach, the shrinking cycles of the above toric ﬁbration are interpreted in terms of gauge enhancement of bulk gauge symmetry Gr × U2 (1). In going from a generic face of the tetrahedron towards a vertex passing through a edge, the Gr × U2 (1) bulk gauge symmetry gets enhanced to Gr+1 × U (1) on the edge and to Gr+2 at the vertex as shown on the following table: In the present paper, we focus our attention mainly on the study of the typical family of base surfaces S of eq(1.1) involving the non planar complex tetrahedral surface and its blow ups denoted here below as T0 and Tn respectively. In the conclusion section, we give comments on the engineering of GUT-like 4D N = 1 supersymmetric quiver gauge models based on T0 and Tn. A more involved and explicit study for the engineering of Ftheory GUT-like models along the line of the BHV approach; but now with T0 and Tn as complex base geometries in the local Calabi-Yau four-folds of eq(1.1) will be reported in [18]. The presentation of this paper is as follows: In section 2, we review general aspects of del Pezzo surfaces dPk; in particular their 2- cycle homology classes and their links to the exceptional1 Lie algebras. This review on real 2- cycle homology of the dPks is important to shed more light for the study and the building of the blow ups of the tetrahedron. In section 3, we introduce the complex tetrahedral surface T0; ﬁrst as a complexiﬁcation of the usual real tetrahedron (hollow triangular pyramid); that is as a non planar complex surface given by the intersection of four projective planes CP 2. Second as a complex codimension one divisor ( ”a toric boundary”) of the complex three dimension projective space CP 3. We take also this opportuinity to recall useful results on CP 3 thought of as a toric manifold and its Chern classes ck � TCP 3� . These tools are used in section 4 to study the blow ups of the tetrahedron; in particular the toric blow ups of its vertices by projective planes and the blow up of its edges by the del Pezzo surface dP1. In section 5, we give a conclusion and make comments on supersymmetric GUT-like quiver gauge theories embedded in F-theory compactiﬁcation on local Calabi-Yau four-folds. 1Here E3, E4, E5 denote respectively SU (3) × SU (2), SU (5) and SO (10) and E6, E7, E8 are the usual exceptional Lie algebras in Cartan classiﬁcation.
Knowledge-Augmented Language Models for Cause-Effect Relation Classification<|sep|>Automatic extraction and classiﬁcation of causal relations in the text have been important yet challenging tasks in natural language understanding. Early methods in the 80s and 90s (Joskowicz et al., 1989; Kaplan and Berry-Rogghe, 1991; Garcia et al., 1997; Khoo et al., 1998) mainly relied on deﬁning hand-crafted rules to ﬁnd cause-effect relations. Starting 2000, machine learning tools were utilized in building causal relation extraction models (Girju, 2003; Chang and Choi, 2004, 2006; Blanco et al., 2008; Do et al., 2011; Hashimoto et al., 2012; Hidey and McKeown, 2016). Word-embeddings and Pretrained Language Models (PLMs) have also been leveraged in training models for understanding causality in language in recent years (Dunietz
Query Language for Complex Similarity Queries<|sep|>Information has always been a valuable article but it has always been diﬃcult to obtain. These days, we have an unprecedented advantage of having huge and rich data collections at our ﬁngertips. On the other hand, we still need more eﬃcient tools for data management to be able to locate the desired information in the vast amounts of resources. With the emergence of complex data types such as multimedia, traditional retrieval methods based on attribute matching are no longer satisfactory. Therefore, a new approach to searching has been proposed, exploiting the concept of similarity between complex objects. In recent years, we have witnessed intensive research in the ﬁeld of indexing methods and search algorithms for similarity-based retrieval. As a result, state-of-the-art search systems already support quite complex similarity queries with a number of features that can be adjusted according to individual user’s preferences. To communicate with such a system, it is either possible to employ lowlevel programming tools, or a higher-level communication interface that shields
Ergodic and localized regions in quantum spin glasses on the Bethe lattice<|sep|>In recent years the study of the diﬀerent dynamical regimes of isolated quantum systems has received a lot of attention, due to improved experimental techniques [1, 2] and theoretical progress. In the latter, one can identify two distinguished but not independent lines of research: the ﬁrst is the study of how ergodicity is realized in isolated quantum systems, a mechanism that goes under the name of eigenstate thermalization hypothesis [3]; The second is the study of the most typical mechanism for failure of ergodicity in presence of quenched disorder [4, 5, 8, 9, 10] (although some authors have suggested disorder in the initial state suﬃces [6, 7]) named many-body-localization. While dynamical phases satisfying ETH can be described by the usual tools of statistical mechanics and thermodynamics, MBL systems behave a lot like integrable systems [15] with local integrals of motions [9, 11, 12, 13, 14]: Transport is suppressed [16, 17], entanglement entropy grows slowly to its thermodynamic value [18], and some symmetry breaking phases can exist, protected from the Mermin-Wagner theorem, in low dimensions and at high temperature [19]. Soon after its inception, it has been pointed out that MBL phases can be detrimental [20] for the performance of Adiabatic Quantum Computation (AQC) protocol introduced in [21] (see also [22]). This has been contested in later works [23] and it remains a controversial claim. Since this protocol has proven to be the most promising for the realization of a quantum computer [24], sorting out this question is of paramount importance for both theoretical discussions and technological implications. In a series of recent works, which involve one of the present authors [25, 26, 27], the question of the appearance of an MBL phase in some models of quantum spin glasses has been addressed with the result that, for realistic, mean-ﬁeld glasses MBL can exist only in ﬁnite-connectivity models, while in fully-connected models only a weaker form, remnant of the clustering phase existing in the phase space of the classical model [28], exists. These earlier works point at the necessity to examine a ﬁnite-connectivity quantum spin glass, in search of MBL. In this work we set to do exactly this: we analyze the quantum dynamics of an isolated quantum spin glass showing that there is an ETH phase at large transverse ﬁeld, possibly extending all the way down to the spin-glass phase while at all system sizes we were able to study, an MBL phase exists for small transverse ﬁeld. Therefore there should be an MBL-ETH dynamical transition in between.
Asynchronous Snapshots of Actor Systems for Latency-Sensitive Applications<|sep|>Snapshotting persists a program’s state so that the program can be restored and continued later. Programming environments such as Lisp and Smalltalk use snapshotting to create images of the system’s state. These images allow developers to deploy a pre-configured system into production or continue development at a previous state. Snapshots can also facilitate time-traveling and record & replay debugging [4, 5], by restoring a program execution to an earlier point in time to investigate events that lead to the occurrence of a bug. Finally, snapshots enable quick crash recovery and moving a program’s execution to a different machine. This paper focuses on snapshotting support for actorbased applications. Popular implementations of the actor model such as Akka,1 Pony [12], Erlang [2], Elixir [25], and Orleans [9] are used to build complex responsive applications. We present a novel technique to snapshot such responsive actor-based applications avoiding stop-the-world pauses. Creating a snapshot requires the program state to be persisted. In Lisp, Smalltalk, and other systems [4], this is done with heap dumps integrated with the garbage collector (GC). This, however, requires virtual machine (VM) support and usually stops the world to create a consistent snapshot, making the program unresponsive. This is problematic for applications that aim to respond consistently with low latency. Snapshotting is also common in high-performance computing [8, 11, 14, 15, 20] to address distributed failures. These approaches provide inspiration but in this work we do not address such failures and focus on non-distributed actor applications, simplifying the problem significantly. In this paper, we present an efficient approach for transparent asynchronous snapshots of non-distributed actor-based systems. It is implemented on top of an unmodified Java VM and does not rely on GC integration. By snapshotting the state of each actor individually, we avoid stop-the-world synchronization that blocks the entire application. We applied our approach to communicating event loop (CEL) actors [22] in SOMns. SOMns is an implementation of Newspeak [7] built on the Truffle framework and the Graal just-in-time compiler [28]. We evaluated the performance of our approach with the SOMns implementation. On the Savina benchmark suite [18], we measure the run-time overhead and memory impact of frequent snapshot creation. On the modern webapplication Acme Air [26], we measure the effect of snapshot creation on request latency, to ensure that user experience remains acceptable, i.e. additional latency is below 500ms [1]. The main contribution of this paper is a novel snapshotting approach for non-distributed actor programs that minimizes latency by avoiding stop-the-world synchronization and persisting program state asynchronously. Furthermore, our approach does not require changes to the VM nor GC integration. Our evaluation shows that for the Acme Air experiment snapshotting increases the number of slow requests with latency over 100ms by 5.43% while the maximum latency is unchanged.
Technology Ethics in Action: Critical and Interdisciplinary Perspectives<|sep|>If digital technology production in the beginning of the 2010s was characterized by the brash spirit of Facebook’s motto “move fast and break things” and the superficial assurances of Google’s motto “do not be evil”, digital technology  toward  the  end  of  the  decade  was characterized by a “crisis of conscience”[1]. While many have long been aware of digital technology’s harms, an influx of stories about salient harms led to widespread critique  of  digital  technology.  The  response  was  the “techlash”:  a  growing  public  animosity  toward  major technology  companies.  In  2018,  Oxford  Dictionaries and the Financial Times both deemed techlash to be one of the words of the year[2, 3]. Disinformation:  Throughout  the  2016  US presidential election between Donald Trump and Hillary Clinton,  social  media  was  plagued  with  fraudulent stories  that  went  viral[4, 5].  In  turn,  numerous commentators—including  Hillary  Clinton—blamed Facebook  for  Donald  Trump’s  presidential  election victory[6−9].  Later  reporting  revealed  that  Facebook’s leadership has actively resisted taking strong measures to curb disinformation, instead prioritizing the company’s business strategies[10, 11]. • Ben Green is with  the Society of Fellows and the Gerald R. Ford School of Public Policy, University of Michigan, Ann Arbor, MI 48109, USA. E-mail: bzgreen@umich.edu. ©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/). and The Guardian reported that the voter-profiling firm Cambridge  Analytica  had  harvested  information  from millions of Facebook users, without their knowledge or permission, in order to target political ads for Donald Trump’s 2016 presidential campaign[12, 13]. Cambridge Analytica had acquired these data by exploiting the sievelike nature of Facebook’s privacy policy. Military  and  ICE  Contracts:  In  2018,  journalists revealed  that  Google  was  working  with  the  US Department of Defense (DoD) to develop software that analyzes drone footage[14]. This effort, known as Project Maven, was part of a ＄7.4 billion investment in AI by the DoD in 2017[14] and represented an opportunity for Google  to  gain  billions  of  dollars  in  future  defense contracts[15].  Another  story  revealed  that  Palantir  was developing  software  for  Immigration  and  Customs Enforcement (ICE) to facilitate deportations[16]. Algorithmic Bias: In 2016, ProPublica revealed that an algorithm used in criminal courts was biased against Black defendants, mislabeling them as future criminals at  twice  the  rates  of  white  defendants[17].  Through popular books about the harms and biases of algorithms in  settings  such  as  child  welfare,  online  search,  and hiring[18−20], the public began to recognize algorithms as both fallible and discriminatory. These  and  other  tech-related  controversies  were  a shock to many, as they arrived in an era of widespread (elite)  optimism  about  the  beneficence  of  technology. Yet these controversies also brought public attention to what scholars in fields such as Science, Technology, and Society (STS), philosophy of science, critical data and algorithm studies, and law have long argued: technology is shaped by social forces, technology structures society often in deleterious ways, and technology cannot solve every  social  problem.  Broadly  speaking,  these  fields bring  a  “sociotechnical”  approach  to  studying technologies,  analyzing  how  technologies  shape,  are shaped  by,  and  interact  with  society[21−24].  As  tech scandals mounted, a variety of sociotechnical insights, long ignored by most technologists and journalists, were newly recognized (or in some form recreated). Many in the tech sector and academia saw the harms of digital technology as the result of an inattention to ethics. On this view, unethical technologies result from a  lack  of  training  in  ethical  reasoning  for  engineers and  a  dearth  of  ethical  principles  in  engineering practice[1, 25−28]. In response, academics, technologists, companies,  governments,  and  more  have  embraced  a broad set of goals often characterized with the label “tech ethics”: the introduction of ethics into digital technology education, research, development, use, and governance. In the span of just a few years, tech ethics has become a  dominant  discourse  discussed  in  technology companies,  academia,  civil  society  organizations,  and governments. This article reviews the growth of tech ethics and the debates that this growth has prompted. I first describe the primary forms of tech ethics in practice. I focus on the people  and  organizations  that  explicitly  embrace  the label of “tech ethics” (and closely related labels, such as AI ethics and algorithmic fairness). I then summarize the central critiques made against these efforts, which call into question the effects and desirability of tech ethics. Against the backdrop of these critiques, I argue that tech ethics is a terrain of contestation: the central debate is not whether ethics is desirable but what ethics entails and who has the authority to define it. These debates suggest the need for a sociotechnical approach to tech ethics that focuses on the social construction and real-world effects of  tech  ethics,  disambiguating  between  the  value  of ethics as a discipline and the limits of tech ethics as a practical  endeavor.  I  introduce  this  approach  through four  frames:  objectivity  and  neutrality,  determinism, solutionism, and sociotechnical systems.
On the Exponentially Weighted Aggregate with the Laplace Prior<|sep|>We investigate statistical properties of the Exponentially Weighted Aggregate (EWA) in the context of high-dimensional linear regression with ﬁxed design and under the sparsity scenario. This corresponds to considering data that consist of n random observations y1, . . . , yn ∈ R and p ﬁxed covariates x1, . . . , xp ∈ Rn. We further assume that there is a vector β⋆ ∈ Rp such that the residuals ξi = yi − β⋆ 1x1 i − . . . − β⋆ pxp i are independent, zero mean random variables. In vector notation, this reads as y = Xβ⋆ + ξ, (1) where y = (y1, . . . , yn)⊤ is the response vector, X = (x1, . . . , xp) ∈ Rn×p is the design matrix and ξ is the noise vector. For simplicity, in all mathematical results, the noise vector is assumed
Global Synchronization of Pulse-Coupled Oscillator Networks Under Byzantine Attacks<|sep|>Inspired by ﬂashing ﬁreﬂies and contracting cardiac cells, pulse-based synchronization is attracting increased attention in sensor networks and wireless communications [2]–[5]. By exchanging simple and identical messages (so-called pulses), pulse-based synchronization incurs much less energy consumption and communication overhead compared with conventional packet-based synchronization approaches [6]. These inherent advantages make pulse-based synchronization extremely appealing for event coordination and clock synchronization in various networks [7]–[11]. In the past decade, plenty of results have been reported on pulse-based synchronization. For example, by optimizing the interaction function, i.e., phase response function, the synchronization speed of pulse-coupled oscillators (PCOs) is maximized in [12]; with a judiciously-added refractory period in the phase response function, the energy consumption of PCO synchronization is reduced in [13]–[15]; [16]–[18] show that PCOs can achieve synchronization under a general coupling topology even when Zhenqian Wang and Yongqiang Wang are with the Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, 29630 USA. e-mail: {zhenqiw,yongqiw}@clemson.edu. The work was supported in part by the National Science Foundation under Grants 1738902, 1912702, and in part by the China Scholarship Council. their initial phases are randomly distributed in the entire oscillation period. Recently, synchronization of PCOs in the presence of time-delays and unreliable links is also discussed [19], [20]. Other relevant results include [21]–[29]. However, all the above results are obtained under the assumption that all oscillators behave correctly with no nodes compromised by malicious attackers. Due to the distributed and unattended nature, wireless sensor nodes are extremely vulnerable to attacks, making it imperative to study synchronization in the presence of attacks. Recently, some results have emerged for attack-resilient pulse synchronization [30]–[42]. In [30], the authors showed that pulse-based synchronization is more robust than its packet-based counterpart in the presence of a faulty node. In [31], a new phase response function was proposed to combat non-persistent random attacks. The authors in [32] considered pulse-based synchronization in the presence of faulty nodes which ﬁre periodically ignoring neighboring nodes’ inﬂuence. However, all the above results only apply to a priori synchronized PCO network, i.e., all legitimate nodes are required to have identical phases when faulty pulses are emitted. Furthermore, these results also require that the communication topology of legitimate oscillators is all-toall. To relax the constraint on initial phase distributions, [33] proposed a pulse-based synchronization approach that is applicable even when legitimate oscillators have different but restricted initial phases; [34] further proposed a pulse-based synchronization mechanism that can achieve synchronization under stealthy attacks even when the initial phases of legitimate oscillators are randomly distributed in the entire oscillation period (global synchronization) under all-to-all connection. The authors in [36]–[39] proposed to achieve global synchronization by exchanging packets besides pulses. On the other hand, to relax the constraint on all-to-all topology, our most resent result [35] proposed a new attack resilient pulse-based interaction mechanism to synchronize non-all-to-all connected PCOs when their initial phases are restricted in a half cycle; the authors in [40], [41] employ extra packet based communication to achieve global synchronization under Byzantine attacks even when the network is generally connected. Using a similar approach, the authors in [42] showed that a (5 f + 1)-connected network can achieve global synchronization in the presence of f attackers with each attacker unable to send two attack pulses in one natural oscillation period. Because of the introduction of extra packet messages, these approaches have higher communication and computation overhead, which will further restrict scalability as well as achievable synchronization accuracy. In this paper, we propose an approach to synchronizing densely connected PCO networks from an arbitrary initial phase distribution under Byzantine (arbitrary) attacks. The approach only employs content-free pulses. It is worth noting that the content-free pulse-based communication reduces the attack surface and avoids the manipulation of message contents by Byzantine attacks. In fact, what can be manipulated by Byzantine attacks becomes the timing of attack pulses, which will be elaborated in Sec. III. Table 1 summarizes the advantage of our approach over existing results on pulse-based synchronization. More specifically, compared with existing results, our contributions are as follows: 1) Under Byzantine attacks, our proposed mechanism can synchronize legitimate oscillators even when their initial phases are arbitrarily distributed in the entire oscillation period; 2) Our mechanism is applicable to densely connected PCO networks that are not necessarily all-to-all; 3) We consider an attack model that is much more difﬁcult to deal with than existing results like [31]–[35]; 4) Our mechanism only use contend-free pulses, which is different from [36]–[42] relying on the assistance of packet communication to achieve synchronization; 5) Our proposed mechanism guarantees that the collective oscillation period is identical to the free-running period irrespective of attacks, which is superior to existing mechanisms (e.g., [32]–[34]) that lead to a collective oscillation period affected by attacker pulses. It is worth noting that the results in this paper are fundamentally different from our recent result [35] in the following aspects: 1) The attack model in this paper is much stronger. [35] considers an attack model in which an attacker is restricted to send at most one attack pulse in any time interval of length T/2 (to stay stealthy) whereas this paper allows attackers to send as many attack pulses as possible under a given communication channel with a ﬁxed bit rate. So synchronization under attacks in this paper is much more challenging; 2) This paper has more relaxed requirement on the initial distribution of oscillator phases compared with [35]. [35] requires legitimate oscillators to have initial phases contained in a half cycle whereas this paper allows legitimate oscillators’ phases to be arbitrarily distributed in the entire cycle; 3) This paper proves ﬁnite-time synchronization whereas [35] only proves asymptotic synchronization even in the case of l = 1. More speciﬁcally, [35] proves that the length of the containing arc of legitimate oscillators will decrease to no greater than (1−l/2) of its original value after every two consecutive ﬁring rounds, and hence can only yield synchronization when time goes to inﬁnity. (It is worth noting that our prior result on non-all-to-all PCO networks in [33] needs 0 < l < 1 to address the practical case of non-identical initial phases of legitimate oscillators and hence also only proves asymptotic synchronization.) This paper is organized as follows. Sec. II reviews the main concepts of PCO networks. Sec. III presents the attack model considered in this paper. Sec. IV presents a new pulsebased synchronization mechanism. Sec. V addresses the case of multiple Byzantine attackers and Sec. VI addresses the case where the total number of oscillators is unknown to individual oscillators. Simulation results are presented in Sec. VII. II. PRELIMINARIES Consider a network of N pulse-coupled oscillators. Each oscillator is equipped with a phase variable which evolves clockwise on a unit circle. When the evolving phase of an oscillator reaches 2π rad, the oscillator ﬁres (emits a pulse). Receiving pulses from neighboring oscillators will lead to the adjustment of the receiving oscillator’s phase, which can be designed to achieve a desired collective behavior such as phase synchronization. To deﬁne synchronization, we ﬁrst introduce the concept of containing arc. The containing arc of legitimate oscillators is deﬁned as the shortest arc on the unit circle that contains all legitimate oscillators’ phases. Deﬁnition 1 (Phase Synchronization): We deﬁne phase synchronization as a state on which all legitimate oscillators have identical phases and ﬁre simultaneously with a period of T = 2π seconds. An edge (i, j) from oscillator i to oscillator j means that oscillator j can receive pulses from oscillator i but not necessarily vice versa. The number of edges entering oscillator i is called the indegree of oscillator i and is represented as d− i . The number of edges leaving oscillator i is called the outdegree of oscillator i and is represented as d+ i . The value di ≜ min{d− i ,d+ i } is called the degree of oscillator i. The degree of a network is deﬁned as d ≜ mini=1,2,···,N{di}. Since an oscillator cannot receive the pulse emitted by itself, the maximal degree of a network of N PCOs is d = N − 1, meaning that the network is all-to-all connected. In this paper, we consider dense networks where the network degree d is assumed to be greater than ⌊2N/3⌋. Making use of the fact d ≜ mini=1,2,···,N{di}, we always have di −⌊2N/3⌋−1 ≥ 0 for i = 1, 2, ··· ,N. III. ATTACKER MODEL In this section, we present the model of Byzantine attacks. We assume that Byzantine attacks are able to compromise an oscillator and completely take over its behavior. Since the communicated messages in PCO networks are identical and content-free, i.e., pulses, a Byzantine attacker cannot manipulate the content of pulses, but rather, it will judiciously craft attacks via injecting pulse trains at certain time instants to negatively affect pulse-based synchronization. Because in realistic wireless sensor networks (WSNs), the bit rate of a communication channel between two connected oscillators is limited, an attacker cannot send inﬁnitely many pulses in any ﬁnite time interval. In other words, there is always a nonzero time interval between two consecutive pulses from an attacker. Therefore, Byzantine attackers will launch attacks with a time separation greater than ε seconds, where ε is the minimum time separation between two consecutive pulses that can be conveyed by a channel. We summarize the Byzantine attacker model in this paper as follows: Byzantine Attacker: a Byzantine attacker will emit attack pulses with a time separation greater than ε seconds, where ε is the minimum time separation between two pulses that can be successfully conveyed by a communication channel. Remark 1. In PCO networks, the communication messages are all content-free pulses. So the transmission of one pulse will only occupy the communication channel for a very short time. Only after ﬁnishing transmitting one pulse, an attacker can initiate the transmission of another attack pulse. Hence, ε is determined by the length of the pulse and the bit rate of the communication channel. For example, the bit rate of the IEEE 802.15.4 channel is 250kbps. If we use a control packet (21 bytes) to realize a pulse, then transmitting such pulses will need time separation ε = (21×8)/250000= 0.672×10−3 Remark 2. All existing attack patterns considered under pulse-based synchronization such as random attacks [30], [31], static attacks [32], and stealthy attacks [33]–[35] are special cases of the attacker model considered in this paper. IV. A NEW PULSE-BASED SYNCHRONIZATION MECHANISM Motivated by the fact that the conventional pulse-based synchronization mechanism is vulnerable to attacks, we propose a new pulse-based synchronization mechanism to combat attacks. To present our new mechanism, we ﬁrst describe the conventional pulse-based synchronization mechanism. 1) The phase φi of oscillator i evolves from 0 to 2π rad with a constant speed ω = 1 rad/second. 2) Once φi reaches 2π rad, oscillator i ﬁres and resets its phase to 0. 3) Whenever oscillator i receives a pulse, it instantaneously resets its phase to: In the above conventional pulse-based synchronization mechanism, every incoming pulse will trigger a jump on the receiving oscillator’s phase, which makes it easy for attackers to perturb the phases of legitimate oscillators and hence destroy their synchronization. Moreover, we have that synchronization can never be maintained when attackers only affect part of the network, even when the coupling strength is set to l = 1. This is because attack pulses can always exert nonzero phase shifts on affected legitimate oscillators and make them deviate from unaffected ones. This is also conﬁrmed by numerical simulation results in Figure 8 and Figure 9, which illustrate that existing results in [32]–[34] cannot achieve synchronization in the presence of Byzantine attacks when the topology is not all-to-all. To overcome the inherent vulnerability of existing pulsebased synchronization approaches, we propose a new pulsebased synchronization mechanism (Mechanism 1) to improve the attack resilience of PCO networks. Our key idea to enable attack resilience is a “pulse response mechanism” which can restrict the number of pulses able to affect a receiving legitimate oscillator’s phase in any oscillation period and a “phase resetting mechanism” which resets the phase value of a legitimate oscillator upon reaching phase 2π rad to different values depending on the number of received pulses. The “pulse response mechanism” and the “phase resetting mechanism” only allow pulses meeting certain conditions to affect a receiving oscillator’s phase and hence can effectively ﬁlter out attack pulses with extremely negative effects on the synchronization process. Noting that all pulses are identical and content-free, Mechanism 1 is judiciously designed based on the number of pulses an oscillator received in the past, i.e., based on memory. The new pulse-based synchronization mechanism is detailed below: 1) The phase φi of legitimate oscillator i evolves from 0 to 2π rad with a constant speed ω = 1 rad/second. 2) Once φi reaches 2π rad at time t, oscillator i ﬁres (emits a pulse) if it did not ﬁre within (t − ε, t] and an entire period T = 2π seconds has elapsed since initiation. Then oscillator i resets its phase from 2π rad to 0 if it received over ⌊N/3⌋ pulses within (t − ε, t], where ⌊•⌋ is the largest integer no greater than “ • .” Otherwise, it resets its phase from 2π rad to π rad. 3) When oscillator i receives a pulse at time t′, it shifts its phase to 2π rad only if φi ∈ [π, 2π] at time instant t′ and one of the following conditions is satisﬁed: a) before receiving the current pulse, oscillator i has already received at least di − ⌊2N/3⌋ − 1 pulses in [t′ − T/2, t′] and it did not reset its phase from 2π rad to 0 within (t′ − T, t′). b) before receiving the current pulse, oscillator i has already received at least di − ⌊2N/3⌋ − 1 pulses in (t′ − ε, t′]. Otherwise, the pulse has no effect on φi who will evolve freely towards 2π rad. Remark 3. Following [8], [24]–[26], we assume that when a legitimate oscillator receives multiple pulses simultaneously, it can determine the number of received pulses and processes them consecutively. In other words, no two pulses will be regarded as an aggregated pulse.
Technical Report: A Receding Horizon Algorithm for Informative Path Planning with Temporal Logic Constraints<|sep|>In this paper we propose an algorithm for controlling a mobile sensing robot to collect the most valuable information in its environment, while simultaneously carrying out a required sequence of actions described by a temporal logic (TL) speciﬁcation. Our algorithm is useful in situations where a robot’s main objective is to collect information, but it must also perform pre-speciﬁed actions for the sake of safety or reliability. Consider searching for a survivor trapped in the rubble of a collapsed building. Our algorithm would drive the robot to locate the survivor while avoiding obstacles, and returning to a rescue worker to report on the progress of its search. The obstacle avoidance and visit to the worker are represented as temporal logic constraints. In order to locate the survivor, the robot plans a path on-line, in a receding horizon fashion, such that it localizes the survivor as precisely as possible, while still satisfying the temporal logic constraints. This work brings together methods from information theory and formal control synthesis to create new tools for robotic information gathering under complex constraints. More speciﬁcally, the robot uses a recursive Bayesian ﬁlter to estimate the quantity of interest in its environment (e.g. the location of a survivor) from its noisy sensor measurements. The Shannon entropy of the Bayesian estimate is used as a measure of the robot’s uncertainty about the quantity of Austin Jones is with the Division of Systems Engineering, Mac Schwager and Calin Belta are with the Division of Systems Engineering and the Department of Mechanical Engineering at Boston University, Boston, MA 02115. Email: {austinmj,schwager,cbelta}@bu.edu This work was partially supported by ONR under grants MURI N0001409-1051 and ONR MURI N00014-10-10952 and by NSF under grant CNS1035588 interest. The robot plans a path to maximally decrease the expected entropy of its estimate over a ﬁnite time horizon, subject to the TL constraints. The path planning is repeated at each time step as the Bayesian ﬁlter is updated with new sensor measurements to give a reactive, receding horizon planner. Our algorithm is guaranteed to satisfy the TL speciﬁcation. We compare the performance of our algorithm to a non-reactive, exhaustive search method. We show in statistics compiled from extensive simulations that our receding horizon algorithm gives a lower entropy estimate with lower computational complexity than the exhaustive search method. The algorithm we present is applicable to many scenarios in which we want a robot to gather informative data, but where safety and reliability are critical. For example, our algorithm can be used by a mobile robot deployed on Mars that is tasked with collecting soil samples and images while gathering enough sunlight to charge its batteries and avoiding dangerous terrain. In an animal population monitoring scenario, our algorithm can drive a robot to count animals of a given species whose positions are unknown while avoiding sensitive ﬂora and fauna, eventually uploading data to scientists. Our algorithm could also be used, for example, in active SLAM to control a robot to build a minimum uncertainty map [22] of its environment while avoiding walls and returning to a base station for charging. Extensive work already exists in using information theoretic tools in robotic information gathering applications. Most of this work uses a one-step-look-ahead approach [5], [18], a receding horizon approach [4], [8], or an ofﬂine plan based on the sub-modularity property of mutual information [16], [20], [21]. The key innovation in our algorithm is that it gives a path which is guaranteed to satisfy rich temporal logic constraints. Temporal logic constraints can specify complex, layered temporal action sequences that are considerably more expressive than the static constraints considered in previous works. Indeed, much of the work in constrained informative path planning can be phrased as a special case of the TL constraints that we consider here. For example the authors of [4] solve an information-gathering problem in which an underwater agent must avoid high trafﬁc areas and communicate with researchers—constraints which can be naturally expressed as TL statements. In this work, we consider a particular kind of temporal logic called syntactically co-safe linear temporal logic (scLTL) [12]. Synthesis of trajectories from scLTL speciﬁcations is currently an active area of research [1], [3], as is the use of receding horizon control to solve optimization problems over TL-constrained systems. Receding horizon control (RHC), sometimes referred to as model-predictive control, is a control technique in which current information is used to predict performance over a ﬁnite horizon [14], [17]. The authors of [23] use a receding horizon path planning algorithm that satisﬁes TL constraints in a provably correct manner and is capable of correcting navigational errors online. In [7], the authors extended this principle to provide a receding horizon algorithm for gathering time-varying, deterministic rewards in a TL-constrained system. The analysis of our informative planning algorithm was inspired by [7], with the signiﬁcant difference that information gain is a stochastic quantity which depends on noisy sensor measurements. The paper is outlined as follows. We deﬁne the necessary mathematical preliminaries in Section II. In Section III, we formalize the scLTL-constrained informative path planning problem. In Section IV we present our receding horizon algorithm, and prove that it satisﬁes the scLTL constraints. Results from simulations comparing our algorithm to a baseline exhaustive search method are presented in Section V. Finally, in Section VI, we give our conclusions and discuss directions for future work. As mentioned in the abstract, this report is an extended version of a paper accepted to the 2013 ICRA conference. The main additions are some information theory deﬁnitions in Section II and a proof of Theorem 1 in Section IV.
Early and Late-time Cosmic Acceleration in Non-minimal Yang-Mills-$f(G)$ Gravity<|sep|>Inﬂation in the early time and acceleration in the current expansion of the universe are two periods of accelerated expansion in our universe which are conﬁrmed by cosmological observations [1-4]. There are two ways to explain the current accelerated expansion of the universe. The ﬁrst one is introducing some unknown matter, which is called dark energy [5] in the framework of general relativity and the second one is modiﬁed gravity. The modiﬁed gravity in the simplest type, uses an arbitrary function f of the Ricci scalar instead of R in Einstein-Hilbert action which is known as f(R) gravity (see [6-8] for reviews). There are also other modiﬁed gravity models which are the generalizations of f(R) gravity and among them, the modiﬁed Gauss-Bonnet (GB) gravity i.e. f(G) gravity, is more interesting [9,10]. In order to play some role in the Friedmann equation, it is required that the GB combination G, a topological invariant in four dimensions, couples to a scalar ﬁeld φ or the Lagrangian density be a function of G i.e f(G). The GB coupling with a scalar ﬁeld appears in the low energy eﬀective action of string/M-theory [11] and the cosmological solution in such a theory have been studied in great details [12]. It was shown that, if the GB term is responsible for DE, this model does not satisfy local gravity constraints easily [13]. Furthermore, the modiﬁed f(G) gravity has the possibility to describe the inﬂationary era, a transition from a deceleration phase to an acceleration phase, crossing the phantom divide line and passing the solar system tests for a reasonable deﬁned function f [14-16]. The f(G) models might be less constrained by local gravity constraints compared to the f(R) models [17]. Hence modiﬁed f(G) gravity represents a quite interesting gravitational alternative for dark energy (for a recent review see [8]). The non-minimal coupling of the Ricci scalar and matter Lagrangian, can be seen as a source of inﬂation and late-time accelerated expansion of the universe [18-20]. Such a non-minimal coupling has been studied widely in the literature. For example, the non-minimal coupling between f(R)/f(G) gravity and the kinetic part of Lagrangian of a massless scalar ﬁeld has been investigated in Ref. [21]. Non-minimal coupling of a viable model of f(R) gravity and electromagnetic Lagrangian has been considered in Ref. [22] and it has been shown that inﬂation and current cosmic acceleration can be explained in such a model. The coupling between scalar curvature and Lagrangian of the electromagnetic ﬁeld arises in curved spacetime due to one-loop vacuum polarization eﬀects in Quantum Electro Dynamics (QED) [23] and breaks the conformal invariance of the electromagnetic ﬁeld, so that electromagnetic quantum ﬂuctuations can be generated at the inﬂationary stage and they act as a source for inﬂation. It has been shown that both inﬂation and late-time accelerated expansion of the universe can be realized in a modiﬁed non-minimal Yang-Mills-f(R) gravity [24]. Also, this result can be realized in a non-minimal vector-f(R) gravity in the framework of modiﬁed gravity [24]. Moreover, the conditions for the non-minimal gravitational coupling of the electromagnetic ﬁeld in order to remove the ﬁnite-time singularities have been investigated in Ref. [25]. Ref. [26] has considered f(R) gravity coupling to non-linear electrodynamics. The criteria for the validity of a non-minimal coupling between scalar curvature and matter Lagrangian have been studied in [27-30]. Furthermore, realizing power-law inﬂation in non-minimal gravitational coupling of electromagnetic ﬁeld with a general function of Gauss-Bonnet invariant has been done in [31]. Also, it has been demonstrated that both inﬂation and late-time acceleration of the universe can be realized in a modiﬁed Maxwell-f(G) gravity proposed in Ref. [32] in the framework of modiﬁed Gauss-Bonnet gravity. In this paper we study early and late-time cosmic acceleration in non-minimal Yang-Millsf(G) gravity in which Yang Mills (YM) ﬁeld couples to a function of Gauss-Bonnet invariant. Non-minimal coupling appears in some string compactiﬁcation where extra curvature terms exist in front of YM Lagrangian. Also, since the energy scale of the YM theory is higher than the electroweak scale, the existence of YM ﬁeld with a non- minimal gravitational coupling might have inﬂuence on models of grand uniﬁed theories (GUT) [24]. In the past studies, the eﬀective YM condensation as a candidate for dark energy has been proposed in [33,34] and the possibility for cosmic acceleration driven by a ﬁeld with an anisotropic equation of state has been studied in [35]. We show that power-law inﬂation can be realized in non-minimal gravitational coupling of the YM ﬁeld in the Einstein frame. Additionally, we demonstrate that both inﬂation and late-time acceleration of the universe can be realized in YM-f(G) model in the framework of modiﬁed Gauss-Bonnet gravity in which the function for f(G) is consistent with the solar system tests. An outline of this paper is as follows. In section 2 we examine power-law inﬂation in a model of non-minimally coupled YM ﬁeld with f(G) gravity in the general relativity (GR) framework. In section 3 we show that both inﬂation and late-time cosmic acceleration can be realized in such a model but in the framework of modiﬁed Gauss-Bonnet gravity proposed in [32]. Section 4 is devoted to our conclusion.
Effect of Cd2+ on the Growth and Thermal Properties of K2SO4 crystal<|sep|>Potassium sulfate K2SO4 crystallizes at room temperature in the orthorhombic olivine type structure and has four formula units per D16 2h = Pnma unit cell, with lattice constants a0 = 7.476 ˚A, b0 = 10.071˚A, and c0 = 5.763 ˚A [1]. The mineral olivine (Mg,Fe)2SiO4 is a major component of the earth’s crust. (Mg,Fe)2SiO4 and its isomorphs A2BO4 (where A = K, NH4, Rb, Cs, B = S, Se) exhibit many interesting physical properties and phase equilibria that were studied repeatedly [2,3,4,5]. Upon heating K2SO4 undergoes a ﬁrst order transformation at Tt ≈ 580 ◦C to a hexagonal structure D4 6h = P63/mmc with a0 = 5.92 ˚A and c0 = 8.182 ˚A (measured at 640 ◦C [6]). In this high-T structure the oxygen positions of the SO2− 4 tetrahedra are only partially occupied as a result of rotational disorder. K2SO4 crystals that are grown at room temperature from aqueous solution incorporate OH+ 3 ions. The OH+ 3 concentration decays in the temperature region from 300 ◦C to 450 ◦C and usually crystals are destroyed by this process [6]. El-Kabbany [7] has reported a thermal hysteresis for the solid phase transformation with Tt = 571 ◦C at heating and Tt = 566 ◦C at cooling. Electrical conductivity measurements on single crystals have been carried out by Choi et al. [8] who found Tt = 586.9 ◦C on heating and Tt = 581.5 ◦C on cooling, with a thermal hysteresis of 5.4 K. Most of the samples crack near 500 ± 30 ◦C and show an abrupt drop in electrical conductivity. The hysteresis phenomena, cracking, and electrical “pretransition phenomena” were attributed to rotational disorder of SO2− 4 ions [8] and inclusions of OH+ 3 [6]. The present authors reported recently, that the addition of Cd2+ to the nutrient solution during K2SO4 crys tal growth leads to an improvement of the crystalline quality [9]. In this paper, the inﬂuence of Cd2+ on the growth and on thermal and optical properties of K2SO4 crystal is studied in more detail.
Stochastic Bound Majorization<|sep|>Stochastic learning algorithms are of central interest in machine learning due to their simplicity and, as opposed to batch methods, their low memory and computational complexity requirements [1, 2, 3]. For instance, stochastic learning algorithms are commonly used to train deep belief networks (DBNs) [4, 5] which perform extremely well on tasks involving massive data-sets [6, 7]. Stochastic algorithms are also broadly used to train Conditional Random Fields (CRFs) [8], solve maximum likelihood problems [9] and perform variational inference [10]. Most stochastic optimization approaches fall into two groups: ﬁrst-order methods and second-order methods. Popular ﬁrst-order methods include stochastic gradient descent (SGD) [11] and its many extensions [12, 13, 14, 15, 16, 17]. These methods typically have low computational cost per iteration (such as O(d) where d is the data dimensionality) and either sub-linear (most stochastic gradient methods) or linear (as shown recently in [12]) convergence rate which makes them particularly relevant for large-scale learning. Despite its simplicity, SGD has many drawbacks: it has slow asymptotic convergence to the optimum [8], it has limited ability to handle certain regularized learning problems such as l1-regularization [18], it requires step-size tuning and it is difﬁcult to parallelize [19]. Many works have tried to incorporate second-order information (i.e. Hessian) into the optimization problem to improve the performance of traditional SGD methods. A straightforward way of doing so is to simply replace the gain in SGD with the inverse of the Hessian matrix which, when naively implemented, induces a computational complexity of O(d3). This makes the approach impractical for large problems. The trade-offs in large-scale learning for various prototypical batch and stochastic learning algorithms are conveniently summarized in [20]). Therein, several new methods are developed, including variants of Newton’s method which use both gradient and Hessian information to compute the descent direction. By carefully exploring different ﬁrst- and second-order techniques, the overall computational complexity of optimization can be reduced as in the Stochastic Meta-Descent (SMD) algorithm [21]. Although it still uses the gradient direction to converge, SMD also efﬁciently exploits certain Hessian-vector products to adapt the gradient stepsize. The algorithm is shown to converge to the same quality solution as limited-memory BFGS (LBFGS) an order of magnitude faster for CRF training [8]. There also exists stochastic versions of quasi-Newton methods like online BFGS and online LBFGS [22], the latter applicable to large-scale problems, which while using convenient size mini-batches performs comparably to a well-tuned natural gradient descent [23] on the task of training CRFs, but at the same time is more scalable. In each iteration the inverse of the Hessian, that is assumed to have no negative eigenvalues, is estimated. Computational complexity of this online LBFGS method is O(md) per iteration, where m is the size of the buffer used to estimate the inverse of the curvature. The method degrades (large m) for sparse data-sets. Another second-order stochastic optimization approach proposed in the literature explores diagonal approximations of the Hessian matrix or Gauss-Newton matrix[5, 24]. In some cases this approach appears to be overly simplistic [25], but turned out successful in very particular applications, i.e. for learning with linear Support Vector Machines [24]. There is also a large body of work on stochastic second-order methods particularly successful in training deep belief network like Hessian-free optimization [25]. Finally, there are also many hybrid methods using existing stochastic optimization tools as building blocks and merging them to obtain faster and more robust learning algorithms [9, 26]. This paper contributes to the family of existing second-order stochastic optimization methods with a new algorithm that is using a globally guaranteed quadratic bound with a curvature different than the Hessian. Therefore our approach is not merely a variant of Newton’s method. This is a stochastic version of a recently proposed majorization method [27] which performed maximum (latent) conditional likelihood problems more efﬁciently than other state-of-the-art ﬁrst- and second- order batch optimization methods like BFGS, LBFGS, steepest descent (SD), conjugate gradient (CG) and Newton. The corresponding stochastic bound majorization method is compared with a well-tuned SGD with either constant or adaptive gain in l2-regularized logistic regression and turns out to outperform competitor methods in terms of the number of iterations, the convergence time and even the quality of the obtained solution measured by the test error and test likelihood.
Intriguing microstructures of five-dimensional neutral Gauss-Bonnet AdS black hole<|sep|>The study of black hole thermodynamics and phase transition has been one of the increasingly active areas among the last couple of decades. In particular, black hole chemistry attracts much more attention. The cosmological constant was treated as a thermodynamic pressure and its conjugate quantity as volume in the extended phase space [1]. Rich phase transitions and phase structures were observed, for examples, the small-large black hole phase transition, reentrant phase transition, triple point, λ-line phase transition [2–16], for a recent review, see Refs. [17, 18]. It is extensively shown that the small-large black hole phase transition is extremely similar to the liquid-gas phase transition of Van der Waals (VdW) ﬂuids. As we know, VdW ﬂuids consist of micromolecules with ﬁnite size and interaction between them. Therefore, a natural question is what is the microstructure of the black holes. In order to examine it, we proposed that an AdS black hole is also constructed by some unknown micromolecules [19]. Combining with the black hole phase transition, we studied the properties of the black hole microstructures. It was also shown that the interaction between these micromolecules can be tested by the Ruppeiner geometry. This meaningful approach has been generalized to other black holes in AdS space [20–31]. In Ref. [19], we chose the entropy as the thermodynamic potential and took the black hole mass and the pressure as the ﬂuctuation coordinates. Then we constructed the Ruppeiner geometry and calculated the corresponding scalar curvature. Although some interesting properties were reﬂected by the scalar curvature, it does not go to negative inﬁnity at the critical point, where a second order phase transition takes place. This particular property is not consistent with the original idea of the Ruppeiner geometry [32]. This inconsistency is generally neglected in previous study. For the purpose, we further investigated this issue [33, 34], recently. Starting with the Boltzmann’s entropy formula, the general Ruppeiner geometry was constructed. Then taking the temperature and volume as the ﬂuctuation coordinates, a universal Ruppeiner metric was worked out and the corresponding scalar curvature was calculated. For a VdW ﬂuid, the scalar curvature indicates that the attractive interactions dominate amongst the ﬂuid microstructures. While for the charged AdS black hole, we found that the Ruppeiner geometry will become problematic due to the vanishing heat capacity. In order to cure this problem, we introduced a new quantity, the normalized scalar curvature, for the charged AdS black hole [33]. This treatment can be understood as that the black hole microstructure not completely depends on the heat capacity. For example, VdW ﬂuid has a constant heat capacity 3kB/2. And it almost has no inﬂuence on its microstructure. Employing this new scalar curvature, we observed that the repulsive interaction can exist for the small black hole of high temperature. This novel property shows that, even they share the similar phase transition and critical phenomena, there still exists a large diﬀerence between the microstructures of a black hole and VdW ﬂuid. This property was also observed for the higher-dimensional charged AdS black holes [34]. Furthermore, we also examined the critical phenomena of the normalized scalar curvature [34]. At the critical point, the normalized scalar curvature RN goes to negative inﬁnity, which indicates that the correlation length tends to inﬁnity. It was also found that the normalized scalar curvature has a universal exponent 1/2. Another universal dimensionless parameter is RN(1 − ˜T)2, where ˜T is the reduced temperature. For the VdW ﬂuid, numerical result shows that it is −1/8 near the critical point. For the four-dimensional charged AdS black hole, the analytical result conﬁrms this constant. While for higher-dimensional charged AdS black holes, the values are more negative than −1/8. Therefore, the observation of the same of the critical phenomena and the universal dimensionless parameter further conﬁrms that the normalized scalar curvature RN can be used to test the black hole microstructures. We also expect to uncover more information of the black hole microstructures. Since the analytical results can give us the exact information of the black hole microstructures, we would like to further study them. As we know, the coexistence curve of the ﬁve-dimensional neutral Gauss-Bonnet (GB) AdS has an analytical form [35]. This gives us a good chance to exactly investigate it. The result will provide us a possible way to understand the microstructures of the black hole in this modiﬁed gravity. This work is organized as follows. In Sec. II, we brieﬂy introduce the Ruppeiner geometry. Thermodynamics and phase diagrams of the GB-AdS black hole are discussed in Sec. III. Then we apply the geometrical method to the black hole in Sec. IV. The black hole microstructures are studied in detail. Furthermore, the critical phenomena of the normalized scalar curvature are investigated. Finally, the conclusions and discussions are given in Sec. V.
Towards efficient representation identification in supervised learning<|sep|>Representation learning (Bengio et al., 2013) aims to extract low dimensional representations from high dimensional complex datasets. The hope is that if these representations succinctly capture factors of variation that describe the high dimensional data (e.g., extract features characterizing the shape of an object in an image), then these representations can be leveraged to achieve good performance on new downstream tasks with minimal supervision. Large scale pre-trained language models demonstrate the major success of representation learning based approaches (Brown et al., 2020; Wei et al., 2021; Radford et al., 2021). However, we should look at these results with a dose of caution, as neural networks have also been shown to fail often at out-of-distribution generalization (Beery et al., 2018; Geirhos et al., 2020; Peyrard et al., 2021). To address out-of-distribution generalization failures, recent works (Sch¨olkopf, 2019; Sch¨olkopf et al., 2021; Wang and Jordan, 2021) have argued in favour of incorporating causal principles into standard training paradigms— supervised (Arjovsky et al., 2019) and unsupervised (von K¨ugelgen et al., 2021). The issue is that the current deep learning paradigm does not imbibe and exploit key principles of causality (Pearl, 2009; Sch¨olkopf, 2019)—invariance principle, independent causal mechanisms principle, and causal factorization. This is because the traditional causal inference requires access to structured random variables whose distributions can be decomposed using causal factorization, which is impossible with complex datasets such as images or text. Therefore, to leverage the power of deep learning and causal principles, we ﬁrst need to disentangle raw datasets to obtain the causal representations that generated the data, and then exploit tools from causal structure learning to pin down the relationships between the representations. (Ke et al., 2019; Brouillard et al., 2020). It has been shown that the general process of disentanglement is impossible in the absence of side knowledge of the structure of the data generation process (Hyv¨arinen and Pajunen, 1999; Locatello et al., 2019). However, under additional structural assumptions on the data generation process, it is possible to invert the data generation process and recover the underlying factors of variation (Hyvarinen and Morioka, 2016). Recently, there have been works (Hyvarinen et al., 2019; Khemakhem et al., 2020a) which present a general framework that relies on auxiliary information (e.g., labels, timestamps) to disentangle the latents. While existing works (Hyvarinen et al., 2019; Khemakhem et al., 2020a) have made remarkable progress in the ﬁeld of disentanglement, these works make certain key assumptions highlighted below that we signiﬁcantly depart from. • Labels cause the latent variables. In supervised learning datasets, there are two ways to think about the data generation process—a) labels cause the latent variables and b) latent variables cause the labels. (Sch¨olkopf et al., 2012) argue for the former view, i.e., labels generate the latents, while (Arjovsky et al., 2019) argue for the latter view, i.e., latents generate the label (see Figure 1). Current non-linear ICA literature (Khemakhem et al., 2020a; Hyvarinen et al., 2019) assumes the label knowledge renders latent factors of variation conditionally independent, hence it is compatible with the former perspective (Sch¨olkopf et al., 2012). But the latter view might be more natural for the setting where a human assigns labels based on the underlying latent factors. Our goal is to enable disentanglement for this case when the latent variables cause the labels (Arjovsky et al., 2019). Khemakhem et al. (2020b) require a lot of auxiliary information, e.g., the number of label classes should be twice the total dimension of the latent factors of variation to guarantee disentanglement. We seek to enable disentanglement with lesser auxiliary information. Contributions. We consider the following data generation process – latent factors generate the observations (raw features) and the labels for multiple tasks, where the latent factors are mutually independent. We study a natural extension of the standard empirical risk minimization (ERM) (Vapnik (1992)) paradigm. The most natural heuristic for learning representations is to train a neural network using ERM and use the output from the representation layer before the ﬁnal layer. In this work, we propose to add a constraint on ERM to facilitate disentanglement – all the components of the representation layer must be mutually independent. Our main ﬁndings for the representations learned by the constrained ERM are summarized below. • If the number of tasks is at least equal to the dimension of the latent variables, and the latent variables are not Gaussian, then we can recover the latent variables up to permutation and scaling. • If we only have a single task and the latent variables come from an exponential family whose log-density can be expressed as a polynomial, then under further constraints on both the learner’s inductive bias and the function being inverted, we can recover the latent variables up to permutation and scaling. • To implement constrained ERM, we propose a simple two-step approximation. In the ﬁrst step, we train a standard ERM based model, and in the subsequent step we carry out linear ICA (Comon, 1994) on the representation extracted from ERM. We carry out experiments with the above procedure for regression and classiﬁcation. Our experiments show that even with the approximate procedure, it is possible to recover the true latent variables up to permutation and scaling when the number of tasks is smaller than the latent dimension.
Cosmological constraints for the Cosmic Defect theory<|sep|>From the very beginning of the theory of relativity both in its special form (SR) and in its general form (GR) a problem of interpretation of the nature of space-time has been present. This problem, in most cases, is implicit rather than explicit, but it is there. Is space-time a sort of ﬁeld? which would be perfect for people trying to quantize the gravitational interaction. This cannot be, however, since, in general, ﬁelds are described in space-time; what
An Efficient Optimal Planning and Control Framework For Quadrupedal Locomotion<|sep|>Motion control in robotics promises to bring more autonomy to robots in the sense that they can plan and control their motion in the environment without or with minimum operator interference. In our terminology, motion control refers to the whole process of planning and control of the motion of a robot over the course of time. Due to the hybrid and nonlinear nature of the legged locomotion problem, ﬁnding an optimal plan and controller is a challenging task. Furthermore, a practical implementation of a motion control framework requires to deal with uncertainty and dynamic changes of the environment. The Model Predictive Control (MPC) approach can address these issues to some extent. MPC is an optimal control framework which repeatedly plans the system’s motion in the future and then only executes the ﬁrst part of the plan until new information becomes available. Among the different ways to solve the MPC problem, using optimal feedback planners is one of the most promising approaches. In contrast to conventional methods where only the feedforward plan is calculated, the feedback planners design the stabilizing controller parallel to the open–loop plan. Using feedback planners is more essential on the high dimensional problems where the MPC loop runs slower than the controller rate. These feedback planners are mostly based on the Dynamic Programming (DP) framework rather than the widely used Trajectory Optimization (TO) approaches. However, known as the curse of dimensionality, DP does not naturally scale to high dimensional systems. Therefore, for a long time, it has been the belief that a DP-based algorithm ∗All authors are with the Agile & Dexterous Robotics Lab, ETH Z¨urich, Switzerland, email: {farbodf, neunertm, winklera, buchlij}@ethz.ch †Gonzalo Rey is with Moog grey@moog.com cannot solve real-life, high dimensional problems such as legged robot locomotion. However, the recent progress in efﬁcient DP-based algorithms as well as the advent of fast and cheap processors have brought attention back to the DP approaches once again. While algorithms like Mayne’s DDP (Differential Dynamic Programming) have existed since 1966 [1], it was only until recently, that the fast DP-based algorithms were revisited [2], [3] and their performance were demonstrated in different robotic platforms such as quadrotors, swimming robots, and legged robots. This class of fast and efﬁcient algorithm are formally known as Sequential, Linear, Quadratic (SLQ) methods. In order to avoid the curse of dimensionality which arises from the value function calculation, it uses a local quadratic approximation of the value function to calculate its value in the vicinity of the current operating points. Then, each iteration of this approximation is followed by a forward integration of the system dynamics in order to update the nominal operating points for the next iteration. In spite of their efﬁciency, the SLQ type algorithms have a major drawback in that they often cannot efﬁciently handle equality or inequality constraints. In this contribution, we address this issue by proposing a method to formulate legged robot locomotion as a constrained switched system optimal control problem which can be solved via our efﬁcient feedback planner. Broadly speaking, there are two basic approaches for solving the optimal motion control problem namely Dynamic Programming and Trajectory Optimization. The DP approach is based on the principle of optimality. The methods in this framework normally break the optimal control problem into a collection of the simpler subproblems, solve each of those subproblems just once, and store their solutions. On the other hand, the TO methods are techniques for computing open– loop solutions to the optimal control problem. TO transforms the original inﬁnite–dimension continuous problem into a ﬁnite dimensional Nonlinear Programing (NLP). TO can be categorized into direct and indirect methods [4]. The indirect methods which are based on the Pontryagin’s maximum principle have a limited application in robotics. Direct methods can be divided into three main groups namely direct single shooting, direct multiple shooting [5] and direct transcription [6]. The single shooting approach starts by discretizing the control inputs, then performs a forward integration through an ODE solver. Due to the unappealing numerical characteristics of this method, its application on problems with long optimization horizon is rather limited. The multiple shooting approach is based on a similar idea as single shooting with the difference that it divides the long integration into smaller pieces. This technique helps to reduce the cumulative affect of the early parameters of trajectory on the later ones which results in a better NLP problem. However, in order to ensure continuity, it adds a set of matching conditions at each interval. This method has been employed successfully for trajectory planning on a number of locomotion tasks [7], [8]. On the other hand, the direct transcription methods use an approximation scheme to transform the optimal control problem to a ﬁnite dimension NLP. To do so, the direct transcription methods discretize the state and control trajectories into a ﬁnite number of nodes and then interpolate in between the nodes by spline approximation. In [9], [10] a direct transcription approach is applied to legged robot motion control where the end-effector placement is incorporated into the whole-body planning. Regardless of the chosen method, many of these approaches have shown their capabilities in planning contact rich motion on legged robots. However, the transformation of the optimal control problem to a general NLP introduces a high dimensional optimization problem. In addition to slow convergence rates and a complex objective landscape, the outcome of TO trajectories cannot be implemented directly on hardware and in most cases require a tracking controllers to implement the designed trajectories [11], [12]. This can produce non-optimal motion in real hardware experiments. Recently, there has been an increasing interest in efﬁcient DP-based feedback planning approaches where the feedforward plan is designed together with the feedback controller [1–3]. DP–based approaches have been applied for controlling a humanoid [13] and a quadruped robot [14]. Both applications use smooth contact models in order to discover the contact sequence as well as the motion trajectories and stabilizing feedback controller. This smooth contact assumption is required since none of such methods is able to deal with the state–input constraints introduced by contacts. There are few examples of constrained DP–based methods that can potentially scale to the legged robotics problem [15–17]. However, they either need a near–optimal initial guess to converge [15], [16], or are computationally inefﬁcient due to an extra backward pass in each iteration of algorithm [17]. In this contribution, we introduce an approach to transform the optimal motion planning and control problem for legged robots into a more tractable optimization problem based on a multi–level optimization algorithm. To do so, we reformulate this problem into an optimal control problem for switched systems assuming that the switching mode sequence is given. In the new problem, the optimization variables are the original system’s control inputs as well as the switching times between different modes of motion. Based on this model, we use a multi–level optimization approach introduced in [18] which optimizes the switching time and the continuous control inputs in two alternating optimization problems. Furthermore, we introduce a continuous-time, constrained SLQ algorithm with state and input equality constraints which has a O(n) time–complexity. The idea of constrained SLQ has originally been introduced in [19] for the discrete–time formulation. In our work, we extend this work to the continuous case while we reduce its complexity to O(n). Furthermore, the continuous time version allows us to use adaptive stepsize integrators, which helps to achieve shorter runtimes in practice. Finally, we evaluate our framework performance on a quadruped robot where we use the centroidal dynamics and full kinematics model for motion planning and control.
An Efficient Schmidt-EKF for 3D Visual-Inertial SLAM<|sep|>Enabling centimeter-accuracy positioning for mobile and wearable devices such as smart phones and micro air vehicles (MAVs), holds potentially huge implications for practical applications. One of the most promising methods providing precision navigation in 3D is through the fusion of visual and inertial sensor measurements (i.e., visualinertial navigation systems or VINS) [30, 13, 22, 21, 17, 15]. This localization solution has the advantages of being both cheap and ubiquitous, and has the potential to provide position and orientation (pose) estimates which are on-par in terms of accuracy with more expensive sensors such as LiDAR. To date, various algorithms are available for VINS problems including visual-inertial (VI)-SLAM [19, 45] and visual-inertial odometry (VIO) [30, 29, 22], such as the extended Kalman ﬁlter (EKF) [30, 20, 14, 22, 17, 16, 50, 37], unscented Kalman ﬁlter (UKF) [10, 4], and batch or slidingwindow optimization methods [46, 18, 21, 33, 52, 45, 40], among which the EKF-based approaches remain arguably the most popular for resource constrained devices because of their efﬁciency. While current approaches can perform well over a short period of time in a small-scale environment (e.g., see [13, 22, 15]), they are not robust and accurate enough for long-term, large-scale deployments in challenging environments, due to their limited available resources of sensing, memory and computation, which, if not properly addressed, often result in short mission duration or intractable real-time estimator performance. In this paper, we will primarily focus on EKF-based VISLAM rather than VIO. VI-SLAM has the advantage of building a map of the surrounding environment, which enables “loop closing” to bound long-term navigation drift. VIO systems do not build a map and therefore cannot leverage information from prior observations to help improve estimator performance. However, one of the largest downsides of SLAM is that its computational complexity grows quadratically with the number of landmarks in the map, which commonly makes it computationally intractable without simplifying assumptions to allow for them to run on resource constrained sensor platforms such as mobile devices. To address this complexity issue, we leverage the computationally-efﬁcient multi-state constraint Kalman ﬁlter (MSCKF) [30] and selectively keep a number of features (say n) in the state vector as a map of the environment, enabling the system to use them for a long period of time and thus allowing for (implicit) loop closures to bound drifts. This, however, would still exhibit O(n2) computational complexity as in the standard EKF-based SLAM. By observing that features’ estimates do not have signiﬁcant updates if they approach their steady state (i.e., becoming matured/converged), we could gain substantial computational savings by avoiding performing EKF updates for those matured map features while still taking into account their uncertainty. To this end, we adapt the Schmidt Kalman ﬁlter (SKF) [44] and treat map features as nuisance parameters which will no longer be updated but whose covariance and cross-correlations to other states are still utilized in the EKF update. As a result, this renders only O(n) computational complexity, making our proposed Schmidt-EKF VisualInertial SLAM (SEVIS) signiﬁcantly more amenable to running on resource-constrained sensor platforms. In particular, the main contributions of the paper include: • We design a high-precision, efﬁcient Schmidt-EKF based VI-SLAM (i.e., SEVIS) algorithm which leverages the Schmidt-KF formulation to allow for concurrent estimation of an environmental map used for longterm loop closures to bound navigation drifts with linear computational complexity. • We propose a keyframe-aided 2D-to-2D matching scheme for the challenging data association problem of matching 2D visual measurements to 3D map features, without performing 3D-to-2D matching (which may not be applicable to sparse 3D environmental maps). This 2D-to-2D matching is not effected by estimation performance, allowing for long-term loop closures and recovery from extreme drifts. • We validate the proposed SEVIS algorithm extensively in both Monte-Carlo simulations and real-world experiments, showing the applicability and performance gains offered by our system. The experimental study of computation requirements further shows that the proposed SEVIS remains real-time while building and maintaining a 3D feature-based map.
Enhanced Transfer Learning Through Medical Imaging and Patient Demographic Data Fusion<|sep|>In medical applications such as diagnosis from medical images, clinicians will typically have access to relevant information that is not contained within the image. A patient’s clinical or demographic history may have a signiﬁcant role in the clinical decision making pipeline. For instance, the age of a patient may signiﬁcantly affect the probability or risk of a disease and hence may increase the weighting on speciﬁc features or borderline instances. Other cases may not be as obvious or explicitly known to the clinician. In any case, analysis based on the imaging data alone does not fully utilise relevant data available and limits the potential of AI-assisted decision making in biomedical applications. Deep learning has emerged as a powerful suite of tools for image classiﬁcation [1], and has a huge potential to solve challenges in healthcare settings. The use of deep neural networks is successful at tasks such as classiﬁcation of medical images [2], analysis of electronic health records [3]– [5] and segmenting data from emerging medical technologies [6], [7]. This enormous potential comes with the caveat that very large amounts of data are required to train robust models that generalise beyond the training set. This requirement is unfortunately difﬁcult to satisfy in the majority of biological and medical studies due to barriers to data availability. Transfer learning has emerged as a promising method for circumventing the need for vast amounts of data to train deep networks [8]. For domains with limited data, transfer learning utilises networks pre-trained on similar tasks with large amounts of data [9]. Transfer learning is often used in medical imaging [10]–[12] due to the limited availability of data that require expert labeling [13]. Transferring the image features from one domain to another can at least match the performance of models trained directly on the new domain [14], though it is not known if this is a general property as some cases the performance is worse than hand crafted features [15], Moreover, the conﬁguration of the transfer can be performed in a number of ways [13], [16] and more research is needed in this area. Medical imaging data often has associated metadata used by clinicians in patient assessments. These metadata are multi type (numeric, categorical, etc) and are essential for maintaining the value of archived data [17]. The information may be content related, e.g. scanner parameters, or relevant extracts from computerised medical records (CMR). These resources contain rich information relating to diseases [18], [19], and data driven methods can identify patterns of patients [3], [4]. Classiﬁcation tasks based on the combination of imaging with genomics data has been shown to surpass clinical experts in digital pathology [20]. Combining relevant information about the sample, e.g. patient demographics, with imaging data can lead to higher accuracy in binary classiﬁcation tasks [21]. An enhancement in classiﬁcation performance has been observed in transfer learning for speciﬁc conﬁgurations with a single dataset [22], though it is unknown if this applies to other methods of transfer learning or target domains. Furthermore, the explain-able origin of an enhancement in performing from such a framework is needed. Clinicians will typically base diagnosis on several information sources either implicitly or explicitly. Demographic factors such as age can inﬂuence the likelihood of disease prevalence. In this work we investigate the combination of imaging data with related metadata to enhance classiﬁcation performance evaluated by several metrics. We utilise transfer learning due to the limited volumes of data available, comparing the performance with and without metadata. Additionally we repeat the experiments with and without data augmentation during the training of the model. Metadata is needed for data curation and essential for maintaining the value of archived data [17]. Metadata for medical applications contain vital information regarding provenance and parameters of acquisition that could impact data processing or interpretation of results. The potential of metadata for healthcare data not only lies in the curation and knowledge of the data, but it can be a rich source of informa tion itself. Computerised medical records contain a patient (meta) data and are often complex high dimensional data sources. Methods for analysing CMRs have uncovered low dimensional patterns relating to sub-groups of patients [3], [4] and have enormous potential for insight into biomedical sciences. This paper focuses on the potential of integrating imaging data with non-imaging data (e.g. metadata, CMRs, etc) as a means to improve performance of classiﬁcation of medical imaging. The value for information within nonimaging data may enhance the predictive power of deep learning classiﬁcation methods through the combination of relevant information with traditional image features extracted by a deep network. These multi-modal data can be combined prior to the classiﬁcation in a deep network offering a new direction of research in how to combine these data to most improve model performance. Here we investigate how the inclusion of non-imaging data affects the performance of transfer learning based multiclass classiﬁcation problems using real world data. For generality, we consider the ISIC [23] skin imaging and image representations of the PTB XL [24] ECG datasets. We perform experiments with eight popular convolutional neural network models to investigate the effects of combining imaging and non-imaging data on the classiﬁcation performance assessed via several metrics. We conduct experiments for each network with and without image augmentation. Finally, we repeat these experiments using network weights obtained directly from training on the ImageNet [25] dataset, retraining the classiﬁcation layer only (so called bottleneck feature extraction), and compare to retaining the ImageNet weights using the target dataset (so called ﬁne tuning).
Deep Ordinal Regression for Pledge Specificity Prediction<|sep|>Election manifestos play a critical role in structuring political campaigns. Campaign communication can inﬂuence a party’s reputation, credibility, and competence, which are primary factors in voter decision making (Fernandez-Vazquez, 2014). Among the various campaign-related functions fulﬁlled by manifestos (Eder et al., 2017), perhaps the most important is the contract they represent between parties and voters in terms of pledges and prioritisation of political issues (Royed et al., 2019). Political scientists have long studied how speciﬁc pledges translate into government programs and actual policy (Royed, 1996; Thomson, 2001; Naurin, 2011; Schermann and Ennser-Jedenastik, 2014). Other work relates speciﬁc pledges to the issue clarity of a political party Issue clarity has also been shown to be inﬂuenced by a party’s ideological position and its role in government (Praprotnik, 2017). Although pledge speciﬁcity prediction is an important task for the analysis of party position, priorities, and post-election policy framing, to date, almost all research has relied on manual analysis. Subramanian et al. (2019) is a recent exception to this, in performing speech act classiﬁcation over political campaign text, where the class schema includes the distinction between speciﬁc and vague pledges (binary speciﬁcity class). In this paper, we perform ﬁne-grained pledge speciﬁcity prediction, which is more expressive than binary levels (Li et al., 2016; Gao et al., 2019). We use a class schema proposed by Pomper and Lederman (1980) as detailed in Table 1, which captures seven levels of speciﬁcity, forming a nonlinear increasing order of commitment and speciﬁcity (Pomper and Lederman, 1980). Given the non-linear nature of the scale, we use deep ordinal regression models for this task, with distributional loss (Imani and White, 2018), where we model the output as a uni-modal distribution (Beckham and Pal, 2017). Our goal is to capture the intuition that a pledge with speciﬁcity level k, has higher commitment than all the levels < k, producing a smoothly varying prediction over the ordinal classes. This can be modeled as a uni-modal distribution which has a probability mass that decreases on both sides of the most probable class. Lastly, as it is expensive to obtain large-scale annotations, in addition to developing a novel annotated dataset, we also experiment with a semisupervised approach by using unlabeled text. The contributions of this paper are as follows: (1) we develop and release a dataset1 for ﬁne-grained pledge speciﬁcity prediction based on election manifestos covering eleven Australian federal election cycles (1980–2016), from the two major political parties — Labor and Liberal; (2) we propose to use deep ordinal regression models for the prediction task, and evaluate the model under sparse supervision scenarios using the teacher– student framework; and (3) we evaluate the utility of pledge speciﬁcity towards ideology prediction, and provide further qualitative analysis by correlating model predictions with party-speciﬁc issue salience across major policy areas.
A model of discrete Kolmogorov-type competitive interaction in a two-species ecosystem<|sep|>An ecosystem is a discrete map defined on a compact, normed space. The various states of  ecosystem that are observed in nature are the orbits of the map. Following common usage of  the term, we shall henceforth refer to an orbit of the above map as an ecosystem. A given ecosystem evolves in time by evolving complex structures and patterns due to the  emergent behaviour of the interactions between the components comprising it, qualifying as a  complex adaptive system. These patterns, in turn, serve as indicators of long-term  coexistence or extinction of the constituent species populations. This feature of the ecosystem  qualifies it to be a complex adaptive system [1, 2, 3, 4, 5]. The question of dynamic coexistence of competing species in a complex ecosystem has been  a problem of prime importance in dynamical systems research, attracting a significant body  of studies [6, 7, 8, 9, 10, 11, 12]. Both, intra-and inter-specific competitions constitute one of the fundamental classes of ecological interaction, leading to possible coexistence or  extinction of competing species in a given ecosystem. Hence, competitive interactions are  important objects of study in order to gain an understanding of the dynamics of evolution and  emergence of complex structures in the ecosystem. The objective of the present work is to explore the discrete-time dynamical behaviour of a  competitive two-species “toy” model of an ecosystem such that both the species occupy the  same trophic level. We use numerical experiments with an aim to describe and understand  some possible characteristic features of the emergence of dynamical regimes and hence of  complexity in the evolving ecosystem. The work is a descriptive modelling of the “toy”  ecosystem behaviour. In particular, we study the explicit influence of the competitive  interaction on a long-term coexistence of the species, which have non-overlapping  generations.  We assume that the “toy” ecosystem comprises populations of only two species  at the same trophic level, which we call species A and B respectively, such that the length of  their generations is similar. The paper makes a further simplification by assuming that apart  from these two species, populations of no other species, floral or faunal, is present in the  ecosystem, meaning thereby that the ecosystem entirely comprises species A and B only. It may be noted that discrete-time complex dynamics of ecosystems have been well-studied  since long. Such systems have been analysed by authors, primarily with a focus on the  predator-prey communities or on a single species population [7, 8, 9]. The work presented  here has its focus instead on the inter-specific competition that may best be conceptualized at  the same trophic level of the ecosystem. An appropriate model for addressing our objective is the discrete-time Kolmogorov system,  which adequately describes the population dynamics two-species population with nonoverlapping generations [10, 11, 12, 13, 14, 15, 16]. In this work we study the time evolution of non-overlapping generations of the two species A  and B, comprising a typical “toy” ecosystem. Without loss of generality, we choose to study  the competitive response of population of A to population of B using numerical simulations  to understand the various discrete dynamical signatures of such an interaction, and address  the question of coexistence of these populations. In section II, we propose the mathematical  model of discrete-time Kolmogorv system governing the dynamics of the toy ecosystem and  pose the research question. In section III, we report sample results of the numerical  experiments and simulations performed on the proposed model, in order to address and  answer the question posed, and conclude the paper in section IV, after drawing inferences  from the results obtained.
Sampling Strategy for Fine-Tuning Segmentation Models to Crisis Area under Scarcity of Data<|sep|>Remote sensing has been extensively used by humanitarian aid teams, and collaborative mapping has become a crucial part of crisis response. Some of the success stories includes response after the Haiti and Nepal earthquakes, and Ebola outbreak in North West Africa in 2014 [3]. Despite engagement of thousands of volunteers and NGOs there is still a place for improvement and making interventions such as Missing Maps more effective, therefore allowing Humanitarian Aid teams to be prepared better for handling the crisis. One of the problems that has emerged with the introduction of deep learning solutions in the ﬁeld is their lack of integration with current humanitarian workﬂows. Although it is a natural idea – the improvement of annotating new maps with the usage of deep learning models to automate the process – such a system is not a part of typical workﬂows. One of the reasons is trust and reliability of predictions. A model that works on one geographic area might not work on another. The aim of this paper is to present a method of selecting a subsample of a new dataset for manual labeling to be used during ﬁne-tuning. The approach is to provide an estimated priority list of samples, based on estimated IOU scores. The goal is to assign higher importance to areas of interest, choosing instances that maximizes variety of instances. We can look at this problem in two ways: in the notion of improvement current model based on the knowledge of errors on the test set and in the notion of ﬁne-tuning the model speciﬁcally to a new region. Improving the model over parts that poorly generalize on the data coming from the same distribution might be seen as less important then retraining the model to cover new areas, as we can assume that the deployed model had acceptable performance. On which data shall we ﬁne-tune it, if we cannot obtain labels for the whole region? How can we distribute the task of manual labeling to volunteers that will be the most
Service-Based Drone Delivery<|sep|>Smart cities are composed of several smart components including smart homes, smart agriculture, smart buildings, smart campus, smart economy, smart logistics, etc. [1]. Smart cities integrate various technologies that offer innovative services to improve the quality of citizens’ life [2]. The key stakeholders of smart cities include citizens, governments, urban businesses, and service providers [3]. In this respect, Unmanned Aerial Vehicles (UAVs) offer unique capabilities and functionalities required to realize the vision of smart cities [4]. A UAV is a ﬂying robot that is controlled remotely or is operated autonomously to perform speciﬁc tasks [5]. A drone is a speciﬁc type of UAV that provides support for a variety of civilian applications in smart cities [6]. Examples of drone applications include trafﬁc monitoring, ﬁreﬁghting, surveillance, agriculture, and package delivery [7]. Drones provide a ubiquitous (i.e., anytime anywhere), cost-effective, fast, contactless, and environmentally-friendly alternative for package delivery to end users. Several logistics companies such as Amazon, Google, Alibaba are stepping up efforts to use drones for delivery services [8]. The service paradigm provides a key mechanism to abstract the functional capabilities of drones and their non-functional attributes as drone delivery services [9], [10]. In this context, the key functional aspect of a drone delivery service is the transport of a package from one location (e.g., warehouse rooftop) to another location (e.g., customer’s building rooftop) in a skyway network [11]. The non-functional (i.e., Quality of Service (QoS)) aspect of a drone delivery service may include payload capacity, ﬂight range, battery capacity, etc. A skyway network is made up of a set of line segments whose endpoints constitute the nodes of the network [12]. These nodes typically represent the rooftops of buildings and dwellings. The nodes may concurrently act as both delivery targets and/or recharging stations. The transport of a package along a line segment would represent a drone delivery service operating under a set of constraints. Drones for delivery services present unique challenges to fully deliver on their potential. In this respect, the inherent limitations and contextual constraints of a drone exhibit key challenges for the wide-range deployment of drone-based delivery services. Examples of inherent limitations include limited payload, limited ﬂight range, and battery capacity [13]. Examples of contextual constraints include those related to recharging pads at each recharging station as well as uncertain weather conditions such as wind [14]. In addition, current drone ﬂying regulations only allow the use of small drones for delivery (payload < 2.5 kg). There are instances where there is a need to deliver goods by a deadline and which weigh more than the maximum of a single drone’s payload. In this case, the use of drone swarms is an effective alternative to address the aforementioned regulations and requirements [15]. However, the use of drone swarms also presents unique challenges when used for delivery services. These challenges are mostly related to the requirement that all drones in a swarm arrive together within a time window, their varying ﬂight performance, formation, etc. [16]. The evolution of package delivery methods is presented in Fig. 1. Current approaches mainly focus on point-to-point drone-based deliveries ignoring the drone’s recharging requirements and complexity of the environments [17]. In these approaches, the drone delivery problem is usually formulated as Travelling Salesman Problem (TSP) [18] or Vehicle Routing Problem (VRP) [19]. However, there is an increasing number of studies that focus on service-based approaches for multipoint single drone-based and drone swarm-based deliveries in skyway networks [15]. These service-based approaches take into account different constraints affecting the delivery services, e.g., payload capacity, battery capacity, wind conditions, etc. A single drone delivery service may not satisfy a delivery plan because of the aforementioned challenges. In such cases, a service composition is required to deliver packages. An optimal service composition is deﬁned as the selection and aggregation of the best drone delivery services (i.e., skyway segments) in a skyway network from a given source to a destination [20]. The composition of drone delivery services creates a value-added service while meeting the QoS requirements of end-users which include fast, safe, costeffective, and contactless delivery. Fig. 2 presents a drone delivery service composition scenario where a drone delivers a package from point A to point B. In this scenario, a drone may not travel directly from the source to the destination due to ﬂight regulations and ﬂight range limitations. As each drone service has its speciﬁc QoS attributes such as ﬂight time and delivery cost. Therefore, different compositions will provide different aggregate QoS. This paper maps out a strategy to leverage the service paradigm for the potential utilization of drones for delivery services in smart cities. We envision a green and sustainable logistics future for smart cities provided by drone delivery services. We highlight the beneﬁts of service-based drone delivery from providers and consumers perspective. We present a service-based drone delivery framework that includes enabling technologies, operating environment, types of deliveries, and service-oriented architecture of sensor-cloud infrastructure. In this paper, we also provide the details of contemporary approaches for service-based drone delivery. Finally, we describe the open challenges and draw a road map for future research directions.
Strongly Universal Reversible Gate Sets<|sep|>The study of reversible and conservative binary gates was pioneered in the 1970s and 1980s by Toﬀoli and Fredkin [3,5]. Recently, Aaronson, Greier and Schaeﬀer [1] described all binary gate sets closed under the use of auxilliary bits, as a prelude to their eventual goal of classifying these gate sets in the quantum case. It has been noted that ternary gates have similar, yet distinct properties [7]. In this article, we consider the problem of ﬁnitely-generatedness of various families of reversible logic gates without using auxiliary bits. In the case of a binary alphabet, it is known that the whole set of gates is not ﬁnitely generated, but the family of gates that perform an even permutation of {0, 1}n is [1,6]. In ⋆ The authors would like to acknowledge the contribution of the COST Action IC1405 This work was partially funded by Austrian national research agency FWF research grants P24077 and P24285, and by FONDECYT research grant 3150552. [7], it is shown that for the ternary alphabet, the whole set of reversible gates is ﬁnitely generated. In this paper, we look at gate sets with arbitrary ﬁnite alphabets, and prove the natural generalization: the whole set of gates is ﬁnitely generated if and only if the alphabet is odd, and in the case of an even alphabet, the even permutations are ﬁnitely generated. In [6], it is proved that in the binary case the conservative gates, gates that preserve the numbers of symbols in the input (that is, its weight), are not ﬁnitely generated, even with the use of ‘borrowed bits’, bits that may have any initial value but must return to their original value in the end. On the other hand, it is shown that with bits whose initial value is known (and suitably chosen), all permutations can be performed. We prove for all alphabets that the gates that perform an even permutation in every weight class are ﬁnitely generated, but the whole class of permutations is far from being ﬁnitely generated (which implies in particular the result of [6]). Our methods are rather general, and the proofs both in the conservative case and the general case follow the same structure. The negative aspect of these methods is that our universal gates are not the usual ones, and for example in the conservative case, one needs a bit of work (or computer time) to construct our universal gate family from the Fredkin gate. We start by introducing our terminology, taking advantage of the concepts of clone theory [4] applied to bijections as developed in [2], leading to what we call reversible clones or revclones, and reversible iterative algebras or revitals. We generalize the idea of the Toﬀoli gate and Fredkin gate to what we call ‘controlled permutations’ and prove a general induction lemma showing that if we can a single new control wire to a controlled permutation, we can add any amount. We then show two combinatorial results about permutation groups that allow us to simplify arguments about revitals. This allows us to describe generating sets for various revclones and revitals of interest, with the indication that these results will be useful for more general revital analysis, as undertaken for instance in [1]. While theoretical considerations show that ﬁnite generating sets do not exist in some cases, in other cases explicit computational searches are able to provide small generating sets.
Shortcuts to adiabaticity applied to nonequilibrium entropy production: An information geometry viewpoint<|sep|>Understanding nonequilibrium properties of dynamical systems is a fascinating topic in physics and has been studied intensively. The ﬂuctuations of thermodynamic functions are considered to be key properties, and the Jarzynski equality [1, 2] and the ﬂuctuation theorem [3, 4] play the prominent roles. The nonequilibrium entropy production is one of quantities to measure nonequilibrium properties of the system and has been studied in many contexts. Especially, knowing the lower bound is an important task since it determines irreversibility, dissipation properties, eﬃciency and so on [5, 6, 7, 8, 9, 10, 11]. Thermally-isolated quantum systems can be treated by the unitary dynamics of the Schr¨odinger equation. In this paper we characterize the dynamics by the method of shortcuts to adiabaticity (STA). This method enables us to achieve an adiabatic dynamics in a ﬁnite time. To prevent the nonadiabatic transitions, we introduce an additional term called the counterdiabatic term. The fundamental idea was pointed out using a simple two-level Hamiltonian in [12] and the general formulation was developed in several works [13, 14, 15, 16]. Since then, the method has been intensively studied in various way [17]. We can ﬁnd applications to simple systems [18, 19], scale-invariant systems [20], many-body systems [21, 22, 23], and classical systems [24, 25, 26, 27] and so on. The method can also be implemented experimentally to several systems [28, 29, 30, 31]. It is also expected to be applied to quantum computations such as the quantum annealing. It should be stressed that STA is applied to any dynamical systems. STA is useful not only to control the system but also to describe general unitary dynamics. As we describe below, the system Hamiltonian is separated into two parts, ˆH(t) = ˆH0(t) + ˆH1(t), and the state satisfying the Schr¨odinger equation is given by adiabatic states of ˆH0(t). Then, it would be an interesting problem to apply this separation to general nonequilibrium processes. In STA, a cost of the time evolution was studied in [32, 33, 34, 35]. A trade-oﬀ relation between time, energy ﬂuctuation and state distance is known as the quantum speed limit [36] and was discussed in the context of STA in [37]. The applications of STA to thermodynamic systems were studied in several works [38, 39, 40, 41]. A universal trade-oﬀ relation was derived from work ﬂuctuation relations in [42]. In this paper, we study properties of the nonequilibrium entropy production that are applicable to general nonequilibrium processes. In thermally-isolated systems, the entropy is directly related to the work average and the present result is essentially equivalent to the result in [42]. However, the entropy is represented by the Kullback– Leibler (KL) divergence, which leads us naturally to the information-geometric interpretation of the nonequilibrium process. Establishing this novel picture is the main aim of the present work. The organization of this paper is as follows. In section 2, we discuss how a given Hamiltonian is separated into two parts. Then, the method is applied to the nonequilibrium entropy production in section 3. In section 4, we discuss lower bounds of the entropy by using the improved Jensen inequalities and derive a trade-oﬀ relation. The last section 5 is devoted to conclusions.
Density of warm ionized gas near the Galactic Center: Low radio frequency observations<|sep|>The central few hundred parsec region of the Galaxy is characterized by a dense and turbulent interstellar medium, where the density and velocity widths of spectral lines could almost be an order of magnitude higher than the disk of the Galaxy. This environment prefers massive stars to form, which provide copious amount of ultraviolet radiation causing ionization of molecular clouds and thereby producing large column density of warm ionized medium (WIM). This WIM causes high dispersion and scattering to the radiation passing through it. Dispersion measure causes a frequency dependent delay, while scattering due to electron density irregularities cause angular broadening of sources viewed through it. Dispersion is quantiﬁed by dispersion measure and can be measured towards time variable sources like pulsars. It directly provides column density of electrons towards that line of sight. At low radio frequencies, free-free absorption by WIM also causes a frequency dependent absorption to the radiation passing through it (Anantharamaiah et al. 1991), that provides important information on the emission measure of this gas that can be used to estimate the density of the WIM. Scattering due to WIM provides a measure of turbulence in the medium and is related to the square of the electron density ﬂuctuations in the medium (see e.g., Taylor & Cordes (1993)). Using pulsar dispersion measures, Taylor & Cordes (1993) made a model of electron density distribution in our Galaxy that provides a valuable tool in providing the distribution of the WIM in the Galaxy and a distance estimate of pulsars from their dispersion measures. However, due to lack of pulsars discovered close to the Galactic Center (GC), modeling electron density distribution near it was not possible. Later, Lazio & Cordes (1998) predicted the electron density in the central ∼ 100 pc to be about 10 cm−3. Their model is largely based on (i) scatter broadening of Galactic sources (e.g., masers seen towards the GC have angular sizes ranging from a few hundred milli arc-sec (van Langevelde et al. 1992) to about 1′′ for Sgr A* at 1 GHz) and (ii) free-free absorption towards the GC sources like Sgr A complex. They used scattering sizes of a few known extragalactic sources to limit the angular size of the scattering screen. This model has been incorporated in the improved model of Galactic electron density distribution, NE2001 (Cordes 2004). Since the Hyperstrong scattering screen in this model is shown to be close to the GC, it is ineﬃcient in scattering the GC sources. This model predicts scattering size of extragalactic sources seen through it as ∼ 100′′ at 1 GHz, but prediction of scattering diameter of EG sources based on scattering of GC sources goes inversely proportional to the distance of the screen from the GC. Moreover, Galactic masers could be preferentially located in dense clouds, the ionized surface of which could give rise to scattering, and thereby would introduce error in scattering seen towards directions away from the clouds. Scattering size of extragalactic sources are required to check the model prediction. Lazio et al. (1999) found the radio galaxy G359.87+0.18 seen through the Hyperstrong scattering regime at 0.33 GHz with a scattering size of ∼ 20′′, that showed our lack of knowledge on ﬁlling factor of the Hyperstrong scattering screen. Electron density is shown to drop sharply by two orders of magnitude just outside the Hyperstrong scattering region. This sudden drop in electron density is unrealistic and results from lack of constraints in NE2001. Indeed, Bower et al. (2001) have shown the scattering sizes of 3 extragalactic sources close to but outside the Hyperstrong scattering screen to be signiﬁcantly larger than the Taylor & Cordes (1993) model. Moreover, In the last one decade, some of the constraints used in the model has changed. For example, emission from Sgr A* is shown not to undergo free-free absorption due to Sgr A West below 950 MHz, but has been observed at much lower frequencies (Nord et al. 2004; Roy & Pramesh Rao 2004). Also, the GC region has been mapped at low radio frequencies of 0.154 (Roy & Rao 2009) and 0.074 GHz (Brogan et al. 2003). Free-free absorption of sources seen through the GC region at these low frequencies could better constrain the density of the WIM. Scattering size is roughly proportional to the square of the observing wavelength. Therefore, low radio frequency observations of the central 2◦ region of the GC could provide more detection of background extragalactic sources, and measuring their angular sizes at more than one wavelength would provide a measure of their true scattering size. We have carried out Giant Metrewave radio telescope (GMRT) observations of this region at 0.255 and 0.154 GHz. Preliminary results of these observations were published in Roy & Pramesh Rao (2006); Roy & Rao (2009). Here, we present more detailed analysis from these data sets and examine existing higher frequency observations in the region to ﬁnd the properties (i.e., electron density, angular extent and ﬁlling factor) of the WIM in the central 2◦ region of the GC. The remaining sections are arranged as follows - In sect. 2, observing and data analysis procedures are described. In Sect. 3 and 4, we describe the results obtained and discuss the implications respectively. Conclusions are presented in Sect. 5.
Response Aware Model-Based Collaborative Filtering<|sep|>Recently, online music and video streaming services have seen an explosive growth. As the user base and contents expanding tremendously, recommender systems become crucial for service providers. Cloudbased music streaming services such as iTunes Match, Google Music, Yahoo! Music, Pandora, Songify, etc. make it easier than ever to rate songs and buy new music. With the rocketing growth of the number of users, their explicit ratings become more and more accessible. Eﬀective usage of these ratings can lead to high quality recommendation, which is vital for the cloud-based online streaming services. This is because most services charge very little subscription fee, if not none. The main income comes from the selling of music. Nowadays, online streaming services often have a large user base, so even if a small change in recommen Due to immense market value, various recommendation techniques have been proposed. Generally, these approaches can be classiﬁed into neighborhoodbased methods and model-based methods. Typical neighborhood-based approaches includes user-based methods [1, 4] and item-based methods [3, 11, 25]. The state-of-the-art model-based methods include restricted Boltzmann machines [24], SVD++ [8, 9], Probabilistic Matrix Factorization (PMF) [22], and multi-domain collaborative ﬁltering [32], graphical models [7], pair-wise tensor factorization [21], and matrix factorization with social regularization [14], etc. However, in real-world rating systems, users’ ratings carry twofold information. Firstly the rating value indicates a user’s preference on a particular item as well as an item’s inherent features. The scores that a user assigns to diﬀerent items convey information on what the user likes and what the user dislikes. The rating values that an item received from diﬀerent users also carry information on intrinsic properties of the item. Second, the ratings also reveal users’ response patterns, i.e., some items are rated while others not. This information can be utilized to improve the model performance. However, previously proposed methods usually assume that all the users would rate all the inspected items, or more generally, randomly select inspected items to rate. These methods ﬁt the users’ ratings directly and ignore the key factor, users’ response patterns. The ignorance will degrade the model performance. In this paper, we explore previously ignored response information to further boost recommender system’s quality. Practically, the assumption of all inspection or randomly rate is not true in real-world rating systems. Users are unlikely to rate all the inspected items or randomly select the inspected items to rate. Shown in Figure 1(a) is the rating value distribution of the items that users choose to rate, while Figure 1(b) shows the distribution of ratings for randomly selected songs
Investigation of activation cross-section data of proton induced nuclear reactions on rhenium<|sep|>To meet requirements for diﬀerent practical applications we started to establish an experimental activation database some years ago by performing new experiments and a systematical survey of existing data of proton induced cross-sections up to 100 MeV and deuteron induced cross-sections up to 50 MeV (T´ark´anyi, 2011; T´ark´anyi et al., 2011). The proton activation data for rhenium (Re) are relevant for accelerator and target technology (rhenium-tungsten alloy target in Fermilab, Austron, etc.); for medical radioisotope production; for producing radioactive ion beams (RIB); for controlled fusion experiments and reactors (ITER, DEMO, etc. ); for space applications; for thin layer activation (TLA); etc. Rhenium is a very heavy (atomic mass: 186.207 g/mol) and dense metal (ρ = 21.02 g/cm3), with high melting point (3180.0 oC) and it is resistant to heat, wear and corrosion. Presently it is mostly used as an additive in super-alloys for aviation technology. The technical and medical applications are discussed in more detail in recently submitted work on activation data of deuteron induced reactions on rhenium (Ditri et al., 2012). Earlier literature contains only few experimental data on proton activation of rhenium. Armini and Bunker investigated the long-lived isotope production cross-sections from proton bombardment of rhenium, from 15 to 160 MeV, 185Os, 183Re(cum), 184gRe (Armini and Bunker, 1975). Dmitriev measured the thick target yield data for production of 185Os at 22 MeV in (Dmitriev and Molin, 1981) and (Dmitriev, 1983), Ignatyuk (Ignatyuk et al., 1984) and Okolovich (Okolovich et al., 1974) investigated the properties of rhenium in the frame of a study on ﬁssibility of sub-actinide nuclei by protons . Results for estimation of production cross-sections on rhenium by protons through model calculations were produced by ALICE-IPPE code (Dityuk et al., 1998) in the MENDL-2p (Shubin et al., 1998) database, with TALYS code (Koning et al., 2007) in TENDL-2011 (Koning and Rochman, 2011) library and recently Maiti (Maiti, 2011) published calculations for diﬀerent light ion induced reactions for production of rhenium isotopes.
An organ deformation model using Bayesian inference to combine population and patient-specific data<|sep|>In radiotherapy (RT), the dose is carefully shaped to the patient anatomy as seen in the CT acquired before start of treatment (plan CT), to achieve a good compromise between disease control and risk of inducing complications. Since the variability of the organ positions and deformations is unknown before start of treatment, diﬀerent measures have been adopted to safeguard against motion uncertainties through planning margins (Stroom et al., 1999; van Herk et al., 2000), robust optimization (Unkelbach et al., 2018) and/or treatment plan adaptation (Yan et al., 1997). A statistical model for the deformation of organs of individual patients using principal component analysis (PCA) of the organ’s surface shape vectors was ﬁrst proposed by S¨ohn et al. (2005). The main drawback of the patient-speciﬁc model is that the number of data samples (in the form of organ contours derived from 3D images) per patient is often low, which limits the robustness of the motion estimates (Th¨ornqvist et al., 2013b). Budiarto et al. (2011) proposed a population based statistical model, under the assumption that, although the size, shape and position of organs diﬀer greatly between patients, the patterns of deformation are generally the same. The advantage is that an estimate of a patient’s deformation patterns exists even when only a single observation is available. When applied to prostate target deformation, they showed that about 50% of the variation could be explained by 15 population deformation modes (i.e. principal components). Subsequent uses of the population model include Bondar et al. (2014), who used it to create margins for rectal cancer patients, Rios et al. (2017), who modeled bladder deformation for prostate cancer RT, Szeto et al. (2017) who modeled daily variations in the thorax, and MagallonBaro et al. (2019), who modeled deformation in the stomach, duodenum and bowel for pancreatic cancer RT. A weakness of the population model is its inability to model patient-speciﬁc deformation patterns, even when multiple scans are available for the patient in question. The aim of the current work is to combine the strengths of the population and patient-speciﬁc models by introducing Bayesian models that take in to account both the population deformation patterns (in terms of a prior distribution) and patient-speciﬁc measurements, forming an individualized posterior distribution. Bayesian models have previously been applied to the problem rigid shifts of the patient, termed setup errors (Lam et al., 2005; Herschtal et al., 2012). In this paper, we introduce two Bayesian models, which diﬀer in their choice of priors. The choice of model to use will be a trade-oﬀ between accuracy and simplicity. We derive necessary algorithms to eﬃciently calculate the approximate posterior distributions in high dimensions. We apply the introduced models to a realistic example with complex motion, in terms of the rectal wall of prostate cancer patients. We use the models to estimate coverage probability matrices (CPMs), i.e. 3D-arrays of voxels where the value in each voxel is the probability that the voxel will be covered by the rectal wall at any given time. We compare the accuracy of CPMs estimated using the two Bayesian methods, the patient-speciﬁc model by S¨ohn et al. (2005) and the population model by Budiarto et al. (2011). In addition to the presentation of new models, this is to our knowledge the ﬁrst comparison between these two previous models, as well as the ﬁrst time such an organ deformation model has been applied to the rectum.
Inducing the Lovelock action<|sep|>The natural generalization of the Einstein-Hilbert action to a spacetime of dimension greater than four is provided by the Lovelock action [2, 3]; see e.g. [4] for a review. It consists of the sum of all dimensionally continued Euler densities, each term coming with its own coupling constant. The Lovelock action gives rise to a symmetric conserved ﬁeld equation which contains derivatives of the metric no higher than second order and is quasilinear in these second derivatives. This leads to a well-deﬁned classical Cauchy problem [5, 6], and to unitarity at the quantum level, unlike generic higher derivative actions [7, 8]. Unitarity is also the reason why one expects the Lovelock action to make its appearance in the low-energy limit of string theory [9, 10]. More recently, numerous papers have appeared in which this same action appears in connection with brane scenarios in string theory, see e.g. [4, 11]. This partly motivated our examination of the ultraviolet behavior of the Lovelock action. It is well known that a perturbative treatment of Einstein gravity gives rise to a one loop ﬁnite S-matrix in four dimensions. The background ﬁeld method and dimensional regularization predict one loop divergences of curvature-squared form [12]. Although these do not have the functional form of the classical action, the vacuum ﬁeld equations combined with the Gauss-Bonnet identity would imply that such divergences do not survive. However, this reasoning was criticized in a little known paper by Capper and Kimber [1]. They pointed out that dimensional regularization requires one to work consistently in d dimensions and, although the ﬁeld equation can trivially be continued, is ill deﬁned. This motivated Capper and Kimber to add a Riemann-squared term to the classical action. They then demonstrated that this extra term has no eﬀect on tree-level graviton-graviton scattering. This was shown to be not due to explicit factors of d − 4, as none appear, but rather caused by the imposition of on shell conditions and setting d equal to four for all external legs. Although no one loop calculation was performed, an appeal to consistency with other regularization schemes led them to conclude that the one loop S-matrix of standard gravity is ﬁnite after all. The re-examination in the present paper answers the question of Capper and Kimber for the divergent parts of the eﬀective action at one loop level. No changes are found, so we conﬁrm what one would naively have expected from adding such a d = 4 topological term. However, in the process we discover that from the d-dimensional point of view advocated by Capper and Kimber, adding the Gauss-Bonnet term to the classical action next requires the d = 6 Euler density as a counter term. Hence, it would seem that the renormalization process induces the full Lovelock action. Besides the well known general diﬃculty of any calculation in quantum gravity, a further reason why the problem was not yet studied beyond tree level is that one is then faced with a non-minimal wave operator. If the leading part of a wave operator equals the Laplacian, one speaks of a minimal operator. In gauge theories this form can usually be arranged by a judicious gauge choice. Minimal wave operators allow the use of the powerful Schwinger-DeWitt method and convenient background ﬁeld algorithms are then available. We will show that the addition of the Gauss-Bonnet term inevitably leads to a non-minimal wave operator for the graviton. An elegant extension of the SchwingerDeWitt method to the non-minimal case was given in [13] but we did not rely on this work. Since the oﬀending term in the wave operator turns out to be of ﬁrst order in the curvature, we may treat it perturbatively and thus return to the minimal setting. Using the fully covariant Schwinger-DeWitt method, we reproduce and extend to curved space an earlier ﬂat space algorithm [14]. There, ’t Hooft’s background ﬁeld algorithm [15] was generalized to second order in the non-minimal part of the wave operator (see also the recent systematic work of [16, 17]). In related work, Berredo-Peixoto and Shapiro recently demonstrated that no new one loop divergences are generated upon adding the Gauss-Bonnet term to either conformal [18] or general [19] higher derivative gravity. Such theories are known to be renormalizable, though not unitary. The authors of [18, 19] employed dimensional regularization and the Schwinger-DeWitt method for their involved calculations. Interestingly, the Gauss-Bonnet term was shown to actually aﬀect the d-dimensional renormalization group equations and new non-trivial ﬁxed points were found. However, these papers do not answer the original issue of [1]: Although the classical action in [19] includes EinsteinHilbert and cosmological terms, a higher derivative gauge ﬁxing term was chosen which does not allow one to regain the special case of two-derivative gravity. Hence, the analysis in [19] excludes the case of Lovelock gravity we are interested in.
Entanglement transfer via Chiral Quantum Walk on a Triangular Chain<|sep|>Fast and accurate transmission of quantum states through communication or computation networks is a critical objective for quantum technologies [1, 2, 3]. Proposed schemes to achieve this goal consider engineered couplings between the network sites [4, 5, 6, 7, 8, 9, 10, 11, 12, 13], external ﬁelds [14, 15, 16, 17], weak measurements [18, 19], or transport in noisy environments of biological or synthetic systems [20, 21, 22, 23, 24]. Such methods are challenging to implement in practice for quantum entanglement transfer, due to quantum decoherence and disorder [25, 26]. Pretty-good State Transfer (PGST) can be achieved on dual-spin chains [27], spin chains with weakly coupled endpoints [7, 28, 29, 30, 31], and projective measurements [32] with Quantum Walk (QW) schemes. Continuous time quantum walk (CTQW) is a paradigmatic model of quantum transport [33, 34]. Both discrete- [35, 36] and continuous-time [37, 38, 39, 40] quantum walks have been discussed for PGST. CTQW can be made one way by taking complex-valued couplings, which is called a chiral quantum walk (CQW) [41, 42, 43]. Chirality emerges due to the breaking of time-reversal symmetry (TRS) [44], and it provides a signiﬁcant boost to transport speed [41]. High-dimensional entanglement (entanglement in high-dimensional degrees of freedom, such as spatial path modes) is advantageous in quantum communication [45] and quantum superdense coding [46, 47, 48]. The perfect state transfer (PST) in spin chains paves the way for the creation of required entangled states and logic gate structures for quantum computation and quantum information [49, 50]. High-dimensional entangled states can be produced by repeatedly generating the entanglement in a low-dimensional system and transferring these to a higher dimensional one [51]. Our ﬁrst goal is to explore if CQW can be used to transmit two-dimensional quantum entangled states; what are the possible advantages it may offer? In addition, we ask if and to which extent CTQW can be used in place of CQW with the same chiral properties. CTQW can be easier to implement than CQW. For that aim, we identify the underlying physics of the chiral nature of QW in terms of quantum path interference, which can be controlled either via the phase in the initial state for CTQW or via the phase in complex hopping coefﬁcients in CQW. We speciﬁcally consider CQW on a linear spin chain of equilateral triangles, as shown in Fig. 1, which is the simplest graph that allows for so-called probability time symmetry (PTS) breaking [42]. A walker can transfer from one node to any other neighboring site on the triangular plaquette by passing through either one or two edges. We consider a uniform complex coupling between the nearestneighbor sites. Due to the path length difference between the odd and even number of edges traveled, and phases of the complex couplings, interference can enhance the transfer rate. Path interference in the context of quantum walk means that the relative phase between different trajectories the particle can traverse from one site to another one can give constructive or destructive interference effects in the site-to-site probability transfer. By using special graph topologies, complex-valued site-to-site couplings, or speciﬁc initial states, one can use path interference to break the PTS. CTQW can only utilize the latter, initial states with speciﬁc phases to exploit quantum interference while CQW can use both the freedom to choose the initial state and complex hopping coefﬁcients to break PTS. The spatial entanglement can be deﬁned in the site basis for quantum walks [52, 53]. We assume a particle (we call a spin excitation as a particle) in a Bell-like spatially entangled state of two sites injected into the chain from the left. The quality of the transfer is examined by calculating the density matrix, concurrence [54], ﬁdelity, and Bures distances explicitly [55, 56, 57]. We have also numerically conﬁrmed that the entanglement state transfer time linearly scales with the chain size [58, 59, 60]. The triangular chain lattice can be realized in superconducting circuits [61, 62], trapped ions [63], NMR systems [42], photonic and spin waveguides [64], and in optical lattices [65]. In the case of optical lattices, complex edge weights could be introduced with the help of artiﬁcial gauge ﬁelds [66, 67], nitrogenvacancy centers in diamonds [68] or with plasmonic non-Hermitian coupled waveguides [69]. This paper is organized as follows. We introduce the CQW on a triangular chain model by presenting the adjacency matrix and present the associated Hamiltonian model with complex hopping rates in Sec. 2. Our results are given in Sec. 3 in ﬁve subsections. PTS breaking and entanglement transfer in CTQW and CQW on a triangular chain are discussed in Sec. 3.1 and Sec. 3.2, respectively. We conclude in Sec. 4.
Infrared-suppressed QCD coupling and the hadronic contribution to muon g-2<|sep|>Perturbative QCD (pQCD) cannot be applied to the studies of phenomenology in the intermediate (|Q2| ∼ 1 GeV2) and infrared (|Q2| < 1 GeV2) regimes of the Q2-complex plane,‡ because the pQCD running coupling a(Q2) ≡ αs(Q2)/π has singularities within or close to such regimes. The problematic singularities appear in the spacelike IRregime, 0 < Q2 < Λ2 Lan. where Λ2 Lan. ∼ 0.1-1 GeV2 is the branching scale of the Landau cut. These singularities lead, above all, to practical diﬃculties of evaluation of a(Q2) and of QCD processes at such Q2. A way out of this problem consists in regaining a correct analytic behaviour by “analytizing” the running coupling, i.e., by replacing the pQCD coupling a(Q2) by another coupling A(Q2) which has the aspired analyticity properties and could, at least in principle, be used for (quasi)perturbative evaluation of low-energy observables. In Ref. [1] we have constructed such an improved coupling and demonstrated its compatibility with intermediate-energy observables. Here we somewhat reﬁne this construction and apply it in addition to quantities determined by even lower energies.
Automatic Active-Region Identification and Azimuth Disambiguation of the SOLIS/VSM Full-Disk Vector Magnetograms<|sep|>SOLIS is a state-of-the art ground-based facility dedicated to the study of the Sun and its magnetic atmosphere for decades to come. It consists of three instruments - the vector spectromagnetograph (VSM), the Integrated Sunlight Spectrometer (ISS), and the Full-Disk Patrol (FDP). An overview on each instrument can be found in Keller, Harvey, & Giampapa (2003). The VSM, in particular, operates from Kitt Peak since May 2004. Among other daily measurements, the VSM performs complete Stokes polarimetry at the Fe I 630.2 nm photospheric spectral line. An inversion of the Stokes images provides full-disk vector magnetograms of the solar photosphere (see Henney, Keller, & Harvey, 2006, for details). The VSM data are the ﬁrst spectrographic full-disk vector magnetograms ever obtained. Partial-disk vector magnetography is performed by a handful of ground-based instruments and, recently, by the spectro-polarimeter of the Solar Optical Telescope (SOT) on board the Hinode satellite (Lites, Elmore,
Inferring population history with DIYABC: a user-friendly approach to Approximate Bayesian Computation<|sep|>Until now, most literature and software about inference in population genetics concern simple standard evolutionary scenarios: a single population (Grifﬁths and Tavar´e, 1994; Stephens and Donnelly, 2000; Beaumont, 1999), two populations exchanging genes (Hey and Nielsen, 2004; De Iorio and Grifﬁths, 2004) or not (Hickerson et al., 2007) or three populations in the classic admixture scheme (Wang, 2003; Excofﬁer et al., 2005). The main exception to our knowledge is the computer program BATWING (Wilson et al., 2003) which considers a whole family of scenarios in which an ancestral population splits into as many subpopulations as needed. However, in practice, population geneticists collect and analyse samples which rarely correspond to one of these standard scenarios. If they want to apply methods developed in the literature and for which computer programs are available, they have to select subsets of samples (to ﬁt these standard situations), at the price of lowering the power of the analysis. The other solution is to develop their own software, which requires speciﬁc skills or the right collaborators. Rare examples of inference in non standard scenarios can be found in O’Ryan et al. (1998) including 3 populations and two successive divergences, or Estoup et al. (2004) (10 populations that sequentially diverged with initial bottlenecks and exchanging migrants with neighbouring populations). Inference in complex evolutionary scenarios can be performed in various ways, but all are based on the genealogical tree of sampled genes and coalescence theory. A ﬁrst approach used in programs such as IM (Hey and Nielsen, 2004) or BATWING consists of starting from a gene genealogy compatible with the observed data and exploring the parameter and genealogy space through MCMC algorithms. One difﬁculty with this approach is to be sure that the MCMC has converged, because of the huge dimension of the parameter space. With a complex scenario, the difﬁculty is increased. Also, although not impossible, it seems quite challenging to write a program that would deal with very different scenarios. A second approach pioneered by Beaumont (2003) consists in combining MCMC exploration of the scenario parameter space with an Importance Sampling (IS) based estimation of the likelihood. The strength of this approach is that the low number of parameters ensures a (relatively) fast convergence of the MCMC. Its weakness is that the likelihood is only approximated through IS, sometimes resulting in poor acceptance rates. When dealing with complex situations, the two previous approaches raise difﬁculties which mainly stem in the computation of the likelihood. Consequently, a line of research including the works of Tavar´e et al. (1997), Weiss and von Haeseler (1998), Pritchard et al. (1999) and Marjoram et al. (2003) developed a new approach termed Approximate Bayesian Computation (or ABC) by Beaumont et al. (2002). In this approach, the likelihood criterion is replaced by a similarity criterion between simulated and observed data sets, similarity usually measured by a distance between summary statistics computed on both data sets. Among examples of inference in complex scenarios given above, all but one (the simplest) have used this approach, showing that it can indeed solve complex problems. The ABC approach presents two additional features that can be of interest for experimental biologist. One characteristic, already noted by Excofﬁer et al. (2005), is the possibility to assess the bias and precision of estimates for simulated data sets produced with known values of parameters with little extra computational cost. To get the same information with likelihood-based methods would require a huge amount of additional computation whereas, with ABC, the largest proportion of computation used for estimating parameters can be recycled in a bias/precision analysis. The second feature is the simple way by which the posterior probability of different scenarios applied to the same data set can be estimated (e.g. Miller et al., 2005; Pascual et al., 2007). In its current state, the ABC approach remains inaccessible to most biologists because there is not yet a simple software solution. Therefore, we developed the program DIY ABC that performs ABC analyses on complex scenarios, i.e. which include any number of populations and samples (samples possibly taken at different times), with populations related by divergence and/or admixture events and possibly experiencing changes of population size. The current version is restricted to unlinked microsatellite data. In this article, we describe the rationale for some methods involved in the program. Then we give the main features of DIY ABC and we provide two complete example analyses performed with this program to illustrate its possibilities.
IRAC Photometric Analysis and the Mid-IR Photometric Properties of Lyman Break Galaxies<|sep|>Observation and study of high-redshift galaxies is essential to constrain the history of galaxy evolution and give us a systematic and quantitative picture of galaxies in the early universe, an epoch of rigorous star and galaxy formation. Large samples of high-z galaxies that have recently become available, play a key role to that direction and have revealed a zoo of diﬀerent galaxy populations at z∼3. There are various techniques for detecting high-z galaxies involving observations in wavelengths that span from optical to far-IR. Among these, the three most eﬃcient are 1)sub-mm blank ﬁeld observations, using the sub-mm Common User Bolometer Array (SCUBA) on the James Clerk Maxwell Telescope (JCMT) (e.g., Hughes et al. 1998) or the Max Plank Millimeter Bolometer array (MAMBO, e.g., Bertoldi et al. 2000), taking advantage of the strong negative K-correction eﬀect and revealing the population of the sub-mm galaxies at z>2 (Chapman et al. 2000, Ivison et al. 2002, Smail et al. 2002) and 2)the U-band-dropout technique (Steidel & Hamilton 1993, Steidel et al. 1999, 2003 Franx et al. 2003, Daddi et al. 2004), sensitive to the presence of the 921˚A break, designed to select z≈3 galaxies and revealing the population of the Lyman Break Galaxies (LBGs), 3) the Near-IR colour (J−K > 2.3 (Franx et al. 2003) and BzK ((z−K)AB − (B−z)AB> −0.2) (Daddi et al. 2004) selection for old and dusty galaxies at 2<z<3 (Distant Red Galaxies (DRGs) and BzK galaxies). With recent deep optical and IR observations of these 3 types of galaxies, there has been considerable progress in understanding the relation between LBGs, SMGs, DRGs and BzK galaxies. Chapman et al. 2005 conﬁrmed that some of the SCUBA galaxies have rest-frame-UV colours typical of the LBGs. Using IRAC and MIPS observations, Huang et al. 2006 has shown that LBGs detected in MIPS 24µm band, (Infrared Luminous LBGs) and cold SCUBA sources share similar [8.0]−[24] colours, while Rigopoulou et al. 2006 suggest that ILLBGs and SCUBA galaxies tend to have similar stellar masses and dust amount. A possible scenario is one in which sub-mm galaxies and LBGs form a continuum of objects with SMGs representing the reddest dustier and more intensively star-forming LBGs, but further investigation is required to establish a more secure link between these two populations. LBGs constitute at the moment the largest and most well studied galaxy population at z∼3 (Steidel et al. 2003). Based on observations of the UV continuum emission, the predicted mean star formation rate of the LBGs is 20–
Valence quark and meson cloud contributions for the gamma* Lambda -> Lambda* and gamma* Sigma0 -> Lambda* reactions<|sep|>With the development of the modern accelerators the study of the meson and light baryon structure has been becoming one of the most exciting topics in physics. Although the underlying theory of strong interaction, quantum chromodynamincs (QCD), is known for a long period, its complexity in low energy region forces us to use some eﬀective theories (aside from lattice QCD), either based on the quark and gluon degrees of freedom, or some eﬀective interactions between the mesons and baryons. Among the various possible meson-baryon reactions, the reactions that involve strangeness are particularly interesting, due to the accessibility of the modern accelerators to strange particles such as kaons, K, antikaons, ¯K, and hyperons, Σ and Λ. In this study we focus on the electromagnetic excitations of Λ hyperon ground state. The Λ ground state, Λ(1116) is JP = 1 2 + and belongs to the spin 1/2 octet baryon multiplet, in which also the nucleons belong. The lowest mass of the Λ excited state (Λ∗) reported by the particle data group [1] is Λ(1405), a JP = 1 2 − state. The Λ(1405) state has collected a lot of interests over the years for the reasons following: (i) it has been suggested as a dynamically generated state (molecular-like state) composed largely of the πΣ and ¯KN states [2– 5]; (ii) it is diﬃcult to classify in terms of naive quark models based on SU(6) symmetry. In the representation of spin-ﬂavor SU(6) symmetry the Λ(1405) state can be a mixture of three distinct 3-quark states including the Λ-singlet state [6–8]. However, its mass is diﬃcult to predict in Karl-Isgur model [8], as well as in cloudy bag model (CBM) [9]. In CBM the Λ(1405) was interpreted primarily as a ¯KN bound state [9]. Thus, there is a strong indication that the Λ(1405) is a dynamically generated meson-baryon molecular-like state with a single or a double pole structure [9–19]. In particular, it was demonstrated that the Λ(1405) is composed substantially of the meson-baryon components within the chiral unitary model [13]. Nevertheless, there are some works that support the Λ(1405) as a 3-quark state [20–22]. Therefore, to study the γ∗Λ → Λ∗ reaction is very interesting also by the reasons following. In one aspect this reaction has a possible analogy with the γ∗N → N ∗(1535) reaction. Because γ∗Λ → Λ∗ is a transition between a JP = 1 2 − states, we have a possibility to interpret the Λ(1405) as a p-state excitation of one quark in the ground state Λ(1116), analogous to N ∗(1535), a p-state excitation of the nucleon [23]. However, the Λ(1405) has considerably lower mass than N ∗(1535). Furthermore, it has a larger mass diﬀerence with the nearest d-state partner Λ(1520) compared to the case of N ∗(1535) and N ∗(1520). The mass order is even reversed for the Λ(1405) case. Because of the reasons discussed above, it is very diﬃcult to interpret naively Λ(1405) as a simple p-state excitation of Λ(1116). Searching for the next higher mass excited state of Λ(1116) with JP = 1 2 −, one ﬁnds Λ(1670), which can be an analogous with S11 excitation of the nucleon, N ∗(1535). Since Σ0 is the neutral Σ ground state (JP = 1 2 +) which belongs to the spin 1/2 octet baryon multiplet, and the γ∗Σ0 → Λ∗ reaction is similar to the γ∗Λ → Λ∗ reaction, we also focus on the γ∗Σ0 → Λ∗ reaction in this study. Because the Λ(1116) and Σ0(1193) are similar in masses, the two reactions diﬀer mainly in the initial state quark conﬁgurations. As for the other interesting aspect, we note that the Λ(1670) resonance can also be described as a dynamically generated meson-baryon state [18, 19], and the γ∗Y → Λ∗ transition form factors for Y = Λ, Σ0, were calculated in chiral unitary model [17]. In the previous works, a valence quark model was applied to study the γ∗N → N ∗(1535) reaction, and the corresponding transition form factors and helicity amplitudes were studied [23, 24]. The reaction was also studied in a coupled-channels chiral dynamics (chiral unitary model) [25]. In the chiral unitary model the contributions for the transition form factors come entirely from the meson-baryon states (meson cloud eﬀect). For the γ∗N → N ∗(1535) reaction the transition form factors F ∗ 1 (Dirac-type) and F ∗ 2 (Pauli-type) can be expressed in terms of the transverse (A1/2) and longitudinal (S1/2) helicity amplitudes [23, 26]. In Ref. [23], it was found that the F ∗ 1 can be explained very well just taking into account the valence quark eﬀect. By contrast, the meson cloud seems to play a very important role for the F ∗ 2 , in particular in the low Q2 region [25]. Then, such diﬀerent roles between the valence quark and meson cloud eﬀects may be reﬂected in the experimentally extracted helicity amplitudes S1/2 and A1/2. This possibility was indeed demonstrated in Ref. [24]. We will brieﬂy review also these results. Therefore, one of our main motivations of this study is to investigate whether or not the diﬀerent roles of the valence quark and meson cloud eﬀects observed for the γ∗N → N ∗(1535) reaction, can also be observed in the γ∗Y → Λ∗ reactions with Y = Λ and Σ0. In particular, we focus on the structure of Λ(1670) in this study. Assuming that Λ(1670) is a radial p-state excitation of Λ(1116), we estimate the valence quark contributions for the γ∗Y → Λ∗ transition form factors as well as the helicity amplitudes. For this purpose, we use the covariant spectator quark model [23, 27–29], which was successfully applied to the study of the γ∗N → N ∗(1535) reaction. The results of the covariant spectator quark model for the γ∗Y → Λ∗ reaction are also compared with those obtained with the chiral unitary model [17], where the Λ∗ is generated as a meson-baryon molecular-like state such as the N ¯K, Λη and ΞK states. Then, one of the interests is the structure of the Λ(1670) state, namely, how it can be interpreted, either it is predominantly a meson-baryon molecular-like state, or dominated by the 3-valence-quark state. Furthermore, we also show that the γ∗Σ0 → Λ∗ transition form factors depend crucially on the combination of the two unknown real phases (signs), a phase between the Λ and Λ∗ three-quark wave functions (to be denoted by ηΛ∗), and a phase between the Λ and Σ0 wave functions (to be denoted by ηΛΣ0). This article is organized as follows. In Sec. II we deﬁne the γ∗Y → Λ∗ (Y = Λ, Σ0) transition form factors, and their relations with the helicity amplitudes. In Sec. III we present the covariant spectator quark model and estimate the valence quark contributions for γ∗Y → Λ∗ (Y = Λ, Σ0). We discuss in Sec. IV the Λ(1670) state based on the chiral unitary model, and estimate the contributions from the meson-baryon states in the γ∗Y → Λ∗ (Y = Λ, Σ0) reactions. In Sec. V we present the results from
Spectra of absolute instruments from the WKB approximation<|sep|>Absolute optical instrument (AI) is a device that provides a perfectly sharp image of all points in some spatial region [1]. The simplest AI is a plane mirror that gives a virtual image of a whole half-space. Another, beautiful example of an AI is Maxwell’s ﬁsh eye, discovered by J. C. Maxwell in 1854 [2], that images sharply the whole space, and all rays form circles. In recent years absolute instruments attracted an increased interest which has led to proposing new devices of various types, e.g. AIs that perform imaging of the whole space, of optically homogeneous regions [3, 4], or provide magniﬁed images [5]. A general method has been proposed in [4] for designing spherically symmetric AIs. This research was based on geometrical optics. Recently AIs attracted attention also from the point of view of wave optics. It was shown both theoretically [6] and experimentally [7, 8] that these devices can provide subwavelength resolution, although this claim has raised controversy [9, 10, 11] and it is still not clear to what extent such a super-resolution can be used practically [12]. A diﬀerent question was addressed in [13], namely what are the general characteristics of the spectrum of eigenfrequencies of AIs. It was shown that the spectrum consists of tight groups of levels with almost equidistant spacing between them. This ﬁnding was based on an analysis of a light pulse propagating in the AI and on the assumption that a short pulse emitted at some point can be absorbed at the image point during a short time as well. Numerically calculated spectra of various AI conﬁrmed this theoretical result very well. In this paper, we investigate the spectra of absolute instruments by a completely diﬀerent method. Employing the WKB approximation with the Langer modiﬁcation and using one of the general properties of AIs, we conﬁrm in a diﬀerent way the previously known results about their spectrum [13]. Our method has two advantages compared to the previous one: it enables to calculate not just the spacing of the level groups but also their oﬀset, and it allows to treat the situations where a mirror is used in the device. We verify our results by comparing the calculated spectra with numerical values for several examples of AIs. The paper is organised as follows. In Sec. 2 we recall absolute instruments and discuss some of their properties. In Sec. 3 we employ the WKB method for calculating the spectra of radially symmetric media and in Sec. 4 we illustrate the results on particular examples of AIs. In Sec. 5 we analyse the situation in AIs that contain mirrors, and we conclude in Sec. 6.
Network as a computer: ranking paths to find flows<|sep|>Initially, Web search was developed as an instance of information retrieval, optimized for a particularly large distributed database. With the advent of online advertising, Web search got enhanced by a broad range of information supply techniques where the search results are expanded by additional data, extrapolated from user’s interests, and from search engine’s stock of information. From the simple idea to match and coordinate the push and the pull of information on the Web as a new computational platform [18] sprang up a new generation of web businesses and social networks. Similar patterns of information processing are found in many other evolutionary systems, from gene regulation, protein interaction and neural nets, through the various networks of computers and devices, to the complex social and market structures [15]. This paper explores some simple mathematical consequences of the observation that the Web, and similar networks, are much more than mere information repositories: besides storing, and retrieving, and supplying information, they also generate, and process information. We pursue the idea that the Web can be modeled as a computer, rather than a database; or more precisely, as a vast multi-party computation [6], akin to a market place, where masses of selﬁsh agents jointly evaluate and generate public information, driven by their private utilities. While this view raises interesting new problems across the whole gamut of Computer Science, the most eﬀective solutions, so far, of the problem of semantical interactions with the Web computations were obtained by rediscovering and adopting the ranking methods, deeply rooted in the sociometric tradition [11,10], and adapting them for use on very large indices, leading to the whole new paradigm of search [19,12,13]. Implicitly, the idea of the Web as a computer is tacitly present already in this paradigm, in the sense that the search rankings are extracted from the link structure, and other intrinsic information, generated on the Web itself, rather than stored in it. Outline of the paper. In section 2 we introduce the basic network model, and describe a ﬁrst attempt to extract information about the ﬂows through a network from the available static data about it. In sections 3 and 4, we describe the structure which allows us to lift the notion of rank, described in section 5, to path networks in section 6. Ranking paths allows us to extract a random variable, called attraction bias, which allows measuring the mutual information of the distributions of the inputs and the outputs of the network computation, which can be viewed as an indicator of non-local information processing that takes place in the given network. In the ﬁnal section, we describe how the obtained data can be used to detect semantical structures in a network. The experimental work necessary to test the practical eﬀectiveness of the approach is left for future work.
Artificial Intelligence Enabled Traffic Monitoring System<|sep|>engineering. Till date, most traffic monitoring centers rely on human operators to track the nature of  traffic flows and oversee any incident happening on the roads. The processes involved in manual  traffic condition monitoring can be challenging and time-consuming. As humans are prone to  inaccuracies and subject to fatigue, the results often involve certain discrepancies. It is therefore, in  best interests to develop automated traffic monitoring tools to diminishing the work load of human  operators and increase the efficiency of output. Hence, it is not surprising that automatic traffic  monitoring systems have been one of the most important research endeavors in intelligent  transportation systems. It is worthwhile to note that most present-day traffic monitoring activity  happens at the Traffic Management Centers (TMCs) through vision-based camera systems. However,  most existing vision-based systems are monitored by humans which makes it difficult to accurately  keep track of congestion, detect stationary vehicles whilst concurrently keeping accurate track of the  vehicle count. Therefore, TMCs have been laying efforts on bringing in some levels of automation in  traffic management. Automated traffic surveillance systems using Artificial Intelligence (AI) have the  capability to not only manage traffic well but also monitor and access current situations that can  reduce the number of road accidents. Similarly, an AI enabled system can identify each vehicle and
PatchPerPix for Instance Segmentation<|sep|>The task of instance segmentation has a wide range of applications in natural images as well as microscopy images from the biomedical domain. A prevalent class of instance segmentation methods, namely proposal-based methods based on RCNN [10, 11], has proven successful in cases where instance location and size can be wellapproximated by bounding boxes. However, in many cases, especially in the biomedical domain, this does not hold: Instances may span widely across the image, and hence multiple instances may have very similar, large bounding boxes. To complicate things, instances may be densely clustered, in some cases overlapping, including crossovers. Proposal-free methods are applicable in such cases, where popular choices include metric learning / instance coloring [7, 18, 4, 17], afﬁnity-based methods [8, 30, 20, 9], and learnt watershed [3, 31]. However, respective pixel-wise predictions do not explicitly capture instance shape, nor are they suitable for disentangling overlapping instances. To overcome these limitations, we propose to (1) densely predict representations of the shapes of instance patches, (2) cover the image foreground with the most plausible shape patches, and (3) puzzle together complete instance shapes from these patches by means of partitioning a patch afﬁnity graph. The approach of covering the image by selecting from a redundant set of instance patch predictions allows for naturally handling overlap (including crossovers), as overlapping instance patches can be selected, potentially resulting in pixels covered by multiple instances. Our general idea is closely related to Singling Out Networks [32]. However, they are different in that they rely on a dictionary of known instances, thereby limiting the variability of objects they can handle, and they only consider predicting whole instances and not patches of instances, thereby limiting the size of feasible object categories. Our shape prediction network predicts, for each pixel of the input image, a representation of the local shape of the instance this pixel belongs to, namely a shape patch of the pixel’s instance. The architecture we propose is derived from the U-Net [26], thus allowing for efﬁcient dense prediction. As representations of instance patch shapes, we explore local binary masks, as well as encodings (i.e. compressed versions) of these. The idea of predicting instance shape masks per pixel of an image has been pursued before [5, 6, 15]. However, all these approaches work on the assumption that a shape mask can capture a complete instance shape. Thus they are designed for object categories common to natural images rather than for disentangling clusters of complex shapes that occupy similar bounding boxes, as relevant in the biomedical domain. Predicting shape encodings instead of binary masks is also not new [15]. However, besides only considering complete instance shapes as opposed to our patches of instances, in [15], shape encoding and respective decoder are trained separately, where we show in our work that end-to-end training yields considerable improvement. The variant of our method that predicts local binary masks as shape representations is closely related to methods that employ long-range afﬁnities [16, 30, 20, 9]. In Figure 1: PatchPerPix overview. Given the raw input image (a), a CNN predicts dense patches for each pixel (b, best seen with zoom) which are then used to ﬁnd a consensus for each pair of pixels within the patch size. The patches that best agree with this consensus are selected (shown in red in b) and connected to form a patch afﬁnity graph. (c) Edges of the patch afﬁnity graph are assigned scores derived from the agreement of the merged shape patches with the consensus. The ﬁnal instance segmentation (d) is obtained by signed graph partitioning. Shown in (c,d) is the result of connected component analysis on the positive subgraph, where edges with negative scores are depicted in red. essence, our predicted binary patches can be interpreted as dense afﬁnities in a neighborhood around each pixel. However, in contrast to afﬁnity-based methods, we instead interpret our predictions as patches of instances, from which we puzzle together complete instances. This way, our yielded global instance shapes are assembled from learned shape patches, a property that does not hold for afﬁnity-based methods. Note that in this respect, our method is related to CELIS [23], which learns to agglomerate super-pixels to form instances with plausible shapes, yet their initial pixelwise predictions do not capture object shape. Furthermore, our method is related to Flood Filling Networks [13], an iterative method that learns to expand instances one-by-one. In contrast, our method segments all instances simultaneously in one pass. We show in a quantitative evaluation that our method is the new state of the art on the ISBI 2012 challenge on segmentation of neuronal structures in EM stacks [1], outperforms the previous state of the art [32, 25, 17] on the BBBC010 benchmark dataset of worm images [28] by a large margin, and also outperforms the state of the art [27, 29, 12] on 2d and 3d light microscopy images of densely packed cell nuclei. Last but not least, we demonstrate that our method also applies to the complex tree-like shapes of neurons in 3d light microscopy images. In summary, our contributions are: • A novel method for segmenting instances of complex shapes that spread widely across an image in crowded scenarios, with overlaps and crossovers. • Instance segmentations are assembled from learnt shape pieces. Our method is, to our knowledge, the ﬁrst such method that is not iterative, i.e. we compute all instances in one pass. • Our method deﬁnes the new state of the art on the competitive ISBI 2012 EM segmentation challenge, considerably outperforms the state of the art on the challenging BBBC010 C. elegans dataset, and also deﬁnes the new state of the art on 2d and 3d benchmark data of cell nuclei.
Intrinsic bottom and its impact on heavy new physics at the LHC<|sep|>Several Standard Model (SM) and New Physics (NP) processes at the CERN Large Hadron Collider (LHC) crucially depend on heavy quark parton distribution functions (PDFs); see for instance [1] for key processes involving the bottom quark PDF. In the standard approach, the heavy quark distributions are generated radiatively via the DGLAP evolution equations, starting with a perturbatively calculable boundary condition. However, a purely perturbative, extrinsic, treatment for the heavy quarks might not be adequate; several models indeed postulate a non-perturbative, intrinsic, heavy quark component e.g. light-cone [2, 3] and meson cloud models [4–6]. Along the years, various global ﬁts have been performed to estimate the amount of intrinsic charm (IC) allowed in the nucleon, see [7] for a recent review. Among them, the two most recent analyses [8, 9] set signiﬁcantly diﬀerent limits on the allowed IC contribution highlighting the utility of the techniques discussed in this paper as we can freely adjust the amount of IC/IB contributions without having to regenerate a complete global analysis for each case. In this contribution, we summarize a technique introduced in [10] which can provide IB PDFs for any generic non-IB PDF set. This approach relies on the fact that the intrinsic bottom PDF evolves (to an excellent precision) according to a standalone non-singlet evolution equation. It is then easy to produce a matched set of IB and non-IB PDFs. Note that obtaining information on the IB content of the nucleon from a global ﬁt is doomed because the data entering global analyses of proton PDFs do not constrain the IB PDFs. Sec. 2, demonstrates that to a good approximation the scale-evolution of the intrinsic PDF is governed by a non-singlet evolution equation; it also provide a set of matched IC/IB PDFs. The IB PDFs are then used in Sec. 3 to obtain predictions for parton–parton luminosities relevant at the LHC. Finally, our results and conclusions are summarized in Sec. 4.
Towards the construction of a model to describe the inter-ELM evolution of the pedestal on MAST<|sep|>The reference scenario for ITER [1] is the high conﬁnement mode (or H-mode) [2]. The improved conﬁnement is the result of a narrow transport barrier that forms at the plasma edge. Due to the strong coupling between pedestal and core conﬁnement, which has been observed in many devices [3], the fusion performance of ITER will strongly depend on the achievable pedestal pressure. Therefore, an accurate prediction of the pedestal height is essential for the prediction and optimization of ITER performance. A class of instabilities called Edge Localized Modes or ELMs [4], [5] limits the maximum pressure pedestal height, Pped, that can be achieved for a given pedestal width. Hence, understanding the width of the barrier is very important in determining the maximum pressure that can be achieved. An MHD stability analysis has been used to show that in order for ITER to attain a temperature pedestal of 4keV, which is thought to be required for ITER to attain its fusion power output objective, the pedestal width would have to be 2.5% of the minor radius [6], [7]. Several empirical scalings for the pedestal width (∆ped) have been proposed based on dimensionless parameters such as the normalized poloidal ion Larmor radius (ρ∗ pol) or pressure (βpol), but a strong co-linearity between pedestal values of βpol and ρ∗ pol limits the power of these datasets to discriminate between very diﬀerent scalings: e.g. early data from DIII-D could be ﬁt equally well with either ∆ped = (ρ∗ pol)0.66 or ∆ped = (βpol)0.4 [8]. Experiments on JT-60U using hydrogen and deuterium plasmas found a scaling of the form ∆ped ∝ (ρ∗ pol)0.2(βpol)0.5 [9]. Recent similarity experiments on DIII-D, ASDEX Upgrade and JET also obtain a pedestal width scaling, in normalised poloidal ﬂux, of the form ∆ΨN ∝ β0.5 pol [10]. Previous studies on MAST have shown no evidence for an increase of the width with ρ∗ pol but a clear increase of width with βpol [11]. Fits of the distributions of MAST pedestal width measurements to the form ∆ped = AβB pol yielded B ≈ 0.5. A similar scaling of the pedestal width has also been observed on DIII-D, C-MOD and AUG [12]. The scaling ∆ped ∝ β0.5 pol has been used to provide basic input to the EPED model for pedestal evolution [13]. The EPED model proposes that: drift wave turbulence is suppressed in the pedestal region by sheared ﬂow; kinetic ballooning mode (KBM) turbulence constrains the pedestal to a critical normalized pressure gradient; and an ELM is triggered by the onset of the intermediate n (∼ 10 − 20) peeling-ballooning mode instability. This model provides a prediction for the pressure pedestal width and height just before the ELM crash. EPED was successfully compared to pedestal conditions in DIII-D and a range of other tokamaks [12],[13]. The model has since been extended to exploit the fact that KBMs are predicted to have an onset condition similar to that of ideal inﬁnite-n ballooning modes [14], where n is the toroidal mode number. In the more recent version of EPED, the inﬁnite-n ideal MHD ballooning mode code, BALOO [15], has been used to compute the threshold pressure gradient set by the condition that either 1% of the pressure proﬁle in normalized poloidal ﬂux [14] or 50% of the steep pedestal gradient region [16] is unstable to inﬁnite-n ideal ballooning modes. This technique has found good agreement between the predicted and observed pedestal widths and heights obtained prior to type I ELMs on DIII-D [14] and JET [17]. In this paper, the ideas incorporated in the EPED model have been tested using data from the spherical tokamak MAST. In order to construct a pedestal model for MAST, the evolution of the pedestal proﬁles has been studied as a function of time during the inter-ELM period, and this is described in Section 2. In Section 3 the HELENA code [18] has been used to reconstruct equilibria during the ELM cycle and to determine inﬁnite-n ideal ballooning stability, and the ELITE code [19, 20] has been used to calculate the ﬁnite-n stability boundary. In Section 4 the local gyrokinetic code, GS2 [21], has been used to perform linear microstability analysis in the pedestal region and to test whether the region unstable to inﬁnite-n ballooning modes is related to the region where KBMs are unstable in MAST (i.e. to test whether ideal inﬁnite-n MHD ballooning analysis can be used as a proxy for KBM stability in MAST).
The Consequences of Gamma-ray Burst Jet Opening Angle Evolution on the Inferred Star Formation Rate<|sep|>Understanding the global star formation rate (SFR) density is a key factor in understanding galaxy formation and evolution throughout the history of our Universe; additionally, it provides a cosmic census of the many diverse astronomical objects in our Universe (e.g., see Hopkins & Beacom (2006); Kennicutt & Evans (2012); Krumholz (2014); Madau & Dickinson (2014) and references therein). However, accurately determining the cosmological SFR is diﬃcult for a number of reasons. Many of these issues have to do with the assumptions invoked when trying to connect observations to a physical star formation rate density, as well as accurately accounting for observational selection eﬀects (see, e.g., Hopkins & Beacom (2006); Madau & Dickinson (2014) for a discussion of these issues). Furthermore, observations themselves are limited - classic techniques using ultraviolet and far infrared measurements of galaxies are diﬃcult at high redshifts; to get an accurate measurement of the star formation rate beyond a redshift of 3 or so, multiple techniques must be employed. Because long gamma-ray bursts (lGRBs) are the most luminous explosions in the universe and because of deﬁnitive evidence of their association with massive star progenitors (Galama et al. 1998; Hjorth et al. 2003; Woosley & Bloom 2006; Hjorth & Bloom 2012), they have long been suggested as tools with which to estimate the high redshift star formation rate (Lloyd-Ronning et al. 2002; Jakobsson et al. 2005; Kistler et al. 2008; Yüksel et al. 2008; Kistler et al. 2009; Wanderman & Piran 2010; Robertson & Ellis 2012; Trenti et al. 2013; Lien et al. 2014; Petrosian et al. 2015; Chary et al. 2016; Le & Mehta 2017; Kinugawa et al. 2019; ElíasChávez & Martínez 2020). However, there are a number of issues that make doing so diﬃcult, essentially related to understanding exactly what types of stars and/or fractions of the global stellar population produce GRBs (including accounting for multiple GRB progenitors), and understanding how this relationship may change over cosmic time. In addition, the distribution of the GRB beaming angle plays an important role in relating the GRB rate to the SFR. And - ﬁnally and importantly - observational selection eﬀects in the detection of high redshift GRBs must be taken into account. Recently, Lloyd-Ronning et al. (2019b, 2020) examined a large sample of lGRBs with redshifts (z, in the range 0.1 ≲ z ≲ 5) and found that the estimates of the jet opening angle, θj, appear to be narrower at high redshifts than at low redshifts, with a best-ﬁt
Context-Aware Neural Machine Translation Learns Anaphora Resolution<|sep|>It has long been argued that handling discourse phenomena is important in translation (Mitkov, 1999; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneﬁcial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling speciﬁc phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models, has demonstrated to yield performance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited. In our work we aim to enhance our understanding of the modelling of selected discourse phenomena in NMT. To this end, we construct a simple discourse-aware model, demonstrate that it achieves improvements over the discourse-agnostic baseline on an English-Russian subtitles dataset (Lison et al., 2018) and study which context information is being captured in the model. Speciﬁcally, we start with the Trans former (Vaswani et al., 2017), a state-of-the-art model for context-agnostic NMT, and modify it in such way that it can handle additional context. In our model, a source sentence and a context sentence are ﬁrst encoded independently, and then a single attention layer, in a combination with a gating function, is used to produce a context-aware representation of the source sentence. The information from context can only ﬂow through this attention layer. When compared to simply concatenating input sentences, as proposed by Tiedemann and Scherrer (2017), our architecture appears both more accurate (+0.6 BLEU) and also guarantees that the contextual information cannot bypass the attention layer and hence remain undetected in our analysis. We analyze what types of contextual information are exploited by the translation model. While studying the attention weights, we observe that much of the information captured by the model has to do with pronoun translation. It is not entirely surprising, as we consider translation from a language without grammatical gender (English) to a language with grammatical gender (Russian). For Russian, translated pronouns need to agree in gender with their antecedents. Moreover, since in Russian verbs agree with subjects in gender and adjectives also agree in gender with pronouns in certain frequent constructions, mistakes in translating pronouns have a major effect on the words in the produced sentences. Consequently, the standard cross-entropy training objective sufﬁciently rewards the model for improving pronoun translation and extracting relevant information from the context. We use automatic co-reference systems and human annotation to isolate anaphoric cases. We observe even more substantial improvements in performance on these subsets. By comparing attention distributions induced by our model against co-reference links, we conclude that the model implicitly captures coreference phenomena, even without having any kind of specialized features which could help it in this subtask. These observations also suggest potential directions for future work. For example, effective co-reference systems go beyond relying simply on embeddings of contexts. One option would be to integrate ‘global’ features summarizing properties of groups of mentions predicted as linked in a document (Wiseman et al., 2016), or to use latent relations to trace en • we introduce a context-aware neural model, which is effective and has a sufﬁciently simple and interpretable interface between the context and the rest of the translation model; • we analyze the ﬂow of information from the context and identify pronoun translation as the key phenomenon captured by the model;
Extending the baseline: Spitzer Mid-Infrared Photometry of Globular Cluster Systems in the Centaurus A and Sombrero Galaxies<|sep|>Massive star cluster formation takes place during major star bursts. The age substructure of a particular star cluster system therefore provides a historical record of signiﬁcant star formation events in its host galaxy. Globular clusters (GCs) are a class of massive star clusters with a mean mass of a few 105M⊙ and are notable for their compactness. Observational evidences generally suggests the bulk of GC formation Systems of GCs are known to host a blue and red GC subpopulations in terms of their photometric colour (e.g. Peng et al. 2006; Strader et al. 2006). This has been interpreted as evidence for a bimodal metallicity distribution, hence most GC systems contain both a metal-poor (or blue) and metal-rich (red) GC subpopulation. Given the close proximity of the GC formation epoch to the time when the universe was free of metals, the existence of two GC metallicity subpopulations likely implies that two distinct phases of GC formation took place at diﬀerent epochs. Obtaining
A Survey on Federated Learning and its Applications for Accelerating Industrial Internet of Things<|sep|>intelligence without compromising data privacy security. The  increasing attention of FL comes from the combined force of  emerging new technologies with applications. Although  Industry 4.0 was proposed in 2013 [2] and Internet of Things (IoT) is being widely applied in mobile services. There are few  reports on applying large-scale data and deep learning (DL) to  implement large-scale enterprise intelligence. One of the  reasons is lack of machine learning (ML) approaches which can  make distributed learning available while not infringing the  user’s data privacy. Clearly, FL trains a model by enabling the  individual devices to act as local learners and send local model  parameters to a federal server (defined in section 2) instead of  training data. This gives a clear advantage in terms of privacyoriented industrial applications. Another key advantage is that  FL does not need large data-sets to be moved to a central  repository (edge/cloud), it avoids known problems related to  the sink node congestion/overloading. Another advantage of FL  is to give small and medium-sized enterprise (SMEs) an  opportunity to make full use of intelligence, which might be  lack of large sets of data and more eager to apply FL into  balancing data intelligence and proprietary for promoting  innovation and enhancing competitiveness. et al. [3] made a seminal survey that introduces the basic  concepts in FL and a secure FL framework. Aledhari et al. [4]  provided a study of FL with an emphasis on enabling software  and hardware platforms, protocols, real-life applications and  use-cases. Li et al. [5] discussed the unique characteristics and  challenges of FL, provided a broad overview of current  approaches, and outlined several directions for future work. Lo  et al. [6] performed a systematic literature review on FL from  the software engineering perspective. Li et al. [7] conducted a review of FL systems, introduced the definition of FL systems  and analyzed the system components. Mothukuri et al. [8]  provided a study concerning FL’s security and privacy aspects  and outlined the areas which require in-depth research and  investigation. The early reviews introduced the basic concepts  and optimization models of FL. Recently, related platforms and  tools are developed, incentive mechanisms are considered, and  benchmarks and personalized FL are added as well. The FL  architecture needs to be updated as well to accommodate the  increasing FL research and development. Meanwhile, it is noted  that most FL pioneers come from the fields of the computer and  information communication community, and may not put  enough emphasis on the communication with industrial  engineering, which seriously hinders the application of FL on  industrial Internet of Things (IIoT) and the development of IIoT. promoting Industry 4.0, incorporating the consideration from  the practice of industrial big data [9] and edge computing [10].  Our contribution in this survey lies in two aspects: a  comprehensive investigation of the state of the art on FL,  including fundamental and applied research;  attracting and  aggregating attentions from informatics and industrial expertise  to advance the application of FL into  Industry 4.0 by  presenting our insights on promoting industrial data protection  and intelligence. II goes over the origin and development of FL, defines the  terminology used in FL and this paper, and describes the FL  mechanism in our terminology. Section III reviews the state of  the art on fundamental FL and future opportunities. Section IV  presents the FL-transformed manufacturing paradigm and  reviews the state of the practice on FL and future opportunities,  specially in Industry 4.0. Section V concludes the paper and  presents the insights for advancing FL studies.
RankGAN: A Maximum Margin Ranking GAN for Generating Faces<|sep|>Generative modeling approaches can learn from the tremendous amount of data around us to obtain a compact descriptions of the data distribution. Generative models can provide meaningful insight about the physical world that human beings can perceive, insight that can be valuable for machine learning systems. Take visual perception for instance, in order to generate new instances, the generative models must search for intrinsic patterns in the vast amount of visual data and distill its essence. Such systems in turn can be leveraged by machines to improve their ability to understand, describe, and model the visual world. Recently, three classes of algorithms have emerged as successful generative approaches to model the visual data in an unsupervised manner. Variational autoencoders (VAEs) [22] formalize the generative problem as a maximum loglikelihood based learning objective in the framework of probabilistic graphical models with latent variables. The learned latent space allows for eﬃcient reconstruction of new instances. The VAEs are straightforward to train but at the cost of introducing potentially restrictive assumptions about the approximate posterior distribution. Also, their generated samples tend to be slightly blurry. Autoregressive models such as PixelRNN [30] and PixelCNN [35] get rid of the
Lepton Flavour Violation in minimal grand-unified type II seesaw models<|sep|>Grand Uniﬁcation is a powerful guiding principle towards unveiling new physics beyond the Standard Model (SM). The coupling constants of the three SM interactions exhibit the tendency to unify to a common value at a very high energy scale Mgut. This has been long regarded as a hint for a Grand Uniﬁed Theory (GUT) where the SM gauge group SU(3)c × SU(2)L × U(1)Y is embedded in a simple gauge group, such as SU(5) or SO(10) [1, 2]. This paradigm is very appealing for a number of theoretical and phenomenological reasons — for a recent review, see [3]. In particular, GUTs give a rationale behind the otherwise unexplained quantum numbers of the SM fermions, thus accounting for the quantisation of the electric charge and the exact cancellation of gauge anomalies within each single generation of fermions. Furthermore, since quarks and leptons are embedded in common irreducible representations of the GUT group, interactions mediated by GUT gauge bosons unavoidably violate baryon and lepton number, making GUT models in principle testable by searches for proton decay. For what concerns the following discussion, even more important is the observation that the contributions to the running of the gauge couplings due to the SM ﬁeld content alone can not achieve a successful uniﬁcation. Hence GUTs provide a strong motivation for the presence of new ﬁelds at intermediate or low-energy scales that can prompt gauge coupling uniﬁcation. Other open problems of the SM, in particular the origin of neutrino masses, nicely lead to the same conclusion. It is therefore very tempting to look for the ﬁelds able to account for gauge coupling uniﬁcation among those responsible for neutrino masses. However, that does not seem straightforward within the context of the simplest extensions of the SM addressing neutrino masses — Dirac neutrinos or Majorana neutrinos from type I seesaw — as they only involve singlet representations of the SM gauge group (the right-handed neutrinos) that (i) do not aﬀect the running of the gauge couplings themselves, and (ii) are naturally embedded in GUT representations that do not comprise extra ﬁelds which could facilitate uniﬁcation (as they are singlets within SU(5) too, while they nicely ﬁt the 16 spinorial representation of SO(10) together with the other SM fermions). Thus, the programme of achieving a minimal connection between neutrino mass models and uniﬁcation should rather focus on the other two types of seesaw mechanism or on radiative neutrino mass models.1 In this paper, we focus on what is perhaps the simplest possibility, type II seesaw [6–9], that is, we are going to introduce a single scalar SU(2)L triplet and its SU(5) partners contained in the 15 representation. Requiring a successful gauge coupling uniﬁcation and enforcing the proton decay bound set non-trivial constraints on the masses of these new particles, leading to interesting and potentially testable phenomenological consequences. This has been extensively studied in the literature [10–16], with a particular focus on the possibility that some states are light enough to be within the reach of high-energy colliders, such as the Large Hadron Collider (LHC).2 In the following, we revisit several variations of the SU(5) embedding of type II seesaw (distinguished by the way employed to ﬁx the “wrong” fermion mass relations predicted by the minimal Georgi-Glashow SU(5) model [1], see Section 2) and extend the existing literature in multiple directions. We ﬁrst perform a Bayesian ﬁt to the gauge coupling uniﬁcation requirement and the proton lifetime constraint, in order to obtain reliable quantitative information about the viable spectrum of the theory (Section 3) and compare it with direct searches for new physics at the LHC. In Section 4, we move to what is the main focus of the paper: the study of charged lepton-ﬂavour-violating (LFV) decays that are induced by the ﬁelds contained in the 15, which are unavoidable since the couplings of these ﬁelds to the SM fermions need to account for the observed neutrino masses and mixing and must thus be ﬂavour-changing (and are to large extent known). Searches for LFV decays are among the most sensitive probes of new physics coupling to SM leptons. In particular, the ongoing experimental programme is capable to reach scales exceeding 107 − 108 GeV [24]. We are going to study the potential of future experiments of testing type II seesaw GUT models and highlight how the interplay of diﬀerent LFV observables can provide information on the masses of the new particles, as well as complementary constraints on the neutrino parameters that have not been measured yet.
Electronic band crossing in sliding bilayer graphene: Tight-binding calculations and symmetry group representation analysis<|sep|>The shape and topological structure of the Fermi energy surface are important features that govern the behavior of a material’s electronic properties.1–4 Both depend on the geometries of the energy surfaces and the crossings between them. Recently, the topic of band crossings has been extensively revisited in the context of topological characterization of the electronic structure of semimetals.5–10 It is well established that if nonsymmorphic symmetries are present in a crystalline lattice, they will enforce the crossing of electronic bands.11–14 Dirac and/or Weyl points are formed and they are globally stable.5–10 The band crossing can also occur at generic k points in the Brillouin zone and is independent of the symmetry properties of the system, including the lattice symmetries and the reality of the Hamiltonian. This case is called accidental band crossings that were ﬁrst discussed by Herring in 1937.15 In this case, Dirac points are formed and protected by space-time inversion.14,16,17 Studying of the band crossings therefore requires not only quantitative calculations of the electronic structure, but also qualitative symmetry analysis to validate the viability of the data-driven predictions. Dirac points are special points in the electronic structure of a material. They are nonsmooth local extremal points of energy surfaces. Their appearance usually induce saddle points. These points essentially deﬁne topological features of energy surfaces. Dirac points may be present “accidentally” by Herring’s means but, they are shown to be topologically protected by spatial and time symmetries.18 Graphene is a typical two-dimensional material showing all such features of Dirac points. The primitive hexagonal lattice of graphene does not possess any nonsymmorphic symmetries, but Dirac points emerge from the touching of the lowest conduction energy surface and the highest valence one at the six corner (K) points of the Brillouin zone.19,20 Thus, the Fermi energy surface simply takes the form of points. The Dirac points, though stable under perturbations that do not break either time reversal or spatial inversion symmetries, may annihilate each other if the three-fold rotation is broken.18–20 Similar to the graphene monolayer, AB-stacked bilayer graphene also possesses the Fermi energy surface comprised of points. However, the dispersion in the vicinity of these points is characterized by the parabolic law, instead of the linear law.21,22 Unlike AB-stacked bilayer graphene, AA-stacked graphene has circles surrounding each K point in its Fermi energy surface. This is the result of the upward and downward shift in the energy surfaces for the two graphene layers due to the interlayer coupling under the mirror symmetry around the lattice plane (see Fig. 1 below). The bilayer graphene conﬁgurations with a twist angle (TBG conﬁgurations) can also have the same degree of symmetry as that of the AA- or AB-stacked conﬁgurations, depending on the value of the twist angle.23–27 However, band crossings in the TBG conﬁgurations become really complicated due to the shrinking of the Brillouin zone and the folding of the energy surfaces.22,28–30 Besides these special bilayer graphene conﬁgurations, there is another class in which the alignment between the two graphene lattices preserves the parallelism of the armchair/zigzag line between the two layers lines.31–34 These structures are characterized by a sliding vector τ and therefore are called the sliding bilayer graphene (SBG). Though having the same translation symmetry as that of the AA- and AB-stacked conﬁgurations, all rotation symmetries with the vertical axes are broken, except for some two-fold rotation axes in the lattice plane. This dramatic change in the symmetry properties leads to a signiﬁcant change in the interlayer coupling, and thus the physical properties.35–41 Electronic structure calculations for the SBG conﬁgurations suggested the presence of Dirac points, but their existence has not yet been rigorously proven. In this paper, by using a tight-binding model for the pz electrons, we show the emergence of Dirac points in the vicinity of the corner points of the hexagonal Brillouin zone. We prove using group representation theory analysis that the emergence of such
Chemical abundance analysis of symbiotic giants - III. Metallicity and CNO abundance patterns in 24 southern systems<|sep|>Symbiotic stars are long-period binary systems consisting of two stars representing a late stage in stellar evolution: the cool primary and hot and luminous secondary (typically white dwarf albeit a neutron star has been found in a few cases) surrounded by an ionized nebula. Based on their near-infrared (IR) characteristics, symbiotic stars are divided into two main classes: S-type with normal red giant (∼80 per cent), and D-type with Mira variable embedded in an optically thick dust shell (∼20 per cent). A strong interaction between components is driven by mass loss from the cool donor that is partly accreted from the wind and/or via Roche lobe overﬂow (Podsiadlowski & Mohamed 2007; Mikołajewska 2012) on to the hot companion. In the past, when the present compact object underwent its red giant stage, mass had to be transferred in the opposite direction from this star to the star that is currently a red giant. That mass transfer episode should have left traces in the chemical composition of the red giant observed today. Indeed such chemical pollution has been detected in some red giant–white dwarf binary systems (Smith & Lambert 1988). Knowledge of the atmospheric chemical composition of symbiotic giants is of special signiﬁcance as it can be used to track the mass exchange history as well as their population origin. However, at the moment reliable measurements of photospheric compositions exist for only 10 symbiotic systems with late-type (M) giants and about a dozen ‘yellow’, i.e. G or K giant, symbiotic systems. Prior to the current series of papers only four M giants in S-type symbiotic systems had been analysed in the literature: V2116 Oph (Hinkle et al. 2006), T CrB, RS Oph (Wallerstein et al. 2008), and CH Cyg (Schmidt et al. 2006). All of them had solar or nearly solar metallicities. The rarer symbiotic stars containing K-type giants are metal poor with s-process elements overabundant (Smith et al. 1996, 1997; Pereira, Smith & Cunha 1998; Pereira & Roig 2009) whereas those with G-type giants have solar metallicity and s-process enhancement (Smith, Pereira & Cunha 2001; Pereira, Smith & Cunha 2005). The number of symbiotic giants with fairly well-determined photospheric composition is too small to perform reliable statistical analysis. To improve this situation we have started a research program of chemical composition measurements for southern Stype symbiotic systems. The motivation for this work and the ﬁrst analysis of two classical S-type symbiotic systems (RW Hya and SY Mus) were presented in Mikołajewska et al. (2014, hereafter Paper I) and results for the next four systems (AE Ara, BX Mon,
Spatial-Temporal Mitosis Detection in Phase-Contrast Microscopy via Likelihood Map Estimation by 3DCNN<|sep|>Living cell analysis has an important role in biomedical research such as investigating the effect of a drug and the analysis of cell fate. To visualize living cell, phasecontrast microscopy, which is a non-invasive technique, has been widely used for long term monitoring cell populations in vitro. Unlike other invasive imaging methods such as ﬂuorescent assays, it allows cell observation without cell staining. The mitosis detection in phase-contrast microscopy provides much information such as the proliferative behavior of the cell population under speciﬁc cultured conditions. It is also expected to improve the automated cell tracking methods [1], [2]. The manual analysis is time-consuming, tedious, and prone to human error for a large amount of data. Therefore, the automated mitosis detection method in the phase-contrast image is necessary and it provides us quantitative information on cell proliferation. The top row in Fig. 1 shows an example process of mitosis. In this process of mitosis, the circularity and pixel value around the mitosis cell becomes high. Then the cell is splitting into two daughter cells, gradually expanding and return to normal cells. Our aim of this study automatically detects the coordinate x,y and frame t of the moment that the two daughter cells ﬁrst appeared and the boundary of two cells can be observed. To detect mitosis events, several methods have been proposed. Almost all methods ﬁrst extract the candidate sequences, and these methods identify the frame of the Fig. 1. Example sequences of mitosis process. Red circles indicate human annotations where and when the mitosis event occurs. Top: an easy case that the single cell becomes shrink and bright and then divided into two cells. The timing is clear. Middle: a difﬁcult case that the appearance of the annotated cell (red circle) is similar to those at before and after the frame (red dotted circles). Bottom: a case that multiple mitosis events occur in a near position. mitosis event from the candidate sequence via graphical model [3], [4], CNN [5], [6], or LSTM [7], [8] to recognize the temporal feature change. However, these methods still have two problems. First, these methods can not detect multiple mitosis events in a candidate sequence since they assume that a candidate sequence contains only one mitosis event. This situation may occur when the mitosis positions are close as shown in the third row in Fig. 1. In addition, the spatial localization (i.e. mitosis event position) relies on the candidate extraction and it does not use temporal information. The temporal information is only used for temporal localization (i.e. mitosis event timing). Second, as shown in the red circle and red dotted circle in the middle row in Fig. 1, the appearance of the mitosis cell looks the same before and after the frame. Therefore, the annotations by different annotators may have a temporal gap, and it is difﬁcult for even an expert to annotate with consistent criteria, i.e., it may contain gaps in time. The existing methods estimate a single frame that seems occurs mitosis event from the candidate sequences. If the estimated frame is very close from the ground-truth, the estimate is almost correct and it is different from a miss estimation. Nevertheless, the previous approaches do not consider this situation and it is treated as a miss estimation. Contribution: The main contribution of this work is to propose a novel mitosis detection method that can detect multiple mitosis events in a candidate sequence and mitigate the human annotation gap. Unlike the previous models that only localize temporal timing, our method can localize the spatio-temporal locations of mitosis events from a candidate sequence. Our model estimates the 3D (2D+t) likelihood map of mitosis events as shown in the network output in Fig. 2. The overview of the proposed method. (a) First, candidate sequences are extracted based on the fact that the pixel value becomes high in the process of mitosis. b) Then, mitosis events are detected from each candidate via estimating the likelihood map of cell mitosis events by using V-Net [9]. The peak point in the estimated 3D map indicates the mitosis position and timing. Fig.2. This likelihood map can represent multiple mitosis events in a 3D volume (a candidate sequence), and thus it can detect multiple mitosis events. In addition, we effectively use temporal information for spatial localization by using 3D (2D+t) convolution. Our likelihood map estimation is effective for reducing the affect from the human annotation gap, in which the annotated coordinates become peak with a Gaussian distribution. In the training of our model for estimating the likelihood map, a small gap of spatial-temporal localization gives a small loss, and thus this mitigates the gaps. In experiments, we evaluated our method using a challenging dataset [10] that was used in the CVPR Workshop contest [11] and we outperformed the other compared methods. Our proposed method won second place in the CVPR Workshop contest [11]. Related work: The tracking-based mitosis detection methods ﬁrst track all cells and then the location of a newly appeared cell in the tracking result is extracted as a candidate for mitosis events. Then, the candidate is classiﬁed using visual appearance [2], [12]–[14]. In this approach, mitosis detection relies on cell tracking results. However, the tracking performance signiﬁcantly decreases when the cell density is high. In addition, the temporal information is not used for classifying the detected candidates if it is true or not. When multiple cells touch each other, the appearance of the touched cells is similar to a mitosis cell. It is difﬁcult to recognize mitosis without temporal information. Therefore, these methods do not achieve enough performance. As discussed above, the appearance change of a mitosis cell is important information for mitosis detection. To effectively use the temporal context, several graphical modelbased mitosis detection methods that model the change of the visual features in time have been proposed. These methods ﬁrst extract candidate sequences from the original sequence using image processing. Next, the visual feature of each patch image is extracted using hand-crafted feature extraction such as SIFT [15]. Then the graphical model such as HMM [16] or EDCRF [3] or MGRF [17] MMHCRF+ MM-SMM [18] is used to recognize the mitosis frame from the candidate sequence. These methods achieved better performance than the tracking-based method by beneﬁt from time-series information. Recently, deep learning is used in the same framework with the graphical model-based method. Nie et al. [5] used 3DCNN for extracting candidate regions and classiﬁed by SVM whether the sequence contains mitosis. However, this method does not have the ability to localize when the mitosis occurs in the sequence. Zhou et al. [19] proposed a method that estimates the score map by 2D CNN, in which the map indicates the candidate of mitosis event on each image by 2D CNN and it is classiﬁed by 3D CNN whether the sequence includes mitosis. However, the input of 3D CNN does not contain cell appearance information, thereby the method can not use the feature extraction ability of 3D CNN. Mao et al. [6], [7] proposed a method that models the appearance changes of a mitosis cell using a bidirectional LSTM for identifying the timing of a mitosis event. Su et al. [8] proposed a model that uses CNN and LSTM for mitosis detection. The CNN extracts the spatial feature and the LSTM extracts the temporal feature. These methods improved the performance compared with those using the hand-crafted feature extraction. However, they assume that a mitosis event occurs one time in the candidate sequence, and thus it is difﬁcult to apply it to the data under the dense condition. In addition, these methods do not deal with the human annotation gap. Sometimes the human may annotate before or after frame that the network estimates. This affects the network training. To address these problems with multiple mitosis detection and annotation gaps, our method estimates the likelihood map that can represent multiple mitosis events and reduce the bad effects of a human annotation gap.
Universal properties of 3d O(4) symmetric models: The scaling function of the free energy density and its derivatives<|sep|>We provide representations of the scaling functions of the three-dimensional O(4) model which can be used in tests of other models on their membership of the corresponding universality class. Contrary to the often analyzed scaling function of the order parameter, the so-called magnetic equation of state, our main interest here is to determine directly the scaling function of the free energy density and its derivatives. This is especially of importance for applications to quantum chromodynamics (QCD) with two degenerate light-quark ﬂavors at ﬁnite temperature. Two-ﬂavor QCD is believed [1]-[6] to belong to the 3d O(4) universality class at its chiral transition in the continuum limit. In the vicinity of the chiral phase transition temperature the reduced temperature variable in QCD also depends quadratically on the quark chemical potential. Derivatives of the singular part of the free energy density of QCD with respect to chemical potential, which deﬁne cumulants of ﬂuctuations of net quark number, thus are controlled by scaling functions that are given by derivatives of the scaling function of the free energy density in a three-dimensional O(4) model. Obtaining explicit parametrizations of higher order derivatives of the scaling functions of the free energy density became of interest recently as these higher order derivatives control the scaling behavior of ﬂuctuations of conserved charges, e.g. the net baryon number [7]. These quantities are currently measured at RHIC [8] and will also be measured in heavy ion experiments at the LHC.
State Identification for Labeled Transition Systems with Inputs and Outputs<|sep|>Starting with Moore’s famous 1956 paper [16], a rich theory of testing ﬁnite-state machines (FSMs) has been developed to discover aspects of their behavior and ensure their correct functioning; see e.g. [12] for a survey. One of the classical testing problems is state identiﬁcation: given some FSM, determine in which state it was initialized, by providing inputs and observing outputs. Various forms of distinguishing sequences were proposed, ranging from sets of sequences to single sequences solving the problem. Moreover, when combined with state access sequences, so called n-complete test suites can be constructed [8]. The challenge in using n-complete test suites is to keep their size as small as possible. Using a single (adaptive) sequence for state identiﬁcation [11], helps to reach this objective. If such a single sequence does not exist, then a distinguishing sequence distinguishing most states may be supplemented with some additional distinguishing sequences that distinguish the remaining states [15]. Although state identiﬁcation algorithms for FSMs have been widely used, e.g., to check conformance of protocol implementations, their applicability is limited by the expressivity of the FSM framework. In FSMs, inputs and outputs strictly alternate, outputs are fully determined by the previous input and state, and inputs must be enabled in every state. Labeled Transition Systems with inputs and outputs (LTSs), as studied in ioco testing theory [24], provide a richer framework for testing component oriented systems: transitions are labeled by either an input or an output, allowing any combination of inputs and outputs, multiple outputs may be starting from the same state, allowing (observable) output nondeterminism, and states do not need to have transitions for all inputs, allowing partiality. However, LTSs lack the algorithms for test generation from FSM theory. Although progress has been made in deﬁning and constructing ncomplete test suites for LTSs [4], an algorithm to solve the state identiﬁcation problem as in [11], and hence to provide slim n-complete test suites, is missing. Therefore we generalize the construction algorithms for adaptive distinguishing sequences, as given in [11]. As in [4], we have to face the problem of compatible states [18,20], which does not occur for FSMs. States are compatible when they cannot be distinguished in case of an adversarial system-under-test, e.g. when two states have a transition for the same output to the same state. As it is easy to construct LTSs with compatible states, we made sure our algorithms can deal with such LTSs: they accept LTSs with compatible states, but they ‘work around’ them, dealing with all incompatible states. The outline of the paper is as follows. We ﬁrst introduce graphs, LTSs, and some syntax for denoting trees. Then we elaborate on compatibility and the related concept of validity. Furthermore, we introduce test cases, and deﬁne when they distinguish states of an LTS. After that we deﬁne a data structure called splitting graph, present an algorithm that constructs a splitting graph for a given LTS, and another algorithm that extracts a test case from a splitting graph. We show that, unlike for FSMs, the splitting graph may have an exponential number of nodes. However, this is worst case behaviour, as our experiments on an industrial case study will show. Analogous to FSMs, it may not be possible to distinguish all states of an LTS with a single test case. Our experiments show that this is typically the case in practice, but nevertheless more than 99% of the incompatible state pairs are distinguished by the constructed test case. Following [11], we show that our algorithms constructs a test case distinguishing all incompatible state pairs, if it exists. Related work There are (at least) three ortogonal ways in which the classical FSM (or Mealy machine) model can be generalized. A ﬁrst generalization is to add nondeterminism. Whereas an FSM has exactly one outgoing transition for each state q and input i, a nonderministic FSM allows for more than one transition. Alur, Courcoubetis & Yannakakis [1] propose an algorithm to generate adaptive distinguishing sequences for nondeterministic FSMs, using (overlapping) subsets of states, similar to our algorithm. However, their sequences only distinguish pairs of states, and are not designed to distinguish more states at the same time. In between FSMs and nondeterministic FSMs we ﬁnd the observable FSMs, which have at most one outgoing transition for each state q, input i and output o; one may use a determinization construction to convert any nondeterministic FSM into an observable one. The LTSs that we consider have observable nondeterminism. A second generalization of FSMs is to relax the requirement that each input is enabled in each state. In a partial FSM, states do not necessarily have outgoing transitions for every state and every input. Petrenko & Yevtushenko [17] derive complete test suites for partial, observable FSMs, which is the closest to the automata model that we study in this paper. Their test generation is based on (adaptive) state counting [10], which is a trace search-based method which recognizes when states are distinguished, but does not provide a constructive way to build a test that distinguishes (many) states at once. Yannakakis & Lee [26] present a randomized algorithm which generates, with high probability, checking sequences, i.e., n-complete test suites consisting of a single sequence. This approach is also applicable to partial FSMs, as opposed to the adaptive distinguishing sequence construction algorithms of [11], which apply to plain FSMs. A third generalization of FSMs is to relax the requirement that inputs and outputs alternate. In our LTS, inputs and outputs may occur in arbitrary order. Bensalem, Krichen & Tripakis [19] give an algorithm for extracting adaptive distinguishing sequences for all states of a given LTS, by translating back and forth between a corresponding Mealy machine. This translation is only possible, if all states of the LTS have at most one outgoing output transition. Van den Bos, Janssen & Moerman [4] do not need such a restriction. They propose an algorithm that generates an adaptive distinguishing sequence for all pairs of incompatible states. In this paper, we generalize the result of [4] to distinguish more states at the same time.
Franck-Condon Effect in Central Spin System<|sep|>The Franck-Condon (FC) principle, which determines the relative intensity of the vibration-assisted electron transition spectrum, is of much signiﬁcance in molecular physics [1, 2]. In these excitation and de-excitation processes, the transition probabilities are proportional to the square of the overlap integrals between the initial and ﬁnal vibrational states (the FC factors). Compared with the fast electronic transition, the vibrational motion is extraordinary slow. As a result, during the electronic transition, vibrational coordinates nearly keep stationary. This corresponds to a “vertical transition” picture on the eﬀective vibrational potential energy surface, and is called FC eﬀect. The FC principle was originally proposed by Franck to study the mechanism of photoninduced chemical reactions [3] and later expanded to the semi-classical formulation by Condon [1]. And then Lax applied this principle to solid-state physics [4]. However, all the previous works focused on electron-phonon coupling system [5–9]. In this paper, we will study the FC eﬀect induced by spin-spin interaction for a model of a central spin in collective-spin environment, like a central spin in quantum dot (QD) [11–15] or in nitrogen-vacancy (NV) center [16–23]. In these systems, the unavoidable hyperﬁne interaction between the central spin and the collective environmental spins is the chief culprit of decoherence of the interested central spin. Thus, it is important to investigate the eﬀect of the spin-spin FC principle on the dynamics of the central spin. On the other hand, in an ideal environment with speciﬁc inter-spin coupling (such as Ising type), the central spin can be used as a probe to explore the supersensitivity of a quantum critical multi-spin system [24–26]. This theoretical prediction has been tested in several experiments [27–29] and its robustness has been numerically shown as the longitudinal ﬁeld (equivalent to the tranverse hyperﬁne coupling in our model) does not eﬀect on the decoherence behaviour around the critical point [28, 30]. We consider the model of a central spin immersed in an environment of nuclear spins. In general, the central spin can be a nuclear spin or an electron spin. The central spin is initially polarized by the crystal ﬁeld in the z direction. The collective environmental spins behave as the vibrational mode in conventional electron-phonon interaction model of FC eﬀect. And the longitudinal hyperﬁne coupling between the central spin and its spin environment is analogous to the diagonal electron-phonon coupling, resulting in the eﬀective Hamiltonian of the environment spin being central-spin-dependent. Due to this hyperﬁne coupling, when the central spin is excited by the external ﬁeld, the spin bath will be excited simultaneously and this co-excitation generates the collective-spin-based FC eﬀect. And the FC factors, which were originally the overlap integrals between the initial and ﬁnal displaced vibrational Fock states, are deﬁned as the overlaps of the rotated collective spin states in our system. An earlier paper investigated the spin FC eﬀect, but it was only devoted to demonstrating the Stokes shift in a spin-spin interaction system [31]. In contrast to that work [31], we study detailedly and systematically the collective-spinbased FC eﬀect and reveal its underlying physical mechanism. We ﬁnd that there exists the similar FC eﬀect in our spin-spin interacting system. In zero temperature case, the original Lorentz absorption spectrum of a naked spin is shifted and split into few small peaks by the weak hyperﬁne coupling, just as same as the vibronic transition spectrum [32]. The distribution of the relative transition intensity is determined by the FC factors. And the most probable transitions, which have largest FC factors, are ruled by the “vertical transition” mechanism. On the other hand, if the collective-spin environment is at ﬁnite temperature, the peaks of the absorption spectrum of the central spin are depressed and broadened signiﬁcantly. Especially, when the hyperﬁne coupling is strong enough, the excitation of the central spin is suppressed intensively. This behavior is called FC blockade. In the next section, we present our central spin model and its implementation in N-V center in detail. In Sec. III, we discuss the low excitation limit of our central spin model and interpret the conventional FC eﬀect schematically. The collective-spin-based FC eﬀect in our central spin system are addressed in Sec. IV. In Sec. V, we study the collective-spin-based FC eﬀect with vertical transition in schematic perspective. Finally, the summery of our main results is given in Sec. VI. Some details about the rotated Dicke state are displayed in Appendix.
mGNN: Generalizing the Graph Neural Networks to the Multilayer Case<|sep|>Complex networks, the abstract representation of interactions and relationships between the constituents complex systems, are nowadays pervasive in many ﬁelds of science. The versatility of networks for mapping the complexity of a broad variety of empirical systems, makes them suitable mathematical objects granting a wide applicability range. The success of network modeling has been widely demonstrated with applications in biology [1], ecology [2], neuroscience [3], economy [4], ﬁnance [5], engineering [6], physics [7], social [8] and computer science [9], where units can be proteins, animal species, neurons, people or machines, and links can encode regulatory interactions, predator-prey interactions, synaptic connections, social relationships or communication channels, just to mention a few emblematic examples. This broad applicability to problems of empirical interest has motivated scientist to invest considerable research efforts in developing new theories and tools, making networks more and more powerful, in terms of both descriptive and prescriptive power, as well as scalability. In fact, many problems on networks require a high computational effort to be solved, and, often, such problems need approximated algorithms to be tractable in real-world situations. This issue has stimulated scientists to develop new techniques inspired to those employed in the machine learning ﬁeld, where complex problems have been successfully solved by using deep–learning models and algorithms. Recently, M. Grassia and G. Mangioni are with the Department of Electric Electronic and Information Engineering, University of Catania, Italy. E-mail: marco.grassia@unict.it, giuseppe.mangioni@unict.it. M. De Domenico is with CoMuNe Lab, Fondazione Bruno Kessler, Via Sommarive 18, 38123 Povo (TN), Italy. E-mail: mdedomenico@fbk.eu. geometric deep learning [10], a completely new branch of research aimed at enabling the application of deep–learning to networks (and to non–euclidean spaces in general), has been developed, offering up new research opportunities and tools to tackle complex problems, as shown in [11]. This has led, in recent years, to the development of many Graph Neural Networks (GNNs) [12] models (see section III-A for details) that differ mainly in the type of problems they are meant to solve and/or in their performance, as described in the section II. However, it has been shown in the last decade that while networks are wide applicable, many empirical systems exhibit multiple types of interactions or relationships simultaneously, introducing a new level of complexity and topological correlations [13] which required the development of an ad hoc mathematical framework [14] to deal with. For instance, in the case of transportation systems places might be connected by different transportation means such as train, bus, tube, etc [15], [16]. Another emblematic example concerns social systems, where people can exhibit several different kind of relations, such as friendship, acquaintance or business, etc [17]. Furthermore, international trade systems can also be characterized by the presence of several kind of relationships among countries, depending on the commodities they trade [18], [19]. Also, many biological systems are well represented by multiple types of relationship among their constituents, such, for instance, in the Homo Sapiens proteome, where protein– protein relations can be of two types, physical and genetic, such as interactions, chemical associations or post-translational modiﬁcations [20]–[22]. We refer the reader to [23]–[27] for more thorough reviews about multilayer and interdependent systems and other examples of applications. A simple way to study systems characterized by multiple relationships is to consider a single network (or monoplex) that is the result of the aggregation of the different kinds of relations. While this approach has been used often in the past, it has multiple critical issues and received a lot of criticism, since it is inherently affected by loss of potentially critical information about the structure of the system and, consequently, about its function. For example, some of the open questions are: how to aggregate the layers?, and, is it correct to do so?. The answer to the ﬁrst question requires the deﬁnition of an aggregation function (sum, average, min– max, etc.) and a proper scaling of the weights associated to each kind of relation. The second question poses an even more important problem: does it conceptually make sense to aggregate? The answer, in general, is negative, since often relations encode different contexts that cannot be simply mixed together without altering the structure or the function of the system under investigation [15], [28]–[34]. Similarly to the monoplex case, many problems on multilayer networks are computationally hard to solve. But, unlike in the case of monoplex networks, there are no geometric deep–learning tools suitable for multi-layer networks, a significant knowledge gap in the literature. In this paper we propose mGNN an extension of Graph Neural Networks to deal with multi-layer networks. Speciﬁcally, we propose a framework able to manage both the intra- and inter–layer relations of these networks by using any kind of GNN layers. To validate our framework, we show its application to solve three real–world problems on multi-layer networks: nodes (genes) classiﬁcation in a genetic multi-layer network related to malaria, link prediction in a multiplex social network (FriendFeed, Twitter and YouTube users) and multi-layer network classiﬁcation in a super-diffusion [35], [36] prediction problem. The paper is organized as in the following. We introduce the fundamentals of Graph Neural Networks and some of their applications in Section III-A, and the fundamentals of multi-layer networks in Section III-B. In Section II we discuss the related works. Section IV describes our framework and Section V presents the results. Finally, Section VI concludes the paper.
Microscopic pairing theory of a binary Bose mixture with interspecies attractions: bosonic BEC-BCS crossover and ultradilute low-dimensional quantum droplets<|sep|>In the weakly interacting regime, quantum phase of ultracold atomic Bose gases is typically determined by their mean-ﬁeld interactions [1]. Attractive mean-ﬁeld interactions can induce mechanical instability towards collapse [2]. This common viewpoint, however, is radically changed due to the seminal work by Petrov [3], who proposed that the mean-ﬁeld collapse could be prevented by the repulsive force provided by quantum ﬂuctuations, i.e., the celebrated Lee-Huang-Yang (LHY) correction to the energy functional [4]. Although the beyond-meanﬁeld LHY correction is usually small, it can be made comparable to the mean-ﬁeld energy by experimentally tuning the interatomic interactions with the Feshbach resonance technique. As a result, self-bound liquid-like quantum droplets may form, even in free space without container [5–7]. Petrov’s ground-breaking proposal has now been surprisingly conﬁrmed in single-component Bose gases with anisotropic dipolar forces [8–12] and in two-component Bose-Bose mixtures with contact interparticle interactions [13–17]. It opens a new rapidly developing research ﬁeld [18], where the beyond-mean-ﬁeld many-body eﬀect could be systematically explored, both experimentally [8–17] and theoretically [19–35]. Despite the great success of Petrov’s proposal, strictly speaking, it is not a consistent microscopic theory. This is particularly clear for three-dimensional Bose-Bose mixtures, with which the Petrov prototype theory of quantum droplets was constructed, within the Bogoliubov approximation [3, 36]. As the mean-ﬁeld solution is not sta ble towards collapse, one of the two gapless Bogoliubov modes becomes softened and acquires a small imaginary component. This results in a complex LHY energy functional [3, 29, 34, 35], which is not physical. To circumvent this technical issue, Petrov assumed a weak dependence of the LHY energy functional on the interspecies interaction strength and ﬁxed the LHY energy to the value on the verge of the collapse where the mechanical instability ﬁrst sets in [3]. Hereafter, we will refer to such an approximation as Petrov’s prescription. Due to the intrinsic inconsistency in the Petrov prototype theory, the resulting energy of quantum droplets shows an appreciable deviation from the numerically accurate diﬀusion Monte Carlo (DMC) predictions [29]. The predicted critical number for the droplet formation also seems to be larger than the one measured experimentally, both in dipolar Bose gases [11] and in Bose-Bose mixtures [13]. Recently, we developed a consistent microscopic theory to remove the annoying loophole in the Petrov theory of quantum droplets [34]. The crucial ingredient of the theory is the inclusion of a weak bosonic pairing between diﬀerent components due to the attractive interspecies interactions. The pairing explicitly removes the unstable softened Bogoliubov excitation and turns it into a stable gapped mode, as in the conventional Bardeen-CooperSchrieﬀer (BCS) theory for interacting fermions [37]. An apparent advantage of the pairing theory is that, it is variational and therefore predicts an upper bound for the ground-state energy. Remarkably, in three dimensions our pairing theory leads to an improved agreement with the DMC simulation, for the energy and the equilibrium density of quantum droplets [34]. In this work, we would like to provide more details of the microscopic pairing theory in three dimensions [34], on the equation derivation and the numerical calculation. In particular, we show that the weakly interacting pairing in our previous study can be naturally generalized to the strongly interacting regime, in which the interspecies scattering length diverges across a Feshbach resonance. Therefore, we predict the existence of a strongly interacting Bose droplet, where a signiﬁcant fraction of atoms turns into bound pairs. This is precisely analogous to the crossover physics from a Bose-Einstein condensate (BEC) to a BCS superﬂuid [38–40], recently observed with fermionic 40K and 6Li atoms [41, 42]. Across the bosonic BEC-BCS crossover, the interspecies scattering length becomes positive and small, and the strongly interacting Bose droplet eventually disappears and turns into a gas of tightly-bound molecules or dimers via a ﬁrst-order phase transition. We then focus on the new cases of low-dimensional quantum droplets and examine systematically their bulk properties. Our pairing theory turns out to work extremely well in one dimension. For nearly all the interaction strengths at which quantum droplets exist, we ﬁnd an excellent agreement between the theory and the DMC simulation for the ground-state energy [28]. In two dimensions, quantum droplets emerges for an arbitrarily weak interspecies interaction strength. Due to the weakness of the interspecies attraction, our pairing theory does not diﬀer too much with the Petrov theory [19]. Both theories fail to have a good agreement with the DMC simulation, presumably due to the beyond-LHYcorrection that becomes increasingly important in two dimensions [43, 44]. Nevertheless, it is remarkable that with increasing interspecies attraction our pairing theory predict a critical attraction, above which quantum droplets cease to exist. This critical value is consistent with the threshold for zero-crossing in dimer-dimer scatterings from a four-body problem in two dimensions [45]. Above the threshold, the eﬀective interaction between dimers (i.e., tightly bound bosonic pairs) changes from weakly attractive to weakly repulsive, indicating the instability of quantum droplets in the few-body limit. The rest of the paper is organized as follows. In the next section (Sec. II), we introduce the model Hamiltonian for two-component Bose-Bose mixtures and present the pairing theory, i.e, the Bogoliubov theory with bosonic pairing. We mention brieﬂy the concept of a bosonic BEC-BCS crossover. In Sec. III, the connection of our pairing theory to the conventional Bogoliubov theory without pairing is discussed. In Sec. IV, Sec. V, and Sec. VI, we consider the three-, one-, and twodimensional cases, respectively. In three dimensions, we show the existence of a strongly interacting Bose droplet and map out the phase diagram of the bosonic BECBCS crossover. In the low-dimensional cases, we discuss in detail the comparison of the pairing theory to the benchmark DMC simulations and the physics near the
Constraining the Bulk Composition of Disintegrating Exoplanets Using Combined Transmission Spectra from JWST and SPICA<|sep|>Constraining the bulk compositions of planets is crucial for understanding their formation and evolution history. The most common approach to infer the bulk composition of an exoplanet is to use its bulk density (e.g., Fortney et al. 2007; Zeng & Sasselov 2013; Zeng et al. 2019), but this single quantity alone cannot uniquely constrain the composition (e.g. Seager et al. 2007; Rogers & Seager 2010; Dorn et al. 2015). For close-in super-Earths, recent studies predict their compositions based on the observed planetary radius distribution (e.g., Lopez 2017; Owen & Wu 2017), but the compositions inferred this way depend on the assumption about how the radius distribution is sculptured (Owen & Adams 2019). So-called disintegrating planets discovered by the Kepler/K2 missions (Rappaport et al. 2012, 2014; SanchisOjeda et al. 2015) could oﬀer a unique opportunity to investigate exoplanets’ solid compositions. Disintegrating planets are ultra-short-period exoplanets that exhibit asymmet ric transit light curves. An example is K2-22b, which is a planet orbiting close to an M-type star and exhibits asymmetric and highly time-variable transit curves (Sanchis-Ojeda et al. 2015). Disintegrating planets are thought to have an evaporating solid surface that produces mineral vapor, and their asymmetric light curves are interpreted as being due to a comet-like tail forming from the recondensing vapor (e.g., Rappaport et al. 2012). Models suggest that they are presumably solid planets of Moon to Mercury masses (e.g., Rappaport et al. 2012; Perez-Becker & Chiang 2013). Therefore, by performing transmission spectroscopy of the dust tails, we may be able to directly probe the solid composition of lowmass planets. Although there are only a handful of known candidates for disintegrating planets (Rappaport et al. 2012, 2014; Sanchis-Ojeda et al. 2015; Jones et al. 2020), current and future missions for transit searches will discover more candidates. Bodman et al. (2018) explored the possibility of inferring the composition of the dust tails of disintegrating planets from infrared observations with the James Webb Space Telescope (JWST). They found that silicate resonant features near 10 µm can produce transit depths that are at least as large as those in the visible, and that most of the features will be de
The 2006 November outburst of EG Aquarii: the SU UMa nature revealed<|sep|>Dwarf novae are a subclass of cataclysmic variables (CVs) that consist of a white dwarf primary and a late-type secondary. The secondary ﬁlls its Roche lobe and transfers gases into the primary through the inner Lagragian point (L1). The transferred matter forms an accretion disk around the white dwarf. The accretion disk causes instabilities, which are observed as an outburst (for a review, see Warner 1995; Osaki 1996; Hellier 2001; Connon Smith 2007). SU UMa-type stars are a subclass of dwarf novae. The majority have orbital periods below 2 hours. This subclass exhibits two types of outbursts. One is a normal outburst, lasting a few days. The other is a superoutburst lasting about 2 weeks. The most characteristic feature during the superoutburst is that the light curve always shows modulations termed superhump. The observed period of superhumps is a few percent longer than the orbital period of the system. This is well explained by the precessing eccentric disk, which is deformed at the 3:1 resonance radius (Whitehurst 1988; Osaki 1989). EG Aquarii (hereafter EG Aqr) was introduced as a candidate for dwarf novae by Luyten, Haro (1959) and later by Vogt, Bateson (1982). The variability of EG Aqr was reported in Haro, Chavira (1960), in which the variable was at 14.0 mag on 1958 November 5. There are no other positive and negative observations except this around this outburst, so that we cannot lead to the conclusion of the nature of the outburst. Szkody, Howell (1992) performed optical spectroscopy of the variable. The result was, however, that the spectrum showed no emission in Balmer series. On the contrary, it resembles that of a K-type star. There is a possibility that the star was simply misidentiﬁed by Szkody, Howell (1992). Our astrometric estimation of the variable from the outbursting images yielded RA 23h25m19s.09 and Dec −08◦18′18′′.5, respectively. This means that the variable is identical with USNO B1.0 0816-0716959 (RA 23h25m19s.17, Dec −08◦18′18′′.9, B1=18.760 R1=18.260).1. Concerning these results, the infrared counterpart may be 2MASS J23251917-0818190 (J=17.432, H=16.309, K=15.681.2 As for the X-ray range, the variable is below the detection limit of the ROSAT faint source catalog (Voges et al. 1999). On 2006 November 8, one of the authors (RS) discovered the eruption of EG Aqr at a visual magnitude of 12.5 ([vsnet-alert 7217]). This is the ﬁrst recorded outburst since the report by Haro, Chavira (1960). Here we report on time-resolved CCD photometry of EG Aqr during the 2006 November superoutburst.
Multi-probe study of excited states in $\mathrm{^{12}C}$: disentangling the sources of monopole strength between the Hoyle state and $E_{x} = 13$ MeV<|sep|>astrophysics. Historically, there has been a strong focus on the 0+ 2 Hoyle state located at Ex = 7.65407(19) MeV, which is the archetypal α-cluster state and which mediates the astrophysically signiﬁcant 3α reaction to produce 12C. Above the Hoyle state, the Evaluated Nuclear Structure Data File (ENSDF) database lists two 0+ resonances situated at Ex = 9.930(30) and 10.3(3) MeV, with widths of Γ = 2.710(80) and 3.0(7) MeV, respectively [1]. Due to the close proximity of these resonances with respect to their relatively large widths, it is currently understood that these two listed resonances are one and the same [2, 3], corresponding to a broad 0+ 3 resonance at Ex ≈ 10 MeV with Γ ≈ 3 MeV. However, a more recent study of β-decay data from 12N and 12B indicates that the 0+ 3 state may exhibit a higher resonance energy of Ex ≈ 11.2(3) MeV with a smaller width of Γ ≈ 1.5(6) MeV [4]. The Hoyle state remains the focus of experimental and theoretical work with recent eﬀorts to measure its properties such as its direct decay [5–10], gamma decay [11] and E0 decay branching ratios [12]. The total and partial widths of the Hoyle state are of great importance, both for testing our understanding of α-particle clustering and the accurate modeling of stellar nucleosynthesis. The Hoyle state exhibits a narrow primary peak and a pronounced high-Ex tail (the “ghost anomaly”), which results from the strong α-cluster character of the Hoyle state and its proximity to the α-separation energy [13]. This produces a strongly increasing α-particle partial width for the Hoyle state in the excitation-energy region above the main peak, resulting in a distinctive high-energy component to the shape of the state. Currently, the total width of the Hoyle state is estimated to be Γ = 9.3(9) eV, as deduced from the pair-decay partial width and branching ratio [13–15]. It has been suggested that the total width of the Hoyle state can also be indirectly measured through the shape of the ghost [16]. However, this is complicated by the limited knowledge of broad states between Ex = 7 and 13 MeV. Theoretical studies have predicted an additional source of monopole strength resulting from the breathing-mode excitation of the Hoyle state which would lie at Ex ≈ 9 MeV with Γ ≈ 1.5 MeV between the 0+ 2 Hoyle state and the broad 0+ 3 state at Ex ≈ 10 MeV. Two independent calculations using the orthogonality condition model (OCM), which is suited for the study of states near and above the particle threshold energy, have predicted this additional collective Jπ = 0+ state at Ex = 8.95 MeV with Γ = 1.48 MeV [17, 18] and at Ex = 8.09 MeV with Γ = 1.68 MeV [19] with similar properties to the Hoyle state and may correspond to a higher nodal state of the Hoyle state [18]. A study with time-dependent fermionic molecular dynamics predicts two modes of collective isoscalar monopole excitations in 12C: one being a crossing of the α clusters through the center-ofmass of the system and the other a small-amplitude breathing-mode at lower excitation energies [20]. Generator coordinate method (GCM) calculations predict two distinct monopole excitations above the Hoyle state at Ex = 9.38 MeV and Ex = 11.7 MeV with diﬀerent characters: the lower mode corresponds to a breathing-mode excitation of the Hoyle state and the higher mode corresponds to a bent-arm 3α structure [21–23]. A variational calculation performed alongside the GCM calculation produced a large monopole transition strength be tween the Hoyle state and its predicted breathing-mode excitation [23]. Similarly, a calculation using the realtime evolution method predicts a dilute 2ℏω breathingmode excitation of the Hoyle state with a large associated monopole transition strength of 6.2 Weisskopf units [24]. Identiﬁcation of this predicted breathing-mode excitation is complicated by theoretical and experimental factors. The high-energy tail of the Hoyle state extends some considerable energy above the main peak of the Hoyle state, overlying the region in which the breathingmode is predicted. Another source of uncertainty in the broad strength at Ex ≈ 10 MeV is the existence of the 2+ rotational excitation of the Hoyle state, which has been the subject of a decades-long search, culminating in its identiﬁcation at Ex = 9.870(60) MeV with Γ = 850(85) keV through both inelastic scattering and photodisintegration [25–27]. This region was studied by Itoh et al. through the 12C(α, α′)12C reaction with Eα = 386 MeV between θc.m. = 0◦ and 15◦. A peakﬁtting analysis with Gaussian lineshapes of the resulting excitation-energy spectra required an additional peak at Ex ≈ 9.04(9) MeV with Γ = 1.45(18) MeV to reproduce the data, however such Gaussian lineshapes do not capture the physical eﬀects of near-threshold resonances or interference [25]. Since a multipole decomposition analysis (MDA) revealed the excitation-energy region at Ex ≈ 9 MeV to be dominantly populated by monopole strength, a number of authors [19, 23, 28, 29] have discussed the additional Gaussian peak in the context of the breathing-mode excitation of the Hoyle state. However, a more detailed analysis is required, taking into account the complex shape of the Hoyle state and potential interference eﬀects between the known resonances [30–35]. The objective of this work is to study the sources of monopole strength between Ex = 7 and 13 MeV and determine whether the data can be explained by the contributions of the two previously established sources of monopole strength and the associated interference eﬀects. Disentangling these contributions to the monopole strength of 12C is important, both for understanding the nuclear structure of 12C and the 3α reaction rate, which is dependent on the theoretical description of the Hoyle state as well as other additional sources of monopole strength. It is imperative that these factors are understood in order to provide a robust evaluation for the 3α rate (which is beyond the scope of the present work). The main implications of this work are summarized in Ref. [36] and the details of the analysis are reported here.
Pure Gauge Configurations and Solutions to Fermionic Superstring Field Theories Equations of Motion<|sep|>It is well known that string ﬁeld theories (SFT) describe inﬁnite number of local ﬁelds. Just by this reason ﬁnding nontrivial solutions to classical SFT is a rather nontrivial problem. This is a reason why the Schnabl construction of the tachyon solution [1] in the Witten open bosonic SFT [2] attend a lot of attentions [3] - [22]. It turns out that the tachyon solution is closely related to pure gauge solutions. More precisely, Schnabl’s solution is a regularization of a singular limit of a pure gauge conﬁguration [1, 3]. The presence of pure gauge solutions in the bosonic SFT is related to the ChernSimons form of the Witten cubic action. The Schnabl solution is distinguished by the fact that it describes a true vacuum of SFT, i.e. a vacuum on which the Sen conjecture is realized. Since the pure gauge solutions do not shift the vacuum energy the correct shift of the vacuum energy by the Schnabl solution is rather non-trivial fact and its deep origin is still unclear for us. The purpose of this report is to present recent results concerning the generalization of the Schnabl solution to the fermionic case. It is natural to expect that a solution being a singular limit of a pure gauge solution also exist in the cubic super SFT (SSFT) [23, 25]. But for the superstring case there is no a priori a reason to deal with the Sen conjecture, since the perturbative vacuum is stable (there is no tachyon). However a nontrivial (not pure gauge) solution to the SSFT equation of motion does exist [33]. The physical meaning of this solution is still unclear. It may happen that it is related with a spontaneous supersymmetry breaking (compare with [24]). There is also a non-polynomial formulation of the SSFT [31]. A solution of equation of motion for marginal deformations in the non-polynomial SSFT has been obtained in [35, 36]. This construction became clear after realization an explicit relation between solutions to the cubic and non-polynomial superstring ﬁeld theories [32]. These theories include only the GSO(+) sector of the NS string. There are also two versions of the NS fermionic SFT that includes both GSO(+) and GSO(−) sectors, cubic [27] and non-polynomial [30]. Just the NS fermionic SFT with two sectors is used to describe non-BPS branes. The Sen conjecture has been checked by the level truncation for the non-polynomial and cubic cases in [34] and [27], respectively. A solution to the equation of motion of the cubic SFT describing the NS string with both GSO(+) and GSO(−) sectors has been constructed in [28]. On this solution the Sen conjectures takes place. To make a construction of the solution [28] more clear it is useful to incorporate a matrix version of NS fermionic SFT with GSO(+) and GSO(−) [29]. In the matrix formulation an explicit relation between solutions to the cubic and non-polynomial theories become more clear and it gives an explicit formula for solutions to the nonpolynomial theory [30] via solutions [28] to ABKM theory [27]. The Schnabl solution Ψ consists of two pieces and is deﬁned by the limit: where the states ψ′ n deﬁned for any real n ≥ 0 are made of the wedge state [37, 38, 39]. It was shown [1] that the string ﬁeld Ψ in (1) solves the equation of motion of Witten’s SFT contracted with any state C in the Fock space with a ﬁnite number of string excitations. ⟨C, QΨ + Ψ ⋆ Ψ⟩ = 0. (2) On the other hand to check the Sen conjecture, one has to use the equation of motion contracted with a solution itself. The ψN piece in (1) is necessary for the equation of motion contracted with the solution itself to be satisﬁed [3, 32]. We note that the pure gauge part of the Erler conﬁguration does not solve the equation of motion contracted with wedge states ψm It is possible to add extra terms ψN to Ψ∞(1) to get a solution in the sense of (3). These are just the terms that have been used previously to get a desirable value of the action [33]. The paper is organized as follows. In Section 2 a matrix formulation for the NS fermionic SFT is presented. In Section 3 we contribute to a discussion [32] of the classical equivalence of the non-polynomial theory of Berkovits with GSO(±) sectors [30] (here we refer to this theory as the Berkovits, Sen and Zwiebach theory) and the cubic theory of Belov, Koshelev and two of us [27]. In Section 4 perturbative parameterizations of special pure gauge conﬁgurations are presented. These pure gauge conﬁgurations are used in the Erler superstring ﬁeld theory solution [33] and in the tachyon fermion solution [28]. We demonstrate that λ = 1 limit of these pure gauge solutions is in fact a singular point and we use a simple prescription to cure divergences. We show that this prescription gives the same answer as the requirement of validity of the equation of motion contracted with the solution itself.
A Graph Model for Imperative Computation<|sep|>This paper is an investigation into the semantics of imperative programs, using a style of model ﬁrst proposed by Reddy [19]. Reddy’s model was a signiﬁcant development, because it was the ﬁrst to model imperative programs without the use of an explicit semantic entity representing the store. Instead, programs are interpreted as “objects” (in Reddy’s terminology) which exhibit history-sensitive behaviour. The store is not modelled explicitly; instead one models the behaviour that results from the use of the store. This new approach turned out to be the key to ﬁnding models that are fully abstract: that is, models whose equational theory coincides with the operationally deﬁned notion of program equivalence. The ﬁrst such models for higher-order imperative programming languages to be discovered were based on game semantics [2, 1]. Although these models used several ideas from Reddy’s work, it was not known whether Reddy’s model was itself fully abstract for the language SCI which it interprets. In this paper, some of which is a much extended exposition of work ﬁrst presented in [13], we show that Reddy’s model is indeed fully abstract. But more than this, we argue that it arises from a straightforward modiﬁcation of Scott’s well-known Pω graph-model of
Alpenglow - A Signature for Chameleons in Axion-Like Particle Search Experiments<|sep|>Both cosmology as well as popular extensions of the standard model of particle physics are pointing to the possible existence of very weakly interacting very light spin-zero (axionlike) particles (ALPs) and ﬁelds. In fact, a plausible explanation for the apparent acceleration of the cosmic expansion rate of the universe is provided by the presence of a spatially homogeneous scalar ﬁeld which is rolling down a very ﬂat potential [1, 2, 3]. Remarkably, in string compactiﬁcations, there are many moduli ﬁelds which couple to known matter with gravitational strength. Interactions of very light scalar ﬁelds with ordinary matter are strongly constrained by the non-observation of “ﬁfth force” eﬀects leading to e.g. violations of the equivalence principle. Correspondingly, if such particles exist, the forces mediated by them should be either much weaker than gravity or short-ranged in the laboratory. The latter occurs in theories where the mass of the scalar ﬁeld depends eﬀectively on the local density of matter – in so-called chameleon ﬁeld theories [4, 5, 6]. Depending on the non-linear ﬁeld self-interactions and on the interactions with the ambient matter, the chameleon may have a large mass in regions of high density (like the earth), while it has a small mass in regions of low density (like interstellar space). Since such kind of particle is able to hide so well from observations and experiments, it has been called a “chameleon”. Various phenomenological consequences of chameleon ﬁeld theories have been discussed in the literature. Clearly, gravitational and other ﬁfth force experiments are natural places where the eﬀects of such particles can show up [7, 8, 9]. More recently, it has been emphasized [10, 11, 12] that chameleons may also be searched for in laser polarization experiments, such as BFRT [13], BMV [14], PVLAS [15, 16], OSQAR [17], and Q&A [18], which were originally planned for the laboratory search for axion-like particles based on the two photon couplings of the latter. In these experiments, polarized laser light is shone across a transverse magnetic ﬁeld and changes in the polarization state are searched for. Similar to the case of standard ALPs, the real (virtual) production of chameleons would lead to an apparent rotation (ellipticity) of the laser photons. But for chameleons the ellipticity can be much larger than the rotation [12]. This eﬀect arises because chameleons are trapped inside the vacuum pipes [10], due to their high eﬀective mass in the walls. Correspondingly, another type of laser experiments – namely light-shiningthrough-a-wall experiments – are not sensitive to chameleons. In these experiments, such as ALPS [19, 20], BMV [14], GammeV [21], LIPSS [22], OSQAR [17] and PVLAS [23], laser light is shone onto a wall which separates the magnetic ﬁeld into two regions and one searches for photons that might appear behind the wall due to ALP–photon conversion in the magnetic ﬁeld. Clearly, chameleons do not reveal themselves in these kind of experiments, since they cannot pass through the wall. Figure 1: Illustration of an afterglow experiment to search for chameleon particles. (a) Filling the vacuum tube by means of a laser beam with chameleons via photon-chameleon conversion in a magnetic ﬁeld. (b) An isotropic chameleon gas forms. (c) Afterglow from chameleon-photon conversion in a magnetic ﬁeld. The basic idea is depicted in Fig. 1. In the beginning (a), a laser beam is shone through a vacuum cavity in a strong magnetic ﬁeld. The end caps of the cavity are transparent to laser photons. Some of the photons are converted to chameleon particles whose mass m in the vacuum is smaller than the energy ω of the laser beam. However, both the matter density and thus also m are much higher within the end caps such that there m ≫ ω holds. As a consequence, the chameleons are then reﬂected by these end caps and remain trapped inside the cavity [10, 12], which gets more and more ﬁlled (b), whereas the photons escape. After switching oﬀ the laser (c), one can search for afterglow photons, that emerge from the back-conversion of the trapped chameleon particles in the magnetic ﬁeld. The organization of the paper is as follows. Firstly, we introduce the basic idea and some properties of chameleon theories and derive the conversion probability between photons and chameleons from their equations of motion. Secondly, we consider in some detail the loading and unloading rate of vacuum cavities and estimate the parameter space, which is accessible by afterglow experiments, using the experimental parameters of the ALPS [19, 20] experiment. We also discuss the possibility of a coherent evolution of the chameleon wave, which would drastically increase the sensitivity of the experiment. Finally, we state our conclusions and give an outlook on open questions.
Linear quantum systems: a tutorial<|sep|>The dynamics of quantum systems are governed by quantum-mechanical laws. The temporal evolution of a quantum system can be described by its state evolution in the Schr¨odinger picture. Alternatively, it can also be described by the evolution of system variables for example position and momentum of a quantum harmonic oscillator, this is the so-called Heisenberg picture. System variables are operators in a Hilbert space, instead of ordinary functions. Therefore, the operations of these system variables may not commute. Speciﬁcally, let A, B be two system variables (operators), AB ̸= BA may occur. Non-commutativity renders quantum systems fundamentally diﬀerent from classical systems where system variables are functions of time and two system variables always commute. A linear quantum system is a quantum system whose temporal evolution in the Heisenberg picture can be described by a set of linear diﬀerential equations of system variables. Many physical systems can be well modeled as linear quantum systems, for instance, quantum optical systems [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], circuit quantum electro-dynamical (circuit QED) systems [12, 13, 14, 15], cavity QED systems [16, 17, 18], quantum opto-mechanical systems [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], atomic ensembles [32, 33, 24, 34, 28], and quantum memories [35, 36, 37, 38, 39]. Quantum linear systems have been studied extensively, and many results have been recorded in the well-known books [3, chapter 7], [9], [5, chapter 6], and a recent survey paper [40]. The aim of this tutorial is to give a concise introduction to quantum linear systems with emphasis on recent development. This tutorial is organized as follows. Quantum linear systems are introduced in Section 2. Some important structural properties, such as stability, controllability and observability, are summarized in Section 3. It is shown that these concepts, widely used in systems and control theory, are closely related to important properties of quantum linear systems such as decoherence-free subsystems, quantum non-demolition variables, quantum mechanics-free subsystems and quantum back-action evasion measurement. In Section 4, quantum Gaussian states are introduced. The Wigner function is given, and an example is used to demonstrate the Heisenberg uncertainty relation. Skew information and an information-theoretic uncertainty relation is presented. In Section 5, the quantum Kalman ﬁlter is introduced. A general introduction to quantum ﬁlters is ﬁrst presented in Subsection 5.1, after that the quantum Kalman ﬁlter for quantum linear systems as well as a derivation procedure is given in Subsection 5.2. The purpose of providing a derivation procedure is to illustrate some commonly used techniques in the study of quantum linear systems such as Eqs. (2) and (107). An example is given in Subsection 5.3 which illustrates the quantum Kalman ﬁlter and also demonstrates measurement back-action eﬀect. In Section 6, several interesting structural properties of quantum linear systems are summarized, then the quantum Kalman canonical form is presented. An example, taken from a recent experiment [31, Fig. 1(A)], is analyzed in Subsection 6.2. In Subsection 7, continuous-mode single-photon states are introduced, and the response of quantum linear systems to this type of quantum states is given. In Section 8, a general form of quantum coherent feedback linear networks is presented in Subsection 8.1, and a recent experiment is analyzed based on the proposed theory in Subsection 8.2. Some concluding remarks are given in Section 9. −1 is the imaginary unit. Ik is the identity matrix and 0k the zero matrix in Ck×k. δij denotes the Kronecker delta; i.e., Ik = [δij]. δ(t) is the Dirac delta function. • x∗ denotes the complex conjugate of a complex number x or the adjoint of an operator x. Clearly. (xy)∗ = y∗x∗. Given two operators x and y, their commutator is deﬁned to be [x, y] ≜ xy − yx. • For a matrix X = [xij] with number or operator entries, X⊤ = [xji] is the matrix transpose. Denote X# = [x∗ ij], and X† = (X#)⊤. For a vector x, we deﬁne ˘x ≜ � x x# � . If X is a row vector of operators of length m and Y is a column vector of operators of length n, their commutator is deﬁned as • Let Jk ≜ diag(Ik, −Ik). For a matrix X ∈ C2k×2r, deﬁne its ♭-adjoint by X♭ ≜ JrX†Jk. The ♭-adjoint operation enjoys the following properties: • Given two matrices U, V ∈ Ck×r, deﬁne their doubled-up [41] as ∆(U, V ) ≜ � U V V # U# � . The set of doubled-up matrices is closed under addition, multiplication and ♭ adjoint operation. • A matrix T ∈ C2k×2k is called Bogoliubov if it is doubled-up and satisﬁes TT ♭ = T ♭T = I2k. The set of Bogoliubov matrices forms a complex non-compact Lie group known as the Bogoliubov group. • Let Jk ≜ � 0k Ik −Ik 0k � . For a matrix X ∈ C2k×2r, deﬁne its ♯- adjoint X♯ by X♯ ≜ −JrX†Jk. The ♯-adjoint satisﬁes properties similar to the usual adjoint, namely • A matrix S ∈ C2k×2k is called symplectic, if SS♯ = S♯S = I2k. Symplectic matrices forms a complex non-compact group known as the symplectic group. The subgroup of real symplectic matrices is one-to-one homomorphic to the Bogoliubov group.
Constraining extra dimensions on cosmological scales with LISA future gravitational wave siren data<|sep|>The expansion of the universe is undergoing a late time acceleration, the reality of which was for the ﬁrst time directly conﬁrmed in the late 1990s by surveys of cosmologically distant Type Ia supernovae (SNeIa) and then further supported with more recent SNIa observations [1, 2]. Low-redshift data of baryon acoustic oscillations (BAO) [3], cosmic microwave background (CMB) data from the Planck Collaboration et.al [4] and data from other cosmological probes provide further, independent support for an accelerating universe. In the standard model of cosmology where General Relativity (GR) is the underlying framework, this acceleration may be explained by a non-zero but very small cosmological constant, Λ, that opposes the self-attraction of pressureless matter and causes the expansion of the universe to accelerate at the largest scales [5]. Although the Λ-Cold-Dark-Matter model (ΛCDM) is very successful in explaining almost all observations, it has some theoretical issues. These include the mysterious physical origin of the two largest contributions to the energy content of the late-time universe: cold dark matter (CDM) and the cosmological constant (Λ), together with the unsatisfactory predictivity and testability of the inﬂation theory [6]. Therefore it is important to explore alternative explanations for the late-time acceleration of the universe. There are several proposals in the literature in this regard, e.g. [7] presented a study using data from the Hubble Space Telescope and the large-scale galaxy distribution data to demonstrate the disadvantages of ΛCDM in speciﬁc cosmic acceleration scenarios over the redshift range of these observations, thus providing the motivation to approach other modiﬁed models such as the Randall-Sundrum proposal, wherein GW data can set new constraints on extra dimensional cosmological parameters [8]. Therefore, in this work we will focus on the Dvali, Gabadadze and Porrati (DGP) braneworld model proposed in [9], in which our observed four-dimensional universe is considered to be a surface (called a brane) embedded in an inﬁnite-volume, ﬁve-dimensional space (called the bulk). This proposed model explains the late time acceleration of the expansion of the universe through a large-scale modiﬁcation of gravity arising from the slow evaporation of gravitational degrees of freedom into the inﬁnite-volume extra dimension, and without requiring a non-zero cosmological constant. The reason for ﬁrst studying the DGP model is that we will borrow notions from it to then consider cosmological models including additional non-compact spacetime dimensions. Until now, by precisely mapping the distance-redshift relation up to a redshift of z ∼ 1, SNeIa measurements were the most sensitive probe of the late time acceleration of the universe. Another potentially very powerful probe of the universe are GWs emitted from merging binary black holes and neutron stars. In particular, since the propagation of gravitons and photons could diﬀer at a fundamental level, we argue here that GWs emitted by coalescing pairs of massive black holes, at the centre of distant galaxies, observed with an EM counterpart, may be used as an alternative way to test modiﬁed theories of gravity. To measure GWs emitted by these massive black hole binary (MBHB) mergers, with high signal-to-noise ratio (SNR) and in the previously unobserved redshift range 1 < z < 8, ESA and NASA will build the Laser Interferometer Space Antenna (LISA 1). Such GW sources can be thought of as standard sirens (gravitational analogues of standard candles such as SNIe) in the sense that the GWs emitted by a compact binary directly encode information about the gravitational-wave luminosity of the binary system, and thus its luminosity distance [10]. Still, MBHBs have the big disadvantage that the redshift needs to be measured independently, e.g. by optically identifying the host galaxy. However, if we do successfully identify the host galaxy then one question we might be able to answer with such joint events is whether long-wavelength gravitational waves and short-wavelength EM radiation experience the same number of spacetime dimensions [11]. In higher dimensional theories such as the DGP model, as the GWs propagate through spacetime they leak into the extra dimensions, leading to the eﬀect that cosmologically distant sources appear dimmer than they truly are – hence resulting in a systematic error in the inferred distance to the gravitational wave source. Assuming, as is the case for the DGP model, that light and matter propagate in four spacetime dimensions only, EM waves remain unaﬀected. Hence, if we can make an independent measurement of the luminosity distance to the GW source by measuring the redshift of its EM counterpart then this allows us to place limits on the gravitational leakage. In this paper we investigate the consequences of higher dimensional theories with non-compact extra dimensions, borrowing notions from the DGP model to forecast the capability of LISA to constrain theories with gravitational leakage from gravitational wave standard sirens with an observable EM counterpart. We ﬁnd that LISA’s ability to set bounds on the number of additional non-compact spacetime dimensions and the other parameters characterising the theory, namely the screening scale and the transition steepness, will depend strongly on the actual redshift distribution of massive black hole binary merger events, the corresponding eﬃciency in identifying their host galaxy and the uncertainty on these measurements. Also, to compare our forecast at high redshift, we perform a DGP test with the standard Pantheon SNeIa sample at low redshifts, in the sense that this test will be our pivot model in order to carry out a model selection dependent on the number of dimensions and screening scale. This paper is organised as follows: in Sec.2 we introduce the dynamics of the background metric of the universe in the DGP model. Borrowing notions from the DGP model, according to the positive branch, in Sec.3 we write expressions for the luminosity distance expressions for supernovae and GW standard sirens, and then describe the eﬀects of gravitational leakage on the GW waveform in higherdimensional theories with non-compact dimensions. In Sec.4 we investigate the predicted ability of LISA to constrain higher-dimensional gravity. First, in Sec.4.1 we summarise the properties of the MBHB population catalogues that we use to simulate realistic GW data. In Sec.5 we then brieﬂy review our statistical methods and perform the model selection comparison on the Pantheon SNe sample, using the DGP model as a pivot model. Finally, in Sec.6 we present and analyse our results for the predicted LISA constraints on the parameters characterising higher-dimensional theories. In addition, we present the analyses for the joint Pantheon and LISA samples. Our main conclusions are then presented and summarised in Sec.7.
Attention-Based Models for Speech Recognition<|sep|>Recently, attention-based recurrent networks have been successfully applied to a wide variety of tasks, such as handwriting synthesis [1], machine translation [2], image caption generation [3] and visual object classiﬁcation [4].1 Such models iteratively process their input by selecting relevant content at every step. This basic idea signiﬁcantly extends the applicability range of end-to-end training methods, for instance, making it possible to construct networks with external memory [6, 7]. We introduce extensions to attention-based recurrent networks that make them applicable to speech recognition. Learning to recognize speech can be viewed as learning to generate a sequence (transcription) given another sequence (speech). From this perspective it is similar to machine translation and handwriting synthesis tasks, for which attention-based methods have been found suitable [2, 1]. However, compared to machine translation, speech recognition principally differs by requesting much longer input sequences (thousands of frames instead of dozens of words), which introduces a challenge of distinguishing similar speech fragments2 in a single utterance. It is also different from handwriting synthesis, since the input sequence is much noisier and does not have as clear structure. For these reasons speech recognition is an interesting testbed for developing new attention-based architectures capable of processing long and noisy inputs. Application of attention-based models to speech recognition is also an important step toward building fully end-to-end trainable speech recognition systems, which is an active area of research. The 1An early version of this work was presented at the NIPS 2014 Deep Learning Workshop [5]. 2Explained in more detail in Sec. 2.1. dominant approach is still based on hybrid systems consisting of a deep neural acoustic model, a triphone HMM model and an n-gram language model [8, 9]. This requires dictionaries of hand-crafted pronunciation and phoneme lexicons, and a multi-stage training procedure to make the components work together. Excellent results by an HMM-less recognizer have recently been reported, with the system consisting of a CTC-trained neural network and a language model [10]. Still, the language model was added only at the last stage in that work, thus leaving open a question of how much an acoustic model can beneﬁt from being aware of a language model during training. In this paper, we evaluate attention-based models on a phoneme recognition task using the widelyused TIMIT dataset. At each time step in generating an output sequence (phonemes), an attention mechanism selects or weighs the signals produced by a trained feature extraction mechanism at potentially all of the time steps in the input sequence (speech frames). The weighted feature vector then helps to condition the generation of the next element of the output sequence. Since the utterances in this dataset are rather short (mostly under 5 seconds), we measure the ability of the considered models in recognizing much longer utterances which were created by artiﬁcially concatenating the existing utterances. We start with a model proposed in [2] for the machine translation task as the baseline. This model seems entirely vulnerable to the issue of similar speech fragments but despite our expectations it was competitive on the original test set, reaching 18.7% phoneme error rate (PER). However, its performance degraded quickly with longer, concatenated utterances. We provide evidence that this model adapted to track the absolute location in the input sequence of the content it is recognizing, a strategy feasible for short utterances from the original test set but inherently unscalable. In order to circumvent this undesired behavior, in this paper, we propose to modify the attention mechanism such that it explicitly takes into account both (a) the location of the focus from the previous step, as in [6] and (b) the features of the input sequence, as in [2]. This is achieved by adding as inputs to the attention mechanism auxiliary convolutional features which are extracted by convolving the attention weights from the previous step with trainable ﬁlters. We show that a model with such convolutional features performs signiﬁcantly better on the considered task (18.0% PER). More importantly, the model with convolutional features robustly recognized utterances many times longer than the ones from the training set, always staying below 20% PER. Therefore, the contribution of this work is three-fold. For one, we present a novel purely neural speech recognition architecture based on an attention mechanism, whose performance is comparable to that of the conventional approaches on the TIMIT dataset. Moreover, we propose a generic method of adding location awareness to the attention mechanism. Finally, we introduce a modiﬁcation of the attention mechanism to avoid concentrating the attention on a single frame, and thus avoid obtaining less “effective training examples”, bringing the PER down to 17.6%.
Fast and Accurate Optical Fiber Channel Modeling Using Generative Adversarial Network<|sep|>modeling is based on split-step Fourier method (SSFM), which  is carried out by solving the nonlinear Schrödinger equation  (NLSE) approximately [1]. However, the iteration steps of  SSFM result in high complexity of computation. To avoid such  computational complexity, many fiber channel models are  proposed to estimate the channel conditions directly [2-5]. For  example, the Gaussian noise (GN) model evaluates the signalto-noise ratio (SNR) of fiber channel with minor inaccuracies,  and the closed-form approximation of GN model further reduce  the complexity and offer almost real time calculation for  performance prediction [4, 5]. Although GN model class can National Key R&D Program of China (2018YFB1800904), National Nature  Science Fund of China (No. 62071295, No. 61775137, No. 61431009, No.  61433009), and National “863” Hi-tech Project of China. predict the SNR of the signal accurately, they are not available  to model the specific distortions during the fiber transmission.  Therefore, the fast and accurate channel modeling to reflect the  specific distortions is still an open issue. method in channel modeling, which can fit the channel transfer  functions by neural networks (NNs) according to the channel  input and the output data [6]. Compared with the conventional  model-driven method, the data-driven approach prevents  complex mathematical theories and expert knowledge [6, 7].  Moreover, the calculation operation of DL is mainly composed  of multiplications and additions without complicated operations,  such as fast Fourier transform (FFT) in the SSFM-based  modeling method. With the help of graphic processing units  (GPU), the parallel calculations of NNs can be realized, which  further improves the running speed of the model [8].  Additionally, the channel model based on DL has high  compatibility with other neural network structures. For example,  the DL-based channel model has been considered as an  approach to address the gradient back-propagation problems of  the end-to-end communication system by embedding the DLbased channel model into the autoencoder structure [9].  Moreover, the NN-based models can also be utilized for  performance prediction. The noise power can be obtained by  the NN model transmission and digital signal processing (DSP).  However, the traditional NNs with mean square error (MSE) as  loss function, such as back-propagation deep neural network  (BP-DNN), only approximate the means of the distributions of  the channel. These NNs are not able to model the optical fiber  channel with the random distortions, such as the amplified  spontaneous emission (ASE) noise. (GAN) consists of two neural networks, which are trained by  playing games against each other. GAN can learn and represent  a random distribution, pdata, through a neural network [10], and  the new samples satisfying the target distribution can be  generated. In recent years, GAN has been widely used in image  and visual computing, language and speech processing, etc.  [11-15]. Considering the communication channel can also be  regarded as a generative model that satisfies a certain
Limits of the Stokes and Navier-Stokes equations in a punctured periodic domain<|sep|>The study of ﬂuid ﬂow around an obstacle is a challenging and interesting problem in ﬂuid mechanics, and has been the subject of much experimental and numerical investigation (see, among others, [1, 4, 8, 9, 23, 27, 31, 32]). The mathematical analysis of the inﬂuence of an obstacle on the behaviour of the ﬂow when the size of the obstacle is small when compared to that of the reference spatial scale has recently received increased attention. The case of a single obstacle in a two-dimensional ideal ﬂow was analysed by Iftimie, Lopes Filho, & Nussenzveig Lopes [11]; then Iftimie et al. [12] and Iftimie & Kelliher [10] considered the viscous case, Lopes Filho [19] treated bounded domains with several holes, Lacave [14, 15, 16] considered obstacles that shrink to a curve. For problems in exterior domains (i.e. extending to inﬁnity) the ﬂow is usually assumed to vanish at inﬁnity, although the case of ﬂows constant at inﬁnity has been considered by Lopes Filho, Nguyen, & Nussenzveig Lopes [20]. A related ‘small body’ problem was considered by Robinson [25], who treated a simpliﬁed model of combustion in which physical particles were replaced by diﬀuse but compact regions of inﬂuence in the ﬂow. Very recently, Lu [21] treated the Dirichlet problem in the three-dimensional unit ball with a shrinking hole. Uniform estimates, as the size of the hole goes to zero, in W 1,p for any 3/2 < p < 3 and counterexamples that the uniform W 1,pestimates do not hold when 1 < p < 3/2 or 3 < p < +∞ are provided. These estimates were extended by the same author [22] to the Stokes problem in a ndimensional bounded domain, showing uniform estimates for any n′ < p < n and counterexamples for 1 < p < n′ or n < p < +∞. Notice that last two papers do not consider the two-dimensional case for p = 2. Here we are interested in the vanishing obstacle problem in a two-dimensional periodic domain with a particularly simple geometry. More precisely, we are concerned with periodic ﬂows on the punctured domain where Dr = B(0, r) is the disc of radius r centred at the origin, and we study the behaviour of the solutions of various models when the radius r of the disc tends to zero. Throughout the paper we refer to the excised disc Dr as the ‘obstacle’ in keeping with the ultimate application to problems of ﬂuid ﬂow. Our primary motivation for this geometry was the moving ‘tracer particle’ problem considered in two dimensions by Dashti & Robinson [3] and in three dimensions by Silvestre & Takahashi [26]: given a solid disc/sphere of radius r moving in the ﬂuid, does the motion of the particle follow that of the ﬂuid in the limit r → 0? Our aim was to include rotation of the tracer in the 2D case, which was excluded in [3]. However, in the course of the analysis that follows we observed the failure of certain uniform elliptic regularity estimates that are required in both these papers (see Section 2.1); while the two-dimensional case has now been resolved by Lacave & Takahashi [17] for small initial data (using maximal regularity estimates for the Stokes equation) the threedimensional case remains open. (We choose a particularly simple geometry and a somewhat simpler problem in which these uniform estimates fail, but there is no reason to believe that this has any signiﬁcant eﬀect of the nature of this phenomenon.) In order to clarify the setting and provide some background to these uniform elliptic estimates, as well as allowing us to outline the main ideas that will then be applied in the more complicated Stokes and time-dependent Navier–Stokes problems (which have the added component of incompressibility) we ﬁrst consider the Poisson equation as a model problem. Thus our initial aim (in Section 2) will be to determine the asymptotic behaviour of the solution of the following problem when r → 0: We will show that when (1.2) holds then the solutions of (1.1) are uniformly bounded in r in the sense that ˆ Ω u denotes the average of u over Ω (note that this is the whole domain and not just Ωr). This is enough to show that ur − in H1(Ω) and that u satisﬁes the limiting equation. If (1.2) does not hold then the limiting problem has no solution, and in this case it follows that ∥ur∥H1 is unbounded as r → 0. We remark here, and will return to this later, that we have been unable to obtain a uniform bound on ﬄ The main change from the case of the pure Laplacian is that we now have to deal with divergence-free vector-valued functions. The key technical result that allows us to do this is a method for approximating divergence-free periodic functions deﬁned on the whole of Ω by a sequence of divergence-free functions that satisfy the zero boundary condition on Dr (Lemma 3.3). Once again, we require that ´ Ω ur converges to a solution of the limiting problem, but we are unable to bound the average of ur over Ω. Ω ur were suﬃcient to pass to the limit, this is not the case here. Informally, if we set ⟨ur⟩ = ﬄ which contains the additional term (⟨ur⟩ · ∇)˜ur. A uniform bound on ⟨ur⟩ would enable us to pass to the limit in this term, but we do not currently have such a bound. An additional factor that makes this problem diﬀerent in character from the others we consider here is that there is no known general uniqueness result for solutions of (1.3), even on the entire periodic domain. As such, it is perhaps more natural to consider a perturbation problem (given a solution of the equation on Ω, investigate the existence of nearby solutions for r small) than as a limiting problem; or to treat a restricted setting in which uniqueness results are available (when f is small in an appropriate sense). For more discussion of this stationary problem we refer to the classical work of Ladyzhenskaya [18] and Temam [29, 30]. We therefore instead turn in Section 4 to the time-dependent Navier– Stokes problem, which turns out to be more straightforward and for which we do not require the use of the Poincar´e inequality, since a bound on the L2 norm follows immediately from the energy inequality. In this case we obtain convergence of ur to the solution u of the periodic Navier–Stokes equations, where the convergence is strong in L2(0, T; L2(Ω)) and weak in L2(0, T; H1(Ω)). We note that this falls short of L∞ convergence of the velocity ﬁeld; this is unsurprising since uniform convergence coupled with the fact that ur = 0 on ∂Dr would imply that the limiting ﬂow was stationary at the origin.
Robust Estimators in Generalized Pareto Models<|sep|>This paper deals with optimally-robust parameter estimation in generalized Pareto distributions (GPDs). These arise naturally in many situations where one is interested in the behavior of extreme events as motivated by the Pickands-Balkema-deHaan extreme value theorem (PBHT), cf. Balkema and de Haan [2], Pickands [39]. The application we have in mind is calculation of the regulatory capital required by Basel II [1] for a bank to cover operational risk, see H., R. and Bae [24]. In this context, the tail behavior of the underlying distribution is crucial. This is where extreme value theory enters, suggesting to estimate these high quantiles parameterically using, e.g. GPDs, see Neslehova et al. [37]. Robust statistics in this context offers procedures bounding the inﬂuence of single observations, so provides reliable inference in the presence of moderate deviations from the distributional model assumptions, respectively from the mechanisms underlying the PBHT. Literature: Estimating the three-parameter GPD, i.e., with parameters for threshold, scale, and shape, has been a challenging problem for statisticians for long, with many This work was supported by a DAAD scholarship for N. Horbenko. It is part of her PhD thesis, a preprint of it is Ruckdeschel and Horbenko 2010a. ∗Peter Ruckdeschel. Email: peter.ruckdeschel@itwm.fraunhofer.de proposed approaches. In this context, estimation of the threshold is an important topic of its own but not covered by the framework used in this paper. Here we rather limit ourselves to joint estimation of scale and shape and assume the threshold to be known. In the meantime, for threshold estimation we refer to Beirlant et al. [3, 4], while robustiﬁcations of this problem can be found in Dupuis [11], Dupuis and Victoria-Feser [14], and Vandewalle et al. [53]. We also do not discuss non-parametric or semiparametric approaches for modelling the tail events (absolute or relative excesses over the high threshold) only specifying the tail index α through the number of exceedances over a high threshold. The most popular estimator in this family is the Hill estimator [23]; for a survey on approaches of this kind, see Tsourti [51]. With their semi/non-parametric nature, these methods can take into account the fact that the GPD is only justiﬁed asymptotically by the PBHT and for ﬁnite samples is merely a proxy for the exceedances distribution. On the other hand, none of these estimators considers an unknown scale parameter directly, but deﬁne it depending on the shape, so these estimators do not fall into the framework studied in this paper. In parametric context, for estimation of scale and shape of a GPD, the maximum likelihood estimator (MLE) is highly popular among practitioners, and has been studied in detail by Smith [50]. This popularity is largely justiﬁed for the ideal model by the (asymptotic) results on its efﬁciency, see van der Vaart [52, Ch. 8], by which the MLE achieves highest accuracy in quite a general setup. The MLE looses this optimality however when passing over to only slightly distorted distributions which calls for robust alternatives. To study the instability of the MLE, Cope et al. [8] consider skipping some extremal data peaks, with the rationale to reduce the inﬂuence of extreme values. Grossly speaking, this amounts to using a Skipped Maximum Likelihood Estimator (SMLE), which enjoys some popularity among practitioners. Close to it, but bias-corrected, is the weighted likelihood method proposed in Dupuis and Morgenthaler [12]. Dupuis [11] studies optimally bias-robust estimators (OBRE) as derived in [22, 2.4 Thm. 1], realized as M-estimators. Generalizing He and Fung [19] to the GPD case, Peng and Welsh [38] propose a method of medians estimator, which is based on solving the implicit equations matching the population medians of the scores function to the data coordinatewise. Pickands estimator (PE) [39] matches certain empirical quantiles against the model ones and strikes out for its closed form representation. This idea has been generalized to the Elementary Percentile Method (EPM) by Castillo and Hadi [7]. Another line of research may be grouped into moments-based estimators, matching empirical (weighted, trimmed) moments of original or transformed observations against their model counterparts. For the ﬁrst and second moments of the original observations this gives the Method of Moments (MOM), for the probability-transform scaled observations this leads to Probability Weighted Moments (PWM), see Hosking and Wallis [25]; a hybrid method of these two is studied in Dupuis and Tsao [13]; with the likelihood scale, this gives Likelihood Moment Method (LME) as in Zhang [55]. Brazauskas and Kleefeld [5] cover trimmed moments. Clearly, except for the last one, all these methods are restricted to cases where the respective population moments are ﬁnite, which may preclude some of them for certain applications: for the operational risk data even ﬁrst moments may not exist [37] so ordinary MOM estimators cannot be used in these cases. Examples of minimum distance type estimators like the Minimum Density Power Divergence Estimator (MDPDE) or the Maximum Goodness-of-Fit Estimator (MGF) can be found in Ju´arez and Schucany [28] and Luzeno [33], respectively. Considered estimators: Except for Dupuis [11], non of the mentioned robustiﬁcations heads for robust optimality. This is the topic of this paper. In the GPD setup, we study estimators distinguished as optimal, i.e., the maximum likelihood estimator (MLE), the most bias-robust estimator minimizing the maximal bias (MBRE), and the estimator minimiz ing the maximal MSE on gross error neighborhoods about the GPD model, when the radius of contamination is known (OMSE) and not known (RMXE). These estimators need globally-robust initialization estimators; for this purpose we consider Pickands estimator (PE), the method-of-median estimator (MMed) and a particular Location-Dispersion (LD) estimator, MedkMAD. From our application of these estimators to operational risk, we take the skipped maximum likelihood estimator (SMLE) and the Cram´er-von-Mises Minimum Distance estimator (MDE) as competitors. Contribution of this article: Our contribution is a translation of asymptotic optimality from Rieder [42] to the GPD context and derivation of the optimally-robust estimators MBRE, OMSE, and RMXE in this context together with their equivariance properties in Proposition 3.3. This also comprises an actual implementation to determine the respective inﬂuence functions in R, including a considerable speed-up by interpolation with Algorithm 4.4. Moreover, for initialization of MLE, MBRE, OMSE, RMXE, we propose a computationally-efﬁcient starting estimator with a high breakdown—the MedkMAD estimator, which improves known initialization-free estimators considerably. For its distinction from alternatives, common ﬁnite sample breakdown point notions to assess global robustness have to be replaced by the concept of expected ﬁnite sample breakdown point introduced in R.& H. [47]. While the optimality results of Rieder [42] do not quantify suboptimality of competitor estimators, our synopsis in Section 4.5 provides a detailed discussion of this issue. To this end, in Appendix A, in Propositions A.1–A.6, we provide a variety of largely unpublished results on inﬂuence functions, asymptotic (co)variances, (maximal) biases, and breakdown points of the considered estimators. The optimality theory we use is conﬁned to an asymptotic framework for sample size tending to inﬁnity; the simulation results of Section 5 however close this gap by establishing ﬁnite sample optimality down to sample size 40. Structure of the paper: In Section 2 we deﬁne the ideal model and summarize its smoothness and invariance properties, and then extend this ideal setting deﬁning contamination neighborhoods. Section 3 provides basic global and local robustness concepts and recalls the inﬂuence functions of optimally robust estimators; it also introduces several efﬁciency concepts. Section 4 introduces the considered estimators, discusses some computational and numerical aspects and in a synopsis summarizes the respective robustness properties. A simulation study in Section 5 checks for the validity of the asymptotic concepts at ﬁnite sample sizes. To illustrate the stability of the considered estimators at a real data set, in Section 6, we evaluate the estimators at the Danish ﬁre insurance data set of R package evir [35] and at a modiﬁed version of it, containing 1.5% outliers. Our conclusions are presented in Section 7. Appendix A provides our calculations behind our results in the synopsis section. Proofs are provided in Appendix B.
On the Geometry of Supersymmetric Quantum Mechanical Systems<|sep|>In the following, a supersymmetric system will mean a supersymmetric quantum mechanics (SUSYQM) according to the following deﬁnition1: On a complex separable Hilbert space acts a hamiltonian H, a number of supercharges Qj=1,...,N , and a grading operator K which splits the Hilbert space H = Hb ⊕ Hf into a bosonic and a fermionic sector. These operators are self-adjoint on their respective domains and satisfy the relations where {A, B} := AB + BA is the anticommutator. A classic example of a SUSYQM with a geometric interpretation is provided by the Dirac operator [1, 3, 4]. That is perhaps a rather uninteresting example in the ﬂat space case, where only local geometry is non-trivial, but its extension to the setting of curved manifolds has led to new insights in global geometry and index theory. Here, we will focus on the local geometry of supersymmetric systems with Schr¨odinger-like hamiltonians. In particular, we are interested in hamiltonians of the form H = HB + HF , where the so-called bosonic part HB is an ordinary Schr¨odinger operator and the fermionic part HF is a matrix- or algebra-valued multiplication operator. Since such operators involve a laplacian, their corresponding supercharges will necessarily have to involve some form of Dirac operator. de Crombrugghe and Rittenberg [5] have carried out a rather general algebraic analysis of SUSYQM hamiltonians, but with focus on cases when the supercharges are linear in the Cliﬀord generators. This is true for a single Dirac operator, but will apart from that generally not be the case in the examples we are considering. Furthermore, when studying the algebraic properties of supersymmetric systems it is common to work with creation and annihilation operators aj, a† j, ck, c† k and consider a Fock representation of these on a Hilbert space with a particle interpretation. We will on the other hand stick to the alternative Schr¨odinger representation involving coordinates and momenta xj, pxj acting as multiplication and diﬀential operators on an L2-space, and Clifford generators ek1, ek2 acting in a representation of the corresponding Cliﬀord algebra. We will emphasize the real geometry in the systems we consider and use it to ‘explain’ the appearance of complex structures. This leads to the identiﬁcation of additional structures, properties and possible extensions of these systems which might not have been at all obvious from the conventional complex formulation. We will also point out that it is possible to ﬁnd a notion of supersymmetric system even in a purely real setting with no canonical complex structure. Apart from the purely mathematical interest in investigating the structure of these types of systems, one motivation from physics is that it seems worthwhile to explore the possibility of giving more complicated, but related, SUSYQM systems such as supersymmetric matrix models a more geometric interpretation.
Structure and Magnetic Fields in the Precessing Jet System SS433 III. Evolution of the Intrinsic Brightness of the Jets from a Deep Multi-Epoch VLA Campaign<|sep|>SS 433 was the ﬁrst known microquasar, a type of x-ray binary (XRB) system consisting of a collapsed star accreting material from a less evolved donor star. The feature that distinguishes a microquasar from other XRBs is that they are variable radio sources that can eject pairs of oppositely directed relativistic jets (Mirabel & Rodríguez 1999). In many ways they are analogous to their AGN counterparts only smaller in size resulting in much shorter variability time scales. This provides a distinct observational advantage for studying accreting black holes and relativistic jet systems. SS 433 has been intensely studied since the discovery of oscillating simultaneously red and blue shifted hydrogen lines in the optical spectrum (Margon et al. 1979a). A kinematic model was proposed that explained these features as the result of a pair of mildly-relativistic precessing jets (Margon et al. 1979b). This model was soon conﬁrmed and several parameters disambiguated by comparison with early VLA radio images (Hjellming & Johnston 1981). The jets precess with a period of 163 days about a cone with a semi-opening angle of ∼ 20◦. The central axis is inclined from the line of sight by ∼ 80◦. For indepth reviews of previous work, see Margon (1984) and Fabrika (2004). Previously in Roberts et al. (2008) we analyzed the structure of SS 433 in both total and polarized intensity at 0.1′′ resolution from a 15 GHz VLA image. In Roberts et al. (2010) (henceforth Paper II) we described in detail the procedure for measuring the intrinsic brightness of the jets, a measure of the source emission as observed in a comoving reference frame, and analyzed the proﬁle of a single, high dynamic range observation. We found that the east and west jet proﬁles were consistent with an assumption of intrinsic symmetry and that the proﬁle was more complex than could be ﬁt by a simple exponential or power law. We suggested that the proﬁle shape may be indicative of variation in the power injected into the jet. In this paper we present a series of deep twofrequency VLA observations of the jets of SS 433 and use them to study the jets as they evolve over an 80 day period. Proper characterization of the jet dynamics is important for testing and developing jet models which predict, amongst other things, the brightness decay rate based on assumed physical parameters. In Section 2 we describe the observations and data reduction and present the images. In Section 3 we present maps of the spectral index across the source. In Section 4 we calculate the intrinsic brightness proﬁle at each epoch and investigate how it evolves in time. We discuss the implications of our results in Section 5 and present our conclusions in Section 6.
K\"ahler-driven Tribrid Inflation<|sep|>2 Introducing Tribrid Inﬂation 3 2.1 Supersymmetric Hybrid Inﬂation . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Supersymmetric Tribrid Inﬂation . . . . . . . . . . . . . . . . . . . . . . . 3 3 Scalar Potential for K¨ahler-driven Tribrid Inﬂation 5 3.1 Identifying the Relevant Terms . . . . . . . . . . . . . . . . . . . . . . . . 5 3.2 Inﬂaton Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.3 Inﬂaton-Dependent Masses m2 h and m2 S . . . . . . . . . . . . . . . . . . . . 7 4 Slow-roll Predictions 8 4.1 Calculation of φ0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.2 Slow-roll Predictions for the CMB Spectrum . . . . . . . . . . . . . . . . . 9 4.3 Hilltop-Type Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.4 Vacuum Energy V0 during Inﬂation . . . . . . . . . . . . . . . . . . . . . . 12 4.5 Critical Inﬂaton Value φc . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 5 Constraints on Model Parameters 14 5.1 Constraint from the Waterfall Requirement . . . . . . . . . . . . . . . . . . 14 5.1.2 Conventional Tribrid Superpotential: l = m = 2 . . . . . . . . . . . 15 5.2 No Inverted Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 6 Smallness of Loop Corrections 17 6.1 Loop Corrections for l > 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 6.2 Loop Corrections for l = 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 6.3 General Conclusion Concerning Loop Corrections . . . . . . . . . . . . . . 19
NIR jets from a clustered region of massive star formation: Morphology and composition in the IRAS 18264-1152 region<|sep|>The formation of massive stars (M∗ ≳ 8 M⊙) is a process that is not yet clearly understood. There are many observational challenges preventing us from uncovering the mechanisms behind their birth, such as the limited number of high-mass young stellar objects (HMYSOs), their location at large distances (typically a few kiloparsecs), and high visual extinction (see, e.g. Tan et al. 2014; Rosen & Krumholz 2020, for recent reviews). We do know that, similarly to their low-mass counterparts, HMYSOs eject great amounts of material in the form of bipolar outﬂows and jets (e.g. Frank et al. 2014; Bally 2016). These outﬂows are intrinsically related to the process of accretion onto the protostellar surface (Blandford & Payne 1982; Pudritz & Norman 1983; Shu et al. 1994). Thus, jets and outﬂows are crucial for pinpointing the location of massive protostars and providing insights into the physical processes unfolding in the central highly extinguished regions. When observing in the near infrared (NIR) regime, the driving source is usually not accessible as it is very often totally obscured. However, it is possible to observe the jets driven by massive young stars in the NIR and these can provide a wealth of information about the star-forming complex. Moreover, the excellent angular resolution usually achieved in this regime allows us to probe the collimated jet and individual knots. Molecular hydrogen (H2) is a particularly good shock tracer as it is the primary coolant in the NIR and its emission can be extended over spatial scales of several parsecs (Davis et al. 2004; Caratti o Garatti et al. 2008; Davies et al. 2010; Caratti o Garatti et al. 2015; Fedriani et al. 2018), displaying numerous strong emission lines. In the K-band (1.9 − 2.5 µm) in particular, the strongest transition is the H2 1 − 0 S(1) at 2.12 µm, which has been used in many studies to probe the jet morphology in massive protostellar environments (Gredel 2006; Caratti o Garatti et al. 2008). HMYSO accretion is indeed revealed by their outﬂows, extending parsecs away from the star, but also through reﬂected light in their outﬂow cavity walls (Fedriani et al. 2020). This reﬂected light emission can further reveal the YSO nature when no clear association can be done with their outﬂows. In particular, the Brγ at 2.16 µm is an excellent tracer of young protostars as it probes phenomena occurring very close to the YSO. The Brγ line has been detected at the base of powerful jets driven by a HMYSO (Caratti o Garatti et al. 2016), as well as in accretion discs in a sample of HMYSOs (Koumpia et al. 2021). The target of our study is the IRAS 18264−1152 massive star-forming region. H2O and Class I and II CH3OH masers have been detected in this region (Sridharan et al. 2002; Beuther et al. 2002b; Chen et al. 2011; Rodríguez-Garza et al. 2017). Such masers are signposts of massive star formation and outﬂows. IRAS 18264−1152 is also associated with the extended green object (EGO) G19.88−0.53 (Cyganowski et al. 2008), which has an estimated bolometric luminosity of 7.8 × 103 L⊙ from the Red Midcourse Space Experiment Source survey (MSX, Lumsden et al. 2013). The kinematic distance ambiguity to IRAS 18264−1152 was broken by Roman-Duval et al. (2009). He et al. (2012) locate it at 3.31+0.34 −0.37 kpc away and compute the local standard of rest velocity (vLSR) of the molecular cloud to be 43.7±0.02 km s−1, which are the values adopted in our study. In the past twenty years, this region has been surveyed in a variety of wavelengths, and there is growing evidence to support the scenario that IRAS 18264−1152 harbours more than one protostar. Qiu et al. (2007) found two peaks in their 1.3 mm and 3.4 mm continuum maps. One of these peaks had been resolved as a triple source by Zapata et al. (2006) in their Very Large Array (VLA) data at 1.3 cm and 7 mm. Rosero et al. (2016) detected twelve compact radio sources in their sub-arcsecond resolution VLA observations at 6 cm. In Rosero et al. (2019), the authors analysed eight of those sources (those within the EGO boundaries), suggesting that at least one could be an ionised thermal radio jet. This is in agreement with the analysis of Zapata et al. (2006) for that source. The observations of Issac et al. (2020) obtained with the upgraded Giant Meterwave Radio Telescope (uGMRT) at 1391.6 MHz revealed two ionised thermal jets, one of which is associated with a hot dense core at 2.7 mm as observed with the Atacama Large Millimeter/submillimeter Array (ALMA). In total, these authors found six 2.7 mm cores in ALMA archival data consistent with being protostars with M∗ > 9 M⊙. The multi-wavelength analysis carried out by Issac et al. (2020) strongly supports the protocluster hypothesis for this high-mass star-forming region. Various outﬂows have also been observed in the IRAS 18264−1152 region via diﬀerent molecular species and transitions. In the NIR regime, De Buizer & Vacca (2010) detected EGO emission extending approximately 30′′ that was associated to H2 outﬂow activity; Varricatt et al. (2010) reported an eastwest (EW) outﬂow extending at least 43.2′′ (i.e. > 0.69 pc at d = 3.31 kpc), with lobes apparently bent northwards (raising the possibility that they belong to diﬀerent outﬂows), as well as additional H2 features in the north-east (NE) and W directions; Lee et al. (2012) found a 1.6′ (1.54 pc) EW outﬂow, and additional knots to the NE and N of the region (which the authors speculate could be linked to multiple protostars); Ioannidis & Froebrich (2012) measured the E outﬂow lobe to be 42′′ (0.67 pc), the W counterpart to be 50′′ (0.80 pc), the N knot found in Ioannidis & Froebrich to be 18′′ (0.29 pc), and a new SW knot to be 10′′ (0.16 pc); Navarete et al. (2015) found a bipolar EW structure divided into six components, and three other non-aligned knots (one in the NE and two in the W directions). Summarising, there is wide agreement on the existence of two main H2 lobes in the east and west directions. However, nothing is known about the kinematics of these ﬂows and it remains unclear whether the H2 knots compose one single outﬂow or if they belong to diﬀerent outﬂows. As for outﬂows in the sub-millimetre, millimetre, and radio regime, a CO (2−1) bipolar structure has been observed in the region of IRAS 18264−1152 by Beuther et al. (2002a) with the IRAM 30 m telescope at a resolution of 11′′. A blue lobe was found to extend from E to W for ∼ 60′′, with a SW red counterpart of ∼ 20′′. The low angular resolution of these observations did not allow individual outﬂow structures to be resolved. More recently, Issac et al. (2020) obtained low-resolution C18O (1−0) data which revealed a large-scale NE - SW outﬂow consistent with the results of Beuther et al. (2002a). Moreover, this study presents high-resolution C18O (1−0) and C17O (3−2) observations, which traced a smaller scale collimated outﬂow in the SE NW direction. The presence of outﬂows created by the shock tracer SiO (2−1) has been studied by Qiu et al. (2007), who found two quasi-perpendicular outﬂows in the SE - NW and in the NE direction (with the former being consistent with the CO outﬂow reported in Issac et al. 2020). These outﬂows showed several blue and red knots, leading to the proposition that there might be more than one protostar in the region. Indeed, Issac et al. (2020) found six millimetric peaks, which hint at the presence of a protostellar cluster. Sánchez-Monge et al. (2013) have also observed SiO (2−1), SiO (5−4), and HCO+ (1−0) in the region of IRAS 18264−1152 using the IRAM 30m telescope (resolution from 10′′ to 30′′), reporting bipolar outﬂow structures in all three cases. In this paper, we present new VLT/SINFONI integral ﬁeld unit observations of the central region with the highest angular resolution achieved so far (0.2′′). Our goal is to study the morphology and investigate the line emitting regions of IRAS 18264−1152 in the NIR, with emphasis on the location and structure of the molecular hydrogen jets. We complement these observations with KMOS archival data in order to obtain a better understanding of the entire star-forming complex in the K-band. Furthermore, we present millimetre data of the CO (2−1) transition from the Submillimeter Array (SMA) in order to compare morphologies with the NIR H2 outﬂows. Lastly, we create the spectral energy distribution (SED) for this source using archival data from Spitzer, Herschel, and WISE. The SED is ﬁtted with radiative transfer models from Zhang & Tan (2018), which are based on the Turbulent Core Accretion hypothesis for massive star formation (McKee & Tan 2003). The best ﬁt models provide constraints on the environmental conditions, evolutionary stage, and protostellar core mass for the protostar(s) in the IRAS 18264−1152 region. This paper is structured as follows: in Sect. 2, we present the observations and the data reduction processes; in Sect. 3, we describe the results found from the spectro-imaging analysis, the physical properties derived for the H2 jets, the comparison to the CO outﬂows, and the SED ﬁtting; we discuss the possible morphologies and jet structures in Sect. 4; ﬁnally, we summarise and highlight our conclusions in Sect. 5.
Feedback-Controlled Sequential Lasso Screening<|sep|>Sparse dictionary-based representation of data has proved effective in a wide range of applications in computer vision, machine learning, signal processing and statistics [10, 35]. A sparse representation uses a dictionary D ∈ Rn×p, with columns {di}p i=1 called features, to represent a data point y as a linear combination �p i=1 widi of a relatively small number of the features. Thus for many i, we require wi = 0. One way to obtain such representations is by solving the lasso problem [24]: In many of today’s sparse coding applications, the dictionary can be very large, both because the data dimension is large ([36, 28, 23, 14]) and because an adequate representation requires many features ([36, 22, 1, 34, 14]). For example, in music genre classiﬁcation the authors of [7] employed scattering representations [1, 2] of short segments of music data, resulting in dictionaries of size n = 199 and p = 12, 000 (ﬁrst order scattering), and n = 11, 726 and p = 12, 000 (second order scattering). Once dictionaries reach this size, solving (1) can become a bottleneck in the overall computation. More importantly, for signiﬁcantly larger dictionaries, ﬁtting the dictionary into available memory is a concern. Several approaches are possible for solving (1) with a large dictionary. For example, assuming the dictionary ﬁts into memory, one could use early termination of an iterative solver (e.g., FISTA [3]) to quickly obtain an approximate solution. Alternatively, one could use a ﬁnite number of steps of a sequential greedy method (e.g., OMP [26]) to approximate a solution of (1). OMP iteratively selects a feature with the largest correlation with the current residual. Searching for this feature is the most time consuming part of the algorithm but only requires part of the dictionary to be loaded at once. To obtain a solution ˆw that is m-sparse, the algorithm needs to hold at most m features in the memory. However, since the true sparsity is unknown, in general, OMP and similar algorithms will only yield an approximate solution. Both of these approaches lead (in general) to an approximate solution of (1). Several recent studies have investigated the following alternative approach. For a given y and λ, use duality to quickly identify a subset of features in D that are certiﬁed to have zero weight in a solution ˆw of (1). By removing these features from the dictionary and solving a smaller lasso problem, we can obtain an exact solution to the original problem. In [9] this approach is called SAFE screening, indicating that screening does not reject any needed feature. Just as important, screening can be executed with only a few features loaded into memory at once. Hence it has the potential to signiﬁcantly reduce the size of dictionary that is provided to a lasso solver. Recent work on safe screening has further developed this idea [39, 37, 8, 29, 33, 11, 19, 20]. The survey [38] reviews these recent methods. A related but distinct form of lasso screening [25] allows false rejection of features with small probability. We do not explicitly consider such screening tests, but our results are also relevant to this approach. The core idea of SAFE screening is to bound the solution of the dual problem of (1) within a region R and compute µ(di) = maxθ∈R dT i θ. The value µ(di) is then used to decide if feature di can be removed. Since these tests are applied once prior to solving (1), we refer to them as “one-shot” tests. A parameter that is often useful in such tests is the value λmax = maxi |dT i y|. Roughly, this gives a measure of how well a feature in D matches y. Empirical studies indicate that current one-shot screening tests perform well for normalized values of λ/λmax of moderate to large size, but performance quickly declines as λ/λmax falls below 0.4 [38]. To address this problem, one can employ a more complex form of screening known as sequential screening [9, 25]. In sequential schemes, one screens and solves a sequence of N lasso problems with λ1 > · · · > λN. The key point is that for instance (y, λk), one utilizes the dual solution ˆθk−1 to obtain a region bound on ˆθk. This is used to apply a one-shot screening test to reduce the dictionary. And a standard solver is then used to solve the reduced problem to ﬁnd ˆwk. Existing state-of-the-art sequential screening algorithms include sequential Dome [32], sequential Strong rule [25], sequential Enhanced DPP rule [29] among others [9]. They are situated in the context of model selection. In this setting, all solutions to the sequence of problems are of interest, and the sequence of regularization parameters is thus ﬁxed a priori (typically taking a log-grid of values). Sequential screening is then conducted along this predetermined sequence to expedite this parameter tuning process. In contrast, we focus on another distinct application context where the best regularization parameter, denoted by λt, has already been chosen via cross-validated model selection, and then we need to solve many lasso instances using this ﬁxed value. Of our particular interest, is λt/λmax around 0.1 since many applications of the sparse representation framework have found this range to be most helpful. For example, [39] found that λt/λmax in the range [0.1, 0.2] maximizes SVM classiﬁcation accuracy for the handwritten digit recognition problem. In music genre classiﬁcation, the authors of [7] selected λt/λmax in the range [0.1, 0.15] via cross-validation to maximize classiﬁcation accuracy using the SRC classiﬁer [36]. Once the proper λt to use is known, the problem of efﬁcient sequential screening can be very different from the sequential screening that is targeted for the model selection purpose: the solution at λt is the only one that is of interest, and all other problem instances are merely way points. We are thus given the freedom to design the sequence of regularization parameters {λk}N k=1 with λN = λt that speciﬁcally target a single problem instance at λt. In this scenario, the open-loop sequential screening schemes using a geometrically spaced sequence {λk}N k=1, which is determined before any problem in the sequence is solved, may not be a good idea. And it would be an advantage if {λk}N k=1 could be tuned individually to each instance (D, y, λt). This paper makes two contributions. First, we design a feedback mechanism to adaptively select the sequence {λk}N k=1 and N for sequential screening. The feedback mechanism automatically selects the next value λk as a function of the results seen in previous steps of the sequential screening process. It also determines when to stop, and hence automatically selects N. We call our feedback controlled sequential screening scheme, data-adaptive sequential screening (DASS). DASS has the advantage that N and {λk}N k=1 are automatically adapted during the screening process to the particular instance (D, y, λt). In addition, the feedback mechanism bounds the region diameter used for one-shot screening at each iteration by a value set by the user. Second, we examine the effects of the inevitable errors that accrue in obtaining the solutions (and dual solutions) of the intermediate lasso problems during sequential screening. At each step, sequential screening assumes exact knowledge of the previous dual solution. However, practical lasso solvers introduce a small error. In the context of classiﬁcation, we show that the DASS scheme is robust to these errors. We give required background in §2. Then we introduce DASS and examine its properties in §3. We show that DASS ensures the diameters of the regions used for the one-shot screening are bounded by a user selected value, and the number of intermediate lasso problems is also bounded. We then address the issue of inaccuracies in lasso solutions. §4 discusses the performance of DASS on a selection of datasets, and we conclude in §5. Proofs are given in the supplementary material.
Back to Futures<|sep|>Concurrency has been a very useful tool in increasing performance of computations and in enabling distributed computation, and consequently, there are a wide variety of diﬀerent approaches to programming languages for concurrency. A common pattern is to begin with a sequential language and add some form of concurrency primitive, ranging from threading libraries such as pthreads to monads to encapsulate concurrent computation, as in SILL [33,32,16], to futures [17]. Many of these approaches have seen great practical success, and yet from a theoretical perspective, they are often unsatisfying, with the concurrent portion of the language being attached to the sequential base language in a somewhat ad hoc manner, rather than having a coherent theoretical backing for the language as a whole. In order to give a more uniform approach to concurrency, we take the opposite approach and begin with a language, Seax, whose semantics are naturally concurrent. With a minor addition to Seax, we are able to force synchronization,
Ensemble Learning Applied to Classify GPS Trajectories of Birds into Male or Female<|sep|>The Animal Behavior Challenge was organized by the 2018 Symposium on Systems Science of BioNavigation 1, sponsored by Technosmart 2 and proposed as a CodaLab competition 3. It consisted in classifying the gender of shearwater based on trajectories (latitude and longitude) and some metainformation associated to each shearwaters’ trip. Such prediction models could help to understand shearwater more efﬁciently and how they navigate themselves, like male and female shearwater could use different trajectories along the way of trip. The training dataset is composed of all the GPS trajectories of 631 streaked shearwaters (326 male and 305 female) breeding on Awashima Island, Japan. Each datapoints in the training dataset representing a complete bird trip and being composed of the following attributes 4 : In the competition setup, the testing dataset is composed of all the GPS trajectories of 275 streaked shearwaters. In the Development Phase of the competition 10% of the submission labels are randomly modiﬁed to report score. In the Final Phase of the competition, all the submission are recalculated for the ﬁnal ranking. Our approach uses extensive feature engineering prior to ensemble learning. Section 2 describes feature engineering techniques. Section introduces our winning model, which is based on a ensemble learning architecture. Section 4 and Section 5 compares and analyses our various models quantitatively and qualitatively on the competition dataset set. The source code of our ﬁrst-place solution can be found online https://github.com/dfayzur/Animal-Behavior-Challenge-ABC2018.
Simplifying Multiloop Integrands and Ultraviolet Divergences of Gauge Theory and Gravity Amplitudes<|sep|>The past few years have brought remarkable advances in understanding scattering amplitudes in the maximally supersymmetric N = 4 super-Yang-Mills (sYM) theory [1] in the planar limit of a large number of colors. It may soon be possible to completely determine all planar scattering amplitudes in this theory, for all values of the coupling, going far beyond the (now thoroughly understood) cases of four and ﬁve external gluons [2]. Much of this progress has been surveyed recently [3]. Planar scattering amplitudes exhibit a new symmetry known as dual conformal symmetry [4, 5], which severely restricts their structure. Together with supersymmetry and (position space) conformal symmetry, dual conformal invariance gives rise to a Yangian [6]—an algebraic structure common in integrable models. Indeed, it is widely believed that several aspects of the planar sector of N = 4 sYM theory are controlled by an integrable model (see e.g. ref. [7]). In contrast, much less is known about the nonplanar sector—the subject of the present paper. Consider N = 4 sYM theory for the gauge group SU(Nc). In the limit Nc → ∞, the nonplanar, or subleading-color, contributions are suppressed by powers of 1/Nc. Once one takes into account these corrections, for ﬁnite Nc, the scattering amplitudes no longer appear to possess dual conformal symmetry, nor do they demonstrate any obvious integrability properties. Understanding the subleading-color terms is critical to a complete description of the behavior of gauge theories. For example, many types of color correlations are suppressed in the large-Nc limit. Furthermore, the information provided by the full-color expression for N = 4 sYM amplitudes, expressed in terms of their loop-momentum integrands, can be used to construct corresponding amplitudes [8–11] in N = 8 supergravity [12]. From each set of amplitudes one can extract information about ultraviolet divergences in the respective theory. The ultraviolet (UV) properties of N = 8 supergravity have been the focus of intense investigation. There have been several recent reviews of the situation [13]. Long ago, an N = 8 supersymmetric local counterterm at three loops in D = 4 was proposed [14– 18]. An explicit computation of the three-loop four-graviton amplitude ﬁrst revealed that the counterterm has a vanishing coeﬃcient [9]. Subsequently it was realized [19] that this counterterm is forbidden in D = 4 by the E7(7) duality symmetry [12]. Other analyses have extended the ﬁniteness constraints from E7(7) and linearized supersymmetry, such that the ﬁrst potential divergence in D = 4 is now at seven loops [20–23]. Finiteness until this loop order happens to agree with an earlier naive power-counting, based on the assumption of an oﬀ-shell N = 8 superspace [24]. A potential seven-loop divergence is also suggested by other approaches, including an analysis of string theory dualities [25], a ﬁrst-quantized world-line approach [26], and light-cone supergraphs [27]. However, it has also been argued that the theory may remain ﬁnite beyond seven loops [28]. In this paper, we will show how a conjectured duality between color and kinematics [29, 30] provides a powerful method for computing subleading-color terms in N = 4 sYM amplitudes, in a way that makes the construction of the corresponding N = 8 supergravity amplitudes extremely simple. Also, the N = 8 result is expressed in a form that makes manifest the true ultraviolet behavior of the amplitude (when continued to higher space-time dimension D). Thus this method provides unprecedented access to the precise coeﬃcients of potential counterterms in N = 8 supergravity, as well as in its higher-dimensional versions. It may eventually oﬀer a means for settling the question of whether additional UV cancellations exist in N = 8 supergravity, beyond the known or expected ones. Perhaps even more importantly, the method gives a means for constructing complete amplitudes, allowing for detailed studies of their symmetries and properties. A key point is that when the color-kinematics duality holds manifestly, it locks the nonplanar contributions to the planar ones. The nonplanar contributions are essential for evaluating gravity amplitudes, because in gravity theories no separation exists between planar and nonplanar contributions. This duality allows one to eﬃciently export information from the planar sector, e.g. that provided by dual conformal symmetry, to the much more intricate nonplanar sector. A second key point is the claim [30] that if a duality-respecting representation of N = 4 sYM amplitudes can be found, then the loop-momentum integrands of the corresponding N = 8 supergravity amplitudes can be obtained simply by taking the graphs of N = 4 sYM theory, dropping the color factors and squaring their kinematic numerators. This doublecopy property is a loop-level generalization of the corresponding tree-level property [29], equivalent to the Kawai-Lewellen-Tye (KLT) relations between gravity and gauge-theory amplitudes [31]. Using the color-kinematics duality, followed by the double-copy property, advances in constructing integrands for the planar sector of gauge theory can be carried over to the nonplanar sector, and then on to gravity. The color-kinematic duality and the gravity double-copy property do not appear to require supersymmetry, although amplitudes in supersymmetric theories are generally much simpler to work with than non-supersymmetric amplitudes. Another important aspect of the duality is that it appears to hold in any dimension, thus making it compatible with dimensional regularization. In this paper we will exploit the color-kinematic duality to construct the complete fourloop four-point amplitudes of N = 4 sYM theory and N = 8 supergravity. Both amplitudes were constructed previously by us [10, 11]; however, the present construction is considerably more eﬃcient, and makes various properties of the amplitudes more manifest. The colorkinematic duality relations allow us to express the four-loop loop-momentum integrands, for 83 diﬀerent cubic (trivalent) graphs, as functionals of the integrands of just two planar graphs. For the one-, two- and three-loop four-point [30, 32], and the one- and two-loop ﬁve-point cases [33], the duality is even more restrictive: a single planar graph suﬃces to determine all the others. As it is becoming increasingly simple to construct planar amplitudes, a particularly attractive aspect of using the color-kinematic duality is that it determines nonplanar contributions from planar ones. Perhaps even more remarkable, in terms of measuring the redundancy found in local gauge-theory scattering amplitudes, we shall ﬁnd that the entire non-trivial dynamical information in the four-loop four-point amplitude is contained in a single nonplanar graph; all other graphs are related to this one by the duality. While a general proof of the duality conjecture is yet to be given beyond tree level, the four-loop construction we oﬀer in this paper provides further evidence in favor of it, in the form of a highly nontrivial example. In this work, we have conﬁrmed the dualitybased construction by verifying that the integrand matches a complete (spanning) set of generalized unitarity cuts. Based on the double-copy structure of supergravity amplitudes, we will give a new representation of the four-loop four-point N = 8 supergravity amplitude. This construction provides a direct multiloop conﬁrmation of the double-copy property, because we verify the generalized unitarity cuts for the new form of the supergravity amplitude, against the cuts of the known expression [10], originally constructed using the KLT relations. We also explore the ultraviolet properties of the amplitude in D > 4 dimensions. An essential feature of the new representation is that the UV behavior is manifest: Individual integrals diverge logarithmically in precisely the same critical dimension Dc as their sum. This property did not hold for the previous form of the amplitude [10]. The critical dimension is also the same as that for the planar and single-trace sectors of N = 4 sYM theory. In a previous paper [10], we showed that the supergravity amplitude is ﬁnite for D < 11/2, which is also the bound obeyed by N = 4 sYM theory. However, the previous form of the amplitude did not display this bound manifestly. To see the cancellation of potential UV divergences, the integrals had to be expanded a few orders in powers of the external momenta. The lack of manifest UV behavior in that representation made it diﬃcult to carry out the required integration in D = 11/2 and to determine whether the amplitude actually does diverge in this dimension. With the new form, this task is greatly simpliﬁed, allowing us to carry it out here. Due to the double-copy construction, the numerators of the integrands for the N = 8 supergravity amplitude are perfect squares. However, they multiply propagator denominators that do not have deﬁnite signs. Therefore, individual integrals contributing to the amplitude can have diﬀerent signs. To probe whether or not the four-loop amplitude diverges in D = 11/2, it is necessary to actually evaluate the UV divergences in the loop integrals in this dimension. Using the double-copy form of the four-loop four-point amplitude we do so, ﬁnding that the N = 8 ﬁniteness bound is in fact saturated at D = 11/2 at four loops, which matches the behavior of N = 4 sYM theory. Moreover, we calculate the precise coeﬃcient of the N = 8 supergravity divergence. We ﬁnd that it exactly matches the coeﬃcient of the divergence of the 1/N2 c -suppressed single-trace term in the four-loop four-point amplitude of N = 4 sYM theory, up to an overall rational factor. Although this property is most striking at four loops, only emerging after a number of simpliﬁcations, it is consistent with lowerloop behavior. Presumably this consistent connection is a clue for unraveling the general UV properties of N = 8 supergravity. Regularization is a crucial point in the construction of loop-level amplitudes in massless theories, because such amplitudes are usually either infrared or UV divergent. The issue of regularization has been studied in some detail in the context of unitarity cuts in ref. [34], where the six-dimensional helicity formalism [35] was suggested as a general means for implementing either dimensional regularization [36] or a massive infrared regulator equivalent to the one in ref. [37]. In the present paper we take advantage of an earlier construction of the four-loop four-point amplitude of N = 4 sYM theory, which provides expressions with demonstrated validity for D ≤ 6 [11, 34]. In this paper, we compare the D-dimensional unitarity cuts of the new results with the cuts of the earlier results. We ﬁnd exact agreement, conﬁrming the new representations. This paper is organized as follows. In section II we explain our strategy for constructing multiloop integrands, illustrating it with the three-loop four-point N = 4 sYM amplitude. In section III (and appendix B) we present the new forms of the four-loop integrands of N = 4 sYM theory and N = 8 supergravity. We also outline their construction. In section IV, we obtain the explicit value of the UV divergence of the N = 8 supergravity amplitude in D = 11/2 and discuss its properties. We also determine the UV divergence of the color doubletrace terms in the four-loop N = 4 sYM amplitude in D = 6. (We had found earlier [11] that the double-trace divergence canceled in the next possible lower dimension, D = 11/2.) We give our conclusions and outlook in section V. Several appendices are included. The ﬁrst one gives functional deﬁning relations between the numerators in the four-loop fourpoint amplitude, which are derived from the Jacobi relations after imposing some auxiliary conditions valid for N = 4 sYM theory. Appendix B presents the analytic expressions for the numerators. Appendix C gives the values of the vacuum integrals entering the UV divergence for the supergravity amplitude in the critical dimension, as well as expressions for the vacuum integrals’ numerators. Explicit expressions for the color factors for each contribution to the full four-loop amplitude may be found online [38], where we also provide plain-text, computer-readable versions of the numerator factors and the kinematic dual Jacobi relations that they obey.
The High-$z$ Universe Confronts Warm Dark Matter: Galaxy Counts, Reionization and the Nature of Dark Matter<|sep|>Dark matter dominates the evolution of gravitational perturbations, leading to the formation of haloes and galaxies. In the prevalent paradigm of cold dark matter (CDM), the primordial perturbation spectrum extends to very small scales; galaxy formation proceeds from the bottom up, commencing in the smallest dark matter haloes where gas cooling can occur. If instead there exists a non-negligible minimal scale for primordial perturbations as in the case of warm dark matter (WDM), halo formation is delayed, and early galaxy formation is suppressed considerably. Early galaxy formation has been understood to be a challenge for WDM models for some time (Barkana, Haiman & Ostriker 2001; Somerville, Bullock & Livio 2003). Today, the tension is only heightened by mounting evidence that structure formation is proceeding in earnest at very early cosmic times. There are now direct detections of galaxies at redshifts as high as ∼10 (Ellis et al. 2013; Oesch et al. 2013), clearly indicating that there are collapsed structures at this time. More indirectly, studies of quasar spectra show that the intergalactic medium was almost fully ionized by redshift z ∼ 6 (Fan et al. 2006) and the measured electron scattering optical depth from the cosmic microwave background may could imply reionization as early as z ∼ 10 (Ade et al. 2013). The maintenance of reionization back to these early times seems to require contributions from numerous, low-mass galaxies (Kistler et al. 2009; Kuhlen & Faucher-Giguere 2012; Robertson et al. 2013). In this paper, we examine how current and future observations of high-z galaxies, together with observational probes of reionization, can constrain the dark matter power spectrum on small scales, and by extension the particle nature of dark matter. There has been considerable interest in the WDM paradigm for galaxy formation, owing to potential problems with the LCDM model on sub-galactic scales. Most recently, it has been recognized that the observed central densities of low-luminosity Milky Way dwarf satellite are signiﬁcantly lower than expected in dissipationless CDM simulations (Boylan-Kolchin et al. 2012). This issue can be be alleviated if the dark matter is warm (Lovell et al. 2012, 2013; Polisensky & Ricotti 2013). Here, we speciﬁcally study a WDM
Scaling Properties of Ge-SixGe1-x Core-Shell Nanowire Field Effect Transistors<|sep|>ECENT years have witnessed a remarkable progress in  emerging research materials, such as semiconductor  nanowires or carbon nanotubes, as alternatives to  conventional  complementary  metal-oxide-semiconductor  (CMOS) technology [1-5].  A key question regarding such  devices, relevant to both benchmarking potential application  as well as gaining insight into fundamental electronic  properties, is the device performance scaling with channel  length.  For carbon nanotubes it has been experimentally  established that the nanotube resistance scales linearly with  length for channel lengths larger than a few microns, where  diffusive transport applies, and is independent of length for  channel lengths smaller than one micron, in the ballistic  transport regime [4].  Here we present the first scaling study of  high performance germanium (Ge) – silicon-germanium  (SixGe1-x) core-shell nanowire (NW) field-effect transistors  (FETs) with highly doped source (S) and drain (D).  The  highly doped (>1020 cm-3) source and drain, realized using  boron (B) ion implantation, enable efficient carrier injection  with a contact resistance much lower than the nanowire  resistance.  The nanowire FET resistance scales linearly with  the channel length down to 300 nm, indicating that the  transport in these nanowires is diffusive at room temperature.        Manuscript received July 23, 2009.  This work was funded by DARPA  contracts HR0011-08-1-0050 and N66001-07-12013.  Junghyo Nah, E. -S.  Liu, K. M. Varahramyan, D. Shahrjerdi, S. K. Banerjee, and E. Tutuc are with  the Microelectronics Research Center, University of Texas, Austin, TX 78758  USA (e-mail: jnah@ieee.org, etutuc@mer.utexas.edu). Semiconductor nanowires enable the realization of novel  device geometries, such as gate-all-around field effect  transistors, which allow for more energy efficient electronics  at a given switching speed, thanks to a better electrostatic  control of the channel [6-9].  Germanium and Ge-Si core-shell  nanowires have attracted interest as a platform for  aggressively scaled field effect transistors, thanks to Ge higher  carrier mobility than Si, and its compatibility with CMOS  technology [10-12].  A main, albeit mundane obstacle, that has  often impeded both an accurate electrical characterization, as  well as the realization of high performance devices using  nanomaterials is the carrier injection.  Generally NW FETs  employ metal contacts at the source and drain terminals [1314], which limits the device performance because of the  Schottky barrier existent at the metal-semiconductor interface.   Moreover, ambipolar behavior is usually observed in such  devices.  The contact material that provides low contact  resistance and unipolar carrier injection should be highly  conductive, with a Fermi level aligned with the nanowire  conduction or valence band, depending on the carrier type to  be injected.  A highly doped section of the same nanowire  satisfies these conditions.  Nanowire doping with axial  modulation can be achieved via vapor-solid-liquid (VLS)  growth mechanism [15], thermal diffusion from dopantcontaining molecule [16], and ion implantation [17-20].  Ion  implantation allows for accurate axial doping control along the  NW and is widely used in existing CMOS technology.  Here,  we employ low energy ion implantation in order to realize  NW FETs with highly doped source and drain.
Synergies between Vera C. Rubin Observatory, Nancy Grace Roman Space Telescope, and Euclid Mission: Constraining Dark Energy with Type Ia Supernovae<|sep|>Type Ia Supernovae (SNe Ia) are a key probe for measuring dark energy, and currently hold a critical place in two of the three large surveys: the Legacy Survey of Space and Time (LSST) with the Vera C. Rubin Observatories and Nancy Grace Roman Space Telescope. Both LSST and the Roman SN Ia surveys will be unprecedented in their statistical sample size. The LSST optical sample at redshift 0 < z < 1 will be a factor of 300× larger than the cumulative sample of SNe Ia today and the Roman NIR sample at 0.3 < z < 3.0 will discover and measure 50× more SNe Ia at z > 1 than in our sample today and increase the sample of NIR light-curves by a similar magnitude. While Euclid does not have a planned SN Ia survey in its baseline plan (see here for a proposed strategy: Astier et al. 2014), it will revisit deep ﬁelds with a regular cadence, which can produce SN Ia detections and be combined with the LSST sample. The synergy between combined data sets is enormous — especially if the surveys coordinate observations. Combining data sets allows increases in redshift range and wavelength range, boosting the statistical precision and the systematic control. Furthermore, as discussed in other responses to this call, the SN Ia programs will immensely beneﬁt from cross-survey data products to improve calibration and photometric redshifts. Particularly due to the time-dependent nature of SN Ia surveys, planning must be done now for survey coordination and combined analyses. In this response to the DOE/NASA Request for Information, we separate aspects that need decisions now versus those where planning should begin soon to be considereded for future synergies. For the SN Ia programs, it is particularly important that the multiple science agencies who have issued this RFI, continue to work together because the cosmological analyses will be built on SN Ia data that must be obtained using large time allocations from facilities that are run by these (and other) agencies. The quality of the results will also strongly depend on the development of techniques and knowledge that individually may go beyond each diﬀerent agency’s traditional focus areas. To maximize the science impact of each agency’s investment, scientists must have broad support, including for ways that allow them to work together across agencies’ traditional boundaries. A scientist should only need to apply to a single agency for funding to fully participate in all aspects of the problem.
Investigating light neutralinos at neutrino telescopes<|sep|>In the papers of Ref. [1] it was shown that light neutralinos with a mass in the range 7 GeV ≲ mχ ≲ 50 GeV are interesting candidates for particle dark matter, with direct detection rates accessible to experiments with large exposure (of order 100 000 kg day) and low energy threshold (of a few keV). This population of light neutralinos arises in the Minimal Supersymmetric extension of the Standard Model (MSSM) when the uniﬁcation of gaugino masses at a Grand Uniﬁed (GUT) scale is not assumed [2]. Indeed, in this supersymmetric framework the lower bound of ∼ 7 GeV on the neutralino mass is set by a cosmological bound on the neutralino relic density [1]. This is at variance with the lower bound mχ ≳ 50 GeV, which is derived from the LEP lower limit on the chargino mass within the MSSM with gaugino mass uniﬁcation at the GUT scale. A direct comparison of the theoretical predictions of Ref. [1] with experimental data was made possible when the DAMA Collaboration published the results of its measurements collected with a NaI detector of 100 kg over 7 annual cycles [3]. Actually, in Ref. [4] it was proved that the population of light neutralinos ﬁtted well these DAMA results. The inclusion of the channeling eﬀect in the experimental analysis [5] and the further results of the DAMA/LIBRA combined data [6] conﬁrmed the good agreement of the predictions of Ref. [1] with the experimental data, as discussed in Refs. [7, 8]. Possible indirect eﬀects of light neutralinos were studied in Refs. [8, 9], where it was shown that present cosmic antiproton data set sizable constraints on the supersymmetric space; measurements of cosmic antideuterons [10] with forthcoming airborne experiments [11] were indicated in [8, 9] as a very promising investigation mean for the light neutralino population. In Ref. [9] also possible signals induced in neutrino telescopes by the neutrinos produced by pair-annihilations of light neutralinos captured in the Earth and the Sun were discussed. The analysis was carried out in the case of a single category of events, those due to upward through-going muons. In the present paper we resume the analysis of Ref. [9], by implementing and extending it in various distinctive features: a) the particle-physics uncertainties in hadronic quantities which aﬀect the capture rate of relic neutralinos by the celestial bodies are taken into account and discussed in detail; b) all main processes that occur during the neutrino propagation: neutrino oscillations and neutrino incoherent interactions with matter are included; c) the investigation comprises also the category of upward stopping muons (actually, this turns out to be the most promising case for upward-going muons induced by pair annihilation of light neutralinos). Indirect evidence of WIMPs in our halo by measurements of upward-going muons at neutrino telescopes, generated by WIMP pair-annihilation in celestial bodies, has been the subject of many investigations in the past, see for instance [12, 13, 14, 15]. Recently, a number of papers appeared where possible signals at neutrino telescopes due to pair annihilation of light DM particles are discussed. These consider either generic WIMPs with assumed dominance of speciﬁc annihilation channels [16, 17] or discuss speciﬁc DM candidates other than neutralinos: WIMPless dark matter and mirror dark matter [17], leptonically interacting dark matter [18] and DM particles which directly annihilate to neutrinos [19]. All these investigations are restricted to signals from the Sun. At variance with these analyses, the present paper deals with neutrino signals produced in cosmic macroscopic bodies by pair annihilation of light neutralinos whose speciﬁc supersymmetric properties are those which satisfy all existing particle-physics and cosmological constraints, as analyzed in Refs. [1, 8]. Furthermore, our investigation entails neutrino signals expected from the Sun as well as those from the Earth. Our results are given ﬁrst for the whole population of light neutralinos, then for those subsets which are selected by the DAMA/LIBRA results [6], when these are interpreted in terms of relic neutralinos. We separate the case where the channeling eﬀect is included from the one where this eﬀect is neglected. The scheme of the present paper is the following. In Sect. 2 the main features of the supersymmetric model employed here are summarized. The formulae providing the capture rates of relic neutralinos by celestial bodies and the annihilation rates due to neutralino pair-annihilation are recalled in Sect. 3, together with some properties of the hadronic quantities which enter in the evaluations of the neutralino-nucleon cross section. The generation and the propagation of the neutrino ﬂuxes is described in Sect. 4, their ensuing muon ﬂuxes are derived in Sect. 5. Results and conclusions are given in Sect. 6.
Multi-Region Neural Representation: A novel model for decoding visual stimuli in human brains<|sep|>A universal unanswered question in neuroscience is how the human brain activities can be mapped to the diﬀerent brain tasks? As one of the main techniques in taskbased functional Magnetic Resonance Imaging (fMRI) analysis, Multivariate Pattern (MVP) is a conjunction between neuroscience and computer science, which can extract and decode brain patterns by applying the classiﬁcation methods [1, 2]. Indeed, it can predict patterns of neural activities associated with diﬀerent cognitive ∗Department of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics Nanjing, China, email: myousefnezhad@nuaa.edu.cn. †Department of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics Nanjing, China, email: dqzhang@nuaa.edu.cn. states [3, 4] and also can deﬁne decision surfaces to distinguish diﬀerent stimuli for decoding the brain and understanding how it works [5, 6]. Analyzing the patterns of visual objects is one of the most interesting topics in MVP classiﬁcation, which can enable us to understand how brain stores and processes the visual stimuli. It can be used to ﬁnd novel treatments for mental diseases or even to create a new generation of the user interface. Technically, MVP classiﬁcation is really a challenging problem. Firstly, most of the fMRI data sets are noisy and sparse, which can decrease the performance of MVP methods [7]. The next challenge is deﬁning the regions of interest (ROIs) [4]. As mentioned before, fMRI techniques allow us to study what information are represented in the diﬀerent regions. So, it is really important to know what are the eﬀects of diﬀerent stimuli on the brain regions, especially in complex tasks (doing some simple tasks at the same time such as watching photos and tapping keys). On the one hand, most of the previous studies manually selected the ROIs. On the other hand, deﬁning wrong ROIs can signiﬁcantly decrease the performance of MVP methods [3, 4]. Another challenge is the cost of brain studies. Combining diﬀerent homogeneous fMRI data sets can be considered as a solution for this problem but data must be normalized in a standard space. The procedure of normalization can increase the time and space complexities and decrease the robustness of MVP techniques, especially in voxel-based methods [5]. The last challenge is visualization. As a machine learning technique, MVP represents the numerical results in the voxel-level, network connections, etc. Sometimes, it is so hard for neuroscientists to ﬁnd a relation between the generated results and the cognitive states. The contributions of the paper are four fold: ﬁrstly, the proposed method estimates and analyzes a snapshot of brain image for each stimulus based on the level of using oxygen in the brain instead of analyzing whole of fMRI time series. Indeed, employing these snapshots can dramatically decrease the sparsity. Secondly, our methods can automatically detect active regions for each stimulus and dynamically deﬁne ROIs for each data set. Further, it develops a novel model of neural representation for analyzing and visualizing functional activities in the form of anatomical regions. This model can provide a compact and informative representation of neural activities for neuroscientists to understand: what is the eﬀect of a stimulus on each of the automatically detected regions instead of just study the ﬂuctuation of a group of voxels in the manually selected ROIs. The next contribution is a new Gaussian smoothing method for removing noise of voxels in the level of anatomical regions. Lastly, this paper employs the L1-regularization Support Vector Machine (SVM) [8] method for creating binary classiﬁcation at the ROIs level and then combine these classiﬁers by using the Bagging algorithm [9, 10] for generating the MVP model.
Power-balancing dual-port grid-forming power converter control for renewable integration and hybrid AC/DC power systems<|sep|>A major transition in the operation of electric power systems is the replacement of conventional fuel-based power generation interfaced via synchronous machines by distributed renewable generation interfaced via dc/ac power converters. In contrast to machine-interfaced conventional generation that stabilize power systems through their physical properties (e.g., rotational inertia) and controls (e.g., speed governor), today’s converter-interfaced renewables are controlled to maximize energy yield and can jeopardize system reliability [1], [2]. Control strategies for grid-connected dc/ac voltage source converters (VSC) are typically categorized into (i) gridforming (GFM) strategies that impose a stable ac voltage waveform (e.g., frequency and magnitude) at the point of connection, and (ii) grid-following (GFL) controls that require stabilization of the ac voltage waveform (e.g., frequency and magnitude) at their point of interconnection by other devices in the grid (e.g., machines). While this classiﬁcation commonly refers to the converter ac terminal (i.e., ac-GFM and ac-GFL), it is also useful to characterize the dc terminal, i.e., dc-GFM strategies control the dc terminal voltage and dc-GFL strategies require stabilization This work was partially funded by the Swiss Federal Ofﬁce of Energy under grant number SI/501707. I. Suboti´c is with the Automatic Control Laboratory at ETH Z¨urich, Switzerland, D. Groß is with the Department of Electrical and Computer Engineering at the University of Wisconsin-Madison, USA; email:subotici@ethz.ch, dominic.gross@wisc.edu of the dc terminal voltage by other devices connected to the converter dc terminal. Conceptually, standard ac-GFM/dc-GFL strategies impose an ac terminal voltage with variable frequency and magnitude that is adjusted in response to deviations of the ac side active and reactive power from their setpoints. The prevalent acGFM/dc-GFL strategies in the literature are droop-control [3], synchronous machine emulation [4], [5], and (dispatchtable) virtual oscillator control [6]–[8], that provide fast and reliable grid-support and are envisioned to replace synchronous machines as the cornerstone of future large-scale ac power systems [2], [9]. Moreover, ac-GFM/dc-GFL strategies are used in high-voltage dc (HVDC) applications (e.g., to impose a stable ac voltage in offshore ac networks). In contrast, acGFL/dc-GFM strategies control the ac current to stabilize the dc voltage. The prevalent ac-GFL/dc-GFM strategies in the literature rely on a phase-locked loop (PLL) and are used to interface renewable generation with limited or no controllability (e.g., control the dc voltage of a solar photovoltaic (PV) system operating at its maximum power point (MPP) [10]) and HVDC (e.g., control the dc terminal voltage based on the deviation of the dc power from its setpoint [11]). The two approaches are complementary in the sense that acGFM/dc-GFL requires a stable dc voltage and controls the ac voltage, while ac-GFL/dc-GFM require a stable ac voltage and control the dc voltage. While ac-GFL control cannot operate without sufﬁcient ac-GFM resources [12] and is vulnerable to grid disturbances [13], ac-GFM/dc-GFL control fails if the converter dc voltage is not tightly controlled by a power source [14]. Thus, at present, a mix of ac-GFL/dc-GFM and acGFM/dc-GFL control is needed to operate emerging power systems that contain renewable generation as well as a mix of high-voltage ac and HVDC transmission [11]. The resulting complex heterogeneous system dynamics pose signiﬁcant challenges for system operation and stability analysis. The literature on stability analysis of ac power systems with power converters can be categorized into works using analytical [7], [8], [15]–[18] and numerical [9], [12], [14], [19]– [22] approaches. Most of these works only consider ac-GFM converters with the dc terminal modeled as constant voltage source [8], [15], [20], [22], [23] and only few numerical works [9] consider renewable generation with limited controllability. Moreover, none of these works [8], [9], [15], [20], [22], [23] consider synchronous machines, synchronous condensers, GFL converters, or dc transmission. Some numerical studies consider ac-GFM droop and PLL-based ac-GFL [21] or acGFM droop, PLL-based ac-GFL, and synchronous machines [12], but model the dc terminal as constant voltage source. In contrast, [9], [14], [16]–[18] assumed that every converter is connected to a dc power source that implements proportional dc voltage control. Overall, to the best of our knowledge, no analytical stability results are available in the literature that cover power systems containing both ac and dc networks, synchronous machines and synchronous condensers, power generation with limited controllability, and power converters providing common ac-GFM (e.g., primary frequency control) and ac-GFL functions (e.g., maximum power point tracking). Our contribution is two fold. First, we propose the concept of dual-port GFM control that subsumes the functions provided by standard ac-GFM and ac-GFL controls in a simple single universal control strategy that signiﬁcantly reduces system complexity. Second, we leverage the properties of dualport GFM control to develop analytical stability conditions for linear reduced-order models of power systems with ac and dc transmission, renewable generation either at a curtailed operating point or at its MPP, and conventional generation. Speciﬁcally, dual-port GFM control imposes the ac frequency through active power droop [3] and dc voltage droop [17], [24], [25] while ensuring power balancing between the ac and dc terminal by mapping the signals indicating power imbalance (i.e., the ac frequency and dc voltage deviation) between the converter ac and dc terminals (see Fig. 1). This universal control (i) can be applied to all aforementioned technologies, (ii) signiﬁcantly reduces the complexity of the overall system dynamics, and (iii) enables analytical stability conditions for power systems containing a wide range of legacy technologies (e.g., synchronous machines and synchronous condensers) and emerging technologies (e.g., PV, wind power, and HVDC). First, in Sec. II, we develop a graph representation of complex power systems with ac and dc networks and present a tractable reduced-order linearized models of converters, machines, and power sources that, in abstraction, model devices ranging from machines with turbine governor system to synchronous condensers, solar PV systems, and HVDC converters. Based on this model, we propose a novel uniﬁed power-balancing dual-port GFM controller that simultaneously imposes the converter ac voltage and controls its dc voltage while ensuring power balancing between the ac and dc terminals. We also illustrate that, in this general setup, stability conditions that are independent of the network topology can no longer be obtained. Next, in Sec. V, we develop our main theoretical contribution and provide conditions for ac frequency / dc voltage stability of hybrid ac/dc power systems that account for devices (e.g., synchronous condensers, HVDC converters, renewables operating approximately at MPPT) that are not directly connected to a stabilizing power source (e.g., turbine governor system or dc power source providing dc voltage control) and only require partial knowledge of the system topology. Finally, in Sec. VI electromagnetic transient (EMT) simulations of a detailed case study are used to illustrate and validate the results, and Sec. VII concludes the paper. We use R and N to denote the set of real and natural numbers and deﬁne R≥a := {x ∈ R|x ≥ a} and, e.g., R[a,b) := {x ∈ R|a ≤ x < b}. Given a matrix A, AT denotes its transpose. We write A ≽ 0 (A ≻ 0) to denote that A is symmetric and positive semideﬁnite (deﬁnite). For column vectors x ∈ Rn and y ∈ Rm we use (x, y) = [xT, yT]T ∈ Rn+m to denote a stacked vector. Furthermore, In denotes the identity matrix of dimension n, matrices of zeros of dimension n × m are denoted by 0n×m, and 0n and 1n denote column vector of zeros and ones of length n. If a matrix M does not contain a row or column, we call it an empty matrix. The cardinality of a set X ⊂ N is denoted by |X|.
Private Information Retrieval from Non-Replicated Databases<|sep|>Private information retrieval (PIR), introduced in [1], is a canonical problem to study the privacy of users as they download content from public databases. In the classical setting, a user is interested in retrieving a single message (ﬁle) out of K messages from N replicated and non-colluding databases, in such a way that no database can know the identity of the user’s desired ﬁle. The PIR problem has become a vibrant research topic within information theory starting with trailblazing papers [2–8]. In [9], Sun and Jafar introduce the PIR capacity, which is the supremum of the ratio of the number of bits of desired information (L) that can be retrieved privately to the total downloaded information. They characterize the PIR capacity of the classical PIR problem to be CPIR = (1+ 1 NK−1)−1. The fundamental limits of many interesting variants of the problem have been investigated in [10–56]. A common assumption in most of these works is that the entire message set is replicated across all databases. This is crucial for constructing capacity-achieving schemes, as in many existing schemes the undesired symbols downloaded from one database are exploited as side information in the remaining databases, and replication is the key that enables downloading any bit from any database and using it as side information at any other database. However, the replication assumption may not be practical in next-generation storage systems and networks. From a storage point of view, message replication is impractical as it incurs high storage cost, especially for storage systems with a large number of messages or ﬁles with a large size. From a network structure point of view, in next-generation networks where peer-to-peer (P2P) connections will be prevalent, nodes (i.e., databases) may not necessarily possess the same set of messages. These practical scenarios, which challenge the replication assumption, motivate investigating PIR in non-replicated storage systems. In this work, we aim at devising achievable schemes that do not rely on message replication, and at the same time, that are more eﬃcient than the trivial scheme of downloading the contents of all databases. We aim at evaluating the loss in the PIR rate due to non-replication and investigating the interplay between the storage structure and the resulting PIR rate. A few works have considered relaxing the replication assumption: Reference [14] investigates the case when the contents of the databases are encoded via an (N, Kc) MDS code instead of assuming data replication. [14] derives the PIR capacity for this setting, which reveals a fundamental tradeoﬀ between storage cost and retrieval cost. Reference [40] studies the PIR problem from storage constrained databases. In this problem, each database is constrained to store µKL uncoded bits with µ ≤ 1 (as opposed to KL bits needed in replicated databases). [40] shows that symmetric batch caching, which was originally introduced for centralized coded caching systems in [57], results in the largest possible PIR rate under storage constraints. This problem is extended to the decentralized setting in [54], where each database stores µKL bits randomly and independently from any other database. [54] shows that uniform and random bit selection, which was introduced for decentralized coded caching systems in [58], results in the largest possible PIR rate under storage constraints. The work that is most closely related to our work here is [55]. The databases in [55] store diﬀerent subsets of the message set. Diﬀerent from the previous works on non-replication such as [40,54], in [55] databases store full messages and not portions of every message. In particular, [55] investigates the case when every message is replicated across two databases only. This storage system, in this case, can be represented by a graph, in which every two databases are connected via an edge corresponding to the common message. [55] proposes an achievable PIR scheme that is immune against colluding databases, that do not form a cycle in the graph. The scheme in [55] achieves a retrieval rate of 1 N . The work in [55] highlights some interesting insights about the relation between some combinatorial properties of the graph and the immunity against database collusion. In the extended version of [55] in [56], which has appeared concurrently and independently of our work here, an upper bound is proposed to show that their PIR rate is at most a factor of 2 from the optimal value for regular graphs, and the techniques are extended to larger replication factors. In this paper, we consider PIR of a single message out of K messages from N nonreplicated and non-colluding databases. In our formulation, each message appears in R diﬀerent databases, and every database stores M diﬀerent messages. Thus, the storage system is parameterized by (K, R, M, N) such that KR = MN, where K is the total number of messages in the system, R is the replication factor of each message, M is the storage constraint of each database, and N is the number of databases. We focus on the case M = 2. For this case, the storage system can be uniquely speciﬁed by an R-regular graph. In our graph formulation, the messages correspond to the vertices and the databases correspond to the edges. This is in contrast to [55], where R = 2, and the roles of messages and databases are reversed on the graph. Hence, our graph formulation may be considered as the dual graph formulation to [55]. Our goal is to characterize the PIR capacity of this system. First, we derive a general upper bound on the retrieval rate for storage systems described by R-regular graphs. Interestingly, the upper bound depends on the structure of the graph and not only on (K, R, M, N). In particular, the upper bound is related to the longest sequence of databases that cover all of the K messages in the storage system. We specialize the problem further to two classes of graphs, namely, cyclic graphs and fully-connected graphs, where we obtain exact results. In cyclic graphs, all vertices form a circle connected by edges. Therefore, each vertex (a message) emanates two edges (two databases), which means that each message is common among two adjacent databases which are arranged in a cycle. Thus, in this case R = 2, and since M = 2 in this paper, using KR = MN mentioned above, we have, K = N. For this type of graphs, we show that CPIR = 2 K+1. The achievable scheme starts from the greedy algorithm of Sun and Jafar [9] and then compresses the requests to K − 2 databases by replacing the individual symbols of the scheme in [9] by sum of two messages. This compression necessitates exploiting side information even in databases that do not contain the desired messages. In fully-connected graphs, each vertex is connected to all of the remaining K−1 vertices. Therefore, each vertex (a message) emanates K − 1 edges (K − 1 databases), which means that each message resides in K − 1 databases. Thus, in this case R = K −1, and since M = 2, from KR = MN, we have N = K(K −1)/2, i.e., N = �K 2 � . That is, all �K 2 � combinations of two messages appear in a diﬀerent database. In this case, we show that CPIR = min{ 2 novel achievable scheme, which is based on retrieving a single weighted sum (with respect to suﬃciently large ﬁeld) of two symbols from every database. For the comparable cases with [55], our scheme outperforms their scheme in terms of the PIR rate. We note that, in both cyclic and fully-connected graph cases, the PIR capacity converges to zero as N → ∞, which implies a severe degradation in the PIR eﬃciency due to non-replication. Finally, we show an example for a storage system with M = 3. We provide a novel achievable scheme that uses processed side information and outperforms the scheme in [55].
Extragalactic Point Source Search in WMAP 61 and 94 GHz Data<|sep|>The Wilkinson Microwave Anisotropy Probe was launched in 2001 for making precise measurements of the CMB anisotropy (Bennett et al. 2003a). Since on small scales the CMB signal in the WMAP full-sky maps is primarily contaminated by microwave emission from extragalactic point sources, it is important to detect and mask out point-source-contaminated pixels4. Accordingly, two extragalactic point source searches were made in WMAP ﬁrst-year maps and three-year co-added maps5 by the WMAP science team (Bennett et al. 2003c, Hinshaw et al. 2007). The temperature maps were ﬁrst weighed by N1/2 obs and then ﬁltered by bl/(b2 l CCMB l +Cnoise l ) in harmonic space, where Nobs is the number of observations, bl is the WMAP beam transfer function, CCMB l is the CMB angular power spectrum and Cnoise l is the noise power. Peaks that are greater than 5σ in any band of the ﬁltered maps were interpreted as source detections. This procedure yielded a catalog of 208 point sources in the ﬁrst-year data and an enlarged catalog of 323 point sources in the three-year data, with a position uncertainty of 4′ for both searches. A point source mask of nearly 700 objects was then constructed to include all the 323 WMAP directly detected sources, along with the sources from Stickel et al. (1994), sources with 22 GHz ﬂuxes ≥ 0.5 Jy from Hirabayashi et al. (2000), ﬂat-spectrum objects from Ter¨asranta et al. (2001), and sources from the X-ray/radio blazar survey of Perlman et al. (1998) and Landt et al. (2001). Each source was masked to a radius of 0.6◦. On top of that, the power spectra of the unresolved sources were modeled into with A = 0.014±0.003 µK2 sr, β = −2.0, νQ = 40.7 GHz. Here ωl is the window function that gives the combined smoothing eﬀects from the beam and the ﬁnite sky map pixel size, and (i, i′) denotes a pair of DA indices (Hinshaw et al. 2003, 2007). This model was subtracted from the WMAP derived power spectra preceding any cosmology analysis. In this paper, we present a new extragalactic point source search in the WMAP V- and W-band data, using a method that has no CMB dependence. The main purpose of this work is to ﬁnd more point sources in these two bands by lowering the noise level as only 76.2% and 37.5% of the WMAP three-year cataloged sources are identiﬁed with > 2σ conﬁdence in V- and W-band, respectively. Secondly, we are interested in seeing sources that might 4The Hierarchical Equal Area isoLatitude Pixelization (HEALPix) of the sphere is used to deﬁne WMAP map pixels on the sky in Galactic coordinates. A resolution level r=9 is chosen, which corresponds to 3,145,728 pixels with a pixel resolution of 0.115◦ (Bennett et al. 2003b). peak at these high frequency bands, for instance, the so-called Gigahertz-Peaked Spectrum (GPS) sources which are believed to be at the very early state of the evolution of powerful radio sources (O’Dea 1998). A third motivation arises because in a recent re-analysis of WMAP temperature maps by Eriksen et al. (2007), an unexpected systematic discrepancy was found between the V- and W-band power spectra on small angular scales (few percent at ℓ ≳ 300). This was partly explained by Huﬀenberger et al. (2006) to be an over-subtraction of unresolved point sources, and the signiﬁcance of this diﬀerence was reduced from 3σ to 2σ by applying a smaller source correction A = 0.011 ± 0.001 µK2 sr. This amplitude was later revised to A = 0.012 ± 0.005 µK2 sr for ℓ < 500 and A = 0.015 ± 0.005 µK2 sr for ℓ > 500 in Huﬀenberger et al. (2007). Therefore, we also aim at getting a better constraint on the unresolved source contamination by using direct measurements.
Reactive Liquid: Optimized Liquid Architecture for Elastic and Resilient Distributed Data Processing<|sep|>Not only is data a fundamental value in business today but also many companies such as Google and Facebook are built on it. As a result, data processing is the primary concern of these companies. David Reinsel et al. [1] have forecasted that size of the global datasphere, which is all data created and captured and replicated on earth, is going to reach 160 zettabytes by 2025. Hence, the companies are trying to improve their processing power to ﬁt their business for such massive data. Distributed computing is one of the best solutions to increase the processing power. Distributed systems have attracted many IT companies’ attention in the last decades. This concept offers higher processing power by distributing the works among multiple decentralized components in a network. Big data companies have employed distributed technologies and frameworks such as HDFS (Hadoop Distributed File System) and MapReduce to handle the massive volume of data. HDFS, the ﬁle system component of Hadoop, provides fault tolerant data storage distributed on clusters [2]. Distributed systems are exposed to a broad range of network, hardware, and software failures. Hence, resiliency is of crucial importance in distributed processing. Resilient systems should not only be fault tolerant but also heal themselves and recover their original state. Moreover, distributed systems may be overloaded with the ﬂood of users’ requests. Thus, the system should be scalable to such an immense amount of workload, yet scalability alone cannot guarantee the system health. A system should scale itself automatically based on system status. In fact, the system should scale on demand, which is called elasticity. We present an elastic and resilient architecture named Reactive Liquid for distributed data processing to confront distributed data processing challenges. The architecture which this paper proposes is designed based on the Liquid architecture, a nearline data integration architecture, since the Liquid is one of the best big data processing architectures which can ﬁt the context of elastic and resilient distributed data processing. Nevertheless, the Liquid has some limitations to be a perfect solution to this problem. The most burdensome limitation of the Liquid architecture is that the scalability of the processing layer is limited to the messaging layer. The Reactive Liquid surmounts the shortcomings of the Liquid architecture by separating the processing layer from the messaging layer and makes it highly qualiﬁed for elastic and resilient distributed data processing. The rest of the article is organized as follows. Section 2 examines renowned big data architectures and the Reactive Manifesto as guidelines for constructing elastic and resilient systems. Section 3 elaborates on improving the Liquid architecture, to be the basis of our design, and designing the proposed architecture. Section 4 presents the evaluation results of the presented architecture. Section 5 summarizes the main conclusions of our work and introduces a future work.
$f(R)$ Dual Theories of Quintessence : Expansion-Collapse Duality<|sep|>Observational evidence shows that currently the universe is going through a phase of accelerated expansion [1, 2]. This observation necessitates the existence of an exotic ﬂuid that violates the ‘Strong Energy Condition’ [3–5], referred to as the dark energy (DE) [6–8]. The cosmological constant (Λ) model is the simplest implementation of dark energy in Einstein’s general theory of relativity [7, 9, 10]. The ΛCDM (Λ plus Cold Dark Matter) model is also consistent with observations [11–16], it is in fact the simplest and most widely used model to describe dark energy. The energy density contributed by Λ is conventionally associated with the energy density of vacuum. From a theoretical point of view, the Lagrangian for the cosmological constant model, i.e., the Einstein-Hilbert action with an added constant (L ∼ √−g(R−2Λ)), has somewhat a unique status, as in 1 + 3 dimensional spacetime it is arguably the simplest generally covariant Lagrangian, that leads to second order equations of motion [17, 18]. The energy density corresponding to Λ does not vary with the scale factor of the universe and the equation of state parameter (w = P/ρ, where P, ρ are pressure and energy density respectively) associated with Λ is precisely a constant, wΛ = −1. Although wDE = −1 is consistent with observations, deviation from this value is not completely ruled out. Apart from this, the cosmological constant model suﬀers from the ﬁne tuning problem [7, 9, 10]. For example, a naive estimate of the vacuum energy density of quantum ﬁelds, integrated up to the Planck length cut oﬀ, can be given as ρvacuum ∼ ℏk4 Planck ∼ 1074GeV4, whereas, observations predict the energy density of the cosmological constant to be ρΛ ∼ 10−47GeV4, which has a huge discrepancy of the order of 121 with the theoretical estimate. This discrepancy can only be rectiﬁed by ﬁne tuning ρvacuum, which is one of the shortcomings of the ΛCDM model [7, 9, 10]. It has also been pointed out that a higher value of the Λ would make dark energy take over the matter energy density (ρm) at an early epoch. If this would have happened at a suﬃciently early time, the accelerating expansion could have prevented the possibility of structure formations in the universe. The cosmological constant is needed to be ﬁne tuned even in the very early universe, such that the dark energy-matter equality (ρΛ/ρm ∼ O(1)) occurs during the current epoch. To overcome these problems with the cosmological constant, several dynamical models of dark energy have been proposed. Unlike the cosmological constant model, a time-dependent model of dark energy can be ﬁtted with the observations from the current accelerating phase, where it really makes the impact. A scalar ﬁeld minimally coupled to gravity, or quintessence, was introduced to provide a simple dynamical description of dark energy (see, for example, [19–31]). The potential (V ) of the quintessence ﬁeld (ϕ) can be set up to be suﬃciently ﬂat during the current epoch, such that the kinetic term of the ﬁeld becomes negligible (∂µϕ ∼ 0). This makes the equation of state parameter of the ﬁeld, wϕ → −1, hence the quintessence ﬁeld mimics the cosmological constant model at the current epoch (see [6–8] and their references for detailed review). There is also the possibility that an explanation for dark energy might arise from the outside of Einstein’s gravity framework. A simple form of such extended gravity theories is f(R) gravity, where instead of Ricci scalar R, a general function of the curvature scalar, f(R), is used in the Einstein-Hilbert action (see, for example, [32, 32–41] and references therein for recent works in f(R) gravity, for reviews see [42–44]). There are roughly two approaches one can take to describe dark energy vis-a-vis f(R) gravity. The ﬁrst approach is to treat modiﬁed gravity as a ‘correction’ to Einstein’s gravity, such that the ‘correction’ itself is responsible for the acceleration of the universe. Here it is convenient to treat the deviation of the Einstein ﬁeld equation from the modiﬁed ﬁeld equation as an eﬀective energy momentum tensor of a perfect ﬂuid. One of the early examples of this approach can be found in [45] in the context of inﬂation. Later it was extensively studied in the context of late-time acceleration as well (for example, see [46–49]). The cosmological viability of f(R) in such models was discussed in [42, 43, 50]. There is also another approach where the dark energy implemented by a quintessence ﬁeld (in Einstein gravity) can eﬀectively be studied as a pure gravity theory governed by an f(R) action. It is well known that a conformal transformation of the metric of the f(R) action can lead to a theory of Einstein’s gravity with a quintessence ﬁeld, in a conformally connected spacetime (see, for example, [42, 43, 51–63]). The description of the universe, where the gravity action becomes Einstein-Hilbert action, is referred to as the ‘Einstein frame’, whereas, the initial description is referred to as the ‘Jordan frame’. The conformal parameter of this transformation is given by the f(R) model itself. This establishes a duality between f(R) gravity without quintessence ﬁeld in Jordan frame and Einstein gravity with quintessence ﬁeld in Einstein frame, such that, an f(R) function corresponds to a quintessence potential. Due to this duality, one can treat a quintessence model in Einstein gravity as an f(R) gravity model, without the need of a quintessence ﬁeld. In this paper, we are interested mainly in quintessence models with an equation of state w(a) = w0 − w′ ln a. The corresponding quintessence potential has a closed and relatively simple analytical form. This parameterization was ﬁrst introduced in [64] to ﬁt with three tracking quintessence models. It was shown to be a good ﬁt with observations in the redshift range z ≲ 4. This model was further extended in [65] to account for observation from a wider range of redshift parameter. Models with logarithmic w(a) were further constrained from the SNIa+BAO+H(z) data in [66, 67]. We obtain a class of f(R) theories that can recover these potentials in Einstein frame. The reconstruction of the f(R) function can be broken in to two parts. For the near current time in the Einstein frame, we ﬁnd perturbative solutions of f(R) which are valid in the small Jordan frame curvature limit. From this we estimate the perturbative solution which is most suited in the current epoch of the Einstein frame universe. For the distant future in Einstein frame, we obtain an asymptotic solution for f(R). We further show that the logarithmic parameterization of w(a) belongs to a class of quintessence models for which the Jordan frame scale factor has a ﬁnite maximum value. In the late time limit of the Einstein frame universe, the Einstein frame scale factor increases indeﬁnitely, while the Jordan frame universe collapses after attaining a maximum. A general condition for the collapse of the Jordan frame is then obtained which takes into account other components of the universe. Using this we ﬁnd that the presence of dust prevents the Jordan frame collapse. Finally, we show that the introduction of positive spatial curvature [68] may still allow the expansion-collapse duality in the presence of non-relativistic matter. The general condition for the expansion-collapse duality of the Einstein and Jordan frame can be used in further studies to explore other viable quintessence models. An expanding universe with quintessence ﬁeld can also be looked at as a collapsing universe with diﬀerent equations of motion. Such a correspondence between expanding and collapsing geometries can have applications in studies of growth of cosmological perturbations. For example, one can study the back reaction of the curvature and matter perturbations in our universe collectively as a gravitational perturbation of a collapsing geometry leading to a good estimate of back reaction at late times. The paper is organized as follows. In Sec. 2 we review the quintessence model and the reconstruction of the quintessence ﬁeld potential. In Sec. 3 we obtain the analogous f(R) theories consistent with the quintessence model; ﬁrst perturbatively near current era and then for remote future. A relation between the scale factors of the FRW universes in the Einstein and the Jordan frame is then obtained in Sec. 4. Here we demonstrate that at late times the Jordan frame universe collapses back unlike the universe driven by the quintessence ﬁeld in the Einstein frame. In Sec. 5 we derive a general condition for the expansion-collapse duality and further explore the eﬀect of dust and spatial curvature on the collapse of the Jordan frame. We conclude the paper with a summary and discussion of implications in Sec. 6.
Nuclear effects in neutral current quasi-elastic neutrino interactions<|sep|>The MiniBooNE collaboration has recently collected an extensive data set of quasielastic neutrino nucleus scattering events, in both the chargedcurrent (CCQE) [1] and neutral current (NCE) [2] channels, using a Carbon target. In the CCQE channel, quasielastic neutrino-nucleon interactions are described in terms of the vector form factors F p,n 1 (Q2) and F p,n 2 (Q2) (Q2 = −q2, q being the four-momentum transfer, while the superscripts p and n correspond to proton and neutron, respectively), that have been precisely measured in electron-proton and electron-deuteron scattering experiments [3], and the axial form factor FA(Q2) [4, 5, 6]. In addition, NCE interactions are also aﬀected by the form factors F s 1 , F s 2 and F s A, arising from strange quark contributions [7, 8, 9, 10]. The results of recent experiments [7] indicate that F s 1, F s 2 are vanishing, whereas the axial form factors FA and F s A are assumed to be of dipole form, and their Q2-dependence is parametrized in terms of the axial mass MA. The measured cross sections turn out to be consistently larger than the predictions of Monte Carlo simulations carried out using the relativistic Fermi gas (RFG) model of the nucleus and the value of the axial mass resulting from the world average of the deuterium data, MA = 1.03 GeV [6]. In order to bring the predictions of the RFG model into agreement with the data, the authors of Refs.[1, 2] use a signiﬁcantly larger value of the axial mass, MA > ∼ 1.35 GeV, and introduce the additional parameter κ, meant to improve the treatment of Pauli blocking. The K2K collaboration also reported a large value of the axial mass, MA ∼ 1.2 GeV, resulting form the analysis of its sample of CCQE events [11]. Moreover, the best ﬁt to the neutral current data is obtained using a non vanishing strange quark contribution ∆s, determining the value of F s A at Q2 = 0 [2]. It has been suggested that the large value of MA may be regarded as an eﬀective axial mass, modiﬁed by nuclear eﬀects not taken into account in the RFG model [1]. However, the results obtained using more advanced models appear to rule out this explanation. In fact, numerical calculations carried out using realistic nuclear spectral functions, extensively employed in the analysis of electron-nucleus scattering data [12], indicate that reproducing the CCQE measured cross sections requires an even larger value of MA [13, 14]. The purpose of this work is the extension of the spectral function approach of Refs.[13, 14] to the description of NCE interactions and the quantitative analysis of the MA and ∆s dependence of the resulting cross sections. The main elements of our approach are outlined in Section 2, while the numerical results are discussed in Section 3. Finally, in Section 4 we summarize our ﬁndings and state the conclusions.
Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic Experiences for Robot Action Execution<|sep|>Humans are ingenious: We have unique abilities to predict the consequences of observed actions, remember the most relevant experiences from the past, and transfer knowledge from previous observations in order to adapt to novel situations. The episodic memory which encodes contextual, spatial and temporal experiences during development plays a vital role to introduce such cognitive abilities in humans. A core challenge in cognitive robotics is compact and generalizable mechanism which allow for encoding, storing and retrieving spatio-temporal patterns of visual observations. Such mechanisms would enable robots to build a memory system, allowing them to efﬁciently store gained knowledge from past experiences and both recalling and applying such knowledge in new situations. Inspired by infants that learn by observing and memorizing what adults do in the same visual setting, we investigate in this paper how to extend cognitive abilities of robots to autonomously infer the most probable behavior and ultimately adapt it to the current scene. Considering the situation of the humanoid robot ARMAR-IIIa standing in front of a table with a juice carton (see Fig. 1) one can ask what the most suitable action is and how it would best be performed. To achieve this goal, we introduce a novel deep neural network architecture for encoding, storing, and recalling past action experiences in an episodic memory-like manner. The The research leading to these results has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 641100 (TimeStorm) and from the German Research Foundation (DFG: Deutsche Forschungsgemeinschaft) under Priority Program on Autonomous Learning (SPP 1527). †The authors are with the Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany. ‡Eren E. Aksoy is with with the School of Information Technology, Halmstad University, Sweden ∗The ﬁrst two authors contributed equally to this work. proposed deep network encodes observed action episodes in a lower-dimensional latent space. Such a formulation in the latent space allows robots to store visual experiences, compare them based on their conceptual similarity and retrieve the most similar episodes to the query scene or action. Further, the same network leads to predict and generate the next possible frames of a currently observed action. To the best of our knowledge, this is the ﬁrst study introducing that vision-based cognitive abilities concerning action representing, storing, memorizing, and predicting can be achieved in a single coherent framework. We hypothesize that latent subsymbolic encodings that our network generates from visual observations are rich and descriptive enough to be compared with those collected from previously experienced episodes. In this way, ARMAR-IIIa can trace all previous observations and select the most similar episode (e. g. “pushing the juice" or “grasping the juice") in the latent space. The robot can further generate similar behavior by adapting to new situations based on memorized action representations. Contribution: (1) We implement a new deep network to encode action frames into a low-dimensional latent vector space. (2) Such a vector representation is used to reconstruct the action frames in an auto-encoder manner. (3) We show that the same latent vectors can also be employed to predict future action frames. (4) We introduce a mechanism for matching and retrieving visual episodes and provide an evaluation of the proposed method on two action datasets. (5) Finally, we demonstrate how this meachanism can facilitate case-based reasoning for robotic object manipulation in an unstructured real-world scenario.
The Generalised Colouring Numbers on Classes of Bounded Expansion<|sep|>The colouring number col(G) of a graph G is the minimum k for which there is a linear order <L on the vertices of G such that each vertex v has back-degree at most k − 1, that is, v has at most k − 1 neighbours u with u <L v. The colouring number is a measure for uniform sparseness in graphs: we have col(G) = k if and only if every subgraph H of G has a vertex of degree at most k − 1. Hence, provided col(G) = k, not only G is sparse, but also every subgraph of G is sparse. The colouring number minus one is also known as the degeneracy. ∗This work was initiated during Sebastian Siebertz’s visit at the Institute of Informatics of the University of Warsaw, which was supported by the Warsaw Centre of Mathematics and Computer Science. Michał Pilipczuk is supported by the Foundation for Polish Science (FNP) via the START stipend programme. Stephan Kreutzer, Roman Rabinovich and Sebastian Siebertz’s research has been supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (ERC Consolidator Grant DISTRUCT, grant agreement No 648527). Recently, Nešetřil and Ossona de Mendez introduced the notions of bounded expansion [12] and nowhere density [14] as very general formalisations of uniform sparseness in graphs. Since then, several independent and seemingly unrelated characterisations of these notions have been found, showing that these concepts behave robustly. For example, nowhere dense classes of graphs can be deﬁned in terms of excluded shallow minors [14], in terms of uniform quasi-wideness [2], a notion studied in model theory, or in terms of a game [8] with direct algorithmic applications. The generalised colouring numbers admr, colr, and wcolr were introduced by Kierstead and Yang [11] in the context of colouring and marking games on graphs. As proved by Zhu [17], they can be used to characterise both bounded expansion and nowhere dense classes of graphs. The invariants admr, colr, and wcolr are deﬁned similarly to the classic colouring number: for example, the weak r-colouring number wcolr(G) of a graph G is the minimum integer k for which there is a linear order of the vertices such that each vertex v can reach at most k − 1 vertices w by a path of length at most r in which w is the smallest vertex on the path. The generalised colouring numbers found important applications in the context of algorithmic theory of sparse graphs. For example, they play a key role in Dvořák’s approximation algorithm for minimum dominating sets [4], or in the construction of sparse neighbourhood covers on nowhere dense classes, a fundamental step in the almost linear time model-checking algorithm for ﬁrst-order formulas of Grohe et al. [8]. In this paper we study the relation between the colouring numbers and the above mentioned characterisations of nowhere dense classes of graphs, namely with uniform quasi-wideness and the splitter game. We use the generalised colouring numbers to give a new proof that every bounded expansion class is uniformly quasi-wide. This was ﬁrst proved by Nešetřil and Ossona de Mendez in [13]; however, the constants appearing in the proof of [13] are huge. We present a very simple proof which also improves the appearing constants. Furthermore, for the splitter game introduced in [8], we show that splitter has a very simple strategy to win on any class of bounded expansion, which leads to victory much faster than in general nowhere dense classes of graphs. Every graph G from a ﬁxed class C of bounded expansion satisﬁes wcolr(G) ≤ f(r) for some function f and all positive integers r. However, the order that witnesses this inequality for G may depend on the value r. We say that a class C admits uniform orders if there is a function f : N → N such that for each G ∈ C there is one linear order that witnesses wcolr(G) ≤ f(r) for every value of r. We show that every class that excludes a ﬁxed topological minor admits uniform orders that can be computed eﬃciently. Finally, based on our construction of uniform orders for graphs that exclude a ﬁxed topological minor, we provide an alternative proof of a very recent result of Eickmeyer and Kawarabayashi [7], that the model-checking problem for successor-invariant ﬁrst-order (FO) formulas is ﬁxed-parameter tractable on such classes (we obtained this result independently of but later than [7]). Successor-invariant logics have been studied in database theory and ﬁnite model theory, and successor-invariant FO is known to be more expressive than plain FO [15]. The model-checking problem for successor-invariant FO is known to be ﬁxed-parameter tractable parameterized by the size of the formula on any graph class that excludes a ﬁxed minor [6]. Very recently, this result was lifted to classes that exclude a ﬁxed topological minor by Eickmeyer and Kawarabayashi [7]. The key point of their proof is to use the decomposition theorem for graphs excluding a ﬁxed topological minor, due to Grohe and Marx [9]. Our approach is similar to that of [7]. However, we employ new constructions based on the generalised colouring numbers and use the decomposition theorem of [9] only implicitly. In particular, we do not construct a graph decomposition in order to solve the model-checking problem. Therefore, we believe that our approach may be easier to extend further to classes of bounded expansion, or even to nowhere dense classes of graphs.
Automated and Explainable Ontology Extension Based on Deep Learning: A Case Study in the Chemical Domain<|sep|>Ontologies represent knowledge in a way that is both accessible to humans and is machine interpretable. Reference ontologies provide a shared vocabulary for a community, and are successfully being used in a range of different domains. Examples include the OBO ontologies in the life sciences [1], the Financial Industry Business Ontology for the financial domain [2], and the Open Energy Ontology in the energy domain [3]. While these ontologies differ in many respects, they share one important feature: they are manually created by experts using a process by which each term is manually added to the ontology including a textual definition, relevant axioms, and ideally some additional documentation. Often, this process involves extensive discussions about individual terms. Hence, developing such ontologies is a time-intensive and expensive process. This leads to a challenge for ontologies that cover a large domain. For example, the ChEBI (Chemical Entities of Biological Interest) ontology [4] is the largest and most widely used ontology for the domain of biologically relevant chemistry in the public © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). domain. It currently (as of June 2021) contains 59,122 fully curated classes, which makes it large in comparison to other reference ontologies. ChEBI is largely manually maintained by a team of expert curators. This is an essential prerequisite for its success, because it enables it to capture the terminology and classification logic shared by chemistry experts. However, the number of chemicals covered by ChEBI is dwarfed by the 110 million chemicals in the PubChem database [5], which itself is not comprehensive. The manually curated portion of ChEBI only grows at a rate of around 100 entries per month, thus will only ever be able to cover a small fraction of the chemicals that are in its domain. ChEBI tries to navigate this dilemma by extending the manually curated core part of the ontology automatically using the ClassyFire tool [6]. This approach has tripled ChEBI’s coverage to 165,000 classes (as of June 2021). However, there are limitations to this approach. Firstly, ClassyFire uses a different underlying classification approach to ChEBI (e.g. conjugate bases and acids are not distinguished), thus, mapping to ChEBI loses classification precision. More importantly, ClassyFire is rule-based and while the extension of the ontology is automated, the creation and curation of the ClassyFire’s rules is not. This limits the scalability of this approach. Somewhat inspired by ChEBI’s workflow, we suggest navigating the ontology scaling dilemma by using a new kind of approach to ontology extension, which transfers the design decisions of an existing ontology analogously to new classes and relations. Our starting point is an existing, manually curated reference ontology. We suggest the use of machine learning methods to learn some of the criteria that the ontology developers adopted in the development of the ontology, and then use the learned model to extend the ontology to entities that have not been covered by the manual ontology development process yet. We will illustrate this approach in this paper for the chemistry use case by training an artificial neural network (with a Transformer-based architecture) to automate the extension of ChEBI with new classes of chemical entities. The approach has several benefits: since it builds on top of the existing ontology, the extension will preserve the manually created consensus. Moreover, the model is trained solely on the content of the ontology itself and does not rely on any external sources. Finally, as we will see, the chosen architecture allows explanation of the choices of the neural network, and, thus to validate the trained model to some degree by manual inspection. In the next two sections we discuss related work and the overall methodology that we are using to train a model for classifying new classes of chemical entity as subclasses of existing classes in ChEBI.
Beam Designs for Millimeter-Wave Backhaul with Dual-Polarized Uniform Planar Arrays<|sep|>Cell densiﬁcation has been a most direct and practical way of supporting the exponential growth of mobile devices and data rates [1]–[3]. With more cells, both macro and small cells, it is important to deploy cost-efﬁcient backhauls among cells since conventional backhauls using optical ﬁbers are expensive. In terms of cost efﬁciency, a simple solution is using wireless links for backhauls. Especially, millimeter-wave (mmWave) wireless communications can be considered for backhauls satisfying both high data rates and bearable cost [2]–[4]. MmWave communications use a carrier frequency of 30 to 300 GHz and the corresponding wavelength of 1 to 10 mm. With its huge bandwidth, mmWave communications can support enormous data rates [5]–[7]. An important characteristic of mmWave systems is its high attenuation [7], [8], which requires sharp beam patterns to concentrate signal power and compensate for the attenuation. To enjoy the beneﬁt of the large bandwidth, hence, beamforming techniques are necessary to overcome harsh mmWave environments. This work was supported by the National Research Foundation (NRF) grant funded by the MSIT of the Korea government (2019R1C1C1003638). S. Kim and J. Choi are with the Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (e-mail: {loehcusmik, junil}@kaist.ac.kr). J. Choi is the corresponding author. J. Song is with the Department of Electrical Engineering, University of Ulsan (e-mail: jihosong@ulsan.ac.kr). Many beamforming designs have been proposed under various environments. Beamformers were designed to maximize capacity using limited information of channels at the transmitter in [9], [10]. These beamformers were based on an assumption of rich scattering environments, e.g., sub-6 GHz spectrum, and the resulting beamformers rarely have physical beam-like patterns. In the mmWave systems, on the contrary, channels can be represented with their dominant lineof-sight (LOS) component [11] or sum of a few dominant components [12], [13]. This leads the beamformer design problem to consider graphical or geometrical shape. The codebook-based beamformings in [2], [14], for example, ﬁnd the best beamformer by gradually narrowing the beamwidth of possible beamformers. In [15]–[18], based on the small numbers of channel components, codebook designs for the beam alignment and channel estimation are proposed. In this paper, we also approximate the channel as its LOS component and design beamformers based on physical beam patterns. Since the beamforming depends on the combination of weights for each antenna, use of multiple antennas is essential. In mmWave system, by virtue of its extremely small wavelength, it is possible to deploy large numbers of antennas of uniform linear arrays (ULAs) or uniform planar arrays (UPAs) within a small form factor. Considering the use of numerous antennas in mmWave systems, the digital beamforming is practically infeasible due to the cost and power consumption of RF chains [19]–[21]. One feasible solution is the analog beamforming, which requires only one RF chain in return for abandonment of amplitude control and variety of beam pattern shapes [2], [7]. The hybrid beamforming, which reduces the use of RF chains by conjoining the digital and analog beamformings, is another practical solution to accomplish both the feasibility and diversity of beam pattern shapes [13], [15]–[18], [22]–[26]. For example, the algorithm in [17] iteratively updates a beamformer to reduce the ripple and raise average gain of beam pattern in the covering region, and the beamformer in [18] is designed to minimize the mean squared error (MSE) of its beam pattern in the covering region. The two designs, however, are focused on the singlepolarization ULA structure. Further increase of the number of the antennas in a limited form factor is possible by exploiting dual-polarization antennas [27]. The channel of dual-polarization antennas, though, is fundamentally different from that of single-polarization antennas [28]. For example, the imbalance of channel gains and the orientation difference between the transmit (Tx) and receive (Rx) antennas need to be considered, each of which changes the structure of channel and causes additional randomness. The complicated channel structure hinders the decomposition of Tx and Rx beamforming gain, making it difﬁcult to design Tx and Rx beamformers. In [29], cross-polarization discrimination (XPD), which is a parameter about the dual-polarization, is considered in the beamformer design. In [28], the beamformer of dual-polarization is generated by modifying the beamformer of single-polarization based on the structure of the dualpolarization channel. Most of previous works on the dualpolarization beamforming, however, are based on the digital beamforming with ULAs. In this paper, we propose the hybrid beamforming designs for mmWave multiple-input multiple-output (MIMO) backhaul systems with dual-polarization UPAs. To the best of authors’ knowledge, the hybrid beamforming design for mmWave MIMO system with dual-polarization UPAs has not been considered before. We propose ﬂexible beamforming design methods that can deal with variable backhaul links. Although the backhaul links are usually ﬁxed, and using a preﬁxed beamformer may be sufﬁcient for communications, the new installation, movement, or demolition of small cell base stations (BSs) would be frequent, which requires a new beamformer for each event. To design a beamformer, we ﬁrst deﬁne an ideal beam pattern, and then beamformers are optimized to mimic the ideal beam pattern. As speciﬁc examples of optimization criterion, squared error (SE) and magnitude of inner product (MIP) are considered respectively. Based on the common structure of optimization solutions, we propose a uniﬁed method to generate the optimal dual-polarization beamformer from the optimal single-polarization beamformer sharing the same optimality. The proposed design methods depend on partial channel information, and we also propose the use of pilot sequences to measure the required channel information. Numerical results show that the SE beamformer has the most uniform beam patterns and the MIP beamformers has the highest average and peak beamforming gain for the dualpolarization UPA structure. In the rest of the paper, system and channel models are described in Section II. The details of proposed beamforming designs are explained in Section III. The numerical results of the proposed beamformers are shown in Section IV, and the conclusion follows in Section V. Notations: N, R, and C represent set of natural numbers, real numbers, and complex numbers. Matrices and vectors are written in bold face capital letters A and bold face small letters a. (·)∗, (·)T, and (·)H mean element-wise conjugate, transpose, and Hermitian of the corresponding matrix or vector. ⊗ and ⊙ represent the Kronecker product and the Hadamard product. The b-th component of the vector a is remarked as (a)(b). Ia is the a×a identity matrix, ea,b is the b-th column of the identity matrix Ia, and 1a represents the a × 1 all one vector. The concatenation of matrices is denoted as [A, B] where A and B have the same number of rows. Λmax(·) is the maximum eigenvalue of the corresponding matrix, and vmax(·) is the maximum eigenvector of the corresponding matrix.
Modeling Transitivity in Complex Networks<|sep|>Most of real-world networks such as World Wide Web, social networks, Internet and biological networks exhibit structural properties which are not in either entirely regular or purely random graphs. For example, graphs produced by the model of Paul Erd˝os and Alfr´ed R´enyi (the ER model) Erds and Rnyi (1960), do not have the two important properties observed in many real-world networks. The ﬁrst property is related to the degree distribution. In a network, the degree distribution is deﬁned as the probability distribution of the degrees of vertices over the whole network. In many real-world networks a power-law distribution is observed. More formally, the probability that the degree of a vertex is k is proportional to k−γ. Networks with this property are called scale-free networks. However, the degree distribution of graphs produced by the ER model converges to a Poisson distribution. The second property is related to the clustering coefﬁcient. Clustering coefﬁcient is used to measure how well vertices in a network tend to be clustered together. In most of real-world networks, vertices tend to create tight groups characterized by dense ties Watts and Strogatz (1998). However, in the ER model, every two vertices are connected with a constant and independent probability and therefore, the model generates graphs with a low clustering coefﬁcient. The β model (the Watts-Strogatz model), proposed by Watts and Strogatz Watts and Strogatz (1998), produces graphs with the small-world property and high clustering coefﬁcient. In small world networks, the distance between each pair of vertices is proportional to the logarithm of the number of vertices in the network. However, the β model produces an unrealistic degree distribution. The Barab´asi-Albert (BA) model, proposed by Albert-L´aszl´o Barab´asi and R´eka Albert produces scale-free graphs Barabasi and Albert (1999). The model is based on two important concepts: growth and preferential attachment. Growth means that the number of vertices in the network increases over time. Preferential attachment means that vertices with higher degree are more likely to receive new edges. The degree distribution of a graph resulting from the BA model is a power-law in the form of Pr[k] ∼ k−3. However, the clustering coefﬁcient of graphs produced by the BA model is signiﬁcantly lower than the clustering coefﬁcient of real-world networks. Takemoto and Oosawa Takemoto and Oosawa (2005) propose a model for evolving networks by merging complete graphs (cliques) as building blocks. The model shows power-law degree distribution, power-law clustering spectra and high average clustering coefﬁcients independent of the size of network. However, in most cases, real-world networks are evolved in a different way: they usually grow during the time by obtaining new vertices, rather than by merging complete graphs. An important source of high clustering coefﬁcient in networks is transitivity. Transitivity means if u is connected to v and v is connected to w, the probability of having a connection between u and w is higher than any other pair of vertices in the network. Most of edges in real-world networks are local and they are drawn between vertices which have a common neighbor Leskovec et al. (2008). The model of Newman et al. (2001) incorporates transitivity and generates graphs with high clustering coefﬁcient. However, it produces bipartite networks which are limited to situations like company directors and movie actors. Clustering coefﬁcient in the graphs produced by the model of Li and Maini (2005) is still signiﬁcantly lower than clustering coefﬁcient of real-world networks. Leskovec et.al. Leskovec et al. (2008) propose several mechanisms for modeling transitivity in complex networks. However, they do not provide any theoretical argument for the clustering coefﬁcient of the mechanisms. The importance of such a theoretical analysis is that it guarantees that the model will reﬂect important properties of real-world networks, since a high clustering coefﬁcent, independent of the network size, is seen in many real-world networks. On the other hand, for most of network models, it is not easy to theoretically analyze the clustering coefﬁcent. For example, up to now, clustering coefﬁcient of BA networks has only been determined by numerical simulations1, and it is known to be very difﬁcult to theoretically analyze it. Therefore, it is interesting to develop a model for transitivity in complex networks such that its clustering coefﬁcent can be veriﬁed by theoretical arguments. In this paper, we present the η model for modeling transitivity in complex networks. At every time interval t, the network obtains a new vertex and the new vertex is connected to some existing vertices. This step is similar to the BA model. Then, each vertex is selected with a probability proportional to its degree . If it is selected, then a pair of its neighbors are chosen randomly and an edge is drawn between them. The model has two adjustment parameter η and m. We theoretically analyze the model and prove that it produces networks with power-law degree distribution, high clustering coefﬁcient and the small-world property. Compared to the clustering coefﬁcient of random graphs or graphs produced by existing scale-free models, the clustering coefﬁcient of the η model is signiﬁcantly higher. In particular, by theoretical arguments, we prove that it is independent of the network size and depends solely on parameters like η and m. We also empirically evaluate the model and show that it can precisely simulate networks from different domains (biology,
Using Duality in Circuit Complexity<|sep|>In Boolean circuit complexity, deriving lower bounds on circuit size and depth has up to now shown to generally be diﬃcult. While there have been results proving lower bounds, we still lack methods that are applicable in general. Algebraic methods have improved our understanding of circuit complexity. Here we are especially interested in the constant depth circuit complexity classes AC0, CC0, and ACC0 that have tight connections to algebra via programs. For instance the class AC0 is equal to the class of languages recognized by polynomial-length programs over ﬁnite aperiodic monoids [6]. Using these connections allowed the usage of algebraic methods in circuit complexity [5, 4, 3, 14, 21]. For an overview see the book of Straubing [19]. It is a well known method from algebra to characterize regular language classes by identities and has successfully been applied to describe varieties of regular languages stemming from various logic classes (see for example the book of Pin [16]). Recently, Gehrke, Grigorieﬀ and Pin generalized the approach to work with non-regular language classes [9]. While many concrete characterizations via identities or equations exist for classes of regular languages (see for example the book of Almeida [1]), only few concrete examples are known for non-regular classes [10]. One of the main diﬃculties is, that these equations hold for all languages in a circuit class and not only the regular ones, for which we have other manageable descriptions. Furthermore, the question arises how to achieve an abstract method to obtain equations for circuit classes, instead of calculating them concretely for each class. As the result of [9] shows the existence of equational descriptions for arbitrary Boolean algebras, circuit classes form suitable candidates. However, it is not clear how to obtain these equations ina constructive way. The method presented in the paper allows us to obtain equations for more complex classes of circuits, starting with equations from simple © Silke Czarnetzki and Andreas Krebs; licensed under Creative Commons License CC-BY Leibniz International Proceedings in Informatics Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany
The large deviations of the whitening process in random constraint satisfaction problems<|sep|>II. Deﬁnitions and main results 6 A. The hypergraph bicoloring/NAESAT problem and its known properties 6 B. From Warning Propagation to the whitening dynamics 11 C. The whitening dynamics for typical solutions of the hypergraph bicoloring problem 13 D. Large deviations of the whitening dynamics 14 E. Main results 17 III. A statistical mechanics treatment 19 A. Factor graph representation 19 B. The factorized solution on regular hypergraphs 21 C. The T = 1 case 23 1. Resolution of the equations 23 2. Phase diagram 24 D. The ﬁxed points of the whitening process (the limit T → ∞) 24 1. The entropy of unfrozen solutions for l > lr 26 2. The frozen solutions of the ﬁrst kind 28 3. The frozen solutions of the second kind 29 E. The intermediate case 1 < T < ∞ 29 F. The locked solutions (θ = 1) 30
A linear-time algorithm for semitotal domination in strongly chordal graphs<|sep|>A dominating set in a graph G = (V, E), is a set D ⊆ V , such that any vertex not in D is adjacent to a vertex in D. The minimum size of a dominating set is called domination number, denoted by γ(G). The MINIMUM DOMINATION problem involves computing a minimum cardinality dominating set of a graph G. The domination number is one of the most studied parameter in the graph theory. A thorough treatment and detailed study on domination can be found in the books [8, 9, 10, 11]. Due to numerous applications in the real world problems, many researchers introduced several variations of domination by imposing one or more additional conditions on dominating set. One of the most important variation of domination is total domination. In a graph G = (V, E), without isolated vertices, a dominating set D ⊆ V is called a total dominating set (TD-set in short), if G[D], the graph induced by D in G has no isolated vertex. The total domination number, denoted by γt(G), is the cardinality of a minimum total dominating set of G. The MINIMUM TOTAL DOMINATION problem requires to compute a total dominating set of a graph G with no isolated vertex, of size γt(G). See [13, 21] for the detailed results on total domination. Goddard, Henning, and McPillan, introduced a relaxed notion of total domination, called semitotal domination in [7] and further studied in [6, 12, 14, 15, 16, 17, 18, 19, 20, 23, 28, 29, 32], from both algorithmic and combinatorial point of view. In a graph G with no isolated vertices, a semitotal dominating set(in short, semi-TD-set) is a dominating set D ⊆ V such that for every vertex u ∈ D, there is another vertex v ∈ D, such that the distance between u and v is at most two in G. The semitotal domination number, denoted by γt2(G), is the cardinality of a minimum semi-TD-set of G. It follows directly from deﬁnitions that every total dominating set is a semitotal dominating set. Hence, for a graph G with no isolated vertices, we have the following relation between the three parameters: Therefore, the semitotal domination number is squeezed between two important parameters, domination number and total domination number. The minimum semitotal domination problem and and its decision version are deﬁned as follows: The SEMITOTAL DOMINATION DECISION problem in NP-complete [7] for general graphs. The problem remains NP-complete, even when restricted to chordal graphs, chordal bipartite graphs, and planar graphs [20]. On positive side, we have polynomial-time algorithms to compute a minimum cardinality semi-TD-set in trees [7], interval graphs [20, 28] and block graphs [19]. Galby et al. [6] proved that, a minimum semi-TD-set can be computed in polynomial-time in bounded MIM-width graphs, which includes many important graph classes. The complexity status of the problem in some well known graph classes is shown in Fig. 1. In the ﬁgure, P stands for polynomial-time and NPC stands for NP-complete. The complexity status of the problem in graph classes with question mark is still unknown. Henning and Pandey studied the approximation hardness of MINIMUM SEMITOTAL DOMINATION problem [20]. They proved that the problem can not be approximated within (1 − ϵ)ln(|V |) for any ϵ > 0, unless NP ⊆ DTIME(|V |O(loglog(|V |)). On other side, they proved that the MINIMUM SEMITOTAL DOMINATION problem is in the class log-APX. They also proved that the problem is APX-complete for the bipartite graphs with maximum degree 4. Although, we have noticed that the semitotal domination number is squeezed between domination number and total domination number. But, MINIMUM SEMITOTAL DOMINATION problem and MIN IMUM TOTAL DOMINATION problem differs in complexity, see [20]. Indeed, the MINIMUM TOTAL DOMINATION problem is polynomial-time solvable in chordal bipartite graphs but decision version of MINIMUM SEMITOTAL DOMINATION is NP-complete for chordal bipartite graphs. Further, Galby et. al. proved that it is NP-hard to decide γt2(2) = γt(G), even when G is a planar graph with degree at most 4, see [6]. As the MIM-width of strongly chordal graphs is unbounded, the complexity of the problem was left open in strongly chordal graphs by Galby et al. [6]. Henning and Pandey in [20], also asked to ﬁnd the complexity status of the problem in strongly chordal graphs. In this paper, we prove that the MINIMUM SEMITOTAL DOMINATION problem can be solved in linear-time in strongly chordal graphs. The further structure of the paper is as follows. In Section 2, we discuss some notations and deﬁnitions. In Section 3, we discuss strongly chordal graphs and their properties. In Section 4, we design a linear-time algorithm to compute a semi-TD-set in strongly chordal graphs. Finally, Section 5, concludes the paper.
Elastic Coulomb breakup of $^{34}$Na<|sep|>Exotic nuclei are a result of movement away from the valley of stability towards the drip line regions, where the increase in neutron (or proton) excess and low binding energy changes the nuclear structure, and leads to unconventional properties for the nuclei concerned. Some of these exotic nuclei exhibit special character and are known as halo nuclei [1]. Nuclear halos are, essentially, a threshold eﬀect occurring because of the presence of a bound state near the continuum in the energy spectrum [2] and as expected, neutron halos are more pronounced than proton halos owing to the large Coulomb barrier in the latter case. Characterized by a large spatial and hence, a small momentum distribution, they reveal interesting aspects about two- and three-body (Borromean) systems [3]. Though they are highly unstable in most cases, the fact that they have non-negligible reaction rates in stellar plasma [4] has garnered the interest of experimentalists all over the world [5–8]. Considerable interest has also been shown in exotic-halo nuclei ever since they ﬁrst came to light [7]. The theoretically explained models of the various reaction cycles, viz., the pp-chains, CNO cycle, the r-, and s-processes, etc., led to the study of nuclei near the drip line in the medium mass region as they are also speculated to form connecting links within these chains which produce energy to power the stars [4, 9–11]. In Ref. [2], the authors explicitly state that it would be wise to carry out detailed research with exotic nuclei in the medium mass region. Though these drip line halo nuclei have indeed been studied quite extensively for lower atomic masses [2, 7, 12–17], the same, however, cannot yet be said about the medium mass region, where investigations have been comparatively fewer, but equally important nevertheless. As has been customary with exotic nuclei, the studies in this region near the drip line [18–20] have shown interesting characteristics in terms of their ground state conﬁgurations. Ground state (g.s.) conﬁgurations are important as their knowledge can lead one to the information about the isospin dependence of eﬀective nuclear interaction, variation in the traditional shell structure or vanishing of the shell gaps, because even the ‘magic numbers’ valid for the valley of stability get modiﬁed away from it [21–25]. Normally, the nucleons inside the nucleus are expected to ﬁll up according to the conventional shell model, but near the drip line the usual shell gaps break. Conventionally, one would expect the domination of the f7/2 orbit to form the ground state in nuclei in the vicinity of N = 20 - 28. This would mean a large centrifugal barrier with l = 3. But this does not favour the formation of halos as they are characterized by small angular momentum values (l = 0, 1) to limit the eﬀect of centrifugal barrier [26]. Nevertheless, in the ‘island of inversion’ [27], rapid changes in the shell structure lead to conﬁguration mixing due to the ν(sd)−2(fp)2 intruder conﬁgurations, with ν representing the neutron. This conﬁguration reversal results in a deformation in the nuclei away from the valley of stability. Deformation could be a factor for heavier nuclei surviving within the neutron drip line in this region [28]. Lying close to the drip line in this region, 34Na is such a nucleus. The progress in experimental technology over the years has made it possible to show that the one neutron separation energy (Sn) for 34Na is (0.17 ± 0.50) MeV [29], whereas the National Nuclear Data Centre (NNDC) database [30] shows it to be ≃ (0.80 ± 0.008) MeV. Evidently, the uncertainty in the value cannot be ignored. Besides, its ground state spin-parity and shape (spherical or deformed, and oblate or prolate if the latter) have also not yet been established. If one goes by the recently observed trends around the mass region of N = 20 - 28, for example in certain isotopes of Ne, Mg or Al, there is strong possibility that the ground state of 34Na is p- or f -wave dominant [31–35]. This knowledge is important because very weakly bound deformed nuclei in the island of inversion with signiﬁcant p3/2 contribution could generate halos and 34Na is a strong candidate for both. Besides, the separation energy of a nucleus can also be used to calculate its matter radius [36] which can then be used to determine its shape. Further, 34Na is expected to be an innate fragment in the alternate r-process paths where light to medium mass neutron rich nuclei play an important role either as or in the production of seed nuclei [37]. 35Na is supposedly the most abundant isotope of Na near the neutron drip line and as such, binding energies of 34Na and 35Na are important to ﬁnd their reaction rates for their production and consumption in stellar environments. In this paper we use the method of Coulomb dissociation (CD) under the aegis of the post-form ﬁnite range distorted wave Born approximation (FRDWBA) theory [34, 38] to evaluate diﬀerent reaction observables for 34Na. CD, in principle, is the absorption of electromagnetic radiation by a projectile while it moves in the varying Coulomb ﬁeld of a stable, heavy target and then fragmentizes into a core with one (or more) separate nucleon(s) [5]. In this contribution, we apply the FRDWBA theory to calculate various reaction observables like total cross-sections, relative energy spectra, momentum and angular distributions, etc., and investigate the possible allowed ground state conﬁgurations for 34Na along with its binding energy. The post-form of the theory is fully quantum mechanical and includes the non-resonant continuum up to all orders. The only input that it requires is the ground state wavefunction of the projectile [38]. In the next section we present our formalism, while in Section III we show the results and discussion for the various quantities calculated for the reaction considered. Section IV highlights the conclusions.
Resource-efficient adaptive Bayesian tracking of magnetic fields with a quantum sensor<|sep|>Control and measurement of individual electron spins, achieved in the last two decades, enables highly sensitive measurements of magnetic ﬁelds [1–3], with spatial resolution on the order of tens of nanometres [4]. This is typically achieved through a point defect in diamond, the nitrogen-vacancy (NV) centre, which allows optical spin polarisation and readout even at room temperature [5]. Capitalising on atomic-scale wavefunctions, the NV centre enables detection of static and periodic magnetic ﬁelds through the Zeeman shift on its electronic spin state with nanoscale resolution [6,7]. This capability has opened unprecedented opportunities in the ﬁeld of condensed matter physics [8], such as the possibility to detect stray ﬁelds associated to currents in nanodevices [9–11] and magnetisation in a variety of solid-state systems [12–16]. NV centres can also be used as nanoscale sensors for temperature [17], strain [18, 19] and electric ﬁelds [20, 21]. In addition to condensed matter systems, the biological compatibility of nanodiamonds containing NV centres allows monitoring of nanoscale, in-vivo processes [22–26]. Despite its success, one of the big challenges to further the deployment of NV sensing to practical applications is the data acquisition time. Room-temperature spin readout relies on the detection of a spin-dependent ∼ 30% photoluminescence variation, on a signal which is typically (for a single NV) much less than one photon per readout shot. This results in the need of averaging over multiple repetitions, leading to long signal acquisition time [27]. A possible solution to this problem is to employ adaptive signal acquisition techniques, so that the experimental settings are optimised in real-time to minimise the detection time. In addition, sensing faster, or tracking a changing physical quantity in real-time could give new insight into previously inaccessible timescales. Sensing of static magnetic ﬁelds with an NV centre can be carried out via an experiment made up of a series of measurements. It is the combination of many measurements that produces a reading for static magnetic ﬁeld. The measurement parameters can be chosen optimally to narrow in on the magnetic ﬁeld value sooner. This reduction in the number of measurements naturally allows for faster sensing. Bayesian modeling and estimation, coupled with adaptive rules for experiment optimisation can be performed between measurements to select measurement parameters. Recent work has demonstrated the power of these techniques in speeding up the magnetic ﬁeld sensing process [27–31]. While several algorithms have been proposed and analysed [32–34], only one experimental implementation so far has enabled fully online operation [35]. An important point is that real-time implementation requires fast computations, on the microsecond scale, to optimise the settings for the following measurements. The computation timescales must be comparable to a single measurement time, otherwise it is more eﬀective to simply keep taking measurements, without adaptive optimisation. For protocols relying on single-shot spin readout (at low temperatures), spin detection takes ∼ 10 µs, therefore the computation time should be shorter than this. Time-critical computation with minimal latency can be performed in parallel with a fast digital electronic system, such as a ﬁeld-programmable gate array (FPGA). However, the number of (sequential) operations still needs to be kept as low as possible to keep the overall computational overhead real-time compatible. Here we address this issue by adopting an approximate Bayesian estimation technique. In particular, at each point in time, we approximate the likelihood function and the posterior distribution of the parameter of interest as ﬁnite sums of Gaussian functions, i.e., Gaussian mixtures. Since Gaussian functions can be fully described by only three parameters (amplitude, centre and width), this allows faster processing as the number of parameters propagated over time is small compared to what would be required if the distributions were discretised on a grid or approximated via particle ﬁltering [36]. We ﬁnd that our Bayesian approach can typically be performed using only one or two Gaussian functions, achieving a 10-fold reduction in terms of computation time compared to previous non-approximate implementations [32,33]. While the work detailed here focuses on quantum sensing with NV centres in diamond, the protocol we examine can be readily applied to any other single-qubit quantum sensor [37,38].
Structural and excited-state properties of oligoacene crystals from first principles<|sep|>Organic solids are promising candidates for optoelectronics applications due to their strong absorption, chemical tunability, ﬂexibility, and relatively inexpensive processing costs, among other reasons. The acene crystals, a speciﬁc class of organic semiconductors, are well-characterized, known to possess relatively high carrier mobilities,1 and exhibit a propensity for unique excited-state transport phenomena, notably singlet ﬁssion (SF).2–7 The larger acenes in particular have received recent attention because SF was reported to be exothermic, or nearly so, for tetracene, pentacene, and hexacene.8–12 The interesting optoelectronic properties of acene crystals, combined with the potential for materials design via functionalization at the monomer level, have generated signiﬁcant fundamental theoretical interest in these systems. Theoretical studies of excited state properties of acene crystals have often been performed with small molecular clusters, using wavefuction-based methods7,13–18, or with extended systems, using density functional theory (DFT) and many-body perturbation theory (MBPT).19–26 These calculations have often yielded excellent agreement with experiment and new insights into excited-state properties of acene crystals. As shown in Fig. 1, acene crystals consist of aromatic monomers packed in ordered arrangements. Their constituent monomers possess strong intramolecular covalent bonds, but weak intermolecular dispersive interactions govern the crystal structure. Because the approximate exchange-correlation functionals most commonly used in DFT calculations do not account for dispersive interactions, the above-mentioned theoretical calculations have nearly always made use of experimental data for intermolecular distances and orientation. This limits predictive power, because experimental lattice parameters can be scarce or conﬂicting. In particular, diﬀerent polymorphs of the same material may exist, sometimes even coexisting in the same sample.22,27–3326 Fortunately, the last decade has seen rapid development of DFT-based methods that can capture dispersive interactions and several studies have demonstrated that addressing these interactions allows for predicting accurate geometries and cohesive energies of molecular solids in general and acenes in particular – see, e.g., Refs. 22, 34–46. Speciﬁcally, Ambrosch-Draxl et al.22 have suggested that a combination of dispersion-inclusive DFT methods – which they found to predict lattice parameters in agreement with experiments for acene crystals – followed by MBPT calculations, can be used to explore quantitative diﬀerences in optical properties of pentacene polymorphs. Their work suggests that a broader study of the entire acene family with MBPT methods, especially their recent reﬁnements, would be highly desirable. FIG. 1. (Color online) The acene family. a) General formula. b) Herringbone structure, taken up by most acenes in the solid state, with space group P21/a for naphthalene and anthracene and P1 for larger acenes. c) Benzene crystallizes in an orthorhombic unit cell with four molecules per unit cell, with space group Pbca. In this article, we combine dispersion-inclusive DFT and MBPT to study the geometry and excited states of the entire series of acene crystals, from benzene to hexacene. In each case, we compare the computed geometry, electronic structure, and optical excitations with experiment, for both the gas-phase and solid-state. To account for long-range vdW dispersive interactions, we use primarily non-local vdW density functionals (vdW-DFs), but also employ Grimme ”D2” pair-wise corrections47 and compare our results where possible with previously reported data computed with the TkatchenkoScheﬄer (TS)48 pair-wise correction approach.37,49 We ﬁnd that the new consistent-exchange (cx) vdW density functional (vdW-DF-cx)50,51 can predict acene lattice parameters within 1% of low-temperature measurements, as can the TS method. For optimized acene crystal structures, our MBPT calculations within the GW approximation and using the Bethe-Salpeter equation approach lead to gas-phase ionization potential energies, solid-state electronic band structures, and low-lying singlet and triplet excitations in good quantitative agreement with experiments. For larger acene crystals, we demonstrate that a standard G0W0 approach based on a semi-local DFT starting point is insuﬃcient, and that eigenvalue-self-consistent GW calculations are required. Interestingly, we ﬁnd that low lying excited states are sensitive to crystal geometry, particularly so for singlets, which are signiﬁcantly more delocalized than triplets. This work constitutes a comprehensive survey and validation study of both crystal structure and excited state electronic structure for this important class of molecular crystals. Furthermore, it suggests strategies for accurate predictive modeling and design of excited states in less-explored molecular systems, using current state-of the-art methods. The manuscript is organized as follows. First, we summarize the computational methods used in this work in Section. II. Next, in Section. III A we provide a detailed account of our calculations for the structural properties of the acene crystals, demonstrating and reviewing the accuracy of several diﬀerent vdW-corrected DFT methods. We then turn to presenting MBPT results for charged and neutral excitations. We start with charged and neutral excitations in gas-phase acene molecules, given in Section III B, followed by similar results for the solidstate in Sections III C and III D, where we provide calculations for charged and neutral excitations, respectively, at the experimental Geometry. In Section III E we critically examine the sensitivity of GW and GW-BSE calculations to structures optimized with diﬀerent DFTbased approaches. Finally, we present conclusions in Section IV.
Interface-resolved direct numerical simulations of sediment transport in a turbulent oscillatory boundary layer<|sep|>In nature, ﬂows which involve the motion of solid particles coupled to that of a ﬂuid are quite common and diﬀerent models have been developed to predict phenomena involving
Constraints on decaying dark matter from weak lensing and cluster counts<|sep|>Dark matter (DM) is one of the most important building blocks of the Λ cold dark matter (ΛCDM) model, which is the standard paradigm of modern cosmology. DM makes up about 25 % of the present Universe and DM particles should be stable over the age of the Universe. However, this does not necessarily mean that they are perfectly stable. In fact decaying dark matter (DDM) can be realized in a broad class of particle physics models. Such decay would give a signiﬁcant impact on various aspects of astrophysics and cosmology such as cosmic rays, the cosmic microwave background (CMB), large scale structure and so on. As a result, a lot of work has been devoted to investigate DDM models and their observational consequences. In particular, DDM models have attracted attention recently as it has been suggested that DDM can relax cosmological tensions between the CMB and low-redshift observations, such as in the recovered values of σ8 and the Hubble constant [1–7]1. In [2], we investigated this issue by using CMB data from the Planck 2013 data release [11] and weak lensing shear from CFHTLens [12], and showed that the tension in σ8 between CMB and weak lensing survey can be alleviated to some extent by DDM. In this paper, we extend the work of [2] with the recent weak lensing data from KiDS450 [13, 14] along with the Planck 2015 data [15] and the Planck CMB lensing spectrum [16]. We also include other low-redshift observations from the Sunyaev-Zeldovich (SZ) cluster count from Planck [17] and baryon acoustic oscillation scales [18–20]. We assume that all DM decays with the same decay rate Γ and do not consider a mixed model (i.e., a CDM + DDM model). As we argue in this paper, although KiDS450 and the SZ cluster count from Planck hint at a lower value of σ8 compared to Planck, when we include multiple data from low redshift observations, the DDM model does not give a much better ﬁt to the data, and is rather severely constrained. This is due to the fact that the diﬀerent low-redshift observations are sensitive to diﬀerent scales and redshifts and the DDM model cannot ﬁt the data overall for a given decay rate Γ.2 Therefore in this paper we aim to obtain a constraint on the decay rate of DDM by using the above mentioned data set rather than pursuing a possibility of 1For speciﬁc and motivated models beyond Standard model, we refer to e.g. [8–10]. 2 Compared to Planck 2015, the use of the recent Planck 2018 data [21] (Planck TT,TE,EE+LowE+lensing) would make this tendency more noticeable as the later relase prefers a slightly resolving the tension of σ8. The analysis is done by extending our previous work [2], but using a halo mass function calibrated from N-body simulation, which allows us to include the SZ cluster count data in our analysis. This is the new ingredient in the present paper. As will be shown in the following, the inclusion of the SZ cluster count provides a signiﬁcant eﬀect constraint on the dark matter decay rate. Regarding the halo mass function in DDM model, let us comment on the diﬀerences between the one obtained in our present work and in previous studies. Ref. [24] studied the halo mass function in the same DDM model as ours, based on an analytical argument. They argued that the abundance of cluster-sized halos is suppressed in the DDM model and the deviation from standard CDM becomes prominent at later times, which agrees with our result. On the other hand, Refs. [25, 26] also considered eﬀects on halo mass function in DDM models, but where the decay products are not massless. Refs. [27] studied the mass-concentration relation as well as the mass function in DDM models with massive decay products. Our paper is organized as follows. In the next section, we present the DDM model we consider in this paper. We also brieﬂy describe the methodology of our analysis. In Section 3, we discuss the eﬀects of the DDM model on the halo mass function, based on which in Section 4 we derive constraints on the DDM from recent cosmological observations. We conclude in Section 5. Appendices A and B respectively describe our ﬁtting formula for the DDM halo mass function, and the eﬀects on the concentration of the haloes.
The effect of Galactic foreground subtraction on redshifted 21-cm observations of quasar HII regions<|sep|>The reionisation of cosmic hydrogen by the ﬁrst stars and galaxies was an important milestone in the history of the Universe (e.g. Barkana & Loeb 2001). A powerful tool for study of the reionisation history will be provided by the redshifted 21-cm emission from neutral hydrogen in the intergalactic medium (IGM), and several probes of the reionisation era in redshifted 21-cm emission have been suggested. These include observation of the emission as a function of redshift averaged over a large area of sky. This observation would provide a direct probe of the evolution in the neutral fraction of the IGM, and is referred to as the global step (Shaver et al. 1999; Gnedin & Shaver 2004; Furlanetto 2006). Unless reionisation is uniform throughout the whole IGM however, the global step will be very diﬃcult to detect beneath the bright foreground. A more powerful probe will be provided by observation of the 21-cm power spectrum of ﬂuctuations together with its evolution with redshift. This observation would trace the evolution of neutral gas with redshift as well as the topology of the reionisation process (e.g. Tozzi et al. 2000; Furlanetto et al. 2004; Loeb & Zaldarriaga 2004; Wyithe & Morales 2007; Iliev et al. 2006). Finally, observation of individual HII regions will probe quasar physics as well as the evolution of the neutral gas (Wyithe & Loeb 2004; Kohler et al. 2005; Vald´es et al. 2006). Kohler et al. (2005) have generated synthetic spectra using cosmological simulations and conclude that quasar HII regions will provide the most prominent individual cosmological signals. Work by Datta et al. (2007) has focused on the detection of ionised bubbles in redshifted 21-cm maps. Their results suggest it may be possible to blindly detect spherical HII regions of radius >∼ 22 comoving Mpc during the epoch of reionisation. More recently, Geil & Wyithe (2007) (hereafter GW07) have studied the impact of a percolating IGM on the detection of HII regions, and shown that quasars will leave a detectable imprint until very late in the reionisation era. Various experiments are planned to measure 21-cm emission from the pre-reionisation IGM, including the Low Frequency Array1 (LOFAR) and the Murchison Wideﬁeld Array2 (MWA). In addition to their physical conﬁguration,
Boosting Deep Neural Networks with Geometrical Prior Knowledge: A Survey<|sep|>Deep Neural Networks (DNNs) achieve state-of-the-art results on various tasks such as speech recognition, object detection or machine translation (LeCun et al. (2015)). Usually, DNNs are trained on a large amount of training data in order to generalize to similar, but unseen test data. However, gathering labeled data, which is needed for supervised learning methods, is both labor-intensive and time-consuming. Thus, it is desirable to increase the dataefﬁciency, i.e. to ensure good performance even when training data is limited. Furthermore, DNNs are afﬂicted by some key disadvantages when being applied in ﬁelds with strict safety require ments such as autonomous driving. In these ﬁelds, DNNs need to be robust and explainable in order to enable assessing their behavior in safety-critical cases. This contradicts the current approach of threatening DNNs as a black box and training them in an end-to-end fashion. Finally, the solution space of all parameters within deep neural networks is high-dimensional which impedes ﬁnding the optimal solution to the learning problem. Thus, it makes sense to restrict the solution space using problemdependent, reasonable constraints. Consequently, current research tries to combine expert knowledge - e.g. the knowledge utilized to design classical pattern recognition systems - with the architectures and optimization methods of DNNs. This is often called a hybrid approach aiming to combine the best of both worlds: stateof-the-art results by data-driven optimization while using as few labeled training samples as needed. Additionally, an explainable, robust system behavior is desired. Expert knowledge can be expressed in many different ways. A simple example is knowledge about the expected size of different objects in images for object detection. A more sophisticated one is the exact deﬁnition of a ﬁlter sequence used to detect those objects. However, this kind of prior knowledge is hard to formalize which makes it difﬁcult to include it into a neural network in a principled manner. Another possibility to incorporate knowledge is to model a prior distribution over the output of a neural network in order to predict a posterior distribution with a Bayesian neural network. For many tasks, certain transformations of the input which affect the desired output in a predictable way or do not affect it at all can be determined. Both, knowledge about physical transformations, e.g. when a camera is moved to a novel viewpoint (Coors et al. (2019)), or knowledge about certain transformations frequently occurring in the input data, e.g. rotated and translated patterns, can be leveraged. The latter is referred to as geometrical prior knowledge. For example, a system for image classiﬁcation should clas sify a dog correctly independent of its orientation. Therefore, it needs to learn a representation that does not change when the input is rotated, i.e. it should be invariant with respect to rotations. A common approach to increase the robustness of a neural network to such transformations is to randomly transform the input during the training phase, which is called data augmentation. While this approach is straightforward to implement, the DNN only approximates the desired properties and fails to provide mathematical guarantees. A renowned example of incorporating geometrical prior knowledge to DNNs in a mathematically guaranteed way are convolutional neural networks (CNNs) which share a convolutional kernel among their input space. This procedure is called translational weight tying and allows to reduce the parameter consumption of DNNs while also facilitating the DNN to recognize patterns independent of their location in the input. Therefore, CNNs are equivariant to translations. The success of CNNs in computer vision tasks conﬁrms that utilizing geometrical prior knowledge is an important inductive bias for DNNs. In this work, we investigate the integration of geometric prior knowledge to neural networks such that their representations are mathematically guaranteed to be in- or equivariant. First, we review the mathematical concepts underlying in- and equivariant representations. We then provide an overview of several different approaches which allow to enforce those properties by summarizing current work investigating those ﬁelds. Afterwards, we give an overview over common datasets and benchmarks which are used to compare the different presented algorithms. Subsequently, we investigate how those methods can be applied to DNNs used for autonomous driving problems. Finally, we summarize our review paper and give a short outlook on open challenges and future work. Remark: In this paper, we focus mainly on work related to perception for autonomous driving, i.e. computer vision and processing 3D sensor data. We mention some work incorporating geometrical prior knowledge to other domains, but do not claim any completeness in those ﬁelds. We do not propose any new results but hope to give a broad overview over geometrical prior knowledge for neural networks. Thereby, we hope to ease both the entry into this interesting ﬁeld of research for novel researchers and the comparison of different approaches for experienced researchers.
Random phase approximation with exchange for an accurate description of crystalline polymorphism<|sep|>The development of an accurate, yet computationally eﬃcient, electronic structure approach able to treat electron correlation in solids remains an important task in condensed matter physics. Density functional theory (DFT) provides a rigorous and computationally appealing framework [1, 2] that has been widely successful, but available approximations still suﬀer a number of drawbacks. The van der Waals (vdW) interactions, ubiquitous in systems ranging from layered materials to superconducting hydrides, are challenging to include [3], and most exchange-correlation (xc) functionals do not achieve the accuracy needed to satisfactorily describe the energetics of phase transformations and polymorphism, even in common systems like water [4] and silica [5]. The random phase approximation (RPA) represents the highest level of sophistication currently applied in materials science [6–8]. Being based on the exact adiabatic-connection ﬂuctuation-dissipation (ACFD) formula [9, 10], ﬁrst-order Hartree-Fock exchange is exactly incorporated and vdW forces are seamlessly built in, providing an overall improved accuracy. However, the performance of the RPA strongly relies on error cancellation, leading to unpredictable errors when dealing with crystals of low symmetry [11, 12]. Futhermore, vdW forces are underestimated by an average of 20% [13–15]. The origin of these errors can, however, easily be traced to the approximate Hartree response function used for constructing the correlation energy. The combination of the ACFD formula and timedependent (TD) DFT has opened a path for systematic improvements of the RPA [16, 17]. Within TDDFT, xc eﬀects are incorporated via the xc kernel that can be deﬁned as the second density variation of the xc action functional [18–20]. Starting from the RPA, i.e., the Hartree kernel, further reﬁnements then naturally proceed via the local density approximation (LDA) and generalized gradient approximations (GGAs). However, ﬁrst results found that these approximations can worsen the performance with respect to the RPA [21]. A renormalized LDA kernel that introduces spacial nonlocality was, therefore, formulated in Ref. [22]. This kernel, as well as a renormalized PBE (Perdew-Burke-Ernzerhof) kernel, have shown to improve the RPA correlation energy, leading to a better performance in several cases [12, 23, 24]. Within an approach that combines many-body perturbation theory (MBPT) and TDDFT the ﬁrst step beyond RPA is to include the full Fock exchange term in the density response function via the nonlocal and frequency dependent exact-exchange (EXX) kernel. This generates the random phase approximation with exchange (RPAx), that exactly includes the second order exchange diagram as well as higher order exchange eﬀects, in addition to the RPA ring series of diagrams [25–29]. Various partial resummations of the RPAx correlation terms are thus equal to other advanced expressions for the correlation energy deﬁned within MBPT such as, e.g., SOSEX (second order screened exchange) [15, 30–34]. First tests on atoms and molecules have shown that not only accurate correlation energies are obtained with RPAx [15, 35], but also xc potentials, i.e., electronic densities, and polarisabilities [28, 36]. Furthermore, studies on the homogeneous electron gas, the simplest model of a metal, have shown promising results [34]. In this work, we extend the scope of applications to solids and show that it is possible to reach results of similar quality as for molecules. We calculate the relative energies between various crystalline phases within the BN, SiO2 and ice polymorphs. In particular, we investigate cases where the RPA has shown insuﬃcient, as, e.g., for the α-quartz-stishovite energy diﬀerence in SiO2, for which the change in coordination number necessitate accurate correlation energies [5, 11, 12]. A similar problem occurs in cubic and layered BN, inverting their stability order [37, 38]. In both SiO2 and BN the vdW forces play an important role [3, 5, 39]. Here we investigate the eﬀect of the EXX kernel on the binding energy of layered BN as well as the energy diﬀerence between αquartz and cristobalite. A study of purely vdW bonded solids, such as Ar and Kr, conﬁrms that the vdW bond is well described with RPAx, yielding an accuracy similar to that found for molecular dimers. Finally, we perform a detailed analysis of the water dimer potential energy surface and ice polymorphism. The delicate balance between Pauli repulsion, static and dynamic correlation has made water a long-standing problem for electronic structure methods. Our results on this broad and challenging set of systems show promise for future applications. The RPAx stays within the simple computational framework of the RPA, but has an accuracy comparable with more sophisticated methods such as quantum Monte Carlo (QMC) and coupled cluster (CC).
Criticality of Large Delay Tolerant Networks via Directed Continuum Percolation in Space-Time<|sep|>Delay-tolerant networking (DTN) is a last-resort networking paradigm for mobile nodes when direct and multi-hop connections are infeasible, i.e., the network is not connected and nodes have to carry the messages to the next node (store-carryforward). We study the capacity of DTN to deliver a message to its destination(s) in the framework of the percolation theory. To this end, we assume an elementary mobility model where nodes arrive according to a Poisson point process on a plane, move a certain distance ℓ, and then depart. Nodes perform epidemic routing, i.e., whenever two nodes meet, all messages are exchanged. We are interested in ﬁnding the circumstances under which the lifetime of a message becomes inﬁnite (with some positive probability) so that the message could reach a recipient located at an arbitrary distance from the source. We obtain a fundamental criticality condition that characterizes the sufﬁcient mean density of nodes to this end. The criticality condition takes form ν > νc(γ) = 4 ηc(γ), where ν denotes the mean node degree (number of neighbors), γ is the ratio of ℓ to the transmission range d, and ηc(γ) is an unknown function, which we determine in this paper. For a mobile DTN, we ﬁnd that νc(γ) ≤ 1.52 for all γ, while only at ν ≈ 4.51 a (non-DTN) wireless ad-hoc network percolates and a gigantic connected cluster emerges [1]. In other words, DTN communication is possible over a very sparse network provided that the network’s topology changes in time. A convenient characterization in fact is to say that DTN is a network that is super-critical in space-time. In practice, DTN is often sub-critical at a random time instant and the messages ﬁnd their way to the destination through the store-carry-forward routing in space-time. This work is motivated by different opportunistic networking schemes. One such scheme is Floating Content, where nodes replicate messages only within the area where each message is deemed relevant. A criticality condition, characterizing the circumstances under which the lifetime of the message can be expected to be long, was established in [2] under the assumptions of mobile nodes and point contacts (i.e., the transmission range is small compared to the dimension of the area). Similarly, Beachnet [3] the aim is to produce quasi-periodic “information waves” carried by an underlying ﬁeld of immobile nodes (say, devices on a beach). Beachnet is designed to offer regular content dissemination when nodes are (mostly) stationary and their collective cooperation, based on simple local transmission rules, is sought. Similar concepts for one-to-many content dissemination include hovering information [4] and ad-hoc podcasting [5], but our model naturally covers the special case of one-to-one messaging. These types of DTN schemes introduce many interesting problems. Perhaps the most fundamental question is whether a network can transport messages to their intended destinations or not, and this is also our focus in this paper. We assume epidemic routing which was proposed by Vahdat and Becker in [6]. The operational principle is very simple: whenever two nodes meet they exchange the messages only one of them has. The performance analysis of the epidemic routing often assumes exponentially distributed (i.i.d.) intermeeting times leading to Markovian models [7]. As the size of the network grows, solving Markov chains becomes infeasible, and Zhang et al., in [8] obtained ODEs as a ﬂuid limit of such Markovian models. In above work, the spatial dimension has been abstracted away. Jacquet et al. [9] consider the propagation speed of the information in DTN for a ﬁxed set of nodes moving in a ﬁnite region. The spatial dimension is explicitly present in their formulation. Somewhat related, Grossglauser and Tse [10] studied the performance of adhoc networks under mobility. They focused on connected networks, where nodes can either communicate directly or via multi-hop connections, and showed that mobility increases the overall capacity in the network given the users tolerate additional delays. In this paper, we consider a fundamental feasibility problem of DTN-style communication with the elementary mobility model by means of percolation theory [11], [12]. Percolation theory has been succesfully applied to study the performance of wireless multi-hop networks since the early work by Gilbert [13]. Similarly as in [13], most of the work utilizes undirected planar continuum percolation models to argue, e.g., about the network’s connectivity or capacity. In contrast, in [14], we assumed stationary nodes and showed that opportunistic content dissemination schemes, such as the ﬂoating content, can be analyzed by using a threedimensional continuum percolation model. In this paper, we adapt the same approach, but instead of approximating the process by undirected percolation model, we consider the actual directed percolation characterizing the dissemination of messages by mobile nodes exactly in a DTN network. There are less results available for the directed percolation models than there are for the normal undirected percolation, and results for directed continuum models are even more scarce. The basic directed cases are the bond and site percolation models, where, e.g., bonds have ﬁxed directions (e.g., up and right in a square lattice) [11], [15]. Directed models have been also studied in the context of scale-free networks [16], which arise in Internet, social networks etc. The direction property can be deﬁned many ways. The model considered in this paper, is a speciﬁc case of directed continuum percolation, where three-dimensional objects may belong to a given cluster fully or partially. In particular, the third dimension is time and, due to the causality, only the post-contact part of an object belongs to the given cluster. In this setting, we obtain an elegant fundamental result for the minimum feasible mean node degree to support DTN communication as a function of the ratio of movement to the transmission range. This critical percolation threshold is determined by Monte Carlo simulations. The obtained results deﬁne the feasible operation regime for large DTN networks. The rest of the paper is organized as follows. In Section II we introduce the model and the notation. Section III contains the analysis and the description of the Monte Carlo simulations. The numerical results are given in Section IV, and Section V concludes the paper.
Spatial Interactions of Peers and Performance of File Sharing Systems<|sep|>Peer-to-peer (P2P) architectures have been widely used over the Internet in the last decade. The main feature of P2P is that it uses the available resources of participating end users. In the ﬁeld of content distribution (ﬁle sharing, live or ondemand streaming), the P2P paradigm has been widely used to quickly deploy low-cost, scalable, decentralized architectures. For instance, the ideas and success of BitTorrent [1] have shown that distributed ﬁle-sharing protocols can provide practically unbounded scalability of performance. Although there are currently many other architectures that compete with P2P (dedicated Content Distribution Networks, Cloud-based solutions, . . . ), P2P is still unchallenged with respect to its low-cost and scalability features, and remains a major actor in the ﬁeld of content distribution. The Achilles’ heel of todays’ P2P content distribution is the access upload bandwidth, as even high-speed Internet access connections are often asymmetric with a relatively low uplink capacity. Therefore, most theoretical models of P2P content distribution presented so far have been ‘traditional’ in the sense of assuming a common, relatively low access bandwidth, in particular concerning the upload direction, which functions as the main performance bottleneck. However, in a near future the deployment of very high speed access (e.g. FTTH) will challenge the justiﬁcation of this assumption. This raises the need of new P2P models that describe what happens when the access is not necessarily the main/only bottleneck and that A new model. The ﬁrst contribution of the present paper is the model presented in Section III, which features the following two key ingredients which were lacking in previous models of the literature on P2P dynamics: 1) a spatial component thanks to which the topology of the peer locations is used to determine their interactions and their pairwise exchange throughput; 2) a networking component allowing one to represent the capacity of the network elements as well as the transport protocols used by the peers and to determine the actual exchange throughput between them. More precisely, we consider a scenario where peers randomly appear in some metric space, typically the Euclidean plane representing the physical distance, and download from their neighbors with a throughput that may depend on some distance or RTT (it can be the case for e.g. TCP transport). The typical P2P application we have in mind is a BitTorrent-like ﬁle-sharing system. However, the high abstraction level of our model also allows for interpretations beyond this framework. Using proper QoS requirements, it could be extended to any kind of P2P content distribution services (like live and on-demand streaming). The space could also be a representation of the peers’ interests, the position of a peer representing its own centers of interest. In such a space, two close peers share common interests, and therefore are likely to exchange more data. A promising form of scalability. The rationale that is usually brought forward to explain P2P scalability is that the overall service capacity growths with the number of peers. This allows the system to reach an equilibrium point no matters how popular the service is. This equilibrium was ﬁrst analytically studied in [2], under the traditional assumption mentioned above that the upload/download capacity is the bottleneck determining the exchange throughput obtained by peers. The model proposed in [2] leads to an equilibrium point which exhibits the expected scaling property in that the service latency can be shown to remain constant when the system load increases. In our new model, the equilibrium point may exhibit a stronger form of scalability than that in [2], that we propose to call super-scalability, where the Conditions for super-scalability to hold. As we shall see in Sections II and IV, this super-scalability phenomenon is not difﬁcult to understand from a pure queuing theory or graph theory viewpoint. Roughly speaking, super-scalability can be shown to hold in a queue whenever the service rate of a typical customer scales like the number of customers in the system (rather than like a constant as in [2]). Equivalently, it is not difﬁcult to see that it holds if the peer interaction graph is complete at any given time. However, in practice, the network cannot sustain arbitrary high rates. Also, interactions between peers are limited by degree constraints and by the requirement to select peer connections with good throughput. Section VI combines our model together with an abstract network model to determine the conditions on the peering rules, on the network capacity and on the transport protocols for which the mathematical analysis makes sense and for which the super-scalability property can possibly survive. The laws of super-scalability. The paper also provides a full analytical quantiﬁcation of the system at the equilibrium point: in addition to the latency formula, it also provides closed form expressions for e.g. the density of peers present in the P2P overlay or the rate obtained by each peer, as functions of the peering rules and the network parameters. These equilibrium laws, which take speciﬁc forms for each type of transport protocol, are the main analytical contributions of the paper. These are gathered in Section IV for the simplest scenarios and in Sections VII and VIII for a few variants that can be built on our model: generic rate functions, auxiliary servers, seeding behavior of users, access bottleneck condition, etc. These laws have important P2P implications. In particular, they allow one to determine optimal tuning of the parameters of the P2P algorithms e.g. the optimal peering degree or the best parameters of the transport protocols to be used within this context. One theoretically novel feature of our model is the proof of a repulsion phenomenon which was empirically observed in [3]: as close peers get faster rates, they quit the system earlier, so a node “sees” fewer peers in its immediate vicinity than one would expect by considering the spatial entrance distribution alone. All these results are validated through simulations in Section V. Our main scenario is inspired by a BitTorrent-like ﬁlesharing protocol. In BitTorrent [1], a ﬁle is segmented into small chunks and each downloader (called leecher) exchanges chunks with its neighbors in a peer-to-peer overlay network. A peer may continue to distribute chunks after it has completed its own download (it is called a seeder then). Theoretical studies and modeling have already provided relatively good understanding of BitTorrent performance. Qiu and Srikant [2] analyzed the effectiveness of P2P ﬁlesharing with a simple dynamic system model, focusing on the dynamics of leechers and seeders. Massouli and Vojnovic [4] proposed an elegantly abstracted stochastic chunk-level model of uncoordinated ﬁle-sharing. In the case of nonaltruistic peers (who do not continue as seeders), their results indicated that if the system has high input rate and starts with a large and chunk-wise sufﬁciently balanced population, it may perform well very long times without any seeder. However, instability may be encountered in the form of the “missing piece syndrome” identiﬁed by Mathieu and Reynier [5], where one (and exactly one!) chunk keeps existing in very few copies while the peer population grows unboundedly. Hajek and Zhu [6], [7] proved that the syndrome is unavoidable, if the nonaltruistic peers enter empty-handed and if the peer arrival rate is larger than the chunk upload rate offered by persistent seeders. On the other hand, they also proved that the system becomes stable for any input rate, if the peers have enough altruism to stay as seeders as long as it takes to upload one chunk. The missing piece syndrome can be avoided even in the case of non-altruistic peers by using more sophisticated download policies at the cost of somewhat increased download times, see [8], [9], [10]. The above results were obtained in a homogeneous, potentially fully connected network model. The present paper introduces a much less trivial family of peer interaction models, focusing on a bandwidth-centered approach similar to the one proposed by Benbadis et al. [11]. To avoid excessive layers of complexity, we neglect chunklevel modeling in this phase, although realizing that meeting the rare chunk problem will modify and enrich the picture in future research. The natural feature of large variation of transfer speeds in P2P systems has been considered in a large number of papers. For example, part of the peers can rely on cellular network access that is an order of magnitude slower than ﬁxed network access used by the other part. Such scenarios differ however substantially from our model, where the transfer speeds depend on pair-wise distances but not on the nodes as such. There are some earlier papers considering P2P systems in a spatial framework. As an example, Susitaival et al. [12] assume that the peers are randomly placed on a sphere, and compare nearest peer selection with random peer selection in terms of resource usage proportional to distance. However, the distance has no effect on transfer speed in their model. Our paper seems to be the ﬁrst where a peer’s downloading rate is a function of its distances to other peers.
MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning<|sep|>Medical imaging became a standard in diagnosis and medical intervention for the visual representation of the functionality of organs and tissues. Through the increased availability and usage of modern medical imaging like Magnetic Resonance Imaging (MRI), the need for automated processing of scanned imaging data is quite strong [1]. Currently, the evaluation of medical images is a manual process performed by physicians. Larger numbers of slices require the inspection of even more image material by doctors, especially regarding the increased usage of high-resolution medical imaging. In order to shorten the time-consuming inspection and evaluation process, an automatic pre-segmentation of abnormal features in medical images would be required. Image segmentation is a popular sub-field of image processing within computer science  [2]. The aim of semantic segmentation is to identify common features in an input image by learning and then labeling each pixel in an image with a class (e.g. background, kidney or tumor). There is a wide range of algorithms to solve segmentation problems. However, state-of-the-art accuracy was accomplished by convolutional neural networks and deep learning models [2–6], which are used extensively today. Furthermore, the newest convolutional neural networks are able to exploit local and global features in images  [7–9] and they can be trained to use 3D image information as well  [10,11]. In recent years, medical image segmentation models with a convolutional neural network architecture have become quite powerful and achieved similar results  performance-wise  as  radiologists  [5,12].  Nevertheless,  these  models  have  been  standalone  applications  with optimized  architectures,  preprocessing  procedures,  data  augmentations  and  metrics  specific  for  their  data  set  and corresponding segmentation problem  [9]. Also, the performance of such optimized pipelines varies drastically between different medical conditions. However, even for the same medical condition, evaluation and comparisons of these models are a persistent challenge due to the variety of the size, shape, localization and distinctness of different data sets. In order to objectively compare two segmentation model architectures from the sea of one-use standalone pipelines, each specific for a single public data set, it would be required to implement a complete custom pipeline with preprocessing, data augmentation and batch creation. Frameworks for general image segmentation pipeline building can not be fully utilized. The reason for this are their missing medical image I/O interfaces, their preprocessing methods, as well as their lack of handling highly unbalanced class distributions, which is standard in medical imaging. Recently developed medical image segmentation platforms, like NiftyNet [13], are powerful tools and an excellent first step for standardized medical image segmentation pipelines. However, they are designed more like configurable software instead of frameworks. They lack modular pipeline blocks to offer researchers the opportunity for easy customization and to help developing their own software for their specific segmentation problems. In this work, we push towards constructing an intuitive and easy-to-use framework for fast setup of state-of-the-art convolutional neural network and deep learning models for medical image segmentation. The aim of our framework MIScnn (Medical  Image  Segmentation with  Convolutional  Neural  Networks) is to provide a complete pipeline for preprocessing, data augmentation, patch slicing and batch creation steps in order to start straightforward with training and predicting on diverse medical imaging data. Instead of being fixated on one model architecture, MIScnn allows not only fast switching between multiple modern convolutional neural network models, but it also provides the possibility to easily add custom model architectures. Additionally, it facilitates a simple deployment and fast usage of new deep learning models for medical image segmentation. Still, MIScnn is highly configurable to adjust hyperparameters, general training parameters, preprocessing procedures, as well as include or exclude data augmentations and evaluation techniques.
Chance-Constrained Two-Stage Unit Commitment under Uncertain Load and Wind Power Output Using Bilinear Benders Decomposition<|sep|>all over the world in recent decades. Nevertheless, wind power  generation is intermittent and it is quite difficult to give an accurate day-ahead prediction [1]. As a conventional method in  power system operation, the uncertainty caused by load variation and generator’s forced outage is handled by imposing predefined reserve requirements. This method, which is known as  reserve adjustment method, is easy to implement in practice  and has been widely adopted in today’s power industry for  many years [2]. However, dispatching extra generators as reserves is uneconomic to deal with the uncertainty, especially  when the reserve requirement is determined by some rather  simple rules [3]. Indeed, the volatility from wind power generation is generally higher than load variation. Thus, even with  reserves determined by rules, power systems may still suffer  from the reserve scarcity when wind power output deviates significantly from the predicted value [4]. So, to more economically and reliably improve wind power penetration level in the  grid, many research efforts have been devoted to using advanced methods to integrate power system operation, especially unit commitment (UC) [5], with analytically described  wind generation randomness. Among those methods, two most  popular approaches are stochastic programming [3, 6-14] and  robust optimization [4, 15-19]. utilize a set of representative scenarios (through sampling if  necessary) to capture random factors, e.g., wind power generation [7], and introduce a recourse decision problem for every  scenario. As a result, a deterministic UC model will be converted into a two-stage stochastic UC (SUC) model [9]. This  model minimizes the expected cost while satisfying operational  constraints under those scenarios [10]. Thus, it guarantees that  commitment decisions of conventional generators are sufficiently flexible to address the uncertainty associated with wind  power generation [12]. Compared with deterministic UC (DUC)  models, SUC models have advantages of high reliability, as  shown in [3] and [11]. Nevertheless, because all scenarios must  be considered, which may include some extreme scenarios,  computing SUC could lead to costly solutions. treme scenarios could be physically and economically impractical. To address that issue, chance-constrained optimization is  introduced to restrict the consideration of rarely occurred extreme scenarios. Specifically, a small number of scenarios, whose realization probabilities sum up to 𝜖 (<1), can be ignored in deriving an optimal solution. Clearly, when 𝜖 equals  0, the chance constraint disappears and all scenarios must be  considered. By adjusting the value of 𝜖, the decision maker will  be able to have a desired trade-off between cost and reliability. strained optimization has been employed as decision tools for  UC problems in the last decade [20]. In [21-23], chance-constrained UC models have been developed where a large portion  of wind power output should be utilized with high probability.  In the chance-constrained UC problem reported by [24], reserve requirements and transmission line capacity limitations  are formulated as chance constraints to maintain the reliability  of system operation. In addition to UC problems, chance-constrained optimization method has also been employed in other  research fields, such as reserve scheduling [25], generation expansion planning [26], transmission expansion planning [27],  and demand response [28]. modeling, it is well recognized that solving chance-constrained  optimization problems is computationally challenging, especially when the random variable 𝜉 follows an unstructured and  continuous distribution. Note that in general, analytically representing the probability constraint 𝑃𝑟{𝐺(𝑥, 𝜉) ≤ 𝟎} (where 𝑥  represents the set of decision variables and 𝜉 denotes a random  realization) is very difficult as it requires the complex computation of multi-dimensional integration. Certainly, for some  special cases, chance constraint can be converted into a closed  form expression and the whole model can be reformulated into  a regular mixed integer programming (MIP) model (e.g., [20,  24, 26, 29]). In [30, 31], such property is used to study chance  constrained optimal power flow problems subject to uncertain  parameters of the normal distribution. joint chance constraints, are complicated, by following the convention in stochastic programming, sampling-based method  has been extended to generate a finite set of scenarios and then  chance constraints are imposed upon those sampled scenarios.  Specifically, by using a binary variable to indicate whether the  associated scenario should be considered (i.e., the corresponding constraints must be satisfied) or ignored (i.e., the corresponding constraints can be violated), the chance-constrained  model can be converted into an MIP model (through using BigM method). Such strategy has been adopted in [21-23, 25],  where, however, professional MIP solvers are often incapable  to compute the resulting large-scale MIPs and fast heuristic  methods are necessary [22, 25]. Hence, it remains a critical  challenge to efficiently compute chance-constrained UC problems for real applications, especially when a large number of  scenarios are needed and joint chance constraints are required. of conditional value-at-risk (CVaR) sometimes is also considered as a chance-constrained model [32]. As pointed in [5], although both chance-constrained and CVaR-based UC models  reflect the modeler’s risk consideration, the former one is computationally much more challenging. Indeed, chance constraint  usually introduces non-convexity into the original model while address the computational challenge associated with chanceconstrained UC problem. Specifically, a chance-constrained  two-stage UC model is first presented where the power imbalance due to random load demand and wind power generation is  restricted by a predefined low probability. Then, we adopt the  bilinear reformulation to represent chance constraints [34], and  convert that bilinear model into a linear one through McCormick linearization method [35]. To deal with the large number  of stochastic scenarios, we provide the bilinear Benders reformulation of the original model and customize the bilinear variant of Benders decomposition algorithm to achieve fast computation [34]. Our numerical results on standard IEEE systems,  including 118-bus system, demonstrate that (i) the bilinear UC  formulation is stronger than the widely adopted Big-M based  formulation; (ii) our proposed bilinear Benders decomposition  algorithm is generally an order of magnitude faster than using  a professional solver to directly compute both linear and bilinear chance-constrained UC models. To the best of our  knowledge, this is the first effort to develop a fast algorithm to  solve large-scale chance-constrained UC problems. Also, we  believe that, a great computational improvement can be  achieved in solving existing UC variants in [21-23] by adopting  this bilinear Benders decomposition method. strained two-stage UC problem is formulated in Section II. Section III introduces the bilinear reformulation of chance constraints and the bilinear Benders decomposition algorithm.  Section IV provides numerical results from case studies using  an illustrative six-bus system and a modified 118-bus system.  This paper is concluded in Section V.
Optimally fuzzy temporal memory<|sep|>Natural learners face a severe computational problem in attempting to predict the future of time varying signals. Rather than being presented with a large training set of examples, they must compute on-line using a continuously evolving representation of the recent past. A basic question arises here–how much of the recent past is required to generate future predictions? Maintaining past information in memory comes with a metabolic cost; we would expect a strong evolutionary pressure to minimize the resources required. A shift register can accurately represent information from the recent past up to a chosen timescale, while consuming resources that grow linearly with that timescale. However, the prediction-relevant timescale of the signal is generally unknown prior to learning. Moreover there are many examples of naturally occurring signals with scale-free long range correlations [1–9], commonly known as 1/f signals, making the natural prediction-relevant timescale essentially unbounded. Our focus is on the following question: If an independent general purpose learner is to forecast long range correlated natural signals, what is the optimal way to represent the past information in memory with limited resources? We argue that the solution is to construct a memory that reﬂects the natural scale-free temporal structure associated with the uncertainties of the world. For example, the timing of an event that happened 100 seconds ago does not have to be represented as accurately in memory as the timing of an event that happened 10 seconds ago. Sacriﬁcing the temporal accuracy of information in memory leads to tremendous resource conservation, yielding the capacity to represent information from exponentially long timescales with linearly growing resources. Moreover, by sacriﬁcing the temporal accuracy of information in a scale-free fashion, the learner can gather the relevant statistics from the signal in a way that is optimal if the signal contains scale-free ﬂuctuations. To mechanistically construct such a memory system, it is imperative to keep in mind that the information represented in memory should self suﬃciently evolve in real time without relying on any information other than the instantaneous input and what is already represented in memory; reliance on any external information would require additional storage resources. In this paper we describe a fuzzy1, i.e. temporally inaccurate, memory system that (i) represents information from very long timescales, (ii) optimally sacriﬁces temporal accuracy while maximally preserving the prediction-relevant information from the past, and (iii) evolves self suﬃciently in real time. The layout of the paper is as follows. In section 2, based on some general properties of long range correlated signals, we derive the criterion for optimally sacriﬁcing the temporal accuracy so that the prediction relevant information from exponentially long time scales is maximally preserved in the memory with ﬁnite resources. However, it is non-trivial to construct such a fuzzy memory in a self suﬃcient way. In section 3, we describe a strategy to construct a self suﬃcient scale-free representation of the recent past. This strategy is based on a neuro-cognitive model of internal time, TILT [10], and is mathematically equivalent to encoding the Laplace transform of the past and approximating its inverse to reconstruct a fuzzy representation of the past. With an optimal choice of a set of memory nodes, this representation naturally leads to a self-suﬃcient fuzzy memory system. In section 4, we illustrate the utility of the fuzzy memory with some simple time series forecasting examples. Using several very diﬀerent time series we show that the fuzzy memory enhances the ability to predict the future in comparison to a shift register with the same number of nodes. Of course, representing the recent past in memory does not by itself guarantee the ability to successfully predict the future. It is crucial to learn the prediction-relevant statistics underlying the signal with an eﬃcient learning algorithm. The choice of the learning algorithm is largely modular to the choice of the memory system. In this paper, we sidestep the problem of learning, and only focus on the memory. As a place holder for a learning algorithm, we use linear regression in the demonstrations of time series forecasting.
The unusually large population of Blazhko variables in the globular cluster NGC 5024 (M53)<|sep|>In 1907, S. Blaˇzko described the cyclic variation of the pulsation period of RW Dra, without comments on the amplitude behaviour, (Blaˇzko 1907), and in 1916, H. Shapley reported cyclic variations in the shape of the light curve, period and amplitude in the star RR Lyrae (Shapley 1916). Period oscillation and/or amplitude modulation in RR Lyrae type stars is generally referred to as the Blaˇzko or Blazhko effect. Over the last hundred years numerous RR Lyrae stars have been found to exhibit cyclic modulations of both period and amplitude although cases of amplitude modulations with no period variations are common. The cause of these modulations has remained unsatisfactorily explained on theoretical grounds. This is not surprising since, to detect and characterize the variations, even ⋆ Based on observations collected at the Indian Astrophysical Observatory, Hanle, India. † E-mail:armando@astro.unam.mx ‡ E-mail:dan.bramich@hotmail.co.uk § E-mail:rﬁguera@astro.unam.mx ¶ E-mail:giridhar@iia.res.in ∥ E-mail:kuppuswamy@iia.res.in in the cases with large amplitude modulations, large sets of accurate photometric observations obtained over a long time span are required. Most recently, as a result of careful inspection of large data bases (e.g. Moskalik & Poretti et al. 2003 (OGLE); Kolenberg et al. 2010 (Kepler)), the detection of the Blazhko effect in RR Lyrae stars has improved. Long term surveys have proven useful in the detection of very small amplitude modulations, increasing the fraction of known Blazhko variables among the fundamental pulsators RRab (RR0) to about 50% (e.g. the Konkoly Blazhko Survey; Jurcsik et al. 2009) and have also allowed detailed studies of individual objects (e.g. Chadid et al. 2010a (V1127 Aql); Chadid et al. 2010b (S Arae); Poretti et al. 2010 (CoRoT 101128793); Kolenberg et al. 2011 (RR Lyr)). While knowledge of the Blazhko population in globular clusters would yield important insight to the metallicity inﬂuence on the incidence of the Blazhko phenomenon, the study of Blazhko variables in globular clusters is still very much unexploited. Several difﬁculties including that stars are faint and often in very crowded ﬁelds mean that long time series of high quality CCD images and photometry of exceptional high quality are required. During the course of an investigation into the variable stars in
Probabilistic aspects of ZM-groups<|sep|>The probabilistic aspects of ﬁnite groups theory constitute a topic which is regularly studied in the last years. The relevant results obtained in [3] and [5]-[10] using the commutativity degree of a ﬁnite group G, which measures the probability that two elements of the group commute, had a major impact on the research related to the probabilistic aspects associated to groups. Therefore, inspired by the previously mentioned concept, the subgroup commutativity degree and the cyclic subgroup commutativity degree of G were introduced in [16] and [19], respectively. These quantities measure the probability that G is an Iwasawa group, i.e. a nilpotent modular group, and they are deﬁned by where L(G) and L1(G) denote the subgroup lattice and the poset of cyclic subgroups of G, respectively. In other words, the (cyclic) subgroup commutativity degree represents the probability that two (cyclic) subgroups of G commute. There is a major interest in ﬁnding explicit formulas for these two concepts under the assumption that G belongs to a well known class of ﬁnite groups. As a consequence, one can study the asymptotic behavior of sd(G) and csd(G). For such kind of results, we refer the reader to [16, 19, 21]. Another remarkable quantity associated to a ﬁnite group G is its factorization number which is denoted by F2(G). This number is obtained by counting the pairs (H, K) ∈ L(G)2 satisfying G = HK. These pairs of subgroups are called factorizations of G. The factorization number of G was studied in [2, 11, 17] and it is strongly connected with sd(G). More exactly, In [20], the cyclic factorization number of a ﬁnite group G, denoted by CF2(G), was introduced and it was obtained by counting the pairs (H, K) ∈ L1(G)2 such that HK = G. Obviously, the connection between CF2(G) and csd(G) is similar to the one between F2(G) and sd(G), and it is given by Our aim is to study these four probabilistic aspects for ZM-groups. The paper is organized as follows. Section 2 recalls the structure of ZM-groups as well as some of the properties of this remarkable class of groups. Explicit formulas for the (cyclic) factorization number of ZM-groups are obtained in Section 3. Section 4 covers the main results of the paper. More exactly, we ﬁnd an explicit formula that allows us to compute the (cyclic) subgroup commutativity degree of ZM-groups, we show that our results generalize the ones obtained for dihedral groups in [16, 19], and we indicate a class of groups whose (cyclic) subgroup commutativity degree vanishes asymptotically. We end the paper by suggesting some open problems in the last section. Most of our notation is standard and will usually not be repeated here. Elementary notions and results on groups can be found in [4, 13]. For subgroup lattice concepts we refer the reader to [12, 14, 15].
The Dynamical State of The Serpens South Filamentary Infrared Dark Cloud<|sep|>Infrared Dark Clouds (IRDCs) are cold, dense regions of molecular clouds seen as extinction features against bright mid-infrared galactic backgound (e.g., Rathborne et al. 2006; Peretto & Fuller 2009; Pillai et al. 2006). They were ﬁrst identiﬁed on the basis of observations of ISO and MSX satellites (Perault et al. 1996; Egan et al. 1998). Since the abundant discovery by the Spitzer Space telescope, IRDCs have been paid attention as promising sites to study the earliest phases of star cluster formation because their volume and surface densities resemble those of the nearby active cluster-forming clumps like NGC 1333 and ρ Oph (Kauﬀmann & Pillai 2010; Hernandez & Tan 2011). Recent observations have revealed that the IRDCs often show elongated or ﬁlamentary shapes and appear to fragment into dense clumps (e.g., Miettinen 2012). The observations also suggest that the self-gravity is likely to play an important role in the dynamics of the clumps in IRDCs (e.g., Pillai et al. 2006, 2011; Rygl et al. 2010; Miettinen 2012; Busquet et al. 2013). Global inﬂow motions toward the IRDC ﬁlaments have also been reported (Schneider et al. 2010). These observations indicate that kinematics and dynamics of IRDCs and their substructures can be well characterized by the combination between the dust continuum and molecular line observations. Although extensive observations towards IRDCs have just started recently, our current knowledge on the physical properties of IRDCs remain very limitted. In order to further characterize the physical properties of IRDCs and to constrain how clusters form in IRDCs, we carried out mapping observations toward a nearby ﬁlamentary IRDC, Serpens South, which was discovered by the Spitzer observations (Gutermuth et al. 2008). The IRDC associated with the Serpens South cluster is the nearest IRDC that shows a sign of active cluster formation. An interesting characteristic of the cluster is its extremely-high fraction of protostars. In the central region, the number fraction of protostars (Class I) relative to the Young Stellar Objects (YSOs) detected by the Spitzer telescope (Class I+Class II) reaches about 80 % (Gutermuth et al. 2008). This fraction is largest among the cluster-forming regions known within the nearest 400 pc (see Evans et al. 2009). For example, the Serpens Main Cloud, a nearby embedded cluster having the size comparable to Serpens South, have the protostar fraction of about 60 %, about two thirds of Serpens South. Another nearby embedded cluster, NGC 1333, has the protostar fraction of about 30 %, less than a half of Serpens South. Very recently, Bontemps et al. (2010) and K¨onyves et al. (2010) discovered 7 Class 0 protostars in the Serpens South IRDC on the basis of the Herschel observations, all of which haven’t been identiﬁed by the Spitzer observations. These observations strongly suggest that Serpens South is in the very early phase of star cluster formation. We note that the distance to Serpens South is somewhat uncertain in the range from 260 and 700 pc (see the discussion of Bontemps et al. 2010). In this paper, we adopt the distance of 415 pc (Dzib et al. 2010) because our preliminary analysis on the basis of the near-infrared observations implies a relatively larger distance of ≳ 400 pc. Recently, Nakamura et al. (2011) performed 12CO (J = 3 − 2) mapping observations toward the Serpens South IRDC using the ASTE 10-m telescope and discovered that a number of powerfull outﬂows are blowing out of the central dense clump that is located near the southern edge of a long ﬁlamentary cloud (Andr´e et al. 2010; Bontemps et al. 2010; Arzoumanian et al. 2013). The CO (J = 3 − 2) images have revealed that several collimated outﬂow lobes are overlapping and interacting with one another, indicating that the very active star formation is ongoing. From the H2 v = 1−0 S(1) 2.122 µm image, Teixeira et al. (2012) also identiﬁed 10 outﬂow knots in Serpens South. The main ﬁlament appears to be penetrated by a globally-ordered magnetic ﬁeld, implying that the magnetic ﬁeld plays a role in the ﬁlament formation (Sugitani et al. 2011). Several lessdense ﬁlamentary structures also appear to converge toward the main ﬁlament and the dense clump. These less-dense ﬁlaments may give us clues to uncover the formation process of the cluster (Myers 2009). Recently, Kirk et al. (2013) performed a Mopra observation toward Serpens South using several dense gas tracers such as N2H+ (J = 1 − 0), HCO+ (J = 1 − 0), and H13CO+ (J = 1 − 0). The optically-thick lines like HCO+ (J = 1 − 0) showed blue-skewed proﬁles, indicative of the infall motions along the line-of-sight. They also found a signiﬁcant velocity gradient in the N2H+ emission along the southern part of the main ﬁlament. They interpreted that the velocity gradient is caused by a mass infall along the ﬁlament toward the central cluster. In this paper, we present the results of the N2H+ (J = 1− 0) observations toward the ﬁlamentary IRDC, Serpens South, using the Nobeyama 45-m radio telescope, and study the dynamical state of the dense clumps in Serpens South. The details of the observations are described in Section 2. The results of the observations are presented in Section 3. Our N2H+ (J = 1 − 0) data have a ﬁner angular resolution and higher sensitivity than those of Kirk et al. (2013), and therefore allow us to estimate the quantities by ﬁtting the hyperﬁne structure of N2H+. Applying the hyperﬁne ﬁtting to the N2H+ data cube, we estimate the physical quantities of the dense gas in this region in Section 4, and assess in Section 5 the dynamical state of the main ﬁlament that appears to fragment into a few dense clumps. Then, we apply in Section 6 the virial analysis to the dense clumps, and clarify the clump dynamical state. Then, in Section 7, we identify the dense cores using the clumpﬁnd method and derive the physical quantities of the cores. Finally, we summarize our main conclusion in Section 8.
Van der Waals-like phase transition from holographic entanglement entropy in Lorentz breaking massive gravity<|sep|>The study of HEE and quantum phase transitions of black holes have gained a lot of interest recent years. On one hand, HEE can be used as a perfect probe to study quantum information science [1, 2, 3], strongly correlated quantum systems [4, 5, 6, 7, 8, 9, 10, 11, 12, 13] and Many-Body Systems [14, 15]. On the other hand, investigation on HEE of black holes may shed some light on understanding the nature of BHE[16, 17]. Nearly ten years ago, a holographic derivation of the HEE in conformal quantum ﬁeld theories was proposed by Ryu and Takayanagi using the famous AdS/CFT correspondence [18, 19]. Recently the HEE has been used as a probe to investigate the phase structure of the Reissner-Nordstrom AdS black hole[20]. The results showed that there is a Van der Waals-like(VDP) phase transition at the same critical temperature in both the ﬁxed charge ensemble and chemical potential ensemble in the holographic HEE-temperature plane. They also found that the SPT occurring for the HEE at the same critical point as the BHE with nearly the same critical exponent. This work was soon generalized to the extended phase space where the cosmological constant is considered as a thermodynamical variable[21]. Very recently, the equal area law of HEE was proved to hold for the FPT in the HEE-temperature plane [22]. Based on [20], VDP phase transition of HEE in various AdS black holes have been studied in [23, 24, 25, 26, 27, 28, 29, 30, 31, 32]. All of these works showed that the HEE undergoes the same VDP phase transition as that of the BHE. Massive gravity theories have been considerable interest recently. One of these reasons is that this alternative theories of gravity could explain the accelerated expansion of the universe without dark energy. The graviton behaves like a lattice excitation and exhibits a Drude peak in this theory. Current experimental data from the observation of gravitational waves by advanced LIGO require the graviton mass to be smaller than the inverse period of orbital motion of the binary system, that is mg < 1.2×10−22eV/c2[33]. Another important reason for the interest in massive gravity is that the possibility of the mass graviton could be help to understand the quantum gravity eﬀect. The ﬁrst to introduce a mass to the graviton is in [34]. However this primitive linear massive gravity theory contains the so called Boulware-Deser ghosts problem [35] that was solved by a nonlinear massive gravity theory [36, 37], where the mass terms are obtained by introducing a reference metric. Recently Vegh proposed a new reference metric to describe a class of strongly interacting quantum ﬁeld theories with broken translational symmetry in the holographic framework[38]. The recent progress in massive gravity can be found in [39, 40]. Here, we consider AdS black holes in a class of Lorentz breaking massive gravity. In the massive gravity, the graviton acquires a mass by Lorentz symmetry breaking, which is very similar to the Higgs mechanism. A review of Lorentz violating massive gravity theory can be found in [41, 42]. In this paper, we focus on the study of the VDP phase transition of AdS black holes in Lorentz breaking massive gravity using the holographic HEE. The main motivation of this paper is to explore whether the BHE phase transition can also be described by HEE in Lorentz breaking massive gravity. Firstly, we would like to extend proposals in [20] to study VDP phase transitions in AdS black hole with a spherical horizon in Lorentz-violating massive gravity with the HEE as a probe. What’s more, we also would like to check the Maxwell’s equal area law and critical exponent of the heat capacity, which are two characteristic quantities in VDP phase transition. The organization of this paper is as follows. In the next section, we shall provide a brief review of the black hole solution in Lorentz breaking massive gravity ﬁrstly. Then we will study the VDP phase transitions and critical phenomena for the AdS black hole in the BHE-temperature plane. In Section 3, we mainly concentrate on the VDP phase transition and critical phenomena in the framework of HEE. The last section is devoted to our discussions and conclusions.
On the frequency correlations of low-frequency QPOs with kilohertz QPOs in accreting millisecond X-ray pulsars<|sep|>Accreting low mass X-ray binaries (LMXBs) with a neutron star (NS) or black hole (BH) primary show remarkably similar timing behaviour. Typically, below 50 Hz, both their X-ray signals contain quasi-periodic oscillations (QPOs) with characteristic frequencies that shift as the source state changes, see van der Klis (2006) for a review. At high frequencies (>100 Hz), however, NS show additional noise features and (twin) kHz QPOs with characteristic frequencies that correlate to those at low frequency. In early work by van Straaten et al. (2003), a match was presented between observations, and predictions by the relativistic precession model (RPM, Stella & Vietri 1998) of frequency correlations between the upper kHz QPO frequency and a feature <50 Hz in three non-pulsating atoll NS-LMXBs. In the RPM, the kHz QPO occurs at frequency νK of an orbit, and a ∼20 Hz LF-QPO is the Lense-Thirring (LT) precession frequency where νs = 300νs,2.5 Hz is the neutron star spin frequency, νK = 103νK,3 Hz the Keplerian orbital frequency. M = m·M⊙ and I = 1045I45 g cm2 are the neutron star mass and moment of inertia, respectively. For realistic equations of state I45/m ranges from 0.5 to 2 (Friedman et al. 1986). The frequency of the low frequency QPO is predicted to be proportional to the spin frequency and quadratically related to the upper kHz QPO frequency in the same way as for a precessing testparticle orbit. Remarkably, van Straaten et al. (2003) found power-law indices for correlations of the kHz QPO frequency with a LF feature of 2.0±0.02. In van Doesburgh & van der Klis (2017) (hereafter DK17) we performed a deeper investigation of this result focusing on the non-pulsating sources with known spin. As the RPM predictions are for fundamentally periodic phenomena made quasi-periodic by ’clump’ lifetime broadening
Exploration of dynamical regimes of irradiated small protonated water clusters<|sep|>The structure of water is generally considered to be rather ordered due to hydrogen bonds and with certain structures prevailing. There is, in turn, a general agreement among models about the existence of small and mediumsized clusters of water molecules [1]. Furthermore, experimental evidence point to the fact that under appropriate conditions, water clusters may ionize. There thus exists, since rather long, a large body of investigations on charged water clusters, either anions [2,3,4], cations [6] or protonated clusters [7,8]. The dynamics of water clusters appears as a key issue in various scenarios, e.g., for the chemistry of formation of droplets and clouds in atmosphere [9]. Water clusters are also possible candidates as transient intermediate stages in liquid water [10]. They might thus play a key role in many chemical and physical processes [11,12,13,14,15]. The case of charged clusters, and especially ionized clusters, is particularly relevant for studies on radiation damage, as water is the natural ”environment” of biomolecules. This has triggered recent studies on the inﬂuence of a ﬁnite number of water molecules coating a biomolecule [16]. The direct analysis of the irradiation of protonated water clusters is also a key issue [17]. From the theoretical point of view, water clusters have been considered since long at a structural level [2,4,5], for a recent summary see [18]. In recent years, studies of irradiation of biomolecules, possibly in a water environment have also been started [19,20], but usually without an explicit time-dependent treatment of the ionization process itself. In the present paper, we aim at studying the response of small protonated water clusters to an irradiation by a short laser pulse. The choice of laser irradiation rather than excitation by a charged projectile is to some extent a matter of convenience as it allows for exploratory studies a discrimination and a systematic scan of various dynamical regimes without dealing with complications of scattering geometry. Furthermore, there exists a well documented literature on the laser irradiation of clusters, metallic or rare gas mostly [21,22] in a wide range of excitations, which provides a robust background to study irradiation of other sorts of clusters such as water clusters. The paper is organized as follows. We ﬁrst brieﬂy recall the basic inputs of the model, in section 2. As a next step we ﬁrst consider the optical response which is known to provide a key entry point to understand irradiation dynamics (section 3). We then address irradiation by an intense laser ﬁled, exploring in particular the inﬂuence of laser frequency and intensity as key characteristics of
Sensitive survey for 13CO, CN, H2CO, and SO in the disks of T Tauri and Herbig Ae stars II: Stars in $\rho$ Oph and upper Scorpius<|sep|>Star formation occurs in clusters that are embedded in collapsing molecular clouds (Lada & Lada 2003). The diﬀerent stellar populations and properties, such as age, mass or density, can have eﬀects on the circumstellar disk evolution, which strongly depends on the environment. In regions of high stellar density, disks can be more easily photoevaporated by the intense UV radiation (Johnstone et al. 1998). This means that the evolution of disks in the vicinity of massive stars will diﬀer from the evolution of isolated disks (see, e.g., Mann et al. 2014). The disk structure and chemistry can therefore vary from region to region. Circumstellar disks are the sites where planets form and their gas and dust provide the raw materials for planet building. Constraining the chemical evolution of protoplanetary disks therefore is a major challenge in understanding the planet formation process. Molecular-line emission is an important tool for deriving the disk characteristics (Dartois et al. 2003; Qi et al. 2006). Because H2, the most abundant molecule, cannot be observed, we rely on lower abundance tracers, principally simple molecules, to derive those characteristics. In a previous study Send oﬀprint requests to: S.Guilloteau, e-mail: guilloteau@obs.u-bordeaux1.fr ⋆ Based on observations carried out with the IRAM 30-m telescope. IRAM is supported by INSU/CNRS (France), MPG (Germany) and IGN (Spain). of ∼ 45 stars in the Taurus region, we showed that CN N=21 is a good tracer of disks because it is readily detectable in at least 50 % of the observed sources (Guilloteau et al. 2013, hereafter Paper I). Contamination by molecular clouds was minimal in this line; it was found in fewer than 10 % of the cases, but some sources exhibited strong lines that most likely originate from outﬂows rather than circumstellar disks. However, the Taurus-Auriga area is a region of isolated star formation, and the previous result may not be applicable for other star-forming regions, either because of stellar density or more simply, because of age. We extend here our study to the ρ Oph star-forming region, which is both younger and denser than the Taurus region. We report a search for CN, ortho-H2CO, SO, 13CO, and C17O emissions in 22 young stars located in the ρ Oph region and 7 stars in the upper Scorpius area. We attempt to compare these results with previous observations performed in the Taurus region (Paper I), and more particularly, the CN content in these star-forming regions. The paper is organized as follows. The observations and data analysis are described in Sect. 2. We present the results and the source analysis in Sect. 3. In Sect. 4 we discuss the diﬀerence in molecular composition observed between the Taurus region and that of ρ Oph. We conclude in Sect. 5. An Appendix displays all the observed spectra. Object α δ S ν(3.4) S ν(1.3) Other Sp Type L∗ Mass Age rms (J2000) (J2000) (mJy) (mJy) Name (L⊙) (M⊙) (Myr) (mK) V1146 Sco 15:57:20.0 -23:38:50.0 M0 28 J1603-1751 16:03:23.7 -17:51:42.3 M2 36 AS 205A 16:11:31.4 -18:38:26.0 27.2 450 HBC 254 K5 4.0 67 SR 4 16:25:56.1 -24:20:48.3 4.4 31 AS 206 K5 2.17 1.14 1.1 41 GSS 26 (⋆) 16:26:10.3 -24:20:54.9 24.2 125 M0 1.39 0.56 0.5 34 EL 20 16:26:18.9 -24:28:20.2 7.3 50 VSSG 1 M0 0.93 0.62 1.1 54 LFAM 1 16:26:21.7 -24:22:50.8 17.5 250 37 DoAr 24E 16:26:23.4 -24:21:00.7 8.3 70 GSS 31 35 DoAr 25 16:26:23.7 -24:43:14.1 25.0 280 WSB 29 K5 1.43 1.12 2.1 40 EL 24 16:26:24.1 -24:16:14.0 48.8 390 WSB 31 K6 2.58 0.96 0.6 38 EL 27 16:26:45.0 -24:23:08.2 38.7 300 GSS 39 M0 0.78 0.58 1.2 38 WL 18 16:26:49.0 -24:38:25.7 3.1 85 GY 129 K7 0.3 70 SR 24S 16:26:58.5 -24:45:37.1 26.6 530 HBC 262 K2 4.4 78 SR 21 16:27:10.2 -24:19:12.9 4.2 95 EL 30 G3 11.38 1.97 2.2 33 J1627-2451B 16:27:15.1 -24:51:38.8 M2 28 IRS 41 16:27:19.3 -24:28:44.4 6.2 < 60 WL 3 K7 1.61 0.80 0.8 50 CRBR 85 16:27:24.7 -24:41:03.2 1.5 150 1.4 22 YLW 16c 16:27:26.5 -24:39:23.4 6.5 60 GY 262 M1 1.11 0.48 0.5 23 IRS 49 16:27:38.3 -24:36:58.8 4.4 25 GY 308 M0 1.02 0.64 1.0 47 DoAr 33 16:27:39.0 -23:58:19.1 3.7 40 WSB 53 K4 1.81 1.44 2.9 38 WSB 52 16:27:39.5 -24:39:15.9 10.2 51 GY 314 K5 0.95 1.04 4.2 41 IRS 51 16:27:39.8 -24:43:15.0 12.7 110 GY 315 83 Flying Saucer(⋆) 16:28:13.7 -24:31:39.1 3 M1 0.14 40 WSB 60 16:28:16.5 -24:36:58.0 15.3 89 YLW 58 M4 0.23 0.20 0.9 31 SR 13 16:28:45.3 -24:28:19.2 10.0 60 HBC 266 37 DoAr 44 16:31:33.5 -24:27:37.7 10.4 105 HBC 268 K3 1.55 1.29 5.1 38 RNO 90 16:34:09.2 -15:48:16.9 7.6 25 HBC 649 G5 10.24 1.87 2.3 31 Wa Oph 6 16:48:45.6 -14:16:36.0 10.3 130 HBC 653 K6 2.32 0.98 0.7 26 AS 209 (⋆) 16:49:15.3 -14:22:08.6 17.5 300 HBC 270 K5 2.11 1.18 1.2 24 HD 163296 (⋆) 17:56:21.3 -21:57:22.0 A1 2.25 5.0 50 Table 1. Source sample. Sources in upper Scorpius are listed in italics. Sources marked with (⋆) are the sources for which CN emission from disk has been detected. The epoch 2000 coordinates are adopted from SIMBAD. Sν is the continuum ﬂux at 3.4 mm and 1.3 mm. Spectral types and luminosities (given for ∼ 130 pc) are taken from Ricci et al. (2010) except for AS 205A (Bast et al. 2011), WL 18 (Andrews et al. 2010), SR 24S (Andrews et al. 2010), CRBR 85 (Pontoppidan et al. 2005), and the Flying Saucer (Grosso et al. 2003). Masses and ages are derived from the tracks reported in Palla & Stahler (1999) using the luminosities and eﬀective temperatures from Ricci et al. (2010). AS 209 2.1 ± 0.3 4.5 ± 0.4 4.2 ± 0.7 GSS 26 2.2 ± 0.3 4.0 ± 0.2 2.2 ± 0.4 IRS 41 1.5 ± 0.3 5.5 ± 0.1 0.9 ± 0.2 CRBR 85 ≈ 2.5 see Fig.2 Flying Saucer 3.4 ± 0.5 4.1 ± 0.3 4.1 ± 0.6
Quantum error correction with only two extra qubits<|sep|>Fault-tolerant quantum computation is possible: quantum computers can tolerate noise and imperfections. Fault tolerance will be necessary for running quantum algorithms on large-scale quantum computers. However, faulttolerance schemes have substantial overhead; many physical qubits are used for each encoded, logical qubit. This means that on small- and medium-scale devices in the near future, it will be diﬃcult to test fault tolerance, and to explore the eﬃcacy of diﬀerent fault-tolerance schemes. We aim to reduce the qubit overhead, especially for small devices. Five-qubit systems are enough to test very limited fault-tolerance schemes [1, 2]. The [[4, 1, 2]] Bacon-Shor subsystem code encodes one logical qubit into four physical qubits. A ﬁfth qubit is needed for fault-tolerant error detection. Since the code has distance two, it can only detect an error, not correct it. Until recently, the smallest known scheme that can correct an error used the [[9, 1, 3]] Bacon-Shor code, plus a tenth ancilla qubit. Although smaller, more eﬃcient error-correcting codes are known, such as the [[5, 1, 3]] perfect code (the smallest distance-three code), the [[7, 1, 3]] Steane code, or the [[8, 3, 3]] code, fault-tolerance schemes using these codes have required at least as many qubits total. For example, Shor-style syndrome extraction [3, 4] requires w + 1 or w ancilla qubits, where w is the largest weight of a stabilizer generator. Steane-style error correction [4–6] uses at least a full code block of extra qubits, and Knill’s scheme [7] uses an encoded EPR state and thus at least two ancilla code blocks. Yoder and Kim [8] have given fault-tolerance schemes using the [[5, 1, 3]] code with eight qubits total, and the [[7, 1, 3]] code with nine qubits total. Extending the latter construction, we introduce fault-tolerance procedures that use only two extra qubits. Figure 1 highlights some examples, and Table I compares the qubit overhead of our scheme to the Shor and Stephens-Yoder-Kim methods (described in Appendix A). In particular, with the [[5, 1, 3]] code our scheme uses only seven qubits total, or ten qubits with the [[8, 3, 3]] code. A particularly promising application is to the [[15, 7, 3]] Hamming code: 17 physical qubits suﬃce to protect seven encoded qubits. In [9], we give fault-tolerant procedures for applying arbitrary Cliﬀord operations on these encoded qubits, also using only two extra qubits, and fault-tolerant universal operations with four extra qubits, 19 total. Substantial fault-tolerance tests can therefore be run on a quantum computer with fewer than twenty qubits. Our procedures here are based on adding “ﬂags” to the syndrome-extraction circuits in order to catch the faults that can lead to correlated errors on the data. Figure 2(c) shows an example. Provided that syndromes are extracted in a careful order, detecting the possible presence of a correlated error is enough to correct it. FIG. 1. Our ﬂagged syndrome-extraction method uses just two ancilla qubits to correct errors fault tolerantly, for a variety of distance-three codes. For example, seven qubits total are enough to correct errors on one encoded qubit, using the [[5, 1, 3]] code, and 17 qubits are enough to protect seven encoded qubits. FIG. 2. Flagged syndrome extraction for the [[5, 1, 3]] code. (a) Stabilizers and logical operators. (b) Circuit to extract the syndrome of the XZZXI stabilizer into an ancilla qubit. This is not fault tolerant because a fault on the ancilla can spread to a weight-two error on the data. (c) This circuit also extracts the XZZXI syndrome, and if a single fault spreads to a data error of weight ≥ 2 then the X measurement will return |−⟩, ﬂagging the failure. (d) The nontrivial data errors that can result from a single gate failure in (c) that triggers the ﬂag; these errors are distinguishable by their syndromes and so can be corrected. [[5, 1, 3]] 5 3 2 ⋄ [[7, 1, 3]] 5 3 2 [8] [[9, 1, 3]] 1 – – [[8, 3, 3]] 7 3 2 [[10, 4, 3]] 9 4 2 [[11, 5, 3]] 9 4 2 ⋄ [[15, 7, 3]] 9 4 2 ⋄ [[31, 21, 3]] 17 8 2 ⋄ [[2r − 1, 2r − 1 − 2r, 3]] 2r−1 + 1 2r−2 2 TABLE I. Numbers of ancilla qubits required for diﬀerent fault-tolerant syndrome-extraction methods. Shor’s method, using a veriﬁed cat state, requires w + 1 ancilla qubits, where w is the largest weight of a stabilizer generator. (The [[9, 1, 3]] Bacon-Shor code is an exception, as it is a subsystem code.) Stephens, Yoder and Kim [8, 10] have proposed using a cat state on only max{3, ⌈w/2⌉} qubits, combined with a decoding trick from [4]; see Appendix A. When applicable, our ﬂagged error-correction procedure needs only two extra qubits, as observed for the [[7, 1, 3]] code by [8]. Codes marked ⋄ are Hamming codes.
Trusted Multi-View Classification<|sep|>Multi-view data, typically associated with multiple modalities or multiple types of features, often exists in real-world scenarios. State-of-the-art multi-view learning methods achieve tremendous success across a wide range of real-world applications. However, this success typically relies on complex models (Wang et al., 2015a; Tian et al., 2019; Bachman et al., 2019; Zhang et al., 2019; Hassani & Khasahmadi, 2020), which tend to integrate multi-view information with deep neural networks. Although these models can provide accurate classiﬁcation results, they are usually vulnerable to yield unreliable predictions, particularly when presented with views that are not well-represented (e.g., information from abnormal sensors). Consequently, their deployment in safety-critical applications (e.g., computer-aided diagnosis or autonomous driving) is limited. This has inspired us to introduce a new paradigm for multi-view classiﬁcation to produce trusted decisions. For multi-view learning, traditional algorithms generally assume an equal value for different views or assign/learn a ﬁxed weight for each view. The underlying assumption is that the qualities or importance of these views are basically stable for all samples. In practice, the quality of a view often varies for different samples which the designed models should be aware of for adaption. For example, in multi-modal medical diagnosis (Perrin et al., 2009; Sui et al., 2018), a magnetic resonance (MR) image may be sufﬁcient for one subject, while a positron emission tomography (PET) image may be required for another. Therefore, the decision should be well explained according to multi-view inputs. Typically, we not only need to know the classiﬁcation result, but should also be able to answer “How conﬁdent is the decision?” and “Why is the conﬁdence so high/low for the decision?”. To this end, the model should provide in accurate uncertainty for the prediction of each sample, and even individual view of each sample. Uncertainty-based algorithms can be roughly divided into two main categories, i.e., Bayesian and non-Bayesian approaches. Traditional Bayesian approaches estimate uncertainty by inferring a posterior distribution over the parameters (MacKay, 1992a; Bernardo & Smith, 2009; Neal, 2012). A variety of Bayesian methods have been developed, including Laplace approximation (MacKay, 1992b), Markov Chain Monte Carlo (MCMC) (Neal, 2012) and variational techniques (Graves, 2011; Ranganath et al., 2014; Blundell et al., 2015). However, compared with ordinary neural networks, due to the doubling of model parameters and difﬁculty in convergence, these methods are computationally expensive. Recent algorithm (Gal & Ghahramani, 2016) estimates the uncertainty by introducing dropout (Srivastava et al., 2014) in the testing phase, thereby reducing the computational cost. Several non-Bayesian algorithms have been proposed, including deep ensemble (Lakshminarayanan et al., 2017), evidential deep learning (Sensoy et al., 2018) and deterministic uncertainty estimate (van Amersfoort et al., 2020). Unfortunately, all of these methods focus on estimating the uncertainty on single-view data, despite the fact that fusing multiple views through uncertainty can improve performance and reliability. In this paper, we propose a new multi-view classiﬁcation algorithm aiming to elegantly integrate multiview information for trusted decision making (shown in Fig. 1(a)). Our model combines different views at an evidence level instead of feature or output level as done previously, which produces a stable and reasonable uncertainty estimation and thus promotes both classiﬁcation reliability and robustness. The Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. In summary, the speciﬁc contributions of this paper are: (1) We propose a novel multi-view classiﬁcation model aiming to provide trusted and interpretable (according to the uncertainty of each view) decisions in an effective and efﬁcient way (without any additional computations and neural network changes), which introduces a new paradigm in multi-view classiﬁcation. (2) The proposed model is a uniﬁed framework for promising sample-adaptive multi-view integration, which integrates multi-view information at an evidence level with the DempsterShafer theory in an optimizable (learnable) way. (4) We conduct extensive experiments which validate the superior accuracy, robustness, and reliability of our model thanks to the promising uncertainty estimation and multi-view integration strategy.
A detailed view of the gas shell around R Sculptoris with ALMA<|sep|>The chemical evolution of evolved stars between 0.8–10 M⊙ on the asymptotic giant branch (AGB) is driven by thermal pulses (TP). Thermal runaway burning of helium in a shell around the dormant carbon–oxygen core leads to a mixing of the inner layers of the star, resulting in an intricate nucleosynthetic network and the production of new heavy elements (e.g. Karakas & Lattanzio 2007). These elements are mixed to the surface of the star through deep convection during the third dredge-up after the pulse and are incorporated into the stellar wind. The winds from AGB stars replenish the interstellar medium (ISM) with this newly processed material (e.g. Schneider et al. 2014). The stellar mass loss is high enough (up to 10−4 M⊙ yr−1) to eventually terminate the evolution of the star on the AGB with most of the stellar mass lost to the ISM (e.g. Herwig 2005). As such, AGB stars are signiﬁcant contributors to the chemical evolution of the ISM and galaxies. The material is included in molecular clouds where new stars and planets are formed. The phase of rapid helium burning during a TP cycle lasts only a few hundred years (Vassiliadis & Wood 1993). Although the helium luminosity experiences a large increase, the surface luminosity only increases slightly. However, this increase in luminosity leads to additional radiation pressure on the dust grains in the upper atmosphere of the star and, combined with a lower temperature and larger radius, results in an increased mass-loss rate and expansion velocity. After He-burning ceases, the surface luminosity, and hence the mass-loss rate and expansion velocity, decrease again (Steﬀen & Sch¨onberner 2000; Mattsson et al. 2007) An indication of highly variable mass loss from AGB stars was ﬁrst observed in CO emission in the form of detached shells around carbon AGB stars, and a formation connected to TPs was suggested (Olofsson et al. 1988, 1990). Since then, detached shells of dust and gas around ∼ten carbon AGB stars have been observed. The strongest argument for the connection between detached-shell sources and TPs are the detection statistics that are based on a volume-limited sample of carbon stars (Olofsson et al. 1993). No detached CO shells have been observed around oxygen-rich (M-type) AGB stars, where dust opacity eﬀects possibly prevent the formation of a shell during the TP (e.g. Bladh
The Hagedorn temperature Revisited<|sep|>In 1965 Hagedorn 1 proposed that the number of hadronic resonances increases exponentially with the mass m of the resonances. The idea, which was debated strongly when ﬁrst proposed, has since been widely accepted and discussed by many authors 2,3,4,5,6,7,8,9,10,11. The concept was based on the assumption that the observed increase in the number of hadronic resonances would continue towards higher and higher masses as more experimental data became available 12. The scale of the exponential increase determines the value of the Hagedorn temperature, TH. Recent papers 13,14,15,16,17,18 have used the latest results from the Particle Data Group 12 to revisit the original analysis of Hagedorn to update the value of TH. This resulted in a surprising wide spread of possible values, with large variations as to whether one considers mesons or baryons with values ranging from TH = 141 MeV to TH = 340 MeV depending on the parametrization used and on the set of hadrons (mesons or baryons). There thus exists uncertainty as to the value of the Hagedorn temperature. These have two origins: • sparse information about hadronic resonances certainly above 3 GeV, • the analytical form of the Hagedorn spectrum, especially the factor multiplying the exponential.
Deciphering a novel image cipher based on mixed transformed Logistic maps<|sep|>In the cyber era facing 5G (5th generation) mobile networks, all kinds of security problems about image data are encountering serious challenges [Li & Lo, 2011; Li et al., 2015]. The seeming similarity between chaos and cryptography promoted their combination to design eﬃcient and secure encryption schemes,
Comparative study of the discrete velocity and lattice Boltzmann methods for rarefied gas flows through irregular channels<|sep|>When the mean free path λ of gas molecules is comparable to or even larger than the characteristic ﬂow length H, the Navier-Stokes (NS) equations derived from the continuum-ﬂuid hypothesis fails, and the gas kinetic theory based on the fundamental Boltzmann equation for the velocity distribution function (VDF) of gaseous molecules is required to describe rareﬁed gas dynamics. According to the Chapman-Enskog expansion, NS equations are only the ﬁrst-order approximation in the Knudsen number (Kn = λ/H) to the Boltzmann equation [1]. Therefore, they are valid in the continuum ﬂow regime where Kn ≲ 0.001 [2]. As the Knudsen number increases, higher-order (non-equilibrium) terms begin to dominate, and NS equations gradually loss validity; for instance, when Kn is large, non-equilibrium terms embodied in the macroscopic mass, momentum and energy transport cannot be expressed in terms of the lower-order macroscopic quantities. The non-equilibrium eﬀects not only cause noticeable velocity slip and temperature jump at solid surfaces in the slip ﬂow regime (0.001 ≲ Kn ≲ 0.1), but also modify the constitutive relations in the transition (0.1 ≲ Kn ≲ 10) and free-molecular (10 ≲ Kn) ﬂow regimes such that the Newton’s law for stress and strain and Fourier’s law for heat ﬂux and temperature gradient do not hold anymore. This leads to a number of counterintuitive phenomena, including the thermal transpiration where gas molecules along solid surface move from the cold region to hot [3], the Knudsen paradox where the dimensionless mass ﬂow rate in Poiseuille ﬂow could increase when the gas pressure decreases [4], the temper ature bimodality in the force-driven Poiseuille ﬂow [5], the inverted velocity in cylindrical Couette ﬂow [6], and the gas anti-resonance where the shear stress in an oscillating lid-driven cavity ﬂow could be smaller than that of the one-dimensional (1D) Couette ﬂow [7]. In the past decades, due to the rapid development of microelectromechanical systems [8] and the shale gas revolution in North America [9], extensive works have been devoted to construct eﬃcient numerical schemes to simulate gas ﬂows at the micro scale, where the ﬂow velocity is usually far smaller than the most probable speed of the gas molecules. In most of these applications, gas ﬂows vary from the slip to the free-molecular ﬂow regimes and the gas-surface interaction dominates ﬂow behavior. High-ﬁdelity numerical methods to solve the Boltzmann equation include the numerical kernel method [10], the conservative projection-interpolation method [11], the low-variance direct simulation Monte Carlo method [12], and the fast spectral method [13], to name just a few. Due to the high computational cost, however, the Boltzmann equation is usually simpliﬁed by the BhatnagarGross-Krook (BGK) equation under the single relaxation time (SRT) approximation [14], which is often solved by the discrete velocity method (DVM) where the continuous molecular velocity space is represented by a few number of discrete velocities [15–19]. Generally speaking, rareﬁed gas ﬂows with higher values of Kn need a larger number of discrete velocities to resolve the large variations/discontinuities in the VDF, for instance see the numerical examples in Refs. [13, 20]. Rooted from the gas kinetics, the lattice Boltzmann method (LBM) is a popular and powerful tool in modeling the NS hydrodynamics and beyond. Historically, the SRT-LBM is ﬁrstly developed as an alternative solver for the NS equations. Since it uses a very limited but highly optimized number of discrete velocities, e.g. the D2Q9 scheme for 2D problems, the SRT-LBM can be viewed as a special type of DVM to solve the BGK equation [21]. To capture the non-equilibrium eﬀects beyond the NS hydrodynamics, a rigorous procedure for obtaining high-order approximations to the BGK equation is proposed [22] and tested in various canonical problems [23–29]. By expanding the VDF into high-order Gauss-Hermite polynomials or Gauss quadrature in the spherical coordinate system, it is found that, with a not very large number of discrete lattice velocities, e.g. D2Q36 or D2Q64, high-order LBM schemes accurately describe rareﬁed gas ﬂows up to the early transition ﬂow regime, i.e. Kn ≲ 0.5. In addition to high-order quadrature, Zhang et al. [30] attempted to capture the Knudsen layer structure in rareﬁed gas ﬂows within the framework of the SRT-LBM, but by introducing the gas kinetic boundary condition and the eﬀective relaxation time that is a function of the gas-wall distance. In the pressure-driven ﬂow, this “wall-scaling” approach provides a signiﬁcant improvement for Knudsen numbers up to 0.5. Later, this scheme has also been applied to study the rareﬁed thermal [31] and oscillatory Couette ﬂow [32] in the early transition ﬂow regime with great success. Further improvement has been achieved by Guo, Zheng and Shi [33], who developed a numerical scheme within the framework of multiple relaxation time (MRT) LBM. In addition to the “wall-scaling” of the relaxation time, the combined bounce-back and specularreﬂection boundary condition is designed, such that the new scheme is equivalent to solve the NS equations with the second-order slip boundary condition in the slip ﬂow regime. In the Poiseuille ﬂow between two parallel plates, it is found that this MRT-LBM scheme is even able to predict the velocity proﬁle and mass ﬂow rate with good accuracy, in the whole transition ﬂow regime. For this reason, the MRT-LBM has attracted signiﬁcant attentions [33–37]. In the simulation of rareﬁed gas ﬂows, it is quite astonishing that the MRT-LBM of Guo et al. with fewer number of discrete velocities [33] are more accurate than the LBM of higher-order quadrature [22–29]. Although Guo et al. honestly and explicitly pointed out that their scheme is only designed and analyzed for plane walls [33] and is hard to be extended to generalized gas-surface boundaries [38], it has been applied to study the rareﬁed gas ﬂows through the microchannel with rough surface and complex porous media [36, 37]. However, whether this scheme works on walls with curvatures and other complex geometries or not is not clear. In this paper, aiming to address the accuracy of the MRT-LBM in simulating rareﬁed gas ﬂows in complex geometries, the BGK equation will be solved by the DVM, and numerical results will be compared to those of MRTLBM for the Couette ﬂow through rough microchannels [36] in Sec. II, and for the Poiseuille ﬂow through porous media [37] in Sec. III. FIG. 1. Schematic of the Couette ﬂow in a rough channel. Shaded regions are solid surfaces, while the gas ﬂows are in the white region.
Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models<|sep|>Eye tracking provides an accurate millisecond record of where people are looking while reading and is useful for descriptive study of language processing and understanding of the cognitive processing of brain related to reading. Eye movements are many times language-speciﬁc because they depend on structure and ordering of words which are language dependent , however some features tend to be stable and universal and can be observed in all languages. Modeling of human reading has been widely explored in psycholinguistics. The ability to accurately predict eye tracking between languages pushes this ﬁeld forward, facilitating comparisons between models and analysis of their various functions. In this paper , we compare our eye-tracking prediction results with some simple baselines using token-level features, which we improve upon with
Alternating Heisenberg Spin-1/2 Chains in a Transverse Magnetic Field<|sep|>The eﬀect induced by external magnetic ﬁelds in the low-dimensional magnets has attracted much interest recently from experimental and theoretical points of view. In particular, critical properties of the alternating spin1/2 chains in a magnetic ﬁeld have been a ﬁeld of intense studies. This seems pertinent in the face of great progress made within the last years in fabrication of such AF-F compounds. Since, the AF-F chains have a gap in the spin excitation spectrum, they reveal extremely rich quantum behavior in the presence of the magnetic ﬁeld. A typical example of the ferromagnetic-dominant AFF bond alternating chain is (CH3)2CHNH3CuCl3 (isopropylammonium coppertrichloride: IPACuCl3).1, 2) The energy gap in the absence of the external magnetic ﬁeld is estimated from the susceptibility to be 17 − 18K.1) This value is also conﬁrmed by the analysis of the speciﬁc heat.2) From the viewpoint of the crystal structure, the origin of the spin gap was expected to be the spin1/ 2 AF-F alternating chain along the c-axis.1) However, quite recently, it was suggested that this system should be characterized as a spin ladder along the a-axis with the strongly coupled ferromagnetic rungs, namely the antiferromagnetic chain with eﬀective S = 1, and the excitation gap ∆ was estimated as 13.6K by means of the neutron inelastic scattering experiments.3) More recently, it was reported new inelastic neutron scattering results for (CH3)2NH2CuCl3 (Dimethylammonium copper II chloride, also known as: DMACuCl3 or MCCL). The linked-cluster calculations and the bulk measurements show that DMACuCl3 is a realization of the spin-1/2 alternating AF-F chain by nearly the same strength of the antiferromagnetic and ferromagnetic couplings.4) There are other examples for the AF-F alternating spin-1/2 chain compounds such as: [Cu(TIM)]CuCl4 (TIM=2,3,9,10-tetramethyl-1,3,8,10-tetraenecyclo1,4,8,11-tetraazatetradecane) and (4 − BzpipdH)CuCl3 (4-BzpipdH=4-benzylpiperidinium),5, 6) and CuNb2O6.7) Theoretically, the AF-F alternating chain is expected to have relation to the Haldane-gap systems,8) since they are regarded as the spin-1 antiferromagnetic chain in the large ferromagnetic coupling limit. The energy gap exist between the ground state and the ﬁrst excited state. The spin correlation function of the ground state decays exponentially as well as the spin-1 AF chain. The ground state properties9–15) and low-lying excitations16) of this model were well investigated by numerical tools and variational schemes. In particular, the string order parameter originally deﬁned for the spin-1 Heisenberg chains17) was generalized to this system. The ground state has the long-range string order, which is characteristic of the Haldane-gap phase. Hida has shown that the Haldane phase of the AF-F alternating chain is stable against any strength of the randomness.18) The ground state phase diagram of the AF-F alternating chain in a magnetic ﬁeld is studied by the numerical diagonalization and the ﬁnite-size scaling based on the conformal ﬁeld theory.12) It is shown that the magnetic state is gapless and described by the Luttinger liquid phase. It is also found that the magnetic state is characterized by the algebraic decay of the spin correlation functions. Recently, Yamamoto et. al described the magnetic properties of the model in a magnetic ﬁeld in terms of the spinless fermions and the spin waves.19) They employed the Jordan-Wigner transformation and treated the fermionic Hamiltonian within the Hartree-Fock approximation. They have also implemented the modiﬁed spin wave theory to calculate the thermodynamic functions as the speciﬁc heat and the magnetic susceptibility. The eﬀect of an uniform transverse magnetic ﬁeld on the ground state phase diagram of a spin-1/2 AFF chain with anisotropic ferromagnetic coupling is much less studied. Partly, the reason is that AF-F alternating chains with anisotropic ferromagnetic coupling are not still fabricated. However, from the theoretical point of view these systems are extremely interesting, since they open a new wide polygon for the study of complicated quantum behavior, unexpected in the more conventional spin systems. The Hamiltonian of the model under con
Transitivity of codimension one Anosov actions of R^k on closed manifolds<|sep|>It is nowadays a common sense that the Anosov systems lie in the central heart of the theory of dynamical systems, as the most perfect kind of global hyperbolic behavior. It has strong connections with algebra, natural examples arising from number ﬁeld theory or Lie groups theory (see also [14] for an example illustrating the deep interplay between Anosov systems and representation theory), and also with topology, the dynamics of an Anosov system usually reﬂecting the ambient manifold topology. The notion has been introduced by V.V. Anosov in the 60’s in [1], but one should also consider previous works by precursors, including Hadamard, Morse, etc... An Anosov system is (topologically) transitive if it admits a dense orbit. There is a quite extensive literature devoted to transitivity for certain classes of Anosov systems. In particular, by a celebrated result of Newhouse [18] and Franks [8], every codimension one Anosov diﬀeomorphism on a compact manifold is topologically mixing (more than transitive). As a corollary from this theorem and [8, Corollary (6.4)], up to ﬁnite coverings, Anosov diﬀeomorphisms of codimension one (i.e. such that the unstable subbundle has dimension one) on closed manifolds of dimension ≥ 3 are topologically conjugate to hyperbolic toral automorphisms. For ﬂows, in the three-dimensional case, Franks–Williams [9] construct an Anosov ﬂow that is not topologically transitive. In the higher dimensional Date: February 18, 2013. 2000 Mathematics Subject Classiﬁcation. Primary: 37C85. Key words and phrases. Anosov action, compact orbit, closing lemma, robust transitivity, Anosov ﬂow. The second author would like to thank CNPq of Brazil for ﬁnancial support Grant 200464/2007-8.
Outlier-Robust Filtering For Nonlinear Systems With Selective Observations Rejection<|sep|>Sensors play an important role in the functionality of different physical dynamical systems providing useful data for estimation of key system parameters and effectively controlling the underlying processes. To describe dynamical systems, State-Space Models (SSMs) are widely used in diverse applications such as cyberphysical systems, robotics, sensor fusion, navigation, guidance, and tracking systems [1]–[8]. In an SSM, the system is described by latent states evolving with ﬁrst-order (Markovian) dynamics. The states are not directly observable rather manifest through a set of external outputs measured by sensors. Kalman Filter (KF) and its variants [9]–[11] are benchmark state estimation methods for SSMs. These ﬁlters assume precise apriori knowledge of the system’s noise statistics. However, in practice, the data from different sensors can easily be corrupted with outliers due to factors like inherent sensor quality or its degradation over time, communication glitches, environmental effects etc. This results in divergence of the actual and assumed noise statistics making KFs ineffective [12]. Therefore, the development of robust ﬁlters remains a busy research area [6], [13], [14]. Several popular robust ﬁltering methods, classiﬁed as compensation-based methods, aim to utilize information from the outlier-ridden observations for inference updates. Some of these techniques assume prior statistics of the measurement The authors are with Department of Electrical Engineering, Lahore University of Management Sciences, DHA Lahore Cantt., 54792, Lahore Pakistan. (email: aamir.chughtai@lums.edu.pk; tahir@lums.edu.pk; momin.uppal@lums.edu.pk) noise or the residuals. These include methods based on robust statistics [15]–[17] and methods based on modeling the observation noise as Student-t or Laplacian distribution [18], [19]. Their performance is effective owing to the sporadic nature of outliers. However, since these methods are based on static loss functions based on design parameters, meticulous tuning of these parameters is required [20]. Therefore, tuningfree learning-based compensation approaches have also been advocated in the literature [20]–[23]. These methods assume a distribution to describe the measurement noise and subsequently aim to learn the parameters of the distribution. Other methods, known as rejection-based approaches, stem from the argument that generally outliers come from clutter and do not necessarily obey a well-deﬁned distribution. Therefore, corrupted measurements should be completely discarded for state estimation. Traditionally, this is performed by comparing the normalized measurement residuals with some predeﬁned thresholds [24]–[26]. However, the selection of the threshold is mostly subjective. Some theoretical justiﬁcations for threshold selection are provided in an extended KF (EKF) based method [27]. However, the method is tested only for low outliers frequency and needs memory for past observations. Different learning-based rejection approaches have also been proposed in literature [12], [28]. These techniques aim to learn the parameters that determine whether to use (or discard) the observations for state estimation. Compared to the traditional counterparts, learning-based robust ﬁlters have advantages in terms of minimizing user discretion, being more general, and better suited for single-shot applications [20]. As exact inference gets analytically infeasible in their development, approximate inference techniques like Particle Filters (PFs) and Variational Bayesian (VB) methods are usually employed. Since PFs can be computationally very expensive, VB methods are the popular alternatives to devise tractable robust ﬁlters. With this background, we only consider VB-based outlier-robust ﬁlters in this work. Note that for robust estimation non-Bayesian paradigms have also been investigated e.g. considering ambiguity sets to cater for model distributional uncertainties, formulating and solving different optimization problems etc. [29]–[31]. Connections between the Bayesian and other approaches are being currently explored under different assumptions [29], [32]. In this work, we restrict our attention to the Bayesian methods for robust ﬁltering. In several applications, the measurements are commonly obtained from independent sensors e.g. in applications like robotics, wireless sensor networks, Internet of Things (IoT) etc. As the number of sensors and data acquisition rates can be high in such applications, with limitations on the processing power, specialized outlier-robust ﬁltering is required. Estimation quality and computational overhead are the primary design criteria for devising such ﬁlters. A review of existing works indicates that tractable learning-based robust ﬁlters are generally under-parameterized i.e. devised considering common parameters for all the measurement dimensions for outlier compensation. While it is acceptable when the measurements are inter-dependent, it results in loss of useful information and hence robustness, for independent observations. Contributions: The speciﬁc contributions of this work are as follows. • We offer a better parameterization of outliers to avoid needless information loss. We treat the outliers independently when the measurements are obtained from independent sensors. • Using VB inference and general Gaussian ﬁltering, we devise an outlier-robust learning-based tuning-free ﬁlter for nonlinear SSMs, which discards only the corrupted measurements during inference. We consider nonlinear systems in our study in contrast to the methods that are devised for linear systems [33], [34]. Unlike the method in [35], we do not take any restrictive assumption that outliers can occur in at most one measurement dimension at any given instant. • In addition, we propose modiﬁcations to the existing baseline methods to enable them deal with outliers selectively for each dimension. • We also present a modiﬁcation to our originally devised method that is computationally more efﬁcient as the modiﬁcations of the rival methods and exhibit similar estimation quality. • By comparing our method with other learning-based tractable approaches using simulations we verify the performance gains. Moreover, experimental evaluation for different indoor localization scenarios using UWB sensors shows the practical efﬁcacy of the proposed ﬁlter. We present the remaining article with the following organization. In Section II modeling description of the proposed ﬁlter is presented. Section III provides derivation details of the basic proposed method. Subsequently, in Section IV performance evaluation results of the considered methods and their proposed modiﬁcations are discussed. Finally, the conclusions drawn from the work are reported in Section V.
Upper critical field of high quality single crystals of KFe$_2$As$_2$<|sep|>Among several families of iron arsenides showing superconductivity at temperatures up to 56 K,1–4 very few compounds are stoichiometric. Due to the lack of substitution disorder, these compounds reveal the properties of true clean materials and are characterized by signiﬁcantly enhanced residual resistivity ratios, rrr ≡ ρ(300K)/ρ(Tc) up to ∼80 in LiFeAs (Tc ≈18 K),5 The superconducting Tc and rrr of the latter material strongly vary8–10 depending upon sample quality and preparation technique, and are very sensitive to doping with Co11 and Na,12 suggestive of unconventional superconductivity. Indeed, all studies of the superconducting gap structure in K122 agree on the existence of line nodes,7–14 however, their location on the multi-band Fermi surface, symmetry-imposed vs. accidental character and relation to S± or d−wave symmetry15,16 are highly debated. In this study we use high sensitivity of the superconducting transition temperature of KFe2As2 to residual impurities to obtain an insight into another unique feature of iron arsenide superconductors,- unusual temperature dependence of the upper critical ﬁeld. Terashima et al.17 reported anisotropic Hc2 of the K122 crystals with Tc=2.8 K, which revealed very diﬀerent temperature dependence for magnetic ﬁelds parallel and perpendicular to the c-axis of the tetragonal crystal, with virtually T-linear dependence for H ∥ c. This dependence is diﬀerent from the expectations of theories for orbital Werthamer, Helfand and Hohenberg (WHH)18 and paramagnetic19 mechanisms of Hc2, both predicting saturation of Hc2(T) on T → 0. It is also strongly diﬀerent from saturating Hc2 found in LiFeAs.20–22 Recently we found similar T-linear dependence in the optimallydoped SrFe2(As1−xPx)2, x=0.35.23 Because of the nodal superconducting gap of this compound,24 we speculated possible link of nodal superconducting gap and the Tlinear dependence of Hc2. On the other hand, T-linear and doped MgB226 was explained in the orbital-limiting model for two-band superconductivity in the dirty limit, as a cross-over regime between usual WHH saturating and upward curving dependences. In this article, we report synthesis of single crystals of KFe2As2 with rrr up to about 3000 and study their anisotropic upper critical ﬁeld. We ﬁnd higher Hc2 for both directions of magnetic ﬁeld than found by Terashima et al..17 Interestingly, the two data sets for this material, which in both high and low quality samples is in the clean limit, can be matched by a factor corresponding to Tc ratio. As we show, this unusual linear dependence between Hc2(0) and Tc is not expected in any theory for clean superconductors for orbital limiting mechanism. We discuss possible important parameters for its explanation.
Online Dynamic Motion Planning and Control for Wheeled Biped Robots<|sep|>robots in a structured environment. However, legged robots are more capable at traversing challenging terrains such as stairs and narrow trenches. Wheeled-legged robots have the potential to combine the best of both worlds. In this paper, we will focus on hybrid locomotion for wheeled biped robots. Hybrid locomotion refers to simultaneous rolling and walking motion as shown in Fig. 1. It can help the robot stepping over or around obstacles. Such motion is difﬁcult to realise on wheeled biped robots since the robot needs to balance in both forward and lateral directions simultaneously. The balancing problem in the forward direction has been studied extensively on two-wheeled self-balancing robots. The balancing issue in the lateral direction is actually a walking problem and existing studies on biped robots can provide a lot of insight. We unify the two in this paper to realise hybrid locomotion. on balancing in the sagittal plane (pitch motion) [1]. The most commonly used template model is the Wheeled Inverted Pendulum Model (WIPM) [2]. Linearization of this non-linear model around its upright stable equilibrium conﬁguration is needed to enable linear state feedback control [3]. Alternatively, a non-linear approach can be directly applied. In [4] differential dynamic programming (DDP) and model predictive control (MPC) are applied to generate whole body motion for a wheeled humanoid robot. However, the computation time is not given. In this paper, we propose the new Cart-LIPM model for controlling the motion in this direction. It remains linear in a large range of motion compared to the WIPM linearized around a ﬁxed point. Due to its linearity, it enables a much higher MPC update frequency comparing to the non-linear MPC. for biped robots [5] [6]. An important consideration when using this model for walking motion generation is the actuation type associated with the foot. Fully-actuated LIPM assumes planar feet and ankle torques actuation, as a result, the zero moment point (ZMP) can be modulated inside the supporting area. In contrast, the under-actuated LIPM assumes point feet and therefore it has no insole ZMP modulation capability at all. If treating the walking motion generation as a footstep optimization problem, different formulations exist based on the model that has been used. Considering the fullyactuated LIPM, automatic footstep placement [7] is proposed to simultaneously optimize footstep placements and ZMP trajectories. Formulations considering under-actuated LIPM are also proposed [8] [9]. In those formulations, only footstep locations are optimized. These works introduce a similarity regularization term to penalize the deviation of the optimized footsteps from the referenced ones. In our previous work [10], we replaced the absolute similarity minimization term with a relative one which removes the requirement of the reference footsteps generation plan and makes the footstep optimization truly automatic. In this paper, we adopt the same formulation for lateral stepping motion generation. been demonstrated in [11] [12] [13]. These robots use the legs only as an active suspension system while driving around. Furthermore, the authors impose a quasi-static assumption which limits the type of motion the robot is able to achieve. The work presented in [14] and [15] exploits the full robot dynamics of a wheeled humanoid robot which allows for generating joint torque commands and to achieve compliant interaction. However, the large mobile base of the robot makes it difﬁcult to deal with obstacles or uneven terrain. mal demonstrate robust dynamic hybrid locomotion capability. The authors of [16] [17] [18] proposed different trajectory optimization (TO) formulations. These formulations integrate the wheels into the control framework so that the robot is capable of performing walking and driving simultaneously. considered by [4] [19] [20]. The common morphology of these wheeled humanoid robots is that the two wheels are directly connected to the same base link. This limits these robots to only use the wheels for driving. In contrast to this, the Ascento [21] robot from ETH and the Handle robot [22] from Boston Dynamics have wheels attached to the leg structure. This makes it possible to potentially use the wheeled leg for walking motions. To overcome obstacles the authors implemented jumping motions. Although this approach is effective, it is not always efﬁcient to avoid obstacles by jumping, i.e. when the obstacle only blocks part of the way of the robot. In this case, the robot can step over the obstacle. This motion essentially requires the robot to be able to balance while it has only one leg on the ground. In this paper, we will demonstrate how this can be achieved through our proposed motion synthesis approach. LIPM to generate hybrid motions for wheeled biped robots. To the best of our knowledge, we are the ﬁrst to apply Cart-LIPM for rolling motion generation. rolling and walking motions. It differentiates this work from many existing wheeled legged robots with only one pitch joint in the ankle, such as the wheeled ANYmal [16], Ascento [21] and Handle [22]. constraints on the wheel are deﬁned with respect to the center of the wheel instead of the contact point which gets rid of the extra wheel orientation parametrization as needed in [16].
Unconventional quantum Hall effects in two-dimensional massive spin-1 fermion systems<|sep|>One of the most important and intriguing phenomena in condensed matter physics is the quantum Hall effect, the phenomenon that the Hall conductance becomes quantized (i.e., σxy = −ne2/h with n being integer and e being the electric charge) at low temperature in 2D electron gases subject to strong magnetic ﬁelds. This eﬀect is originated from the formation of LLs with a quantized Thouless-Kohmoto-Nightingale-den Nijs (TKNN) number1 in these systems with magnetic ﬁelds. Each occupied LL contributes a −e2/h Hall conductance and if no LLs are occupied, the Hall conductance completely vanishes as shown in Fig. 1(a). Apart from the conventional QHE in 2D electron gases with parabolic dispersion, the unconventional QHE in 2D Dirac materials with relativistic dispersion such as a single layer graphene and bilayer graphene was discovered that the Hall conductance can take only odd or even numbers2–7 (without considering the spin freedom) as shown in Fig. 1(b) and (c). In addition, these materials always exhibit the nonzero QHE plateaus because of the existence of zero energy LLs unless these zero energy levels are gapped and neither particle nor hole LLs are occupied. Other than 2D, the study of relativistic fermions such as Dirac and Weyl fermions in three-dimensional systems, a counterpart of Dirac fermions in graphene, has seen a rapid progress8–16. Beyond these fermions permitted in particle physics, a new fermion violating Lorentz invariance17,18, which is called type-II Weyl fermions18 (also called structured Weyl fermions17), has been discovered in superﬂuids17 and condensed matter materials18–21. Recently, another type of new fermions in three dimensions beyond conventional fermions in particle physics were predicted in solid-state materials22–28; they are named unconventional fermions with highly degenerate points that are described by an eﬀective Hamiltonian H = k · ˆS with k being the momenta and ˆS being the angular momentum matrices. These highly degenerate fermions have also been studied in 2D systems29–48, and solid-state materials48. For massless spin-1 fermion systems without gaps, QHEs have been investigated and it has been found that each spin-1 point contributes an integer Hall conductance39. The conductance vanishes when the Fermi surface lies in the ﬁrst gap of Landau levels (LLs) (the gap between the zero energy and the ﬁrst nonzero energy levels) despite the existence of an unlimited number of zero energy LLs. For the dice model where there exist a pair of spin-1 points, the Hall conductance takes even numbers, reminiscent of the bilayer grephene36. In this paper, we study the QHE in a 2D system with a pair of spin-1 fermions with a mass term mz ˆSz. By exploring a continuous model under magnetic ﬁelds, we ﬁnd that the inﬁnite degeneracy of zero energy LLs in the massless case is lifted in the massive case, and these LLs develop into a series of nonzero energy levels. Interestingly, these levels also contribute to the Hall conductance as the Fermi surface lies in the gap among these levels, leading to a novel QHE that the Hall conductance revives in the opposite direction as an inﬁnite ladder of ﬁne staircase when the Fermi surface is moved into the ﬁne structure developed from the original zero energy bands and then experience a sudden reversal with its sign being ﬂipped due to a Van Hove singularity when the Fermi surface is moved across zero energy. We further study the QHE in the dice model with a pair of massive spin-1 fermions by calculating its LLs, Chern numbers and chiral edge states. All these results are in good agreement with the continuous model’s ones.
On the Null Space Constant for $l_p$ Minimization<|sep|>An important problem that often arises in signal processing, machine learning, and statistics is sparse recovery [1–3]. It is in general formulated in the standard form where the sensing matrix A ∈ RM×N has more columns than rows and the ℓ0 “norm” ∥x∥0 denotes the number of nonzero entries of the vector x. The combinatorial optimization (1) is NP-hard and therefore cannot be solved eﬃciently [4]. A standard method to solve this problem is by relaxing the non-convex discontinuous ℓ0 “norm” to the convex ℓ1 norm [5], i.e., argmin x ∥x∥1 subject to Ax = y. (2) ∗The authors are with the Department of Electronic Engineering, Tsinghua University, Beijing 100084, China. The corresponding author of this work is Yuantao Gu (e-mail: gyt@tsinghua.edu.cn). It is theoretically proved that under some certain conditions [5,6], the optimum solution of (2) is identical to that of (1). Some works try to bridge the gap between ℓ0 “norm” and ℓ1 norm by non-convex but continuous ℓp “norm” (0 < p < 1) [7–10], and consider the ℓp minimization problem where ∥x∥p p = �N i=1 |xi|p. Though ﬁnding the global optimal solution of ℓp minimization is still NP-hard, computing a local minimizer can be done in polynomial time [11]. The global optimality of (3) has been studied and various conditions have been derived, for example, those based on restricted isometry property [7–9,12] and null space property [10,13]. Among them, a necessary and suﬃcient condition is based on the null space property and its constant [10,13,14]. Deﬁnition 1. For any 0 ≤ p ≤ 1, deﬁne null space constant γ(ℓp, A, k) as the smallest quantity such that � holds for any set S ⊂ {1, 2, . . . , N} with #S ≤ k and for any vector z ∈ N(A) which denotes the null space of A. It has been shown that for any p ∈ [0, 1], γ(ℓp, A, k) < 1 is a necessary and suﬃcient condition such that for any k-sparse x∗ and y = Ax∗, x∗ is the unique solution of ℓp minimization [10]. Therefore, γ(ℓp, A, k) is a tight quantity in indicating the performance of ℓp minimization (0 ≤ p ≤ 1) in sparse recovery. However, it has been shown that calculating γ(ℓp, A, k) is in general NP-hard [15], which makes it diﬃcult to check whether the condition is satisﬁed or violated. Despite this, properties of γ(ℓp, A, k) are of tremendous help in illustrating the performance of ℓp minimization, e.g., non-decrease of γ(ℓp, A, k) in p ∈ [0, 1] shows that if ℓp minimization guarantees successful recovery of all k-sparse signal and 0 ≤ q ≤ p, then ℓq minimization also does [10]. In this letter, we give some new properties of the null space constant γ(ℓp, A, k). Speciﬁcally, we prove that γ(ℓp, A, k) is strictly increasing in k and is continuous in p. For random sensing matrix A, the non-decrease of γ(ℓp, A, k) in p can be improved to strict increase with probability 1. Based on them, the performance of ℓp minimization can be intuitively demonstrated and understood.
Automatic morphological classification of galaxy images<|sep|>In the past several years autonomous sky surveys have been becoming increasingly important, and large datasets of astronomical images have been generated and become available by these ventures. The availability of these large datasets has introduced the need for tools that can automatically analyze astronomical images. This includes the need for automatic morphological classiﬁcation of celestial objects that appear inside an astronomical frame. One approach to classiﬁcation of large sets of galaxy images, which was successfully adopted by the Galaxy Zoo project (Lintott et al. 2008), allows hobbyist volunteers to log-in and manually classify galaxies via the project web site. The galaxy images are acquired by the Sloan Digital Sky Survey (SDSS), and displayed by Galaxy Zoo as JPEG images scaled by 0.024Rp, where Rp is the Petrosian radius (Petrosian 1976) for the galaxy. While each volunteer can classify just a limited number of galaxies, the eﬃcacy of the data analysis is enabled by the availability of a very large number of human observers. However, the bottleneck introduced by the manual analysis limits the ability of this method to provide quick analysis of massive galaxy datasets. Here we describe a software tool that can be used for automatic classiﬁcation of galaxy images. The algorithm was originally developed for automatic analysis of cell morphology, but its general-purpose design allows it to be eﬀective for applications outside the scope of cell biology. Full com pilable source code can be freely downloaded. In Section 2 we brieﬂy describe the algorithm, and in Section 3 the experimental results are discussed.
Decaying Dark Atom constituents and cosmic positron excess<|sep|>The possibility of dark matter being in the form of “dark atoms” has been studied extensively [1–21]. In this scenario new stable particles are bound by new dark forces (like mirror partners of ordinary particles bound by mirror electromagnetism [22–26]). However, it turns out that even stable electrically charged particles can exist hidden in dark atoms, bound by ordinary Coulomb interactions (see [27–30] and references therein). Stable particles with charge -1 (and corresponding antiparticles as tera-particles [31]) are excluded due to overproduction of anomalous isotopes. However, negatively doubly charged particles are not constrained by anomalous isotope searches as much as -1 charged particles [32]. There exist several types of particle models where heavy stable -2 charged species, O−−, are predicted: All these models also predict corresponding +2 charge particles. If these positively charged particles remain free in the early Universe, they can recombine with ordinary electrons in anomalous helium, which is strongly constrained in terrestrial matter. Therefore a cosmological scenario should provide a mechanism which suppresses anomalous helium. There are two possible mechanisms than can provide a suppression: (i) The abundance of anomalous helium in the Galaxy may be signiﬁcant, but in terrestrial matter a recombination mechanism could suppress this abundance below experimental upper limits [33, 35]. The existence of a new U(1) gauge symmetry, causing new Coulomb-like long range interactions between charged dark matter particles, is crucial for this mechanism. This leads inevitably to the existence of dark radiation in the form of hidden photons. (ii) Free positively charged particles are already suppressed in the early Universe and the abundance of anomalous helium in the Galaxy is negligible [29,44]. These two possibilities correspond to two diﬀerent cosmological scenarios of dark atoms. The ﬁrst one is realized in the scenario with AC leptons, forming neutral AC atoms [35]. The second assumes a charge asymmety of the O−− which form the atom-like states with primordial helium [29,44]. If new stable species belong to non-trivial representations of the SU(2) electroweak group, sphaleron transitions at high temperatures can provide the relation between baryon asymmetry and excess of -2 charge stable species, as it was demonstrated in the case of WTC [37,45–47]. After formation in the Big Bang Nucleosynthesis (BBN), 4He screens the O−− charged particles in composite (4He++O−−) OHe “atoms” [44]. In all the models of OHe, O−− behaves either as a lepton or as a speciﬁc “heavy quark cluster” with strongly suppressed hadronic interactions. Therefore OHe interactions with matter are determined by the nuclear interactions of He. These neutral primordial nuclear interacting objects can explain the modern dark matter density and represent a nontrivial form of strongly interacting dark matter [48–56]. The cosmological scenario of the OHe Universe can explain many results of experimental searches for dark matter [29]. Such a scenario is insensitive to the properties of O−−, since the main features of the OHe dark atoms are determined by their nuclear interacting helium shell. In terrestrial matter such dark matter species are slowed down and cannot cause signiﬁcant nuclear recoil in the underground detectors, making them elusive in direct WIMP search experiments (where detection is based on nuclear recoil) such as CDMS, XENON100 and LUX [57–61]. The positive results of DAMA and possibly CRESST and CoGeNT experiments [62–66] can ﬁnd in this scenario a nontrivial explanation due to a low energy radiative capture of OHe by intermediate mass nuclei [29,30]. It has been also shown [37, 45–47] that a two-component dark atom scenario is also possible. Along with the dominant O−− abundance, a much smaller excess of positively doubly charged techniparticles can be created. These positively charged particles are hidden in WIMP-like atoms, being bound to O−−. In the framework of WTC such positively charged techniparticles can be metastable, with a dominant decay channel to a pair of positively charged leptons. In this paper we show that even a 10−6 fraction of such positively charged techniparticles with a mass of 1 TeV or less and a lifetime of 1020 s, decaying to e+e+, µ+µ+, and τ +τ + can explain the observed excess of cosmic ray positrons, being compatible with the observed gamma ray background. One should note that as it was shown in [35, 37, 44, 45] (for a review see [29,33] and references therein) the case of -2 charged stable particles is signiﬁcantly diﬀerent from the case of stable or metastable particles with charge -1, avoiding severe constraints on charged particles from anomalous isotope searches and BBN due to their catalytic eﬀects (see e.g. [67–69]). In the essence this diﬀerence comes from the fact that primordial He formed in BBN, captures -2 charged particles in neutral OHe states, while -1 charged particles are captured by He in +1 charged ions, which either (if stable) form anomalous isotopes of hydrogen, or (if long-lived, but metastable) catalyze processes of light element production and inﬂuence their abundance. Nuclear physics of OHe is in the course of development, but a qualitative analysis has shown [46] that the OHe interactions with matter should not lead to overproduction of anomalous isotopes, while OHe catalytic eﬀects in BBN can lead to primordial heavy element production, but not to overproduction of light elements. The paper is organized as follows: In section 2 we give a brief review of dark atoms made of stable charged techniparticles. In Section 3 we present the constraints and the predictions of the scenario with respect to the parameters of the Technicolor model we use as well as how the ratio of lepton over baryon number is deduced. In section 4 we show what GUT operators can implement the decay of the doubly charged particle to leptons. In section 5, we show how the scenario of decaying dark matter can be realized, and how it can explain the PAMELA and AMS-02 results
Correlation effects in superconducting quantum dot systems<|sep|>Conventional Josephson junctions had become a standard building blocks of various electronics devices including SQUIDs [1], RSFQs [2], and qubits [3] in quantum computing. No wonder that their tunable generalizations, the superconducting quantum dots, gain a lot of attention from both theorist and experimentalist. These hybrids, where a quantum dot is placed between two superconducting leads, promise a great deal of technological advances such as quantum supercurrent transistors [4], monochromatic single-electron sources [5] or singlemolecule SQUIDs [6]. Of no less importance is the fact that they are rich and relatively easy to deal with playgrounds for studying various physical phenomena. These include the competition between Kondo eﬀect and superconductivity [7], the Andreev subgap transport [8] as well as quantum phase transitions from impurity spin-singlet to spin-doublet ground state which are in the experiments signaled by the sign reversal of the supercurrent (0-π transition) and accompanied by a crossing of the subgap Andreev bound states (ABS) [9, 10, 11]. The superconducting quantum dot can be adequately described by a single impurity Anderson model (SIAM) coupled to BCS leads [12]. Consequently, a lot of diﬀerent theoretical approaches have been applied to study this system. Many important results have been obtained using various (semi)analytical methods based on diﬀerent perturbation approaches [8, 13, 14, 15]. Moreover, as was shown in recent studies [10, 16], a surprisingly large portion of the parametric space of the superconducting SIAM can be reliably covered with a properly formulated secondorder perturbation theory (2ndPT) in the on-dot electron Coulomb interaction. Unfortunately, this method cannot describe the π-junction behavior due to the ground-state degeneracy. None of the mentioned (semi)analytical perturbative methods can cover all experimentally relevant cases. Therefore there is a big demand for “heavy” numerical methods. A very good quantitative agreement with the experiments can be obtained with the numerical renormalization group (NRG) [17, 9] and quantum Monte Carlo (QMC) [18, 12] methods. Although both methods have some disadvantages, including their computational demands, they have, besides parametric universality, one big practical advantage. Namely, the existence of well-tested versatile opensource software packages. In the present paper we focus on the continuous-time hybridization-expansion quantum Monte Carlo (CT-HYB) [19] implementation for experimentally inspired parameters [9] representing a strong coupling regime, which is beyond the reach for most (semi)analytical techniques. We show how to include the superconductivity into CT-HYB quantum Monte Carlo solver. Then we study various single-particle quantities as functions of the gate voltage. We discus how they behave near quantum phase transition and show that the CT-HYB can be reliably used to obtain the phase diagram. We also use numerical analytical continuation to obtain the spectral function. We compare all obtained CT-HYB results with either 2ndPT or the NRG.
Achieving Large Sum Rate and Good Fairness in MISO Broadcast Communication<|sep|>In the downlink of wireless communication, a multi-antenna transmitter could send information simultaneously to multiple single-antenna users. Such communication channel is commonly referred to as the multiple-input single-output (MISO) broadcast channel (BC) in the literatures [1]. It is the dual of SIMO (single-input multiple-output) multiple-access channel (MAC) [2], where multiple single-antenna users send information simultaneously to a common multi-antenna receiver. The MISO BC has appeared widely not only in traditional mobile communications, but also in the latest Internet of Things (IoT) and device-to-device (D2D) communication systems [3]. When broadcasting information to all users, the transmitter could apply the well-known dirty-paper-coding (DPC) scheme [4], [5] for encoding messages, provided that 1) a certain ordering of users has been established a priori, 2) each user has a perfect knowledge of the channel state information (CSI) of his/her incoming channels, and 3) the transmitter has a complete knowledge of the CSI of all users. The second requirement is commonly referred to as the CSIR — CSI at receiver — and can be achieved through channel estimation; the last requirement is coined as the CSIT — CSI at transmitter — in the literature and can be realized by using a feedback channel from the users to the transmitter. These requirements H. F. Lu is with the Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan (email:francis@mail.nctu.edu.tw). The research of H. F. Lu was funded in part by Taiwan Ministry of Science and Technology under Grants MOST 1062221-E-009 -024 -MY3 and MOST 107-2918-I-009-011. can be easily achieved, as the mechanisms for channel estimation and feedback already exist in modern communication systems [6]. Armed with the ordering and complete knowledge of CSI, the DPC strategy successively encodes each user’s message taking into account the noncausal knowledge of interference signals caused by preceding users. The scheme then converts the broadcast channel into a special kind of the Gelfand-Pinsker channel [7] with states non-causally known at the transmitter. The DPC strategy not only achieves the capacity of the Gelfand-Pinsker channel [4], [8], but also turns out to be optimal for MIMO BC. To elaborate, Caire and Shamai [1] investigated the capacity region of the two-user BC when the base station has arbitrary number of transmit antennas and each user has only single receive antenna. They showed through direct calculation that the DPC is optimal in terms of achieving the sum capacity of the two-user MISO BC. Weingarten et al. [9], [10] studied the capacity region of Gaussian MIMO BC based on the notion of an enhanced broadcast channel under a wide range of input constraints, including the total power and per-antenna constraints. They showed that the capacity region coincides with the DPC rate region. An alternative proof for the capacity region of degraded Gaussian MIMO BC without using the notion of enhanced channel can be found in [11]. Many iterative algorithms [12], [13] have been proposed to ﬁnd the optimal coding (beamforming) vector associated to each user in the DPC strategy, aiming to maximize the overall sum rate. These algorithms are based on the duality between the MAC and BC [14] and suffer from a relatively high computational complexity. In [1] Caire and Shamai proposed a suboptimal transmission strategy, termed zero-forcing DPC (ZFDPC), that combines both the advantages of zero-forcing beamformer and DPC for MISO BC, when the number of transmit antennas at base station exceeds or equals the number of single-antenna users. The ZFDPC eventually decomposes the MISO BC into a group of parallel interference-free channels and simpliﬁes the problem of ﬁnding optimal coding vectors, but is at a cost of certain capacity loss. Since then, the idea of combining zero forcing and DPC has been applied to many other communication problems. For instance, Dabbagh and Love [15] extended the work in [1] and proposed a successive ZFDPC encoder for the MIMO broadcast channel, i.e. when the users are equipped with multiple receive antennas. Mohammed and Larsson [16] proposed a usergroup based ZFDPC precoder by splitting users into disjoint groups. Hu and Rusek [17] considered a generalized zeroforcing beamforming that is not only orthogonal to the channel vectors of the succeeding users but also to part of those of the preceding users, thereby yielding a generalized ZFDPC strategy, where the DPC encoder only has to take into account the multiuser interference caused by a small constant number of immediately preceding users. All the above works share a common objective, namely, maximizing the sum of transmission rates of users [8], and care less whether the scheme is equally beneﬁcial to the individuals. In other words, there can be two (contradicting) objectives for designing communication schemes for BC, one from the transmitter’s viewpoint, i.e. sum rate maximization (as all the above schemes do), and the other from the viewpoint of each user, i.e. fairness maximization. Generally speaking, the former objective can be mathematically and quantitively expressed using the formula of achievable rates of the communication scheme used, say DPC, ZFDPC, ZF beamforming, etc., but the notion of fairness is unfortunately much harder to be quantiﬁed. Several conceptual, philosophical and qualitative deﬁnitions of fairness, such as proportional fairness [18], harmonic mean fairness [19], max-min fairness [20], etc. have been proposed in the literature, each holding a different opinion regarding how it means to be fair. There are also some quantitive measures for fairness in the literatures. Plausible fairness measures are generally required to satisfy axioms such as continuity, homogeneity, asymptotic saturation, and monotonicity [21]. Examples of such fairness measures are the entropy-based index [22], Jains fairness index [23], αfairness from networking research community [18], [20], and a much more complicated construction [21] that includes many existing measures as special cases. In particular, α-fairness measure can be used to justify some of the aforementioned qualitative approaches for fairness by varying the parameter α. For instance, setting α = 0 yields the aim of sum rate maximization [21]. Setting α = 1 gives the proportional fairness criterion, and the case of α = ∞ corresponds to the max-min fairness. Studies of tradeoffs between sum rate and fairness also appear in literatures. In [24], Sediq et al. investigated such tradeoff at network level based on Jain’s index and α-fair utility [20]. In particular, the transmission rates in [24] were replaced by the numbers of resource blocks allocated to each user in the downlink of wireless networks using OFDM, thereby yielding an orthogonal communication. Such communication scheme is extremely suboptimal in terms of maximal achievable rates from the viewpoint of multiterminal information theory [1], [8], [10], [11], [25], since orthogonal schemes have sum degrees of freedom always equal to one, regardless of the increase of transmit antennas and users. It then leaves a signiﬁcant room for improving the tradeoff between rates and fairness. In this paper, we will investigate the tradeoff between the sum rate and fairness for MISO BC at physical layer by employing communication schemes such as DPC or ZFDPC to guarantee the close-to-capacity performance. In addition, we will aim to provide a systematic design that can offer not only a good sum rate but also a reasonable fairness to all users at the same time. To this end, we will ﬁrst review in Section II the system model of MISO BC as well as the DPC and ZFDPC strategies. In Section II-B we will discuss several commonly used design objectives derived from the qualitative notions of fairness. Quantitive fairness measures will be discussed in Section II-C. In particular, it will be seen that the Jain’s index, though widely accepted as a fairness measure, behaves less sensitive to the ﬂuctuations of transmission rates in high SNR regime. An alternative measure based on ℓ1-norm will be proposed in Section II-C for replacement. The new measure is easily computable and satisﬁes almost all axioms listed in [21]. In Section III we will present the new design objective, termed tri-stage, which takes into account both the qualitative and quantitive aspects of fairness. As the name suggests, the proposed approach consists of three stages, where the ﬁrst two stages aim to obtain a tradeoff between sum rate and fairness as well as a byproduct which will be discussed next. Note that the tradeoff is just a function relating sum rate to fairness and says nothing about which pair of sum rate and fairness should be chosen for operation. Choosing the operating pair can be philosophically hard. For instance, it is arguable to allege that scheme A having a sum rate of 10 bits per channel use and 90 per cent fairness is better than scheme B having a sum rate of 11 bits per channel use and 80 per cent fairness, and vice versa. One might assert that schemes A and B are equally good as they are both Pareto optimal points on the sum rate-fairness tradeoff curve from the viewpoint of operational research. Yet, it would be universally agreed that another Pareto optimal, equally good scheme C having a sum rate of 12 bits per channel use and 10 per cent fairness should be totally unacceptable. Thus, we will turn to the qualitative notion of fairness to decide the operating pair in the third stage of the proposed design. Achieving the sum rate and fairness of the chosen operating point makes use of the byproduct obtained in the ﬁrst two stages: a new concept of statistical power allocation, which is in sharp contrast to the ﬁxed, deterministic method used in all existing wireless/wired communication systems. The new scheme randomly — based on an optimal probability distribution derived from the tradeoff — allocates powers to users, thereby offering not only a larger sum rate but also a better fairness than the existing designs. Several simulation results will be provided in Section IV-B to justify the excellent performance of the proposed approach. Concluding remarks are given in Section V. The following notations have been used in this paper. Underlined lowercase letter x represents a vector, and uppercase letter A denotes a matrix of certain size. A† (resp. A⊤) denotes the Hermitian transpose (resp. transpose) of matrix A, and ∥A∥p denotes its ℓp norm for some p ≥ 1. In is the (n × n) identity matrix. ⟨a, b⟩ is the usual Euclidean inner product for a, b ∈ Rn. Matrix inequalities such as ⪰, ⪯, ≻ and ≺, are the partial orderings of positive semi-deﬁnite matrices [26, Section 7.7]. We say x ∼ CN (m, K) when x is a circularly symmetric complex Gaussian random vector with mean m and covariance matrix K.
Design and Analysis of E2RC Codes<|sep|>Low-density parity-check (LDPC) codes [1] have found widespread acceptance in different areas due to their superior performance and low complexity decoding. In this paper, we investigate rate-compatible punctured LDPC codes that have the ﬂexibility of operating at different code rates while having a single encoder-decoder pair. Rate-compatible punctured codes are deﬁned by specifying a systematic mother code that operates at the lowest code rate. The parity bits of higher rate codes in a rate-compatible code family are subsets of the parity bits of lower rate codes. A number of papers have investigated issues around the design of good rate-compatible punctured LDPC codes. The work of [2] presents methods for ﬁnding optimal degree distributions for puncturing. In [3] [4] [5], algorithms for ﬁnding good puncturing patterns for a given mother code were proposed. There have also been attempts to design mother codes (along with puncturing patterns) with good performance under puncturing [6][7][8]. E2RC codes introduced in [6] are linear-time encodable and have good puncturing performance across a wide range of code rates. In this work we present systematic approaches for the design and analysis of E2RC-like codes. Let H = Manuscript received 1 Oct. 2008, revised 22 Jan. 2009. The material in this paper was presented in part at IEEE GlobeCom, New Orleans, LA, USA 2008 and will be presented in part at IEEE ICC, Dresden, Germany, 2009. Cuizhu Shi and Aditya Ramamoorthy are with the Department of Electrical and Computer Engineering, Iowa State University, USA (email: {cshi, adityar}@iastate.edu). This research was supported in part by NSF grants CNS-0721453 and ECCS-0802019. [H1|H2] denote the parity check matrix of a systematic LDPC code where H1 denotes the systematic part and H2 the parity part. We address the design of two types of codes in our work as explained below. i) Semi-structured E2RC-like codes. In these codes the parity part H2 is deterministic. We use the lower triangular form introduced in [6] and introduce a protograph structure for the H2 part. An example is shown in Fig. 1. We assume a random edge interleaver between systematic variable nodes and check nodes, which divides the code into a structured part and an unstructured part, as shown in Fig. 1. We solve the problem of ﬁnding optimal degree distributions for the unstructured part in this case for optimizing the rate-compatible codes at any speciﬁed punctured code rate(s). ii) Structured E2RC-like codes. These codes are protograph codes as introduced in [9]. The distinguishing feature is that the parity part of the protograph has an E2RC structure. We demonstrate that very good rate-compatible punctured code families can be obtained using the design rules we propose for the protograph construction. The protograph structure is especially valuable in practical applications as it allows parallelized decoding and requires signiﬁcantly less storage space for the description of the parity-check matrix than unstructured codes when circulant permutations are used. We obtain semi-structured E2RC codes that have a small gap to capacity across the range of puncturing rates. Furthermore, we present optimized quasi-cyclic protograph codes based on the E2RC structure and demonstrate that very good performance can be obtained with them. This paper is organized as follows. In Section II, we brieﬂy discuss the main contributions of our work. Section IV presents our new method for the design of semi-structured E2RC codes. We also discuss the method of predicting the puncturing performance of semi-structured E2RC codes and the joint optimization of our codes at any speciﬁed punctured code rates. We explain the construction of protograph E2RC codes in Section V, and Section VI outlines our conclusions.
Non-degenerate Bound State Solitons in Multi-component Bose-Einstein Condensates<|sep|>The multi-component coupled Bose-Einstein condensates (BECs) provide a good platform to study the dynamics of vector solitons [1]. Many diﬀerent vector solitons have been obtained in the two-component coupled BEC systems, such as the bright-bright soliton[2, 3], the bright-dark soliton [4], the dark-antidark soliton [5], the dark-dark soliton [6, 7], and the dark-bright soliton [8, 9]. The soliton states can be related with eigen-states in quantum well [10, 11]. From the general properties of eigen-states in one-dimensional quantum wells, one can know that fundamental bright soliton corresponds to ground state and dark soliton is ﬁrst-excited state in the eﬀective quantum wells. Therefore, bright-bright soliton and dark-dark soliton are degenerate solitons (more than one component admits the same spatial mode), brightdark soliton and dark-bright soliton are non-degenerate soliton states. The dark soliton state is a free state, and it admits a wide non-zero density background. This character is admitted by most of previous vector solitons in BECs systems[4–9]. We would like to look for nondegenerate bound state solitons (NDBSSs), for which all eigen-states are bound states. The bound state solitons can be used to investigate much more abundant beating or tunneling dynamics in multi-component BEC systems [12], and discuss many other diﬀerent physical problems, such as spin-orbital coupling eﬀects [13–15], quantum ﬂuctuations [16, 17], and even quantum entanglement states [18]. tive interactions, by performing Darboux transformation. Especially, we note that bright solitons with nodes correspond to the excited eigen-states in the eﬀective quantum well. The incoherent interactions between solitons in different components can be seen as the mechanism of the bound state solitons. Furthermore, we investigate the interference properties of the NDBSSs. We show that the interference between solitons with nodes exhibits multiperiods, signiﬁcantly diﬀering with scalar solitons and bright-dark solitons. Moreover, our analysis reveal that the interactions between NDBSSs are inelastic in general, induced by the incoherent interactions between solitons in diﬀerent components and the coherent interactions between solitons in same component. Double-hump and triple-hump solitons are demonstrated in two-component and three-component BECs respectively. These fascinating dynamics of non-degenerate solitons enrich the nonlinear dynamics in BECs system greatly, and the discussions on the mechanism of NDBBSs and their collision process further deepen our understanding on the vector solitons in BECs. Similar studies can be extended to more than three components cases, and more abundant bound state solitons are expected. Our presentation of the above features will be structured as follows. In Sec. II, we introduce the theoretical model and present the NDBSS solutions. We further show that incoherent interactions between solitons in different components can be used to understand how come these bound state solitons. In Sec. III, we reveal the collisions of NDBSSs are usually inelastic, due to the incoherent interactions and coherent interactions between these bound state solitons. In Sec. IV, we exhibit the NDBSS in three-component BECs systems. Finally, we summarize our results in Sec. V.
Weighted Matching in the Semi-Streaming Model<|sep|>Matching. Consider an undirected graph G = (V, E) without multi-edges or loops, where n and m are the number of the vertices and edges, respectively. Let furthermore w : E → R+ be a function that assigns a positive weight w(e) to each edge e. A matching in G is a subset of the edges such that no two edges in the matching have a vertex in common. With w(M) := � e∈M w(e) being the weight of a matching M, the maximum weighted matching problem MWM is to ﬁnd a matching in G that has maximum weight over all matchings in G. That problem is well studied and exact solutions in polynomial time are known, see [12] for an overview. The fastest algorithm is due to Gabow[4] and runs in time O(nm + n2 log n). Approximation Algorithms. When processing massive graphs even the fastest exact algorithms computing an MWM are too time-consuming. Examples where weighted matchings in massive graphs must be calculated are the reﬁnement of FEM nets [7] and multilevel partitioning of graphs [8]. To deal with such graphs there has been eﬀort to ﬁnd algorithms that in a much shorter running time compute solutions that are not necessarily optimal but have some guaranteed quality. Such algorithms are called approximation algorithms and their performance is given by an approximation ratio. A matching algorithm achieves a c-approximation ratio if for all graphs the algorithm ﬁnds a matching M such that w(M) ≥ w(M∗) c , where M ∗ is a matching of maximum weight. A 2-approximation algorithm computing a matching in time O(m) was given by Preis [11]. The best known approximation ratio approachable in linear time is (3/2 + ε) for an arbitrarily small but constant ε. This ratio is obtained by an algorithm of Drake and Hougardy[1] in time O(m· 1 Streaming Model. If we consider graphs being too big to run exact MWM algorithms on them, also an assumption of the classical RAM model is put in question: It is by no means the case that a massive graph can always be assumed as being stored completely within main memory, it is rather stored on disks or even tapes. Now seek times of read/write heads are dominating the running time. Thus for algorithms as the above ones that do not consider the peculiarities of external memory the running time totally get out of hand. To develop time-eﬃcient algorithms working on these storage devices it is reasonable to assume the input of the algorithm (which is the output of the storage devices) to be a sequential stream. While tapes produce a stream as their natural output, disks reach much higher output rates when presenting their data sequentially in the order it is stored. Streaming algorithms are developed to deal with such large amounts of data arriving as a stream. In the classical data stream model, see e.g. [5], [9], the algorithm has to process the input stream using a working memory that is small compared to the length of the input. In particular the algorithm is unable to store the whole input and therefore has to make space-eﬃcient summarizations of it according to the query to be answered. Semi-Streaming Model. To deal with graph problems in the streaming context Muthukrishnan[9] proposed the model of a semi-streaming algorithm: Random access to the input graph G is forbidden, on the contrary the algorithm gets the edges of G in arbitrary order as the input stream. The memory of the algorithm is restricted to O(n · polylog n) bits. That does not suﬃce to store all edges of G if G is suﬃciently dense, i.e., m = ω(n · polylog n). A semi-streaming algorithm may read the input stream for a number of P passes. The parameter T denotes the per-edge processing time, that is, the time the algorithm needs to handle a single edge. Despite the heavy restrictions of the model there has been progress in developing semi-streaming algorithms solving graph problems. Feigenbaum et al.[2], [3] present semistreaming algorithms for testing k-vertex and k-edge connectivity of a graph, k being a constant. They point out how to ﬁnd the connected components and a bipartition and how to calculate a minimum spanning tree of a weighted graph. Zelke[13] showed how all these problems can be solved using only a constant per-edge processing time. Matching in the Semi-Streaming Model. There are approaches to ﬁnd a weighted matching of a graph in the semi-streaming model. McGregor[6] presents an algorithm ﬁnding a (2 + ε)-approximative solution with a number of passes P > 1 depending on ε. However, for some real-world applications even a second pass over the input stream is unfeasible. If observed phenomena are not stored and must be processed immediately as they happen only a single pass over the input can occur. For the case of one-pass semi-streaming algorithms it is known, see [2], that ﬁnding the optimal solution to the MWM problem is impossible in general graphs. A ﬁrst one-pass semi-streaming algorithm approximating the MWM problem with a ratio of 6 presented in [2] was tweaked in [6] to a ratio of 5.828, which was the best known ratio until recently. Both algorithms use only a per-edge processing time of O(1). Our Contribution. In this paper we present a semi-streaming algorithm that runs in one pass over the input, has a constant per-edge processing time, and that approximates the MWM problem on general graphs with a ratio of 5.585. Therefore it surpasses the known semi-streaming algorithms computing a weighted matching in a single pass. In Section 2 we present our algorithm and its main ideas. While the proof of the approximation ratio if found in Section 3, we conclude in Section 4.
Just DIAL: DomaIn Alignment Layers for Unsupervised Domain Adaptation<|sep|>Many scientists today believe we are witnessing the golden age of computer vision. The massive adoption of machine learning and, in particular, of deep learning techniques as well as the availability of large fully annotated datasets have enabled amazing progresses in the ﬁeld. A natural question is if the novel generation of computer vision technologies is robust enough to operate in real world scenarios. One of the fundamental requirements for developing systems working in the wild is devising computational models which are immune to the domain shift problem, i.e. which are accurate when test data are drawn from a (slightly) different data distribution than training samples. Unfortunately, recent studies in the literature have shown that, even with powerful deep architectures, the domain shift problem can only be alleviated but not entirely solved [1] and several methods for deep domain adaptation have been developed. Domain adaptation focuses on learning classiﬁcation or regression models on some target data by exploiting additional knowledge derived from a related source task. In
A sharp-front moving boundary model for malignant invasion<|sep|>Figure 1: Experimental motivation and model schematic. (a) Experimental protocol where a population of motile and proliferative melanoma cells are placed onto the surface of human skin tissues kept at an air-liquid interface to simulate the in vivo environment. Scale bar is 6 mm. (b) Vertical cross section through the tissues in (a) highlighting the vertical downward invasion of melanoma cells (dark) into surrounding skin tissue (light). The sharp front separating the invading malignant population from the surrounding tissues is visually distinct and highlighted in the red rectangle. Images in (a)-(b) are reproduced from Haridas [19] with permission. (b) Schematic solution of a one-dimensional partial diﬀerential equation solution showing the spatial distribution of a population of cancer cells and skin cells separated by a sharp front. The cancer cells have density u(x, t), diﬀusivity Du and proliferation rate λu. The skin cells have density v(x, t), diﬀusivity Dv and proliferation rate λv.
Cosmological Constraints from the double source plane lens SDSSJ0946+1006<|sep|>The current concordance cosmology of ΛCDM gives a remarkably good ﬁt to current observational data (Planck Collaboration 2013; Bennett et al. 2013; Percival et al. 2010), but a tension seems to be emerging between diﬀerent cosmological probes. For example, assuming ﬂat ΛCDM the recent Planck constraints on the Hubble constant (Planck Collaboration 2013) are signiﬁcantly lower than those found by local measurements using supernovae (Freedman et al. 2012; Riess et al. 2011, but see Efstathiou 2013) and strong lens time delays (Suyu et al. 2013). Although this discrepancy may simply be due to the presence of unknown systematic errors, it might also signal physics beyond the ﬂat ΛCDM model; independent cosmological probes are therefore needed to test the assumption that the universe is spatially ﬂat and that the dark energy is a cosmological constant. Strong gravitational lensing is potentially a powerful tool to test cosmological models (Witt, Mao, & Keeton 2000; Kochanek 2002; Saha & Williams 2003; Schechter 2005; Oguri 2007; Oguri et al. 2012; Suyu et al. 2010, 2013; Gavazzi et al. 2008; Collett et al. 2012), due to its sensitivity on the distances between components of the lens system (e.g., the observer, the foreground massive lensing object, and any background lensed sources). In principle, measurements of the Einstein radius and enclosed lens mass are suﬃcient to constrain cosmological parameters (Grillo, Lombardi, & Bertin 2008; Biesiada, Pi´orkowska, & Malec 2010), but robustly inferring the lensing mass is degenerate with the choice of lens density proﬁle. To make robust inference on cosmological parameters – without making strong assumptions about the lens mass distribution – additional information is required. Gravitational lens systems with two background sources at diﬀerent redshifts (schematic shown in Figure 1) provide sufﬁcient information to make precision measurements of cosmology. In Collett et al. (2012) we showed that double source plane lenses (DSPLs) can be a useful, complementary cosmological probe, allowing the dark energy equation of state to be constrained independently of the Hubble constant. Jullo et al. (2010) constrained cosmological parameters using 12 multiply-lensed sources behind the cluster Abell 1689, but the sparsity of lensed images and the clumpy mass distribution in clusters makes the measurement difﬁcult; the systematic uncertainties are likely to be large (Zieser & Bartelmann 2012) and much of the information provided by the multiple background sources may need to be used to infer the complexity of the lensing mass distribution. Galaxy-galaxy strong lenses, on the other hand, tend to be very well ﬁtted with simple mass distributions (e.g., Vegetti et al., submitted) and therefore may be preferable objects for testing cosmology.
The Hyades Cluster: Identification of a Planetary System and Escaping White Dwarfs<|sep|>For decades it has been recognized that stars have evaporated from the Hyades cluster and that this process may account for the apparent paucity of white dwarf Hyads, especially white dwarfs with cooling ages >300 Myr (Weidemann et al 1992, Tremblay et al 2012 (hereafter TSR2012), and references therein). In a recent series of papers, TSR2012, Schilbach & Roser (2012), and Roser et al (2011) use the PPMXL Catalog (Roser et al 2010) and other data to potentially substantially increase the number of identiﬁed white dwarf members of the Hyades. Table 2 in TSR2012 presents the most recent aspects of the research; six single white dwarfs are identiﬁed as ”candidate” Hyads, plus one more has a membership status of ”uncertain”. All seven candidate Hyads are regarded as currently escaping from the cluster and all but one lie outside of the tidal radius of the cluster (which is ∼9 pc). As described in TSR2012, the missing piece of the puzzle is a measurement of the radial velocity of the seven stars. We measured the radial velocities by means of the hydrogen Balmer lines. These measurements establish that three of the seven candidates are very likely to be true (escaping) Hyades members; one other and, less likely, a second, may also be escaping members. The Hyades contains seven ”classical” single white dwarfs. With high resolution spectroscopy, we observed these and the seven candidate white dwarf Hyads mentioned above to search for evidence of accretion of rocky material from surrounding planetary systems, should they exist. The frequency of planetary systems in the Hyades bears on some major questions in astronomy. For example, based on the transit technique, planetary systems in globular clusters appear to be far less common than at disk stars (Gilliland et al 2000). Is this because of the low metallicities of globular clusters or is a paucity of planetary systems generally characteristic of rich long-lasting clusters? (The Hyades is slightly metal-rich.) In Section 4 we review earlier results on searches for planets and for dusty debris disks in orbit around stars in rich open clusters. Searches via precision radial velocities and via transits are sensitive to planets within a few AU of stars. One anticipates that these close-in regions would not be especially disturbed by encounters with other cluster members. Rather, it is the outer planetary regions – those sampled by direct infrared imaging, infrared photometry of cool dusty debris disks, and white dwarf accretion of rocky objects – that typically would be most aﬀected. Thus, these three techniques are of special interest for evaluation of the relative nature and frequency of planetary systems in long-lived clusters and in the ﬁeld. In Section 2 we present details of our observations and analysis and in Sections 3 and 4 consider escaping white dwarf Hyads and evidence for planetary systems in the Hyades. Whereas our Hyades white dwarf study utilizes optical spectroscopy, Farihi et al (2013) report an ultraviolet spectroscopic investigation of two classical white dwarf Hyads.
Flux tubes at finite temperature<|sep|>Quarks and gluons, the elementary colored degrees of freedom of strong interactions, present some of the most interesting open issues within the Standard Model of particle physics. In fact, strong interactions are described by Quantum ChromoDynamics (QCD), a local relativistic non-Abelian quantum ﬁeld theory, which is not amenable to perturbation theory in the low-energy, large-distance regimes. However, many fundamental questions are linked to the large-scale behavior of QCD. In particular, quarks and gluons appear to be conﬁned in ordinary matter, due to the mechanism of color conﬁnement which is not yet fully understood. Reaching a detailed understanding of color conﬁnement is one of the central goals of nonperturbative studies of QCD. Lattice formulation of gauge theories allows us to investigate the color conﬁnement phenomenon within a nonperturbative framework. Indeed, Monte Carlo simulations produce samples of vacuum conﬁgurations that, in principle, contain all the relevant information on the nonperturbative sector of QCD. It is known since long that, in lattice numerical simulations, tubelike structures emerge by analyzing the chromoelectric ﬁelds between static quarks [1–24]. Such tubelike structures naturally lead to a linear potential between static color charges and, consequently, to a direct numerical evidence of color conﬁnement [25, 26]. Long time ago ’t Hooft [27] and Mandelstam [28] conjectured that the vacuum of QCD could be modeled as a coherent state of color magnetic monopoles, what is now known as a dual superconductor [29, 30]. In the dual superconductor model of the QCD vacuum the condensation of color magnetic monopoles is analogous to the formation of Cooper pairs in the BCS theory of superconductivity. Remarkably, there are several numerical evidences [31–40] for the color magnetic condensation in QCD vacuum. However, it should be recognized [41] that the color magnetic monopole condensation in the conﬁnement mode of QCD could be a consequence rather than the origin of the mechanism of color conﬁnement, that could actually arise from additional dynamical causes. Notwithstanding, the dual superconductivity picture of the QCD vacuum remains at least a very useful phenomenological frame to interpret the vacuum dynamics. In previous studies [10–14, 18–22] color ﬂux tubes made up of chromoelectric ﬁeld directed along the line joining a static quark-antiquark pair have been investigated, in the cases of SU(2) and SU(3) pure gauge theories at zero temperature. The aim of the present paper is to extend the investigation of the structure of ﬂux tubes to the case of the SU(3) pure gauge theory at ﬁnite temperatures. In fact, on one hand the nonperturbative study in full QCD of the chromoelectric ﬂux tubes generated by static color sources at ﬁnite temperature is directly relevant to clarify the formation of c¯c and b¯b bound states in heavy-ion collisions at high energies. On the other hand, the study of the behavior of the ﬂux-tube parameters across the deconﬁning temperature in the SU(3) pure gauge theory allows us to check quantitatively the dual superconductor model of the QCD vacuum and to get hints about mechanisms possibly active also in full QCD. The state of the art is the following. Diﬀerently from full QCD, which exhibits a smooth crossover at about 170 MeV, the SU(3) pure gauge theory undergoes a ﬁrst order phase transition at Tc ≃ 260 MeV (see Ref. [42] and references therein), separating a low-temperature conﬁned phase with a non-vanishing string tension from the high-temperature deconﬁned phase with Debye-screened quark-antiquark potential and vanishing string tension. In the conﬁned phase it has been observed that, as the temperature approaches Tc, the string tension decreases, retaining however a non-zero values at Tc [43, 44]. The interplay among the string tension, which gives the energy per unit length in a (long enough) ﬂux tube, the color ﬁelds, whose square contributes to the energy per unit volume, and the ﬁelds’ spatial distribution, i.e. the shape of the ﬂux tube, is not yet fully understood. There are, however, a few eﬀective descriptions, whose validity domain depends crucially on the distance between the static sources, as nicely discussed in a recent paper [45]. For large enough distances (say, above 2/√ σ), the eﬀective string theory approach [46–48] should hold, according to which the shape of the ﬂux tube is determined by a ﬂuctuating thin string connecting the sources. The implications of this approach on the quark-antiquark potential and on the width of the ﬂux tube have been studied numerically in SU(N) gauge theories, both at T = 0 and at T < Tc, in many papers [49–55]. In several other recent works [23, 56–61], the detailed proﬁle of the color ﬁeld distribution near static sources has been studied, providing with information on the ﬂux tube shape which goes well beyond the one encoded in its width. The eﬀective string theory approach is expected to fail at small distances and close to Tc even at large distances. At short distances between the sources, the dual superconductivity picture should instead be valid, thus implying that color ﬁelds between a quark-antiquark pair can be described in the same fashion as isolated vortex solutions in ordinary superconductors. It would be extremely interesting to study by numerical Monte Carlo simulations the shape of ﬂux tubes in a wide enough range of distances between the static sources and for various temperatures around Tc to cover both domains where the dual superconductivity picture and the eﬀective string theory approach are expected to hold. As a ﬁrst step in this direction, in this paper we study the proﬁle of ﬂux tubes at a ﬁxed distance, about 0.76 fm, corresponding to about 1.6/√ in the range 0.8 Tc ÷ Tc, with the aim of understanding the mechanism underlying the lowering of the string tension, i.e. if it is dominated by the weakening of the color ﬁelds in the ﬂux tube or by the broadening of the ﬂux tube itself. Moreover, we extend our analysis also to temperatures in the range Tc ÷ 1.2 Tc, i.e. in the domain of color charge screening, to see how the expected vanishing of the string tension in this region reﬂects in the shape of ﬂux tubes. The part of investigation in the latter temperature domain shoud be understood just as “numerical experiment”, without any prejudice about possible results and their explanation. To implement this program, however, we need to perform numerical simulations on lattices with very large volumes. To this end, we have made use of the publicly available MILC code [62], which has been suitably modiﬁed by us in order to introduce the relevant observables. Indeed, the use of the MILC code will permit to do simulations for the physically relevant case of full QCD with dynamical quarks. The plan of the paper is as follows. In Section 2 we discuss the observables needed to extract the ﬁeld strength tensor of the static quark-antiquark sources and present some consistency checks of our code. Section 3 is devoted to the discussion of ﬁnite-temperature results. In particular we critically analyze the behavior of the coherence and penetration lengths across the deconﬁning transition. In Section 4 we discuss the structure of the ﬂux tubes in the magnetic sector at ﬁnite temperature, also in the deconﬁned phase. Finally, in Section 5, we summarize our results and present our conclusions.
Water in Comet 2/2003 K4 (LINEAR) with Spitzer<|sep|>The composition of cometary nuclei probes the physical conditions in the early solar nebula, the survival of materials from the interstellar medium (ISM), and the cold dense molecular cloud core in which the solar system formed (Wooden et al. 2004; Ehrenfreund et al. 2004; Mumma et al. 2003). Comet nuclei are highly porous agglomerates of ice and dust grains, perhaps with highly stratiﬁed, inhomogeneous layers of varied density, porosity, and composition (Harker et al. 2007; Belton et al. 2006; Or´o et al. 2006; A’Hearn et al. 2005; Prialnik et al. 2004). The nucleus composition is dominated by ices (primarily water), organic refractory materials, silicates, and carbonaceous materials. When comets are within heliocentric distances of rh ≤ 20 AU, solar insolation triggers sublimation and the release of volatile gases, sometimes sporadically, forming observable comae (Meech & Sovern 2004). In the nucleus of a comet, volatiles are frozen as ices or trapped as gases in amorphous water ice (Capria 2002; Prialnik 2002). Cometary activity occurs when gases are released through sublimation or through the exoergic crystallization of amorphous water ice. Between ∼ 20 to 5 AU, when nuclear surface temperatures reach ≃ 20 − 100 K, CO ice sublimes from the nucleus (Capria et al. 2000; Prialnik 2002), possibly from near the surface (Gunnarsson et al. 2003), and triggers activity and intermittent outbursts. Between ∼ 6 to ∼ 4 AU, a dramatic increase occurs in gas production and grain entrainment and signals the coma onset stage. At nuclear surface temperatures of ∼ 120 − 130 K (Prialnik et al. 2004), the water ice phase transition (amorphous to crystalline) releases a fraction of the trapped volatile gases. Strong erosion maintains the CO-ice sublimation and phase transition fronts relatively close to the surface (Capria et al. 2000). At ∼ 4 to ∼ 3 AU, the near-surface crystalline water ice layer, with its remaining trapped gases, begins to sublime. Water sublimation drives this vigorous activity stage that is often characterized by discrete active areas or jets. Water is the dominate ice in comet nuclei and the production rate of water is correlated with comet activity. It inﬂuences the thermal balance of the coma as a strong coolant. At some wavelengths ≤ 10 µm emission from ro-vibrational transitions of water can dominate the spectral energy distribution (Crovisier et al. 1997b); water can also be observed from its rotational transitions at submillimeter wavelengths (see review of Bockel´ee-Morvan et al. 2004). Probed through spectroscopic observations of coma species, the water production rate, coma temperature, and the nuclear spin temperatures derived from ortho-to-para ratios (OPR) are of particular interest in the study of cometary atmospheres and cometary physics. These physical characteristics, complemented by knowledge of the nucleus refractory and ice composition, provide constraints on solar nebula models (Mumma et al. 2003; Markwick & Charnley 2005), and restrict the formation zones within the protoplanetary disk where cometary nuclei could conglomerate. In particular, the nuclear spin temperature of water measured in comet comae may be indicative of the chemical formation temperature of water (Dello Russo et al. 2005; Mumma et al. 1993) therefore identifying the environment where pre-cometary ices condensed. Here we present longslit Spitzer Space Telescope spectroscopic observations of the 6 µm ν2 vibrational band of water detected in comet C/2003 K4 (LINEAR) at rh = 1.760 AU. The high signal-to-noise and the Infrared Spectrograph (IRS) longslit enable us to extract spatially resolved spectra in the coma and to measure the water production rate, Q(H2O), and the rotational temperature, Trot, and the OPR variation in the coma. Space observations of the strong ν2 fundamental bands near 6 µm present a potentially more advantageous method for constraining water production rates and Trot in comets than the more common ground-based measurement of the weaker non-resonance ﬂuorescent “hot-bands” near 2.9 µm as the complex corrections for telluric extinction, slit-loss due to seeing, and consideration of whether the local radiative pump in the coma is optically thick are minimized (Bonev et al. 2007, 2006; Dello Russo et al. 2004; Bockel´ee-Morvan 1987). In addition, accurate laboratory measurements of the absorption line strengths used to compute Einstein coeﬃcients, Aν′,ν′′(s−1), for the ν2 pump from the ground-state (000) are extant (Barber et al. 2006; Dello Russo et al. 2004; Partridge & Schwenke 1997) while those for the hot bands are more challenging, leading to some uncertainty in estimates of the spontaneous emission rates, gν′,ν′′ (s−1). The infrared ν2 band of water was ﬁrst detected with the Short Wavelength Spectrometer (SWS) of the Infrared Space Observatory (ISO) in the exceptional comet C/1995 O1 (HaleBopp) at rh ≃ 2.9 AU (Crovisier et al. 1997a). The SWS spectral resolution of ∼ 1000 resulted in the detection of several individual ro-vibrational lines. However, the low signalto-noise ratio prevented detailed analysis of their relative intensities. We discuss the Spitzer observations and data reduction techniques in § 2. Section 3.1 discusses the modeling of the ν2 water band. Section 3.2 through § 3.5 present the results, followed in § 4 by a discussion of residual emission features, including a comparison with the ν2 band of water detected in other comets with Spitzer. Section 5 presents a summary of our study of comet C/2004 K4 (LINEAR).
Deconfinement transition in two-flavour lattice QCD with dynamical overlap fermions<|sep|>The study of the QCD thermodynamics, and in particular the conﬁnement - deconﬁnement phase transition is one of the main applications of lattice QCD. Since the parameters of the deconﬁnement transition are of utmost importance for the interpretation of experimental data from heavy-ion colliders, one should reduce any systematical errors when studying them numerically. For this reason lattice QCD simulations with chirally invariant overlap fermions are now the stateof-the-art [24–26]. Unfortunately, these simulations are very expensive computationally and are almost always restricted to a single topological sector. Several important improvements in the Hybrid Monte-Carlo algorithm [2–7] have allowed for large-scale simulations with dynamical overlap fermions and without any restriction to the ﬁxed topology sector. While in general one cannot expect that the use of dynamical overlap fermions will result in a strong modiﬁcation of thermodynamic properties, they can be very useful, e.g. to study ﬂuctuations of topology at ﬁnite temperature. Another important application of the algorithms of [2–7] is the study of the deconﬁnement phase transition in an external magnetic ﬁeld, which has attracted a lot of attention recently and was intensively investigated both theoretically [8–10] and in lattice simulations [11–13]. Lattice simulations [11, 12] have revealed a strong dependence of the sign of the shift of the deconﬁnement temperature in external magnetic ﬁeld on the pion mass. In [11] it was found that at sufﬁciently large pion masses the magnetic ﬁeld increases the chiral condensate by the conventional “magnetic catalysis” mechanism [14, 15] and hence increases the deconﬁnement temperature. On the other hand, the simulations of [12] were performed at physical pion mass and revealed an unexpected decrease of the chiral condensate with magnetic ﬁeld in the vicinity of the deconﬁnement transition, which results in a decrease of deconﬁnement temperature in magnetic ﬁeld (“inverse magnetic catalysis”). At the same time, in recent theoretical works [16, 17] it was suggested that the ﬂuctuations of chirality and topology can play an important role in the inverse magnetic catalysis. Thus the use of chiral lattice fermions with unrestricted topology can be advantageous for numerical studies of the inverse magnetic catalysis. Indeed, in our recent work [18] it was found that in HMC simulations with dynamical overlap fermions [2–7] inverse magnetic catalysis is observed for pion masses as large as 500MeV. Unfortunately, up to now the exact location of the phase transition for the lattice action used in our dynamical overlap simulations [18] is not known. Since the knowledge of the critical temperature is an essential prerequisite for any further ﬁnite-temperature simulations with the algorithms of [2–7], in these Proceedings we report on our preliminary studies of the deconﬁnement phase transition for dynamical overlap fermions. We are able to identify the temperatures which certainty correspond to the conﬁnement and the deconﬁnement regimes, however, the precise location of the phase transition remains elusive with our present statistics. We consider lattice QCD with Nf = 2 ﬂavours of dynamical overlap fermions with equal masses. We use the massive overlap Dirac operator, where K = γ5 (DW −ρ) and DW is the Wilson-Dirac operator with one level of over-improved stout smearing [22, 23]. In order to ensure that lattice gauge ﬁelds are sufﬁciently smooth, we use the tadpole improved Lüscher-Weisz gauge action [20, 21]. The temperature T = 1/(Nta) is changed by varying the inverse gauge coupling β and thus the lattice spacing a. The pion mass and lattice spacing were determined using independent runs on 123 ×24 lattices for β = 7.5 and β = 8.3. We have performed measurements at β = 7.5, which corresponds to a = 0.15fm and T = 220MeV and at β = 8.3, for which a = 0.12fm and T = 280MeV as well as at the intermediate values of β = 7.6,7.7,7.8,7.9 and 8.1. For these intermediate values of β the scale setting has not been performed yet. For every value of β we have between 500 and 1000 successively generated conﬁgurations. The correlations between the conﬁgurations are taken into account using the Jackknife method.
Diffusion of active chiral particles<|sep|>The transport properties of active or self-propelled particles have received particular attention over the past several years. On the one hand, physicists, both theoreticians and experimentalists, have found a fertile ground to probe and explore ideas regarding the out-of-equilibrium conditions at which active motion occurs. On the other, there are potential applications for the designing and/or controlling the self-propulsion mechanisms which would make possible to manipulate the diﬀusive properties of such particles at will [1–7]. The out-of-equilibrium element of active systems relies undoubtedly on the single-particle mechanism that give rise to self-propulsion. Such a mechanisms, breaks the ﬂuctuation-dissipation relation [8], which otherwise characterizes the motion of passive Brownian particles by linking in a direct way, the diﬀusion properties of the particle to the temperature of the surrounding ﬂuid. In practice, the detailed microscopic dynamics of the selfpropelling mechanism occurs at a smaller time scales than the corresponding one of the observed pattern of motion. This time-scales disparity allows us to employ a reductionist approach for which the complexity of the self-propelling mechanism can be simpliﬁed. Such simpliﬁcation considers the over-damped dynamics for time evolution of the particle’s speed, so one can assume that the particle moves at constant speed over a coarse-scale of time at which the pattern of motion is described (see Ref. [9] for instance). This approximation is well supported by experimental studies in many real biological systems [10–15] where ﬂuctuations around the average value are small. In regards to the study of pattern of motion observed in active systems, two wide lines of research can be identiﬁed, on the one hand, there has been a great interest on the emergent patterns of collective motion of collections of a large number of interacting self-propelled particles. Indeed collections of self-propelled particles are ubiquitous in nature, from micro- to macro-organisms in biology [16] and more recently in man-made systems where micron-sized particles self-propel by conversion of chemical energy into mechanical one as has been demonstrated in a variety of example [17, 18]. On the other hand, the diversity of patterns of motion of single active particles, either biological or synthetic, is wide, particularly in the biological realm, where there are as many of such patterns as species of organisms in nature. Thus, no wonder why the other main line of research focuses on developing the theoretical frameworks to describe such, most of the times complex, patterns of motion exhibited by single active particles [19–25]. One aspect of interest corresponds to those swimmers, either alive or passive, that show chiral motion, i.e., a well deﬁned state (clockwise or anticlockwise) of the circular motion component of the particle trajectories. As a matter of fact, a plethora of biological organisms [26–39] and synthetic particles as well [40–45] exhibit chiral motion exhibited as helical motion in three dimensions and circular in two. The processes that lead to chiral motion of active articles may be diverse [46–49], the simplest situation in two dimensions corresponds to a geometric eﬀect, that is to say, to the misaligning of the direction of the propelling force and the orientation of the particle axis [50, 51]. A simple eﬀective-force model, that leads to circular patterns of motion, is the inclusion of an eﬀective constant “torque” in the Langevin equations that drive the orientation of the self-propulsive force [52]. Such constant torque exerts the particle to rotate with constant angular velocity [37, 38, 50, 53], leading to circular trajectories in two dimensions and to helical ones in three dimensions. Such torque, for instance, may be externally caused by a magnetic ﬁeld that act over the magnetic moment of magnetic bacteria or used over nanorods to steer them [54]. This theoretical framework is now standard and has been used in a variety of studies as in the study of the diﬀusion properties of active particles moving in two dimensions [55, 56], of motors having a component of circular motion [45], and of the eﬀects of conﬁnement in the diﬀusion properties of chiral-active particles where directed motion has been observed [57, 58]. Another approach that has been used to study two-dimensional chiral motion is the rotationally persistent random walks, where the introduction of a clockwise or counter-clockwise angular bias at each new step the walker takes [59]. A connection, if any, among all these analytical approaches is still missing in the literature and deserves a future analysis. Analytical studies of diﬀusion of active particles in three dimensions has been received more less attention than its two-dimensional counterpart. In Ref [60], for instance, the diﬀusion of torqued, active particles in threedimensional space is analyzed through overdampedLangevin equations, which are solved for the time dependence of the ﬁrst two moments of the particle positions, namely, the average position and the mean square displacement from which the eﬀective diﬀusion coeﬃcient is computed. In Ref. [61] the diﬀusion properties of swimmers that move in three dimensions with ﬁxed, mean curvature and torsion, are studied by the use of stochastic Frenet-Serret equations which generalizes the deterministic description of helical motion given in [28]. A more general instance is studied in Ref. [62] where a selfpropelled Brownian spinning top is considered through the analysis of overdamped-Langevin equations. A complete description in three dimensions in terms of the Smoluchowski-like equations is challenging and deserves a thorough analysis even in the absence of chirality. This approach leads us directly to the time evolution of the probability distribution of the particle positions, and from it, to relevant information regarding the characteristic features of the pattern of motion as its non-Gaussian nature [23, 25, 63]. In this work we study the diﬀusion of active Brownian particles that move freely with constant speed in inﬁnite three-dimensional space subject to an eﬀective torque. We derive Smoluchowski-like equations that take into account the persistence eﬀects of active Brownian motion and of chirality as well. The equations are derived from the Fokker-Planck equation for the total probability density of ﬁnding a particle at position x moving in the direction ˆv at time t, P(x, ˆv, t) by coarse-graining over the direction of motion. From P (x, t) analytical expression for the mean square displacement and the kurtosis are given. A comparison of our prescription formulas with numerical simulations was carried out by solving the corresponding Langevin-like equations of active particles subject to torques. Our analysis reveals oscillations on time-dependence of the kurtosis in the ballistic regime and for large values of the torque strength. These oscillations points the helical pattern of motion. We also compute the stationary value of the kurtosis for an ensemble of active articles, each particle moving under the eﬀects of an instance of an eﬀective torque uniformly distributed on the sphere. Interestingly this situation exhibit the “anomalous, yet Brownian, diﬀusion” eﬀect, also known as weakly anomalous diﬀusion, where the probability distribution is not Gaussian but the diﬀusion is normal with a mean squared displacement that grows linearly with time. This paper continues as follows: In section II we present the Langevin equations for the trajectories of particles that move with constant velocity and their corresponding Fokker-Planck equation for the probability density P(x, ϕ, t) of a particle being at point x, moving in the direction ϕ at time t is stated in III. In sect the method of analysis is presented IV. Results are discussed in V. We ﬁnally give our conclusion and ﬁnal remarks in section VI.
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?<|sep|>There have been a lot recent interest in the scaling properties of Transformer models (Kaplan et al., 2020; Hernandez et al., 2021; Bahri et al., 2021; Henighan et al., 2020; Tay et al., 2021b; Abnar et al., 2021). However, not much is understood about the scaling properties of different inductive biases imposed by model architectures. Improvements at a a speciﬁc scale (compute, size etc) are often assumed to transfer to different scales and compute regions (So et al., 2019; Choromanski et al., 2020; Lan et al., 2019; Dehghani et al., 2018) and new research is often presented in a point-wise fashion with respect to scale. In short, it is not uncommon for new methods to be presented with data points at very speciﬁc or limited compute regions (e.g., base size). We believe that understanding the interaction between architecture and scaling laws is crucial as designing models that perform well at diverse scales will likely have signiﬁcant impact. This paper is an attempt to understand the effect of inductive bias (architecture) on scaling laws of language models. To this end, we pre-train and ﬁnetune over ten diverse model architectures across multiple compute region and scales (e.g., from 15M to 40 Billion parameters). In total, we pre-train and ﬁnetune over 100 different models of different architectures and sizes and present insights and challenges at scaling these ten diverse architectures. We consider a broad spectrum of models in our extensive experiments. Concretely, we consider several well-established Transformer variants (Vaswani et al., 2017) such as Evolved Transformer (So et al., 2019), Universal Transformers (Dehghani et al., 2018) and Switch Transformers (Fedus et al., 2021). We also consider lightweight models such as ALBERT (Lan et al., 2019) and/or efﬁcient Transformers (Tay et al., 2020) such as Performer (Choromanski et al., 2020) and Funnel Transformers (Dai et al., 2020). In our comparison, we are also interested in ﬁnding out if general improvements to the Transformer architectures such as Mixture-of-Softmax (Yang et al., 2017) and/or Gated Linear Units (Dauphin et al., 2017; Shazeer, 2020) inﬂuence the scaling behaviour of models. Finally, we also evaluate models outside the family of Transformers including Lightweight convolutions (Wu et al., 2019), Dynamic convolutions (Wu et al., 2019) and the recently proposed MLPMixers (Tolstikhin et al., 2021). Figure 1 illustrates an overview about the experiments we run. We also note that scaling these models is not as straightforward as it seems, i.e., there are intricate details of scale that are intertwined with architectural choices which we study in detail in this paper. For example, a distinct feature of Universal Transformers (and ALBERT) is parameter sharing. Figure 1: An overview compute-performance (FLOPs vs performance) plot of all the diverse models and architectures we pretrained and ﬁnetuned in this study. Colors represent different model architectures and size of the circles represent the size of the model (parameters). Hence, compared with standard Transformers, this architectural choice signiﬁcantly warps the scaling behaviour not only with respect to performance but also amongst compute metrics such as FLOPs, speed and number of parameters (Dehghani et al., 2021a). Conversely, models such as Switch Transformers are on the other end of the spectrum with an uncommon relationship between FLOPs and number of parameters, i.e., they have high parameter to FLOPs ratio. This difﬁculty makes navigating this landscape challenging. • For the ﬁrst time, we derive scaling laws for different inductive biases and model architectures. We ﬁnd that this scaling coefﬁcient differs greatly from model to model. We believe this is an important consideration in model development. It turns out that amongst all ten architectures that we consider, the vanilla Transformer has the best scaling behaviour, even if its absolute performance at each compute region is not the greatest. • We observe that models that operate well in one compute-scale region is not necessarily the best in another compute-region. Moreover, we ﬁnd that certain models have difﬁculty scaling despite performing decently (comparably) at lower-compute regions. This has implications, since it is difﬁcult to get the fulll picture of a model’s scalability with pointwise comparisons at a certain compute-region. • We ﬁnd that when it comes to scaling different model architectures, upstream pre-training perplexity might not correlate well with downstream transfer. Hence, the underlying architecture and inductive bias is also crucial for downstream transfer. • We highlight the difﬁculties of scaling with certain architectures and show that some models do not scale (or scale with a negative trend). We also ﬁnd concerning trends where lineartime attention models such as Performer struggle with scaling up.
Federated Learning on Adaptively Weighted Nodes by Bilevel Optimization<|sep|>Federated learning (FL) is an emerging technique for training a model using data distributed over a network of nodes without sharing data between nodes [23, 33]. In this paper, we focus on the case where data distributions across nodes are heterogeneous and each node aims at a model with an optimal local generalization performance. In the classical setting of FL, a globally shared model is learned by minimizing a weighted average loss across all nodes. However, given the heterogeneity of data distributions, a global model is likely to be sub-optimal for some node [10]. Alternatively, each node can train a model only using its local data, but such a local model may not generalize well neither when the volume of local data is small. To achieve a good local generalization performance, each node can still exploit global training data through FL but, at the same time, identify and collaborate only with the nodes whose data distributions are similar or identical to its local distribution. One way to implement this strategy is to allow each node to solve its own weighted average loss minimization problem with weights designed based on the performance on a separate set of local (validation) data. Ideally, each node can learn a better model by allocating more weights on its peers whose data distribution is similar to its local distribution. In this paper, we formulate the choice of the weights as a bilevel optimization (BO) problem [6, 42], which can be solved by a federated bilevel optimization algorithm, and analyze the generalization performances of the resulting model. We consider a standard learning problem where the goal is to learn a vector of model parameters θ from a set Θ that minimizes a generalization loss. This problem can be formulated as where l(θ; z) is the loss of θ on a data point z from a space Z, and Ez∼p0 represents the expectation taken over z when z follows an unknown ground truth distribution p0. Directly solving (P) is challenging as p0 is unknown, and, typically, training data sampled from p0 is needed for learning an approximation of θ∗. In this paper, we consider the scenario where the amount of data sampled directly from p0 may not be sufﬁcient to learn a good approximation of θ∗, but there exist external data distributed on K nodes that can potentially help the learning on θ∗. In particular, we denote the set of nodes by K := {1, . . . , K} and assume a training set Dtrain k is stored in node k. We also deﬁne Dtrain := � Dtrain k �K k=1 and assume |Dtrain k | = nk and Dtrain k = {z(i) k }nk i=1, We assume node k is weighted by wk and the vector of weights w = (w1, . . . , wK) ∈ [0, 1]K is located on the capped simplex ∆b K deﬁned as ∆b K = � w = (w1, . . . , wK) ��� �K k=1 wk = 1, 0 ≤ wk ≤ b, k ∈ K � , When some pk’s are different from p0, w in (1) must be chosen adaptively to ensure �θ(w) is a good approximation of θ∗ in (P). To do so, we assume that there is a validation dataset Dvalid with |Dvalid| = n0 = nvalid and Dvalid = � z(i)�n0 i=1, where z(i) ∈ Z is an i.i.d. sample from p0. We assume Dvalid is stored in a node called node 0 or center, which may or may not be a node in K. Set Dvalid alone may not be sufﬁcient for learning θ∗ precisely but can be used to assist the selection of w. We then propose to estimate the generalization loss of �θ(w) using the loss on Dvalid, i.e., and use this validation loss to guide the procedure for updating w. Presumably, when both the training and validation sets are large enough, the weights in w will be shifted towards the nodes where the data is helpful for learning θ∗. Following this idea, we formulate the federated learning problem on adaptively weighted nodes as the following bilevel optimization (BO) problem: In Section 4, we will present a federated optimization algorithm for solving (�P). Suppose an algorithm can ﬁnd the optimal solution �w of (�P) and the corresponding model parameter �θ(w). We are interested in the optimality gap of the generalization loss of �θ( �w), namely, L0(�θ( �w)) − L0(θ∗), where L0 is deﬁned as in (P). The main contribution of this paper is to establish a high-probability bound of this gap as a function of the sizes of Dtrain and Dvalid as well as a statistical distance between p0 and pk’s. Moreover, we compare our generalization bound with the bound achieved by learning only locally from Dvalid and the bound achieved by solving (1) with evenly distributed weights, and identify the parameter regimes where our method is preferred in theory.
Unified Picture of Electromagnetic and Gravitational Forces in Two-Spinor Language<|sep|>In our collective consciousness, we are all aware of the essential role played by the mathematical framework in the formulation of physical laws. In its original form, Maxwell Equations were not easy to understand and even less of unveiling all its content. The adoption of three dimensional vector formalism revealed the close interrelation between electric and magnetic forces. Later, with the advent of Special and General Relativity (GR), the tensor language lead to a unifying picture in terms of the electromagnetic tensor ﬁeld F αβ and the underlying Lorentz Invariance of physical laws in ﬂat spacetime as well as general covariance in GR. One alternative to tensor calculus is the so called Weyl two-spinor formalism [1]. Simply stated, Two-Spinors are elements of a two-dimensional complex vector space. As four-vectors are acted upon by elements of the homogeneous Lorentz Group, two-spinors also change their components when they are acted by elements of the SL(2, C) group (Covering group of the homogeneous Lorentz group). Any tensor equation can be transformed into a spinor equation by certain rules that will be explained in Section 2. However, regarding spinor equations, the opposite statement need not be necessarily true. Given that there are so many tensor equations, to begin with, our attention will address the famous Dirac Equation (although originally not in two-spinor form). The Weyl two-spinor version of the Dirac Equation was obtained in a previous study [8] fully coincident with Penrose’s result in [1]: Since the rules of spinor algebra are not very familiar among physicists it seems, at least in principle, questionable whether there are any serious reasons for adopting the aforementioned formalism. Notwithstanding, with the devenir of time the concise and elegant Weyl two-spinor language gained adepts and interest among some physicists and mathematicians specially Roger Penrose who saw in two-spinors a seed for a new insight in physics. Indeed, we share Penrose view ”that we have still not yet seen the full significance of spinors (particularly the 2-components ones) in the basis structure of physical laws” [6]. The approach in this paper to the old issue of the uniﬁcation of gravitational and electromagnetic interactions is substantially diﬀerent from what (to my knowledge) have been attempted until now. To begin with, the fundamental variables will not be four-vectors, like momentum or 4-velocity. Instead, in the new picture, they are complex two valued spinors obeying classical spinor equations in which spin 1/2 appears in a natural way via a two dimensional representation of the SL(2, C) group. The replacement of the classical variables in tensor form by two-spinors is a crucial step since it provides a uniﬁed language for both classical dynamics and relativistic quantum mechanics. Perhaps, the reader will note that no mention, or explicit appearance, is made of such important quantities as the Riemann Spinor, Weyl Conformal spinor, and spinor version of Einstein Equations which are already standard material in the literature ([7]) and even in textbooks [2]. The Spinor Equations presented in next section and obtained some time ago ([12]) are just the
Topological Insulators and Superconductors from String Theory<|sep|>The integer quantum Hall eﬀect (IQHE) is one of the most striking phenomenon in the d = 2-dimensional electron system under a strong magnetic ﬁeld, and has been one of the central topics in condensed matter physics. As well-known, the Hall conductance is quantized when the electronic ground state has a non-trivial topological structure. With the recent discovery of the quantum spin Hall effect (QSHE) in d = 2 and the Z2 topological insulator in d = 3 [1–14], it has become clear that topological phases can exist in a much wider context, i.e., in the spatial dimension other then d = 2, and without strong timereversal symmetry (TRS) breaking by a magnetic ﬁeld. The QSHE and d = 3-dimensional Z2 topological insulator can be thought of as a close cousin of the IQHE, but diﬀerent from the IQHE in many essential ways: these states can exist only when time-reversal symmetry is respected, and can be either two- or three-dimensional. Furthermore, Z2 topological insulators are characterized by a binary topological number, unlike the integral Hall conductivity in the IQHE. Recent experiments conﬁrmed HgTe quantum well and Bismuth-related materials, both of which have strong spin-orbit coupling, indeed realize such non-trivial Z2 topological phases. The Bloch wavefunctions in the IQHE or the Z2 topological insulator in d = 2 (the QSHE) and in d = 3 have non-trivial topology, detected by an integer or a binary topological number. For superconductors (superﬂuid), at least within the BCS meanﬁeld theory, it is possible the wavefunctions of fermionic Bogoliubov quasiparticles carry non-trivial topological characters, very much the same way as the electronic wavefunctions in the IQHE or the Z2 topological insulator, in particular when fully gapped (i.e., a quasiparticle gap opens for entire momentum space); such superconductors (superﬂuid) can be called topological superconductor (superﬂuid). A wellknown example of such topological superconductor is the d = 2-dimensional chiral p-wave superconductor which has a px + ipy-wave superconducting order parameter. For non-interacting fermions, an exhaustive classiﬁcation of topological insulators (TIs) and superconductors (TSCs) is proposed in Refs. [15, 16]: TIs/TSCs are classiﬁed in terms of spatial dimensions d and the 10 = 2 + 8 symmetry classes (two “complex” and eight “real” classes), each characterized by presence/absence of discrete symmetries such as time-reversal symmetry (TRS or T), particle-hole symmetry (PHS or C), and chiral (or sublattice) symmetry (SLS or S). (Table I). In relativistic ﬁeld theories, the ﬁrst two are the same as the usual T and C symmetries. For any system which has both T and C symmetries, S symmetry (SLS) is realized as a product C·T, while it can exist on its own, even without T and C symmetries. The ten symmetry classes are in one-to-one correspondence to the Riemannian symmetric spaces without exceptional series (described in Table II) [18–21], and to K-theory classifying spaces [16, 17]. For example, the integer QHE, QSHE, and Z2 TI are a topologically non-trivial state belonging to class A (d = 2), AII (d = 2), and AII (d = 3), respectively. The classiﬁcation reveals a periodicity both in spatial dimensions d and in symmetry classes, and hence is often called “periodic table” of TIs/TSCs. Not only it incorporate many of previously known topological phases, it also predicted new topological phases; e.g., the B phase of 3He was newly identiﬁed as a topological phase; the existence of the d = 3-dimensional topological singlet superconductor was predicted, and veriﬁed by an explicit construction of a lattice model of the BCS superconductor [22]. The complete classiﬁcation of non-interacting TIs and TSCs opens up a number of further questions, most interesting among which are interaction eﬀects[23]: Do non-interacting topological phases continue to exist in the presence of interactions? Can interactions give rise to novel topological phases other than non-interacting TIs/TSCs? What is a topological ﬁeld theory underlying TIs/TSCs, which can potentially describe TIs/TSCs beyond non-interacting examples?, etc. On the other hand, the ten-fold classiﬁcation of TIs/TSCs reminds us of D-branes, which are fundamental objects in string theory, and are also classiﬁed by K-theory [24, 25] (Table III) via the open string tachyon condensation [26, 27]. It is then natural to speculate a possible connection between TIs/TSCs and of D-branes [28]. In this paper, we propose a systematic construction of TIs/TSCs in terms of various systems composed of two kinds of D-branes (Dp- and Dq-branes), possibly with an orientifold plane (O-plane). Besides the appealing mathematical similarity between TIs/TSCs and D-branes, realizing TIs/TSCs in string theory has a number of merits, since string theory and D-branes are believed to be rich enough to reproduce many types of ﬁeld theories and interactions in a fully consistent and UV complete way. Indeed, our string theory realizations of TIs/TSCs give rise to massive fermion spectra, which are in one-to-one correspondence with the ten-fold classiﬁcation of TIs/TSCs, and come quite naturally with gauge interactions. These systems, while interacting, are all topologically stable, as protected by the K-theory charge of D-branes. We thus make a ﬁrst step toward understanding interacting TIs/TSCs. Our approach also reveals the connection between the number of symmetry classes of TIs/TSCs and the critical dimension of superstring theory (=10), via the Bott periodicity of K-theory. In Dp-Dq-systems, massive fermions arise as an open string excitation between the two D-branes. The distance between the branes corresponds to the mass of fermions. Open strings ending on the same D-branes give rise to a gauge ﬁeld, which we call Aµ (Dp) and ˜Aµ (Dq) with gauge group G and ˜G, respectively, and couple to the fermions (they are identiﬁed as in Table IV). These two gauge ﬁelds play diﬀerent roles in our construction: The gauge ﬁeld Aµ “measures” the K-theory charge of the Dqbrane, and in that sense it can be interpreted as an “external” gauge ﬁeld. In this picture, the Dq-brane charge is identiﬁed with the topological (K-theory) charge of TIs/TSCs. On the other hand, ˜Aµ is an internal degree of freedom on the Dq-brane. The massive fermions can be integrated out, yielding the description of the topological phase in terms of the gauge ﬁelds. The resulting eﬀective ﬁeld theory comes with terms of topological nature, such as the ChernSimons (CS) or the θ-terms. In our string theory setup, they can be read oﬀ from the Wess-Zumino (WZ) action of the D-branes, by taking one of the D-branes as a background for the other. In our construction of the QSHE (i.e. AII d=2), the brane system consists of a Dp-brane and a Dq Dq denotes an anti Dq-brane, which has the opposite Ramond-Ramond charge to the Dq-brane. The Dp-brane has an SU(2) gauge ﬁeld and the Dq Dq system has a U(1) gauge ﬁeld. The former couples to the SU(2) spin rotation, while the latter is the usual electric-magnetic ﬁeld. The integration of massive fermions between the Dp-brane and Dq Dq system produces the double Chern-Simons coupling, which agrees with the previously proposed description of quantum spin Hall eﬀect. One can view these gauge-interacting TIs/TSCs from Dp-Dq-systems as an analogue of the projective (parton) construction of the (fractional) QHE [29]. Our string theory realization of TIs/TSCs sheds light on extending the projective construction of the QHE to more generic TIs/TSCs; it tells us what type of gauge ﬁeld is “natural” to couple with fermions in topological phases, and guarantees the topological stability of the system. This paper is organized as follows. In Sec. II, we will present our D-brane construction of TIs/TSCs for two “complex” symmetry classes A and AIII, which correspond to the complex K-group. Expecting readers from both high-energy and condensed matter communities, the underlying principles of the construction are summarized in Sec. II A, together with a brief but pedagogical review of D-branes and open strings. In Sec. III, we will give D-brane constructions for remaining eight classes which are classiﬁed by the real K-group. This will be followed by a detailed comparison between the Dbrane system and topological phases in condensed matter physics in Sec. III F. In section IV, we will draw conclusions and discuss future problems. In Appendix A, we gave a brief explanation on the open string spectrum in the presence of D-branes and orientifolds. This paper is an extend version of our previous brief report [30].
Dissipation and spontaneous emission in quantum electrodynamical density functional theory based on optimized effective potential: A proof of concept study<|sep|>Progress in the ﬁelds of cavity and circuit QED, and especially recent developments in polaritonic chemistry, also referred to as chemistry in cavity or QED-chemistry [1–11] requires the development of theoretical methods for describing realistic many-electron systems strongly coupled to photons. The application of these methods ranges from the exploration of cavity-assisted phase transitions in many-electron systems [12, 13] to cavity engineering of the potential landscapes to taylor the photocatalysis [14]. Such methods are expected to combine the accuracy of modern electronic structure theory with the ability to treat light fully quantum mechanically capturing the eﬀects of strong light-matter interaction, typical for quantum optics [15–17]. Density functional theory (DFT) [18] and its timedependent counterpart (TDDFT) [19–21] are the common methods of choice for modelling realistic materials because of their good balance between the accuracy and computational eﬃciency. It is therefore highly desirable to extend the DFT framework by including quantized electromagnetic degrees of freedom. Such QED generalization of the DFT concept, known as QED-TDDFT or QEDFT, has been indeed proposed few years ago [22, 23]. This theory being a reformulation of the many-body electron-photon problem treats photons on equal footing with electrons, and gives a formally exact access to the electron density and the electromagnetic ﬁeld strength in the cavity. Diﬀerent aspects of QEDFT have been studied in the last years [24–30]. However, many general properties of this promising formalism remain poorly understood, while applications of QEDFT are still limited to the simplest level of mean ﬁeld approximation. Probably one of the most interesting features of QEDFT is that its structure allows for a natural inclusion of dissipative eﬀects. As in many practically important situations a quantum system can not be considered perfectly isolated, the generalization of the TDDFT for modeling dissipative dynamics has always been a challenge. In the last two decades there were several proposals for including dissipation into TDDFT, based on master equation for density matrix [31–33], or starting from many-body stochastic Schr¨odinger equation [34, 35]. It is worth noting that for a closed macroscopic system, dissipative eﬀects related to internal excitation of the electron gas can be captured within the viscoelastic formulation of Vignale-Kohn current density functional [36, 37]. QEDFT is perfectly suited for quantum dissipative systems because it is formulated for electrons interacting with an arbitrary set of cavity modes. Without any modiﬁcation of the formalism, the set of photon modes can be taken continuous with some spectral density and we get (TD)DFT for a system of electrons coupled to a quantum dissipative environment [22]. Depending on a speciﬁc form of the spectral density, QEDFT may describe diﬀerent physical systems ranging from molecules or nanostructures in realistic lossy cavities to manyelectron systems coupled to the Caldeira-Leggett Ohmic bath [38, 39]. Despite a close relation of QEDFT to quantum dissipative systems was recognized essentially from its advent, this important aspect of the formalism remained practically unstudied till now. Very recently extensions of QEDFT to dissipative cavities with applications to the theory of the natural linewidth have been discussed [30, 40], but only within the mean-ﬁeld ap proximation for the electron-photon interaction. Similar to any TDDFT, in QED-TDDFT (QEDFT) dynamics of the electron density is mapped to the dynamics of ﬁctitious nonintercating Kohn-Sham particles moving in the presence of an eﬀective self-consistent potential which contains a mean-ﬁeld (Hartree) and an exchange correlations (xc) contributions. The former corresponds to the classical coherent radiation [22, 24] that describes the radiation reaction self-force [40], whereas the latter is responsible for all remaining purely quantum eﬀects. In some situation, e. g. in the linear response regime, the classical radiation reaction and the corresponding losses on coherent radiation can indeed dominate. However, by neglecting the xc potential in the mean-ﬁeld approximation we completely ignore the quantum nature of the cavity ﬁled and totally miss crucially important physical eﬀects, such as spontaneous emission. For example, if in the course of dynamics the electronic subsystem preserves the inversion symmetry such that its center of mass is not moving, the coherent dipole radiation is absent and at the mean-ﬁeld level the dynamics will be undamped, which is clearly unphysical. In reality the dissipation, that is, the energy transfer from the electrons to the cavity photons occurs via the spontaneous emission of incoherent radiation with zero expectation value of the ﬁeld strength. In the QEDFT framework the physical behaviour should be restored by the quantum xc eﬀects encoded in the xc potential. Apparently the potential doing this important job should be quite nontrivial, and it is absolutely unclear whether the existing approximations can do it, at least to some extent. This is the main question we address in this paper. Speciﬁcally, we study the performance of the QED optimized eﬀective potential (QED-OEP) approximation [24] for lossy cavities and its ability to describe dissipation via spontaneous emission of incoherent radiation. To clearly disentangle the incoherent quantum radiation from the classical recoil eﬀect we analyze the dynamical regimes where the classical radiation is absent and all dissipation is of purely quantum origin. Aiming at the proof of concept, we do this for a minimal 3-site tight-binding model in which a nontrivial density dynamics in the absence of the classical radiation reaction can be realized. By explicit numerical calculations we demonstrate that QED-OEP is able to capture the quantum dissipation both qualitatively, and to a very high accuracy quantitatively, at least in the regimes dominated by one-photon processes. The paper is orgnized as follows: In section II we provide the general description of the formalism used. In section III we apply the formalism to the minimal lattice model, and discuss the properties of the exact solution as well as the Optimized Eﬀective Potential (OEP) approximation. Section IV summarizes the main results of the numerical simulation, and Section V provides the conclusions and outlook.
Non-Abelian two-form gauge transformation and gauge theories in six dimensions<|sep|>The existence of the (2, 0) superconformal theory is one of the most incredible predictions of string theory which was based on string dualities [1]. From M-theory point of view, this theory describes the low energy limit of a stack of the M5-branes. The dynamics of a single ﬁve-brane in eleven- dimensional supergravity was explored in [2], [3]. The (2, 0) superconformal theory has some interesting physical and mathematical consequences. Based on AdS/CFT correspondence, M-theory on AdS7 × S4 is dual to this theory and many interesting four-dimensional superconformal ﬁeld theories can be achieved by compactiﬁcation of this theory on a Riemann surface [4]. On the other hand, it is believed that this theory has some interesting relations to geometric Langlands program [5], [6]. There is no perturbative parameter in M-theory and correspondingly 6D-superconformal theories can not be described perturbatively. In spite of many attempts to describe this theory in Lagrangian formalism, there is no known description based on action functional. There are some reasons for this diﬃculty which the most important of them is the existence of a self-dual two-form in the (2, 0) tensor multiplet. Because of the self-duality, it is diﬃcult to write a covariant action for this two-form. Furthermore, ﬁnding a generalized non-Abelian theory of the two-forms is another problem. From the mathematical point of view, a non-Abelian two-form can be described by non-Abelian gerbes [7], [8]. In this framework in addition to the two-form, we also need to introduce a new gauge ﬁeld A. The tensor multiplet in six dimensions does not contain a vector gauge ﬁeld and if this pattern plays a role in 6d-superconformal ﬁeld theories, it is not clear how do we can manage the role of this gauge ﬁeld which introduce extra bosonic degrees of freedom to the theory. It might be possible as like as three dimensional superconformal ﬁeld theories to introduce a topological ﬁeld theory for the dynamics of the gauge ﬁeld A to resolve this obstacle [9], but as far as we know, there is no topological ﬁeld theory for a gauge ﬁeld in six dimensions. The aim of this paper is to ﬁnd solutions for some of these problems. We consider a simpler theory that only has a Dirac spinor and a two-form and we focus on the non-Abelian generalization part of the problem. The organization of the paper is as follow: In section II, we review some basic ingredients of QFT. In section III, we will introduce a new gauge transformation to the two-form which impose a non-local structure to the theory; and we discuss the properties of the Wilson surface operator for the non-Abelian two-form. In section IV, we deﬁne a covariant derivative with respect to the gauge transformations which was mentioned in section III and we will derive a new Abelian gauge theory in six dimensions which we expect to have some similar structure to the ﬁnal (2, 0) superconformal ﬁeld theory in six dimensions.
Zero-bias anomaly in nano-scale hole-doped Mott insulators on a triangular silicon surface<|sep|>phenomena that are at the center of condensed matter physics research. For example,  while the foundations of unconventional superconductivity continue to be debated  [1], electron correlations appear to be playing a critical role. In high-temperature  superconducting cuprates with a square lattice, the superconducting order parameter subdominant 𝑠 or 𝑑𝑥𝑦  wave symmetry component that would make the Fermi  surface fully gapped [2, 3]. On the other hand, a honeycomb or triangular lattice could  stabilize superconductivity with an order parameter having a spin-singlet chiral superconducting order parameter may give rise to Majorana modes with potential  applications in quantum computing [6]. It is therefore of great interest to investigate  correlated  triangular  or  honeycomb lattice  systems  for  possible  chiral the Si(111) or Ge(111) surface exhibit a (33)R30° surface reconstruction with a  triangular lattice symmetry, as shown in Fig. 1. Such triangular lattices of half-filled  dangling bond orbitals form a conceptually simple platform to explore twodimensional correlated electron physics [10-16]. The -phase of Sn on Si(111)  (henceforth 3-Sn) has drawn special attention since it is a Mott insulator3Sdue to its  relatively strong on-site Coulomb repulsion,  6.0  U eV, that is of the order of the electronic bandwidth W [17-20]. Our recent work has shown that the 3-Sn phase can  be modulation-doped with holes when a p-type Si(111) substrate is used, reaching a  maximum doping level of up to ~ 10 % [21,22]. This hole-doped phase becomes  metallic with a dispersing quasiparticle band that crosses the Fermi level. STS and  quasiparticle interference experiments reveal that the hole-doped system has a Van  Hove singularity (vHs) at 7 mV below the Fermi level and a nested constant energy  contour only 10 mV above the Fermi level, suggesting that the system could be on the  brink of a Fermi surface instability. Theoretical work predicts that this hole-doped  system could become a d-wave superconductor [20] and a recent dynamical mean  field theory calculation predicted a chiral 𝑑 +  i𝑑 wave superconducting phase at >  20 % hole doping [23].     In this paper, we present an extensive scanning tunneling spectroscopy (STS) study on the hole-doped 3-Sn phase at low temperatures. On the most heavily doped  surface, the 3-Sn phase forms isolated nano-domains surrounded by a  semiconducting Si(111)(2323)R30° surface reconstruction [22]. Our data indicate  that the doping level of 3-Sn phase is higher for the smaller domains and near the  edges of the 3-Sn domains; specifically, the doping level increases from ~ 9 % in  large domains to ~ 12 % in the (much) smaller domains. Moreover, STS spectra  recorded in relatively small domains show a prominent suppression of the zero bias conductance (a zero bias anomaly, ZBA for short) that becomes larger as the domain  size decreases. These spectra can be fitted reasonably well with a model based on  chiral 𝑑 +  i𝑑 wave superconductivity, until the size of the domain becomes too  small. We also consider a dynamical Coulomb blockade (DCB) effect, specifically to  model the size dependence of the ZBA [24-30]. The DCB model appears to better  explain the observed spectral features, especially for the smaller nano-domains that  exhibit the strongest ZBA, as well as the temperature dependence of the ZBA. Based  on an analysis of an extensive data set covering variations in doping, domain size, and  temperature, we conclude that the ZBA is predominantly a DCB effect. However, a  scenario including superconductivity cannot be ruled out for the weaker ZBAs found  on larger domains. Further experiments, possibly with higher hole-doping levels, are  needed to verify the potential existence of superconductivity in these systems.     This paper is organized as follows. Sec. II outlines the experimental procedures. Sec. III presents scanning tunneling microscopy and spectroscopy (STM/STS) data  about the formation of the nano-domains and determination of the hole-doping level  for the different sized domains. Sec. IV presents the domain-size dependent ZBA  observed at low temperature, which is then modeled in Sec. V using the chiral  superconductivity and DCB scenarios mentioned above. Sec. VI presents STS data from  substrates with lower carrier density, revealing classical Coulomb blockade behavior.  Summary and conclusions will be presented in Sec. VII.
Analyzing transverse momentum spectra by a new method in high-energy collisions<|sep|>The space-time evolution of hadron–hadron, hadron–nucleus, and nucleus–nucleus (AA) or heavy-ion collisions is a complex process which involves diﬀerent degrees of freedom under diﬀerent spatiotemporal coordinates. Because of this complexity, it is diﬃcult to use a theory to describe the development of the entire system. After the initial stage of heavy-ion collisions, the system undergoes to a pre-equilibrium phase, followed by the de-conﬁned quarkgluon plasma (QGP) phase and then a possible mixing phase, in which it should display at least a signal of the ﬁrst-order phase transition. The hadronization then takes place where the compound hadrons are formed from the original partons. With the increase of collision energy, the energy or temperature at which the phase transition from hadron to QGP may occur initially is referred to as the critical energy or temperature. After the hadronization stage, the chemical composition of the system is frozen and inelastic collisions stop, where the particle ratios are ﬁxed. Immediately afterwards, with the expansion of the system, the mean-free-path of the particles becomes larger than the size of the system, and this stage is referred as the kinetic freeze-out stage. The transverse momentum (pT ) spectra of particles are no longer changed. Finally, the particles ﬂy to the detector and their properties are measured. The temperature at the stage of chemical freeze-out is called the chemical freeze-out temperature (Tch) [1, 2, 3], and the stage of kinetic freeze-out is known as the kinetic freeze-out temperature (T0 or Tkin). We are interested in the study of particles at the stage of kinetic freeze-out. The kinetic freeze-out is an important and complex issue. Diﬀerent literature presented diﬀerent kinetic freeze-out scenarios such as the single [4], double [5, 6], triple [7], and multiple kinetic freeze-out scenarios [8, 9, 10]. In addition, the behavior of T0 with increasing the centrality and collision energy is also very complex [11, 12, 13, 14]. To our knowledge, the behavior of T0 with the collision energy is known to increase from a few GeV to 7 or 10 GeV, after which the trend becomes indeﬁnitely saturated, increscent, or decrescent. This indeﬁnite trend is caused by the diﬀerent exclusions of ﬂow eﬀect. Diﬀerent from T0, the eﬀective temperature T which contains the contributions of thermal motion and ﬂow eﬀect has deﬁnite behavior. Therefore, we focus our attention on the energy dependence of T in central AA collisions. In this work, we will use a new method to analyze the pT spectra of particles so that we can extract T and other parameters. The value of pT for a given particle can be seen as the superposition of contributions of two participant partons with random azimuths, where the two partons are from the projectile and target nuclei generally. In the rest frame of the emission source, partons are assumed to emit isotropically. The Monte Carlo method is performed and the statistical treatment is used in the ﬁt to the spectra. The transverse momentum contributed by each parton is assumed to obey the modiﬁed Tsallis-like distribution. Thus, the particle’s pT is obtained from the synthesis of two vectors with diﬀerent sizes and directions. In order to verify our results, the pT (or the transverse mass mT ) spectra of positively and negatively charged pions (π+ and π−), positively and negatively charged kaons (K+ and K−), protons and antiprotons (p and ¯p), as well as φ produced at mid-(pseudo)rapidity (mid-y or mid-η) measured in central gold-gold (Au-Au) collisions at the heavy-ion accelerator SIS (Schwerionensynchrotron) at the GSI (Gesellschaft f¨ur Schwerionenforschung) in Darmstadt, Germany, by the KaoS [15] and HADES [16, 17] Collaborations, in central Au-Au collisions at the Alternating Gradient Synchrotron (AGS) at BNL (Brookhaven National Laboratory) in Upton, USA, by the E866 [18], E895 [19, 20], and E802 [21, 22] Collaborations, in central Au-Au collisions at the Relativistic Heavy Ion Collider (RHIC) at BNL by the STAR [23, 24, 25, 26, 27] and PHENIX [28, 29] Collaborations, as well as in central lead–lead (Pb-Pb) collisions at the Large Hadron Collider (LHC) at CERN (Conseil Europ´eenn pour la Recherche Nucl´eaire) in Geneva, Switzerland, by the ALICE Collaboration [30, 31, 32] are studied. We can ﬁt the data and extract the excitation functions (energy dependences) of parameters. The remainder of this paper is structured as follows: the formalism and method are shortly described in Section 2. Results and discussion are given in Section 3. In Section 4, we summarize our main observations and conclusions.
Adjoint approach to calculating shape gradients for 3D magnetic confinement equilibria<|sep|>The design of stellarator magnetohydrodynamic (MHD) equilibria requires optimizing within a high dimensional space due to the fully 3-dimensional nature of the magnetic ﬁeld conﬁguration and the sensitive dependence of charged particle trajectories in such ﬁeld conﬁgurations (Boozer 2015). While there are many possible choices for the space in which to optimize, a common choice is the space of the shape of the outer boundary of the plasma (Hirshman et al. 1999; Drevlak et al. 2018). The conﬁning electromagnetic coils must then be designed to reproduce the desired plasma boundary. This approach was used to design Wendelstein 7-X (Grieger et al. 1992) and the Helically Symmetric Experiment (HSX) (Anderson et al. 1995). An alternative approach is to optimize the shape of electromagnetic coils directly to minimize an objective function which includes functions of the equilibria (Hanson & Cary 1984). This can be performed with the merged STELLOPT/COILOPT code, which was used in the late stages of the NCSX design (Strickler et al. 2004). To navigate such spaces, it is often useful to employ gradientbased optimization techniques. For this reason, it is desirable to compute derivatives with respect to shape. As the target optimized conﬁguration can never be realized exactly, an analysis of the sensitivity to perturbations, such as errors in coil fabrication or assembly, is central to the success of a stellarator. Tight tolerances have proven to be a signiﬁcant driver of the cost of stellarator experiments (Strykowsky et al. 2009; Klinger et al. 2013); thus an improvement to the algorithms used to conduct sensitivity studies can have great impact on the ﬁeld. To quantify the coil tolerances for ﬂux surface quality of LHD (Yamazaki et al. 1993) and NCSX (Brooks & Reiersen 2003; Williamson et al. 2005), perturbations of several distributions were manually applied to each coil. Sensitivity analysis can also be performed with analytic derivatives. Numerical derivatives with respect to tilt angle and coil translation of the CNT coils have been used to compute the sensitivity of the rotational transform on axis (Hammond et al. 2016). Analytic derivatives have recently been applied to study coil sensitivities of the CNT stellarator by considering the eigenvectors of the Hessian matrix (Zhu et al. 2018). Thus, in addition to gradient-based optimization, derivatives with respect to shape can be applied to sensitivity analysis. The gradients of ﬁgures of merit with respect to shape have often been represented as derivatives with respect to whichever quantities parameterize the shape. Examples of such quantities are the amplitudes (Rc mn, Zs mn) in the double Fourier series for the cylindrical coordinates of a toroidal surface. Another way to represent derivatives with respect to shape is the shape gradient (Landreman & Paul 2018), which provides a local and coordinate-independent form. Consider any scalar ﬁgure of merit, f, that depends on a 3D MHD equilibrium solution. We can consider f to be a functional of the shape of the outer boundary of the plasma, SP . In this case, a diﬀerential change to the boundary, δr, causes a corresponding change to the ﬁgure of merit, δf, Here n is the outward unit normal and S is the shape gradient. The shape gradient quantiﬁes the local linear sensitivity of a ﬁgure of merit to diﬀerential perturbations of the shape. As tangential displacements to SP do not cause any changes to f, δf only depends on the normal component of δr. The Hadamard-Zol´esio structure theorem (Delfour & Zol´esio 2011) states that under certain assumptions of smoothness, the shape derivative of a functional can be written in the form of (1.1). This can be thought of an instance of the Riesz representation theorem, which states that any linear functional, such as δf(δr), can be written as an inner product over the appropriate space (Rudin 2006). The motivation for the form of (1.1) is discussed in more detail in section 2 of Landreman & Paul (2018). If we consider f to be a functional of the shape of the electromagnetic coils, a diﬀerential change to the coils, δrC, will cause a corresponding change to the ﬁgure of merit, δf, Here the sum is taken over the coils, Ck is a curve describing the ﬁlamentary shape of each coil, C = {Ck}, and δrC = {δrCk}. The shape gradient for coil k is Sk. As tangential displacements to the coil do not change f, Sk must be perpendicular to the tangent vector along Ck. As the shape gradient provides local sensitivity information, it can be used to quantify engineering tolerances with respect to the displacement of coils or magnetic perturbations. The shape gradient representation can be computed from parameter derivatives by solving a small linear system (Landreman & Paul 2018). However, computing parameter derivatives can often be computationally expensive, as numerical derivatives require evaluating the objective function at least N +1 times for N parameters if one-sided ﬁnite diﬀerences are used, or 2N times for centered diﬀerences. As computing the objective function often involves solving a linear or nonlinear system, such as the MHD equilibrium equations, this implies solving the system of equations ⩾ N + 1 times. Numerical derivatives also introduce additional noise, and the ﬁnite diﬀerence step size must be chosen carefully. To avoid these diﬃculties, it is advantageous to compute shape gradients using adjoint methods (Pironneau 1974; Glowinski & Pironneau 1975; Dekeyser et al. 2012, 2014a,b). Adjoint methods allow the analytic derivative with respect to all N parameters to be computed with only two solutions to the system of equations. Adjoint methods are thus much more eﬃcient for computing derivatives with respect to many parameters, and they do not introduce the noise of numerical derivatives. Adjoint methods were recently used in the context of stellarator design by Paul et al. (2018) for shape optimization of coil winding surfaces. Rather than use parameter derivatives, in this work we will use an adjoint method to compute the shape gradient directly. This is sometimes termed adjoint shape sensitivity or adjoint shape optimization, which has its origins in aerodynamic engineering and computational ﬂuid dynamics (Pironneau 1974; Glowinski & Pironneau 1975). As with adjoint methods for parameter derivatives, this technique only requires the solution of two linear or non-linear systems of equations. This technique has been applied to magnetic conﬁnement fusion for the design of tokamak divertor shapes by solving forward and adjoint ﬂuid edge equations (Dekeyser et al. 2012, 2014a,b). As stellarators require many parameters to fully describe their shape, adjoint shape sensitivity could signiﬁcantly decrease the cost of computing the shape gradient. If one is optimizing in the space of parameters describing the boundary of the plasma or the shape of coils, the shape gradient representation obtained from the adjoint method can be converted to parameter derivatives upon multiplication with a small matrix (Landreman & Paul 2018). In section 2, the fundamental adjoint relations for perturbations to MHD equilibria are derived and discussed. These relations take a form that is similar to that of transport coefﬁcients that are related by Onsager symmetry (Onsager 1931). Speciﬁcally, perturbations to the equilibrium are characterized as a set of generalized responses to a complementary set of generalized forces. The responses and forces can be thought of as being related by a matrix operator, which is symmetric. The resulting relations among forces and responses can be used to compute the shape gradient of functions of the equilibria with respect to displacements of the plasma boundary, as in (1.1), or the coil shapes, as in (1.2), Several applications to stellarator ﬁgures of merit will be demonstrated in section 3. While the primary application considered in this work will be stellarator optimization, the relations we obtain are equally applicable for 2D equilibria.
Invariant Jet differentials and Asymptotic Serre duality<|sep|>This text provides several results on the Green-Grifﬁths bundle of k-jets of entire holomorphic curves over a projective manifold X. First, we prove an equivariant (jet coordinate-free) version of Morse cohomology estimates of J. P. Demailly [2], [3] for invariant k-jet metrics on Demailly-Semple bundles. The result will apply to the Green-Grifﬁth conjecture on the entire curve locus in projective manifolds. We extend the main result of Demailly [3] for the bundles EGG k,m(V ∗) of jet differentials of order k and weighted degree m to the bundles Ek,m(V ∗) of the invariant jet differentials of order k and weighted degree m. In this sense, Theorem 0.5 from [3] and Theorem 9.3 from [2] provide a lower bound ck k mn+kr−1 on the number of the linearly independent holomorphic global sections of EGG k,mV ∗ �O(−mδA) for some ample divisor A. We prove the same statement on a lower bound for linearly independent holomorphic global invariant sections on the same bundle. Notice that the group Gk of local reparametrizations of (C,0) acts on the k-jets by orbits of dimension k, so that there is an automatic lower bound ck k mn+kr−1 on the number of the linearly independent holomorphic global sections of Ek,mV ∗ �O(−mδA). In the second result, we prove the existence of a Serre duality for the asymptotic sections of Green-Grifﬁths bundles over the projective variety X. As was mentioned, in [2], [3], the existence of asymptotic global sections for Green-Grifﬁths jets on a projective variety X has been proved by using Morse estimates for the curvature of suitable metrics on these bundles. In other words H0(X,EGG k,mV ∗ ⊗ A−1) = H0(Xk,OXk(m)⊗π∗ kA−1)) are non-trivial when m ≫ k ≫ 0, where A is a Hermitian ample line bundle on X. One wants to formulate a Serre duality for the sheaf of k-jet differentials. In order to state the Serre duality for asymptotic k-jets, one needs to check the existence of the dual asymptotic sections. The formulation of Serre duality is based on the existence of the canonical sheaf. One difﬁculty here is to deﬁne the relative canonical sheaf since the ﬁbers of Xk � X have singularities. However, there is a generalization of the canonical sheaf deﬁned for singular varieties, as explained in [2], [4]. In this case, the relative canonical sheaf KXk/X is replaced by the canonical sheaf KV, where V is a holomorphic subbundle of TX, see [4]. We prove the existence of a Serre duality, written as for m,m′ ≫ k ≫ 0. In [4] Demailly and Reza Rahmati have proved the non-triviality of the cohomology groups in (1.1), see also [2], [3]. We explain how this formula can be helpful toward a proof of the Green-Grifﬁths conjecture. The third result is a formulation of a conjecture generalizing a theorem of J. Merker on the Green-Grifﬁths conjecture for generic projective hypersurfaces and a partial strategy to solve it. In [6], J. Merker proves the Green-Grifﬁths-Lang conjecture for a generic hypersurface in Pn+1. He demonstrated such a result for X ⊂ Pn+1(C), the universal family of hypersurfaces of degree d as a (n+1)!d! −1; by parametrizing all such hypersurfaces the GG-conjecture holds. In the proof by Merker for hypersurface case, the conjecture is established outside an algebraic subset Σ ⊂ Jn vert(X) deﬁned by vanishing of certain Wronskians, by using a result of Y. T. Siu in [1]. Merker speciﬁcally proves that there are constants cn and c′ n such that TJn vert(X) ⊗OXk(cn)⊗π∗ 0kLc′ n is generated at every point by its global sections, where L is an ample line bundle on X. One can ask if the same holds when X ⊂ Pn+1 is a generic member of a family X of projective varieties. We state this conjecture and sketch a methodology to approach this question using invariant metrics on the k-jet bundle. The fourth result is a ﬁniteness theorem for the ﬁber ring of the moduli of k-jets as a differential ring. We prove the differential ﬁniteness property of ﬁber rings in the moduli of jets of curves. The ﬁnite generation of ﬁber rings of k-jets is an open question due to the non-reductiveness of their group of symmetries concerning reparametrizations of the curves. The transformation group of kjets is a non-reductive subgroup Gk = C∗⋉Uk of GLk(C), where Uk is the unipotent radical consisting of upper-triangular k×k matrices of certain type. Unlike the reductive case, one can not deduce that the ring of polynomials invariant under the action of Gk, or its unipotent part, is ﬁnitely generated. The question is whether the ring of invariants C[f ′(0), f ′′(0),..., f (k)(0)]Gk is ﬁnitely generated, where we have considered f (k)(0) as germ of variables. An attempt toward this conjecture has been made in [10]. The ring appears as the local ring of invariant sections of Jk(X) = JkTX at a generic point x ∈ X. We prove that the above ring is differentially ﬁnitely generated. The remainder of this paper is as follows. Section 2 consists of basic deﬁnitions and notations on the jet bundle of holomorphic curves. Section 3 consists of our main results. Speciﬁcally, we present four results in this section. Some conclusions are given in Section 4. Finally, in an appendix, we recall the basics of differential ﬁelds.
Random lasing in an Anderson localizing optical fiber<|sep|>Unlike conventional lasers that require a resonator cavity to operate, random lasers exploit multiple scattering to trap light and provide feedback to the system1–3. One of the ﬁrst observations of random lasing was emissions from a laser dye solution containing micro-particles4–6. Since then, there have been several reports that attribute random lasing in disordered media either to diﬀusive extended modes or Anderson localized modes2,7–14. Anderson localization was ﬁrst used by Pradhan and Kumar to show increased reﬂected intensity due to wave conﬁnement in a mirrorless amplifying 1D structure15. Later, enhancement of lasing in random multilayer stacks and planar waveguides was explained conceptually by Anderson localization16,17. The nature of lasing modes in disordered media, particularly the role of Anderson localization in these systems, is still a matter of debate18–20. Currently, it is accepted that Anderson localization is not required for coherent random lasing in disordered media21,22. However, Anderson localized lasing modes can result in a narrower frequency response analogous to closed cavities in regular lasers23. Here, we present a disordered laser system for which Anderson localization plays an integral role in determining its lasing characteristics–strong transverse wave conﬁnement hosted by a dye-ﬁlled glass Anderson localizing optical ﬁber (g-ALOF) reduces spatial overlap of the lasing modes and results in high spectral stability of the random laser emission. The disordered glass ﬁber has a randomly distributed air-hole pattern in transverse dimensions which remains invariant along the length of the ﬁber for the typical lengths used in Ref.24 and here. The air ﬁll fraction was shown to be higher near the outer boundaries and therefore localization is stronger near the boundaries in comparison to the central regions of the ﬁber. For more details about the disorder structure of g-ALOF please see Supplementary Fig. S4. The output of a random laser is usually multidirectional which can hinder the usefulness of such sources (Fig. 1a). In one of the ﬁrst eﬀorts to tackle this problem, active control of the spatial distribution of the pump beam was proposed to achieve a highly directional random laser25. More recently, Schönhuber et al.26,27 designed a collimated random laser beam by sandwiching an active region between a bottom metallic layer and a planar waveguide patterned with random air-holes to extract the light. Our results present an alternative robust design where transversely Anderson localized (TAL) modes of a dye-ﬁlled disordered optical ﬁber are exploited to obtain directional random lasing; transverse light conﬁnement due to disorder-induced localized modes combined with free propagation in the longitudinal direction result in a highly directional random laser output (Fig. 1b). FIG. 1. Directional random lasing in a g-ALOF. a) Light from a random laser is usually multi-directional. b) Schematic of directional random lasing mediated by a disordered optical ﬁber.
Hadronic Light by Light Contributions to the Muon Anomalous Magnetic Moment With Physical Pions<|sep|>where F1(q2 = 0) = 1, F2(q2 = 0) = (gµ − 2)/2 ≡ aµ, q = p′ − p. The formula is written in a Euclidean gamma matrix convention, [γµ,γν] = δµ,ν. Its value has been measured very precisely by BNL E821 [3]. It can also be calculated theoretically to great precision [8]. The three standard deviation (287±80)×10−11 difference between experiment and theory makes muon g−2 a very interesting quantity. Much more accurate experiments, Fermilab E989 [11] and J-PARC E34 [14], are expected to start in a few years, so a more accurate theoretical determination will be necessary. Figure 1 shows the two diagrams that are the major sources of the theoretical uncertainty. Figure 1: (Left) Hadronic vacuum polarization (HVP). (Right) Hadronic light-by-light (HLbL). In this paper, we will only discuss the lattice calculation of connected hadronic light-by-light amplitude. Previously this quantity has only been calculated using models [15]. Attempts using lattice QCD were begun by T. Blum, M. Hayakawa, and T. Izubuchi more than 5 years ago [12, 6]. We have improved the methodology dramatically, as described in Ref [13, 7], which leads to a reduction in statistical errors by more than an order of magnitude. Since much of the material that was presented at LATTICE 2015 has now appeared in a long paper [7], this proceedings is devoted to an expanded discussion of three topics that were only brieﬂy presented during the conference: a) The sampling strategy that concentrated the result on the more easily evaluated, short distance region in position space. b) A new proposal to perform the QCD and QED portions of the HLbL calculation in different space-time volumes and c) First numerical results from a physical-quarkmass, 483 ×96 study.
How Scale Affects Structure in Java Programs<|sep|>Early on in the history of programming, a metaphor was put forward that has seen wide acceptance in the software community: that of programming as LEGO (Figure 1). The metaphor suggests that building large systems is a matter of connecting small standardized bricks together, one at a Figure 1. Left: Cover of the CACM, Special issue on Object-Oriented Programming, September 1990 [19]. Right: LEGO bricks showing standard dimensions. (Source: Wikipedia, “Cmglee”.) time, through their universal interfaces: the small bricks are independent of the scale and purpose of the construction. This metaphor had a tremendous inﬂuence in the development of OOP languages. Inspired by the simplicity of the LEGO construction model, these languages placed their focus on mechanisms that would allow to connect small computational units together to create large software systems. Meanwhile, in 1975 another idea was put forward that has also seen wide acceptance in the software community: that “programming in the large” has different characteristics from “programming in the small.” This idea was ﬁrst formulated by DeRemer and Kron [8], who argued that “structuring a large collection of modules to form a ”system” is an essentially distinct and different intellectual activity from that of constructing the individual modules.” DeRemer and Kron went on to advocate a “Module Interconnection Language” (MIL) for large systems. These two popular ideas aren’t mutually exclusive: it is possible to imagine system-wide directives and constraints (i.e. architecture) for large LEGO constructions. But DeRemer and Kron’s essay states a premise that puts some pressure on the LEGO metaphor: “Where an MIL is not available, module interconnectivity information is usually buried partly in the modules, partly in an often amorphous collec tion of linkage-editor instructions, and partly in the informal documentation of the project.” In LEGO terms, this might mean that in order to build a large castle, one might need to plumb stronger connection material into the bricks themselves. In short, the scale of the system would affect the internal structure of the construction. This paper focuses on the core of these two popular ideas by asking and answering the following question: Does the scale of the software system affect the internal structure of its modules or are modules scale-invariant? We want to ﬁnd out whether there are mathematical principles related to size in large ecosystems of software projects. Besides shedding light on the differences between programming-in-the-small and programming-in-the-large, this question has important implications for research. A common practice for validating ideas in software research is to collect a number of artifacts, either randomly or using some criteria, measure the effects of the ideas using those artifacts, and reach conclusions from the empirical data. Even though size of software artifacts (projects, classes, etc.) has been known to be an issue in quantitative studies of software, software research continues to be fairly oblivious to its effect in these assorted datasets. This is particularly problematic for any studies involving software metrics, including OO metrics. It also affects performance studies that tend to collect data on relatively small programs that aren’t necessarily representative of large programs. Several studies published in the literature may have reached invalid conclusions by ignoring the effect of size or by treating it inappropriately. The question, as formulated above, is too ambitious to be answered in one single step. This paper takes only the ﬁrst step. We focus on Object-Oriented software systems, since those are the most inﬂuenced by the programming-asLEGO metaphor; other language families should be studied for broader conclusions. Within OOP, we focus on Java, since it is one of the most popular OOP languages; other OOP ecosystems should be studied for broader conclusions. Finally, we report on a dozen metrics that illustrate the main trends, but many more metrics could be studied. We deconstruct the general question into ﬁve research questions for which speciﬁc metrics can be measured: RQ2 Module Type: Is there a statistically signiﬁcant variation in the mix of classes and interfaces for projects of different size scales? This study puts forward strong evidence that, as programs become larger, the internal structure of the modules and the mixture of composition mechanisms used are affected. As such, the paper makes the following contributions: 1. It unveils strong empirical evidence of the existence of super- and sublinear effects in software that have not been measured before, and it shows concrete parameters of many non-linear relations that underly a large and important ecosystem of Java programs.
Dynamical symmetrization of the state of identical particles<|sep|>The symmetrization postulate of quantum mechanics states that any joint state of identical particles (i.e., having all their non-dynamic features, such as mass, charge, spin, etc., the same) should be either symmetric (for bosons) or antisymmetric (for fermions) under permutations of the particles [1–5]. Common sense though suggests that we could, in principle, attach labels and distinguish identical, non-interacting particles emanating from diﬀerent, largely separated sources, at least until their coordinate probability densities start to overlap. Indeed, assume that two distant sources SL and SR produce nearly-simultaneously two identical particles 1 and 2 in states |L⟩ and |R⟩, respectively, in spacelike-separated events. Then, if a particle is detected close to a source, say SL, shortly after the creation, we can conclude with high degree of conﬁdence that this is the same particle 1 that was emitted by SL. The hypothesis that independently generated, spatially separated identical particles can be considered distinguishable, with the joint wavefunction represented by a tensor product of the wavefunctions of each particle, has also been discussed in the past [6–8]. It was then argued that transition to the symmetrized state might occur once the spatial wavefunctions of the particles start to overlap, reducing their distinguishability [7, 9]. It has recently been suggested that such a transition during bound state formation may be related to the excess energy transferred to an environment [10]. Let us recall two common arguments against the hypothesis that the initial state of largely separated particles can be represented by a tensor product of single-particle states. The ﬁrst argument is ﬁeld-theoretical [9]: particles correspond to excitations of a quantum ﬁeld, and particle creation and annihilation correspond to a transition between the Fock states of the ﬁeld eﬀected by appropriate bosonic or fermionic ﬁeld operators. This reasoning is, however, circular: the symmetrization postulate is not derived from second quantization; rather, the symmetrization postulate is used to derive the properties (commutation relations) of the second-quantized bosonic or fermionic ﬁeld operators [5, 11]1. The second argument relies on the vast experimental evidence that identical particles within a small distance from each other, or those that have interacted with each other in the past, are in appropriately symmetrized states. Yet, for spatially separated, non-interacting particles, the results of localized in space measurements (i.e., coordinate, but not momentum) are the same for both product and symmetrized states [4]. One may then argue that product states, even if they were possible, are redundant in the theory. These arguments, however, ignore the possibility of a transition between product and symmetrized states of non-interacting particles prepared far apart and approaching each other. Then, the diﬀerence between product and symmetrized states can be detected via symmetric measurements on the particles, even if their states do not (yet) overlap in space, as discussed below. Before proceeding, we note that measurements on identical particles are insensitive to their permutations, since the interaction of a measuring apparatus with identical particles is, by deﬁnition, symmetric under particle permutations [4, 8, 9]. Hence, all allowed measurements on identical particles should be described by permutation-invariant operators [4, 9]. Such measurements produce permutation-invariant results for any state, symmetrized or not. In 1 In this context, recall the message of the spin-statistics theorem from relativistic ﬁeld theory [11, 12]: integer-spin (half-integer-spin) ﬁelds cannot be quantized via anticommutators (commutators), but can be quantized via commutators (anticommutators). Hence, integer-spin (half-integer-spin) ﬁelds can be represented as bosons (fermions). This statement however does not exclude that, e.g., half-integer spins can be quantized via some other algebraic (paraparticle) structure [11, 12].
Modeling IoT-aware Business Processes - A State of the Art Report<|sep|>This report presents an analysis of the state of the art of modeling IoT-aware business processes. In  this introductory section, we first explain why it is relevant to combine the Internet of Things (IoT)  and business process management (BPM).  This sets the stage for the rest of this report. Next, we  explain the goals and the structure of this report.
Red and Blueshifts in Multi-stranded Coronal Loops: A New Temperature Diagnostic<|sep|>The coronal heating problem is a long-standing issue in solar physics. It aims at explaining the reason why the solar corona has a mean temperature above 1 MK, while the surface of the Sun has an eﬀective temperature of 5800 K, and also how this high temperature can be maintained during a solar cycle. Hence the solar coronal heating problem is to understand how the thermal energy is continuously and uniformly transported and distributed over a large volume like the corona. One of the favorite model is the heating by small bursts of magnetic energy, the so-called nanoﬂare heating problem as ﬁrst mentioned by Parker (1983; 1988): this model explains that the coronal plasma can be heated to high temperatures with a high rate of occurrences of uniformly distributed nanoﬂares explaining the sustainability of the heating. Parker’s idea has been developed for coronal loops in active regions in which the main source of magnetic energy is located, and with the wealth of observations in diﬀerent wavelength ranges, which provide strong observational constraints (Cargill 1993, 1994; Cargill and Klimchuk 1997; Mendoza-Brice˜no et al. 2002; Cargill and Klimchuk 2004). By extension, the nanoﬂare model refers to the heating of loops by a series of energy releases, and does not refer to any particular mechanisms generating these bursts of energy (e.g., magnetic reconnection, wave mode coupling, turbulence). Even if they account for a small fraction of the total coronal heating budget, the understanding of the heating of coronal loops is a crucial step for solving the global coronal heating problem: loops are well observed in a broad range of temperature bands and thus their thermodynamical properties are well constrained leading to a detailed study of the physical processes at play. The stateof-the-art of this ﬁeld of research has been reviewed in length by Reale (2010). Despite the large number of loop observations, their nature is still debated: single ﬁeld line versus multi-stranded ﬂux bundle (e.g., Cirtain et al. 2007), isothermal versus multithermal (e.g., Schmelz et al. 2009). In this paper, we deﬁne a loop as a multistranded ﬂux bundle implying a multithermal plasma. The loop temperature is sustained by a series of small, short releases of energy mimicking the nanoﬂare model. The multi-strandedness of coronal structures has been recently revealed by high-resolution instrumentation (e.g., Kontar et al. 2010; Brooks et al. 2013; Scullion et al. 2014). There is a long history of modelling coronal loop using 0D/1D hydrodynamic models, 3D mhd models, assuming a monolithic or multistranded loop, including or
On profitability of selfish mining<|sep|>The stability of Bitcoin protocol [9] relies on rules aligned with self-interest of participants in the network. One rule is that miners make public blocks as soon as they are validated. “Selﬁsh Mining” is a deviant mining strategy described in [3] where a miner withholds validated blocks and releases them with a well timed strategy designed to invalidate the maximum number of blocks mined by the rest of the network. Other researchers have proposed other selﬁsh mining strategies which are supposed to be “optimal” [12]. The selﬁsh mining strategy is presented in courses and textbooks on Bitcoin such as [1] or [13]. All these articles do not make a proper analysis of the proﬁtability of the attack compared to honest mining, and, more critically, do ignore time considerations. More precisely, the Markov model used in these papers is limited by inception since it does not incorporate an analysis of the time duration of the attack. The main goal of this article is to carry out a proper analysis of proﬁtability that is lacking in the literature. It turns out that without diﬃculty adjustments the strategy is unsound.
Spectral classification of the mass donors in the high-mass X-ray binaries EXO 1722-363 and OAO 1657-415<|sep|>High-mass X-ray binaries (HMXBs) are comprised of massive (≥ 10M⊙) donor stars and an accreting compact object (a neutron star or black hole). They are typically divided into two main classes, Be/X-ray binaries (Be/X) and Supergiant X-ray binaries (SGXRBs). Be/X binaries form the majority (∼ 60%) of HMXB systems (Liu et al. 2006), consisting of a neutron star which accretes matter from the circumstellar equatorial disc of a Be star. These systems have wide orbits and moderate eccentricities, exhibiting two main types of transient X-ray outbursts. Type I occur at the periastron passage of the neutron star with LX ∼ 1036 - 1037 erg s−1. Type II outbursts, which are not correlated with the orbital period, display luminosities of LX ≥ 1037 erg s−1 (Okazaki & Negueruela 2001). SGXRBs have counterparts which are early type supergiants and accrete from either Roche-lobe overﬂow or a radially outﬂowing stellar wind. They are persistent sources of X-ray emission, with stellar wind fed systems having a lower ﬂux than Be/X systems (LX ∼ 1035 1036 erg s−1). For stars that ﬁll their Roche lobe a much higher X-ray luminosity can be achieved of ∼ 1038 erg s−1 (Liu et al. 2006). The Corbet diagram (Corbet 1986) describes the relationship be tween orbital period Porb and pulse period Ps for an accretion powered pulsar in a HMXB system. The correlation Ps ∝ P4/7 orb for wind fed systems was found, contrasting with that for Be/X systems of Ps ∝ P2 orb. Each seperate class lies in a distinct location on the Corbet diagram allowing a diﬀerentiation to be made between Be/X and both underﬁlled and ﬁlled Roche-lobe SGXRBs systems. Eclipsing X-ray pulsar systems provide a means to accurately determine the mass of the neutron star. Such systems are of signiﬁcant importance, as they are the only binary accreting system in which the neutron star mass may be measured, providing insights and constraints on the neutron star equationof-state. Unfortunately, only ten eclipsing HMXB systems have been identiﬁed within the Galaxy, meaning that the characterisation of further examples is a priority. In this paper we undertake the ﬁrst step in this process for two systems in which the neutron star masses have yet to be measured; the classiﬁcation of the mass donors in EXO 1722-363 and OAO 1657-415. EXO 1722-363 (IGR J17252-3616) was discovered in 1984 by EXOS AT Galactic plane observations (Warwick et al. 1988). Further observations carried out in 1987 by the Ginga satellite showed the presence of pulsations with a 413.9 ± 0.2s period.
Decay Replay Mining to Predict Next Process Events<|sep|>With the ongoing development of digitizing and automatizing industries along with the steady increment of interconnected devices, we can project more interactions onto processes [1], [2]. These processes can represent procedures in different industries such as retail [3], software development [4], healthcare [5], network management [6], project management [7], or manufacturing [8]. One illustrative example is the process of a customer loan application in ﬁnancial institutes [9]. An applicant can request money for speciﬁc purposes. The application then undergoes several process steps such as negotiation, request validation, fraud assessment, offer creation and/or application rejection. Each step of the process utilizes different institutional resources such as employees, customer records, IT systems, or thirdparty resources to check the creditworthiness of applicants. Though trivial, the process gets complex with an increasing number of applications and requirements of the institute. While traditional process mining is primarily concerned with the discovery, analysis, and monitoring of processes, predictive process management gains momentum by enhancing process models. Predictive process management plays an important role in the areas mentioned earlier. Knowing when speciﬁc situations occur, or in which state a process will J. Theis and H. Darabi are with University of Illinois at Chicago, Department of Mechanical and Industrial Engineering, 842 West Taylor Street, Chicago, IL 60607, United States. H. Darabi is the corresponding author. E-mail: {jtheis3, hdarabi}@uic.edu. be next, is important to meet qualitative and/or quantitative requirements of businesses and organizations. Many businesses deploy Process-Aware Information Systems such as workﬂow management systems, case-handling systems, enterprise information systems, enterprise resource planning, and customer relationship management systems. These are software tools which manage and execute operational processes involving people, applications, and/or information sources based on process models [10]. Such systems record events associated to different process steps along with time and other related information which can be utilized for predictive process management. Typical use cases comprise the prediction of the next event, forecasting of a process’ ﬁnal state, or time interval prediction of future events [11]. Predicting the next event elicits special attention since it gives organizations the ability to forecast process deviations. This type of early detection is essential for intervenability before a process enters risky states [12]. Moreover, predictive process management assists businesses in resource planning and allocation, providing insights on the condition of a process to fulﬁll for instance service-level agreements [13]–[15]. With this motivation, a range of different methods have been proposed on predicting the events in process sequences. Most recent advances are made in utilizing different deep learning architectures such as Long Short-Term Memory (LSTM) neural networks and stacked autoencoders [15]– [18]. However, these techniques do not discover process models at ﬁrst, but perform their predictions on the raw event logs. This makes decision making hard to understand and difﬁcult to explain, which is crucial to discover the weaknesses of a process. Furthermore, since neural networks are not infallible [19], commonsense knowledge and obvious logical policies are suggested to be introduced into a deep learning model from the beginning to reduce potential vulnerability. This knowledge is easy to obtain from process discovery algorithms. Therefore, modeling processes from scratch using neural networks is costly and partially redundant. Thus, one of the research questions is how to retain process models like Petri nets (PN) [20] with its logic, interpretability, and comprehensibility [21]–[24], and combine it with the strengths of deep learning towards more interpretable models to improve performance at the same time. A further research motivation arises due to weaknesses of recent predictive methods. Some of the state-of-the-art algorithms do not consider event timestamps as features at all [12], [16]. However, the duration between two events and/or sequences of events might be correlated with a future process outcome. Therefore, we suggest taking event times into account for predictive modeling. In the current work, we propose an innovative method to predict the next event of a running process case which engages with the issues mentioned above. We ﬁrst leverage a state-of-the-art process mining algorithm to discover a PN based process model from an event log. Then, we enhance the process model with time decay functions. In this way, we can create continuous and timed state samples which we ﬁnally couple with process resources to train a neural network for the prediction of the next event. We call this approach Decay Replay Mining - Next TrAnsition Prediction (DREAM-NAP). By taking this approach, we demonstrate signiﬁcant performance improvements. Our method outperforms the state-of-the-art techniques on all of the popular benchmark datasets. This paper is structured as follows. Section II discusses related work and most recent advances in the next event prediction of business processes. We introduce preliminaries in Section III. Section IV focuses on the proposed approach, especially on the decay function modeling in PNs and the deep learning architecture. Section V evaluates the approach against different existing methods. Finally, we conclude the paper and discuss future work in Section VI.
Investigating the Origins of Spiral Structure in Disk Galaxies through a Multiwavelength Study<|sep|>The density-wave theory has dominated the interpretation of spiral arms in disk galaxies since the mid-sixties (Lin & Shu 1964; Bertin & Lin 1995; Shu 2016). The original theory, with its disk-spanning standing-wave pattern created by resonant modes, has been challenged by numerical simulations which suggest that the waves should be subject to damping which probably prevents long-lasting modes from generating semi-permanent spiral arms. An alternative mechanism for the production of spiral density waves, known as swing ampliﬁcation (introduced by Goldreich & Lynden-Bell 1965 and Julian & Toomre 1966), proposes that small local disturbances can be ampliﬁed to create transient patterns. Eventually detailed simulations provided strong evidence that density waves are probably incapable of producing the kind of semi-permanent patterns called for by the original theory and that some form of ampliﬁcation along the lines of swing ampliﬁcation must play an important role in periodically causing transient spiral patterns to recur (Sellwood & Carlberg 1984). Currently, the theoretical situation is such that quite diverse views co-exist and there is no consensus that a single mechanism is responsible for all of the observed spiral patterns in galaxies. While some experts insist that spiral patterns typically last only a galactic rotation or two (Sellwood & Binney 2002; Grand et al. 2012), other theorists argue that swing ampliﬁcation can give rise to superposed modes of the system which can last for up to ten rotation periods (Sellwood & Carlberg 2011). This issue is relevant to the current paper because it has been argued that failure to ﬁnd consistent downstream displacements of observational tracers of star formation and stars of different ages is an argument against the reality of long-lasting spiral arms (Foyle et al. 2011). By contrast, in previous work (PourImani et al. 2016) we conﬁrmed a key prediction of the densitywave theory, that spiral-arm pitch angle varies with observation wavelength. This is in contrast to the predictions of rival theories, such as the Manifold theory (for a discussion of this theory test see Athanassoula et al. 2010). The density-wave theory predicts that the density-wave gives rise (through compression of clouds approaching and passing through it) to a star-forming region and that newly born stars will move downstream of this star-forming region before they are observed. How this affects pitch angle depends crucially on the existence of a corotation radius, a point on the disk at which the rotational speed of stars equals the rotational speed of the ﬁxed spiral pattern itself (the density wave)(Peterken et al. 2019). Inside the corotation radius, stars move faster than the pattern speed and downstream means “in advance of the pattern.” Outside the corotation radius stars move slower than the pattern speed and downstream means “falling behind the pattern.” Thus, newly born stars should form a spiral arm that is tighter (with a smaller pitch angle) than the spiral density wave itself, as shown in Figure 1. In an earlier paper (Pour-Imani et al. 2016) we showed that for a sample of 41 galaxies there is a clear difference between the pitch angle of two wavelengths associated with stellar light, the B band and 3.6 μm and two wavelengths associated with star formation, 8.0 μm and 151 nm in the far ultraviolet (FUV). The stellar pitch angles are uniformly smaller (tighter spiral arms) than the star formation pitch angles. In this paper, we look at another wavelength associated with star formation, the H-α line. In conﬁrmation of the earlier result, the pitch angles for this wavelength agree well with those previously measured for 8.0 μm and the FUV. In addition, we have added pitch angles measured in the u band. This band lies midway between the FUV and the B band, which, though close in wavelength, disagree in pitch angle. Not surprisingly, we ﬁnd evidence that the pitch angle associated with the u-band appears to lie between these other two, suggesting that there are some stars that do not live long enough to move from the star-forming region (seen in the FUV), stars which move a short distance away (u band), and stars that live long enough to move clearly away from the starforming region (B band). The existence of a color gradient from blue to red downstream from the star-forming region is not, however, the only prediction of the modal density wave theory. Because the process of gravitational collapse itself takes time it is likely that the star-forming region is itself downstream from the actual density wave. Thus, upstream from the star-forming region there may be an opposite color gradient, which goes from red, old disk stars compressed by the density wave, to blue young stars found in the star-forming region. As we have seen, upstream spirals have looser pitch angles than downstream spirals, therefore the density-wave spiral itself has a looser pitch angle than the star formation spiral and therefore the redder arm is looser than the blue arm, which is the opposite for the gradient on the other side of the star-forming region described above. One has to be careful here though. Although clearly one would expect a looser red spiral upstream from a tighter blue-spiral, there might not be a continuous gradient from blue to red, since the red spiral created by the density wave compressing old disk stars close together affects all old disk stars equally, regardless of their particular age or color. Furthermore, when considering either of these two color gradients, one must remember that extinction may complicate matters by obscuring some wavelengths more than others. In considering these two possible color gradients predicted by density wave theory, a blue to red gradient downstream from the star-forming region and a red-to-blue gradient upsteam from it, our results come down in favor of the former. An important point is that the B-band and 3.6 μm pitch angles agree reasonably well with each other. If anything we see the 3.6 μm pitch angle as being even a little tighter than the B-band one. In short, there may be a steady gradient from blue to red of tightening pitch angles, moving through FUV, the u band, and the B band to 3.6 μm. However, some other groups report a different result, in line with the expectation that in the nearinfrared (NIR) one views the old red disk stars compressed together by the density wave itself (Grosbol & Patsis 1998; Martínez-García 2012; Martínez-García et al. 2014). This redto-blue gradient is just as much a prediction of the density wave theory as the blue-to-red gradient reported by us, but there is obviously a question as to which one is really observable! This is a difﬁcult question to answer, because the differences between B-band pitch angles and 3.6 μm pitch angles are typically small. We shall discuss this issue in more detail below, but the overall picture right now is that the result reported by some groups (Seigar et al. 2006; Davis et al. 2012), that the difference between pitch angles measured in these two bands is very small at best, may be the most that can be said right now. It has been proposed to us that an interpretation is possible that unites both of these scenarios. If it should happen that the star-forming region is upsteam from the density wave, then it could follow that the density-wave compression red spiral would exist downstream and roughly overlap with the blue-to-red spiral reported by us. Presumably, this could happen if the approach to the region of maxiumum densitywave compression was enough to kick start collapse in gas Figure 1. Illustration of spiral-arm structure based upon pitch angle measurements at multiple wavelengths in this paper. We observe tighter arms (lower pitch angle) for 3.6 μm and the B band, looser arms (higher pitch angle) for the u band and even looser arms for 8.0 μm, FUV, and H-α images. Thus, the star-forming arm is upstream from the blue arm, which is in turn upstream from the red arm. This conforms with the density-wave theory’s prediction of variation in pitch angle with the image wavelength of light (Pour-Imani et al. 2016). clouds before they actually crossed the maximum of the spiral potential. Although such a possibility cannot be ruled out, it does not seem to be reﬂected in most simulations of galactic density waves. Nevertheless, we illustrate the scenario in Figure 2.
A Simple Yao-Yao-Based Spanner of Bounded Degree<|sep|>Let G = (V, E) be a connected graph with n vertices embedded in the Euclidean plane. For any pair of vertices u, v ∈ V , an uv-path is deﬁned by a sequence of edges uu1, u1u2, . . . , usv. A subgraph H of G is a length spanner of G if, for all pairs of vertices u, v ∈ V , the length of a shortest uv-path in H is no longer than a constant times the length of a shortest uv-path in G; if the constant value is t, H is called a length t-spanner and t is called length stretch factor. The power needed to support a wireless link uv is |uv|β, where β is a path loss gradient (a real constant between 2 and 5) that depends on the transmission environment. A subgraph H of a graph G has power stretch factor equal to ρ if, for all pairs of vertices u, v in G, the power of a minimum power uv-path in H is no higher than ρ times the power of a minimum power uv-path in G. Li et al. [3] showed that a graph with length stretch factor δ has power stretch factor δβ, but the reverse is not necessarily true: The problem of constructing a sparse spanner of a given graph has received considerable attention from researchers in computational geometry and ad-hoc wireless networks; we refer the reader to the recent book by Narasimhan and Smid [6]. The simplest model of a wireless network graph is the Unit Disk Graph (UDG): an edge exists in the graph if and only if the Euclidean distance between its endpoints is no greater than 1. It is a standing open question to decide whether the Yao-Yao structure for UDGs introduced by Li et al. [3] is a spanner of not. The Yao-Yao graph (also known as Yao plus reverse Yao) is based on the Yao graph [8], from which a number of edges are eliminated through a reverse Yao process, to ensure bounded degree at each node. Progress towards resolving this question has been made by Wang and Li [7], who showed that the Yao-Yao graph has constant power stretch factor in a civilized UDG. For constant λ > 0, a λ-civilized graph is a graph in which no two nodes are at distance smaller than λ. Most often wireless devices in a wireless network can not be too close, so it is reasonable to model a wireless ad hoc network as a civilized UDG. In this paper we show that the Yao-Yao graph for a civilized UDG has constant length stretch factor as well. Although several papers refer to a similar result as appearing in [3], to the best of our knowledge there is no version of [3] that publishes this result. We also analyze the bounded degree spanner generated by the Yao-Sink technique introduced in [4]. The sink technique replaces each directed star in the Yao graph consisting of all links directed into a node u, by a tree T(u) with sink u of bounded degree. We propose an enhanced technique called Yao-Sparse-Sink that ﬁlters out some of the edges in the Yao graph prior to applying the sink technique. This enables an eﬃcient local computation of sparse sink trees, more appropriate for highly dynamic wireless network nodes. Our analysis of the Yao-Sparse-Sink method provides additional insight into the properties of the Yao-Yao structure. We also show that all these structures for UDGs – Yao, Yao-Yao, Yao-Sink and Yao-Sparse-Sink – have arbitrarily large weight. The rest of the paper is organized as follows. In Section 2 we introduce some notation and deﬁnitions and discuss previous related work. In Section 3 we show that the Yao-Yao graph is a spanner for UDGs of bounded aspect ratio (in particular, for λ-civilized UDGs). In Section 4 we discuss the Yao-Sink method and, based on this, we propose a new technique called Yao-Sparse-Sink that computes sparse sink trees eﬃciently. Finally, in Section 5 we show that all these structures for UDGs have unbounded weight.
A Unified, Hardware-Fitted, Cross-GPU Performance Model<|sep|>Being able to approximately predict the running time of computational kernels is a key step towards the automation of performance tuning for complicated, modern, vectorbased, massively parallel processor architectures. We present a simple, effective model to achieve such a prediction that is realized on top of, though technically not dependent on, a program transformation system, providing a self-contained foundational building block to aid the developer of automated tuning solutions in exploring the vast search space of possible and, from the point of view of the result, equivalent program variants. We note that we mainly view our model as a more economical alternative to evaluating the execution time of a computational kernel than, for example, using actual on-device timing runs. Our system primarily targets the execution paradigm embodied by modern GPU Hardware, as exposed in, for example, the CUDA or OpenCL compute abstractions. The system makes no assumptions about the internal organization of the hardware, and devicespeciﬁc parameters are obtained from a black-box adaptation process that needs to run precisely once on each new piece of hardware on which the system is used. GPUs, originally designed for rapid graphics rendering, have highly parallel single instruction, multiple data (SIMD) architectures that make them particularly useful for dataparallel problems. Over the last decade, general purpose GPU programming has risen in popularity. Some of the world’s fastest supercomputers [Meuer et al., 2015], make use of thousands of GPU nodes, including Oak Ridge National Laboratory’s Titan supercomputer. GPU programming has been facilitated by the release of general purpose GPU programming systems, including Nvidia CUDA in 2007 and the Open Computing Language (OpenCL) in 2009 [Munshi et al., 2011, Nvidia Corporation, 2015]. Much of the previous work in GPU performance modeling has focused on constructing analytical models of instruction-level execution based on detailed hardware knowledge and instruction analysis for a single architecture. Many of these models predict well for their speciﬁc target architecture. For example, Hong and Kim [2009] present an analytical performance model for Nvidia GPU architectures that estimates memory-level and thread-level parallelism. They further extend their model for power prediction [Hong and Kim, 2010]. This model achieves a geometric mean error of 13.3% when predicting performance of the MERGE [Linderman et al., 2008] benchmarks on four Tesla generation Nvidia GPUs. It makes extensive use of hardware performance characteristics, such as timing delays between memory transactions, DRAM access latency, and instruction execution cycles, and requires an analysis of PTX assembly instructions. Baghsorkhi et al. [2010] also use deep analytical knowledge of a (single) GPU, and, unlike Hong and Kim, model branch divergence, bank conﬂicts, and SIMD pipeline delays. From the perspective of optimization selection, Cavazos et al. [2006] present a probabilistic predictor of transformation selection using a non-analytical, black-box model based on an artiﬁcial neural network. Joseph et al. [2006] use techniques from machine learning to identify piecewise nonlinearities in cost metrics. Other approaches emphasize the performance of single subsystems, such as branch prediction [Emer et al., 2002]. Zhang and Owens [2011] take a slightly different approach, using the results of microbenchmarks to derive a throughput model for instruction pipeline, shared memory, and global memory costs. They focus on identifying performance bottlenecks and guiding the optimization process rather than predicting execution time. Our work differs from previous performance prediction work in ﬁve ways: performance-relevant kernel properties used to model execution time. To our knowledge, this is the ﬁrst GPU performance model to do so. • Our model is hardware vendor- and generationindependent, and we demonstrate its performance on an AMD GPU and three generations of Nvidia GPUs. • Our model is simple and amenable to analysis: through the exposed weights and their known meanings, it becomes possible to reason about which parts of the kernel execution cost are attributed to which speciﬁc operations. • Evaluation of the prediction in our model is rapid and simple: Obtaining a cost estimate involves only computing a small inner product involving precomputed symbolic expressions dependent on problem size parameters. Given the shifting of landscape large- and extremescale computing in which the scale of a machine is often determined by power and cooling constraints, it is inevitable that individual nodes will need to carry a heavier burden than in prior machine generations. This trend currently shows no signs of reversing. As a result, more and more complex parallel computing architecture is found within each individual node. Key to leveraging this within-node parallelism is the ability to predict its performance on a given computation workload, for needs such as load balancing, job scheduling, performance optimization, machine design and qualiﬁcation, and benchmarking, as detailed in Section 6.1. For array-based workloads, as encountered in much of scientiﬁc computing, these needs are met directly by the modeling machinery supplied by this contribution. First, in Section 2, we deﬁne a set of kernel properties that are linearly related to run time. Second, we present a simple procedure for the extraction of kernel statistics relevant to performance in Section 3. We further demonstrate how these measures can be used to obtain the kernel properties that form the basis of our model. Third, in Section 4, we describe a ﬁtting procedure based on a library of measurement kernels to determine the parameters of our model for each piece of hardware on which it is to be used. Lastly, in Section 5, we evaluate our model’s predictive power on a number of GPUs from various hardware generations and vendors. Figure 1 provides a functional overview of the model.
Electronic structure of GdN, and the influence of exact exchange<|sep|>GdN crystallizes in a rock salt structure and is by now established as a ferromagnet with a Curie temperature of ∼ 69 K [1, 2, 3, 4]. The question whether it is an insulator or not is still under discussion. In the transmittance spectrum, a gap of 1 eV was observed [5]. From optical reﬂection and derivation of the plasma resonance and from the Hall eﬀect[6] it was argued that GdN was a semimetal. Thin GdN layers were found to be insulating in resistivity measurements [7]. In X-ray photoelectron spectroscopy (XPS) experiments, the occupied 4f bands were found at a binding energy of 7.8 eV for GdN [8]. For the related systems GdX (X=P, As, Sb, Bi), the occupied Gd 4f states were found at ∼ 9 eV, and the unoccupied states at ∼ 5 eV in XPS and X-ray bremsstrahlung isochromat spectroscopy (X-BIS) measurements [9]. The ﬁrst theoretical study of GdN was an augmented plane wave calculation using the Slater exchange potential [10], which resulted in an insulator with a very small gap [11]. The f electrons were treated as core states in this calculation. A further calculation with the f electrons in core [12] using the local density approximation (LDA) gave a similar band structure, but without gap. The spin-polarized solution was found to be metallic for majority, and insulating for minority spin. Recently, several calculations with explicitly treating the f electrons were performed. When the pure LDA was used, a metal was obtained [13, 14], with the occupied f states at ∼ -4 to -3 eV below the Fermi level. To take into account the strongly correlated nature of the f electrons, a Hubbard Uf term was applied to these states [15, 13, 14]. This still resulted in a metallic solution, and only when also a Ud term was applied to the Gd d states, an insulator was obtained [13]. Similarly, in [15], LDA+U calculations gave a metallic ground state, and by applying further rigid shifts to the 5d and 4f states, an insulating state was found. When the lattice was expanded, also a transition to an insulator was predicted, at the LDA+U level [16]. A slightly diﬀerent approach to take into account the strongly correlated nature of this system is to apply a self-interaction correction. This led again to a half-metallic ground state for GdN, with a gap in the minority channel [17]. Therefore, there are many reasons to study GdN. Besides these aforementioned reasons, from the theory point of view, a further point is that it would be interesting to test the performance of hybrid functionals, as a further method to treat strongly correlated systems. It has been shown that they give surprisingly good values for the band gaps for such systems, where the local density approximation and other standard functionals often fail [18, 19, 20]. Recently, the B3LYP hybrid functional was applied to UO2, with explicitly treated f electrons, and also a good value for the gap was obtained [21]. Also, plutonium oxides were found to be well described by hybrid functionals [22]. Cerium oxides were studied using screened hybrid density functionals with a local basis set[23], and with a plane wave basis set and hybrid functionals [24]. The CRYSTAL code used in the present work is based on a local Gaussian basis set. The ﬁrst version released in 1988 was a pure Hartree-Fock code. Hybrid functionals, which use an admixture of exact (Fock) exchange, were available from the 1998 release onwards. It turned out that, in the CRYSTAL implementation, the CPU times for calculations with hybrid functionals are comparable to the CPU times for calculations with standard functionals such as the LDA or gradient corrected functionals, and also the memory requirements are similar in both cases. The present CRYSTAL release [25] was announced of being able to use f-functions as polarization functions. In this article, it will be shown that it is also technically possible to perform calculations on systems with f electrons, with GdN as an example. There are various reasons why GdN was chosen as a test case: ﬁrstly, the f-occupancy if f 7, and the half-ﬁlled f-shell is probably one of the easiest examples to start with calculations on f-electron systems. Also, the crystal structure is fairly simple (NaCl type). In addition, as mentioned earlier, the system has already been theoretically described with other codes. This gives the opportunity to compare the present calculations, where possible, with the results obtained earlier.
Alkali vapor pressure modulation on the 100ms scale in a single-cell vacuum system for cold atom experiments<|sep|>Magneto-optical trap (MOT) loading is a limiting factor to the repetition rate of cold atom experiments. Fast loading is crucial for practical applications of cold atoms, such as atom interferometry1, as well as for experiments with high data volume, such as quantum state tomography of multiparticle entangled states2,3. Ideally, the MOT loading time tM should be a small contribution to the cycle time, which means that it should be shorter than about 1 s in experiments involving evaporative cooling. The MOT loading rate R is proportional to the partial vapor pressure of the element to be cooled (87Rb in this article). Consequently, this pressure should be high for fast loading, but low for long lifetimes after loading. A widespread approach is to use a two-chamber vacuum system with diﬀerential pumping. However, this considerably increases the complexity of the vacuum and optical systems. An interesting alternative is to temporally modulate the vapor pressure inside a simple, single-cell apparatus. Light-induced atom desorption (LIAD) from the cell surfaces has been investigated for this purpose4–9. Although it performs well in some experiments, there can be reproducibility issues, at least for Rb. The pressure modulation factor and also the eﬀective capacity of the surfaces (i.e., the number of pulses before depletion) vary by orders of magnitude between experiments. Strong dependence on material properties such as surface contamination is a possible explanation10. Another, straightforward way to modulate the pressure of alkali vapors is to modulate the heating current in commercial alkali dispensers11. A fast pressure rise is easily achieved by using elevated heating currents. However, the timescale of pressure decay is typically between 3 and 4 s11–16, too long to achieve a signiﬁcant reduction of the cycle time. Here we show that this decay can be accelerated by more than an order of magnitude without sacriﬁcing the simplicity and safe operation of a dispenser setup, by optimizing the thermal contact of the dispenser to an outside heatsink. Our device achieves pressure decay constants on the 100 ms timescale as desired, well within the range The long-term pressure evolution in a pulsed-source system is inﬂuenced by slow processes such as adsorptiondesorption dynamics, which can introduce timescales as long as several hours. Therefore, we also investigate the long-term evolution of 87Rb pressure, background pressure and number of trapped atoms. As expected, the background pressure increases with the intensity of the dispenser pulses. We ﬁnd evidence for both adsorptiondesorption dynamics and slow heat-up of the vacuum apparatus. The latter eﬀect is speciﬁc to our setup, which was not designed for this purpose, and we discuss straightforward improvements. In spite of these eﬀects, we ﬁnd that background pressure in our system remains lower than the pressure in a similar system with traditional, continuously heated dispenser, while the pulsed source accelerates MOT loading by more than an order of magnitude. This makes our system very attractive for many applications of cold alkali atoms. The outline of this paper is as follows. We ﬁrst discuss thermal coupling of the dispenser to the environment, motivate why an optimum range exists for this coupling, and estimate that the coupling found in usual experiments is too weak. Section III collects relevant facts about the mechanisms and time constants that govern the pressure evolution in the cell. In section IV, we describe our setup, where the thermal coupling is improved by copper parts which embrace the dispenser shell and thermally connect it to an outside heatsink. Sections V and VI present and discuss the experimental results obtained when this dispenser is used as a pulsed source for a MOT. It is shown that a steady-state regime can be established and its characteristics are investigated. Finally, section VII investigates the features of this steady-state regime as a function the alkali pulse intensity.
The Design and Fabrication of Platform Device for Dna Amplification<|sep|>Since the MEMS technologies had been developed, it was  widely applied for many territories, such as optics,  electric, biology and so on. In biotechnologies,  microfluidic chip and microarray chip were the major  research in bio-MEMS for recent years. It took many  advantages, for example, portable, miniature, short  reactive time and automation. These years, the MEMS  technology was also employed to fabricate the chip for  amplifying DNA, such as continuous-flow PCR [1,2]  flow-through thermocyclers [3,4], or closed chamber  PCR-chip [5,6]. the field of molecular biology. It was a very powerful  technique that could amplify a little amount of nucleic  acid to generate plentiful material for diagnose. Typically,  PCR included three steps which were denaturation of  double-stranded DNA, annealing of oligonucleotide  primer pairs, and extension of new DNA strands  catalyzed by DNA polymerase at temperatures of  approximately 95°C, 55°C and 72°C, respectively [7].  Following the cycles of temperature zones, theoretically,  a DNA segment got amplified 2n times after n cycles of  PCR [8]. Fig. 1 showed the reaction process of PCR. In fact, the enzyme efficiency was decayed after each cycle.  Thus, the efficiency of DNA amplification was A=(1+e)n  where e approximated 0.8-0.9 [4]. thermocycler that took 2~3 hours for 30 cycles. As a  result of time was almost spent on cooling and heating  during the reactive time. In order to expedite the PCR  efficiency, a different method of temperature transition  between three temperature zones and low reaction volume  were presented in this research.
On Neural Network Equivalence Checking using SMT Solvers<|sep|>For two pretrained neural networks of the same or diﬀerent architectures, the problem of equivalence checking concerns with checking whether the networks yield similar outputs (ﬁt for purpose) for the same inputs. The exact deﬁnition of the problem can refer to various “equivalence” criteria, depending on the speciﬁc neural network application (e.g. classiﬁer, regression, etc.), while the motivation behind the quest for a solution is fundamental, for a series of recent developments in machine learning technology. More speciﬁcally, we refer to knowledge distillation [8], i.e. the process of transferring knowledge from a large neural network to a smaller one that may be appropriate for deployment on a device with limited computational resources. Another area of interest includes the techniques, widely known under the term regularization [12], which aim to lower the complexity of neural networks, in order to show better performance during inference time, when they process data that are not in the training data set (avoid data overﬁtting). Moreover, neural network models in systems or programs with learning-enabled components [3] may have to be updated for a number of reasons [16]; for example, security concerns such as the need to withstand data perturbations (e.g. adversarial examples), or possibly incomplete coverage of the neural network’s input domain. In all aforementioned cases, we usually require a diﬀerent neural network model than the original one, which is expected to comply with respect to some given criterion of “equivalence”, depending on the speciﬁc neural network application. ∗Acknowledgment: This work has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 956123. This work presents the experience from our attempt to address the aforementioned problem based on the use of Satisﬁability Modulo Theory (SMT) solvers, which provide certain advantages, as well as limitations that justify the need for further research eﬀorts. Among their advantages, we stress their potential to deliver sound and complete veriﬁcation procedures for the equivalence between two neural networks. Regarding the limitations, we focus on their inability to scale towards solving the problem for real-size neural networks like the ones referred in well-known benchmarks, such as the neural network architectures for the MNIST dataset. We study the scalability bounds of our initial SMT-based encoding for our “equivalence” criteria, with respect to the neural network model parameters and the number of derived SMT variables. More concretely, this article introduces: • experimental results including (i) sanity checks of our SMT-based encoding, as well as (ii) equivalence checks for three diverse neural network applications covering the cases of classiﬁers and regression models Section 2 lays the background of our work by providing a formal deﬁnition of neural network models. In Section 3, we propose diverse “equivalence” criteria for the wide range of common neural network applications and we formally deﬁne the problem of equivalence checking. Section 4 presents our SMT-based encoding for reducing the problem of equivalence checking to a logical satisﬁability problem. Section 5 includes the experimental results and their interpretation. In Section 6 we review the related work and the paper concludes with a summary on our contributions and the future research prospects.
Chromoelectric Dipole Moments of Quarks in MSSM Extensions<|sep|>of beyond the standard model physics (for reviews see e.g., [1, 2, 3, 4]). Such sources can also induce electric dipole moment in elementary particles which can be signiﬁcantly larger operator. Thus the electroweak sector of the standard model produces an EDM which is 10−30 ecm [5, 6, 7] and it lies beyond the possibility of its observation in the foreseeable extension of the minimal supersymmetric standard model (MSSM) with a vectorlike multiplet [8]. Such an extension is anomaly free and thus the nice quantum properties of MSSM been considered by several authors since their discovery would constitute new physics (see, e.g., [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]). Such models have new sources of CP we analyzed the electric dipole operator in such a setting [23] and in this work we analyze the chromoelectric dipole operator in the extended MSSM model and its contribution to the Before discussing the EDM in the new class of models, it is relevant to recall the situation regarding the lepton and quark EDMs in MSSM. Here it is well known that MSSM has a past to remedy this problem. These include a ﬁne tuning of the phases to be small [24], suppression of the EDM by large sparticle masses [25], suppression of the EDM where various of the model and describe the nature of mixing between the vector generation and the standard three generations of quarks. In section 3.1 we discuss the loop contributions to section 3.1 for the exchange of the Z boson. In section 3.3 we compute the contribution from the exchange of charginos in the loop and in section 3.4 a similar analysis for the exchange of neutron dipole moment using the quark dipole moments. In section 5 we give a detailed numerical analysis of the contributions to the quark CEDM and to the neutron CEDM for
Measurement of light charged particles in the decay channels of medium-mass excited compound nuclei<|sep|>Macroscopic nuclear models [1] predict a rapid transition from oblate to prolate shape at high spin for medium-mass nuclei; this transition can be observed thanks to the low-ﬁssility of these systems which can therefore sustain high spins with limited ﬁssion probability. Variations of the GDR spectra can signal this process but, since the eﬀects are small, it is important an accurate description of the Figure 1. The experimental setup was composed of the GARFIELD apparatus (A), a wall of Phoswich detectors (B) and the HECTOR BaF2 crystals (C). decay of the excited compound nuclei (CN). With this in mind and considering that not many data in this region exist from exclusive measurements [2–4], we performed an experimental campaign at the Tandem-ALPI complex of the Laboratori Nazionali di Legnaro. 88Mo compoud nuclei have been studied, formed in the reactions of 48Ti on 40Ca target (500 µg/cm2) at 300 and 600 MeV bombarding energies. Table 1 reports some parameters for these reactions. Table 1. Compound nucleus excitation energy, vanishing ﬁssion barrier spin and grazing angular momentum and estimated total reaction cross section reported for both bombarding energies.
The MAGIC project. III. Radial and azimuthal Galactic abundance gradients using classical Cepheids<|sep|>Radial abundance gradients provide sound constraints to galaxy formation scenarios. Indeed, the star formation history, the accretion history, the radial migration of stars and the radial ﬂows of gas, and their variations with the Galactocentric distance, simultaneously determine the shape of abundance gradients. Chemo-dynamical models of the Milky Way must therefore reproduce the observed gradients and their evolution with time. The advent of Integral Field Spectrographs (IFS) has provided the opportunity to explore a large number of nearby ⋆ Based on observations obtained with the Southern African Large Telescope (SALT), programme 2016-1-MLT-002 (PI: Kniazev). Tables 1-4 are only available in electronic form at the CDS via the anonymous ftp to cdsarc.u-strasbg.fr (130.79.128.5) or via http://cdsweb.u-strasbg.fr/cgi-bin/qcat?J/MNRAS/(vol)/(page) E-mail: vkovtyukh@ukr.net spiral galaxies (for a review, see S´anchez 2020), most of them showing negative abundance gradients from the inner to the outer disc and a ﬂattening of the gradient in the outermost regions (S´anchez-Menguiano et al. 2018). Spectroscopic investigations covering the outermost regions of large spiral galaxies also show a well-deﬁned negative age gradient (e.g., Li et al. 2015), thus supporting the ”inside-out” scenario (e.g., Matteucci & Francois 1989; Spitoni & Matteucci 2011). - only recently, Gaia (Gaia Collaboration et al. 2018) provided accurate parallax-based distances for a large number of tracers (in particular, red giant branch (RGB) stars), but they are limited to the extended (≈5 kpc) Solar neighborhood, and RGB ages could be determined only for a smaller, nearby sample; - tracers with both accurate distances (even far from the Sun) and ages, such as Cepheids and open clusters, do not cover
Spectrum of the Anomalous Microwave Emission in the North Celestial Pole with WMAP 7-Year data<|sep|>The anomalous microwave emission (AME) component is highly correlated with the far infrared dust emission ([21], [23], [2], [22], [7], [12], [6], [9], [30], [17]) and is believed to be the electric dipole radiation from small spinning dust grains ([10]). Spinning dust models predict a peaked spectrum ranging 10–150 GHz depending on the local physical conditions ([1],[31]). Probing the peak of the emission enables us to compare models with observations and is the best way to distinguish the AME from synchrotron emission and free-free emission, both having a power-law spectrum. The AME has been studied in individual dust clouds associated with reﬂection nebulae, molecular clouds, photo-dissociation regions and HII regions ([14],[26]). The best examples of peaked AME spectra are probably the Perseus and ρ Ophiuchi molecular clouds, which have been probed with high accuracy thanks to the availability of many diﬀerent datasets. In these regions the derived peak is at ∼30 GHz ([26]). The study of the AME in more diﬀuse regions essentially becomes a component separation problem, which is often complicated by the lack of low-frequency data covering large sky areas. Diﬀuse dust-correlated emission has been detected with COBE-DMR([2]) and WMAP data ([6], [4], [9], [25], [15]), however the details of the spectrum are still unclear. Template-ﬁtting analyses show that the dust-correlated emission between 20 and 60 GHz is well described by a power-law ([2], [6], [9], [15]). Given the error bars, a peaked spectrum is not ruled out, but a low peak frequency (< 23 GHz) is favored. Alternatively, the observed spectrum could result from the superposition of multiple peaked components. The component separation analysis done by [4] found that a peaked AME model gives better results in terms of CMB cleaning, however no estimation of the AME peak frequency was attempted. In this work we address the estimation of the peak frequency of the diﬀuse AME on WMAP 7-yr data complemented by ancillary data with the Correlated Component Analysis (CCA, [27]) component separation method. We consider the North Celestial Pole (NCP) region of the sky, centered on Galactic coordinates (l, b) ∼ (125◦, 25◦). As already noted by [6], this region of the sky is particularly suited for this analysis since there is signiﬁcant thermal and anomalous dust emission, while synchrotron emission and free-free emission (traced by the 408 MHz from [19] and the Hα map from [8]) are faint.
DeepType: Multilingual Entity Linking by Neural Type System Evolution<|sep|>Online encyclopedias, knowledge bases, ontologies (e.g. Wikipedia, Wikidata, Wordnet), alongside image and video datasets with their associated label and category hierarchies (e.g. Imagenet (Deng et al. 2009), Youtube-8M (Abu-ElHaija et al. 2016), Kinetics (Kay et al. 2017)) offer an unprecedented opportunity for incorporating symbolic representations within distributed and neural representations in Artiﬁcial Intelligence systems. Several approaches exist for integrating rich symbolic structures within the behavior of neural networks: a label hierarchy aware loss function that relies on the ultrametric tree distance between labels (e.g. it is worse to confuse sheepdogs and skyscrapers than it is to confuse sheepdogs and poodles) (Wu, Tygert, and LeCun 2017), a loss function that trades off speciﬁcity for accuracy by incorporating hypo/hypernymy relations (Deng et al. 2012), using NER types to constrain the behavior of an Entity Linking system (Ling, Singh, and Weld 2015), or more recently integrating explicit type constraints within a decoder’s grammar for neural semantic parsing (Krishnamurthy, Dasigi, and Gardner 2017). However, current approaches face several difﬁculties: • Selection of the right symbolic information based on the utility or information gain for a target task. • Hand-labelling large amounts of data. DeepType overcomes these difﬁculties by explicitly integrating symbolic information into the reasoning process of a neural network with a type system that is automatically designed without human effort for a target task. We achieve this by reformulating the design problem into a mixed integer problem: create a type system by selecting roots and edges from an ontology that serve as types in a type system, and subsequently train a neural network with it. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1. heuristic search or stochastic optimization over the discrete variable assignments controlling type system design, using an Oracle and a Learnability heuristic to ensure that design decisions will be easy to learn by a neural network, and will provide improvements on the target task, 2. gradient descent to ﬁt classiﬁer parameters to predict the behavior of the type system. In order to validate the beneﬁts of our approach, we focus on applying DeepType to Entity Linking (EL), the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base (KB) (e.g. Wikipedia). Specifically we compare our results to state of the art systems on three standard datasets (WikiDisamb30, CoNLL (YAGO), TAC KBP 2010). We verify whether our approach can work in multiple languages, and whether optimization of the type system for a particular language generalizes to other languages1 by training our full system in a monolingual (English) and bilingual setup (English and French), and also evaluate our Oracle (performance upper bound) on German and Spanish test datasets. We compare stochastic optimization and heuristic search to solve our mixed integer problem by comparing the ﬁnal performance of systems whose type systems came from different search methodologies. We also investigate whether symbolic information is captured by using DeepType as pretraining for Named Entity Recognition (NER) on two standard datasets (i.e. CoNLL 2003 (Sang and Meulder 2003), OntoNotes 5.0 (CoNLL 2012) (Pradhan et al. 2012)). Our key contributions in this work are as follows: • A system for integrating symbolic knowledge into the reasoning process of a neural network through a type system, to constrain the behavior to respect the desired symbolic structure, and automatically design the type system without human effort. • An approach to EL that uses type constraints, reduces disambiguation resolution complexity from O(N 2) to O(N), incorporates new entities into the system without retraining, and outperforms all existing solutions by a wide margin. We release code for designing, evolving, and training neural type systems2. Moreover, we observe that disambiguation accuracy reaches 99.0% on CoNLL (YAGO) and 98.6% on TAC KBP 2010 when entity types are predicted by an Oracle, suggesting that EL would be almost solved if we can improve type prediction accuracy. The rest of this paper is structured as follows. In Section 2 we introduce EL and EL with Types, in Section 3 we describe DeepType for EL, In Section 4 we provide experimental results for DeepType applied to EL and evidence of cross-lingual and cross-domain transfer of the representation learned by a DeepType system. In Section 5 we relate our work to existing approaches. Conclusions and directions for future work are given in Section 6.
Chemical tracers of high-metallicity environments<|sep|>Understanding the production of metals, and the subsequent enrichment of the ISM is crucial in many ﬁeld of extragalactic astrophysics. For example, the initial mass function (IMF) of stars must have been diﬀerent in the early universe, where no metals were present, and whether there is any remaining variation of IMF with metallicity remains an open question (e.g. Omukai et al. 2005; Santoro & Shull 2006; Tumlinson 2007; van Dokkum & Conroy 2011). Gas metallicity is regulated by a complex interplay between star formation, infall of metal-poor gas and outﬂow of enriched material. The discovery of a relation between galaxy mass and metallicity (Lequeux 1979), showing that the more massive a galaxy is the higher is its metallicity, has increased debate about the metallicity and its relationship with galaxy characteristics and in particular with the properties of its interstellar medium. Estimating the metallicity of a galaxy remains to date a diﬃcult task with few methods available, and these have large uncertainties. Stellar metallicities are usually derived from absorption indices (such as those of magnesium and iron), but these weak features are hard to observe at high redshift, and only give information about the metallicity of the system in the past, when the stars were formed (e.g. McDermid et al. 2011). Optically determined metallicities of the ionised gas provide information on the metal content of star-forming complexes, allowing astronomers to probe the gas phase metallicity further out into the universe but this approach only works if the emission spectrum observed is powered by star formation processes. Where strong AGN activity, old stellar populations or shocks dominate, this method becomes unreliable. Furthermore, there can be strong degeneracies between low and high metallicity populations in some ionised gas metallicity indicators. In this paper, we explore the possibility of determining metallicity via submillimeter observations where no dust attenuation is present, remaining a reliable method even if star formation does not dominate the ionisation of the gas. In particular, we concentrate on exploring the high metallicity range represented by early-type galaxies (hereafter called ETGs). These metal-rich objects (el
Opinions within Media, Power and Gossip<|sep|>Despite the increasing diﬀusion of the Internet technology, traditional media – e.g., news papers and TV - remain the principal instruments for the information diﬀusion. People’s perceptions, knowledge, beliefs and opinions about the world and its evolution, get (in)formed and modulated through the information reported on by the mass-media. An unbalanced distribution of the power on this basin of information (and consensus) could lead to anomalies in the structure and the evolution of society. In this paper we focus upon a) the possible eﬀects that an anomalous distribution (and use) of the mediatic
Weyl locally integrable conformal gravity, rotation curves and cosmic filaments<|sep|>The Weyl conformal geometry has received a growing interest in the past years ([14, 22, 24]), especially on the subject of galactic rotation curves ([4, 5, 15, 16, 17] and references cited therein). It is an extension of Riemannian geometry where the Levi-Civita connection ∇ associated to a metric g is speciﬁed by the invariance condition ∇g = 0. The Weyl connection W∇ is the unique torsion free connection which satisﬁes the weaker condition W∇ξg = −2ψ(ξ)g, where ψ is a 1-form associated to g. This property extends at once to an equivalence class of pairs (g, ψ) where the metrics g are conformally equivalent and the 1-forms ψ diﬀer by exact 1-forms. The case where the 1-forms ψ are exact has been extensively studied as there always exists a compatible metric associated to a null 1-form, so that the Weyl connection is identical to the Levi-Civita connection for that metric. It follows that all derived objects such as Riemann curvature tensors, Ricci tensors, scalar curvatures and Einstein tensors are identical for both connections. The general case of not closed 1-forms is much more complicated as the Ricci tensor resulting from W∇ is no more symmetric. Section 2 recalls basic features of Weyl conformal manifolds and presents some of their particular properties when the 1-forms of the pairs (g, ψ) are locally integrable (closed and non-exact). In these cases the Weyl-Riemann curvature tensor WR, computed with W∇, satisﬁes the same ﬁve properties as the Riemann curvature tensors. The Weyl-Einstein tensor WG has zero divergence and diﬀers form the (pseudo-)Riemannian ones by 2 terms: a non-constant scalar function depending on ψ, in place of the optional cosmological constant Λ, and a shear stress tensor. Existence of closed non-exact 1-forms on a manifold M implies topological restrictions on M like the fact that M is not simply connected. The principal outcome is the Weyl-Einstein tensor and the comparison with the Einstein tensor for a given metric g. In section 3 we consider the geodesics of a Weyl connection and their preferred parametrization depending on (g, ψ) in the Weyl structure. The geodesics equations are expressed with the Leci-Civita connection of g and reveal acceleration terms involving the 1-form ψ. By Poincaré lemma these terms disappear for Levi-Civita connections of metrics g′ such that ψ′ locally vanishes. Section 4 presents a simple example of a locally integrable Weyl structure where g is the Schwarzschild metric. The associated 1-form ψ is locally equal to ε dϕ in Schwarzschild coordinates (ct, r, θ, ϕ) and ε is a dimensionless constant. The rotational force induced by the 1-form is invariant with respect to z translations. The angular momenta of geodesics are aﬃne functions of proper time and, according to initial conditions, total planar velocity curves turn out to be almost ﬂat as functions of r. In the neighbourhood of the z-axis and in the free case rS = 0, geodesics are accelerated or slowed down according to which side of the z-axis they are going.
Nonlocal Exchange Interactions in Strongly Correlated Electron Systems<|sep|>One of the most intensively studied and most fundamental models for the description of correlated electrons on a crystal lattice is the Hubbard model1–5. The central point of this model is to neglect all interactions aside from the local Coulomb repulsion between two electrons occupying the same lattice site. While the approximation of purely local interaction can pose a drastical simpliﬁcation, the model is still capable of describing a wide range of phenomena from Mott-Hubbard metal-insulator transitions to unconventional superconductivity. This is why the model became a key for understanding the competition between itinerancy and localization due to interactions.Several recently emerging quantum materials challenged the Hubbard model paradigm and realize correlated electron physics likely governed by more complex interaction patterns. First, in low-dimensional and insulating systems, the neglected nonlocal Coulomb interactions play a significant role due to reduced screening, and the Hubbard model can fail to provide an adequate description. It is well known by now that nonlocal Coulomb repulsion in the form of so-called density-density terms can drive the system towards a charge density wave (CDW)6–8, effectively screen the local interaction9, inﬂuence possibly existing tendencies towards superconductivity10–12 and lead to a renormalization of Fermi velocities13,14. In systems like twisted bilayer graphene15,16 or other twisted 2D materials17, general four fermion interactions are likely steering the low energy electrons18 due to the intricate real space patterns of the low energy electronic Wannier functions. Currently, very little is known about the eﬀects of general non-local four fermion interaction terms on electronic correlation phenomena. Among these are eﬀects of e.g. correlated hopping terms and importantly non-local exchange interactions J. Traditionally, J has been neglected based on the smallness of the exchange integral (J ∼ 1/40 eV for 3delectrons) in comparison to the on-site repulsion (U ∼ 10 eV1). However, this argumentation can be misleading. Generally, in the strong coupling case with U ≫ t exceeding the hopping t by far, J competes against the kinetic exchange given by −4t2/U, which can be orders of magnitude smaller than the original U. For synthetic quantum materials like twisted bilayer graphene, where the electron correlations emerge beyond the atomic scale, it is very unclear why any estimates made for 3d-electron materials should transfer to this case. In this paper, we advance a theoretical approach to account for interaction terms beyond the on-site Hubbard paradigm. We consider the explicit example of the Hubbard-Heisenberg model, which supplements the Hubbard model with non-local exchange terms, and analyze the interplay of on-site repulsion and non-local exchange eﬀects. This problem has so-far been addressed in two diﬀerent limits: First, SU(N) generalizations of the Hubbard-Heisenberg model have been studied in the large-N limit19–21. Secondly, the important SU(2)-case has been studied in terms of Hartree-Fock mean-ﬁeld theory (MFT)22,23. Here, we develop a variational approach to study the impact of quantum and thermal ﬂuctuations on the interplay of local moment formation with ferromagnetic and antiferromagnetic spin correlations in the SU(2) Hubbard-Heisenberg model. The paper is structured as follows: In Sec.(II A) we introduce the Hubbard-Heisenberg model. Sec. (II B) explains the generalization of the variational ap proach from9 to account for explicitly symmetry broken phases and non-local exchange interactions: We map the Hubbard-Heisenberg Hamiltonian to an auxiliary Hamiltonian, which includes a renormalized Hubbard interaction and allows for breaking of the SU(2) spin symmetry by including an eﬀective, external magnetic ﬁeld in z-direction. In Sec.(II C) we then give the computational details of the simulations of this auxiliary system performed with Determinantal Quantum Monte Carlo (DQMC). In Sec.(III A) we compare the exact solution of a 4site Hubbard-Heisenberg cluster to approximate solutions from the variational approach developed here and to MFT for benchmarking purposes. Sec.(III B) discusses the phase diagram and thermodynamic properties of the SU(2) Hubbard-Heisenberg model on a square lattice. The phase diagrams obtained with the generalized variational principle and MFT are qualitatively similar. We ﬁnd, however, that the ﬂuctuations accounted for in the generalized variational approach render the transitions between ferro- and antiferromagnetically correlated states continuous, while MFT predicts discontinuous transitions. Furthermore, we illustrate and discuss the non-monotonous inﬂuence of a small, direct exchange, on correlation functions such as the double occupancy.
Multiple stellar systems under photometric and astrometric analysis<|sep|>2 Theory 2 2.1 Binaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2.2 Principal methods for analyzing the EBs . . . . . . . . . . . . . . . . . . . 3 2.2.1 Spectroscopy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2.2 Photometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2.3 Additional techniques . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 O − C diagram analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3.1 Constant period . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3.2 Mass transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3.3 Apsidal motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3.4 Light - time eﬀect . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.4 Astrometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.5 Combining the methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.5.1 Error estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.6 Distance determination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.7 Limitations of the methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.8 Numerics and the strategy to solve the problem . . . . . . . . . . . . . . . 16 2.9 The program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3 Systems with LITE 20 3.1 Individual systems under LITE analysis . . . . . . . . . . . . . . . . . . . . 20 3.2 RY Aqr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3.3 BF CMi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.4 RW Cap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.5 TY Cap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.6 SS Cet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.7 TY Del . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.8 RR Dra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.9 TZ Eri . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.10 RV Per . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.11 UZ Sge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.12 BO Vul . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.13 Alternative explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.14 Brief summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4 Systems with combined LITE and astrometry 37 4.1 QS Aql . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.2 VW Cep . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4.2.1 Solution I. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.2.2 Solution II. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.3 ζ Phe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4.4 V505 Sgr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4.5 HT Vir . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 4.6 The problematic case: V2388 Oph . . . . . . . . . . . . . . . . . . . . . . . 58 4.7 Other systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.7.1 HD 123 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.7.2 HD 1082 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.7.3 HD 4134 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.7.4 HD 10543 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.7.5 HD 12180 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.7.6 HD 14817 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.7.7 HD 18925 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.7.8 HD 19356 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.7.9 HD 24071 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.7.10 HD 25833 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.7.11 HD 29911 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.7.12 HD 36486 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.7.13 HD 38735 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.7.14 HD 40183 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.7.15 HD 57061 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.7.16 HD 66094 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.7.17 HD 71581 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.7.18 HD 74956 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 4.7.19 AC UMa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 4.7.20 HD 82780 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 4.7.21 HD 91636 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.7.22 HD 101205 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.7.23 HD 101379j . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.7.24 HD 103483 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.7.25 HD 110317j . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.7.26 HD 114529 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.7.27 SAO 45318 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 4.7.28 HD 133640 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 4.7.29 HD 148121 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 4.7.30 HD 157482 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 4.7.31 HD 163708 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 4.7.32 HD 174932 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.7.33 HD 178125 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.7.34 HD 184242 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.7.35 HD 195434 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 4.7.36 HD 201427 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 4.7.37 HD 217675 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
Robust Domain Decomposition Preconditioners for Abstract Symmetric Positive Definite Bilinear Forms<|sep|>Symmetric positive deﬁnite operators appear in the modeling of a variety of problems from environmental and engineering sciences, e.g. heat conduction in industrial (compound) media or ﬂuid ﬂow in natural subsurfaces. Two main challenges arising in the numerical solution of these problems are (1) the problem size due to spatial scales and (2) high-contrast due to large variations in the physical problem parameters. The latter e.g. concerns disparities in the thermal conductivities of the constituents of compound media or in the permeability ﬁeld of porous media. These variations frequently range over several orders of magnitude leading to high condition numbers of the corresponding discrete systems. Besides variations in physical parameters, the discretization parameters (e.g. mesh size) also lead to large condition numbers of the respective discrete systems. Since in general high condition numbers entail poor performances of iterative solvers such as conjugate gradients (CG), the design of preconditioners addressing the size of the problem and the magnitude in variations of the problem parameters has received lots of attention in the numerical analysis community. The commonly used approaches include domain decomposition methods (cf. e.g. [20, 27, 28]) and multilevel/multigrid algorithms (cf. e.g. [3, 16, 30]). For certain classes of problems, including the scalar elliptic case, these methods are successful in making the condition number of the preconditioned system independent of the size of the problem. However, the design of preconditioners that are robust with respect to variations in physical parameters is more challenging. 2010 Mathematics Subject Classiﬁcation. 65F10, 65N20, 65N22, 65N30, 65N55. Key words and phrases. domain decomposition, robust additive Schwarz preconditioner, spectral coarse spaces, high contrast, Brinkman’s problem, multiscale problems.
Does Bankruptcy Protection Affect Asset Prices? Evidence from changes in Homestead Exemptions<|sep|>Bankruptcy is one of the largest social insurance programs in the US. Americans discharge more debt (formally and informally) than all unemployment beneﬁts combined.1 The sharp rise in personal bankruptcy from .3% of households annually in the 1980s to 1.5% in the early 2000s raised concern about strategic behavior and motivated the 2005 Bankruptcy Abuse Prevention and Consumer Protection Act (BAPCPA).2 The bill raised the costs and reduced the beneﬁts of ﬁling for bankruptcy. A large literature studies strategic bankruptcy. Pattison and Hynes (2019) found that rises in homestead exemptions are followed by a rise in chapter 7 ﬁlings by debtors with home equity. Helland, Jena, Ly, and Seabury (2016) found that in states with unlimited homestead exemptions physicians invest 13% more in their homes compared to other professionals with similar income and demographics. Additionally, the response of physicians to unlimited homestead exemptions is larger in areas with higher liability risk. While the literature has found that households prefer to own assets which give them greater protection, we study whether the market price of these assets reﬂects this protection. There are several advantages of using house prices to detect strategic behavior. First, studies that use formal bankruptcy ﬁlings do not capture a lot of strategic behavior since the majority of defaulting consumers do not ﬁle for bankruptcy, and most debt collection takes place outside of the courtroom (Dawsey, Hynes, and Ausubel (2013)). However, the settlement negotiated between creditors and debtors outside the courtroom is inﬂuenced by local exemption laws under the “threat-point” of bankruptcy.3 Second, changes in house prices in locations with a rise in homestead exemptions can help quantify forward looking strategic behavior via demand for assets that provide this implicit insurance. This study uses annual house price data in 55,316 census tracts from the FHFA combined with 139 changes in homestead exemptions between 1990-2017, collected from other authors,4 legal guide books (Elias, Renauer, and Leonard (1989)), and legal statutes.5 The identifying assumption is that changes in homestead exemptions are uncorrelated with un 1Lefgren, McIntyre, and Miller (2010); Auclert, Dobbie, and Goldsmith-Pinkham (2019); Pattison and Hynes (2019). 2Gross, Kluender, Liu, Notowidigdo, and Wang (2019); Albanesi and Nosal (2018). 3Mahoney (2015); Skeel (2003); Pattison (2017). 4We thank Mariela Dal Borgo, Richard Hynes, Paul Goldsmith-Pinkham, and Jeﬀrey Traczynski for sharing their data. 5We thank Albert Levi for helping us search statutes. observed determinants of house prices. This assumption has been conﬁrmed by a vast legal and economic literature (see Section 2) and by our own falsiﬁcation tests. We ﬁnd that the unconditional average rise in homestead exemptions raises real house prices 0.73%, an eﬀect which is positive, statistically signiﬁcant, and small. However, when the sample is restricted to Pre-BAPCPA observations (when bankruptcy was cheaper and easier), the treatment eﬀect rises to 1.07%, and Post-BAPCPA (when bankruptcy was more expensive and less beneﬁcial), the eﬀect is no longer statistically signiﬁcant. Next, when the sample is restricted to “large” changes in homestead exemptions (deﬁned to be changes greater than or equal to $50, 000) the eﬀect rises to 1.82% and is not statistically signiﬁcant for small changes. Finally, big changes Pre-BAPCPA raise house prices 3.04%, whereas small changes Pre-BAPCPA and all changes Post-BAPCPA have no statistically signiﬁcant eﬀect. Together these estimates reveal evidence of strategic behavior by households to protect their assets before BAPCPA (when bankruptcy was cheaper, easier, and more ﬁnancially beneﬁcial). These estimates also indicate that BAPCPA achieved its stated goal of reducing bankruptcy abuse. The main results are validated by dynamic regressions which ﬁnd parallel pre-trends. A heterogeneity analysis ﬁnds no eﬀect for reductions in the homestead exemption6 on house prices indicating an asymmetric eﬀect. In addition, census-tracts in Metropolitan Statistical Areas with relatively inelastic housing supply had experienced bigger eﬀects, consistent with standard theory. Moreover, tracts in counties with higher pre-treatment unemployment rates had smaller eﬀects. This doesn’t mean that households in counties with higher unemployment rates don’t value bankruptcy protection, but rather that the types of households who strategically protect their assets from creditors tend to be more prosperous. Finally, falsiﬁcation tests ﬁnd that changes in the homestead exemption don’t aﬀect levels and changes in unemployment rates, income per capita, and single family building permits. There is a small drop in population levels and homeownership rates. The drop in homeownership rates suggests that the rise in house prices caused by wealthier strategic households reduced housing aﬀordability. 6In 1993, Minnesota reduced its homestead exemption from unlimited to $200,000. This is the only negative change in the homestead exemption in our sample.
Improving Contextual Coherence in Variational Personalized and Empathetic Dialogue Agents<|sep|>The ultimate goal of open-domain dialogue agents is to engage humans in seamless, natural conversation. To achieve this goal, dialogue agents are expected to generate responses which demonstrate empathy and persona consistency. Hence, the tasks of personalized [1, 2, 3] and empathetic (or emotional) dialogue generation [4, 5, 6, 7] has received significant research attention in recent years. Traditionally, neural personalized or empathetic open-domain dialogue agents demonstrate a tendency to generate generic and repetitive responses. To address the lack of diversity in the generated responses, numerous approaches have explored the application of latent variable models, particularly the Conditional Variational Auto Encoder (CVAE), to personalized and empathetic dialogue generation. Empirical results from prior works [8, 9, 10, 11, 12, 6] have shown that CVAE-based models have largely succeeded in generating diverse dialogue responses. Despite the gains in response diversity, there is still much room for improvement, in particular, when it comes to contextual coherence. To improve the contextual coherence of the responses generated by CVAE-based agents, we propose approximating and incorporating uncertainty during the generation process. There are two types of uncertainty: aleatoric and epistemic uncertainty [13]. While aleatoric uncertainty captures the uncertainty in the input data, epistemic uncertainty quantiﬁes the uncertainty due to the model. Typically, aleatoric uncertainty is predicted as a model output, and the epistemic uncertainty is obtained via ensemble methods [14] or monte carlo simulations [15]. Aleatoric uncertainty can be further categorized as either homoscedastic or heteroscedastic. Homoscedastic, or task-dependant, uncertainty refers to input invariant noise which differs from task to task. On the other hand, heteroscedastic, or data-dependant, uncertainty quantiﬁes the uncertainty (or noise) inherent in each input and is thus unique to each input. Hence, we introduce an Uncertainty-Aware CVAE (UACVAE) framework, which involves approximating and incorporating the heteroscedastic aleatoric uncertainty via an uncertainty aware latent variable. We also propose an Utterance Entailment (UE) score that quantiﬁes the contextual coherence of an agent’s responses. Experimental results show that our framework signiﬁcantly improves overall response coherence. The remainder of this paper is organized as follows: An overview of aleatoric uncertainty approximation in CVAEbased dialogue agents is provided in Section 2. In Section 3, the UA-CVAE framework and UE score are described in detail. Experimental results and the conclusion are provided in sections 4 and 5 respectively.
Suppressing Lepton Flavour Violation in a Soft-Wall Extra Dimension<|sep|>Over the last ten years there has been a large increase in the study of extra dimensional models following the realisation that they could help explain some of the unresolved problems in the Standard Model (SM). In 1999, Randall and Sundrum showed that a warped extra dimension could oﬀer a geometric solution to the gauge hierarchy problem [1]. In the original Randall-Sundrum (RS) model, the ﬁfth dimension consists of a slice of AdS space bounded by ultraviolet (UV) and infrared (IR) branes. The warped space produces an exponential diﬀerence in energy scales between the two branes which solves the hierarchy problem. Matter ﬁelds were originally conﬁned to the IR brane, however, it was soon realised that by allowing fermions to propagate in the extra dimension, the SM fermion mass hierarchy can be explained. By varying the location of the fermion wavefunctions in the ﬁfth dimension, the full scale of fermion masses from neutrinos to the top quark can be generated using only order unity parameters [2, 3, 4]. This setup also contains a built in mechanism suppressing unobserved ﬂavour changing processes that result from couplings between SM fermions and excited gauge bosons which appear in the model [3, 5, 6]. Further interest in warped extra dimensions was generated by the AdS/CFT conjecture, when it was realised that the RS scenario is holographically dual to strongly coupled 4D ﬁeld theories [7, 8, 9]. It was in this context, studying AdS/QCD models, that the idea of a soft wall was ﬁrst introduced [10]. The soft wall is realised by removing the IR brane so the extra dimension extends to inﬁnity, and replacing it with a smoothly varying spacetime cut oﬀ. The original AdS/QCD motivation for this was to more faithfully reproduce the linear Regge-like mass squared spectrum of excited mesons as opposed to the usual quadratic spectrum found in hard wall RS models. Inspired by the possibility of qualitatively diﬀerent phenomenology, the soft wall scenario was subsequently applied to modelling electroweak physics [11, 12]. These models successfully showed that a soft wall extra dimension is generally less constrained by electroweak precision observables than its hard wall counterpart, typically allowing Kaluza Klein (KK) modes with masses of a few TeV. An important issue is related to the stability of a soft-wall setup, which is an open question in the models discussed in Refs. [11, 12]. Such a mechanism was suggested in Ref. [13], promising the soft-wall extra dimension to equally well resolve the gauge hierarchy problem. With the removal of the hard-wall brane the Standard Model matter ﬁelds must necessarily propagate in the bulk. Graviton ﬂuctuations and gauge ﬁelds were successfully analysed in this background but it was found that fermions presented particular technical diﬃculties and only a simpliﬁed single generational model was developed. Later studies of fermions in a soft-wall extra dimension have developed solutions to the fermion problem [14, 15, 16] and have considered the experimental constraints imposed by the electroweak observables. However, the fermion ﬂavour pattern of the SM has not been considered in much detail, in particular with respect to the generation of neutrino masses and the experimental bounds on lepton ﬂavour violation. In this paper we present a numerical solution to analyse a single generation of fermions in the soft-wall extra dimension. We extend this solution to three generations by treating ﬂavour mixings as perturbations to the original solutions, and apply it to the lepton sector of the SM. We construct a setup, where the lepton ﬂavour pattern is accommodated by ﬂavour dependent localisations. It is shown that in order generate small Dirac neutrino masses by this mechanism we need to introduce a hierarchy of scales of order 1015 into the model, making crucial the issue of a suitable stabilisation mechanism. We ﬁnally carry out an analysis of the constraints coming from various lepton ﬂavour violating processes, averaging over random order unity Yukawa couplings, and ﬁnd that models with only a modest hierarchy of scales are relatively mildly constrained, whereas the model with a large hierarchy allowing sub-eV neutrino masses lies well within current experimental constraints, even for a KK scale3 of 2 TeV. In the latter, ﬂavour violation is considerably suppressed relative to its hard wall counterparts, such as the ones analysed in [5, 17], and the range of masses lies in the reach of the LHC experiment. At this stage we do not try to accommodate the ﬂavour structure of the quark sector, which should be possible in a similar way. Also we reproduce the neutrino masses and mixings only at the qualitative level, which is suﬃcient to estimate the rates of lepton ﬂavour violation.
Non-linear quantum critical dynamics and fluctuation-dissipation ratios far from equilibrium<|sep|>The unusual properties observed in a growing number of strongly correlated metals that are close to a continuous zero temperature phase transitions at the brink of magnetism have resulted in attempts of addressing the critical properties within a description beyond the Ginzburg-Landau-Wilson paradigm. In the context of heavy fermion compounds, where the existence of unconventional quantum criticality has been experimentally veriﬁed, it has become clear that one of the key questions concerns the fate of the Kondo eﬀect as the lattice undergoes a magnetic transition as a function of a tuning parameter, e.g. pressure or magnetic ﬁeld, and at zero temperature [1, 2]. A particularly promising alternative to the traditional Hertz-Millis-Moriya or spin-density wave (SDW) scenario [3, 4] is local quantum criticality (LQC) where Kondo screening is critically destroyed at the quantum critical point (QCP) [1, 5]. Within LQC, the QCP is characterized by a scale-invariant spectrum that results in dynamical or ω/T-scaling of correlation functions. This is in line with a growing number of experimental results [5]. The study of correlated matter, including the heavy fermion systems, has undergone a considerable evolution over the past decades that led from the measurements of thermodynamic quantities like e.g. speciﬁc heat to modern microscopic probes of dynamic correlation functions. Even for heavy fermion compounds which are dominated by an enhanced density of states in close vicinity of the Fermi energy and concomitantly small characteristic energy scales, these spectroscopic probes, e.g. angle-resolved photoemission spectroscopy (ARPES) [6, 7] and scanning tunneling spectroscopy (STM) [8, 9, 10] have recently become available. Thus, ARPES, STM, inelastic neutron scattering, etc. are capable of determining dynamic correlation functions and their scaling properties near quantum criticality. The ﬂuctuation-dissipation theorem (FDT), valid in the linear response regime, connects the measured response functions to the correlation functions of the system. For a quantum critical system, possessing a scale-invariant spectrum, i.e. in the absence of any intrinsic scale, any perturbation may probe the system in the non-linear regime beyond the validity of the FDT [11]. Understanding the properties of quantum criticality out of equilibrium has accordingly become a topical issue. This applies to both its theoretical description as well as its experimental signatures. Here, we address the concept of eﬀective temperatures in the context of quantum critical steady states far from equilibrium, both for the SDW as well as for a model of critical Kondo destruction, and present an analysis of a noninteracting, critical reference system, the pseudogap resonant level model, in terms of eﬀective temperatures and contrast these results with those obtained near fully interacting quantum critical points.
Acquisition-invariant brain MRI segmentation with informative uncertainties<|sep|>The substantial soft-tissue contrast of MRI makes it the tool of choice in a myriad of applications, especially in the ﬁeld of are deliberately chosen depending on the task at hand. There is therefore a high demand for algorithms that can process such
Equilibrium-charge diagram of single quantum dot in an axial magnetic field<|sep|>One quantum dot and two coupled quantum dots containing two conduction electrons deﬁne two-dimensional analogies to the H atom and the H2 molecule. Such devices may serve as building blocks for future quantum processors1,2,3,4. Recent advances in experimental techniques have made it possible to study quantum dots in the few-electron regime when quantum dots contain only one or two conduction electrons5,6. Based on the exact-diagonalization method, it was shown that the twoelectron ground state exhibits a phase transition from a singlet to a triplet state due to interactions7,8. The electronic structure of a laterally coupled two-electron quantum dot molecule is computed for diﬀerent conﬁnement strengths and for varying inter-dot separations in external magnetic ﬁelds9. The stability diagram of a oneand two-electron quantum dot was calculated for larger inter-dot separations in a related exponential double-well potential10. In theses studies, the number of electrons in quantum dots and whether tunnel resonance occurs are determined by combination between the chemical potentials of dots and leads, and the chemical potentials of quantum dots can be adjusted by modulating gate voltages. Meanwhile, external magnetic ﬁelds are frequently applied to control the electronic states and spins of quantum dots7, but whether the number of electrons in quantum dots is changed unexpectedly by magnetic ﬁelds has been studied to a lesser extent. To analyze the dependence of chemical potentials on magnetic ﬁelds allows one to estimate the eﬀect of magnetic ﬁelds on the number of electrons in quantum dots. In the paper, we numerically compute the chemical potential of single quantum dot parabolically conﬁning one or two electrons in an axial magnetic ﬁeld. One- and twoelectron Schr¨odinger equations are solved by exact diagonalization to obtain the chemical potentials as functions of the magnetic ﬁeld. The number of electrons in the dot is given by combination between the chemical potential of the dot and leads. The equilibrium-charge diagram of the dot is obtained in the phase space determined by the magnetic ﬁeld and the lead voltage. The dependence of the electron number on the magnetic ﬁelds and the lead voltage is clear at a glance in the equilibrium-charge diagram. Meanwhile, the diagram is a compact tool to show the condition of the electron transport. The result helps to control the electron number of single quantum dot in external magnetic ﬁeld and to use single quantum dot as a magnetic electron-transport switch.
Studying stellar halos with future facilities<|sep|>Nowadays there is plenty of evidence for the presence of streams and substructures in the halo of galaxies, as is expected in the hierarchical models of galaxy formation (e.g. Atkinson, Abraham, & Ferguson 2013). These features, detected down to very low surface brightness (µV ∼ 29 mag/arcsec2), are hard to measure against the sky background. Even more diﬃcult is characterizing their stellar populations, estimating total magnitude and color, which trace respectively the mass and the age/metallicity of their stars. In recent years galaxy formation models in a cosmological context have become very sophisticated, yielding detailed predictions about the structure of stellar halos, the amount of substructure, the shapes of shells and streams, the characteristics of the stellar populations in the diﬀerent components, and more (see Johnston et al. 2008; Font et al. 2011; Cooper et al. 2013; Pillepich et al. 2014). The predictions depend on the techniques adopted to construct the models (e.g. whether full hydrodynamical simulations, or n-body models with particle tagging), as well as on parameters describing the physical processes which occur in the troubled galaxy life (e.g. star formation (SF), the initial mass function, the feedback). Although the ubiquitous presence of substructures in galaxy halos qualitatively supports these models, we need to perform a quantitative comparison between predictions and observations, especially to constrain the models’ parameters. This can be done in diﬀerent ways, among which (i) analyzing the demographics of substructures of diﬀerent types and the properties of their stellar populations; (ii) comparing the stellar density proﬁle, checking for the presence and extent of an in situ component; (iii) measuring the metallicity gradient of stellar halos. For example, halos completely built with stars shed by accreted companions will hardly show a metallicity gradient, while according to full hydrodynamical simulations, a sizable metallicity diﬀerence between the outer and the inner halo should exist, since the latter hosts heated disk stars. These kind of studies have been done only for nearby galaxies (Deason, Belorukov & Evans
Sharing pattern submodels for prediction with missing values<|sep|>Machine learning models are often used in settings where model inputs are partially missing either during training or at the time of prediction (Rubin, 1976; Schafer and Graham, 2002). If not handled appropriately, missing values can lead to increased bias or to models that are unapplicable in deployment (Liu et al., 2020; Le Morvan et al., 2020) without imputing the values of unobserved variables. When missingness is dependent on unobserved factors that are related also to the prediction target, the fact that a variable is unmeasured can itself be predictive— so-called informative missingness (Rubin, 1976; Marlin, 2008). This is particularly prevalent in healthcare applications (Janssen et al., 2009; Che et al., 2018). In such cases, imputation of missing values is often insuﬃcient, and it can be beneﬁcial to let models make predictions based on both the partially observed data and on indicators for which variables are missing (Jones, 1996; Groenwold et al., 2012). As mentioned in Morvan et al. (2020), even the linear model—the simplest of all regression models—has not yet been thoroughly investigated with missing values and still reveals unexpected challenges. Pattern missingness emerges in data generating processes where there are structural reasons for which variables are measured—samples are grouped by recurring patterns of measured and missing variables (Little, 1993). In Figure 1, we illustrate an example of this when observing patients from three diﬀerent clinics, each systematically collecting slightly diﬀerent measurements. Pattern submodels have been proposed for this setting, ﬁtting a separate model to samples from each pattern (Fletcher Mercaldo and Blume, 2020; Marshall et al., 2002). This solution Figure 1: Coeﬃcient sharing between a main model θ and pattern submodels for three clinics with diﬀerent patterns in missing values. Without specialization ∆m, an average prediction shared by clinics with diﬀerent patterns may not lead to an optimal solution for any of them. Conversely, ﬁtting separate models for each clinic does not use all of the available data eﬃciently and leads to high variance. does not rely on imputation and can improve interpretability over black-box methods (Rudin, 2019), but can suﬀer from high variance, especially when the number of distinct patterns is large and the number of samples for a given pattern is small. Moreover, if the ﬁtted models diﬀer signiﬁcantly between patterns, it may be hard to compare or sanity-check their predictions. Notably, pattern submodels disregard the fact that the prediction task is shared between each pattern. We propose the sharing pattern submodel (SPSM) in which submodels for different missingness patterns share coeﬃcients while allowed limited specialization. This encourages eﬃcient use of information across submodels leading to a beneﬁcial tradeoﬀ between predictive power and variance in the case where similar submodels are desired and sample sizes per pattern are small. Models with few and small diﬀerences between patterns are easier to describe and interpret by domain experts. We describe SPSM and present a running example to illustrate our idea in Section 3. Next, we prove that in linear-Gaussian data generating processes, coeﬃcient sharing does not introduce additional bias—even when the prediction target depends on partially missing variables and on the missingness pattern (Section 4). Finally, we ﬁnd in an experimental evaluation on real-world and synthetic data that SPSM compares favorably to diverse baseline classiﬁers and regression methods, paying particular attention to how SPSM improves sample eﬃciency in learning over nonsharing pattern submodels (Section 5).
Slipping magnetic reconnections with multiple flare ribbons during an X-class solar flare<|sep|>Solar ﬂares are most energetic magnetic explosions in the solar activities. They can increase the emission in a broad range of the electromagnetic spectrum, from radio wavelengths to X- and γ-rays (Fletcher et al. 2011). In the standard solar ﬂare model, i.e. the CSHKP model (named after Carmichael 1964; Sturrock 1966; Hirayama 1974; Kopp & Pneuman 1976), the erupting ﬂux rope stretched magnetic ﬁled lines to induce the magnetic reconnection; due to the successive reconnections, the ﬂare loops (FLs; originally named as post-ﬂare loops) formed and straddled the magnetic polarity inversion line, and their footpoints are heated by the energy transport from the reconnection site to appear as ﬂare ribbons (FRs). However, the standard ﬂare model is basically two-dimensional, and it remains deﬁcient to explain many inherent three-dimensional (3D) observational features, such as the formation of coronal sigmoids (Aulanier et al. 2010; Green et al. 2011; Savcheva et al. 2015), the erupting ﬂux rope (Zhang et al. 2012), the moving bright emissions along the FRs (Fletcher & Hudson 2002; del Zanna et al. 2006; Chandra et al. 2009), and the strong-to-weak shear transition in FLs (Aulanier et al. 2012). Recently, Aulanier et al. (2012) and Janvier et al. (2013) proposed a fully 3D ﬂare model that incorporates the standard 2D one in one of its cuts, thus satisfying the principle of correspondence. In the 3D magnetohydrodynamic simulation, the magnetic ﬁeld lines passed through the quasi-separatrix layers (QSLs; Priest & D´emoulin 1995), and could undergo a successive reconnection, which exchanged their connectivity with neighboring ﬁeld lines; the continuous reconnections resulted in the obvious slipping motion along the QSLs (Pontin et al. 2005; De Moortel & Galsgaard 2006; Aulanier et al. 2006). Therefore, it is classiﬁed as the slipping magnetic reconnection. As the theoretical models have been developed, some observational cases of the slipping magnetic reconnection have been analysed. Aulanier et al. (2007) ﬁrst presented the direct observations of slipping magnetic reconnection in coronal loops by the X-ray Telescope onboard Hinode. Based on the Atmospheric Imaging Assembly (AIA; Lemen et al. 2012) on the Solar Dynamics Observatory (SDO; Pesnell et al. 2012), Dud´ık et al. (2014) reported the apparent slipping motion of FLs during an eruptive X1.4 ﬂare, and Li & Zhang (2014, 2015) showed two examples of slipping motion of FLs and the quasi-periodic pattern of the latter case. However, the previous observations only focused on the slipping motion in two FRs, and the slippage in multiple FRs with complex magnetic conﬁguration has never been discussed. In this Letter, we present the slipping magnetic reconnections in multiple FRs during an X1.2 eruptive ﬂare.
Robust Unit Commitment Considering Strategic Wind Generation Curtailment<|sep|>many challenges to power system, especially in the scope of  operation. On one hand, wind generation cannot be forecasted  accurately. According to [1], the day-ahead wind generation  forecast error can be 20% or larger. On the other hand, wind  generation is traditionally treated as non-dispatchable, which  means it is fully absorbed unless there are security issues. To  cope with these tough characters of wind generation, operation  strategies with more flexibility are desiderated. One of the  focuses is Unit commitment (UC) as it determines the  operational flexibility of the following day. recently as it can guarantee the operational security given a  prescribed wind generation uncertainty set, which is determined  by the operational reliability requirements (ORR) of power This work is supported in part by the Foundation for Innovative Research  Groups of the NNSF of China (51321005), and State Key Development  Program of Basic Research of China (2013CB228201). systems [2], [3]. To meet ORR, expensive fast-response  generators are dispatched more frequently under RUC to  provide sufficient regulation capability, which significantly  increases the operational cost of RUC. Numerous models and  approaches have been proposed to reduce the operational cost  of RUC, such as minimax regret RUC [4], unified stochastic  and robust UC [5], and hybrid stochastic/ interval RUC [6].  However, these approaches mainly focus on exploring  operational flexibility from conventional generators. flexibility of power systems. In [7], wind farms are treated as a  reserve provider by decreasing its output and the system  dispatch cost is decreased because of the reduction of reserve  requirement. Wind farms can also benefit from this practice as  its uncommitted power can be used to provide ramping-up  reserve. The incentives of wind farms of providing reserve  service in the perspective of electricity market are analyzed in  [8], [9]. Indeed, decreasing the output of wind farms, also called  wind generation curtailment (WGC), is a useful method to  recover operational feasibility. strategic WGC is proposed, aiming to reduce the day-ahead  dispatch costs. The schematic diagram of strategic WGC is  shown in Fig. 1. The blue lines represent forecasted value of  wind generation 𝑤̂𝑡and the red lines represent the committed  wind generation 𝑤̂𝑡 according to operational reliability requirement of power  systems. Intuitively, strategic WGC occurs in period t+1. This  practice may benefit the power system from Four aspects: (1)  Reducing ramping requirement. After strategic WGC, the  maximum ramping requirement of wind generation between  period t and t+1 decreases from 𝑅𝑡 ramping up capability. The ramping up capacity provided by  wind generation between period t+1 and t+2 increases  from  𝑅̂𝑡+1 to  𝑅̂𝑡+1 transmission lines. As the uncertainty band becomes narrower  compared with original one in period t+1, transmission lines  can leave less space for possible over-generation of wind  generation. (4) Enhancing solvability of RUC. In traditional  RUC, robust dispatch strategy may not be generated, especially  under critical operational reliability requirement. With strategic  WGC, the solvability of RUC can always be guaranteed. Meanwhile, wind farms can also benefit from strategic WGC:  on one hand, it can charge power system for providing ramping  up capacity; on the other hand, it can use the uncommitted or  the “curtailed” wind generation to provide auxiliary services.  Detailed discussion can be found in Section IV. C. describes the mathematical formulation of RUC with strategic  WGC. Section III presents the solution methodology. In Section  IV, illustrative examples on different test systems are  proceeded to demonstrate the effectiveness of the proposed  model and algorithm. Finally, Section V gives the conclusion  of the paper.
Insight into bias in time-stratified case-crossover studies<|sep|>Since the initial work by Malcolm Maclure in 1991 [1], the use of case-crossover designs has become widespread in epidemiological and medical investigations of transient associations between risk factors and adverse health events [2–4], notably in the area of ambient air pollution research [5–13]. With a case-crossover design, an investigator samples only cases and compares each individual’s exposure during a short time period (hazard period) just before onset of a case event with exposures at other times (referent periods) in a referent window [14]. Comparing with general case-control study designs and cohort study designs, these self-matching designs have an obvious advantage of controlling by design all time-invariant confounders, either measured or unmeasured. The challenge to the case-only design comes from how to control time-varying factors. Being similar to a cohort or a case-control study, a confounding issue in a self-matching design arises when there exists unbalanced matching in determinants between hazard periods and referent periods, leading to various sources of systematic bias. These sources were systematically reviewed by Mittleman and Mostofsky [15]. To control time-varying factors, various reference-select strategies have been proposed in literature. Janes et al. [2] provided a comprehensive review of these strategies and assessed potential bias associated with them. These strategies ∗Corresponding author. Health Services Statistical and Analytic Methods, Alberta Health Services, #7235, 2nd Floor, West Wing, Aberhart Centre, 11402 University Avenue, Edmonton, AB T6G 2J3. aim to limit the reference-select window to a short period that restricts time-varying confounders to be nearly constant across reference-select windows [14]. The ambi-directional [16] and the time-stratiﬁed [17], were among the top two reference-select strategies in literature. And the latter was claimed to be unbiased [2] and optimal [18], avoiding bias resulting from time trend in the exposure series as well as speciﬁc time-varying confounders. As pointed by Wang et al. [4] in a case-crossover study, there are two kinds of biases. One is point estimate bias from imperfectly controlling of time-varying factors (confounding issue). The other is the estimation bias of standard error (SE), the so-called overlap bias [19–21] from correlations among observations. We could reduce bias in point estimates by using a shorter reference-select window which results in higher overlap bias because shorter referenceselect window results in higher correlation among cases and their matched references. In that paper, the authors pointed out the time-stratiﬁed case-crossover design is by no means a ﬁnal solution to the overlap bias. As a remedy, a calibration strategy based on permutation [22–25] was proposed to overcome the overlap bias issue. One pitfall of this strategy is its computational burden. In this paper, we were seeking a better solution to the overlap bias in case-crossover studies. For this purpose, we scrutinized the ability of the time-stratiﬁed schema on controlling time-varying factors and found its failure on controlling weekly time trend. Based on this ﬁnding, we further propose a logistic regression model in which we do adjustment for weekly time trend. As we hypothesized, this model can effectively eliminate the overlap bias as well as potential bias in point estimate. This implies that the overlap bias issue intrinsically roots from unbalanced matching between hazard periods and referent periods of the time-series under investigate. It allows us to have a deep insight into the collection between bias in point estimate and bias in SE estimate.
Formulating an $n$-person noncooperative game as a tensor complementarity problem<|sep|>The n-person noncooperative game plays a fundamental yet important role in the development of game theory [3, 36]. Nash [34, 35] proposed a very important concept of equilibrium, called Nash equilibrium, for n-person noncooperative games, which is a stable outcome in the sense that a unilateral deviation from a Nash equilibrium point by one of the players does not increase the payoﬀ of that player. Nash [34, 35] has shown that every game of this kind has at least one equilibrium point in mixed strategies. The n-person noncooperative game and its various extensions have been extensively studied, for example, see [3, 4, 11, 16, 20, 22, 27, 30, 33, 45, 46] and references therein. A large number of economic models are formulated in terms of some n-person noncooperative game [1, 13]. In these applications, one of main concerns is how to ﬁnd eﬀectively a Nash equilibrium point, which depends heavily on the good mathematical description of the problem. For the two-person noncooperative game, one of the most popular models is the bimatrix game [18, 29], where the utility function of every player is a quadratic form deﬁned by the payoﬀ matrix of that player. It is well known that the bimatrix game can be reformulated as a linear complementarity problem [9, 29, 32]. The ﬁrst approach for ﬁnding Nash equilibrium point of a two-person game was proposed in [29], which was designed based on the reformulated linear complementarity problem of the concerned game. The polymatrix game is an important subclass of n-person noncooperative games, where the payoﬀ of one player relative to the decisions of any other player is independent of the remaining players’ choices [21]. The utility function of each player is the sum of n−1 quadratic forms where every quadratic form is deﬁned by the payoﬀ matrix of this player with respect to any other player. Obviously, the polymatrix game is an extension of the bimatrix game. It is well known that the polymatrix game can be reformulated as a linear complementarity problem [19, 21]. Recently, Song and Qi [39] introduced a class of complementarity problems, called tensor complementarity problems, where the involved function is deﬁned by some homogeneous polynomial of degree n with n > 2. It is known that the tensor complementarity problem is a generalization of the linear complementarity problem [9]; and a subclass of nonlinear complementarity problems [12, 19]. The tensor complementarity problem was studied recently by many scholars [2, 6, 10, 17, 25, 31, 40, 41, 42, 43, 44]. In this paper, we consider a class of n-person noncooperative games, where the utility function of every player is a homogeneous polynomial of degree n deﬁned by the payoﬀ tensor of that player. The new model is a natural extension of the bimatrix game; and we call it the multilinear game in this paper. We will reformulate the multilinear game as a tensor complementarity problem. We show that ﬁnding a Nash equilibrium point of the multilinear game is equivalent to ﬁnding a solution of the resulted tensor complementarity problem; and especially, we exhibit an explicit corresponding relation between the solutions of these two classes of problems. In addition, we also apply a smoothing-type algorithm to solve the resulted tensor complementarity problem and give some preliminary numerical results for solving some multilinear games.
V1460 Her: A fast spinning white dwarf accreting from an evolved donor star<|sep|>Cataclysmic variable stars (CVs) contain white dwarf primary stars usually accreting from hydrogen-rich mainsequence-like secondaries. In most cases the accretion occurs via a disc and they are the closest and most easily observed examples of accretion onto compact objects. As CVs evolve, their secondary stars continually lose mass and therefore need to adjust their structures to maintain thermal equilibrium. Whether they manage to do so depends upon the thermal timescale of the secondary star compared to the mass loss timescale, M2/− �M2, where M2 is the mass of the secondary star. Both timescales tend to lengthen as orbital periods and secondary star masses decline, but at very low masses (< 0.1 M⊙) and correspondingly short periods (∼ 80 min), the battle to maintain thermal equilibrium is well and truly lost resulting in a minimum orbital period for systems with hydrogen-rich donors, (Paczynski & Sienkiewicz 1981; G¨ansicke et al. 2009). Until this point however, we expect the secondary stars to be fairly close to thermal equilibrium (although departure from thermal equilibrium is thought to be partly responsible for the paucity of systems between 2 and 3 hours known as the period gap). The maintenance of near thermal equilibrium while losing mass, and the one-to-one relationship between orbital period and the mean density of Roche-lobe ﬁlling stars (Faulkner
Future-Aware Diverse Trends Framework for Recommendation<|sep|>Recommender systems assume a central part of many real-world applications (e.g., e-commerce platforms [43, 57, 58]) with the prevalence of the Internet and information technology. On online platforms, users interact with a series of items in a chronological order, implying continuous and temporary correlation between each item. In this scenario, the sequential recommenders have become indispensable techniques in the recommendation area, which aim at predicting the next item that the user may interact with by modeling users’ preferences on the basis of sequential dependencies among the users’ historical interactions. Existing sequential recommendation algorithms model and represent user preferences in various manners. Most conventional models such as Markov chain-based [24, 25, 38] and factorizationbased [31, 39, 60] ones have successfully captured users’ short-term and long-term interests by adopting Markov chains and matrix factorization respectively, but either failed to model intricate dynamics or ignored the time dependency. In contrast, deep learning based techniques typically represent user preferences with lowdimensional embedding vectors. For instance, the deep neural network proposed for YouTube video recommendation(YouTubeDNN) [11] represents each user by one fixed-length vector transformed from the past practices of users, while not appropriate for modeling various interests because of the dimension explosion. To tackle this issue, Deep Interest Network (DIN)[61] makes the user representation vary over different items with attention mechanisms to capture the diversity of user interests. More recent works [8, 32] propose to encode users’ historical behaviors into users’ varying interests by leveraging capsule routing mechanism. Nevertheless, all of these methods model user preferences only taking into account the past behaviors of users, ignoring the potential future preference and failing to capture the time-evolving trends of user diversified preferences. We argue that preferences changing over time of similar users is an extra important factor to model future diverse preference trends in addition to historical preference. Such trends can be summarized from the relative future behaviors of users with similar interests. Specifically, for an inspected user, other users with similar interests tends to have common behaviors (e.g. click the same items) with the inspected user, and the behaviors that happen after the common behavior can be viewed as the relative future behaviors. As shown in 1, models that solely focus on users’ historical interests tend to recommend similar and complementary items. In contrast, the proposed future-aware diverse trends framework is capable of recommending fresh items that may seem irrelevant to the user’s historical preference, while in reality are consistent with one of the preference trends that can be captured from the future behaviors of users’ with similar interest (or at least similar behaviors). In this paper, we focus on the problem of modeling diverse trends of users for sequential recommendation. In order to overcome the limitations of existing methods, we propose the future-aware diverse trends (FAT) framework for learning user representations that reflect diverse trends of users preferences. To infer the user representation vectors, we design an implicit neighbor behaviors extractor(INBE) layer and a novel diverse trends capture layer. To construct neighborhoods implicitly, the INBE module utilizes Pearson Correlation Coefficient [6] and an interaction-based users filter. The diverse trends capture layer applies dynamic routing and timeaware mechanism to adaptively aggregate neighbor user’s relative future behaviors as user trend representation. The user representation is then computed by concatenating the user historical behaviors embedding from traditional sequence modeling and the user trends embedding. The process of dynamic routing can be viewed as soft-clustering, which groups similar users’ relative future behaviors into several clusters. Each cluster of future behaviors is further used to infer the user trend representation vector according to the time-varying attention of each trend corresponding to the specific items. In this way, for a particular user, FAT outputs the final user preference representations considering both the user past preference and potential future preference. To summarize, the main contributions of this work are as follows: • To better infer the dynamics of user behaviors, we design a FAT framework, which leverage the future information and capture diverse trends of user preference. • We first design a neighbor behavior module to extract relative future behaviors from similar users implicitly. We design the diverse trends capture module, which utilizes dynamic routing to adaptively aggregate neighbor’s future behaviors into trend representation vectors. We then leverage timeaware mechanism over trends to better model time-varying user potential preferences. • Compared with existing methods, FAT shows superior performance on several public datasets over metrics such as Recall and NDCG. In addition, we conduct experiments to show that FAT can bring diversity of retrieved items better than other baselines. The remainder of this paper is organized as follows: related works are reviewed in Section 2; Section 3 formulates the sequential recommendation task and elaborates the technical details of FAT; In Section 4, we detail the experiments for comparing FAT with existing methods on several public benchmarks; The last section gives conclusion and future work of this paper.
Learning for Detecting Norm Violation in Online Communities<|sep|>The aligned understanding of a norm is an essential process for the interaction between diﬀerent agents (human or artiﬁcial) in normative systems. Mainly because these systems take into consideration norms as the basis to specify and regulate the relevant behavior of the interacting agents [9]. This is especially important when we consider online communities in which diﬀerent people with diverse proﬁles are easily connected with each other. In these cases, misunderstandings about the community norms may lead to interactions being unsuccessful. Thus, the goals of this research are: 1) to investigate the challenges associated with detecting when a norm is being violated by a certain member, usually due to a misunderstanding of the norm; and 2) to inform this member about the features of their action that triggered the violation, allowing the member to change their action to be in accordance with the understanding of the community, thus helping the interactions to keep running smoothly. To tackle these goals, our main contribution is to provide a framework capable of detecting norm violation and informing the violator of why their action triggered a violation detection. The proposed framework is using data, that belongs to a speciﬁc community, to train a Machine Learning (ML) model that can detect norm violation. We
Few-shot Generation of Personalized Neural Surrogates for Cardiac Simulation via Bayesian Meta-Learning<|sep|>Personalized virtual hearts, customized to the observational data of individual subjects, have shown promise in clinical tasks such as treatment planning [25]
Fragmentation of star-forming filaments in the X-shape Nebula of the California molecular cloud<|sep|>The early phases of star formation are still poorly understood. One of the most pressing open questions is how ﬁlaments fragment into cores at the earliest evolutionary stages. Addressing this issue is crucial to our understanding of the initial conditions of star formation (cf. André et al. 2010). Observations with the Herschel space observatory have revealed that ﬁlaments are truly ubiquitous in the cold interstellar medium (ISM), with lengths ranging from ∼ pc in nearby molecular clouds to ∼ 102 pc in the Galactic plane (Men’shchikov et al. 2010; Wang et al. 2015). Most ( > 75 %) prestellar cores lie inside ﬁlaments with column densities Nﬁl H2 ≳ 7 × 1021 cm−2 (Könyves et al. 2015), implying that ﬁlaments play a key role in the star formation process (André et al. 2014). The typical inner width of ﬁlaments in nearby molecular clouds is ∼ 0.1 pc (Arzoumanian et al. 2011, 2019), but the origin of this charac teristic width is still a controversial topic. The typical value may come from supersonic turbulence in the ISM (Pudritz & Kevlahan 2013; Federrath 2016), or the balance of quasi-equilibrium structure with ambient ISM pressure (Fischera & Martin 2012). Filaments serve as highly eﬃcient routes for feeding material into star-forming cores (André et al. 2014). The material around ﬁlaments is not at all static. For instance, a pronounced transverse velocity gradient provides good kinematic evidence for accretion ﬂows of ambient gas material into the B211/B213 ﬁlament of the Taurus molecular cloud (MC) (Palmeirim et al. 2013; Shimajiri et al. 2019b). Other dense ﬁlaments such as infrared dark clouds or the Serpens South ﬁlament exhibit both transverse and longitudinal velocity gradients, suggesting that, gas is not only accreted by, but also ﬂowing along these ﬁlaments (e.g. Kirk et al. 2013). In most cases, the transverse gradients appear to dominate over the longitudinal gradients (Dhabal et al. 2018). In the Serpens South case, for instance, the mass ﬂow rate along the ﬁlament is estimated to be only ∼1/4 of the accretion rate in the transverse direction (Kirk et al. 2013). The critical line-mass of a cylindrical isothermal ﬁlament in hydrostatic equilibrium is Mline,crit ≡ 2 c2 s/G, where cs is the sound speed and G the gravitational constant, or ∼16 M⊙ pc−1 for a gas temperature of 10 K (e.g. Ostriker 1964; Inutsuka & Miyama 1997). Recently, Arzoumanian et al. (2019) divided observed ﬁlaments into three families according their line-mass Mline: thermally supercritical ﬁlaments (Mline ≳ 2 Mline,crit), transcritical ﬁlaments (0.5 Mline,crit ≲ Mline ≲ 2 Mline,crit), thermally subcritical ﬁlaments (Mline,crit ≲ 0.5 Mline,crit). For an inﬁnitely long cylindrical ﬁlament in hydrostatic equilibrium, core spacing is predicted to be ∼ 4 × the ﬁlament diameter by linear fragmentation models (e.g. Inutsuka & Miyama 1992), or ∼ 0.4 pc taking the typical ﬁlament width into account. However, this does not match the actual core spacing found in observations which, at least on small scales, appears to be dominated by typical Jeans-like fragmentation (e.g. Kainulainen et al. 2013, 2017; Könyves et al. 2020; Ladjelate et al. 2020). Here, based on our ﬁndings in the California MC, we propose that this discrepancy results from the fact that star-forming ﬁlaments are not isolated but accrete fresh matter from their parent clouds at the same time as they fragment into cores. Star formation activity is generally found only in highextinction parts of MCs, but the mass of high-extinction material with AK > 1.0 mag in the California MC is only 10% of that in OrionA MC (Lada et al. 2009), and the star formation rate is accordingly much lower. The number of young stellar objects (YSOs), which may be taken as an indicator of star-formation activity, is only 177/2980∼ 6% of that observed in the Orion A MC (Lada et al. 2017; Großschedl et al. 2019). Only one B-type main-sequence star is found in the south-eastern part of the California MC and is associated with relatively intense star formation in a local region around it (Andrews & Wolk 2008). The global star formation eﬃciency estimated by Zhang et al. (2018) for the California MC is only ∼ 1%, which is half of the typical value (∼ 2%) in the molecular clouds of the Milky Way (Evans 1991). The California MC is therefore an ideal place to study star formation at early stages. The X-shape region lies at the center of the California MC (see Fig. 1). The distance to the California MC as estimated from Gaia DR2 (Gaia Collaboration et al. 2018) stellar parallaxes is 500 ± 7 pc according to Yan et al. (2019) and 470 ± 2 ± 24 pc according to Zucker et al. (2019), which is slightly farther than both the distance (450 ± 23 pc) estimated by Lada et al. (2009) through comparison of foreground star counts with Galactic models and the distance (410 ± 41 pc) estimated by Schlaﬂy et al. (2014) with stars from the PanSTARRS-1 survey. Diﬀerent distance measurement methods bring uncertainties of about 10 % in size and 20 % in mass for the California MC. In this study, we adopt a distance of 500 pc. The most distinctive feature of this region is that it resembles an ’X’. Two low-density ﬁlaments meet at the north dense hub region and extend to the southeast and southwest with an intersection angle of ∼ 60◦ in the plane of sky. Small longitudinal velocity gradients of 0.1 and 0.2 km s−1 pc−1 are measured along the southeast and southwest ﬁlaments (Imara et al. 2017). The hub region harbors at least two YSOs. One is a Class II object and the other is a Class I source (Harvey et al. 2013; Broekhoven-Fiene et al. 2014). These two YSOs may be the driving sources of a low-mass low-velocity outﬂow (Imara et al. 2017). The outline of the present paper is as follows. In Sect. 2, we describe the Herschel submillimeter dust emission data and SMT 10m molecular line observations of the X-shape region. Fig. 1. Large-scale column density map of the California MC derived from Planck 850 µm optical-depth data (5′ resolution). The location of the X-shape Nebula region is marked by black square. Data analysis and results are presented in Sect. 3. In Sect. 4, we discuss evidence of accretion onto an early-stage ﬁlament, as well as the detailed fragmentation properties of the same ﬁlament. We summarize our conclusions in Sect. 5.
Unit Disk Cover Problem<|sep|>In the unit disk cover (UDC) problem, we consider two problems, namely the discrete unit disk cover (DUDC) problem and the rectangular region cover (RRC) problem. In the DUDC problem, given a set P = {p1, p2, . . . , pn} of n points and a set D = {d1, d2, . . . , dm} of m unit disks in the Euclidean plane, we wish to determine the minimum cardinality set D∗ ⊆ D such that P ∩ D∗ = P. In the rectangular region cover (RRC) problem, given a rectangular region R and a set D = {d1, d2, . . . , dm} of m unit disks in the Euclidean plane, the objective is to determine the minimum cardinality set D∗∗ ⊆ D such that R ∩ D∗∗ = R. The DUDC and RRC problems are a geometric version of the general set cover problem which is known to be NP-complete [14]. The general set cover problem is not approximable within c log n, for some constant c, where n is the size of input. Unfortunately, both the DUDC and RRC problems are also NP-complete [14], but unlike general set cover problem, DUDC and RRC problems admit a constant factor approximation results. These two problems have been studied extensively due to their wide applications in wireless networks [6, 11, 19].
Cavity approach to sphere packing in Hamming space<|sep|>The problem of packing rigid objects, and spheres in particular, is a fundamental problem which appears across disciplines [1, 2]. In general, the objects could have arbitrary shapes and the ambient space can be an abstract space Λ. Given the space and the objects, the main question is that of ﬁnding the densest packings. In coding theory one is interested in ﬁnding an optimal representation of N symbols in binary strings of length n, that is Λ = {0, 1}n is the Hamming space of dimension n. This optimal coding contains as many as possible symbols and ensures that after transmitting through a noisy channel, which at most ﬂips d−1 2 variables, one can recover the original messages. The ratio between the characteristic length of the symbols lc ≡ log2 N and length of the transmitted strings n deﬁnes the rate of coding (or packing). The maximum rate of coding is denoted by R. Indeed people are interested to know the asymptotic form of R when n, d → ∞ and δ ≡ d/n remains constant. So far there is a considerable diﬀerence between the best lower and upper bounds for R [3–10]. This means that for large n the best lower and upper bounds for the number of symbols diﬀer by a factor of order 2n. Physically, one can consider the set of symbols as a system of identical particles in the Hamming space of dimension n, interacting by a hard core potential of range d [11, 12]. The aim is then to study the physical states of diﬀerent densities. Clearly for small densities the system is in the liquid phase respecting the translational symmetry. In this case it is easy to calculate, for example, the entropy. At higher densities the liquid entropy becomes incorrect (negative) signaling the onset of other stable phases, either crystalline or glassy [13, 14]. In this study we formulate the packing problem as a constraint satisfaction problem. We consider N variables (the physical particles or the strings of symbols) which take values in Λ and for each pair of the variables we consider a constraint that forbids overlapping assignments of the two variables. A packing is thus an assignment of the variables that satisﬁes all the constraints. This representation diﬀers substantially with the so called lattice gas models where binary variables (representing occupied or empty positions) are deﬁned one each point of the space Λ. The cavity method provides analytical and numerical tools which can be extremely useful in solving optimization problems (or constraint satisfaction problems) over random structures [15–19]. In certain cases it is known to provide sampling results which cannot be obtained by Monte Carlo Markov chains in subexponential times [e.g. optimization problems in the one-step replica symmetry breaking (1RSB) phase]. As an optimization tool it often outperforms linear programming methods [20]. The development of such algorithm is, however, by no means obvious due to the choice of the representation of the problem and to the need of writing the cavity equations in an algorithmically eﬃcient form. As we shall discuss in this paper, insights from the application of the cavity method will turn out to be also useful in the study of the type of packing problems we are interested in. In this paper we study the cavity equations for the packing problem in the replica symmetric (RS) and in the one-step replica symmetry breaking (1RSB) approximations [19]. These equations are called belief propagation (BP) and survey propagation (SP) equations, respectively [17, 21]. In the RS approximation, besides a liquid solution we ﬁnd a crystalline phase where with higher probability spheres are found in one of the sublattices (even or odd) of the Hamming space. This phase has already been observed in Monte Carlo simulations of Ref. [14]. Both the liquid and crystalline solutions predict a maximum rate of packing that behaves asymptotically like the best known lower bound [10]. The same result has been obtained in Ref. [12] where the liquid entropy is computed in the hypernetted chain approximation. To discuss the exactness of the cavity free entropy we also consider some interpolation techniques which connect the cavity free entropy to the true entropy of the system [22–24]. In the 1RSB case, we provide an approximate solution of the SP equations to calculate the conﬁgurational entropy, deﬁned as the (log of) the number of pure states or clusters in the solution space. This quantity acquires a nonzero value at a clustering transition where the liquid entropy is still positive. The maximum rate of packing that is achievable still coincides with the one obtained in the RS approximation. Finally we design a message passing algorithm based on the BP equations which allows us to ﬁnd dense packings in not too large dimensions. These packings are typically hard to ﬁnd by other simple methods like Monte Carlo based algorithms. Unfortunately the computation time and memory increase exponentially with the space dimension n, making larger dimensions very diﬃcult to explore. To partially overcome this problem we introduce an approximate update rule for the message passing algorithm which is restricted to a subspace and makes the computation more eﬃcient. This improvement, together with the distributive nature of the algorithm, could help to run the algorithm in larger dimensions. We also discuss another iterative algorithm that, given a packing conﬁguration of spheres with diameter d, ﬁnds another packing for larger diameter d + 1 by increasing the space dimension n. This is a polynomial time algorithm generating maximum packings in an ultrametric space. When applied to our problem, we ﬁnd crystalline packings predicted by the RS cavity equations for even sphere diameters. The structure of the paper is as follows. In Sec. II we deﬁne the problem more precisely and give a summary of known results. In Secs. III and IV we present the BP and SP equations and study their consequences for the hard sphere packing problem. In Sec. V we study the packing algorithms and Sec. VI is devoted to extension to the q-ary Hamming spaces. Finally the concluding remarks are given in Sec. VII. In the ﬁrst two appendixes we give the details of calculations for checking the stability of the BP solutions and deriving the SP equations. The interpolation methods and some of their properties are presented in Appendix C.
Agent Based Negotiation using Cloud - an Approach in E-Commerce<|sep|>In business negotiation two or more parties come together to find mutually agreeable  contractual decision. For negotiation process both parties must show their interest,  thus, negotiation can be complicated and lengthy process. When all parties have to  come to final decision, negotiation will be stopped. In negotiation, each individual  aim to achieve the best possible outcome for their organization.        Negotiator is an individual representing an organization which listens to all the  parties’ decision carefully and takes his own decision which gives profit to his  organization. In negotiation process organization profit depends on organization's  negotiator so that negotiator needs to understand situation and all other organization's  negotiator. Negotiator must know how to negotiate well to successfully close deals,  avoid conflicts, and establish better relations among the other organization's  negotiators making the organization a better place to work. For successful negotiation  individuals or negotiator must learn to compromise and stop finding faults in each  other. Today cloud computing is widely used and is becoming a popular technology.  Cloud is a remote server, where the user can store their data and access the data  remotely whenever it's required. Cloud computing provides security and privacy to  the user data. User has no burden in maintaining huge amount of data stored on cloud.  Cloud computing is sometimes referred to as "on-demand resources" and is usually  based on pay-per-use basis. If business owner requires more space as compared to his  previous space on cloud, owner can easily request for additional data storage on the  cloud. Also owner can easily request for additional bandwidth, processing speed, and  additional licenses. Using cloud computing, user can access information from any  device like desktop, minicomputer, mobile etc. anywhere and anytime.  Amazon, Microsoft, Openstack, Google all these are the cloud providers. Google  Apps is one example of cloud. provider and a consumer using cloud. In this system, all product information and  multiple agent details are stored on the cloud. Both provider and consumer will select  their agents through cloud for negotiation. An agent acts as a negotiator. Agent has  user’s details and their requirements for a particular product. Using user’s  requirement, agents negotiate on certain features or issues.
Generalized conditional symmetries of evolution equations<|sep|>Generalized conditional symmetries provide an eﬀective method for ﬁnding exact solutions of evolution equations. Similarly to other such methods [44], it can be viewed as an instance of the general method of diﬀerential constraints [60, 65] (or “side conditions” [44]). Within the framework of empiric compatibility theory, generalized conditional symmetries as diﬀerential constraints compatible with an initial equation were investigated by Olver [43] in order to justify the method on “nonlinear separation” of variables by Galaktionov [20]. Another interpretation of generalized conditional symmetries of an evolution equation is to consider them as invariant manifolds of this equation, i.e., manifolds in appropriate jet spaces that are invariant under the ﬂow generated by the equation. This is the terminology in which generalized conditional symmetries of systems of evolution equations were ﬁrst studied by Kaptsov [1, 32] although the importance of invariant manifolds of evolution equations was understood much earlier [36]. From the symmetry point of view, the notion of generalized conditional symmetry arises by merging the notions of generalized and conditional symmetries, cf. Section 2. The idea of signiﬁcantly extending Lie symmetries of diﬀerential equations by including derivatives of the relevant dependent variables in the coeﬃcients of the associated inﬁnitesimal generators ﬁrst appeared in the fundamental paper of Noether [40] in connection with her study of conservation laws. Symmetries of this kind are called, e.g., generalized [42], Lie–B¨acklund [8, 27] or higher-order [6] symmetries in the literature. See additionally the excellent sketch on the history of generalized symmetries and relevant terminology in [42, p. 374–377]. The concept of conditional symmetries arose much later. Its origin can be traced back to the thesis of Bluman [5] and the paper by Bluman and Cole [7], where it was presented in terms of “nonclassical groups” or the “nonclassical” method of ﬁnding similarity solutions, respectively, cf. the detailed discussion in [6, Section 5.2.2]. A version of the corresponding invariance criterion explicitly taking into account the diﬀerential consequences involved in the process was ﬁrst proposed by Fushchych and Tsyfra in [18]. Combining results of [14, 18] and other previous papers, in [13] Fushchych introduced the general concept of conditional invariance. Around this time the terms “conditional invariance” and “Q-conditional invariance” began to be used regularly in connection with the method of Bluman and Cole and soon evolved into the terms Q-conditional [16] or, simply, conditional [19] and nonclassical [37] symmetry. The notions of generalized and conditional symmetries were merged, within the framework of symmetry analysis of diﬀerential equations, by Fokas and Liu [12] in the special case when evolution equations and symmetries do not explicitly involve the time variable and by Zhdanov [67] in the general case. The variety of possible interpretations and related notions and a number of diﬀerent names for the parent notions of conditional and generalized symmetries leads to the diversity of names used for generalized conditional symmetry in the literature. We have already mentioned the terms “invariant manifold” [1, 3, 32] (resp. “invariant set” [21, 24]) and “compatible diﬀerential constraint” [43]. Additionally, combining names of the parent notions of symmetries leads, in particular, to the terms “conditional Lie-B¨acklund symmetry” [29, 30, 67] and “higher (or higher order) conditional symmetry” [4, 68]. Sometimes special names are used for particular cases of generalized conditional symmetries. For example, linear compatible diﬀerential constraints for diﬀusion–reaction equations were called “additional generating conditions” in [9]. For uniformity, we will use the term “generalized conditional symmetry” [12, 53, 54] throughout the paper. This will additionally emphasize the relation of this notion to symmetry analysis although the nature even of usual conditional symmetries is in fact closer to compatibility theory, cf. [35]. The main purpose of this paper is to investigate basic problems concerning generalized conditional symmetry of (1 + 1)-dimensional evolution equations of the general form where r ⩾ 1, ut = ∂u/∂t, u0 := u, uk = ∂ku/∂xk, u(r,x) = (u0, u1, . . . , ur) and Hur ̸= 0. Among these problems are the comparative analysis of diﬀerent versions of the conditional invariance criterion, the study of the possibility of solving the corresponding determining equations as well as relating generalized conditional symmetries to the concept of reduction, multiparametric families of solutions and diﬀerent notions of compatibility for overdetermined systems of partial diﬀerential equations. Most results of the paper can be extended to systems of (1+1)-dimensional evolution equations if certain restrictions for generalized conditional symmetries are imposed, cf. [1]. We restrict our consideration to single evolution equations for the sake of clarity of presentation. Throughout the paper we denote by E a ﬁxed equation of the form (1). The indices a and b run from 1 to ρ, and we use the summation convention for repeated indices. Bar over a letter denotes a tuple of ρ consecutive values. Subscripts of functions denote diﬀerentiation with respect to the corresponding variables, ∂t = ∂/∂t, ∂x = ∂/∂x, ∂u = ∂/∂u and utk = ∂k+1u/∂t∂xk. We also will use another notation for derivatives: uα = uα0,α1 = ∂|α|u/∂tα0∂xα1, where α = (α0, α1) is a multiindex, α0, α1 ∈ N ∪ {0} and |α| = α0 + α1, so that uk = u0,k and utk = u1,k. Any function is considered as its zero-order derivative. Dt = ∂t + uα0+1,α1∂uα and Dx = ∂x + uα0,α1+1∂uα are the operators of total diﬀerentiation with respect to the variables t and x, respectively. All our considerations are carried out in the local setting. In the next section we discuss prerequisites for introducing the notion of generalized conditional symmetries in symmetry analysis and present diﬀerent versions of the corresponding invariance criterion for single evolution equations. Relations of generalized conditional symmetries to formal compatibility and passivity of certain overdetermined systems of partial diﬀerential equations as well as to involutivity of certain systems of vector ﬁelds are established in Sections 3, 4 and 5, respectively. For this purpose we employ a weight of derivatives instead of the usual order (Section 3) and a ranking of derivatives (Section 4), which are associated with evolution equations of a ﬁxed order. Reductions of evolution equations with special ansatzes are studied in Section 6. The Zhdanov theorem [67, 68] (see also [4]) on the connection of generalized conditional symmetries of an evolution equation with ansatzes of a special form reducing this equation is also revisited. This leads to new results on the correspondence between generalized conditional symmetries, ansatzes and parametric families of solutions of evolution equations. In Section 7 we prove a no-go theorem on determining equations for generalized conditional symmetries of evolution equations. Roughly speaking, it is shown that solving the determining equations is equivalent to solving the original equations. An interpretation of usual conditional symmetries of evolution equations as special generalized conditional symmetries is given in Section 8 and is then illustrated by a new nontrivial example. Properties of generalized conditional symmetries of evolution equations are summed up in the conclusion.
A surface moving mesh method based on equidistribution and alignment<|sep|>We are interested in methods that can directly move simplicial meshes on general surfaces with or without analytical expressions. Such surface moving mesh methods can be used for adaptation and/or quality improvements of surface meshes and thus are useful for computational geometry and numerical solutions of partial diﬀerential equations (PDEs) deﬁned on surfaces; e.g., see [7, 10, 25]. There has been some work done on mesh movement and adaptation for surfaces. For example, Crestel et al. [5] present a moving mesh method for parametric surfaces by generalizing Winslow’s meshing functional to Riemannian manifolds and taking into consideration the Riemannian metric associated with the manifolds. The method is simpliﬁed and implemented on a two-dimensional domain for surfaces that accept certain parameterizations. Weller et al. [3] and McRae et al. [24] solve a Monge-Amp´ere type equation on the surface of the sphere to generate optimally transported meshes that become equidistributed with respect to a suitable monitor function. MacDonald et al. [23] devise a moving mesh method for the numerical simulation of coupled bulk-surface reaction-diﬀusion equations on an evolving two-dimensional domain. They use a one-dimensional moving mesh equation in arclength to concentrate mesh points along the evolving domain boundary. Dassi et al. [6] generalize the higher embedding approach proposed in [1]. They modify the embedding map between the underlying surface and R6 to include more information associated with the physical solution and its gradient. The idea behind this mapping is that it essentially approximates the geodesic length on the surface via a Euclidean length in R6. The mesh adapts in the Euclidean space and then is mapped back to the physical domain. The objective of this paper is to present a surface moving mesh method for general surfaces with or without explicit parameterization. The method can be viewed as a nontrivial extension of the moving mesh PDE (MMPDE) method that has been developed for bulk meshes and demonstrated to work well for various applications; e.g. see [19, 20, 21]. The main challenges in the development of surface mesh movement come from the fact that the Jacobian matrix of the aﬃne mapping between the reference element and any simplicial surface element is not square. To overcome these challenges, we start by connecting the area of the surface element in the Euclidean metric or a Riemannian metric with the Jacobian matrix. This connection allows us to formulate the equidistribution and alignment conditions and ultimately, form a meshing energy function for surface meshes. This meshing function is similar to a discrete version of Huang’s functional [13, 17, 22] for bulk meshes which has been proven to work well in a variety of problems. Following the MMPDE approach, we deﬁne the surface moving mesh equation as the gradient system of the meshing function, with the nodal mesh velocities being projected onto the underlying surface. The analytical expression for the mesh velocities is obtained in a compact, matrix form, which makes the implementation of the new method on a computer relatively easy and robust. Several theoretical properties are obtained for the surface moving mesh method. In particular, it is proven that a surface mesh generated by the method remains nonsingular for all time if it is so initially. Moreover, the element altitudes and areas of the physical mesh are bounded below by positive constants depending only on the initial mesh, the number of elements, and the metric tensor that is used to provide information on the size, shape, and orientation of the elements throughout the surface. Furthermore, limiting meshes exist and the meshing function is decreasing along each mesh trajectory. These properties are veriﬁed in numerical examples. It is emphasized that the new method is developed directly on surface meshes, making no use of any information on surface parameterization. It utilizes surface normal vectors to ensure that the mesh vertices remain on the surface while moving, and also assumes that the initial surface mesh is given. Since the surface normal vectors can be computed even when the surface only has a numerical representation, the new method can apply to general surfaces with or without explicit parameterization. A selection of two- and three-dimensional examples are presented. This paper is organized as follows. In Section 2, the area formula for a surface element and the equidistribution and alignment conditions for surface meshes are established. The surface moving mesh equation is described in Section 3 and its theoretical analysis is given in Section 4. Numerical examples are then provided in Section 5 followed by conclusions and further remarks in Section 6. Appendix A contains the derivation of derivatives of the meshing function with respect to the physical coordinates.
The ALICE experiment -- A journey through QCD<|sep|>1.1 Emergent phenomena in QCD: the quark–gluon plasma . . . . . . . . . . . . . . . . . . 7 1.2 Experimental investigations of the QGP and QCD . . . . . . . . . . . . . . . . . . . . . 9 1.2.1 The evolution of a heavy-ion collision . . . . . . . . . . . . . . . . . . . . . . . 9 1.2.2 Observables in heavy-ion collisions . . . . . . . . . . . . . . . . . . . . . . . . 12 1.2.3 Small systems and searches for thresholds of QGP formation . . . . . . . . . . . 15 1.2.4 Theoretical tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.3 Evolution of the ﬁeld: from ﬁrst studies to ALICE and beyond . . . . . . . . . . . . . . 20 1.4 ALICE: design considerations, implementation, operation . . . . . . . . . . . . . . . . . 22 1.5 Overview of the key scientiﬁc questions addressed by ALICE . . . . . . . . . . . . . . . 27
Dependent Inductive and Coinductive Types are Fibrational Dialgebras<|sep|>It is a well-established fact that the semantics of inductive data types without term dependencies can be given by initial algebras, whereas the semantics of coinductive types can be given by ﬁnal coalgebras. However, for types that depend on terms, the situation is not as clear-cut. Partial answers for inductive types can be found in [3, 8, 9, 11, 14, 19, 20], where semantics have been given for inductive types through polynomial functors in the category of set families or in locally Cartesian closed categories. Similarly, semantics for non-dependent coinductive types have been given in [1, 2, 6] by using polynomial functors on locally Cartesian closed categories. Finally, an interpretation for Martin-L¨of type theory (without recursive type deﬁnitions) has been given in [21] and corrected in [16]. So far, we are, however, lacking a full picture of dependent coinductive types that arise as duals of dependent inductive types. To actually get such a picture, I extend in the present work Hagino’s idea [13], of using dialgebras to describe data types, to dependent types. This emphasises the actual structure behind (co)inductive types as their are used in systems like Agda.1 Moreover, dialgebras allow for a direct interpretation of types in this categorical setup, without going through translations into, for example, polynomial functors. Having deﬁned the structures we need to interpret dependent data types, it is natural to ask whether this structure is actually sensible. The idea, pursued here, is that we want to obtain initial and ﬁnal dialgebras from initial algebras and ﬁnal coalgebras for polynomial functors. This is achieved by showing that the dialgebras in this work correspond to algebras and coalgebras, and that their ﬁxed points can be constructed from ﬁxed points of polynomial functors (in the sense of [12]). 1It should be noted that, for example, Coq treats coinductive types differently. In fact, the route taken in Agda with copatterns and in this work is much better behaved.
Separation delay via hydro-acoustic control of a NACA4412 airfoil in pre-stalled conditions<|sep|>he present work explores the possibility of using wall-mounted resonating porous panels, modeled via impedance boundary conditions (IBCs), as a means to passively control the overlying turbulent boundary layer. The fundamental working principle of such control involves acoustic resonance in the panel, being triggered by the energy containing turbulent eddies, generating a wall-normal transpiration pattern at the ∗Assistant Professor, IASE-Supaero, Toulouse France †Assistant Professor, School of Mechanical Engineering, Purdue University, USA ‡Graduate Research Assistant, IASE-Supaero, Toulouse France §Department Head, IASE-Supaero, Toulouse France
